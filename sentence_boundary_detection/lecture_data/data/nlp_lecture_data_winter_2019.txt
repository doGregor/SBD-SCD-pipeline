Okay. Hello everyone.
[LAUGHTER] Okay we should get started.
Um, they're actually are still quite a few seats left.
If you wanna be really bold,
there are a couple of seats right in front of me in the front row.
If you're less bolder a few over there.
Um, but they're also on some of the rows are quite a few middle seat.
So if people wanted to be really civic minded some people could sort of
squeeze towards the edges and make more accessible um,
some of the seats that still exist in the classroom.
Okay. Um, so, um,
it's really exciting and great to see so many people here.
So I'm a hearty welcome to CS224N and occasionally also
known as Ling 284 which is Natural Language Processing with Deep Learning.
Um, as just a sort of a personal anecdote,
is still sort of blows my mind that so many people turn up to this class these days.
So, for about the first decade that I taught NLP here,
you know the number of people I got each year was approximately 45.
[LAUGHTER] So it's an order of [LAUGHTER] magnitude smaller than
it is now but guess it says quite a lot
on about what a revolutionary impact
that artificial intelligence in general and machine learning,
deep learning, NLP are starting to have in modern society.
Okay. So this is our plan for today.
So, um, um, we're really gonna get straight down to business today.
So they'll be a brief, very brief introduction some of the sort of course logistics,
very brief discussion and talk about human language and
word meaning and then we wanna get right into talking about um,
the first thing that we're doing which is coming up with word vectors and looking
at the word2vec algorithm and that will then sort of fill up the rest of the class.
There are still two seats right in
the front row for someone who wants to sit right in front of me,
just letting you know [LAUGHTER].
Okay. Okay. So here are the course logistics in brief.
So I'm Christopher Manning,
the person who bravely became the head TA is Abigail See is right there.
And then we have quite a lot of wonderful TA's.
To the people who are wonderful TA's just sort of stand up for one moment.
So, um, [LAUGHTER] we have some sense for wonderful TAs.
[LAUGHTER] Okay great.
Um, okay.
So you know when the lecture is because you made it
here and so welcome also to SCPD people.
This is also an SCPD class and you can watch it on video.
But we love for Stanford students to turn
up and show their beautiful faces in the classroom.
Okay. So, um, the web-page has all the info about syllabus et cetera et cetera.
Okay. So this class what do we hope to teach?
So, one thing that we wanna teach is, uh, you know,
an understanding of effective modern methods for deep learning.
Starting off by reviewing some of the basics and then
particularly talking about the kinds of techniques including um,
recurrent networks and attention that are widely
used for natural language processing models.
A second thing we wanna teach is a big picture understanding of
human languages and some of the difficulties in understanding and producing them.
Of course if you wanna know a lot about human languages,
there's a whole linguistics department and you can do a lot of courses of that.
Um, but so I wanna give at least some appreciation so you have some clue of what are
the challenges and difficulties and varieties of human languages.
And then this is also kind of a practical class.
Like we actually wanna teach you how you can
build practical systems that work for some of the major parts of NLP.
So if you go and get a job at one of those tech firms and they say "Hey,
could you build us a named entity recognizer?"
You can say "Sure, I can do that."
And so for a bunch of problems,
obviously we can't do everything,
we're gonna do word meaning,
dependency parsing, machine translation and you have an option to do question answering,
I'm actually building systems for those.
If you'd been talking to friends who did the class in the last couple of years,
um, here are the differences for this year just to get things straight.
Um, so we've updated some of the content of the course.
So, uh, between me and guest lectures there's new content.
Well that look bad.
Wonder if that will keep happening, we'll find out.
There's new content and on various topics that are sort of developing areas.
One of the problems with this course is really big area of deep learning at
the moment is still just developing really really quickly.
So, it's sort of seems like one-year-old content is already
things kind of data and we're trying to update things.
A big change that we're making this year is we're
having five-one week assignments instead of
three-two week assignments at the beginning of
the course and I'll say a bit more about that in a minute.
Um, this year we're gonna use PyTorch instead of TensorFlow,
and we'll talk about that more later too.
Um, we're having the assignments due before class on either Tuesday or Thursday.
So you're not distracted and can come to class.
So starting off, um, yeah.
So we're trying to give an easier,
gentler ramp-up but on the other hand a fast ramp-up.
So we've got this first assignment which is sort of easy, uh,
but it's available right now and is due next Tuesday.
And the final thing is we're not having a midterm this year.
Um, okay.
So this is what we're doing.
So there are five of these assignments that I just mentioned.
Um, So six percent for the first one,
12 percent for each of the other ones,
um, and, I already said that.
We're gonna use gradescope for grading.
It'll be really help out the TAs if you could use
your SUnet ID as your gradescope account ID.
Um, so then for the second part of the course,
people do a final project and there are two choices for the final project.
You can either do our default final project,
which is a good option for many people,
or you can do a custom final project and I'll
talk about that in the more in the beginning.
This is not working right.
Um, and so then at the end we have
a final poster presentation session at which your attendance is expected,
and we're gonna be having that Wednesday in the evening.
Probably not quite five hours but it'll be within that window,
we'll work out the details in a bit.
Three percent for participation,
see the website for details.
Six late days, um,
collaboration, like always in computer science classes,
we want you to do your own work and not borrow stuff from other people's Githubs and
so we really do emphasize that you should
read and pay attention to collaboration policies.
Okay. So here's the high level plan for the problem sets.
So, homework one available right now,
is a hopefully easy on ramp.
That's on iPython notebook,
just help get everyone up to speed.
Homework two is pure Python plus numpy but that
will start to kind of teach you more about the sort of underlying,
how do we do deep learning.
If you're not so good or a bit rusty or never seen um,
Python or numpy, um,
we're gonna have an extra section on Friday.
So Friday from 1:30 to 2:50 um,
in Skilling Auditorium, we'll have a section that's a Python review.
That's our only plan section at the moment,
we're not gonna have a regular section.
Um, so encourage to go to that and that will also be
recorded for SCPD and available for video as well.
Um, then Homework three um,
will start us on using PyTorch.
And then homeworks four and five we're then gonna be using
py- PyTorch on GPU and we're actually gonna be using
Microsoft Azure with big thank yous to the kind Microsoft Azure people who have
sponsored our GPU computing for the last um, three years.
Um, yes. So basically I mean all of modern deep learning has moved to the use
of one or other of the large deep learning libraries like PyTorch TensorFlow,
Chainer or MXNet um,
et cetera and then doing the computing on GPU.
So of course since we're in the one building,
we should of course be using, um,
GPUs [LAUGHTER] but I mean in general
the so parallelisms scalability of GPUs is what's powered most of modern deep learning.
Okay. The final project.
So for the final project there are two things that you can do.
So we have a default final project which is essentially our final project in a box.
And so this is building a question answering system and we do it over the squad dataset.
So what you build and how you can improve your performance is completely up to you.
It is open-ended but it has an easier start,
a clearly defined objective and we can
have a leaderboard for how well things are working.
Um, so if you don't have a clear research objective that can be a good choice for you
or you can propose the custom Final Project and assuming it's sensible,
we will approve your custom final project,
we will give you feedback, um,
form someone as a mentor, um,
and either way for only the final project we allow teams of one, two or three.
For the homework should expect it to do them yourself.
Of course you can chat to people in a general way about the problems.
Okay. So that is the course.
All good, and not even behind schedule yet.
Okay. So the next section is human language and word meaning.Um.
You know, if I was um,
really going to tell you a lot about human language that would take a lot of time um,
which I don't really have here.
So I'm just going to tell you um,
two anecdotes about human language.
And the first is this XKCD cartoon.
Um, and I mean this isn't,
and I don't know why that's happening.
I'm not sure what to make of that.
Um, so, I actually really liked this XKCD cartoon.
It's not one of the classic ones that you see most often around the place,
but I actually think it says a lot about language and is worth thinking about.
Like I think a lot of the time for the kind of people who come
to this class who are mainly people like CS people,
and EE people and random others.
There's some other people I know since these people linguists and so on around.
But for a lot of those people like,
you've sort of spent your life looking at formal languages and the impression
is that sort of human language as a sort of somehow a little bit broken formal languages,
but there's really a lot more to it than that, right?
That language is this amazing um,
human created system that is used for
all sorts of purposes and is adaptable to all sorts of purposes.
So you can do everything from describing mathematics and human language
um to sort of nuzzling up to your best friend and getting them to understand you better.
So there's actually an amazing thing of human language. Anyway, I'll just read it.
Um, so it's the first person,
the dark haired person says,
"Anyway, I could care less."
And her friend says,
"I think you mean you couldn't care less."
Saying you could care less implies you care at least some amount.
And the dark haired person says, "I don't know,
we're these unbelievably complicated brains drifting through a void trying
in vain to connect with one another by blindly flinging words out into the darkness."
Every choice of phrasing and spelling, and tone,
and timing carries countless signals and contexts and subtexts and more.
And every listener interprets those signals in their own way.
Language isn't a formal system,
language is glorious chaos.
You can never know for sure what any words will mean to anyone.
All you can do is try to get better at guessing how your words affect people so
you can have a chance of finding the ones that will make
them feel something like what you want them to feel.
Everything else is pointless.
I assume you're giving me tips on how you interpret
words because you want me to feel less alone.
If so, thank you.
That means a lot.
But if you're just running my sentences past
some mental checklist so you can show off how well you know it,
then I could care less.
[NOISE] Um, and so I think um,
I think actually this has some nice messages about how language is this uncertain
evolved system of communication but somehow we have enough agreed meaning that you know,
we can kind of pretty much communicate.
But we're doing some kind of you know
probabilistic inference of guessing what people mean and we're
using language not just for
the information functions but for the social functions etc etc.
Okay. And then here's my one other thought I had review about language.
So, essentially if we want to have artificial intelligence that's intelligent,
what we need to somehow get to the point of having
compu- computers that have the knowledge of human beings, right?
Because human beings have knowledge that gives them intelligence.
And if you think about how we sort of
convey knowledge around the place in our human world,
mainly the way we do it is through human language.
You know, some kinds of knowledge you can sort of
work out for yourself by doing physical stuff right,
I can hold this and drop that and I've learnt something.
So I have to learn a bit of knowledge there.
But sort of most of the knowledge in your heads and why you're sitting in
this classroom has come from people communicating in human language to you.
Um, so one of the famous,
most famous steep learning people Yann Le Cun,
he likes to say this line about,
oh, you know really I think that you know there's not much difference
between the intelligence of human being and orangutan.
And I actually think he's really wrong on that.
Like the sense in which he means that is,
an orangutan has a really good vision system.
Orangutans have very good you know control of
their arms just like human beings for picking things up.
Orangutans um can use tools um and orangutans can make plans so
that if you sort of put the food somewhere where they have to sort of move
the plank to get to the island with the food they can do a plan like that.
So yeah, in a sense they've got a fair bit of intelligence but you know,
sort of orangutans just aren't like human beings.
And why aren't they like human beings?
And I'd like to suggest to you the reason for that is what human beings have achieved is,
we don't just have sort of one computer like
a you know dusty old IBM PC in your mother's garage.
What we have is a human computer network.
And the way that we've achieved that human computer network is that,
we use human languages as our networking language.
Um, and so, when you think about it um,
so on any kind of evolutionary scale language is super super super super recent, right?
That um, creatures have had vision for people don't quite know but you know,
maybe it's 75 million years or maybe it's longer, right?
A huge length of time.
How long have human beings have had language?
You know people don't know that either because it turns out you know,
when you have fossils,
you can't knock the skull on the side and say,
do you not have language.
Um, but you know, most people estimate that sort of language is
a very recent invention before current human beings moved out of um, out of Africa.
So that many people think that we've only had language for
something like a 100,000 years or something like that.
So that's sort of you know blink of an eye on the evolutionary timescale.
But you know, it was the development of language [inaudible]
that sort of made human beings invisible- [NOISE] in invincible, right?
It wasn't that, human beings um,
developed poison fangs or developed ability to run
faster than any other creature or
put a big horn on their heads or something like that, right?
You know, humans are basically pretty puny um,
but they had this um,
unbeatable advantage that they could communicate with
each other and therefore work much more effectively in teams.
And that sort of basically made human beings invincible.
But you know, even then humans were kind of limited, right?
That kind of got you to about the Stone Age right,
where you could bang on your stones and with
the right kind of stone make something sharp to cut with.
Um, what got humans beyond that,
was that they invented writing.
So writing was then an ability where you could take knowledge
not only communicated um mouth to mouth to people that you saw.
You could put it down on your piece of papyrus so your clay tablet or whatever
it was at first and that knowledge could then be sent places.
It could be sent spatially around the world and it could then
be sent temporally through time.
And well, how old is writing?
I mean, we sort of basically know about how old writing is, right?
That writing is about 5,000 years old.
It's incredibly incredibly recent on this scale of evolution but you know,
essentially writing was so powerful as a way of having knowledge that then in those 5,000
years that enabled human beings to go from stone age sharp piece or flint to you know,
having iPhones and all of these things,
all these incredibly sophisticated devices.
So, language is pretty special thing I'd like to suggest.
Um, but you know, if I go back to my analogy that sort of it's allowed humans to
construct a networked computer that is way way more powerful than um,
just having individual creatures as sort of intelligent like an orangutan.
Um, and you compare it to our computer networks,
it's a really funny kind of network, right?
You know that these days um,
we have networks that run around where we have sort of large network bandwidth, right?
You know, we might be frustrated sometimes with
our Netflix downloads but by and large you know,
we can download hundreds of megabytes really easily and quickly.
And we don't think that's fast enough,
so we're going to be rolling out 5G networks.
So it's an order of magnitude faster again.
I mean, by comparison to that, I mean,
human language is a pathetically slow network, right?
That the amount of information you can convey by human language is very slow.
I mean you know, whatever it is I sort of speak at about 15 words a second right,
you can start doing um,
your information theory if you know some right?
But um, you don't actually get much bandwidth at all.
And that then leads- so you can think of,
how does it work then?
So, humans have come up with
this incredibly impressive system which is essentially form of compression.
Sort of a very adaptive form of compression,
so that when we're talking to people,
we assume that they have an enormous amount of knowledge in their heads which
isn't the same as but it's broadly similar to mine when I'm talking to you right?
That you know what English words mean,
and you know a lot about how the wor- world works.
And therefore, I can say a short message and communicate
only a relatively short bit string and you can actually understand a lot. All right?
So, I can say sort of whatever you know,
imagine a busy shopping mall and that
there are two guys standing in front of a makeup counter,
and you know I've only said whatever that was sort of about 200 bits of
information but that's enabled you to construct
a whole visual scene that we're taking megabytes to um,
represent as an image.
So, that's why language is good.
Um, so from that more authorial level,
I'll now move back to the concrete stuff.
What we wanna do in this class is not solve the whole of language,
but we want to represent, um,
the meaning of words, right?
So, a lot of language is bound up in words and their meanings
and words can have really rich meanings, right?
As soon as you say a word teacher,
that's kinda quite a lot of rich meaning or you can have actions that have rich meaning.
So, if I say a word like prognosticate or,
um, total or something you know,
these words that have rich meanings and a lot of nuance on them.
And so we wanna represent meaning.
And so, the question is what is meaning?
So, you can of course you can- dictionaries are meant to tell you about meanings.
So, you can look up dictionaries um,
and Webster says sort of tries to relate meaning to idea.
The idea that is represented by a word or a phrase.
The idea that a person wants to express by word signs et cetera.
I mean, you know,
you could think that these definitions are kind of a cop-out because it seems
like they're rewriting meaning in terms of the word idea,
and is that really gotten you anywhere.
Um, how do linguists think about meaning?
I mean, the most common way that linguists have thought about
meaning is an idea that's called denotational
semantics which is also used in programming languages.
So, the idea of that is we think of meaning as what things represent.
So, if I say the word chair,
the denotation of the word chair includes this one here and that one,
that one, that one, that one.
And so, the word chair is sort of representing
all the things that are chairs and you can sort of, um,
you can then think of something like running as well that you know there's sort of sets
of actions that people can partake that- that's their denotation.
And that's sort of what you most commonly see in philosophy or linguistics as denotation.
It's kind of a hard thing to get your hands on, um, computationally.
So, um, what type of people most commonly
do or use the most commonly do I guess I should say now
for working out the meaning of words on the computer that
commonly that turn to something that was a bit like a dictionary.
In particular favorite online thing was this online thesaurus called WordNet which
sort of tells you about word meanings and relationships between word meanings.
Um, so this is just giving you the very slices sense of,
um, of what's in WordNet.
Um, so this is an actual bit of Python code up there which you can,
um, type into your computer and run and do this for yourself.
Um, so this uses a thing called NLTK.
Um, so NLTK is sort of like
the "Swiss Army Knife of NLP" meaning that it's not terribly good for anything,
but it has a lot of basic tools.
So, if you wanted to do something like just get some stuff out of WordNet and show it,
it's the perfect thing to use. Um, okay.
So, um, from NLTK I'm importing WordNet and so then I can say,
"Okay, um, for the word good tell me about the synonym sets with good participates in."
And there's good goodness as a noun.
There is an adjective good.
There's one estimable good, honorable, respectable.
Um, this looks really complex and hard to understand.
But the idea of word- WordNet makes
these very fine grain distinctions between senses of a word.
So, what sort of saying for good, um,
there's what some sensors where it's a noun, right?
That's where you sort of,
I bought some goods for my trip, right?
So, that's sort of, um,
one of these noun sensors like this one I guess.
Um, then there are adjective sensors and it's trying to
distinguish- there's a basic adjective sense of good being good,
and then in certain, um, sensors,
there are these extended sensors of good in different directions.
So, I guess this is good in the sense of beneficial, um,
and this one is sort of person who is respectable or something.
He's a good man or something like that, right?
So, um, but you know,
part of what's kind of makes us
think very problematic and practice to use is it tries to make
all these very fine-grain differences between sensors that are a human being can
barely understand the difference between them um, and relate to.
Um, so you can then do other things with WordNet.
So, this bit of code you can sort of well walk up and is a kind of hierarchy.
So, it's kinda like a traditional, um, database.
So, if I start with a panda and say- [NOISE] if I start with a panda.
Um, and walk up, um,
the pandas are [inaudible].
Maybe you'd guys to bio which are carnivores,
placentals, mammals, blah, blah, blah.
Okay, so, um, that's the kind of stuff you can get out to- out of WordNet.
Um, you know, in practice WordNet has been.
Everyone sort of used to use it because it gave
you some sort of sense of the meaning of the word.
But you know it's also sort of well-known.
It never worked that well.
Um, so you know that sort of the synonym sets miss a lot of nuance.
So, you know one of the synonym sets for good has
proficient in it and good sort of like proficient
but doesn't proficient have some more connotations and nuance?
I think it does.
Um, WordNet like most hand built resources is sort of very incomplete.
So, as soon as you're coming to new meanings of words,
or new words and slang words,
well then, that gives you nothing.
Um, it's sort of built with human labor,
um, in ways that you know it's hard to sort of create and adapt.
And in particular, what we want to focus on is,
seems like a basic thing you'd like to do with words and it's actually at least
understand similarities and relations between the meaning of words.
And it turns out that you know WordNet doesn't actually do that that well
because it just has these sort of fixed discrete synonym sets.
So, if you have a words in a synonym said that there's
sort of a synonym and maybe not exactly the same meaning,
they're not in the same synonyms set,
you kind of can't really measure the partial resemblance as a meaning for them.
So, if something like good and marvelous aren't in the same synonym set,
but there's something that they share in common that you'd like to represent.
Okay. So, um, that's kinda turn to lead into
us wanting to do something different and better for word meaning.
And, um, before getting there I just sort of wanna again sort
of build a little from traditional NLP.
So, traditional NLP in the context of this course sort of means
Natural Language Processing up until approximately 2012.
There were some earlier antecedents but as basically, um,
in 2013 that things really began to change with
people starting to use neural net style representations for natural language processing.
So, up until 2012,
um, standardly you know we had words.
They are just words. So, we had hotel conference motel.
They were words, and we'd have you know lexicons and put words into our model.
Um, and in neural networks land this is referred to as a localist representation.
I'll come back to those terms again next time.
But that's sort of meaning that for any concept there's sort of one particular,
um, place which is the word hotel or the word motel.
A way of thinking about that is to think
about what happens when you build a machine learning model.
So, if you have a categorical variable like you have words with the choice of word
and you want to stick that into some kind of classifier in a Machine Learning Model,
somehow you have to code that categorical variable,
and the standard way of doing it is that you code it by having
different levels of the variable which means that you have a vector,
and you have, this is the word house.
This is the word cat. This is the word dog.
This is the word some chairs.
This is the word agreeable.
This is the word something else.
This is the word, um,
hotel, um, and this is another word for something different, right?
So that you have put a one at the position
and neural net land we call these one-hot vectors,
and so these might be, ah,
one-hot vectors for hotel and motel.
So, there are a couple of things that are bad here.
Um, the one that's sort of, ah,
practical nuisance is you know languages have a lot of words.
Ah, so, it's sort of one of those dictionaries that you might have still had in
school that you probably have about 250,000 words in them.
But you know, if you start getting into
more technical and scientific English it's easy to get to a million words.
I mean, actually the number of words that you have in a language, um,
like English is actually infinite because we have
these processes which are called derivational morphology,
um, where you can make more words by adding endings onto existing words.
So, you know you can start with something like paternalist,
fatherly, and then you can sort of say from maternal,
you can say paternalist, or paternalistic,
paternalism and pa- I did it paternalistically.
Right? Now all of these ways that you can bake bigger words by adding more stuff into it.
Um, and so really you end up with an infinite space of words.
Um, yeah. So that's a minor problem, right?
We have very big vectors if we want to represent a sensible size vocabulary.
Um, but there's a much bigger problem than that, which is, well,
precisely what we want to do all the time, is we want to,
sort of, understand relationships and the meaning of words.
So, you know, an obvious example of this is web search.
So, if I do a search for Seattle motel,
it'd be useful if it also showed me results that had
Seattle hotel on the page and vice versa because,
you know, hotels and motels pretty much the same thing.
Um, but, you know, if we have these one-hot vectors like we had before they have
no s- similarity relationship between them, right?
So, in math terms,
these two vectors are orthogonal.
No similarity relationship between them.
Um, and so you,
kind of, get nowhere.
Now, you know, there are things that you could do,
I- I just showed you WordNet's.
WordNet's shows you some synonyms and stuff.
So that might help a bit.
There are other things you could do.
You could sort of say, well wait,
why don't we just build up a big table where we have a big table of,
um, word similarities, and we could work with that.
And, you know, people used to try and do that, right?
You know, that's sort of what Google did in 2005 or something.
You know, it had word similarity tables.
The problem with doing that is you know,
we were talking about how maybe we want 500,000 words.
And if you want to build up then a word similarity table out
of our pairs of words from one-hot representations,
um, you- that means that the size of that table,
as my math is pretty bad,
is it 2.5 trillion?
It's some very big number of cells in your similarity, um, matrix.
So that's almost impossible to do.
So, what we're gonna instead do is explore a method in which,
um, we are going to represent words as vectors,
in a way I'll show you just, um,
a minute in such a way that just the representation of
a word gives you their similarity with no further work.
Okay. And so that's gonna lead into these different ideas.
So, I mentioned before denotational semantics.
Here's another idea for representing the meaning of words,
um, which is called distributional semantics.
And so the idea of distributional semantics is, well,
how are we going to represent the meaning of a word is by looking at the contexts,
um, in which it appears.
So, this is a picture of JR Firth who was a British linguist.
Um, he's famous for this saying,
"You shall know a word by the company it keeps."
Um, but another person who's very famous for developing this notion of meaning is, um,
the philosopher Ludwig- Ludwig Wittgenstein in his later writings,
which he referred to as a use theory of meeting- meaning.
Well, actually he's- he used some big German word that I don't know,
but, um, we'll call it a use theory of meaning.
And, you know, essentially the point was, well, you know,
if you can explain every- if- if you can
explain what contexts it's correct to use a certain word,
versus in what contexts would be the wrong word to use,
this maybe gives you bad memories of doing English in high school,
when people said, ah, that's the wrong word to use there,
um, well, then you understand the meaning of the word, right?
Um, and so that's the idea of distributional semantics.
And it's been- so one of the most successful ideas in
modern statistical NLP because it gives you a great way to learn about word meaning.
And so what we're gonna do is we're going to say,
haha, I want to know what the word banking means.
So, I'm gonna grab a lot of texts,
which is easy to do now when we have the World Wide Web,
I'll find lots of sentences where the word banking is used,
Government debt problems turning into banking crises as happened in 2009.
And both these- I'm just going to say all of
this stuff is the meaning of the word banking.
Um, that those are the contexts in which the word banking is used.
And that seems like very simple and perhaps even not quite right idea,
but it turns out to be a very usable idea that does a great job at capturing meaning.
And so what we're gonna do is say rather than
our old localist representation we're now gonna
represent words in what we call a distributed representation.
And so, for the distributed representation we're still going
to [NOISE] represent the meaning of a word as a numeric vector.
But now we're going to say that the meaning of each word is,
ah, smallish vector, um,
but it's going to be a dense vector where by all of the numbers are non-zero.
So the meaning of banking is going to be
distributed over the dim- dimensions of this vector.
Um, now, my vector here is of dimension nine because I want to keep the slide, um, nice.
Um, life isn't quite that good in practice.
When we do this we use a larger dimensionality,
kinda, solid the minimum that people use is 50.
Um, a typical number that you might use on your laptop is
300 if you want to really max out performance,
um, maybe 1,000, 2,000, 4,000.
But, you know, nevertheless [NOISE] orders of magnitude is
smaller compared to a length 500,000 vector.
Okay. So we have words with their vector representations.
And so since each word is going to have a vector, um,
representation we then have a vector space in which we can place all of the words.
Um, and that's completely unreadable, um,
but if you zoom into the vector space it's still completely unreadable.
But if you zoom in a bit further,
um, you can find different parts of this space.
So here's the part that where countries attending to,
um, exist Japanese, German,
French, Russian, British Australian American,
um, France, Britain, Germany et cetera.
And you can shift over to a different part of the space.
So here's a part of the space where various verbs are,
so has have, had, been, be.
Oops. Um, um, [inaudible] be always was where.
You can even see that some morphological forms are grouping together,
and things that sort of go together like say,
think expect to things that take those, kind of, compliment.
He said or thought something.
Um, they group together.
Now, what am I actually showing you here?
Um, you know, really this was built from,
ah, 100 dimensional word vectors.
And there is this problem is really hard to visualize 100 dimensional word vectors.
So, what is actually happening here is these, um,
100 dimensional word vectors are being projected down into two-dimensions,
and you're so- seeing the two-dimensional view,
which I'll get back to later.
Um, so, on the one hand, um,
whenever you see these pictures you should hold on to
the your wallet because there's a huge amount of
detail on the original vector space that got completely killed and went away, um,
in the 2D projection,
and indeed some of what push things together in the 2D,
um, projection may really, really,
really misrepresent what's in the original space.
Um, but even looking at these 2D representations,
the overall feeling is,
my gosh this actually sort of works, doesn't it?
Um, we can sort of see similarities, um, between words.
Okay. So, um, ha- so that was the idea of what we want to do.
Um, the next part, um,
is then how do we actually go about doing it?
I'll pause for breath for half a minute.
Has anyone got a question they're dying to ask?
[NOISE] Yeah.
Where were the- the vectors is each, um,
had a different order in each contact,
like, say the first decimal vector,
second decimal vector, are those standard
across all theory or people choose them themselves?
Um, they're not standards across NLP um and they're not chosen at all.
So what we're gonna present is a learning algorithm.
So where we just sort of shuffle in lots of text
and miraculously these word vectors come out.
And so the l- learning algorithm itself decides the dimensions.
But um that actually reminds me of something I sort of meant to say which was yeah,
I mean, since this is a vector space,
in some sense the dimensions over the arbitrary right,
because you can you know just have your basis vectors in
any different direction and you could sort of re-represent,
um the words in the vector space with a different set of basics,
basis vectors and it'd be exactly the same vector space
just sort of rotate around to your new um, vectors.
So, you know, you shouldn't read too much into the sort of elements.
So, it actually turns out that because of the way a lot of
deep learning um operations work,
some things they do, do element-wise.
So that the dimensions do actually tend to get some meaning to them it turns out.
But um, though I think I really wanted to say was,
that you know one thing we can just think of is how close things
are in the vector space and that's
a notion of meaning similarity that we are going to exploit.
But you might hope that you get more than that,
and you might actually think that there's meaning in
different dimensions and directions in the word vector space.
And the answer to that is there is and I'll come back to that a bit later.
Okay. Um, so in some sense this thing that had
the biggest impact um in sort of turning the world of
NLP in a neural networks direction was that picture.
Um, was this um algorithm that um
Thomas Mikolov came up with in 2013 called the word2vec algorithm.
So it wasn't the first work and having distributed representations of words.
So there was older work from Yoshua Bengio that went
back to about the sort of turn on the millennium,
that somehow it's sort of hadn't really sort of hit the world over their head and had
a huge impact and has really sort of Thomas Mikolov showed this very simple,
very scalable way of learning
vector representations of um words and that sort of really opened the flood gates.
And so that's the algorithm that I'm going to um show now.
Okay. So the idea of this algorithm is you start with a big pile of text.
Um, so wherever you find you know web pages on newspaper articles or something,
a lot of continuous text, right?
Actual sentences because we want to learn wo- word meaning context.
Um, NLP people call a large pile of text a corpus.
And I mean that's just the Latin word for body, right?
It's a body of text.
Important things to note if you want to seem really educated is in Latin,
this is a fourth declensions noun.
So the plural of corpus is corpora.
And whereas if you say
core Pi everyone will know that you didn't study Latin in high school.
[LAUGHTER] Um, okay.
Um, so right- so we then want to say that every word um
in a- in a fixed vocabulary which would just be
the vocabulary the corpus is um represented by a vector.
And we just start those vectors off as random vectors.
And so then what we're going to do is do
this big iterative algorithm where we go through each position in the text.
We say, here's a word in the text.
Let's look at the words around it and what we're going to want to do is say well,
the meaning of a word is its contexts of use.
So we want the representation of the word
in the middle to be able to predict the words that are
around it and so we're gonna achieve that by moving the position of the word vector.
And we just repeat that a billion times and
somehow a miracle occurs and outcomes at the end we have
a word vector space that looks like a picture I showed where it has
a good meaning of word meet good representation of word meaning.
So slightly more, um,
um, slightly more um graphically right.
So here's the situation.
So we've got part of our corpus problems turning into banking crisis,
and so what we want to say is well,
we want to know the meaning of the word into and so we're going to hope that
its representation can be used in a way that'll
make precise to predict what words appear in
the context of into because that's the meaning of into.
And so we're going to try and make those predictions,
see how well we can predict and then change
the vector representations of words in a way that we can do that prediction better.
And then once we've dealt with into,
we just go onto the next word and we say,
okay, let's take banking as the word.
The meaning of banking is predicting the contexts in which banking occurs.
Here's one context.
Let's try and predict these words that occur around banking and
see how we do and then we'll move on again from there.
Okay. Um, sounds easy so far.
Um, [NOISE] now we go on and sort of do a bit more stuff.
Okay. So overall, we have a big long corpus of capital T words.
So if we have a whole lot of documents we just concatenate them all together and we say,
okay, here's a billion words,
and so big long list of words.
And so what we're gonna do,
is for the first um product we're going to sort of
go through all the words and then for the second product,
we're gonna say- we're gonna choose some fixed size window, you know,
it might be five words on each side or something and we're going to try and
predict the 10 words that are around that center word.
And we're going to predict in the sense of trying to
predict that word given the center word.
That's our probability model.
And so if we multiply all those things together,
that's our model likelihood is how good a job it
does at predicting the words around every word.
And that model likelihood is going to depend
on the parameters of our model which we write as theta.
And in this particular model,
the only parameters in it is actually
going to be the vector representations we give the words.
The model has absolutely no other parameters to it.
So, we're just going to say we're representing
a word with a vector in a vector space and that
representation of it is its meaning and we're then going to be able to
use that to predict what other words occur in a way I'm about to show you.
Okay. So, um, that's our likelihood and so what we do in all of
these models is we sort of define an objective function and then we're going to be,
I want to come up with vector representations of words in
such a way as to minimize our objective function.
Um, so objective function is basically the same as what's on the top half of the slide,
but we change a couple of things.
We stick a minus sign in front of it so we can do minimization rather than maximization.
Completely arbitrary makes no difference.
Um, we stick a one and T in front of it,
so that we're working out the sort of average
as of a goodness of predicting for each choice of center word.
Again, that sort of makes no difference but it kinda keeps the scale of
things ah not dependent on the size of the corpus.
Um, the bit that's actually important is we stick a log in front of
the function that was up there um because it turns out that everything always gets nice.
So when you stick logs and find the products
um when you're doing things like optimization.
So, when we do that we then got a log of
all these products which will allow us to turn things you know,
into a sums of the log of this probability
and we'll go through that again um in just a minute.
Okay. Um, and so if we can mi- if we can change
our vector representations of these words so as to minimize this J of theta,
that means we'll be good at predicting words in the context of another word.
So then, that all sounded good but it was all
dependent on having this probability function where you wanna
predict the probability of a word in
the context given the center word and the question is,
how can you possibly do that?
Um, well um, remember what I said is actually our model is just gonna
have vector representations of words and that was the only parameters of the model.
Now, that's, that's almost true.
It's not quite true.
Um, we actually cheat slightly.
Since we actually propose two vector representations for
each word and this makes it simpler to do this.
Um, you cannot do this,
there are ways to get around it but this is the simplest way to do it.
So we have one vector for word when it's the center word that's predicting
other words but we have a second vector for each word when it's a context word,
so that's one of the words in context.
So for each word type,
we have these two vectors as center word, as context word.
Um, so then we're gonna work out this probability of a word in the context,
given the center word,
purely in terms of these vectors and the way we do it is with this equation right here,
which I'll explain more in just a moment.
So we're still on exactly the same situation, right?
That we're wanting to work out probabilities of
words occurring in the context of our center word.
So the center word is C and the context words represented with
O and these [inaudible] slide notation but sort of,
we're basically saying there's one kind of
vector for center words is a different kind of vector
for context words and we're gonna work out this probabilistic prediction um,
in terms of these word vectors.
Okay. So how can we do that?
Well, the way we do it is with this um,
formula here which is the sort of shape that you see over and over again um,
in deep learning with categorical staff.
So for the very center bit of it,
the bit in orange are more the same thing occurs in the um, denominator.
What we're doing there is calculating a dot product.
So, we're gonna go through the components of our vector and we're gonna
multiply them together and that means if um,
different words have B components of the same sign,
plus or minus, in the same positions,
the dot product will be big and if
they have different signs or one is big and one is small,
the dot product will be a lot smaller.
So that orange part directly calculates uh,
sort of a similarity between words where
the similarity is the sort of vectors looking the same, right?
Um, and so that's the heart of it, right?
So we're gonna have words that have similar vectors,
IS close together in the vector space have similar meaning.
Um, so for the rest of it- um,
so the next thing we do is take that number and put an X around it.
So, um, the exponential has
this nice property that no matter what number you stick into it,
because the dot product might be positive or negative,
it's gonna come out as a positive number and if
we eventually wanna get a probability, um, that's really good.
If we have positive numbers and not negative numbers, um, so that's good.
Um, then the third part of which is the bid in blue is we wanted to have
probabilities and probabilities are meant to add up to
one and so we do that in the standard, dumbest possible way.
We sum up what this quantity is,
that every different word in our vocabulary and we divide through by
it and so that normalizes things and turns them into a probability distribution.
Yeah, so there's sort of in practice,
there are two parts.
There's the orange part which is this idea of using
dot product and a vector space as our similarity measure between words
and then the second part is all the rest of it where we feed it
through what we refer to a news all the time as a softmax distribution.
So the two parts of the expen normalizing gives you a softmax distribution.
Um, and softmax functions will sort of map any numbers into
a probability distribution always for the two reasons that I gave and so,
it's referred to as a softmax um,
because it works like a softmax, right?
So if you have numbers,
you could just say what's the max of these numbers, um,
and you know that's sort of a hot- if you sort of map your original numbers into,
if it's the max of the max and everything else is zero,
that's sort of a hard max.
Um, soft- this is a softmax because the exponenti- you know,
if you sort of imagine this but- if we just ignore the problem
negative numbers for a moment and you got rid of the exp, um,
then you'd sort of coming out with
a probability distribution but by and large it's so be fairly
flat and wouldn't particularly pick out the max of
the different XI numbers whereas when you exponentiate them,
that sort of makes big numbers way bigger and so, this,
this softmax sort of mainly puts mass where the max's or the couple of max's are.
Um, so that's the max part and a soft part is that this isn't
a hard decisions still spreads a little bit of probability mass everywhere else.
Okay, so now we have uh, loss function.
We have a loss function with a probability model on the inside that we can
build and so what we want to be able to do is then um,
move our vector representations of words around
so that they are good at predicting what words occur in the context of other words.
Um, and so, at this point what we're gonna do is optimization.
So, we have vector components of different words.
We have a very high-dimensional space again but here,
I've just got two for the picture and we're gonna wanna
say how- how can we minimize this function and we're going to
want to jiggle the numbers that are used in the word representations in
such a way that we're walking down the slope of this space.
I walking down the gradient and um,
then we're gonna minimize the function we found good representations for words.
So doing this for this case,
we want to make a very big vector in
a very high-dimensional vector space of all the parameters of
our model and the only parameters that this model
has is literally the vector space representations of words.
So if there are a 100 dimensional word representations,
they're sort of a 100 parameters for aardvark and context,
100 parameters for the word a- in context et cetera going through,
100 parameters for the word aardvark [NOISE] as a center word et cetera,
et cetera through that gives us a high big vector of parameters to
optimize and we're gonna run this optimization and then um, move them down.
Um, [NOISE] yeah so that's essentially what you do.
Um, I sort of wanted to go through um,
the details of this um,
just so we've kind of gone through things concretely to
make sure everyone is on the same page.
Um, so I suspect that, you know,
if I try and do this concretely,
um, there are a lot of people um,
that this will bore and some people that are- will bore very badly,
um, so I apologize for you,
um, but you know,
I'm hoping and thinking that there's probably
some people who haven't done as much of this stuff recently
and it might just actually be good to do it concretely
and get everyone up to speed right at the beginning. Yeah?
[inaudible] how do we calculate [inaudible] specifically?
Well, so, we- so the way we calculate the,
the U and V vectors is we're literally going to start with a random vector for
each word and then we iteratively going to change those vectors a little bit as we learn.
And the way we're going to work out how to change them is we're gonna say,
"I want to do optimization," and that is going to be implemented as okay.
We have the current vectors for each word.
Let me do some calculus to work out how I could change the word vectors, um, to mean,
that the word vectors would calculate a higher probability for
the words that actually occur in contexts of this center word.
And we will do that,
and we'll do it again and again and again,
and then will eventually end up with good word vectors.
Thank you for that question,
cause that's a concept that you're meant to have understood.
Is that how this works and maybe I didn't
explain that high-level recipe well enough, yeah.
Okay, so yeah, so let's just go through it. So, we've seen it, right?
So, we had this formula that we wanted to maximize, you know,
our original function which was the product of T equals one to T,
and then the product of the words, uh,
position minus M less than or equal to J,
less than or equal to M,
J not equal to zero of, um,
the probability of W. At prime at T
plus J given WT according to the parameters of our model.
Okay, and then we'd already seen that we were gonna convert that
into the function that we're going to use where we have J of Theta,
where we had the minus one on T. Of the sum of T equals one to T of the sum of minus M,
less than or equal to J less than or equal to M,
J not equal to zero of the log of the probability of W times T, plus J, W,
T. Okay, so we had that and then we'd had
this formula that the probability of the outside word given
the context word is this formula we just went through of xu ot vc over
the sum of W equals one to the vocabulary size of xu wt vc.
Okay, so that's sort of our model.
We want to min- minimize this.
So, we wanna minimize this and we want to minimize that by changing these parameters.
And these parameters are the contents of these vectors.
And so, what we want to do now,
is do calculus and we wanna say let's work out in terms of these parameters which are,
u and v vectors, um,
for the current values of the parameters which we initialized randomly.
Like what's the slope of the space?
Where is downhill?
Because if we can work out downhill is,
we got just gotta walk downhill and our model gets better.
So, we're gonna take derivatives and work out what
direction downhill is and then we wanna walk that way, yeah.
So, why do we wanna maximize that probable edge and like,
like going through every word,
it's like [inaudible] given the [inaudible]
So, well, so, so,
I'm wanting to achieve this, um,
what I want to achieve for my distributional notion of meaning is,
I have a meaningful word, a vector.
And that vector knows what words occur in the context of,
um, a word- of itself.
And knowing what words occur in its context means,
it can accurately give
a high probability estimate to those words that occur in the context,
and it will give low probability estimates
to words that don't typically occur in the context.
So, you know, if the word is bank,
I'm hoping that words like branch,
and open, and withdrawal,
will be given high probability,
cause they tend to occur with the word bank.
And I'm hoping that some other words, um,
like neural network or something have
a lower probability because they don't tend to occur with the word bank.
Okay, um, does that make sense?
Yeah.
Yeah. And the other thing I was,
I'd forgotten meant to comment was, you know, obviously,
we're not gonna be able to do this super well or it's just not gonna be able,
that we can say all the words in the context is going to
be this word with probability 0.97, right?
Because we're using this one simple probability distribution
to predict all words in our context.
So, in particular, we're using it to predict 10 different words generally, right?
So, at best, we can kind of be giving sort of five percent chance to one of them, right?
We can't possibly be,
so guessing right every time.
Um, and well, you know,
they're gonna be different contexts with different words in them.
So, you know, it's gonna be a very loose model,
but nevertheless, we wanna capture the fact that, you know,
withdrawal is much more likely, um,
to occur near the word bank than something like football.
That's, you know, basically what our goal is.
Okay, um, yes, so we want to maximize this,
by minimizing this, which means we then want to do some calculus to work this out.
So, what we're then gonna do is,
that we're going to say, well,
these parameters are our word vectors
and we're gonna sort of want to move these word vectors,
um, to, um, work things out as to how to, um, walk downhill.
So, the case that I'm going to do now is gonna look at the parameters of
this center word vc and work out how to do things with respect to it.
Um, now, that's not the only thing that you wanna do,
you also want to work out the slope with respect to the uo vector.
Um, but I'm not gonna do that because time in class is going to run out.
So, it'd be really good if you did that one at
home and then you'd feel much more competent.
Right, so then, um, so what I'm wanting you to do is work out the partial derivative with
respect to my vc vector representation of this quantity,
that we were just looking at.
Which is, um, the quantity in here,
um, where we're taking the log of that quantity.
Right, the log of the x of u,
o, T, v, c,
over the sum of W equals one to V of the x of u,
o, T, v, c. Okay,
so this, um, so now we have a log of the division,
so that's easy to rewrite, um,
that we have a partial derivative of the log of
the numerator minus and
I can distribute the partial derivative.
So, I can have minus the partial derivative,
um, of the denominator,
um, which is log of this thing.
[NOISE]
Okay. Um, so this is sort of what was the numerator and this is what was the denominator.
Okay. So, um, the part that was the numerator is really easy.
In fact maybe I can fit it in here.
Um, so log on exp are just inverses of each other,
so they cancel out.
So, we've got the partial derivative of U_o T V_c.
Okay, so this point I should, um, just, um,
remind people right that this V_c here's a vector of- um,
it's still a vector right because we had a 100 dimensional representation of a word.
Um, so this is doing multivariate calculus.
Um, so you know, if you're,
if you at all, um,
remember any of this stuff,
you can say, "Ha this is trivial".
The answer to that is you are done, um and that's great.
But you know, if you're, um, feeling, um,
not so good on all of this stuff, um,
and you wanna sort of, um,
cheat a little on the side and try and work out what it is,
um, you can sort of say,
"Well, let me um,,
work out the partial derivative,
um with respect to one element of this vector like the first element of this vector".
Well, what I actually got here for this dot product is I have U_o one times V_c one,
plus U_o two times V_c two plus dot, dot,
dot plus U_o 100 times V_c 100, right,
and I'm finding the partial derivative of this with respect to V_c one,
and hopefully remember that much calculus from high school
of none of these terms involve V_c one.
So, the only thing that's left is this U_o one,
and that's what I've got there for this dimension.
So, this particular parameter.
But I don't only want to do the first component of the V_c vector,
I also want to do the second component of the V_c vector et cetera,
which means I'm going to end up with all of them
turning up in precisely one of these things.
Um, and so the end result is I get the vector U_o.
Okay. Um, but you know,
if you're sort of getting confused and your brain is falling apart,
I think it can be sort of kind of useful to re- reduce things to sort of um,
single dimensional calculus and actually sort of play out what's actually happening.
Um, anyway, this part was easy.
The numerator, we get um, U_o.
Um, so things aren't quite so nice when we do the denominator.
So we now want to have this, um, B_d,
V_c of the log of the sum of W equals
one to the P_x of U_o T V_c.
Okay. So, now at this point,
I'm not quite so pretty.
We've got this log sum X combination that you see a lot,
and so at this point you have to remember that there was E, the chain rule.
Okay. So, what we can say is here's you know,
our function F and here is the body of the function,
and so what we want to do is um,
do it in two stages.
Um, so that at the end of the day,
we've got this V_c at the end.
So, we have sort of some function here.
There's ultimately a function of V_c,
and so we gonna do with a chain rule.
We'll say the chain rule is we first take
the derivative of this outside thing putting in this body,
and then we remember that the derivative of log is one on X.
So, we have one over the sum of W equals one to V of the exp of U_o T V_c
and then we need to multiply that by then taking
the derivative of the inside part which is um,
what we have here.
Okay. Times the derivative of the inside part with
the important reminder that you need to do a change of variables,
and for the inside part use a different variable that you're summing over.
Okay. So, now we're trying to find the derivative of a sum of X.
The first thing that we can do is v-very easy.
We can move the derivative inside a sum.
So, we can rewrite that and have at the sum first of the X equals one to
V of the partial derivatives with respect to V_c of the [inaudible].
Um, so that's a little bit of progress.
Um and that point we have to sort of do the chain rule again, right.
So, here is our function and here's the thing in it again which is some function of V_c.
So, we again want to do um, the chain rule.
So, [NOISE] we then have well,
the derivative of X um, is exp.
So, we gonna have the sum of X equals one to V of exp of U_x T V_c,
and then we're going to multiply that by the partial derivative with
respect to T V_c of the inside U_x T V_c.
Well, we saw that one before, so,
the derivative of that is U- well,
yeah, U_x because we're doing it through a different X, right.
This then becomes out as U_x,
and so we have the sum of the X equals one to
V of this exp U X T B C times the U_of X.
Okay. So, by doing the chain rule twice, we've got that.
So, now if we put it together, you know,
the derivative of V_c with respect of the whole thing,
this log of the probability of O given C, right.
That for the numerator it was just U_o,
and then we're subtracting,
we had this term here, um,
which is sort of a denominator,
and then we have this term here which is the numerator.
So, we're subtracting in the numerator,
we have the sum of X equals one to V of
the exp of U_x T V_c times U_x,
and then in the denominator, we have um,
the sum of W equals one to V of exp of U_w T V_c.
Um, okay, so we kind of get that.
Um, oh wait. Yeah. Yeah, I've gotten.
Yeah, that's right. Um, okay.
We kind of get that and then we can sort of just re-arrange this a little.
So, we can have this sum right out front,
and we can say that this is sort of a big sum of X equals one to V,
and we can sort of take that U_x out the end and say, okay.
Let's call that put over here a U_x,
and if we do that,
sort of an interesting thing has happened because look right here,
we've rediscovered exactly the same form
that we use as our probability distribution for predicting the probability of words.
So, this is now simply the probability of X given C according to our model.
Um, so we can rewrite this and say that what we're getting is U_o minus the sum of
X equals one to V of the probability of X given C times U_x.
This has a kind of an interesting meaning if you think about it.
So, this is actually giving us, you know,
our slope in this multi-dimensional space
and how we're getting that slope is we're taking
the observed representation of
the context word and we're subtracting from that what our model thinks um,
the context should look like.
What does the model think that the context should look like?
This part here is formal in expectation.
So, what you're doing is you're finding the weighted average
of the models of the representations of each word,
multiplied by the probability of it in the current model.
So, this is sort of the expected context word according to our current model,
and so we're taking the difference between
the expected context word and the actual context word that showed up,
and that difference then turns out to exactly give
us the slope as to which direction we should be
walking changing the words
representation in order to improve our model's ability to predict.
Okay. Um, so we'll,
um, assignment two, um, yeah.
So, um, it'll be a great exercise for you guys,
um, to in- um,
to try and do that for the cen-, wait,
um I did the center words trying to look context words as well
and show you that you can do the same kind of piece of math and have it work out.
Um, if I've just got a few minutes left at the end.
Um, what I just wanted to show you if I can get all of this to work right.
Um, let's go [inaudible] this way.
Okay, find my.
Okay. Um, so I just wanted to just show you a quick example.
So, for the first assignment,
um, again it's an iPython Notebook.
So, if you're all set up you sort of can do Jupyter Notebook.
Um, and you have some notebook.
Um, here's my little notebook I'm gonna show you,
um, and the trick will be to make this big enough that people can see it.
That readable? [LAUGHTER] Okay, um,
so right so, so Numpy is the sort of,
um, do math package in Python.
You'll want to know about that.
If you don't know about it.
Um, Matplotlib is sort of the,
one of the most basic graphing package
if you don't know about that you're going to want to know about it.
This is sort of an IPython or Jupyter special that
lets you have an interactive matplotlib um, inside.
And if you want to get fancy you can play it- play with your graphic styles.
Um, there's that.
Scikit-learn is kind of a general machine learning package.
Um, Gensim isn't a deep learning package.
Gensim is kind of a word similarity package which started off um,
with um, methods like Latent Dirichlet analysis.
If you know about that from modelling words
similarities that sort of grown as a good package um,
for doing um, word vectors as well.
So, it's quite often used for word vectors and
word similarities that sort of efficient for doing things at large-scale.
Um, yeah.
So, I haven't yet told you about will next time we have
our own homegrown form of word vectors which are the GloVe word vectors.
I'm using them not because it really matters for what I'm showing but you know,
these vectors are conveniently small.
It turns out that the vectors that Facebook and Google
distribute are extremely large vocabulary and extremely high dimensional.
So take me just too long to load them in
the last five minutes of this class where conveniently uh,
in our Stanford vectors we have a 100 dimensional vectors, um,
and 50 dimensional vectors which are kinda
good for doing small things on a laptop frankly.
Um, so, what I'm doing here is Gensim doesn't natively support
GloVe vectors but they actually provide a utility that
converts the GloVe file format to the word2vec file format.
So I've done that. And then I've loaded a pre-trained model of word vectors.
Um, and, so this is what they call a keyed vector.
And so, the keyed vector is nothing fancy.
It's just you have words like potato and there's a vector that hangs off each one.
So it's really just sort of a big dictionary with a vector for each thing.
But, so this model has been a trained model where
we just use the kind of algorithm we looked at and,
you know, trained at billions of times fiddling our word vectors.
Um, and once we have one we can then, um,
ask questions like, we can say,
what is the most similar word to some other words?
So we could take something like, um,
what are the most similar words to Obama let's say?
And we get back Barrack, Bush, Clinton,
McCain, Gore, Hillary Dole, Martin, Henry.
That seems actually kind of interesting.
These factors from a few years ago.
So we don't have a post- post-Obama staff.
I mean if you put in another word, um, you know,
we can put in something like banana and we get coconut,
mango, bananas, potato, pineapple.
We get kind of tropical food.
So, you can actually- you can actually ask uh,
for being dissimilar to words.
By itself dissimilar isn't very useful.
So if I ask most similar and I say um,
negative equals, um, banana,
um, I'm not sure what your concept of what's most dissimilar to,
um, banana is, but you know,
actually by itself you don't get anything useful out of this, um,
because, um, you just so get these weird really rare words um,
which, um, [LAUGHTER] definitely weren't the ones who are thinking of.
Um, but it turns out you can do something really useful with this negative idea
which was one of
the highly celebrated results of word vectors when they first started off.
And that was this idea that there is actually dimensions of meaning in this space.
And so this was the most celebrated example um, which was look,
what we could do is we could start with the word king and subtract
from it the meaning of man and then we could add to it the meaning of woman.
And then we could say which word in our vector space as
most similar in meaning to that word.
And that would be a way of sort of doing analogies.
Would be able to do the, um, analogy,
man is the king as woman is to what?
And so, the way we're gonna do that is to say we want to be similar to king
and woman because they're both positive ones and far away from man.
And so, we could do that manually,
here is said manually,
most similar positive woman king, negative man.
And we can run this and lo and behold it produces queen.
To make that a little bit easier I defined this analogy,
um, analogy predicates so I can run other ones.
And so I can run another one like analogy Japan Japanese,
Austria is to Austrian.
Um, and you know,
I think it's fair to say that when people first
saw that you could have this simple piece of math and run it,
and learn meanings of words.
I mean it actually just sort of blew people's minds how effective this was.
You know. Like there- there's is no mirrors and strings here, right?
You know it's not that I have a separate-
a special sort of list in my Python where there's a difficult I'm looking up,
er, for Austria Austrian,
uh, and things like that.
But somehow these vector representations are
such that it is actually encoding these semantic relationships,
you know, so you can try different ones,
you know, like it's not that only this one works.
I can put in France, it says French.
I can put in Germany, it says German,
I can put in Australia not Austria and it says Australian,
you know that somehow if you want this vector representations of words that
for sort of these ideas like understanding the relationships between words,
you're just doing this vector space manipulation on these 100 dimensional numbers,
that it actually knows about them.This not only the similarities of word meanings but
actually different semantic relationships
between words like country names and their peoples.
And yeah that's actually pretty amazing.
It really-you know, it's sort of surprising that running such a dumb algorithm on um,
vectors of numbers could capture so well the meaning of words.
And so that's sort of became the foundation of a lot of sort
of modern distributed neural representations of words.
Okay I'll stop there.
Thanks a lot guys and see you on Thursday. [NOISE]
 Okay. Hello everyone.
Um, welcome back to the second class of, um, CS224N.
Okay, so right at the end of last time I was just showing you a little,
um, from this, um,
IPython Notebook of things that you could do with word vectors
but I kind of ran out of time a little for a bit.
So, I'll just spend a couple of more minutes first,
um, showing the end of this.
I stuck this IPython Notebook up on the course page.
So, under lecture one you can find a copy of it and you can download it.
So, I both stuck up just an HTML version of it and a zip file.
Like HTML file is only good to look at.
You can't do anything with it.
So, you wanna, if you wanna play with it by yourself, um,
download the zip file and get the IPython Notebook out of that.
Okay. So we were looking at
these Glove word vectors which I'll talk about a bit more today and so there were
these sort of basic results of similarity in this vector space work very nicely for,
um, discovering similar words and then going on from that,
there was this idea that we'll spend some more time on today which was, um,
maybe this vector space is not only a similarity space where
close together things have similar meaning but it actually captures meaning
in a considerably deeper and more profound way which is to say that there are
actually directions in the space that you can point which have a certain meaning.
So, that if you are pointing in one direction it means this is more so the case,
if you are pointing in a different direction and the meaning space it might be this is
the capital of this country or
all sorts of different meanings could be encoded in the space.
And a way of testing that,
is to use these analogy, um, problems.
And I quickly showed this at the end but just to make sure if you're
unguarded since it's sort of- it's sort of a clever thing right?
So, the idea is that we're going to start with a pair of words like king and man.
And so what we're gonna do is we're gonna say well,
there's a vector for king in the space and there's a vector for man in
the space and but what we're gonna do is we're going to
subtract as in just good old vector subtraction that you hopefully learned in your,
um, linear algebra class.
We're gonna subtract the man vector from
the king vector and the idea we have in our head then is if we do
that what will happen is we'll be left with the meaning of kingship without the manness.
Um, and so then there's also a direct vector for a woman.
So, we can add the woman vector to that resulting vector and then we could say well,
in the vector, we end up at some point in the vector space and then we're gonna say well,
what's the closest word that you're gonna find the here
and it's gonna print out the closest word and as we saw,
um, last time, um,
lo and behold if you do that,
um, you get the answer.
I'm saying you get,
um, king, man, woman.
No? All right. [LAUGHTER].
You gotta reverse king and man.
I have to reverse king and,
ah, sure, sure, sure. I'm sorry.
Oops. Yeah, okay, I kinda do it well like man, king.
Ah, [LAUGHTER] Okay. Yeah, that's right.
Sorry. Okay. Yeah, because it should be
man is to king as woman is to something sorry yeah.
I was getting [LAUGHTER] my order of components wrong.
Okay. Um, and, you know,
as I was sort of I guess I was showing some examples last time with
nationality words but I mean this in a way that is sort of surprising to shocking,
this actually works for all kinds of things that you can get meaning in this space.
So, I can ask various kinds of analogies of sorts.
So I can say Australia is to beer as France is to-.
Wine.
Wine. You might think wine.
What it gives back as champagne which seems a pretty good answer.
[LAUGHTER] Um, I'll go with that.
Um, um, you can do more syntactic facts.
So, I can say tall ta- tall is to tallest as long is to longest and it gets set.
Um, if I say good is to fantastic as bad is to terrible.
That it seems to get out that there's some kind of notion of
make more extreme direction and get this direction out.
I skipped over one.
A bomber is to Clinton as Reagan is to.
You may or may not like the answer it gives for this one
as Obama is to- as Reagan is to Nixon.
Um, now one thing you might notice at
this point and this is something I actually want to come back to at the end.
Um, well, there's this problem because Clinton's ambiguous, right?
There's Bill or there's Hillary.
Um, and, um, I forget,
you know, so this data as I said is a few years old.
So, this data was done in 2014.
So, in sort of in- it definitely doesn't
have Trump really in it as a politician, um, but, you know,
it would have variously both Clintons but as sort of makes sense if probably um,
for a sort of proof for 2014 data,
um, that Bill Clinton dominated.
So, I think what we're getting, um,
out of this is that Clinton and Nixon are sort of similar of people in dangers,
um, of being impeached.
Um, and, uh, on both sides of the aisle had us thinking primarily of Bill Clinton.
But, um, if this sort of brings up something that
I'll come back to right at the end of, um,
it sort of looks like we've got a sort of a problem here because we
just have this string literally Clinton and that, um,
string is any possible sense and meaning of the string Clinton and so minimally um,
that we have Bill Clinton and Hillary Clinton that near.
Maybe you have some friends that are called Clinton as well, right,
and they're all mixed together in this Clinton.
And so that seems kinda problematic and that's sort of been an issue
that's been discussed some for these word vectors and I'll come back to that.
Um, another thing you can do is you can give
a set of words and say which is the odd one out.
Maybe you used to do puzzles like that in middle school or something.
Um, and so you can do that and it decides
that cereal is the odd one out of that set. It seems okay.
Um, and then one other thing I'll just show you is, so, um,
it'll sort of be nice to look at these words that I've drawn
them in some of the slide pictures.
So, this is saying to put together a PCA or
Principal Components Analysis, um, scatter plot.
Um, so, I can do that and then I can say, "Um,
give it a set of words and draw me these as a scatter plot" and um,
hopefully if I can just about fit it in,
um, here's my scatter plot.
And it works pretty well, right?
I've got the wine, champagne,
beer up here then the coffee and tea.
Um, here are the countries.
Here is the schools, college institute, universities.
Um, the animals are down here.
Um, foodstuffs there.
So, yeah, this sort of really does work with this two direction- dimensional display.
It basically shows you similarity.
Now, um, there are, you know,
to some extent though you want to hold on to your wallet with these PCA displays.
So, it's as I've discussed before since you're
taking something that was 100-dimensional and we're just doing
this 2D projection that is capturing some of the major geometry of
the space but it just has to be losing a huge amount of the information.
So, when things end up close together,
they might be really close together in the original space or
they might just have been words that lost in
the 2D projection because they- there are other patterns that
were more dominant and were chosen as the first two principal components.
So, you sort of don't wanna over trust
these things and something if you like Infoviz you might think
about is how there are other ways that I might be able to
represent the distances in a way that was more accurate.
Um, but anyway this is very simple to do and I'm just getting
a PCA to reduce the dimensionality of the matrix and then,
um, transforming with it these word vectors and printing them.
Um, it's mainly easy to do.
The bit that wasn't easy for me to do, um,
but if someone's got some clever Python um plotting tips I'd like one,
if someone wants to send me a message after class.
I would have thought there'd be some default way in which you could just
label points in a scatter plot but I wasn't able to find one.
So, what I did, um,
was I'm just sort of plotting the texts and
I'm offsetting it a little bit from the points.
Um, now that works kinda crappily
because they just collide with each other as you can see.
Um, so, it'd be better if there was a better way to do point labeling in Python plots.
So, if anyone knows the answer to that one you can send it to me.
Um, okay. So, that's that. Ah.
And if you haven't used IPython Notebooks
before and don't want your computer to run really slowly,
it's a good idea to halt
your IPython Notebooks when you're not gonna be using them anymore,
um, especially if they're computing something.
Um, okay. [NOISE] Um.
[NOISE]
Okay. [NOISE] So now,
[NOISE] um, lecture two and so for today,
we're gonna keep on talking about things you can do with
Word Vectors and say a little bit at the end about Word sensors.
So, in more detail, [NOISE] um,
I'm gonna say a bit more about, um, Word2Vec.
I'm gonna have a sort of a very brief excursion on optimization, um,
but then I sort of want to explain a bit more of the space of what
people have done and can do with dense word representations.
So I am gonna say something about
count-based approaches to capturing meaning and how do they work.
I'm gonna talk for a bit about a,
a different model of Word Vectors which was the GloVe model that,
um, as a post-doc of mine, um,
Jeffrey Pennington and, uh,
me worked on a couple of years ago,
um, talk some about evaluation,
really quite dominant theme on a lot of what
we do on natural language processing is how do we,
how do we evaluate things and how much do we trust our evaluations,
um, and then say a little bit about, um, word sensors.
I have a sort of a goal here which is that by the end of the class,
um, you should actually sort of understand, um,
enough of the lay of the land that you could
read papers about word vectors such as the ones that
are in the syllabus and actually understand
them and where they're coming from and roughly how they work.
And so, you know, if you really wanna minimize
work for your c- this class, you could think, "I,
I know everything I need to know after the first week and I'm gonna
do a final project on word vectors and I'll be okay."
Um, and you know, you could actually do that,
I mentioned during the wo- um,
class, um, a couple of recent pieces of work on word vectors.
On the other hand, um,
doing things with word vectors as a fairly mined out areas,
so you're probably better off, um,
also listening to some of the later parts of the class.
Okay. So, remember we had this idea of Word2Vec,
so it was an iterative updating algorithm that learned, um,
these vector representations of words,
then in some sense capture their meaning and the way it worked was we kinda
moved position by position through a corpus and each point in time,
we had a center word here into and it's trying to predict
the words around that by having
a probability distribution over words will occur around that,
and that probability distribution is defined simply in terms
of the.product of the word vectors via the Softmax function.
And so, what we wanna do is change those vectors in
a way that this gives good probability predictions,
that gives as high probability as possible to words that you tend to see in the context.
And so, just to drill that in a little bit more, you know,
what we actually have is we have two matrices, right?
We have for center words,
we have a matrix where for each word in our vocabulary,
we have a vector, um, and at this,
this is probably as good a point as any to say that it
turns out that all the major deep learning packages,
TensorFlow, PyTorch, etc., for their word vectors,
the word vectors are represented as rows.
If you've done a bunch of math classes,
that might not be what you would expect.
You might have expected the other way around,
but they all put them in rows.
So we can have rows for our,
um, so we have six words and a five dimensional vector each.
Okay. And then, we have this outside, um,
matrix where we also have a second, um,
vector for each word which is this representation in context.
Um, so when we have a particular center word here,
word four, you know,
when we're doing our computations,
we're taking a.product between v_4 and each row of
U and that's then giving us a vector of dot product scores.
And so, then after that,
we're running Softmaxes on each of those numbers doing it
element-wise and that's been giving us
a probability distribution over words in the context.
Um, and the sort of things to notice there, um,
which hopefully you noticed last time,
but to make sure you noticed that,
um, you know, we've just got one probability distribution, right?
So in terms of what words we predict,
we're predicting exactly the same probability distribution, every position.
We've sort of saying the most likely word one to the left
is whatever it is house or most likely word to the left is house,
three to the left is house,
the one to the right should be house too, right?
So, it's sort of no sort of find us a prediction,
it's just an overall kind of
probability distribution of words that are likely to occur in my context.
So, all we're asking for is a model that gives
reasonably high probability estimates to all words that
occur in the context of this word relatively often,
is nothing more to it than that.
And that's part of why it's sort of surprising when you've got
such a simplistic thing that it seems like at the end of the day,
it can end up capturing so much about
the meanings of words and aspects of the meanings of words,
like in the examples I've just showing you in the IPython Notebook.
Um, and [NOISE] there's one other thing I was gonna say, oh yeah,
one other thing I was gonna say was the other thing that might occur to you from this is,
um, well, wait a minute,
there was like that and-and,
and-of that occur all the time.
Um, so that means every word must have a high dot product with words like that and of and,
um, they get their probabilities right.
And the first answer to that is, "Yup, that's true."
And it turns out that all word vectors, [NOISE] um,
have a very strong prob- word probability component that reflects that.
And I mean, one of the things that some workers discuss,
so on the readings,
there are two papers from Sanjeev Arora's group in Princeton and one of
those papers sort of discusses, um, this probability,
high frequency effect and your crude way of [NOISE] actually
fixing this high frequency effect is that normally, um,
the first, um,
the first biggest component in
your word vectors is actually a frequency effect and if you just lop it off,
you can make your semantic similarities better.
Um, but there are other things that we do to sort of deal with high frequencies.
Okay, so we get these lovely spaces that I've shown some of.
But I'll make one more remark. Um.
Yeah, so did I say this last time? Oh, oh.
Um, my remark anyway is that,
um, we show all these two-dimensional pictures.
They're exceedingly, exceedingly misleading because in these pic,
two-dimensional pictures, you know,
you have these effects that if, you know,
Samsung is close to Nokia,
it has to be over here and then it has to be far away from words that are over here.
Um, whereas you might sort of also want to have the effect that
Nokia is close to Finland for a different reason,
um, and you can't do that in two-dimensional, um,
vector spaces but, you know, one of the, um,
most of the properties of high dimensional vector spaces are very unintuitive,
and one of the ways that they're unintuitive is in a high dimensional vector space,
a word can be close to lots of other words in different directions.
Um, okay. So um,
we sort of started to talk about how we went about learning these word vectors.
I'm sort of going to take about a five minute detour into optimization.
Now, this isn't really an optimization class,
if you want to learn a lot about optimization.
Well you can learn more about optimization if you do
229 and if you do something like Stephen Boyd's optimization class,
you can learn a lot of optimization but this is
sort of really baby optimization but just to make sure everyone's on the same page,
here are three slides.
Right, so what we did at the end,
what we did over there,
where I apologized that my writing was too small,
but that will give you the chance to when doing homework too and you have to
write that out to work it out for yourselves and learn more in the process.
Right, so what we had was a cost function that we wanted to
minimize and so what we did was we did our bit of
calculus to calculate the gradient of the cost function with respect
to our word vectors which were our variables theta and then what we want to do is say,
well if we take a small step in
the direction of the negative of the gradient that will be taking us down,
down hill in this space and we want to keep on
doing that and sort of head to the minimum of our space.
I mean, of course in our high multi-dimensional space,
you know, it might not be a nice smooth curve like this.
It might be a horrible and non-convex curve but that's just the idea.
So, essentially we're saying we've got the old parameters,
we work out the gradient of the objective function using those old parameters.
We multiply that by a small alpha which is
our step size or learning rate because we only want to move a
little bit each time because if back here,
if we sort of said downhill is this way and said,
"Great let's go a long way that way."
You could kind of completely overshoot,
so we only want to go a little bit each time.
So we normally have a small learning rate alpha and so we
subtract a small multiple of the gradient and we,
from the old parameters and we get
our new parameters and that sort of effectively being worked out,
component wise as is shown below,
that we're just doing that to each of the partial derivatives and then,
that our hope is that that will let us gradually walk down this surface.
Now, if you actually did this,
it would be unbelievably bad for the kind of
systems that we build and there's a lot of work on
clever optimization but the most basic thing
which you definitely need to know is that well,
our objective function here,
J of theta was a function of our entire corpus, right?
And to get this to work well,
the first thing you want to do is,
you know collect a few billion words of your favorite language and then say,
"Go and build a Word2Vec model for me, " and so,
if you have to evaluate
a billion center words and maybe then to- for each of 10 billion context words,
if you have a window size of five and you- so you have to do these sort of 10 billion um,
Softmax calculations before you work out what your gradient is,
that you're going to be having your computer compute for a quite a long time before
you make one little step in the gradient and so things are going to go so, so slowly.
So, no one does that in deep learning systems.
Um, so what people- everyone does is use
stochastic gradient descent and in stochastic gradient descent,
we sample our window in the simplest case.
We, just for this one window,
work out an estimate of the gradient and we use it as a parameter update.
So, this is sort of an amazingly,
amazingly noisy estimate of
the gradient but it sort of doesn't matter too much because as soon as we've done it,
we're going to choose a different center word and do it again and again,
so that gradually we sort of approach what we would have gotten if we'd sort
of looked at all of the center words before we took any steps,
but because we take steps as we go,
we get to the minimum of the function orders and magnitude more quickly.
So thi- this shows the simplest case where we're just sampling one window.
In practice, that's not what we normally do.
We normally sample as- a small bunch,
you know, order of approximately 32 or 64.
Um, so if we have a sample that's bigger,
that's generally referred to as a mini-batch and we
calculate a gradient estimate from the mini-batch.
Um, so that has two advantages.
One advantage is that you kind of get less noisy estimates of
the gradient because you've kind of averaged
over a bunch of examples rather than just using one,
but the second advantage,
which is the one why we really care,
is if we want our computations to go fast when we're using a GPU,
that you need to get parallelization of doing the same operation a whole bunch of
times and then you gain a lot by using
a mini-batch of 64 examples or something like that.
Um, and you don't have to but you know,
it turns out the details of the guts of the hardware that you know,
it isn't- [inaudible] GPUs, you know, they have these,
whatever they have inside them,
there in powers of two.
So, you get better speedups if you use batches like 32 or 64,
rather than just deciding that 42 is still your favorite number from
high school [LAUGHTER] and you're going to use that as the size of your mini-batch.
Okay. um, yeah here's
one other interesting thing which
actually has some optimization details in it, it turns out.
Um, if you think of these um,
doing stochastic gradients with word vectors,
that's actually very different to
some other deep learning problems like vision deep learning problems.
Because for either a single window or even a sort of a reasonably sized mini-batch,
it will turn out that those mini-batch,
mini-batch only has, you know,
relatively speaking a handful of words in it, right?
So, if you have mini-batch of size 32 and a window size of ten,
you know, probably there are only about a 100,150 different words in it.
Um, but yet we're building this model over
a vocabulary of quarter of a million words or something like that.
So, just about all of the elements in this vector are zero.
Um, and so, um,
we sort of really have this very sparse um,
perimeter update and so, um,
that sort of suggests that we actually probably um,
want to sort of only update the word vectors that
appear and then the question is whether you can achieve that, right?
The dumb way to do it, is you just have this matrix that's normally,
nearly all zeros and you say add
those two matrices together and there you go and then the question is,
can you actually have a sparse matrix update which only updates
the certain rows of the matrix that contain
the words that you've entered and do things much faster?
And if you're doing something even cleverer like doing
distributed computation over multiple computers and sharing your parameters,
well then definitely you just sort of only want to update
the word vectors that you've actually been getting a parameter estimate for.
So, there's sort of some details there but I'm
going to skip past them for more details, um.
Right. So, a couple of people asked afterwards, yeah,
why are there these two word vectors that sort of center and the outside one?
And, I mean the answer to that is,
it makes that math I showed you easy, right?
So that if, um,
if you do it as I showed you, well,
you know, for working out, um,
the partial derivatives for the center word.
It's just as I showed you, it's easy.
Um, but if you use only one set of word vectors,
well then the same word,
that's the center word,
will be one of the choices for
the context word when you're working out that Softmax for the context word.
And then you'll get these terms that are then
squared terms in terms of the two references,
so that same word,
and that makes your math more difficult.
Um, so it's sort of just a practical thing,
um, in the end.
I mean it sort of doesn't make very much difference,
because if you sort of think about it since you're going along through all the,
um, positions, you know.
What was a center word at one point is immediately afterwards
a context word of what used to be a context word,
which is now the center word.
So, sort of doing the same computations because, you know,
the dot product is symmetric actually,
um, all over again.
So, they get pretty similar vector representations.
So, it seems like in general you can get the best results
by averaging what comes out for your two vectors,
and you end up with just one vector per word.
Okay, more substantively, um,
if you go to the word2vec paper,
you'll discover that there's sort of more to word2vec that they
define as sort of a family of word2vec models.
And there are so two main parts of that family.
Um, firstly, there's a choice between the Continuous Bag of Words model,
and the skip-grams model.
And what I presented with the skip-grams models.
So, in the skip-grams model,
you've got one center word and you're trying to
predict all the words in context one at a time.
For the Continuous Bag of Words model it's the opposite.
You've got all of the outside words and you're trying to use all of them,
though considered independently like a Naive Bayes model to predict the center word.
Um, and then the second one is, um,
the way I presented learning this was
the method that's using the so called Naive Softmax.
So, therefore when we are wanting to work things out,
we were sort of saying okay we want probability estimates for the context words,
and so we're just going to sum over
the whole vocabulary and we'll come up with these probability estimates.
Um, in practice, that turns out to be a sort of
a bad idea because that would also make things mega slow.
So, in homework two,
coming up next week, um,
you will get to implement a much more practical, um,
way of doing this which they present in the word2vec papers, right?
So, the problem is,
if we're using this equation that we use to do the calculus,
that down in this denominator here,
we're doing the sum over the entire vocabulary.
So, if you have a vocabulary of a quarter million words,
we're sort of doing a quarter of a million dot products and
exponentials and adding them all to work out that denominator.
And that sort of seems uh,
sort of a really bad idea if you want things to be fast.
Um, so, um, Tomas Mikolov and
colleagues came up with this idea of negative sampling would be near enough.
And so the idea of negative sampling,
is we're going to train binary logistic regressions instead.
And so, we're going to train one binary logistic regression
for the actual word observed what's in the numerator,
and you want to give high probability to the word that was actually observed.
And then, what we're going to do,
is we're going to sort of randomly sample a bunch of other words,
they're the negative samples and say they weren't the ones that were actually seen.
So, you should be trying to give them as low a probability as possible.
Okay, so, um, the sort of notation that
they use in the paper is sort of slightly different to the one I've used.
They actually do maximization not minimization,
and that's the equation which I'll come back to.
Um, though before we do that here's the sigmoid function.
So, the sigmoid function is normally written like this,
one over one plus E to the minus X.
But, um, essentially,
the sigmoid function is like a binary case of the Softmax function, right?
That we have two possible outcomes, yes or no,
and that you're sort of again got an input that is any real number,
and it's mapping it onto a probability distribution between
zero and one which represents these two binary outcomes.
And to the extent that the number is positive,
it kind of ceilings to one and negative goes down to zero.
Okay, so with this time,
we're going to take the dot for- for the good word,
we're going to take the dot product of the two vectors,
shove it through our sigmoid function
and then we're going to want that probability estimate,
um, to be as high as possible.
So, if I show you this version,
which is just written slightly differently, um,
to look as much as possible like the notation that we used last time,
here is our new objective function for using negative sampling.
And we've got two terms,
the first one, um,
is the log of the sigmoid of the observed context word,
the outside words, dot producted with the center word,
and we're going to want that to be big.
Um, and then on the other hand,
um, we've got, um, the,
um, randomly chosen K words,
which are just other words,
and we're going to work out dot products between them and the center word.
And we're going to want those to be as small as possible.
Um, note that extra minus sign in there which is causing
the sign of the two things to be different, right?
So, those are our negative samples.
And for big K, it can be a reasonably modest number,
you can just take kind of 10,
15 negative samples and that works pretty fine.
Um, I said we sort of sampled some words,
um, to be the negative samples.
They in particular propose a sampling distribution that helps them along
a little in partly dealing with this pro- problem of very frequent words.
Um, so the starting point of how you sample words is you
use what we call the- the unigram distribution.
So, that just means you take words on a large corpus and count up
how often each one occurs just as a count of independent words,
so there's the called unigram counts.
And so you start off with unigram counts,
but then you raise them to the three quarters power.
And raising to the three quarters power,
has the effect of, um,
decreasing how often you sample very common words,
and increasing how often you sample rarer words.
Okay, um, and that's that.
Okay, so that's everything about word2vec I am going to say.
Anyone have any last thing.
Yes. [NOISE]
Oh, oh [NOISE]. This is a- sorry Z,
that capital Z is often used as a normalization term and so this is saying,
well if you want the probability distribution of words,
is you work out this three quarters power of the count
of the word for every word in the vocabulary and then these
numbers you just sum them up over the vocabulary and it'll be sum
total and we're dividing by that so we get a probability distribution.
Good question because i hadn't explained that.
Um, in this class, when you see the letter Z with no explanation,
it normally means I am a normalization term to turn things into
probabilities and you sort of iterate over
the numerator term and summing them and divide through.
Any other questions of things I haven't explained or otherwise? Yes.
So the window [inaudible] that's a [inaudible]
Yeah, yes.
So, [NOISE] what size window do you use?
I'll actually come back to that in a bit and show a little bit of data on that,
but yeah, we haven't done anything about that.
At the moment we're guessing a window size like five,
which isn't a bad one um,
but you know there isn't- there hasn't really been any science behind that, um,
that people treat that as what's then called a hyperparameter which means that um,
you try a few different numbers and see which one seems
best and that's the one that you use in your future work. Yeah.
Um, [inaudible] three quarters power
chosen for any theoretical reason or just because it seems to work in practice?
Um, no. Um, that,
that was um, also chosen as a hyperparameter and improved performance.
I mean, actually um, you know,
for this Word2Vec paper, I mean,
you know, it turns out that um,
in the actual paper um,
the model looks very- fairly clean but what
people's discovered when they started digging through the code,
which to- to their credit they did make available, reproducible research,
that there are actually a whole bunch of tricks
of different things like these hyperparameters of um,
how you sample, and how you wait windows and various things to make the numbers better.
So, you know, people play quite a few tricks to make
the numbers go up which aren't particularly theoretical.
Are we good?
Yeah.
[inaudible] [NOISE].
Ah, sometimes.
I so- I- you- so in general for a lot of these sampling things,
it's a bad idea if you're going to be doing multiple passes if you just go bloom,
bloom, bloom and then bloom, bloom, bloom again,
that's a bad idea,
but a common technique a lot of the packages use
is that they do use this shuffling operation at the beginning.
So for each epoch,
they'll shuffle the data randomly and then they'll go through it in sequence and that has
the benefits of faster computation from locality et cetera um,
while ha- meaning that when you do it differently epoch,
it will work out differently.
Uh, yeah, yeah.
[inaudible] [NOISE] [inaudible].
That last question I think was talking about taking the mini-batches from the corpus and
contrasting whether you actually say sample 20
randomly from the whole corpus versus just sort of working from left to right.
Yes, do you have a question?
Um, yeah [inaudible] [NOISE].
Yeah. So- so you could argue- you could argue
whether or not this was written in the clearest way, but, right.
So, we're making this dot product and then we're negating it
which is then flipping which side of the space we're on, right?
Because the sigmoid is symmetric around zero.
So, if we've got some dot product um,
and then we negate it,
we're sort of working out a one minus probability and so
that's the way in which we're actually for the first um,
for the first time we're wanting the probability to be
high and then for the negative samples,
we're wanting their probability to be low.
Okay, I'll maybe run ahead now.
Um, so this was an algorithm which um,
sort of you're going through this corpus position by position and you're sort of doing
this prediction of words and then you're
updating some parameters and you're learning something and you know,
by job it seemed to work based on what we saw in the examples,
but you know, you might have thought um,
that that was kind of weird right?
Look we have this whole big pile of data you know,
sort of traditional, I'm thinking of statistics, right?
So you have a big pile of data,
you aggregate it and it sort of seems like there are obvious things you could do here.
You could say, well there's a word like,
whatever word we're using, banana.
Let's just see what words occur in the context of the gut banana and count
them all up and then we'll be able to use those to predict somehow and you know,
those kinds of methods were traditionally
used including even with distributed representation techniques.
Um, so I want to say a bit about that,
so you're fully educated and don't sound like one of those people who were
aware of no work that happened before 2013 when your network's took off.
Um, okay. So, what we could do is we can essentially
do the same thing as sort of Word2Vec.
We could say there's a five word window around
each word instance that's often referred to as a word token, right?
So at NLP, we often want to distinguish between a particular kind of type like banana
or apple versus particular instances
often in the text and that's referred to as sort of a type token distinction.
So we could, um,
look at each um token with a word,
and the words five around that,
and then we could so start counting up which words occur,
occur with it and so we can then have a matrix of co-occurrence counts.
Um, okay.
So, we'll have again,
and I'm going to give me an example of this.
So, normally again you use the five to 10 but you know I can just
use a window of one to keep my counts very simple and small.
I ignore left or right just like Word2Vec did,
and so if I have a teeny baby corpus like this,
you know, what I could do,
is just say here is the matrix of word co-occurrence accounts.
So, within my window size of one,
I occurs next to like twice,
and that means that like occurs next I twice it's symmetric,
and all my other accounts here are singletons, um.
And so this gives me a big huge sparse matrix of word co-occurrence accounts.
And so one thing that you could do is just use this matrix directly,
because I haven't really got enough data here.
But, you know, if you sort of,
um, decided that, you know,
the word like is like the word learning,
what you'd do is you'd expect that
these two vectors would end up kind of similar to each other.
And [NOISE] they do.
So, you can just measure, um,
similarity of the vectors directly in terms of these co-occurrence counts.
But, you know, it's a little bit unappealing doing things this way, right?
If you have a quarter million word vocabulary that's where
you are in this space where my math is bad,
but it's in the trillions of the number of cells of this matrix,
might require a lot of storage.
Though if you're clever and notice that most of the cells were zero and could do
some clever sparse matrix representation might take a little bit less.
Um, your classification models might have sparsity issues cause, you know,
a lot of those cells aren't present and so it might not be very robust.
And so those are traditional answer to all of these things which is well,
maybe we could have that big co-occurrence counts matrix
and somehow reduce its dimensionality of just, um,
find a corresponding low dimensional matrix which preserves,
uh, most of the information, um,
in the original matrix and, you know,
maybe we'll reduce things to a dimensionality of somewhere around the size 25 to a 1,000,
um as is done with Word2Vec.
So, there's sort of a standard most common way of doing
this dimensionality reduction and you don't really have to understand all the math,
but you get to play with this and homework one which is, um,
for any matrix you can do what's called the singular value decomposition, um,
which is a way you can take an arbitrary matrix and decompose it into three matrices, um,
where the center one is diagonal and has what- in it what are
called singular vectors which are weightings of the different dimensions.
So, they decrease in size as you go downwards.
And then these two U and V are then
orthogonal bases corresponding to the rows and columns.
And so in particular,
it's even simpler than the case where we just have these word-word vectors,
because you have a square matrix and so they are effectively the same.
But, you know, for the general case, um,
although you get these sort of full orthogonal bases,
you then have these bits sort of don't really
matter cause they end up being used for nothing when you work out the product.
Um, and then if you want to reduce the dimensionality, what you say is,
throw away the smallest singular values which remember there are in decreasing
size and that means you're then effectively
throwing away rows and columns of these other matrices.
And then it says,
behold I've now reduced these things to
a two-dimensional representation from
the original three-dimensional representation and that's referred to as
the reduced SVD and the classic result is in terms of least squares error in
estimation that this- the product of these three things will give X k which is the best,
um, k- rank k approximation to the original X in terms of,
uh, X squared least squares criterion.
So, we could do this and we could build word vectors.
So, I can, um,
make use of, um,
NumPy's SVD function and I can throw into it,
um, matrices and, um,
I can make word vectors.
And these ones look really bad, but hey,
I give it a dataset of three centers [LAUGHTER] and it's not exactly a fair comparison.
But- so this technique was in, um,
popularized around, um, the turn- the turn of the millennium.
It generally, um, went for
some word applications under the name of latent semantic analysis or
latent semantic indexing and the idea was that you could have
these semantic directions that you are
finding in this low dimensional space that had meaning.
And people worked with it quite a bit for techniques
like t- trying to do information retrieval
using these LSA approximations and it sort of worked a bit.
It kind of never really worked very well I think,
um, and so it never sort of hugely caught on.
Um, but it's- the methods kind of continued to be explored actually mainly in the sort of
COG psych- COGS psych community where people were doing things with word meaning.
And there's this sort of kind of interesting, um,
the [NOISE] to the literature that there was this guy Doug Rohde, um,
who, um, did a PhD at CMU, um, in 2005.
And basically what he discovered was,
look if rather than just using raw counts,
I start doing quite a bit more in terms of,
you know, fiddling with the counts,
I can start to produce results that are much better.
So, rather than using low counts,
you have to do something to deal with those very high-frequency words.
So, one idea is you could log scale them which
is also commonly used in information retrieval.
Another idea is you could just use something like,
uh, a ceiling function,
so you take the minimum of X,t for t set and that some number like around 100.
Um, he had- he used the idea which was also another of the hacks that was put into
the Word2Vec was rather than just treating the whole window the same that you should,
um, count words that are closer more.
So, in Word2Vec, they sample closer words more commonly than further away words.
Um, in his system, you're sort of having to have
a differential count for closer words et cetera.
And then, um, compared to any of that rather than using counts at all,
he then started using Pearson correlations which
helped and set they're sometimes negative and he decided that it helped,
um, if you then got rid of the negative values.
So, in- in some sense,
this sounds like a bag of hacks,
um, but on the other hand,
he was able to show that, you know,
these transformed counts could actually then give
you very useful word vectors as I'm about to show.
And- well, we have to realize that actually in slightly different forms,
several of these exact same counts are actually being used in Word2Vec as well.
Do you hear that?
Yeah. Were they [inaudible].
Yeah. So, so that's an- I'm about to show exactly that.
Um, that's actually a really interesting little,
um, bit of the data.
So, you know, what, um, yeah,
so the, the thing- if you do that,
you not only get word similarities pretty good.
Let me show you this example which is cleaner.
Um, so this- the precise idea of
evaluating with analogies was not something that had really been developed.
So, that was actually something that Marsh Mikolov, um, suggested.
But actually, um, Doug Rohde made this, um,
really interesting observation which was- he said, look,
"Once I do these kind of transformations to
improve the semantic representation of my word vectors,
look this really interesting property emerges.
Um, that what you find is that there is semantic vectors
are which basically linear components in my carefully-constructed space.
So, here we have the sort of, um,
verb to the doer of the verb direction,
drive, driver, um, clean,
janitor, swim, swimmer, learn,
teacher or teach, teacher,
doctor, treat, priest, pray.
I mean, you know, it's not exactly perfect,
you know, there's a little bit of wiggle there, right?
But, you know, roughly it's completely clear that there's sort of a direction
in the space that corresponds to- from a verb to the doers of a verb.
Um, and yeah, so he [inaudible] - he-
no one had thought of this idea of doing the analogies and tests.
But the thing in retrospect that's obvious is,
if you can construct a vector space that has this linearity property,
then you're definitely gonna do well in analogy.
So, effectively he had invented a vector space that do
well in analogies because this means that you've got
this direction which is the doer and then you can immediately
say that's the doer vector which you can get from subtracting clean from swimmer.
And the- Right. So, it's clean from janitor.
And then we can add it on to swim and we'll get somewhere close to swimmer.
Um, so his space actually did do that.
And so, um, this is- so the,
the moral in some sense is,
if you have- if you kind of do carefully control accounts and so on,
that conventional methods can also give you good word vector spaces and- I mean,
so that was actually the starting off point for our work on GloVe.
Um, so that essentially,
there had been these two schools of work.
Um, there had been the school of work that had
been explored more in COG psych than anywhere else,
which had been based on counting and transforming counts.
And, you know, it had some advantages or it seemed it had some advantages, right?
That, um, you're making sort of efficient use of statistics as you're using
the global statistics of the whole matrix directly to estimate things.
Um, and at that poi- up until then,
it had really only being used to capture word similarity, um,
and a lot of it had suffered from disproportionate im- importance given to large counts.
But Doug Rohde, he had sort of started to show how to solve both of these problems.
And so on the other hand,
there had been these neural network methods
which are kind of direct prediction methods that
we're defining that probability distribution and trying to predict the words that occur.
And they had some advantages, right?
The fact that your sampling means that you're not going to run out of memory hopefully.
I know we've had some memory problems with homework one, but in principle,
you're not as bad memory position and if you have to
construct a huge matrix because you're going linearly,
um, but, you know, since you're doing it sample by
sample it's inefficient use of statistics, um.
Okay. And so, but on the other hand Mikolov's work it performed perfectly.
Not perfectly, but really well.
Um, so this is sort of led into this work,
um, that Jeffrey Pennington, um,
Richard Socher [inaudible] can we sort of combine these ideas
and sort of have some of the goodness of the neural net methods,
um, while trying to do things with some kind of count matrix.
And so in particular, um,
we wanted to get the result in
a slightly less hacky way that you want to have components of meaning
being linear ope- linear operations in
the vector space that they're just some effective or adding or something like this.
And so the crucial observation of this model was that we could use
ratios of co-occurrence probabilities to encode meaning components.
And so the idea here is,
if you have a word like ice and
you say how often the thing's going to co-occur with that,
well solid should co-occur a lot and gas shouldn't.
But well water is also going to co-occur a lot and some random word won't occur much.
If you have, oops.
If you have steam,
you get the opposite pattern with solid and gas, right?
But so the thing to notice is,
it's not enough to just have large by itself because large
appears both here and here or small appears there and there,
the thing that's interesting and sort of the difference between
these components in there indicating a meaning component.
And so we can get it that if we look at the ratio of co-occurrence probabilities.
And so for the ratio of co-occurrence probabilities this is a dimension of
meaning and where for other words and this sort of ratio cancels out to about one.
And so in this slide I've moved so it's not how my
small and large that these are actually actual counts from a corpus.
So we roughly get dimension of meaning between
solid and gas are the ones coming out
as about one because they are not the dimension of meaning.
And so, it seems like what we want is we want to have ratio of
co-occurrence probabilities become linear and our space.
And then we're in a good business.
And so that's what we want to set about doing.
Well, how can you do that?
Well, the way you can do that,
is by if you can make the dot products equal to the log of the co-occurrence probability,
then immediately you get the fact that when you have
a vector difference it turns into a ratio of the co-occurrence probabilities.
And so, essentially the whole of the model is that we
want to have dot products or logs of co-occurrence probabilities.
And so, that's what we do.
So, here is our objective function here
and it's made to look a little bit more complicated.
But essentially we've got this squared loss here
and then we wanting to say the dot-product should be as similar
as possible to the log of
co-occurrence probability and so you'll they'll
be lost to the extent that they're not the same,
but we kind of complexify it a little by putting in bias terms for both of the two words.
Because maybe the word is just overall common and likes to
co-occur things or uncommon or does end.
And then we do one more little trick because every [inaudible] does tricks to make the performance
better is that we also use this f-function in front,
so that we're sort of capping the effect that
very common word pairs can have on the performance of the system.
Okay. And so that gave us the GloVe model of word vectors.
And theoretically, the interest of this was,
you know, a lot of the preceding literature had been there had
been these count methods and there had been these prediction methods.
And the hope was that this could sort of unify the
two by showing you how you could have a method that
is estimated simply of a count matrix but it's done in the same kind of
iterative loss based estimation method that's
used for the neural methods to get good word vectors.
And this also worked to give good word vectors.
So here's GloVe results for the word frog.
And frogs and toad are obvious.
But there are these different kinds of words, uh,
various kinds of pretty tree frogs and things like that.
Okay. Um, so I'll then go from here and say a little
bit more about some of the work on evaluating word vectors.
And this is maybe also a chance just talk a little bit about evaluation altogether.
So, normally in NLP when we do a valuation,
the first thing that comes up is intrinsic versus extrinsic evaluation.
So, normally if there's something we trying to do like model, um,
word similarity with word vectors or we're trying to, um,
put parts of speech on words or something,
we can just have an intrinsic evaluation of saying how good a job did you get.
Are you guessing the right part of speech?
Are you putting synonyms close together?
And that's sort of normally very easy to do and fast to compute.
And it's useful to do because it helps us understand the system.
On the other hand, a lot of the time those intrinsic evaluations,
it's not very clear where- where they're having done well on that task is really going to
help us build the amazing natural language understanding robots
that we so ardently desire.
Um, so, people are also very interested in extrinsic evaluations.
And so extrinsically is then saying well suppose you use
this new stuff in a real system doesn't make performance go up.
And it's then sort of definitional what counts
to you as a real system that normally that's
meaning it's some application that human beings actually care about and liked to use.
So that's something like web search, or question answering,
or phone dialog system or something like that, um,
hat you can put it into that system and the numbers get- go up.
So, that seems what you want to do.
You want to have stuff that works in real tasks.
Of course, there are sort of on the other hand a lot of things are a lot harder than.
So much more work to do such an evaluation and run different variance of a system.
And even when the results, uh,
poor or great sometimes it's hard to diagnose.
You know, if- if your great new word vectors don't work better in the system, you know,
it might be for sort of some extraneous reason about
how the system was built at sort of hiding all your magic.
And if you just change the rest of the system and suddenly show its good effects.
So, it's kind of hard to do,
um, sort of, um,
apportionment of goodness and badness Okay.
So, um, so, today I'm mainly going to say a little bit more about
these intrinsic word vector evaluations that we've talked about.
So we've talked quite a bit about these analogies.
So if we're actually working out the analogies,
it turns out that normally what people are doing is working out
a cosine distance and angle between, um,
different word candidates, um,
to work out which is the word that solves the analogy which
is an Norbert little tiny wrinkle of difference there.
And there's also one other trick that people commonly use.
They forbid the system from returning one of the three word she put
into the analogy Okay.
But nevertheless, so, this is something that you can evaluate.
Here now some GloVe visualizations.
And so these GloVe visualizations show exactly the same kind of
linearity property that Doug Rohde discovered which means that analogy's work.
Sort of by construction,
because our vector space wanted to make meaning components linear.
So, this is then, um,
showing a gender display.
This is showing one between companies and their CEOs, kind of cool.
And you can also do more syntactic facts.
So this is showing, um,
positive comparative and superlative of adjectives.
Yeah. So, Tomas Mikolov came up with this idea of doing these analogy tasks.
And so he built a data-set with a lot of analogies in it.
It's sort of- it's a bit of a weirdo data-set because it's sort of tests a few
random different things which may have been things that his system worked well on, um,
but you know, it tests countries and capitals,
country, cities and states, countries and currency.
So there are a bunch of semantic things that tests.
And then there are some, um,
syntactic things that tests so bad, worst,
fast fastest for superlatives.
But, you know, even some of the ones I was showing before, you know,
there's no- there's no Obama is to
Clinton kind of ones that are actually in this evaluation set.
Um, here's a big table of results,
um, that comes from our GloVe paper.
So not surprisingly the GloVe paper perform best in this evaluation.
Because that was our paper. Um, [LAUGHTER]
[LAUGHTER] But I mean perhaps- you know,
perhaps the things to start to notice is,
yeah, if you just do a plain SVD on counts.
You know that that works abominably badly for these, um, analogy tasks.
But, you know, kind of as Doug Rohde showed,
if you start then doing manipulations of the count matrix before you do an SVD,
you can actually start to produce
an SVD based system that actually performs quite well on these tasks.
Um, you know, not badly against other things.
Um, other things that you will discover,
right at the top there are a 100 dimensional ones,
and at the bottom there are some 1000 dimensional ones,
and other 300 dimensional ones.
At least when you're training on a big amount of text,
bigger dimensionality definitely works better.
And I'll come back to that in a minute.
Um, the amount of text makes a difference as well, right?
So we're going up from- so one to 1.5 billion words at the beginning,
to these ones down here are being trained over 42 billion words of text,
and perhaps unsurprisingly, the 42 billion words of texts ones work better.
Um, so it's big data.
Um, here are a couple more steps from this paper.
So this is a graph of dimensionality and what the performance is.
So for the three lines the green one's semantic,
the blue one's the syntactic analogies and so red's the overall score.
So sort of what you see is up to dimensionality
300 things that clearly increasing quite a bit,
and then it gets fairly flat,
which is precisely why you find a lot of word vectors,
um, that are of dimensionality 300.
Um, this one's showing what window size.
So this is sort of what we talked about symmetric on both sides window size,
and as it goes from 246810.
And sort of what you see is,
if you use a very small window like two, that actually works.
That the, the syntactic prediction is stronger because well,
syntactic effects are very local.
Whereas as you go out,
the semantic prediction gets better and better.
Actually this syntactic gets a bit better as well,
but it's especially the semantic that gains.
Um, the right graph shows that if you only use context on one side,
um, your numbers aren't as good.
Okay, um, so, I sort of just wanted to sort of sneak in a little cameos of a couple of,
um, recent bits of work,
as sort of a first of what things people are doing,
um, with word vectors.
Um, so this one, um,
was actually by two Stanford people.
Um, now the best- this would be the best story.
If I could say that this was a final project,
um, in this class last year,
but unfortunately that's not true.
This paper has nothing to do with this class [LAUGHTER].
But it-- right.
Um, Zin Yin and Yuanyuan,
um, actually had, um,
some sort of clever and very mathy ideas,
where they're using matrix perturbation theory.
Um, and sort of just showing how, um,
dimensionality in word vectors actually sort of feeds into the bias-variance trade-off.
If you've seen that,
um, in other parts of machine learning.
And I'm not even going to attempt to explain their paper.
Um, but here it is,
that they did really well with this paper,
they gone all talk in Europe's from it.
Um, and so- but there's sort of
an interesting result of what you see with these word vectors,
which is in a way kind of surprising.
So this is showing doing word vector dimensions from zero up to 10,000.
So we're going way higher than we talked about before.
And so what you discover which people have known for ages is,
that there's sort of a little blip that somewhere around two or 300,
which seems to optimize performance.
So, I've used those sizes.
But the thing that they were sort of doing a lot of their theory about,
and it's kind of surprising is, well,
surely if you have a humongous humongous number, like,
if you are using 10,000,
um, dimensional vectors, you know,
you're trying to estimate another two orders of magnitude more numbers for every word,
surely things should just fall apart, um,
because you've got hopelessly many parameters relative to
the amount of training data that you're trying to estimate these numbers from.
And so the interesting result that they show is,
that things don't fall apart.
Um, and that you can essentially go out to these huge huge dimensionalities,
and the performance stays flat.
And that they've got a lot of theory,
sort of for predicting why that that's actually going to end up being the case.
Um, yeah.
So for training these models iteratively,
this is- orange is showing, um, GloVe training.
You know, they keep on getting better for a while.
So you know, just go out,
go sleep and see in the morning how it's doing, right?
So that if you were running it, um,
for 24 hours your numbers are better than if you only ran it for six hours.
Um, and that's true for a lot of deep learning models, sorry.
So this is the key reason why you don't want
to start your assignment the night before it's due.
Because even if you program it perfectly,
you might just not have enough time for it to run,
um, so that you produce good numbers at the end of it.
Um, okay. Uh, yeah so,
so couple of more, um,
things, on that, um.
Yes. So, um, what are we showing here?
So these are again semantics in tactic and overall numbers.
So there are sort of two things that are sort of being mixed together here.
One is, if we just look at the overall numbers,
they're highest over here, um,
which is this 42 billion Common Crawl web-pages corpus,
that gives us the highest overall number.
But there's sort of something else that's interesting in this graph, which is,
um, that using Wikipedia works frequently well.
So that you actually find that 1.6 billion tokens of Wikipedia works
better than 4.3 billion tokens of News-wire newspaper article data.
And so I, I think that's sort of actually make sense,
which is well, you know,
the job of encyclopedias is to just sort of
explain concepts and how they relate to each other, right?
So that encyclopedias are
just much more expository text that show all the connections between things,
whereas newspapers in general aren't trying to expose at how things fit together.
They're just telling you about, you know,
who got shot dead last night or something like that, right?
So, um, so this is sort of interesting fact, um,
that this Wikipedia data kind of really,
it sort of is differentially useful, um,
for, um, making word vectors.
And you know, in fact, you know,
when we did very well without GloVe word vectors and lots of people use those.
You know, I think actually one of the reasons why they work so well is that
the original word2vec vectors that Google distributes are built only on Google News data,
where else sort of have this,
um, Wikipedia data inside them.
Okay, um, rushing ahead.
Um, yes, so the- there's all the work on analogy,
but the other more basic evaluation is this one of capturing similarity judgments.
And I haven't said much about this, but you know,
there is this sort of large sub-literature in the psychology community,
where people have wanted to model humans judgments of similarity.
So like a good psych person, what you do,
is you find your classroom of Psych one undergrads,
and you show them pairs of words and say rate
these things for similarity on a scale of one to 10.
And lots of that data has been collected,
and you work out the mean over human beings,
and they give numbers like this of tiger and cat, 7.35.
Tiger's similar to Tiger 10, book and paper,
plane and car, stock and phone,
stock and CD, and you get numbers.
So then, what we're doing is wanting to say,
well let's use distance in the space to map directly onto these similarity judgments,
and how well does it map?
And so that's sort of similarity judging has
also then being used for evaluating these systems.
So again, here are a lot of models.
This is again from our GloVe paper.
But so there are these various similarity data-sets.
So one of the best-known ones that I had on the slide before is this, um, Wordsim 353.
It has 353, um,
different ones in it,
and so you are sort of then modeling a correlation
between your judgments of similarity and the ones that came from the human beings.
Okay. Two more things I want to say. Um, yes.
So, we had that problem right at the beginning
of Clinton and how that could be various people.
And that's perhaps in some sense the simplest case of words being ambiguous,
when you have names which have reference to different people.
Um, but it's not only true of names.
So by and large,
words in human languages are ambiguous and have lots of meanings.
Um, that's especially true of common words.
They always have lots of meaning.
It's especially true of words that have existed for a long time.
It's not true of new very technical words, you know, carcinoma.
I think that only has one meaning.
Um, but, you know,
if you think of any relatively, um,
common word and starts, um,
scratching your head for a moment,
you'll find it has lots of meanings.
I- maybe this isn't even such a common word,
but my random word I've got here is Pike.
Um, pike has lots of meanings,
it has meanings like?
Fish.
Fish, it's a kind of fish, yeah.
So there's a fish that's a pike.
What else is a pike?
A large spear.
A large spear.
Yes, so a large spear is a pike.
Other kinds of pike's?
Gymnastics move.
It's a road.
Gymnastics move or in diving move.
It's a road.
Um, yeah. Um, so there are lots of meanings.
Um, there are other meanings.
Um, in Australian English,
pike is also used as a verb to mean,
um, to pull out from doing something.
Like, "We were all going to go out to a nightclub later, but Joe piked."
[LAUGHTER] Um, I don't think that usage is common in this country,
but, um, you can try that, um. [LAUGHTER]
Right. But lots of meanings and, you know,
this isn't only true of the word pike, right?
Pick any other simple word, right?
You can pick a word like shell or field or house or make,
you know, they have lots of meanings when it comes down to it.
So, you know, but, uh,
how can this work if we just have one meaningful words?
And that's the interesting question and it was something that
[NOISE] we were actually interested in early on.
So, I'm even before the Word2Vec paper came out back in 2012,
um, we were playing around, um,
with neural word vectors and, um, we thought,
boy this is so broken having only one,
um, cents, for a word.
Why don't we come up with a model that has multiple sensors for a word?
And so we did that and we did it in a pretty crude way,
I guess, [NOISE] um,
the way we did it is say,
well, let's for each common word,
let's cluster all the contexts in which it occurs.
And then we'll see if there seem to be
multiple clear clusters by some criterion for that word.
And if so, we'll just sort of split the word into pseudo words.
So, if it seems like that there are five clusters,
um, for the word,
the example I meant to use here is jaguar.
Five clusters for the word jaguar,
I will just call them jaguar_1, jaguar_2, jaguar_3, four,
five, so it's just literally changed
the word in our corpus according to its cluster number.
And then we run our word vectoring algorithm and so we get
a representation that each of those sensors of the word.
And basically, that works,
right up the top is jaguar_1 next,
uh, luxury and convertible.
Um, here is, I guess there's a very old version of MacOS called Jaguar,
any remem- remember that one?
Um. Right. So, jaguars right next to software and Microsoft up there, so that's hopeful.
Um, here's the jaguar that's right next to the Hunter, um,
and I'm being confused on this one,
is jaguar as near solo musical keyboard and string.
Is there a band, [NOISE] a brand of keyboard called jaguar?
I'm not quite sure about that one,
but anyway, it's sort of basically works.
Um, but that was sort of crude and it's also perhaps problematic,
so a lot of time, the divisions between sensors aren't very clear, right?
A lot of sensors are actually related to each other and overlapping because
when how sensors normally arrive is that people stretch the meanings of words.
It's not that they just sort of randomly
wake up the next morning and say, "I know carpet.
I could also refer to that as stone," um,
and given a new sense to the word stone, right?
You so take something that you know about like
a web and you extend it metaphorically to other uses of webbing.
Um, so here's a perhaps more interesting things,
so this is the other Sanjeev Arora,
um, paper that I was going to mention.
So, that what happens if you don't,
um, if you don't have more than one cents for each word?
Well, effectively what you get is that
the word vector that you learn is what's referred to by
physicists and fancy people as a superposition
of the word vectors of the different sentence, different sensors.
By supersitio- superposition just means a weighted average.
Um, um, [LAUGHTER] so that effectively
my meaning of pike is sort of
a weighted average of the vectors for the different sensors of pike,
and the components are just weighted by their frequency.
Um, so that part maybe is perhaps not too surprising,
but the part that's really surprising is well,
if we just averaging these word vectors,
you'd think you couldn't get anything out of the average, right|?
Like if I tell you I'm thinking of two numbers and they're here,
weighted sum is 54,
what are my two numbers, right?
You are sort of really short of information to be able to answer my question.
But, well, you know,
for these word vectors, um,
we have these high dimensional spaces and even though there
are a lot of words that the space is so vast for thoughts dimensions,
that actual words or sensors are very sparse in that space.
And so it turns out that there's this whole literature on, um,
sparse coding, compressed sensing,
um, some of which has actually done by people in the stats department here,
um, which shows that in these cases where you have these sort of sparse,
um, codes in these high dimensional spaces,
you can actually commonly reconstruct out the components of a superposition,
even though all you've done is sort of done this weighted average,
and so, um, this paper looks at how you can do this and so they have,
um, these underlying meaning components,
and they sort of separated out.
So, tie has one meaning component,
there's in this space of trousers, blouse, waistcoat,
that makes sense, and other one in this meaning component of seasoned teams,
winning league, makes sense.
Um, scoreline goal with equalizer clinching scorers,
this one seems to overlap with this one a bit.
Um, but here tie,
this is sort of cable ties and wire ties and things like that.
So, they are actually able to pull out the different sense meanings,
um, from outside, out of the meaning of the word.
Um, so that is a kind of a cool thing.
I just wanna, um,
say one more thing.
Okay. [NOISE] All the evaluations so far was intrinsic,
um, you also might wanna do extrinsic evaluation.
Why, why word vectors excited people on NLP so much?
Is it turned out that having this meaning,
having this representation meaning just turned out to be
very useful and sort of improve all of your tasks after that.
Um, and so, um,
this is doing named entity recognition which is labeling
persons and locations and organizations, but, you know,
it's typical of many tasks of what people found,
was if you started with a model without sort of word representations
and you throw in your word vectors regardless of whether they were to vehicle GloVe ones,
just kind of your numbers go up a couple of percent or more?
And so word vectors were just sort of this useful source that you could
throw into any NLP system that you build and your numbers went up.
So, that there was just a very effective technology, um,
which actually did work in
basically any extrinsic tasks you type tried it on. Okay. Thanks a lot.
 Okay. Hi everyone.
Okay. Let's get started.
Um- great to see you all here.
Welcome back for um- week two of CS224N.
Um- so- so this is a little preview
of what's coming up in the class for this week and next week.
Um- you know, this week is perhaps the worst week of this class.
[LAUGHTER]. Um- so in week two of the class our hope is to actually kind of
go through some of the nitty gritty of neural networks and how they're trained,
and how we can learn good neural networks by backpropagation,
which means in particular we're gonna be sort of talking about the training
algorithms and doing calculus to work out gradients from proving them.
Um, so we are looking a bi- a little bit,
at- um- um, word window classification named entity recognition.
So there's a teeny bit of natural language processing in there, but basically,
sort of week two is sort of,
um- math of deep learning and
neural network models and sort of really neural network fundamentals.
Um, but the hope is that that will give you kind
of a good understanding of how these things really work,
and we'll give you all the information you need to do,
um- the coming up homework and so then,
in week three we kind of flips.
So, then week three is going to be mainly about
natural language processing so we then gonna talk about how
to put syntactic structures over sentences,
um- for building dependency parses of sentences
which is then actually what's used in homework three.
So we're chugging along rapidly.
And then we'll talk about this idea of the probability of
a sentence which leads into neural language models.
Um- so on the homeworks.
Homework one was due approximately two minutes ago,
um- so I hope everyone has submitted their homework one, I mean as,
um- one just sort of admonition,
um- in general so you know homework one we
hope you found was a good warm up and not too too hard
and so really be best to get homework one in quickly
rather than to burn lots of your late days doing homework one.
Um, and now right now out on the website,
um there's homework two.
Um so, we are chugging along.
So homework two kind of corresponds to this week's lectures.
So on the first part of that we are expecting you to
grind through some math problems of working out gradient derivations.
Um- and then the second part of that is then implementing
your own version of word2vec making use of NumPy.
And so this time sort of writing a Python program.
It's no longer an IPython notebook.
Um, I encourage you to get early,
um- look at the materials,
um- on the web.
I mean, in particular corresponding to today's lecture there's,
um- some quite good tutorial materials that are available
on the website and so also encourage you to look at those.
[NOISE].
Um- more generally,
just to make a couple more comments on things.
I mean, I guess this is true of a lot of classes at Stanford but,
you know when we get the course reviews for this class we always get
the full spectrum from people who say the class is terrible and it's way too much work,
um- to the people who say it's a really great class,
one of their favorite classes at Stanford,
obvious the instructors care, et cetera.
And I mean, partly this reflects that we get this very,
um- wide range of people coming to take this class on the one hand,
on the right hand margin perhaps we have the physics PhDs,
and on the left hand margin we have some fresh who think this will be fun to do anyway.
Um, we welcome e- we welcome everybody,
um- but in principle this is uh, graduate level class.
You know, that doesn't mean we want to fail people out,
we'd like everyone to succeed but also like graduate level class.
Um- we'd like you to- you know,
take some initiative in your success.
Meaning, if there are things that you need to know to
do the assignments and you don't know them,
um- then you should be taking some initiative to find some tutorials,
come to office hours and talk to people and get
any help you need and learn to sort of for any holes in your knowledge.
Okay. So here's the plan for today.
Um- so that was the course information update.
So you know, is- this is sort of,
in some sense you know machine learning neural nets intro- Just to
try and make sure everyone else is up to speed on all of this stuff.
So I'll talk a little bit about classification, um,
introduce neural networks, um,
little detour into named Entity Recognition,
then sort of show a model of doing um
Window- Word Window classification and then the end part,
we sort of then dive deeper into what kind of tools we
need to learn neural networks and so today um we're gonna go
through um somewhere between review and primer of matrix calculus and then that will
lead into next time's lecture where it's talking
more about backpropagation and computation graphs.
So, yeah. So this material was especially the part at the end.
You know for some people it'll seem really
babyish if- it's the kind of stuff you do every week, um,
for other people it um- might seem impossibly
difficult but hopefully for a large percentage of you in the middle
this will be kind of a useful review of doing this kind of
matrix calculus and the kind of things that we hope that you can do on homework two.
Um, okay. So um, yeah.
So sorry if I'm boring some people.
If you sat through 229 last quarter you
saw um what a classifier was like and hopefully this
will seem familiar but I'm just sort of hoping to try and have
everyone in week two sort of up to speed and on roughly the same page.
So here's our classification setup.
So we have assumed we have a- training data set where we have these um vector
x um of our x points and then for each one of them we have a class.
So the input might be words or sentences documents or something,
there are d to mention vector, um,
the Yi, the labels or classes that we want to
classify to and we've got a set of C classes that we're trying to predict.
And so those might be something like the topic of the document,
the sentiment positive or negative um of
a document or later we'll look a bit more at named entities.
Okay. So if we have that um-
for this sort of intuition is we got this vector space which we again have
a 2D picture and we have points in that vector space which
correspond to Rx items and what we'd want to do is we'll
look at the ones in our training sample and see which ones are
green and red for our two classes here and then we want to sort of learn
a line that could divide between the green and
the red ones as best as possible and that learned line is our classifier.
So on traditional machine learning or statistics we have the sort of
XI vectors that are data items that are purely fixed but
we're going to then multiply those XI by
some estimated weight vector and
that estimated weight vector will then go into a classification decision.
And the classifier that I'm showing here is
a softmax classifier which is almost identical but not quite to
logistic regression classifier which you should've seen in CS 109 or a stats
class or something like that which is giving a probability of different classes.
Okay. And in particular if you've got
a softmax classifier or a logistic- logistic regression classifier,
these are what are called linear classifiers.
So the decision boundary between two classes here
is a line in some suitably high-dimensional space.
So it's a plane or a hyperplane once you've got a bigger expecter.
Okay. So here's our softmax classifier.
Um, and there are sort of two parts to that.
So in the- in the weight matrix double U
we have a row corresponding to each class and then for
that row we're sort of dot-producting it with
our data point vector XI and that's giving us a kind of a score for
how likely it is that the example belongs to that class and then we're
running that through a softmax function and just as we saw on week one,
the softmax takes a bunch of numbers and turn them into a probability distribution.
Does that makes sense to people?
People remember that from last week? Good so far?
Okay. Um, I'm not gonna go to this in detail but I mean,
ah- essentially this is what the logistic regression does as well.
Um, the difference is that here in this setup we have a weight vector um
for each class whereas what
the statisticians doing logistic regression is they say weight,
that gives us one more number of weight vectors than we really need.
We can get away for- for C classes,
we can get away with C minus one weight vectors.
So in particular if you're doing binary
logistic regression you only need one weight vector whereas
this softmax regression formulation you've
actually got two weight vectors one for each class.
Um, so there's that sort of a little difference there which we
could get into but basically the same.
It's just say it's we're either doing softmax or logistic regression, doesn't matter.
Um, so when we're training what we want to
do is we want to be able to predict um the correct class.
And so the way we're gonna do that is we're gonna wanna
train our model so it gives us highest probability as
possible to the correct class and therefore they'll give us
low probability po- as possible um to um the wrong classes.
And so our criterion for doing that is we're going to create
this negative log probability um of our assignments and then
we're gonna want to minimize the negative log probability which corresponds to maximizing
the log probability which corresponds to maximizing um the probability. Um.
And, but, um, sort of,
pretty soon now, we're gonna start doing more stuff with deep learning frameworks,
in particular PyTorch and you can discover in that,
that there's actually a thing called NLL
loss which stands for negative log-likelihood loss.
Basically, no one uses it because the more convenient thing to use is what's called
the cross entropy loss and so you'll
hear everywhere that we're training with cross entropy loss.
So, I just wanted to briefly mention that and explain what's going on there.
Um, so the concept of cross entropy comes from
baby Information Theory which is about the amount of information theory I know.
Um, so, we're assuming that there's some true probability distribution P and our model,
we've built some probability distribution, Q.
That's what we've built with our soft-max regression and we want to have
a measure of whether our estimated probability distribution is a good one.
And the way we do it in cross entropy is,
we go through the classes and we say,
"what's the probability of the class according to the true model?"
Using that waiting, we then work out the log of, um,
the probability according to our estimated model and we sum those up and negate it,
and that is our cross entropy measure.
Okay. Um, but- so this in general gives you
a measure of sort of information, um, between distributions.
But in our particular case,
remember that for each example,
we've sort of assuming that this is a piece of
labeled training data so we are saying for that example,
the right answer is class seven.
So therefore, our true distribution,
our p is- for this example,
it's class seven with probability one and it's class,
um, anything else with probability zero.
So if you think about then what happens with this formula,
you've got this summation of all the classes.
The PFC is gonna be either one or zero and it's gonna be one
only for the true class here and so what you're left with is,
this is going to equal minus the log of qc, um,
for the true class which is sort of what we were then computing in the previous slide.
Okay. So that's- um, yeah.
So that's basically where you'd get with cross entropy loss.
Um, but one other concept to mention.
So when you have a full data-set of a whole bunch of examples,
the cross entropy loss is then taking the per example average.
So, I guess it's what information theory people sometimes call the cross entropy rate.
So additionally, factored in there.
If you are training it on any examples is that one on in vector that's coming in there.
Okay. Um, okay.
Um, so that's cross entropy loss.
Is that okay? Yeah.
[NOISE] There's some- there's some mixture of the actual labels in the ground?
Sure. Good question.
Right. So, the simplest case is that your gold data,
someone has hand labeled it and,
um, they've labeled one and the rest is zero.
Um, they are- you can think of cases where that isn't the case.
I mean, one case is you could believe that human beings
sometimes don't know the right answer so if human beings said,
"I'm not sure whether this should be class three or four," you could imagine that we
can make training data where we put probability half on both of them,
um, and that wouldn't be a crazy thing to do,
and so then you'd have a true cross entropy loss using more of a distribution.
Um, the case where it's much more commonly used in actual practice is,
there are many circumstances in which people wanna do semi-supervised learning.
So, I guess this is a topic that
both my group and Chris Re's group have worked on quite a lot,
where we don't actually have fully labeled data,
but we've got some means of guessing what the labels
of the data are and if we try and guess labels of data,
well then quite often we'll say,
"Here's this data right in.
It's two-thirds chances this label,
but it could be these other four labels," and we'd use a probability distribution,
and yeah, then it's more general cross entropy loss.
Okay? Um, right. So, um,
that's cross entropy loss, pretty good with.
Um, this bottom bit is a little bit different, um,
which is to say, "Well now we,
this is the sort of the full data-set."
The other thing to notice, um,
when we have a full data- we can have a full data-set of x's,
um, and then we have a full set of weights.
Um, where here we're working a row,
a row vector for the weights for one class,
but we're gonna work it out for all classes.
So, we can sort of simplify what we're writing
here and we can start using matrix notation
and just work directly in terms of the matrix w. Okay.
So for traditional ML optimization,
our parameters are these sets of weights,
um, for the different classes.
So for each of the classes,
we have a d-dimensional, um,
row vector of weights because we're gonna
sort of dot-product wi- with rd, dimensional, input vector.
So we have c times d items and our W matrix and those are the parameters of our model.
So if we want to learn that model using the ideas of gradient descent,
stochastic gradient descent, we're gonna do
sort of what we started to talk about last time.
We have these set of parameters.
We work out, um,
the gradient, the partial derivatives of all of these, um,
of the loss with respect to all of these parameters and we
use that to get a gradient update on our loss function,
and we move around the w's,
and moving around the w's corresponds to sort of
moving this line that separates between the classes and we fiddle
that around so as to minimize our loss which corresponds to choosing a line that
best separates between the items of the classes in some sense.
Okay. So, that's a basic classifier.
So the first question is, well,
how are things gonna be different with a neural network classifier?
Um, so the central observation is that sort of
most of the classic classifiers that people used a lot of the time,
so that includes things like Naive Bayes models, um,
basic support vector machines,
Softmax or logistic regressions.
They're sort of fairly simple classifiers.
In particular those are all linear classifiers which are going to classified by drawing
a line or in the higher dimensional space by
drawing some kind of plane that separates examples.
Having a simple classifier like that can be useful in certain circumstances.
I mean, that gives you what a machine learning as a high bias classifiers,
there's lots of, talk of in CS229,
but if you have a data-set, um,
that's like this, you can't do a very good job at classifying
all the points correctly if you have
a high bias classifier because you're gonna only draw a line.
So you'd like to have a more powerful classifier.
Essentially, what's been powering a lot of the use of
deep learning is that in a lot of cases when you have natural signals,
so those are things like, um, speech,
language, images, and things like that,
you have a ton of data so you could learn a quite sophisticated classifier.
Um, but representing the classes in terms of the input data is sort of very complex.
You could never do it by just drawing a line between the two classes.
So, you'd like to use some more complicated kind of classifier.
So neural networks, the multi-layer
neural networks that we're gonna be starting to get into now,
precisely what they do is provide you a way to learn very complex,
you know, almost limitlessly complex classifiers.
So that if you look at the decisions that they're making in terms of the original space,
they can be learning cases like this.
Um, I put this- I put the,
um, pointer on a couple of the slides here.
Um, this- this is a visualization that was done by Andrei Karpathy.
He was a PhD student here until a couple of years ago.
So this is a little JavaScript, um,
app that you can find off his website and it's
actually a lot of fun to play with to see what kind of,
um, decision boundaries you can get a neural net to come up with.
Okay. Um, so for getting- for getting more advanced classification out of,
um, a neural net used for natural language,
there are sort of two things going- that you can do,
that I want to talk about which are in
some sense the same thing when it comes down to it.
But I'll sort of mention separately at the beginning that one of them is that we
have these word vectors and then
the second one is that we're gonna build deeper multi-layer networks.
Okay. So, at first crucial difference said,
um, we already started to see, um,
with what we were doing last week is rather than
sort of having a word being this is the word house,
we instead say house is a vector of real numbers and what we can do is
change the vector that corresponds to house in
such a way as we can build better classifiers,
which means that we are gonna be sort of moving houses representation around
the space to capture things that we're interested in like word similarity,
analogies, and things like that.
So this is actually, you know,
kind of a weird idea compared to conventional steps or ML.
So rather than saying we just have the parameters w,
we also say that all of these word representations are also parameters of our model.
So, we're actually going to change
the representations of words to allow our classifiers to do better.
So, we're simultaneously changing the weights
and we're changing the representation of words,
and we're optimizing both of them at once to try and make our model as,
um, good as possible.
So, this is the sense in which people often talk about
the deep learning models that we're doing representation learning.
I sort of said there are two ways,
I was going to mention two things.
One is this sort of, um,
word vector representation learning and then the second one
is that we're going to start looking at deeper multi layer neural networks.
Um, sort of hidden over here on the slide
is the observation that really you can think of word,
word vector embedding as just putting your,
having a model with one more neural network layer.
So, if you imagine that each word was a one hot vector,
um, with, for the different word types in your model.
So, you had a, uh, you know,
150,000 dimensional vector with the one-hot encoding of different words.
Um, then you could say you have a ma-, um,
matrix L which is sort of your lexicon matrix and you will pass your one-hot vector for
a word through a layer of neural net which
multiplies the one-hot vector or L1, the one-hot vector.
And since this was a one-hot vector,
what that will have the effect of doing is taking out a column of L. So,
really, we've got an extra layer of matrix, um,
in our neural net and we're learning
the parameters of that matrix in the same way as we're learning,
um, a deep neural network for other purposes.
So, mathematically that completely makes sense and
that's sort of a sensible way to think about,
um, what you're doing, um,
with word embeddings in neural networks.
Um, implementation wise, this makes
no sense at all and no one does this because it just doesn't
make sense to do a matrix multiply when the result of the matrix multiply will be, okay.
This is word ID 17, um, sort of,
then constructing a one-hot vector of length a 150,000 with a
one in position 17 and then doing a matrix multiplied, makes no sense.
You just take up, um,
column or, or, the row,
as we've discussed, 17 of your matrix and that's what everyone actually does.
Okay. Here's my one obligatory picture of neurons, um, for the class.
So, don't miss it, I'm not going to show it again, all class.
Okay. So, the origins [LAUGHTER] of Neural Networks, um,
was in some sense to try and construct
an artificial neuron that seemed to in
some sense kind of capture the kind of computations,
um, that go on in human brains.
It's a very loose analogy for what was produced but, you know,
our model here is these are our,
this is our a TB part of our human brains.
So, here are neurons,
this is a neuron cell here and so,
what does a neuron consist of.
Um, so, up the back,
it's got these dendrites, lots of dendrites.
Then it's got a cell body and if there's stuff coming in on the dendrites, um,
the cell body will become active and then it all starts
spiking down this long thing which is called the Axon.
So, then these axons lead to
the dendrites of a different cell or lots of different cells, right.
This one, um, I'm not sure it's shown but
some of these are kind of going to different cells.
Um, and so, you then have these sort of, um,
terminal buttons on the Axon which are kind of close
to the dendrites but have a little gap in them and some min-,
miracles of biochemistry happen there.
So, that's the synapse, of course,
which you'll then have sort of activation flowing which goes into the next neuron.
So, that was the starting off, um,
model that people wanted to try and simulate in computation.
So, people came up with this model of an artificial neuron.
So, that we have things coming in from other neurons at some level of activations.
So, that's a number X0, X1, X2.
Um, then synapses vary depending on how excitable
they are as to how easily they'll let signal cross across the synapse.
So, that's being modeled by multiplying them by a weight W0, W1, W2.
Then the cell body, sort of correctly,
is sort of summing this amount of excitation it's
getting from the different dendrites, um,
and then it can have its own biases to how likely it is to fire,
that's the B. Um, so,
we get that and then it has some overall kind of threshold or propensity for firing.
So, we sort of stick it through an activation function,
um, which will sort of,
will determine a firing rate and that will be,
um, the signal that's going out on the output axon.
So, that was sort of the starting point of that but,
you know, really, um,
for what we've ended up computing.
We just have a little bit of baby math here which actually, um,
looks very familiar to the kind of baby math you see
in linear algebra and statistics and so it's really no different.
So, in particular, um,
a neuron can very easily be a Binary Logistic Regression Unit.
Um, so that, this is sort of,
for logistic regression you're taking for your input X,
you multiply it by a weight vector.
You're adding, um, your, um,
bias term and then you're putting it through,
um, a non linearity,
like the logistic function.
Um, and then, so you're calculating a logistic regression,
um, inside this sort of neuron model.
Um, and so this is the,
this is the difference between the soft maximum logistic regression,
that I was saying that there is the soft-max for two classes has two sets of parameters.
This sort of just has one set of parameters Z and your modeling
the two classes by giving the probability of one class from 0 to one,
depending on whether the input to
logistic regression is highly negative or highly positive.
Okay. So, really, we can just say these artificial neurons are sort of like
binary logistic regression units or we can make variants of
binary logistic regression units by using some different F function.
And we'll come back to that again and pretty soon.
Okay. Um, well, so that gives us one neuron.
So, one neuron is a logistic regression unit for current purposes.
So, crucially what we're wanting to do with neural networks is say, well,
why only run one logistic regression,
why don't we, um,
run a whole bunch of logistic regressions at the same time?
So, you know, here are our inputs and here's our little logistic regression unit, um,
but we could run three logistic regressions at
the same time or we can run any number of them.
Um, well, that's good but sort of for conventional training of
a statistical model which sort of have to
determine for those orange outputs of the logistic regression.
You know, what we're training each of them to try and capture.
We have to have data to predict what they're going to try and capture.
And so, the secret of sort of then building bigger neural networks is to say,
we don't actually want to decide ahead of time what
those little orange logistic regressions are trying to capture.
We want the neural network to self-organize,
so that those orange logistic regression,
um, units learn something useful.
And well, what is something useful?
Well, our idea is to say,
we do actually have some tasks that we want to do.
So, we- we have some tasks that we want to do.
So maybe, we want to sort of decide whether a movie review is positive or negative,
something like sentiment analysis or something like that.
There is something we want to do at the end of the day.
Um, and we're gonna have, uh,
logistic regression classifier there telling us positive or negative.
Um, but the inputs to that aren't going to
directly be something like words in the document.
They're going to be this intermediate layer of logistic regression units
and we're gonna train this whole thing to minimize our cross entropy loss.
Essentially, what we're going to want to have
happen in the back propagation algorithm will do for us,
is to say, you things in the middle,
it's your job to find some useful way to calculate values from
the underlying data such that it'll help our final classifier make a good decision.
I mean in particular, you know,
back to this picture, you know.
The final classifier, its just a linear classifier,
a soft-max or logistic regression.
It's gonna have a line like this.
But if the intermediate classifiers,
they are like a word embedding,
they can kind of sort of re-represent the space and shift things around.
So, they can learn to shift things around in such a way as
you're learning a highly non-linear function of the original input space.
Okay. Um, and so at that point,
it's simply a matter of saying,
well, why stop there?
Maybe it gets even better if we put in more layers.
And this sort of gets us into the area of deep learning and sort of precisely,
um, this is, um,
that sort of there was- sort of being three comings of neural networks.
So the first work in the 50s which is
essentially when people had a model of a single neuron
like this and then only gradually worked out how it
related to more conventional statistics than there was.
Um, the second version of neural networks which we saw the 80s and early 90s, um,
where people, um, built neural networks like this that had
this one hidden layer where a representation could be learned in the middle.
But at that time it really wasn't effective.
Of all people weren't able to build deeper networks and get them to do anything useful.
So you sort of had these neural networks with one hidden layer and so precisely with
research that started in- into deep learning that precisely the motivating question is,
um, we believe we'll be able to do even more sophisticated,
um, classification for more complex tasks.
Things like speech recognition and image recognition if we could
have a deeper network which will be able to more
effectively learn more sophisticated functions of the input which
will allow us to do things like recognize sounds of a language.
How could we possibly train such a,
um, network so they'll work effectively?
And that's the kind of thing,
um, will go on to,
um, more so starting this lecture more so in the next lecture.
But before we get to there,
um, just to underline it again.
So once we have something like this is our,
um, layer of a neural network.
We have a vector of inputs,
we have a vector of outputs and everything is
connected so that we've got this sort of weights along every one of these black lines.
And so we can say A1 is you're taking
weights times each component of X1 and adding a bias term,
um, and then you're going to be running which is sort of
this part and then running it through our non-linearity and that will give us an output.
And we're gonna do that for each of A1, A2, and A3.
Um, so again, we can kind of regard A is a vector and we can
kind of collapse it into this matrix notation for working out the effects of layers.
The fully connected layers are effectively matrices of weights, um,
and commonly rewrite them like this where we have a bias term as a vector of bias terms.
There's sort of a choice there.
You can either have an always on import and then
the bias terms become part of the weights of a slightly bigger matrix with one extra,
uh, one extra either column or row.
One extra, a- row, right?
Or you can just sort of have them separately within those Bs.
Okay. Um, and then the final note here- right?
So once we've calculated this part,
we always put things through non-linearity which is referred to as
the activation function and so something like
the logistic transform I showed earlier is an activation function.
And this is written as sort of vector in port, um,
activation function giving a vector output,
and what this always means is that we apply this function element-wise.
So we're applying the logistic function which is sort of a
naturally a one input one output function like the little graph I showed before.
So when we apply that to a vector,
we apply it to each element of the vector element-wise.
Okay. We will come back very soon to sort of saying
more about non-linearities and what non-linearities people actually use.
Um, but, you know,
something you might be wondering is well,
why does he always have these non-linearities
and say there has to be an f function there?
Why don't we just, um,
calculate Z equals WX plus B in one layer and then go
on to another layer that also does Z2 equals W2,
Z1 plus B and keep on going with layers like that?
And there's a very precise reason for that which is if you want
to have a neural network learn anything interesting,
you have to stick in some function F which is
a non-linear function such as the logistic curve I showed before.
And the reason for that is that if you're sort of doing
linear transforms like WX plus B and then W2 Z1 plus B,
W3Z2 plus B and you're doing a sequence of linear transforms.
Well, multiple linear transforms just composed to become a linear transform, right?
So one linear transform is rotating and stretching the space somehow and you can
rotate and stretch the space again but the result of
that is just one bigger rotate and stretch of the space.
So you don't get any extra power for a classifier
by simply having multiple linear transforms.
But as soon as you stick in almost any kind of non-linearity,
then you get additional power.
And so you know in general,
what we're doing when we're doing deep networks, um,
in the middle of them we're not thinking, "Ah,
it's really important to have
non-linearity thinking about probabilities or something like that."
Our general picture is well,
we want to be able to do effective function approximation or curve fitting.
We'd like to learn a space like this and we can only do that if we're sort of putting in
some non-linearities which allow us to learn these kind of curvy decision, um, patterns.
And so- so F is used effectively for doing accurate
[NOISE] fu- function approximation or sort of pattern matching as you go along.
Okay. You are behind already. Um, okay.
So that was the intro to baby neural networks.
All good? Any questions?
Yes?
Yeah, like er, feature one and feature
four if- if you multiply it together it's highly indicative of like the label Y,
can you get to that product relationship
to just say [NOISE] couple of layers that are linear?
Um, yes. Good question.
So, in conventional stats,
you have your basic input features and when
people are building something like a logistic regression model by hand,
people often say well,
something that's really important for classification is
looking at the pair of feature four and feature seven.
Um, that you know,
if both of those are true at the same time something i-important
happens and so that's referred to normally in stats as an interaction term,
and you can by hand a-add interaction terms to your model.
So, essentially a large part of the secret here is having these intermediate layers.
They can learn, build interaction terms by themselves.
Yeah, so it's sort of, um,
automating the search for higher-order terms that you wanna put into your model.
Okay. I'll go on, other questions?
Okay. Um, so um, yeah.
So here's a brief little interlude on a teeny bit more of
NLP which is sort of a kind of problem we're gonna to look at for a moment.
So this is the task of named entity recognition that I very briefly mentioned last time.
So, um, if we have some text,
wait, it isn't appearing here.
Okay. Uh, okay.
If we have some text,
something that in all sorts of places people want to do
is I'd like to find the names of things that are mentioned.
Um and then normally, as well as,
finding the names of things you'd actually like to classify them,
say it's like to say some of them are organizations,
some of them are people,
um, some of them are places.
And so you know this has lots of uses, you know,
people like to track mentions of companies and
people and newspapers and things like that.
Um, people when they do question-answering that a lot of the time the answers
to questions are what we call named entities the names of people,
locations, organizations, pop songs,
movie names all of those kind of things are named entities.
Um, and if you want to sort of start
building up a knowledge base automatically from a lot of text,
well, what you normally wanna do is get out
the named entities and get out relations between them.
So this is a common task.
So, how can we go about doing that?
And a common way of doing that is to say well,
we're going to go through the words one at a time
and they're gonna be words that are in a context just like they were for word to deck,
and what we're gonna do is run a classifier and we're going to assign them a class.
So we're gonna say first word is organization,
second word is organization,
third word isn't a named entity,
fourth word is a person,
fifth word is a person and continue down.
So in running a classification of a word within
a position in the text so it's got surrounding words around it.
Um and so to say what the entities are
many entities are multi-word terms and so the simplest thing you can
imagine doing is just say we'll take the sequence that are all classified the
same and call that the e-entity Shen Guofang or something like that.
There's a reason why that's slightly defective and so
what people often use is that BIO encoding,
um, that I show on the right but I'll just gonna run ahead and not do that now.
Um so, it might seem at first that named entity recognition is trivial because you know,
you have company names Google and Facebook are company names.
And whenever you see Google or Facebook you just say company and how could you be wrong?
But in practice, there's a lot of subtlety and it's
easy to be wrong in named entity recognition.
So this is sort of just some of the hard cases.
So it's often hard to work out the boundaries of an entity.
So in this sentence,
First National Bank don-donates two vans to Future School of Fort Smith.
So, there's presumably the name of a bank there but is it National Bank and the first is
just the first word of a sentence which is cap-capitalized
like first she ordered some food or something.
So kind of unclear what it is.
Sometimes it's hard to know whether something's an entity at all.
So at the end of this sentence is Future School the name of
some exciting kind of 21st-century school or is it
just meaning it's a future school that's gonna be built in this town, right?
Is it an entity or not at all?
Working out the class of an entity is often difficult so to find out more
about Zig Ziglar and read features by what class is Zig Ziglar?
Kinda hard to tell if you don't know.
Um, it's actually a person's name, um,
and there are various entities that are ambiguous, right?
So Charles Schwab in text is
90% of the time an organization name because there's Charles Schwab Brokerage.
Um, but in this particular sentence here,
in Woodside where Larry Ellison and
Charles Schwab can live discreetly among wooded estates,
that is then a reference to Charles Schwab the person.
So there's sort of a fair bit of understanding variously that's needed to get it right.
Okay. Um, so what are we gonna do with that?
And so this suggests, um,
what we wanna do is build classifiers for language that work inside a context.
Um, so you know, in general,
it's not very interesting classifying a word
outside a context we don't actually do that much in NLP.
Um, but once you're in a context, um,
then it's interesting to do and named entity recognition
is one case there are lots of other places that comes up.
I mean, here's a slightly cool one,
that there are some words that can mean
themselves and their opposite at the same time, right?
So to sanction something can either mean to allow
something or it can mean to punish people who do
things or to seed something can either mean to plant seeds
and things that you're seeding the soil or it can
take seeds out of something like a watermelon, right?
You just need to know the context as to which it is.
Okay. So, that suggests the tasks that we can classify a word
in its context of neighboring words and any has an example of that.
And the question is how might we do that?
And a very simple way to do it might be to say, "Well,
we have a bunch of words in a row
which each have a word vector from something like word to vec.
Um, maybe we could just average
those word vectors and then classify the resulting vector.
The problem is that doesn't work very well because you lose position information.
You don't actually know anymore which of
those word vectors is the one that you're meant to be classifying.
So, a simple way to do better than that is to say,
"Well, why didn't we make a big vector of a word window?"
So, here are words and they each have a word vector,
and so to classify the middle word in the context of here plus or minus two words,
we're simply going to concatenate these five vectors together and say now we have
a bigger vector and let's build a classifier over that vector.
So, we're classifying this x window which is then a vector in,
ah, 5D if we're using D dimensional word vectors.
We can do that um in the kind of way that we did previously which is, um,
that we could say, "Okay,
for that big vector we're going to learn w weights
and we're put- gonna put it through a softmax classifier,
and then we're going to do the decisions."
Um, that's a perfectly good way to do things and,
um, for the purpose of it.
What I want to get to in the last part of this is to
start looking at my, um, matrix calculus.
And you know we could use this model and do
a classifier and learn the weights of it and indeed, um,
for the handout on the website that we suggest you look at it does
do it with a softmax classifier of precisely this kind.
Um, but for the example I do in class I try to make it a bit simpler.
Um, and I've wanted to do this I think very quickly because I'm fast running out of time.
So, one of the famous early papers of neural NLP, um,
was this paper by Collobert and Weston which was first
an ICML paper in 2008 which actually just a couple of weeks ago,
um, won the ICML 2018 test of time award.
Um, and then there's a more recent journal version of it 2011.
And um, they use this idea of
window classification to assign classes like named entities,
ti- to words in context, um,
but they did it in a slightly different way.
So, what they said is, "Well,
we've got these windows and this is one with the, um,
location named entity in the middle and
this is one without a location entity in the middle.
So, what we want to do is have a system that returns a score,
and it should return a high score just as a real number in this case and
it can should return a low score if it- if there isn't,
ah, location name in the middle of the window in this case.
So, explicitly the model just return the score.
So, if you had the top level of your neural network a,
and you just then dot product did with a vector u,
you then kind of with that final dot product,
you just return a real number.
They use that as the basis of their classifier.
So in full glory,
what you had is you had this window of words,
you looked up a word vector for each word, you then, um,
multiplied that the, the- well you
concatenated the word vectors for the window.
You multiplied them by a matrix and edited a bias to get
a second hidden layer which is a and then you multiply that by
a final vector and that gave you a score for the window and you
wanted the score to be large if it was the location and small,
if it wasn't a location.
So, in this sort of pretend example where we have four dimensional word vectors,
um, that's meaning you know for the window,
this is a 20 x 1 vector.
Um, for calculating the next hidden layer we've
got an 8 by 20 matrix plus the bias vector.
Then, we've got this sort of 8-dimensional second hidden layer
and then we are computing a final real number.
Okay. Um, and so crucially this is an example of what the question was about.
Um, we've put in this extra layer here, right?
We could have just said here's a word vector,
a big word vector of, of context.
Let's just stick a softmax or
logistic classification on top to say yes or no for location.
But by putting in that extra hidden layer
precisely this extra hidden layer can calculate
non-linear interactions between the input word vectors.
So, it can calculate things like if
the first word is a word like museum and the second and the second
was a word like the preposition in or
around then that's a very good signal that this should be,
ah, location in the middle position of the window.
So, extra layers of a neural network let us calculate
these kind of interaction terms between our basic features.
Okay. Um, so there's
a few more slides here that sort of go through the details of their model,
but I'm gonna just skip those for now because I'm a little bit behind.
And at the end of it we've just got this score.
So this is our model which is the one that I just outlined where we're
calculating the score and we're wanting a big score, um, for location.
And so, what we're gonna want to do is consider, um,
how we can use this model,
um, to learn, um,
our parameters in a neural network.
Um, so in particular,
remember it's the same story we've had before.
We had a loss function J,
and we're wanting to work out, um,
the gradient with respect to our current theta parameters of the loss function.
Then, we want to sort of subtract a little multiple of that, um,
given by the learning rate from our current parameters to get updated parameters,
and if we repeatedly do then stochastic
gradient descent we'll have better and better parameters
which give higher probability to the things
that we're actually observing in our training data.
So, the thing we want to know is, well,
in general how can we do this um,
differentiation and work out the gradient of our loss function?
And so, I sort of wanted to sort of this the remaining time in this lecture,
um, go through how we can do that by hand, um,
using math and then that'll lead into sort of
discussing and more generally the backpropagation algorithm,
um, for the next one.
Okay. So, if we're doing um,
gradients by hand well we're doing multi-variable calculus, multi-variable derivatives.
But in particular normally the most useful way to think about this is as doing
matrix calculus which means we're directly working with
vectors and matrices to work out our gradients,
and that that's normally sort of much faster and more convenient for
summarizing our neural network layers than trying to do it in a non vectorized way.
But that doesn't mean that's the only way to do it.
If you're sort of confused about what's going on,
sometimes thinking it through in
the non vectorized way can be a better way to understand what's going on and,
um, make more progress.
So, like when, um,
last time I did the word2vec um
derivatives when I was writing too small on that board,
sorry, um, that was doing it in a non vectorized way of working out the weights,
talking about them individually.
Um, but here we're going to do it with,
um, vectors and matrices.
And again, look for the lecture notes to cover this material in more detail.
In particular, so that no one misses it.
Um, let me just clarify what I mean by lecture notes.
So, if you look at the course syllabus on the left-hand column, um,
there's the slides that you can download and,
on straight under the slides,
it says lecture notes.
That's what I'm meaning by the lecture notes.
In the- in the middle column it then has some readings and
actually there are some diffe- additional things there that cover similar material.
Um, so there's, um,
so there's they might be helpful as well.
But first the thing that's closest to what I'm about to present,
it's the lecture notes that appear immediately under the slides link.
Okay. Um, so my hope here, um,
my hope here is the following: Um,
if you can't remembered how to do single variable calculus,
sorry you're basically sunken and might as well leave now.
Um, [LAUGHTER] I'm assuming you know how to do
single-variable calculus and I'm assuming you know what a um a vector and a matrix is.
Um, but you know, um,
I sort of hope that even if you never
did multi-variable calculus or you can't remember any of it,
it's sort of for what we have to do here,
not that hard and you can do it.
So, here's what, um, what you do.
Um, all right.
So, if we have a simple function f of x equals x cubed, right.
Its gradient, um, and so the gradient is the slope, right?
Saying how steep or shallow is the slope of something,
and then when we and also saw the direction of slope when we go into multiple dimensions.
Um, its gradient is just as derivatives.
So, its derivative is 3x squared.
Um, so if you're at the point x equals 3, that you know,
the sort of this 27 of sloppiness,
um, is very steep.
Okay. So well, what if we have a function with one output but now it has many inputs?
Um, so that we're sort of doing that sort of, um,
function that was like the dot products where we're doing the sort of the UTV or WTX,
um, to calculate a value.
Well, then what we're gonna calculate is
a gradient which is a vector of partial derivatives with respect to each input.
So, you take, um,
the slope of the function as you change x1,
the slope of the function as you change x2 through the slope of the, ah,
function as you change xn and each of these you can just calculate as if you were doing
single variable calculus and you just put them all in a vector and
that's then giving you the gradient and then the gradient and multi-dimensional,
um, spaces then giving you the direction and slope of a sort of
a surface that touches your multi-dimensional, um, f function.
Okay. So that's getting a bit scarier,
but it gets a little bit scarier than that
because if we have a neutral network layer, um,
we then have a function which will have n inputs,
which are the input neurons,
and it will have m outputs.
So if that's the case, um,
you then have a matrix of partial derivatives which is referred to as the Jacobian.
So in the Jacobian, um,
you're sort of taking these partial derivatives, um,
with respect to each, um,
output along the rows and with respect to each input down the columns.
And so you're getting these m by n partial derivatives,
considering every combination of an output and an input.
Um, but again, you can fill in every cell of this matrix
just by doing single-variable calculus provided you don't get yourself confused.
Okay. Um, then we already saw when we were doing word2vec,
that sort of a central tool that we have to use to work out,
um, to work out, um,
our derivatives of something like
a neural network model is we have
a sequence of functions that we run up one after another.
So, um, in a neural network you're sort of
running a sequence of functions one after another.
So we have to use, um,
the chain rule to work out derivatives when we compose functions.
So if we have one variable function, so we have,
um, C equals 3y and y equals x squared.
If we want to work out, um,
the derivative of z with respect to x,
we say, aha, that's a composition of two functions.
So I use the chain rule.
And so that means what I do is I multiply, um, the derivative.
So I take, um, dz/dy.
So that's 2x, um,
wait, [NOISE] Sorry, I said that wrong, right?
Is my example wrong?
Oh yeah, its right, dz/dy.
So yeah, dz/dy is just three.
That's, right, that's the derivative of the top line,
and then dy/dx is 2x.
And I multiply those together and I get the answer, um,
that the derivative of z with respect to x is 6x.
Okay. Um, this bit then gets a little bit freakier, but it's true.
If you have lots of variables at once,
you simply multiply the Jacobians and you get the right answer.
So if we're now imagining our neural net,
well sort of, this is our typical neural net right?
So we're doing the neural net layer where we have
our weight matrix multiplied their input vector plus,
um, the bias, and then we're putting it through a non-linearity.
And then if we want to know what's the partials of h with respect to x,
we just say, huh, it's a function composition.
So this is easy to do.
We work out our first Jacobian,
which is the partials of h with respect to z,
and then we just multiply it by the partials of z with respect to x,
and we get the right answer.
Um, easy.
Um, so here's sort of um
an example Jacobian which is a special case that comes up a lot.
Um, so it's just good to realize this one which we'll see with our neural net.
So well one of the things that we have are these element-wise activation function.
So we have h equals f of z.
So, um, what is the, um,
partial derivative of h with respect to z. Um,
well the thing- remember that we sort of apply this element-wise.
So we're actually saying hi equals f of zi.
So, you know, formally this function has n inputs and n outputs,
so it's partial derivatives are going to be an n by n Jacobian.
But if we think about what's happening there,
um, what we're actually going to find is, sort of,
when we're working out the terms of this so we're working out,
how does f of zi change as you change zj?
Well, if j is not equal to i,
it's gonna make no difference at all, right?
So if my f function is something like putting it through
the logistic function or anything else absolute valuing a number,
it's gonna make no difference for the calculation of f of zi
if I chains zj because it's just not in the equation.
And so, therefore, the only terms that are actually going to occur
and be non-zero are the terms where i equals j.
So for working out these partial derivatives if i does not equal j, um, it's zero.
If i does equal j,
then we have to work out a single-variable calculus.
What's the derivative, um,
of the, um, activation function, um,
for- and so this is what,
a um, Jacobian looks like for an activation function.
It's a diagonal matrix.
Everything else is zero,
and we thought this activation function,
we work out its derivative,
and then we calculate that for the difference, um,
we have it for the different kind of um, zi values.
Okay. Um, so that's a,
um, Jacobians for an activation function.
What are the other main cases,
uh, that we need for a neural network?
And these I'll go in through a little bit more slowly in the same lecture notes.
But they're kind of similar to what we saw in the very first class.
So if we are wanting to work out the partial derivatives of wx plus b with respect to x,
um, what we get is w. Um,
and if we want to work out the partial derivative of wx plus b with respect to b,
um, that means that we get an identity matrix because b is sort of like a 1b, right?
It's this almost always on vector,
so you're just getting the ones coming out to preserve the b. Um,
this was the case, um,
that we saw, um,
when we were doing the word vectors.
That if you have a vector dot product of u and h and you say,
what's the partial derivatives of that with respect to u,
then you get out h transpose.
Um, if you haven't seen those before,
um, look at the lecture notes handouts, um,
and see if you can compute them and they make sense at home, um,
but for the moment we're gonna believe those and use those to
see how we can then work out derivatives inside the neural network.
Okay. So here's the same neural network we saw before.
So we have a window of words,
we're looking at word vectors,
we're putting it through a hidden layer,
and then we're just doing a vector modal, um,
vector dot product, you get this final score.
And so, what we [NOISE] want to do to be able to train our neural network,
is we want to find out how- how s changes depending on all the parameters of the model.
The x, the w, the b, the u. Um,
and so we want to work out partial derivatives of S with respect
to each of those because we can then work out okay if you move b up,
um, the score gets better,
which is good if it's actually a plus in the middle,
and therefore we'll want to nudge up,
um, elements of b appropriately.
Okay, um, and so I'm just doing the gradient with
respect to the score here and I skipped over those couple of slides.
Um, so if you're just, sort of,
staring at this picture and say, well,
how do I work out the partial derivative of s with respect to b?
Um, probably it doesn't look obvious.
So the first thing here that you want to do is sort of break up
the eq- equations into simple pieces that compose together, right?
So you have the input x,
and then that goes into z equals wx plus b,
and then you compose that with the next thing.
So h equals f of z, our activation function,
and then this h goes into the next thing of s equals uTh.
So we've got these sequence of functions.
And pretty much you want to break things up as much as you can.
I mean, I could have broken this up even further.
I could have said z1 equals wx,
z equals z1 plus b. Um,
it turns out um,
but if you've just got things added and subtracted,
you can sort of do that in one step because that sort of pathway separating the,
when doing the derivatives,
but sort of anything else that composes together you want to pull it out for the pieces.
Okay. So now our neural net is doing a sequence of function compositions.
And when we say, okay,
we know how to do that, the chain rule.
So if you wanna work out the partials of s with respect to b,
it's just going to be the product of the derivatives of each step along the way.
So it's gonna be um the partial of s with respect to h times h with
respect to z times z with respect to b and that will give us the right answer.
So then all we have to do is actually compute that.
Um, so, I think this just sort of
shows okay we're taking the partials of each step of that composition.
Okay. So now we want to compute that.
And so this is where I'm going to sort of use the Jacobians that I
sort of asserted without much proof on the preceding slide.
Okay. So first of all um we have ds/dh.
Well, that's just the dot product of two vectors.
So the um, the Jacobian for that is just h transpose.
Okay, that's a start.
Then we have um h equals f of z.
Well, that's the activation function.
So the um Jacobian of that is
this diagonal matrix made of the element wise um derivative of the function
f. And then we have the partial of z
with respect to b and that's the bit that comes out as the identity matrix.
And so that's then giving us our calculation of the partial of s with respect to b.
And so we can see that the- the identity matrix sort of goes
away so we end up with this composition of ht times f prime of z.
Okay, suppose we then want to go on and compute now the partial of s with respect to w?
Well, as starting off point is
exactly the same chain rule that we work out each of the stages.
So, that first of all you're working
out the z from the wx part then putting it through the non linearity,
then doing the dot product of the vectors.
So that part is the same.
And what you should notice is that if you
compare the partial of s with respect to w versus s with respect to b,
most of them are the same and it's only the part at the end that's different.
And that sort of makes sense in terms of our neural net right?
That when we had our neural net that the w and the b were coming in here.
And once you've sort of done some stuff with them you're putting things through
the same activation function and doing the same dot product to create a score.
So, you're sort of doing the same calculations that you're then composing with.
So it sort of makes sense that you should be getting
the same derivatives that are
occur- same partial derivatives that occurring at that point.
Oops. And so effectively you know
these partial dev- derivatives correspond to
the computations in the neural network that are above where w and b are.
And so those are commonly referred to as delta,
note delta which is different from partial derivative d. And so
delta is referred to as the error signal and neural network talk.
So, it's the what you're calculating as
the partial derivatives above
the parameters that you are working out the partial derivatives with respect to.
So, a lot of the secret as we'll see next time,
a lot of the secret of what happens with backpropagation is
just we want to do efficient computation in
the sort of way that's computer science people like to do efficient computation.
And so precisely what we want to notice is that there is
one error signal that comes from above and we want to compute it once.
And then reuse that when calculating
both partial derivatives with respect to w and with b.
Okay. So there's sort of two things to still do.
So one is well,
it'd be kind of useful to know what the partial derivative
of s with respect to w actually looks like.
I mean, is that a number, a vector,
a matrix, a three-dimensional tensor?
And then we actually want to work out its values
and to work out its values we're going to still have to work
out the partial derivative of z with respect to
w. But if first of all we just try and work out its shape,
what kind of shape does it have?
And this is actually sort of a bit tricky and is
sort of a dirty underbelly of doing this kind of matrix calculus.
So, since our weight vector is an n by m matrix,
the end result of the partial of s with respect to w is we have a function with
n times m inputs all of the elements of w and simply one output which is our score.
So, that makes it sound like according to what I said before we
should have a one by n times m Jacobian.
But it turns out that's not really what we want, right?
Because what we wanted to do is use what we calculate
inside this stochastic gradient descent update algorithm.
And if we're doing this with sort of like to have
the old weight matrix and we'd like to subtract a bit format to get a new weight matrix.
So, be kind of nice if the shape of our Jacobian was the same shape as w. And so we-
we and in general what you always want to do with
neural nets is follow what we call the shape convention which
is we're going to sort of represent the Jacobian so it's in the same shape as the inputs.
And this whole thing is kind of the- the bad part of
the bad part of doing matrix calculus.
Like there's a lot of inconsistency as to how people represent matrix calculus.
That in general if you just go to different fields like
economics and physics some people use a numerator convention.
Some people use a denominator convention.
We're using neither of those.
We're going to use this shape convention so we match the shape of
the input so it makes it easy to do our weight updates.
Okay. So. Right. So that's what we want the answer to look like.
So, then the final thing we need to do to work out on the
partial of s with respect to w is we have the error signal delta
that's gonna be part of the answer and then we want to work out the partial
of z with respect to w. Well,
um what's that going to be.
Well, it turns out and I'm about to be
saved by the bell here since I'm down to two minutes left.
Um, it turns out that what we end up with for that is we take
the product of the partial- the product of delta times x.
So effectively we've got the local error signal above w. And then we
have the inputs x and we are working out an outer product of them.
And the sort of way to think about this is sort of for the w's.
You know, we've got the elements of the w matrix,
these different connections between our neurons.
And so each one of these is connecting one output to one input.
And so we're going to be sort of making this n by
m matrix of our partial derivatives that are going to be the product of
the error signal for the appropriate output
multiplied by input and those goes give us the partial derivatives.
I'm skipping ahead quickly in my last one minute.
Okay. So uh, right.
So this is sort of what I said have used
the shape con- convention. I'm going to skip that.
Okay. So, um, I- I ran out of time a teeny bit at the end but I mean,
I think hopefully that's conveyed most of
the idea of how you can sort of use the chain rule and
work out the derivatives and work them out in
terms of these vector and matrix derivatives.
[NOISE] And essentially what we wanna do for backpropagation is to say how can we
do ah get a computer to do this automatically for us and to do it efficiently.
And that's what's sort of the deep learning frameworks like
TensorFlow and PyTorch do and how you can do that.
We'll look at more next time.
 Okay. So great to see everyone back for lecture four of the class.
Um, so, for lec,
for today's lecture, um,
what I want to do for most of the time is actually
get into the heart of these ideas of having
the backpropagation algorithm for neural nets and how we can construct
computation graphs that allow sufficiently to do backpropagation,
neural nets to train the neural nets.
So, overall, um, this is sort of what I plan to do it today.
So, at the end of last lecture,
I slightly ran out of time and I started mumbling and waving my hands about the,
um, doing the derivatives with respect to the weight gradients.
So, I kinda of wanted to do that but again.
So hopefully it actually communicates slightly better.
So, we'll do that and talk a bit more about sort of just tips for doing matrix gradients,
um, and a particular issue that comes up with word vectors.
And so then the main part of the class,
we'll be talking about the backpropagation algorithm
and how it runs over computation graphs.
Um, and then for the last part of the class,
um, is I'm not going to hide that, um,
this is sort of just a grab bag of miscellaneous stuff you should
know about neural networks and training neural networks.
Um, like, I think,
you know we dream of a future of artificial intelligence where our machines are
really intelligent and you can just say to them this is the data and this is my problem,
go and train me a model and it might work.
Um, and in some future world,
that may be [NOISE] that comes along.
It's something that's certainly being actively
researched at the moment under the topic of Auto ML.
I guess the question is whether it turns out that Auto ML was a scalable solution or
the climate change consequences of Auto ML techniques are
sufficiently bad that someone actually decides that these much lower power,
um, neural systems might actually be better still for doing some parts of the problem.
But anyway, either way we're not really there yet.
And the fact of the matter is,
when you're training neural networks,
there's just a whole bunch of stuff you have to know about
initialization and nonlinearities and learning rates and so on.
And, you know, when I taught this class
last time I somehow thought that people would pick this up by osmosis.
That if we gave starter,
cut code to people and now start
a code we initialized how matrices and we set our learning rates,
that by osmosis people would understand that's what you have to do and do it.
Um, it didn't really sort of teach in class the practical tips and tricks enough,
but it was perfectly obvious that when we got to
final project time that at least for quite a few people, osmosis hadn't worked.
Um, so this time,
[LAUGHTER] I'm at least wanting to spend a few minutes on that
and at least point out some other things that are important.
And, I mean just in general,
you know the reality of 2018, deep learning, no,
wait, it's 2019 now, 2019, um,
deep learning, is deep learning is still kind of a craft.
There's quite a bit you have to know of techniques of doing things that lead
neural net training to work successfully as
opposed to your models failing to work successfully.
Okay. One final announcement and I go in to it.
Um, so, we've sort of been doing some further working on Office,
our placement and I guess there are sort of multiple issues which
include the opportunities for local ICPD students without Stanford IDs.
We have to, um,
get, um, to office hours.
So for the Thursday night office hour,
um, that's after this class,
if you'd like to go and talk about,
um, the second homework, um,
the Thursday night office hour is going to be in Thorton- Thornton 110.
Um, now I didn't know where Thornton was.
It made more sense to me when I translated that as that's the old terman annex,
but that's probably just showing my age since probably none
of you remember when they used to be a building called Terman.
So that probably doesn't help you either.
Um, but you know,
if you're heading, right,
I don't know which direction we're facing.
If you're heading that way I guess
and if you know where the Papua New Guinea Sculpture Garden is, um, the,
the sort of open grassy area before you get to the Papua New Guinea Sculpture Garden,
that's where Terman used to be and the building that still stands in there is Thornton.
Um, Thornton 110 um tonight.
I think it starts at 6:30,
right? 6:30 to nine.
Okay. Right. So, let me just finish off where we were last time.
So remember we had this window of five words and then we're
putting it through a neural net layer of C equals WX plus B,
non-linearity of H equals F of X,
and then we're, um,
going to just get a score as to whether this has in its center [NOISE]
named entity like Paris which is sort of
taking this dot product of a vector times the hidden layer.
So this was our model,
and then we are wanting to work out partial derivatives of S with
respect to all of our variables and we did various of the cases,
but one we hadn't yet done is the weights,
and the weight through all of this neural net layer here.
Okay. So, chain rule, um,
the partial of ds dw is DS times HD,
um, dHDZ times DZ, DW.
And well, if you remember last time,
we had sort of done some computation of what those first two,
um, partial derivatives were.
And we could say that we could just call
those delta which is our error signal coming from above.
And that concept of having an error signal coming from above is
something I'll get back to in the main part of
the lecture and a sort of a central notion.
But the bit we hadn't dealt with is this dz,
dw and we started to look at that and I made the argument, um,
based on our shape convention that the shape of
that should be the same shape as our W matrix.
So it should be, um,
same in times M shape as this W matrix.
So we want to work out the partial of Z by W which is the same as this,
um, [NOISE] dwx plus b, dw.
And so we want to work out what that derivative is.
Um, and if that's not obvious,
one way to think about it is to go back to this elements of the matrix
and actually first off work it out element-wise and think out what it should be,
and then once you've thought out what it should be, um,
to rewrite it back in matrix form to give the compact answer.
So what we have is we have the inputs here and a biased term
and we're going to do the matrix multiply it this vector to produce these.
And if you think about what's happening there,
so we've got this matrix of weights and for a particular weight,
a weight is first index is going to correspond to a position in
the hidden layer and its second index is going to
correspond to a position in the input vector.
And one weight in the matrix ends up being
part of what's used to compute one element of the hidden layer.
So, the one element of the hidden layer you're taking, um,
a row of the matrix and you're multiplying it by
the components of this vector so they sum together when the bias
is added on but one element of the matrix is sort of only being
used in the computation between one element of the,
um, important one element of the hidden vector.
Okay. So, well, that means, um,
if we're thinking about what's the partial derivative with respect to WIJ, well,
it's only contributing to ZI and it's only,
it's only doing anything with XJ.
So, that we end up with,
we're getting the partial with respect to WIJ,
we can work that out with respect to,
just to respect to ZI.
And when we're going to look at this multiplication here,
what we're ending up is this sort of sum of terms WIK times
Xk where there's sort of weights in that row
of the matrix going across the positions of the vector.
So the only position in which WIJ is used is multiplying, um, by XJ.
And at that point,
what we have in terms of sort of,
in our basic one variable doing a differentiation,
this is just like we have 3x,
um, and we say what's the derivative of 3x?
Actually X is confusing,
so I shouldn't say that.
Is like we have three W and what's the derivative of three W with respect to W?
It's three, right?
So, that we've have a term here which is what would have been W,
will be WIJ times XJ,
and its derivative with respect to WIJ is just XJ.
Does that makes sense?
Everyone believe it?
[NOISE] Fingers crossed.
Okay. Um, so, so for one element of this matrix,
we're just getting out XJ.
And at that point,
um, we say, um,
well of course we want to know what the Jacobian is for the full matrix W. Well,
if you start thinking about it,
this argument applies to every cell.
So, that for every,
um, cell of, um,
the Jacobian for W,
um, it's going to be XJ.
So, that means, um,
we're just going to be able to make use of that in calculating our Jacobian.
So, the derivative for a single WIJ is delta IXJ and that's true for all cells.
So we wanted to have a matrix for our Jacobian which has delta I,
um, XJ in every cell of it.
And the way we can create that is by using an outer products.
So, if we have a row vector of the deltas,
the error signals from above and a column,
right, I said that wrong, sorry.
If we have a column of the delta error signals
from above and we have a row of X transfers vectors,
um, when we multiply those together we get the outer product
and we get delta IXJ in each cell and that is our Jacobian answer,
um, for working out,
um, the delta S delta W that we started off with at the beginning.
Okay. And this, um,
and we get this form where it's a multiplication of
an error signal from above and our computed local gradient signal.
And that's the pattern that we're going to see over and over
again and that will exploit and our computation graphs.
Okay, all good?
Okay. Um, so, here's just,
um, here's homework two.
You're meant to do some of this stuff.
Um, here are just over a couple of collected tips,
um, which I hope will help.
I mean keeping here track of your variables and
their dimensionality is really useful because we
just can work out what the dimensionality of things should be.
You're often kind of halfway there.
I mean basically what you're doing is sort of
applying the chain rule over and over again.
It always looks like this.
Um, but doing it in this sort of matrix calculus sense of the chain rule.
Um, in the homework you have to do a softmax,
which we haven't done in class.
Um, something that I think you'll find useful,
if you want to break apart the softmax is to consider two cases.
One, the case is to when you're working it out for the correct class.
And then, the other case is for all the other incorrect classes.
Um, yeah.
Um, in the the little derivation,
I did before, I said well,
let's work out an element-wise partial
derivative because that should give me some sense of what's going on,
what the answer is.
I think that can be a really good thing to do
if you're getting confused by matrix calculus.
And I sort of,
um, slightly skipped past another slide.
Last time that was talking about
the shape convention that I talked about it for a moment that
for the homeworks you can work out your answer however you want,
you can work it out in terms of;
you know numerator ordered Jacobians,
if that seems best to you.
But we'd like you to give the final answer to
your assignment questions following the shape convention.
So, that the derivative should be shaped in
a vector matrix in the same way as the variable,
with respect to which you're working out your derivatives.
Okay. Um, the last little bit for finishing up this example from last time,
I want to say a little bit about,
is what happens with words.
And one answer is nothing different.
But another answer is they are a little bit of a special case here because,
you know, really we have a matrix of word vectors, right?
We have a vector for each word.
And so then you can think of that as sort of this matrix of word vectors,
which row has a different word.
But we're not actually kind of connecting up
that matrix directly to our classifier system.
Instead of that, what we're connect connecting up to the classifier system is
this window and the window will have it in at five words.
Most commonly they're different words.
But you know occasionally the same word might appear,
um, in two positions in that window.
And so, we can nevertheless do
exactly the same thing and continue our gradients down and say okay,
um, let's work out, um,
the gradients of this word window vector.
And if, um, these are of dimension D we'll have this sort of 5-D, um, vector.
But, you know then what do we do about it,
and the answer of what we do about it.
Is we can just sort of split this window vector into five pieces and say aha,
we have five updates to word vectors.
We're just going to go off and apply them to the word Vector Matrix.
Um, and you know if we if the same word occurs twice,
um, in that window we literally apply both of the updates.
So, it gets updated twice or maybe
actually you want to sum them first and then do the update once but yeah,
that's a technical issue.
Um, so what that actually means is that we're extremely sparsely
updating the word Vector Matrix because most of
the word Vector Matrix will be unchanged and just a few rows of that,
um, will be being updated.
And if- um, soon we're going to be here doing stuff with PyTorch
Um, and if you poke around Pytorch it even has some special stuff.
Um, look for things like Sparse SGD for meaning
that you're sort of doing a very sparse updating like that.
Um, but there's one other sort of interesting thing that you should know about.
For a lot of um,
things that you do is just what actually happens if we push
down these gradients into our word vectors.
Well, the idea is no,
if we do that would be just like all other neural net learning,
that we will sort of in principle say move the word vectors around in such a way
as they're more useful in helping determine
named entity classification in this case because that was our motivating example.
Um, so you know it might for example learn that the word in is
a very good indicator of a named entity fall or sorry the place name following.
So, after n you often get London, Paris et cetera.
Right, so it's sort of got a special behavior that
other prepositions don't as being a good location indicator.
And so, it could sort of um,
move it's location around and say here are words that are
good location indicators and therefore help our classifier work even better.
So, in principle that's good and it's a good thing to do,
to update word vectors to help you perform better on
a supervised task such as this Named Entity Recognition classification.
But, there's a catch which is that it doesn't always work actually.
And so, why doesn't it always work?
Well, suppose that we're training a classifier.
Um, you know it could be the one I just did or a softmax or logistic regression.
And we wanting to classify um,
movie reviews sentiment for positive or negative.
Well, you know if we have trained our word vectors,
we've got some word vector space and maybe in the word vector space, um, TV,
telly and television are all very close
together because they mean basically the same thing.
So, that's great, our word vectors are good.
But, well suppose it was the case,
that in our training data for our classifier.
So, this is our training data for movie sentiment review.
We had the word TV and telly but we didn't have the word television.
Well, then what's going to happen,
is well while we try and train our sentiment classifier,
if we push gradient back down into the word vectors what's likely to happen
is that it will move around the word vectors of the words we saw in the training data.
But, necessarily television's not moving, right?
Because we're only pushing gradient down to words that are in our training data.
So, this word goes nowhere,
so it just stays where it was all along.
So, if the result of our training is words get moved around.
So, here a good words for indicating negative sentiment, um,
will actually if at test time,
when we're running our model,
if we evaluate on a sentence with television in it,
it's actually going to give the wrong answer.
Whereas if we haven't changed the word vectors at all and had just left
them where our word embedding learning system put them.
Then it would have said television,
that's a word that means about the same as TV or telly.
I should treat it the same and
my sentiment classifier and it would actually do a better job.
So, it's sort of two-sided whether you gain by training word vectors.
And so, this is a summary um, that says;
that it's two sided and practically what you should do.
So, the first choice is G is a good idea to use pre-trained word vectors like
the word2vec vectors that you used in assignment one or
using the training methods that you're doing right now for homework two.
And the answer that is almost always yes.
And the reason for that is this word vector training methods are
extremely easy to run on billions of words of texts.
So, we you know train these models like [inaudible] on billions or tens of billions of words.
And it's easy to do that for two reasons.
Firstly, because the training algorithms are very simple, right?
That um, the word2vec training algorithms skip grams very simple algorithm.
Secondly; because we don't need any expensive resources,
all or we need as a big pile of text documents and we can run it on them.
So, really easy to run it on,
you know five or 50 billion words.
Whereas, you know, we can't do that for most of the classifiers that we
want to build because if it's something
I sentiment classifier or a named entity recognizer,
we need labeled training data to train
our classifier and then we ask someone how many words have labeled training data,
do you have for named entity recognition and they give this back
a number like 300,000 words or one million words, right.
It's orders a magnitude smaller.
Okay. Um. So, therefore,
we can gain using pre-trained word vectors,
because they know about all the words that aren't
now supervised, classifies training data.
And they also know much more about the words that actually
are in the training data, but only rarely.
So, the exception to that is,
if you have hundreds of millions of words of data,
then you can start off with random word vectors and go from there.
And so, a case where this is actually commonly done,
is for machine translation,
which we do later in the class.
It's relatively easy for
large languages to get hundreds of millions of words of translated text.
If you wanted to build something,
like a German- English or Chinese-English machine translation system.
Not hard to get 150 million words of translated texts.
And so, that's sort of sufficiently much data,
that people commonly just start with word vectors, um,
being randomly initialized and start training,
um, their translation system.
Okay. So then the second question is, okay.
I'm using pre-trained word vectors.
Um, when I train my supervised classifier,
should I push gradients down into the word vectors and up, and update them?
Which is often referred to as fine tuning the word vectors, um,
or should I not,
should I just sort of throw away
those gradients and not push them down into the word vectors?
And you know, the answer to that is it depends,
and it just depends on the size.
So, if you only have a small training data set, um, typically,
it's best to just treat the pre-trained word vectors as fixed,
um, and not do any updating of them at all.
If you have a large data set,
then you can normally gain by doing fine tuning of the word vectors.
And of course, the answer here,
is what counts as large.
Um, you know, if certainly,
if you're down in the regime of 100 thousand words,
a couple of hundred thousand words, you're small.
If you're starting to be over a million words,
then maybe you're large.
But you know, on practice, people do it both ways and see which number is higher,
and that's what they stick with.
Um. Yes. Um, then, the sort of,
there's the sort of point here that is just worth underlying is " Yes",
so on principle, we can back-propagate this gradient to every variable in our model.
Um, it's actually a theorem that we can arbitrarily
decide to throw any subset of those gradients away,
and we are still improving the log-likelihood of our model, all right?
It kind of can't be inconsistent.
You can just sort of pick some subset and say only
train those 37 and throw away all the rest.
And the algorithm will still improve,
um, the log-likelihood of the model.
Perhaps not by as much as if you trained the rest of the variables,
as well, um, but yes,
it can't actually do any harm not to train anything.
Um, that's one of the reasons why often people don't notice bugs in their code, as well.
It is because if your code is kind of broken
and only half of the variables are being updated,
it will still seem to be training something and improving.
Um. It's just not doing as well as it could be doing,
if you've coded correctly.
Okay. Um, so, at this point, um,
that's sort of, um,
almost shown you back propagation, right?
So, back-propagation is really taking derivatives with a generalized chain rule,
with the one further trick which we sort of represented with that delta,
which is G. You want to be, um,
clever in doing this, so,
you minimize computation by reusing shared stuff.
Um, but now what I want to move on is to sort of look at how we can do
that much more systematically, which is this idea.
We have a computation graph and we're going to run
a back-propagation algorithm through the computation graph.
So, this is kind of like an abstracts syntax tree,
expression tree that you might see in a compiler's class,
or something like that, right?
So, when we have an arithmetic expression of the kind that we're going to compute,
we can make this tipped over on its side tree representation.
So, we've got the X and W variables,
we're going to multiply them.
There's the B variable,
we're going to add it to the previous partial result.
We're going to stick it through our non-linearity F
and then we're going to multiply it by U.
And that was the computation,
that we're doing in our neural network.
So, um the source nodes or inputs,
the interior nodes of this tree are operations.
And then we've got these edges that pass along the results of our computation.
And so, this is the computation graph for precisely the example
I've been doing for the last lecture [NOISE].
Okay, so there are two things that we want to be able to do.
The first one is,
we want to be able to start with these variables and do this computation,
and calculate what S is.
That's the part that's dead simple,
that's referred to as forward propagation.
So, forward propagation is just expression evaluation,
as you do in any any programming in language interpreter.
Um, that's not hard at all.
Um, but the difference here is, "Hey,
we want to do a learning algorithm" so we're going to do the opposite of that, as well.
What we want to be able to do is also backward propagation,
or back-propagation or just back-prop, it's commonly called,
which is we want to be able to go,
um, from the final part.
The final part here.
And then at each step,
we want to be calculating
these partial derivatives and passing them back through the graph.
And so, this was sort of the notion before that we had an error signal, right?
So, we're starting from up here,
we've calculated a partial of S by Z,
which is this with respect to that.
And so, that's sort of our calculated error signal, up to here,
and then we want to pass that further back, to start, um,
computing, um, um, gradients further back.
Right? And we started off, um, right here,
with the partial of S by S. What's the partial of S by S going to be?
One. Okay, yes.
So, the rate at which S changes is the rate at which S changes.
So, we just start off with one,
and then we want to work out how this gradient changes as we go along.
Um, so what we're doing here is when we're working out things for one node,
that a node is going to have passed in towards it upstream gradient,
which is its error signal.
So, that's the partial of our final, f- final result,
which was our loss, um, by um,
the va- variable was the output of these computation nodes.
So, that's the partial of S I H, here.
And then, we did some operation here.
Here's the non-linearity, but it might be something else.
And so what we want to then work out is a downstream gradient,
which is the partial of S by Z,
which was the input to this function.
And well then the question is,
how do we do that?
And the answer to that is,
we use the chain rule, of course, right?
So, at, we have a concept of a local gradients.
So, here's H as the output,
um, Z is the input.
So, this function here,
this is our non-linearity, right?
So, this is whatever we're using as our non-linearity,
like a logistic or T and H. We calculate H in terms of Z,
and we can work out the partial of H by Z.
So, that's our local gradient.
And so then, if we have both the upstream gradient and the local gradient.
We can then work out the downstream gradient because we know the
partial of S by Z is going to be DSDH times, um, DHDZ.
And so, then we'll be able to pass down the downstream gradient to the next node.
Okay. So our basic rule,
which is just the chain rule written in different terms
is downstream gradient equals upstream gradient times local gradient.
Um, easy as that,um, okay.
So, this was um,
the very simplest case where we have a node with one input and one output.
So, that's a function um,
like our logistic function.
But, we also want to have things work out for general computation graphs.
So, how are we going to do that?
Well, the next case is,
um, what about if we have multiple inputs?
So, if we're calculating something like Z equals W times X.
Um, where actually yes Z and X are themselves vectors and W um,
is a matrix, but we're treating X as an input and W as an input,
and Z as our output, right?
We kind of group vectors and matrices together.
Well, if you have multiple inputs,
you then end up with multiple local gradients.
So, you can work out um,
the partial of Z with respect to X,
or the partial of Z with respect to W. And so,
you essentially you take the upstream gradient,
you multiply it by each of the local gradients,
and you pass it down the respective path,
and we calculate these different downstream gradients to pass along.
Is that making sense?
Yeah. Okay. How chug.
Okay. So, let's sort of look in an example of this and then we'll see one other case.
So here's the little baby example.
This isn't kind of really looking like a neural net,
but we've got three inputs x, y, and z.
And x and y get added together and y and z you get maxed.
And then we take the results of those two operations and we multiply them together.
So overall what we're calculating is x plus y times the max of y plus z.
But, you know, we have here a general technique and we can apply it in any cases.
Okay, so if we wanted to have this graph and we want to run it forward,
well, we need to know the values of x, y, and z.
So, for my example x equals one y equals two z equals zero.
Um, so, we take the values of those variables and
push them onto the calculations for the forward arrows.
And then well the first thing we do is add and the result of that is three.
And so we can put that onto the arrow.
That's the output of add.
Max it's two as the output of the value of add times is six.
And so the forward pass we have evaluated the expression.
Its value is six.
That wasn't hard. Okay. So then the next step is we
then want to run back-propagation to work out gradients.
Um, and so we sort of want to know how to sort of,
um work out these local gradients.
So a is our right a is the result of sum.
So here's a as the result of sum.
So a equals x plus y.
So if you're taking da dx that's just one and d a d y is also one that makes sense.
Um, the max is slightly trickier because where
there's some slopes and gradient for the max depends on which one's bigger.
So, if y is bigger than z d- delta,
the partial of b by z,
plus partial b by y is one otherwise it's 0 and conversely for the partial of b by z.
So that one's a little bit dependent.
And then we do the multiplication, um,
case at the end, um,
and work out its partials with respect to a and b.
And, um, since that's a and b which has the values two and three.
If you're taking the partial of f by a it equals b which is two and vice versa.
Okay. So that means we can work out the local gradients at each node.
And so then we want to use those to
calculate our gradients backwards and the back-propagation paths.
So we start at the top.
The partial of f with respect to F is one.
Because if you move if you know by a tenth then you've moved the f by a tenth.
So that's a cancels out as one.
Okay. So then we want to pass backwards.
So, the first thing that we have is this sort of multiply node.
And so we worked- we know its local gradients that partial of f by a is two,
and the partial of f by b is three.
And so we get those values.
So formally we're taking the local gradients
multiplying them by the upstream gradients and getting our three and two.
And notice the fact that so effectively what happens is the values on the two arcs swaps.
Um, and then we sort of continue back.
Okay. There's a max node.
So our upstream gradient is now three and then we want to multiply by the local gradient.
And since the max of these two as two has a slope of one on this side.
So you get three,
there's no gradient on this side and we get zero.
And then we do the similar calculation on
the other side where we have local gradients of one.
And so both of them come out of two And then the one other thing to do is we notice,
well, wait a minute.
There are two arcs that started from the y
both of which we've backed complicated some gradient on.
And so what do we do about that.
Um, what we do about that is we sum.
So, the partial of f by x is to the partial of f by z is 0 that the
partial of f by y is the sum of the two and five, right?
And so this isn't complete voodoo.
This is something that should make sense in terms of what gradients are, right?
So, that what we're saying,
is what we're calculating,
is if you wiggle x a little bit
how big an effect does that have on the outcome of the whole thing?
And so, you know, we should be able to work this out.
So, our x started offers one but let's suppose we wiggle it up a bit
and make it 1.1 well according to this output should change by about 0.2,
it should be magnified by two.
And we should be able to work that out, right?
So it's then 1.1 plus two,
so that's then 3.1.
And then we've got the two here that multiplies by it and it's 6.2.
And lo and behold it went up by 0.2, right?
So that seems correct.
And if we try and do the same for,
well, let's do the z. It's easy.
So if we wiggle the z which had a value of zero by 0.1.
This is 0.1.
When we max if this is still two and
so a calculated value doesn't change, it's still six.
So the gradient here is zero.
Wiggling this does nothing.
And then the final one is y.
So, it's starting off value as two.
So, if we wiggle it a little and make it 2.1,
our claim is that the results are change by about 0.5.
It should be multiplied by five times.
So, if we make this 2.1 we then have 2.1 plus one and b 3.1.
When we get the max here would also be 2.1.
And so we'd have 2.1 times 3.1.
And that's too hard arithmetic for me to do in my head.
But if we take 2.1 times 3.1 it comes out to 6.51.
So, basically it's gone up by half.
We don't expect the answers to be exact of course, right?
Because you know that's not the way calculus works, right?
[NOISE]. Where that it's showing that we're getting the gradients right.
Okay. So this actually works.
So, what are the techniques that we need to know?
Um, so we've sort of already seen them all.
So, you know, we discussed when there are multiple incoming arcs,
how he saw workout the different local derivatives.
The main other case that we need to know is if, um,
in the function computation there's a branch
outward the resultant something is used in multiple places.
And so this was like the case here.
I mean, here this was an initial variable,
but you know, it could have been computed by something further back.
So, if this thing is used in multiple places and
you have the computation going out in different ways.
It's just this simple rule that when you do backpropagation
backwards you sum the gradients that you get from the different output branches.
Okay. So, if a equals X plus Y and while that's the one we showed you
before that were doing this some operation to work out the total partial of f by y.
Okay. And if you sort of think about it just a little bit more,
there's sort of these obvious patterns,
um, which we saw in this very simple example.
So, if you've got a plus that really the upstream gradient is going to
be sort of heading down every one of
these grant branches when you have multiple branches are things being summed.
Now, in this case,
it just as copied unchanged but that's because our computation was x plus y.
You know, it could be more complicated,
but we're passing it down down each of those branches.
So plus distributes upstream gradient.
When you have a max that's kind of like a routing operation,
because max is going to be sending the gradient to in the direction that's the max,
and other things are going to get no gradient being passed down to them.
Um, and then when you have, um,
a multiplication this has this kind of
fun effect that what you do is switch the gradient, right?
And so this reflects the fact that when you have u times
v regardless of whether u and v are vectors or just,
um, scalars that the derivative of the result with respect to
u is v and the derivative of those spot- result with respect to v is u.
And so, the, um,
gradient signal is the flip,
um, of the tw- two numbers on the different sides.
Okay. Um, so this is sort of most of how we have
these computation graphs and we can work out backpropagation backwards in them.
There's sort of one more part of this to do,
um, which is to say g,
we want to do this eff- efficiently.
So, there's a bad way to do this which is to say, "Oh well,
we wanted to calculate the partial of this by b and so we can calculate that partial."
Which was essentially what I was doing on last time slides.
We say, "Um, partial of s by b equals the partial of s by h,
times the partial of h by z,
times the partial of z by b,
and we have all of those partials.
We work them all out and multiply them together and then someone says,
um, what's the partial of s by w?
And we say, huh, that's the chain rule again, I'll do it all again.
It's the partial of s by,
um, h times the partial of h by z,
times the partial of and z by x,
no, no, right, ah, lost it.
But you do big long list of them and you calculate all again.
That's not what we want to do.
Instead we want to say, "Oh,
look there's this shared stuff.
There's this error signal coming from above."
And we can work out the error signal the upstream gradient for this node.
We can use it to calculate the upstream gradient for this node.
We can use this to calculate the upstream gradient for this node and then,
using the local gradients of which there are two calculated
this node we can then calculate this one and that one.
Um, and then, from here having knowing this upstream gradient,
we can use the local gradients at this node to compute this one and that one.
And so, we're sort of doing this efficient computer science like computation,
um, where we don't do any repeated work. That makes sense?
Yeah. Okay. Um, and so if that is,
um, the whole of backprop.
So, um, here's sort of a slightly sketchy um graph
which is sort of just re-capitulating this thing.
So, if you have any computation that you want to perform, um, well,
the hope is that you can sort your nodes into
what's called a topological sort which means that things that are arguments,
variables that are arguments are sorted before
variables that are results that depend on that argument.
You know, providing you have something there's an a cyclic graph,
you'll be able to do that.
If you have a cyclic graph, you're in trouble.
Um, well, I'd be there actually techniques people
use to roll out those graphs but I'm not gonna go into that now.
So, we've sorted the nodes which is kind of loosely represented here from
bottom to top in a topological sort area, sort.
Okay. So then, for the forward prop we sort of go through the nodes in
the topological sort order and we
if it's a variable we just set its value to what it's favorite val- variable value is.
If it's computed from other variables their values must have been
set already because there earlier in the topological sort, um,
and then we compute the value of those nodes according to their predecessors,
and we pass it up and work out the final output,
the loss function of our neural network and that is our forward pass.
Okay. So then, after that we do our backward pass and so for
the backward pass we initialize the output gradient with one.
The top thing is always one,
the partial of z with respect to z.
And then, we now sort of go through the nodes in reverse topological sort.
And so therefore, each of them will all ready- anything that's,
ah, anything that's, uh, language is complex.
Anything that's above that.
Anything that we calculated based on it in terms of, ah,
forward pass will already have had calculated it's, um,
it's gradient as a product of upstream gradient
times local gradient and then we can use that,
um, to compute the next thing down.
Um, and so basically the ov- the overall role
is for any node you work out its set of successors,
the things that are above it that it,
that depend on it and then you say, "Okay,
the partial of z with respect to x is simply the sum over the set of
successors of the local gradient that you
calculated the node times the upstream gradient of that node."
Um, and in the examples that I gave before there was never,
never multiple upstream gradients.
But if you imagine a, a general big graph there could actually be
so different upstream gradients that are being used in- for the various successors.
So, we apply that backwards and then we've worked out in backpropagation, um,
the gradient of every,
the gradient of the final result z with respect to every node in our graph.
Um, and the thing to notice about this is,
if you're doing it right and efficiently,
the bigger o order of complexity of doing backpropagation is exactly the
same as doing forward propagation i.e expression evaluation.
So, it's not some super expensive complex procedure
that you can imagine doing and scaling up.
Um, you're actually in exactly the same complexity order.
Okay. Um, so as [inaudible] entered it here this procedure,
you could just think of something that you're running on
an arbitrary graph and calculating this forward pass and the backwards pass.
I mean, almost without exception that the kind of
neural nets that we actually use have a regular layer
like structure and that's then precisely why it makes
to- sense to work out these gradients in terms of,
um, vectors matrices and Jacobian's as the kind we were before.
Okay. Um, so since we have this sort of really nice algorithm now, um,
this sort of means that, um,
we can do this just computationally and so we don't have to think or know how to do math.
Um, and we can just have our computers do all of this with this.
Um, so that using this graph structure, um,
we can just automatically work out how to apply, um, backprop.
And there are sort of two cases of this, right?
So, if what was calculated at each node,
um, is given as a symbolic expression,
we could actually have our computer work out for
us what the derivative of that symbolic expression is.
So, it could actually calculate, um,
the gradient of that node and that's referred to as often as automatic differentiation.
So, this is kind of like Mathematica Wolfram Alpha.
You know how you can do your math homework on it?
You just type in your expression,
say what's a derivative and it gives it back to you right?
Um, it's working doing symbolic computation and working out the derivative for you.
Um, so that- so that method could be used to
work out the local gradients and then we can use
the graph structure and now rule
upstream gradient times local gradient gives downstream gradient,
i.e the chain rule, um,
to then propagate it through the graph and do
the whole backward pass completely automatically.
And so that sounds, um, great.
Um, slight disappointment, um,
current deep learning frameworks don't quite give you that.
Um, there was actually a famous framework that attempted to give you that.
So the Theano Framework that was developed at the University of Montreal, um,
those they've now abandoned in the modern era
of large technology corporation, deep learning frameworks.
Theano did precisely that.
It did the full thing of automatic differentiation, um,
for reasons that we could either think of good or bad,
current deep learning frameworks like TensorFlow or
PyTorch actually do a little bit less than that.
So what they do is, say,
well for an indiv- for the computations at an individual node,
you have to do the calculus for yourself.
Um, for this individual node,
you have to write the forward propagation, say, you know,
return X plus Y and you have to write the backward propagation,
saying the local gradients, uh,
one and one to the two inputs X and Y, um,
but providing you or someone else has
written out the forward and backward local step at this node,
then TensorFlow or PyTorch does all the rest
of it for you and runs the backpropagation algorithm.
[NOISE] Um, and then, you know, effectively,
that sort of saves you having to have a big symbolic computation engine,
because somewhat, the person coding
the node computations is writing
a bit of code as you might normally imagine doing it whether in,
you know, C or Pascal,
of saying returning X plus Y,
and, you know, local Gradient return one.
Right? And- and you don't actually have to have a whole symbolic computation engine.
Okay. So that means the overall picture looks like this.
Right? So um, schematically,
we have a computation graph, um,
and to calculate the forward computation, um,
we, um, so- sort of put inputs into
our computation graph where there's sort of X and Y variables,
and then we run through the nodes in topologically sorted order,
and for each node we calculate its forward and
necessarily the things that depends on and have already been
computed and we just do expression evaluation forward.
And then we return, um,
the final gate in the graph,
which is our loss function, or objective function.
But then, also we have the backward pass,
and for the backward pass,
we go in the nodes in reversed topological, um, resorted order,
and for each of those nodes,
we've return their backward value,
and for their top node,
we return backward value of one,
and that will then give us our gradients.
And so that means, um,
for any node, any piece of computation that we perform,
we need to write a little bit of code that um
says what it's doing on the forward pass and what it's doing on the backward pass.
So on the forward pass, um,
this is our multiplication,
so we're just saying return X times Y.
So that's pretty easy.
That's what you're used to doing.
But while we also need to do the backward passes,
local gradients of return what is the
partial of L with respect to Z and with respect to X.
And well, to do that,
we have to do a little bit more work.
So we have to do a little bit more work,
first of all, in the forward pass.
So, in the forward pass,
we have to remember to sort of stuff away in some variables
what values we computed in the for-
what- what values were given to us in the forward pass,
or else we won't be able to calculate the backward pass.
So we store away the values of X and Y,
um, and so then,
when we're doing the backward pass,
we are passed into us the upstream Gradient,
the error signal, and now we just do calculate, um,
upstream Gradient times local Gradient- upstream Gradient times local Gradient,
and we return backwards,
um, those um downstream Gradients.
And so providing we do that for all the nodes of our graph,
um, we then have something that, um,
the system can learn for us as a deep learning system.
And so what that means in practice,
um, is that, you know,
any of these deep learning frameworks come with a whole box of tools that says,
um, here is a fully connected forward layer,
here is a sigmoid unit,
here is other more complicated things we'll do later,
like convolutions and recurrent layers.
And to the extent that you are using one of those,
somebody else has done this work for you.
Right? That they've um defined, um,
nodes or a layer of nodes that have forward and backward already written for- for them.
And to the extent that that's true, um,
that means that making neural nets is heaps of fun. It's just like lego.
Right? You just stick these layers together and say,
"God, I have to learn on some data and train it."
You know, it's so easy that my high school student is building these things.
Right? Um, you don't have to understand much really,
um, but, you know,
to the extent that you actually want to do some original research and think,
"I've got this really cool idea of how to do things differently.
I'm going to define my own kind of different computation."
Well, then you have to do this and define your class,
and as well as, sort of saying,
how to compute the forward value,
you will have to pull out your copy of
Wolfram Alpha and work out what the derivatives are,
um, and put that into the backward pass.
Um, yeah.
Okay. So here's just one little more note on that.
Um, you know, in the early days of deep learning,
say prior to 2014,
what we always used to state to everybody very sternly is,
"You should check all your Gradients,
by doing numeric Gradient checks.
It's really really important."
Um, and so what that meant was, well, you know,
if you want to know whether you have coded your backward pass right,
an easy way to check, um,
whether you've coded it right,
is to do this numeric Gradient
where you're sort of estimating the slope by wiggling it a bit,
and wiggling the input a bit,
and seeing what effect it has.
So I'm working out the value of the function the F of X plus H,
for H very small like E to the minus four,
and then F of X minus H, um,
and then dividing by 2H,
and I'm saying well, what is the slope at this point,
and I'm getting a numeric estimate of the Gradient with respect,
um, to my variable X here.
Um, so this is what you will have seen in
high school when you did the sort of first um estimates of Gradients,
where you sort of worked out F of X plus H divided by H
and you're doing rise over run and got a point estimate of the Gradient.
Um, exactly the same thing,
except for the fact,
in this case, rather than doing it one sided like that,
we are doing it two-sided.
It turns out that if you actually wanna do this,
two-sided is asymptotically hugely [NOISE] better,
and so you're always better off doing
two-sided Gradient checks rather than one-sided Gradient checks.
Um, so since you saw that- since it's hard to implement this wrong,
this is a good way to check that your Gradients are
correct if you've defined them yourselves.
Um, as a technique to use it [NOISE] for anything,
it's completely, completely hopeless,
because we're thinking of doing this over
our deep learning model for a fully connected layer.
What this means [NOISE] is that,
if you've got this sort of like a W matrix of N by M and you want to, um,
calculate um your partial derivatives to check if they're correct,
it means that you have to do this for every element of the matrix.
So you have to calculate the eventual loss,
first jiggling W11, then jiggling W12,
then jiggling one- W13, 14 et cetera.
So you have- in the complex network,
you'll end up literally doing millions of function evaluations
to check the Gradients at one point in time.
So, you know, it's,
it's not like what I advertised for
backprop when I said it's just as efficient as calculating,
um, the forward value.
Doing this is forward
value computation time multiplied by number of parameters in our model,
which is often huge for deep learning networks.
So this is something that you only want to
have inside- if statements that you could turn off.
So you could just sort of run it to check that your code isn't bre- um, debuggy.
Um, you know, in honesty,
this is just much less needed now because, you know,
by and large you can plug together your components and layers and PyTorch,
um, and other people wrote the code right and it will work.
Um, so you probably don't need to do this all the time.
But it is still a useful thing to look at and to know
about if things um, are going wrong.
Yeah. Okay, so we- we've now mastered the core technology of neural nets.
We saw now well, basically everything we need to know about neural nets,
and I sort of just, um, summarized it there.
Um, just to sort of emphasize um once more.
Um, you know, I think some people think,
why do we even lear- need to learn all this stuff about gradients?'
And there's a sense in which it's [inaudible] really,
because these modern deep learning frameworks will compute all of the gradients for you.
You know, we make you suffer on homework two,
but in homework three,
you can have your gradients computed for you.
But, you know, I- so you know it's sort of just, like, well,
why should you take a c- a class on compilers, right?
That there's actually something useful in understanding what goes on under the hood,
even though most of the time,
we're just perfectly happy to let the C compiler do its thing,
without being experts on X86 assembler every day of the wa- week.
But, you know, there is more to it than that.
Um, you know, because even though backpropagation is great,
once you're building complex models,
backpropagation doesn't always work as you would expect it to.
Perfectly is maybe the wrong word,
because you know mathematically it's perfect.
Um, but it might not be achieving what you're wanting it to.
And well, if you want to sort of then debug an improved models,
it's kind of crucial to understand what's going on.
So, there's a nice medium piece by Andre Karpathy,
of yes you should understand backprop um that's on the syllabus page, um,
that talks about this and indeed um, um,
week after next, Abby is actually going to lecture about recurrent neural networks,
and you know one of the places, um,
where you can easily fail um,
and doing backpropagation turns up there,
um, is a good example.
Okay. So anyone have any questions about backpropagation and computation graphs?
Okay. If not the remainder of the time is, um,
the grab bag of things that you really should know about,
if you're going to be doing deep learning.
And so, yeah, this is just itsy-bitsy and,
but let me say them.
Um, so up until now,
when we've had um loss functions,
and we've been maximizing the likelihood of our data,
and stuff like that,
we've sort of just had this part here which is the likelihood of our data,
and we've worked to maximize it.
Um, however, um, in practice that works badly usually,
and we need to do something else which is regularize our models.
And if you've done the Machine Learning class,
or something like that you will have seen regularization.
And there are various techniques to do regularization, but, um,
compared to anything else,
regularization is even more important,
um, for deep learning models, right?
So, um, the general idea is if you have a lot of parameters in your model,
those parameters can just essentially memorize what's in the data that you trained at.
And so they're very good at predicting the answers.
The model becomes very good at predicting the answers to the data you trained it on,
but the model may become poor at working in the real world, and different examples.
And somehow we want to stop that.
And this problem is especially bad for deep learning models,
because typically deep learning models have vast,
vast numbers of parameters.
So in the good old days when statisticians ruled the show,
they told people that it was completely ridiculous to
have a number of parameters that approached your number of training examples.
You know, you should never have more parameters in your model,
than one-tenth of the number of your training examples.
So it's the kind of um rules of thumb you are told,
so that you had lots of examples with which to estimate every parameter.
Um, that's just not true with deep learning models,
is just really common that we trained
deep learning models that have 10 times as many parameters,
as we have training examples.
Um, but miraculously it works.
In fact it works brilliantly.
Those highly over parameterized models,
and this one of the big secret sources of why deep learning has been so brilliant,
but it only works if we regularize the model.
So, if you train a model without sufficient regularization,
what you find is that you're training it and working out your loss on the training data,
and the model keeps on getting better,
and better, and better, and better.
Um, necessarily, alg- algorithm has to improve loss on the training data.
So the worst thing that could happen,
is that the graph could become absolutely fa- flat.
What you'll find is with most models that we train,
they have so many parameters that this will just keep on going down,
until the loss is sort of approaching the numerical precision of zero,
if you leave it training for long enough.
It just learns the correct answer for every example,
beca- because effectively can memorize the examples.
Okay, but if you then say,
''Let me test out this model on some different data.''
What you find is this red curve,
that up until a certain point, um,
that you are also building a model that's better at predicting on different data,
but after some point this curve starts to curve up again.
And ignore that bit where it seems to curve down again,
that was a mistake in the drawing.
Um, and so this is then referred to as over-fitting,
that the- from here on the training model is
just learning to memorize whatever was in the training data,
but not in a way that later generalized to other examples.
And so this is not what we want.
We want to try and avoid over-fitting as much as possible,
and there are various regularization techniques that we use for that.
And simple starting one is this one here where we penalize the log-likelihood by saying,
''You're going to be penalized to the extent that you move parameters away from zero.''
So the default state of nature is all parameters are zeros,
so they're ignored on computations.
You can have parameters that have big values,
but you'll pee penalized a bit four,
and this is referred to as L-2 regularization.
And, you know, that's sort of a starting point of
something sensible you could do with regularization,
but there's more to say later.
And we'll talk in this sort of lecture before we discuss
final projects of other clever regularization techniques at neural networks.
Okay. Um, grab bag number two,
vectorization is the term that you have here,
um, but it's not only vectors.
This is also matrixization,
and higher dimensional matrices what are called tensors,
in this field tensorization.
Um, getting deep learning systems to run fast and
efficiently is only possible if we vectorize things.
Um, and what does that mean?
What that means is, you know,
the straightforward way to write a lot of code um,
that you saw in your first CS class,
is you say for I in range in um calculate random randi-1.
Um, but when we want to be clever,
um, people, um, that are doing things fast,
um, we say rather than work out this W dot one word vector at a time,
and do it in a four loop,
we could instead put all of our word vectors into one matrix,
and then do simply one matrix-matrix multiply of W by our word vector matrix.
And even if you run your code on your laptop on a CPU,
you will find out that if you do it the vectorized way,
things will become hugely faster.
So in this example,
it became over an order of magnitude faster,
when doing it with a vector- vectorized rather than,
um, with a full loop.
Um, and those gains are only compounded when we run code on a GPU,
that you'll get no gains and speed of tall on a GPU,
unless your code is vectorized.
But if it is vectorized,
then you can hope to have results, of oh,
yeah, this runs 40 times faster,
than it did on the CPU.
Okay, um, yeah, so always try to use vectors and matrices not for loops.
Um, of course it's useful when developing stuff to time your code,
and find out what's slow.
Um, okay.
Point three.
Um, okay, so we discussed this idea, um, last time,
and the time before that after- after having the sort of affine layer,
where we took, you know,
go from X to WX, plus B.
That's referred to as an affine layer,
so we're doing this, um,
multiplying a vector by a matrice- matrix,
and adding um biases.
We necessarily to have power and a deep network, um,
have to have some form of non-linearity.
And so, I just wanted to go through a bit of background
on non-linearity is in what people use,
and what to use.
So, if you're sort of starting from the idea of what we know is logistic regression, um,
what's commonly referred to as the sigmoid curve,
or maybe more precisely is the logistic,
um, function is this picture here.
So something that's squashes any real
number positive or negative into the range zero to one.
It gives you a probability output.
Um, these- this use of this, um,
logistic function was really really common in early neural nets.
If you go back to '80s, '90s neural nets,
there were, um, sigmoid functions absolutely everywhere.
Um, in more recent times,
90 percent of the time nobody uses
this and they've been found to sort of actually work quite poorly.
The only place these are used is when you
actually want a value between zero and one is your output.
So we'll talk later about how you have gating in networks,
and so gating as a place where you want to have a probability between two things.
And then you will use one of those,
but you use some absolutely nowhere else.
Um, here is the tanh curve.
Um, so the formula for tanh, um,
looks like a scary thing with thoughts of exponentials in it,
and it doesn't really look much like a logistic curve whatsoever.
Um, but if you um dig up your math textbook you can convince yourself that
a tanh curve is actually exactly the same as
the logistic curve apart from you multiply it by two,
so it has a range of two rather than one,
and you shift it down line.
So, this is sort of just a re-scaled logistic.
There's now symmetric between one and minus one,
and the fact that some metric in the output actually helps
a lot for putting into neural networks. Um.
So, tanh's, are still reasonably widely used
in quite a number of places um in um your networks.
So, tanh should be a friend of yours and you should know about that.
But you know, one of the bad things about using
um transcendental functions like the sigmoid or tanh is,
you know, they involve this expensive math operations um that slow you down.
Like, it's sort of a nuisance to be kind
of computing exponentials and tanh's in your computer,
things are kind of slow.
So people started um playing around with ways
to make things faster and so someone came up with this idea like,
maybe we could come up with a hard tanh,
um where it's just sort of flat out here
and then it has a linear slope and then it's flat at the top.
You know, it sort of looks like a tanh but we just squared it off.
Um, and while this is really cheap to compute right, you say,
x less than minus one,
return minus one, return plus one or just return the number.
No complex transcendentals.
The funny thing is,
it turns out that this actually works pretty well.
You might be scared and you might justifiably be
scared because if you start thinking about gradients,
once you're over here,
there's no gradient, right?
It's completely flat at zero.
So, things go dead as soon as they're at one of the ends.
So, it's sort of important to stay in this middle section at least for
a while and then its just got a slope of one, right?
It's a constant slope of one.
But this is enough of a linearity that actually it
works well in neural networks and you can train neural networks.
So, that's sent the whole field in the opposite direction and people thought,
oh, if that works,
maybe we can make things even simpler.
And that led to the now famous what's referred to [inaudible] as ReLU.
So there is a mistake in my editing there,
delete off hard tanh.
That was in slides by mistake.
[LAUGHTER] The ReLU unit,
everyone calls it ReLU which stands for rectified linear unit.
So, the Re-, the ReLU is essentially the simplest non-linearity you can have.
So the ReLU is zero,
slope zero as soon as you're in the negative regime and it's just a line slope one,
when you're in the positive regime.
I mean, when I first saw this,
I mean, it's sort of blew my mind it could possibly work.
Because it sort of, I guess,
I was brought up on these sort of tanh's and sigmoids and the sorts of these arguments
about the slope and you get these gradients and you can move around with the gradient.
And how is it meant to work if half of this function just says
output zero and no gradient and the other half is just this straight line.
And in particular, when you're in the positive regime,
this is just an identity function.
And, you know, I sort of argued before that if you just compose linear transforms,
you don't get any power but provided when this is the right-hand part of the regime.
Since this is an identity function,
that's exactly what we're doing.
We're just composing linear transforms.
So you- you sort of believe it just can't possibly
work but it turns out that this works brilliantly.
And this is now by far
the default choice when people are building feed for deep networks.
That people use ReLU non-linearities and they are very fast,
they train very quickly and they perform very well.
And so, effectively, you know,
it is, it is simply just each u-,
depending on the inputs,
each unit is just either dead or it's passing things on as an identity function.
But that's enough of lini-,
non-linearity that you can do
arbitrary function approximation still with a deep learning network.
And people now make precisely the opposite argument which is,
because this unit just has a slope of one over it's non-zero range, that means,
the gradient is past spec very efficiently to
the inputs and therefore the models train very efficiently whereas,
when you are with these kind of curves,
when you're over here, there's very little slope so your models might train very slowly.
Okay. So, you know,
for feed-forward network, try this before you try anything else.
But there's sort of then been a sub literature that says,
well, maybe that's too simple and we could do a bit better.
And so that led to the leaky ReLU which said,
"Maybe we should put a tiny bit of slope over here so it's not completely dead."
So you can make it something like one,
one 100th as the slope of this part.
And then people had, well,
let's build off that,
maybe we could actually put another parameter into
our neural network and we could have a parametric ReLU.
So, there's some slope over here but we're also going to
backpropagate into our non-linearity which has this extra alpha parameter,
which is how ma- much slope it has.
And so, variously people have used these,
you can sort of find 10 papers on archive where people say,
you can get better results from using one or other of these.
You can also find papers where people said it made
no difference for them versus just using a ReLU.
So, I think basically,
you can start off with a ReLU and work from there.
Yes. So, parameter initialization,
it's when, so, when we have these matrices and parameters in our model,
it's vital, vital, vital,
that you have to initialize those parameter weights with small random values.
This was precisely the lesson that
some people hadn't discovered when it came to final project time.
So I'll emphasize it is vital, vital.
So, if you just start off with the weights being zero,
you kind of have these complete symmetries,
right, that everything will be calculated the same,
everything will move the same and you're not actually training
this complex network with a lot of units that are specializing to learn different things.
So, somehow, you have to break the symmetry and we
do that by giving small random weights.
So, you know, there's sort of some fine points.
When you have biases,
you may as well just start them at Zero,
as neutral and see how the system learn the bias that you want et cetera.
But in general, the weights you want to initialize to small random values.
You'll find in PyTorch or other deep learning practi- packages,
a common initialization that's used and often recommended is this Xavier Initialization.
And so, the trick of this is that,
for a lot of models and a lot of places,
think of some of these things like these ones and these,
you'd like the values in the network to sort of stay small,
in this sort of middle range here.
And well, if you kind of have a matrix with big values in it
and you multiply a vector by this matrix,
you know, things might get bigger.
And then if you put in through another layer,
it'll get bigger again and then sort of everything
will be too big and you will have problems.
So, really, Xavier Initialization is seeking to avoid that by saying,
how many inputs are there to this node?
How many outputs are there?
We want to sort of temp it down the initialization based on the inputs
and the outputs because effectively we'll be using this number that many times.
It's a good thing to use, you can use that.
Optimizers. Up till now,
we saw, just talked about plain SGD.
You know, normally plain SGD actually works just fine.
But often if you want to use just plain SGD,
you have to spend time tuning the learning rate,
that alpha that we multiplied the gradient by.
For complex nets and situations or to avoid worry,
there's sort of now this big family and more sophisticated adaptive optimizers.
And so, effectively they're scaling the parameter adjustment by accumulated gradients,
which have the effect that they learn per parameter learning rates.
So that they can see which parameters would be useful to move
more and which one is less depending on the sensitivity of those parameters.
So, where things are flat,
you can be trying to move quickly.
Where things are bouncing around a lot,
you are going to be trying to move just a little so as not to overshoot.
And so, there's a whole family of these; Adagrad,
RMSprop, Adam, there are actually other ones.
There's Adam Max and whole lot of them.
I mean, Adam is one fairly reliable one that many people use and that's not bad.
And then one more slide and I'm done.
Yes, so learning rates.
So, normally you have to choose a learning rate.
So, one choice is just have a constant learning rate.
You pick a number, may be 10 to the minus three and say that's my learning rate.
You want your learning rate to be order of magnitude, right.
If your learning rate is too big,
your model might diverge or not converge because it just sort of leaps you around by
huge cram movements and you completely miss the good parts of your function space.
If your model, if your learning rate is too small,
your model may not train by the assignment deadline and then you'll be unhappy.
So, you saw that, you know,
commonly people sort of try powers of 10 and sees how it looks, right.
They might try, you know, 0.01, 0.001,
0.0001 and see, look at how the loss is declining and see what seems to work.
In general, you want to use
the fastest learning rate that isn't making things become unstable.
Commonly, you could get better results by decreasing the learning rate as you train.
So, sometimes people just do that by hand.
So, we use the term epoch for a full pass
through your training data and people might say,
half the learning rate after every three epochs
as you train and that can work pretty well.
You can use formulas to get per epoch tra- learning rates.
There are even fancier methods.
You can look up cyclic learning rates online if you want,
which sort of actually makes the learning rates
sometimes bigger and then sometimes smaller,
and people have found that that can be useful for getting you out
of bad regions in interesting ways.
The one other thing to know is,
if you're using one of the fancier optimizers,
they still ask you for a learning rate but that learning rate is
the initial learning rate which typically the optimizer will shrink as you train.
So, commonly if you're using something like Adam,
you might be starting off by saying the learning rate is 0.1,
so of a bigger number and it will be shrinking it later as the training goes along.
Okay, all done. See you next week.
 Okay. Let's get started again.
Okay. So welcome back to, um,
week three of CS224N.
Okay. So we- we've got a bit of a change of pace today after week two.
So, um, this week in week three,
we're actually going to have some human language,
and so this lecture has no partial derivative signs in it.
And so we'll be moving away, um,
from sort of working out the so technicalities of doing, um,
new networks and back propagation,
um, and a sort of math heavy week two.
So then, this week,
what we actually want- well,
in today's lecture, we want to look at, well,
what kind of structures do human language sentences have,
and how we can build models that,
um, build that kind of structure for sentences that we see.
Um, so first of all,
I'm gonna sort of explain and motivate a bit about,
um, structure of human language sentences.
So, that's kind of like, um,
linguistics in 20 minutes or something.
Um, then going particularly focusing on dependency grammars,
and then gonna present a method for doing dependency structure,
dependency grammar parsing called transition-based dependency parsing.
And then talk about how you can make neural, um, dependency parsers.
Um, so, um, going on just,
you know, a couple of announcements.
So, assignment two was due one minute ago,
so I hope everyone's succeeded,
um, in getting assignment two out of the way.
If you're still working on it,
do make sure to make, um,
use of the office hours and get help for that.
Coming out just today is assignment three.
Um, assignment three, um,
is basically about this lecture.
Um, so, [LAUGHTER] in assignment three,
what you're doing is building a neural dependency parser,
and so we hope that you can put together what you learned about
neural networks last week and the content of today,
and jump straight right in to building a neural dependency parser.
Um, the other thing that happens in assignment three is that,
we start using a deep learning framework PyTorch.
So, for doing assignment three, instruction zero,
and this is in the PDF for the assignment,
is to install PyTorch as a Python package,
and start using that.
Um, so we've attempted to make assignment three sort of be a highly scaffolded tutorial,
where you can start to learn how to do things in PyTorch by just,
um, writing a few lines of code at a time.
Hopefully that works out for people.
Um, if you have any issues with,
with that, um, well,
obviously, you can send Piazza messages,
come to office hours.
I mean, the one other thing you could think of doing is that there's sort of
a one hour introduction to PyTorch on the PyTorch site,
where you down- where you're directed for installing PyTorch,
and you could also look at that if that was maybe helpful.
Um, now the final mentions, yes.
So, um, final projects, um, you know,
we're going to sort of focus on those more in week five,
but if it's not bad to be thinking about things you could do,
if you're under a custom final project.
You're certainly encouraged to come and talk to me or the TAs.
We have under the sort of office hours page on the website,
a listing of the expertise of some of the different TAs.
Um, since I missed my office hours yesterday,
I'm gonna have a shortened office hour tomorrow from 1:00 to 2:20.
Um, that's at the same time as the,
um, normal CS224N, um,
office hours, so you can kind of come for any reason you want,
but it might be especially good to come to me if you want
to talk about, um, final projects.
Okay. So, let's leap in and start talking about the structure of sentences.
And so, I just sort of want to explain something about human language sentence structure,
and how people think about that structure,
and what kind of goals then people in natural language processing
have of sort of building structure to understand the meaning of sentences.
Um, all of the examples I'm going to give today are in English,
um, because that's the language that you're all expected to have some competence in.
But this really isn't meant to be sort of facts about English.
This is meant to be sort of ideas of how you can think about the structure of
human language sentences that are applied to all sorts of languages.
Okay. So in general,
there are two different ways that
linguists have thought about the structure of sentences,
though there's some relations to them.
One of them is called phrase structure,
or phrase structure grammars.
And if you vaguely remember from CS103 if you did that,
when you spent about the lecture on context-free grammars, um,
phrase structure grammars are using the tools of
context-free grammars to put structures over sentences.
So, I'm first of all going to just briefly introduce that, so you've seen it,
but actually the main tool that we're going to
use in this class and for assignment three,
is to do put dependency structures over,
um, sentences, so I'll then go about that.
So, the idea of phrase structure is to say that
sentences are built out of units that progressively nest.
So, we start off with words that, cat, cuddly,
et cetera, and then we're gonna put them into bigger units that we call phrases,
like "The cuddly cat by the door",
and then you can keep on combining those up into even bigger phrases,
like, "The cuddly cat by the door."
Um, [NOISE] Okay, that's that.
So, how does this work?
Well, so the idea of it,
and this is sort of the way linguists thinks,
is to say, "Well,
here's this language, which,
you know, might not be English.
It might be Oaxacan or some other language.
What kind of structure does it have?
And well, we could look at lots of sentences of the language.
And so the linguist is gonna think,
"Well, I can see,
um, patterns, like the cat,
a dog, the dog,
a cat, et cetera.
So, it's sort of seems like there's one word class here,
which linguists often referred to as determiners.
Um, they're also referred to as article sometimes in English.
There's another word class here of nouns.
And so, what I- to capture this pattern here,
it seems like we can make this unit, um,
that I see all over the place in language, um,
which is made of a,
um, a determiner, followed by a noun.
So, I've write, um,
a phrase structure grammar role,
a context-free grammar role of- I can have
a noun phrase that goes to a determiner, and a noun.
Okay. But, you know,
that's not the only thing that I can, um, see.
So, I can also see, um,
other examples in my language of the large cat,
or a barking dog,
or the cuddly cat, the cuddly dog.
So, that seems that I need to put a bit more stuff into my grammar.
So, maybe I can say from my grammar that a noun phrase goes to a determiner,
and then optionally, you can put in an adjective,
and then you can have a noun.
And then I poke around a little bit further and I
can find examples like the cat in a crate,
or a barking dog by the door.
And I can see lots of sentences like this.
And so I want to put those into my grammar.
But at that point, I noticed something special, because look,
here are some other things,
and these things look a lot like the things I started off with.
So, it seems like,
which sort of having a phrase with
the same expansion potential that's nested inside this bigger phrase,
because these ones can also be, um, expanded, right?
I could have something like the green door something in here.
So, I just wanna capture that in some way.
So, maybe I could say that a noun phrase goes to a determiner,
optionally an adjective, a noun,
and then a something else,
which I'll call a prepositional phrase.
And then I'm gonna write a second rule saying that
a prepositional phrase goes to a preposition,
that's gonna be these words here,
um, followed by a noun phrase.
So then I'm reuse- [NOISE] I'm reusing my noun phrase that I defined up here.
So then I could immediately generate other stuff.
I can sort of say,
"The cat by the, the large door."
Or indeed I could say,
"The cat by the large crate."
Um, "The cat by the large crate on the table",
or something like that,
because once I can have the prepositional phrase includes a noun phrase,
and a noun phrase includes a prepositional phrase,
I've already got something that I can kind of
recursively go back and forth between noun phrases,
and I can make infinitely big sentences, right?
Yeah?
Yeah? So, I could write something like, yeah,
"The cat by
the large crate on the,
um, large table, um, by the door."
Right. I can keep on going and make big sentences.
And I could say, well,
I've got a- I don't have space to fit it on this slide,
but I've got an analysis of this according to my grammar,
where that's a noun phrase goes to a determiner noun prepositional phrase.
The prepositional phrase goes to a preposition,
and a noun phrase,
and this noun phrase goes to a determiner,
adjective, noun prepositional phrase.
And that goes to a preposition,
and another noun phrase,
and I keep on going and I can produce big sentences.
Okay. You know, that kind of then continues on,
because, um, you know,
I can then start seeing more bits of grammar.
So, I could say, "Well,
I can now talk to the cat."
Um, and so if I wanna capture,
um, this talking to a cat here, well,
that now means I've got a verb,
because words like talk and walk are verbs.
And then talk to the cat,
it seems like after that,
it could become a prepositional phrase.
And so I could write another rule saying that a verb phrase
goes to a verb followed by a prepositional phrase.
And then I can make more bigger sentences like that.
And I could look at more sentences of the language and start building up these,
these context-free grammar rules to describe the structure of the language.
And that's part of what linguists do,
and different languages, um, have different structures.
So, um, for example,
like in this, uh,
little grammar I've had and in general in English, um,
what you do, what you find is that prepositional phrases following the verb.
But if you go to a different language like Chinese,
what you find is the prepositional phrases come before the verb.
And so, we could say okay,
there are different rules for Chinese, um,
and I could start writing a context-free grammar for them. Okay, beauty.
Um,so that's the idea of context-free grammars,
and actually, you know,
this is the dominant approached linguistic structure
that you'll see if you go and do a linguistics class in the linguistics department,
people make these kinds of Phrase Structure Grammar trees.
Um, but just to be contrary,
no, it's not actually just to be contrary,
it's because this alternative approach has been
very dominant in computational linguistics.
What I'm going to show you instead, um,
is the view point of dependency structure.
So, the idea of dependency structure
is rather than having these sort of phrasal categories,
like, noun phrases and prepositional phrases,
and things like that,
we are going to directly, um,
represent the structure of sentences by saying,
how words, how arguments or modifiers of other words in a recursive faction.
Which is sort of another way of saying how the dependence on other words.
So, we have a sentence,
''Look in the large crate in the kitchen by the door''.
And if we want to we can give these word,
words word classes, so we can still say this is a verb,
and this is a preposition,
and this is a determiner,
and this is an adjective,
and this is a noun.
But to represent the structure,
what we're going to say is, "Well,
look here is the the root of this whole sentence."
So, that's where things start.
Um, and then, well, where are we going to look is in the large crate,
so that is a dependent of look.
And well, if we- then we have for the crate,
it's got some modifies its a large crate.
So, that's a dependent of crate.
Its the large crate,
that's a dependence of crate.
And in this system of dependencies I'm going to show you,
we've got in as kind of,
um, a modifier of crate in the large crate.
I could come back to that.
Well, but this crate has its own modification,
because it's a crate in the kitchen.
So, we have, in the kitchen,
as a modifier of crate.
And it's the kitchen in the kitchen,
these are dependence of crate.
And well, then we have this next bit by the door.
And as I'll discuss in a minute, well,
what does the by the door modifying?
It's still modifying the crate,
it saying, ''It's the crate by the door.''
Okay. So, the by the door is also a dependent of crate,
and then we've got the structure of dependencies coming off of it.
Okay. And so that's then, um,
the structure you get may be drawn a little bit more
neatly when I did that in advance like this.
And so we call these things, uh, dependency structure.
And so crucially, what we're doing here,
um, is that we're- sorry,
I had two different examples.
[NOISE] different examples.
[LAUGHTER] Um, um,
what we're doing is saying, what,
what words modify other words?
And so, that allows us to sort of
understand how the different parts of the sentence relate to each other.
And so, overall, you know,
then- let me just so say here,
you might want to why do we need sentence structure?
You know, the way, um,
language seems to work when you're talking to
your friends is that you just blab of something,
and I understand what you're saying, and, um,
what goes on beyond that, um,
is sort of not really accessible to consciousness.
But well, to be able to have machines that interpret language correctly,
we sort of need to understand the structure of these sentences,
because unless we know what words are arguments and modifiers of other words,
we can't actually work out what sentences mean.
And I'll show some examples of that as to how things go wrong immediately,
because actually, a lot of the time there are
different possible interpretations you can have.
And so, in general,
our goal is, you know,
up until now we've sort of looked at the meaning of words, right?
We did word vectors,
and we found that words there was similar meaning,
and things like that.
Um, and you can get somewhere in human languages with just saying words.
I mean you can say, "Hi",
and friendly, um, and things like that,
but you can't get very far with just words, right?
The way human beings can express
complex ideas and explain and teach things to each other,
is you can put together words to express more complex meanings.
And then, you can do that over and over again
recursively to build up more and more complex meanings,
so that by the time you're reading the morning newspaper,
you know most sentences are sort of 20-30 words long,
and they're saying, um,
some complex meaning, like you know,
"Overnight Senate Republicans resolve that they would not do blah blah blah blah.''
And you understand that flawlessly,
by just sort of putting together those meanings of words.
And so, we need to be able to know what is connected to
what in order to be able to do that.
And one of the ways of saying, um,
that's important is saying,
''What can go wrong?''
Okay. So here, is a newspaper article.
Uh, ''San Jose cop kills man with knife''.
Um, now, this has two meanings and the two meanings, um,
depend on, well, what you decide depends on what,
you know, what modifies what?
So, what are the two meanings. Meaning one.
The cop stabs the guy. [LAUGHTER]
The cop stabs the guy.
Right. So, meaning one is the cop stabs that guy.
So, what we've got here is,
we've got the cops that are killing.
So, this is what we'll say is the subject of kill,
is the cops, and I'll just call them the San Jose cops here.
And well, there's what they kill which say that,
the man is an object of killing.
Um, and then while one person is the,
the cop using knife to kill the person.
And so that's then that this is, um,
modifier and here if we complex we call it an instrumental
modifier to say that the cops are killing people with a knife.
That's one possible analysis.
Okay. Then, there's a second meaning sentence can have.
The second meaning sentence can have. [NOISE]
Okay. The second meaning the sentence can have is,
that's the man has a knife.
So, um, in that case,
what we wanna say is, well, you know,
is this word man,
and this man has, uh,
noun modifier, um, which is sort of saying something that the man possesses,
and then this dependency is the same,
and it's a man with a knife.
Okay. And so, the interpretations of these sentences that you can get depend on putting
different structures over the sentences in terms of who is- what is modifying what?
Um, here is another one that's just like that one.
Um, scientists count whales from space.
[LAUGHTER] Okay.
So again, this sentence has two possible structures, right?
[LAUGHTER] That we have, the scientists are the subject that are
counting and the whales are the object.
Um, and, well, one possibility is that this is how they're doing the counting,
um, so that they're counting the whales from space using something like a satellite.
Um, but the other possibility is that these parts are the same,
this is the subject,
and this is the object,
but these are whales from space which, you know,
we could have analyzed as a noun phrase goes to,
um, and now, on a PP,
you know, um, constituency grammar,
but its dependency grammar we saying, "Oh,
this is now a modifier of the whales,
and that they are whales from space, um,
that are starting to turn up as in the bottom example."
Right? So, obviously what you want is this one is correct and this one is here wrong.
Um, and so this choice is referred to as a prepositional phrase attachment ambiguity,
and it's one of the most common ambiguities in the parsing of English, right?
So, here's our prepositional phrase from space.
And so in general,
when you have prepositional phrases and before it you have verbs,
and noun phrases, or nouns,
that the prepositional phrase can modify
either of the things that come beforehand, right?
And so this is a crucial way in which
human languages are different from programming languages, right?
In programming languages, we have hard rules
as to how you meant to interpret things that dangle afterwards, right?
So, in programming languages,
you have an else is always construed with the closest if.
Well, if that's not what you want, um,
you have to use parentheses or indentation or something like that.
I guess, it's different in Python because you have to use indentation.
But if we think of something like C or a similar language, right?
Um, if you haven't used,
um, braces to indicate,
it's just deterministically, the else goes with the closest if.
Um, but that's not how human languages are.
Human languages are, um,
this prepositional phrase can go with anything proceeding,
and the hearer is assumed to be smart enough to work out the right one.
And, you know, that's actually a pa- large part of why
human communication is so efficient, right?
Like, um, we can do such a good job at communicating with
each other because most of the time we don't have to say very much,
and there's this really smart person on the other end, um,
who can interpret the words that we say in the right way.
Um, so, that's where if you want to have artificial intelligence and smart computers,
we then start to need to build language understanding devices who can also,
um, work on that basis.
That they can just decide what would be the right thing for form space to modify.
And if we have that working really well,
we can then apply it back to programming languages,
and you could just not put in any braces in your programming languages,
and the compiler would work out what you meant.
Um, okay. So, this is prepositional phrase attachment.
It's sort of seems maybe not that hard there,
but you know, it, it gets worse, I mean,
this isn't as fun an example,
but it's a real example of a sentence from The Wall Street Journal actually.
The board approved this acquisition by Royal Trustco Limited of Toronto for $0.27,
$27 a share at its monthly meeting.
Boring sentence, but, um,
what is the structure of this sentence?
Well, you know, we've got a verb here,
and we've got exactly the same subject,
and for this noun,
um, object coming after it.
But then what happens after that?
Well, here, we've got a prepositional phrase.
Here, we've got a prepositional phrase.
You've just got a see four prepositional phrases in a row.
And so, well, what we wanna
do is say for each of these prepositional phrases what they modify,
and starting off there only two choices,
the verb and the noun proceeding as before.
But it's gonna get more complicated as we go in, because look,
there's another noun here,
and another noun here,
and another noun here.
Um, so once we start getting further in there'll be more possibilities.
Okay. So, let's see if we can,
um, work it out.
So, um, by Royal Trustco Limited, what's that modifying?
[NOISE] Right. You see acquisition,
so it's not the board approved by Royal Trustco Limited,
it's an acquisition by Royal Trustco Limited.
Okay. So, this one is a dependent of the acquisition.
Okay. Um, now, we went to of Toronto,
and we have three choices,
that could be this, this, or this.
Okay. So, of Toronto is modifying.
Acquisition. [NOISE]
Its acquisition of Toronto?
[LAUGHTER] No, I think that's a wrong answer.
Um. [LAUGHTER] Is there another guess for what of Toronto is modifying?
Royal Trustco.
Royal Trustco, right. So, it's Royal Trustco Limited of Toronto.
So, this of Toronto is a dependent of Royal Trustco Limited.
And Royal Trustco Limited,
right, that's this again,
sort of this noun phrase,
so it can also have modifiers by prepositional phrase.
Okay. For $27 a share is modifying acquisition, right?
[NOISE] So now, we leap right back.
[NOISE] I'm drawing this wrong.
Now, we leap right back and,
um, is now the acquisition that's being modified.
And then finally, we have at its monthly meeting is modifying?
[NOISE]
Approved.
Well, the approved, right?
It's approved, yeah.
It's approved that its monthly meeting.
Okay. [NOISE] I drew that on,
[NOISE] I drew that one the wrong way around with the arrow.
Sorry, it should have been done this way.
I'm getting my arrows wrong. [NOISE] Um, um.
Okay. So that we've got this pattern of how things are modifying.
Um, [NOISE] and so actually, you know,
once you start having a lot of things that have choices like this,
you stop having- if I wanna put an analysis ac-
on to this sentence I've to work out the, the right structure,
I have to potentially consider an exponential number of possible structures because,
I've got this situation where for the first prepositional phrase,
there were two places that could have modified.
For the second prepositional phrase,
there are three places that could have modified.
For the fourth one,
there are five places that could have modified.
That just sounds like a factorial.
It's not quite as bad as the factorial, because normally,
once you've let back that kind of closes off the ones in the middle.
And so, further prepositional phrases have to be
at least as far back in terms of what they modify.
And so, if you get into this sort of combinatorics stuff the number of analyses you get
when you get multiple prepositional phrases is the sequence called the Catalan numbers.
Ah, but that's still an exponential series.
And it's sort of one that turns up in a lot of places when they're tree-like contexts.
So, if any of you are doing or have done CS228,
where you see, um,
triangular- triangulation of, ah,
probabilistic graphical models and you ask how many triangulations there are,
that's sort of like making a tree over your variables.
And that's, again, gives you the number of them as the Catalan series.
Okay. But- so the point is,
we ha- end up with a lot of ambiguities.
Okay. So, that's prepositional phrase attachments.
A lot of those going on.
They are far from the only kind of ambiguity.
So, I wanted to tell you about a few others.
Um, okay, shuttle veteran and longtime NASA executive Fred Gregory appointed to board.
Um, why is this sentence ambiguous?
What are the different reading of this statement?
[NOISE].
Yes?
Uh, it's a better [inaudible]
Okay. So, um, right answer.
So, yeah there are two possibilities, right?
That is either that there's somebody who's
a shuttle veteran and a long time NASA executive,
and their name is Fred Gregory,
and that they've been appointed to the board.
Um, or, um, the other possibility
is that there's a shuttle veteran and there's a long time NASA executive,
Fred Gregory, and both of them have been appointed to the board.
And so, again, we can start to indicate the structure of that using our dependency.
So, we can ether,
um, say, okay, um,
there's Fred Gregory and then this person is, um,
a shuttle veteran and long ta- and whoops,
and longtime NASA executive.
Or we can say, well,
we're doing appointment of a veteran and the longtime NASA executive, Fred Gregory.
And so, we can represent by dependencies,
um, these two different structures.
Okay. Um, that's, um, one.
Um, That one is not very funny again.
So- so, here's a funnier example that illustrates the same ambiguity effectively.
Um, so, here's precedence first physical.
Doctor: No heart, cognitive issues.
[LAUGHTER] Um, so, there isn't actually an explicit,
um, coordination word here.
But effectively in, um,
a natural language or certainly English, um,
you can use kind of just comma of sort of list intonation
to effectively act as if it was an "And" or an "Or", right?
So, here, um, we have again two possibilities that either we have
issues and the dep- and the dependencies
of- the dependencies of issues is that there are no issues.
So, that's actually a determiner, ah, no issues.
Um, and then it's sort of like no heart or cognitive issues.
So, heart is another dependent.
It's sort of a non-compound heart issues.
And so, we refer to that as an independency,
and then it's heart or, um, cognitive.
Um, so that heart or cognitive is
a conjoined phrase inside of this "No heart" or "Cognitive issues".
But there's another possibility,
um, which is, um,
that the coordination is at the top level that we have "No heart" and "Cognitive issues".
And, um, at that point,
we ha- have the "Cognitive" as an adjective modifier of the "Issues" and the "No heart",
the determiner is just a modifier of "Heart",
and then these being conjoined together.
So, um, "Heart" has a depend- has a coordinated dependency of "Issues".
Okay. That's one one.
Um, I've got more funny ones.
Susan gets- [NOISE] [LAUGHTER] Okay.
So, what the person [LAUGHTER] who wrote this intended to
have is that there- we- Here we've got an adjective modifier ambiguity.
So, the intended reading was, um,
that "First" is an adjectival modifier of "First hand" and it's firsthand experience.
Um, so, the "First hand" is a modifier of
"Experience" and the "Job" is also a modifier of "Experience".
And then we have the same kind of subject,
object, um, reading on that one.
Um, but unfortunately, um, this sentence, um,
has a different reading, um,
where you change the modification relationships.
Um, and you have it's the first experience and it goes like this. Um. [LAUGHTER] Okay.
[NOISE] One more example.
Um, "Mutilated body washes up on Rio beach to be used for Olympics beach volleyball."
Um, wha- what are- [LAUGHTER]
what are the two ambigui- What are the two readings that you can get for this one?
[NOISE]
We've got this big phrase that I want to try and put
a structure of to be used for Olympic beach volleyball,
um, and then, you know,
this is sort of like a prepositional phrase attachment ambiguity
but this time instead of it's a prepositional phrase that's being attached,
we've now got this big verb phrase we call it, right,
so that when you've sort of got most of a sentence but without any subject to it,
that's sort of a verb phrase to be used for
Olympic beach volleyball which might be then infinitive form.
Sometimes it's in part of CPO form like being used for beach volleyball.
And really, those kind of verb phrases they sort of just like, um, prepositional phrases.
Whenever they appear towards the right end of sentences,
they can modify various things like verbs or nouns.
Um, so, here, um, we have two possibilities.
So, this to be used for Olympics beach volleyball.
Um, what the right answer is meant to be is that that is a dependent of the Rio beach.
So, it's a, um,
modifier of the Rio Beach.
Um, but the funny reading is,
um, that instead of that, um,
we can have here is another noun phrase muti- mutilated body,
um, and it's the mutilated body that's going to be used.
Um, and so then this would be, uh,
a noun phrase modifier [NOISE] of that.
Okay. Um, so knowing the right structure of sentences is
important to understand the interpretations you're
meant to get and the interpretations you're not meant to get.
Okay. But it's, it's sort of, um, okay,
you know, I was using funny examples for the obvious reason, but, you know,
this is sort of essential to all the things that
we'd like to get out of language most of the time.
So, you know, this is back to the kind of
boring stuff that we often work with of reading through
biomedical research articles and trying to extract facts
about protein-protein interactions from them or something like that.
So, you know, this is, um,
the results demonstrated that KaiC interacts rhythmically with SasA Ka- KaiA and KaiB.
Um, and well, [NOISE] I turned the notification's off.
[NOISE] Um, so, if we wanna get out sort of protein-protein interaction,
um, facts, you know, well,
we have this KaiC that's interacting with these other proteins over there.
And well, the way we can do that is looking at patterns in our dependency analysis,
and so that we can sort of, um,
see this repeated pattern where you have, um,
the noun subject here interacts with a noun modifier,
and then it's going to be these things that are beneath that of the SasA
and its conjoin things KaiA and KaiB are the things that interacts with.
So, we can kind of think of these two things as essentially, um, patterns.
[NOISE] I actually mis-edited this.
Sorry. This should also be nmod:with.
[NOISE] Um, we can kind of think of
these two things as sort of patterns and
dependencies that we could look for to find examples of,
um, just protein-protein interactions that appear in biomedical text.
Okay. Um, so that's the general idea of what we wanna do,
and so the total we want to do it with is these Dependency Grammars.
And so, I've sort of shown you some Dependency Grammars.
I just want us to sort of motivate Dependency Grammar a bit more,
um, formally and fully, right?
So, Dependency Grammar, um,
postulates the what is syntactic structure is is that you have, um,
relations between lexical items that are sort of
binary asymmetric relations which we draw as arrows,
because they are binary and asymmetric,
and we call dependencies.
And there's sort of two ways, common ways,
of writing them, and I've sort of shown both now.
One way is you sort of put the words in a line and that makes it.
He see, let's see the whole sentence.
You draw this sort of loopy arrows above them and
the other way is you sort of more represent it as a tree,
where you put the head of the whole sentence at the top,
submitted and then you say the dependence of submitted,
uh, bills were in Brownback and then you say,
um, the dependence of each of those.
Um, so, it was bills on ports and immigration.
So, the dependence of bills and were submitted words,
the dependent of submitted and you're giving this kind of tree structure.
Okay. Um, so, in addition to the arrows commonly what we do is we
put a type on each arrow which says what grammatical relations holding them between them.
So, is this the subject of the sentence?
Is it the object of the verb?
Is that a, um,
a conjunct and things like that?
We have a system of dependency labels.
Um, so, for the assignment,
what we're gonna do is use universal dependencies,
which I'll show you more,
a little bit more in a minute.
And if you think,
"Man, this stuff is fascinating.
I wanna learn all about these linguist structures."
Um, there's a universal dependency site, um,
that you go and can go off and look at it and learn all about them.
But, if you don't think that's fascinating, um,
for what we're doing for this class,
we're never gonna make use of these labels.
All we're doing is making use of the arrows.
And for the arrows,
you should be able to interpret things like prepositional phrases as to what they're
modifying just in terms of where
the prepositional phrases are connected and whether that's right or wrong.
Okay. Yes. So formally,
when we have this kind of Dependency Grammar,
we've sort of drawing these arrows and we sort of refer to
the thing at this end as the head of a dependency.
And the thing at this end as the dependent of the dependency.
And as in these examples are normal expectation
and what our policies are gonna do is the dependencies form a tree.
So, it's a connected acyclic single,
um, rooted graph at the end of the day.
Okay. So, Dependency Grammar has an enormously long history.
So, basically, the famous first linguists that human beings know about his Panini who,
um, wrote in the fifth century before the Common Era
and tried to describe the structure of Sanskrit.
And a lot of what Panini did was working out things about all of
the morphology of Sanskrit that I'm not gonna touch at the moment.
But beyond that, he started trying to describe the structure of Sanskrit sentences.
And, um, the notation was sort of different but, essentially,
the mechanism he used for describing the structure of
Sanskrit was dependencies of sort of working out these,
um, what are arguments in modifies of what relationships like we've been looking at.
And indeed, if you look at kind of the history of humankind, um,
most of attempts to understand the structure of
human languages are essentially Dependency Grammars.
Um, so, sort of in the later parts of the first millennium,
there was a ton of work by Arabic grammarians and essentially what
they used is also kind of basically a Dependency Grammar.
Um, so compared to that, you know,
the idea of context-free grammars and
phrase structure grammars is incredibly incredibly new.
I mean, you can basically, um, totally date it.
There was this guy Wells in 1947 who first proposed
this idea of having these constituents and phrase structure grammars,
and where it then became really famous is through the work of Chomsky, um,
which love him or hate him is by far the most famous, um,
linguist and also variously contributed to Computer Science.
Who's head of the Chomsky hierarchy?
Do people remember that 103?
Yeah. Okay, the Chomsky hierarchy,
the Chomsky hierarchy was not invented to torture beginning computer science students.
The Chomsky hierarchy was invented because Chomsky wanted to make
arguments as to what the complexity of human languages was, um.
Okay. Yeah. So, in modern work,
uh, there's this guy Lucie Tesniere.
Um, and he sort of formalized
the kind of version of dependency grammar that I've been showing you.
So, um we sort of often talk about his work.
And you know it's- it's long-term being influential and computational linguistics.
Some of the earliest parsing work in
US Computational Linguistics was dependency grammars.
But I won't go on about that um more now.
Okay. Um, just one,
two little things um, to note.
I mean, if you somehow start looking at other papers where their dependency grammars,
people aren't consistent on which way to have the arrows point.
There's sort of two ways of thinking about this um,
that you can either think okay,
I'm gonna start at the head and point to the dependent.
Or you can say I'm going to start at the dependent and say what its head is,
and you find both of them.
Uh, the way we're gonna do it in this class is to do it the way Tesniere did it,
which was she started the head and pointed to the dependent.
Uh, sorry. I'm drawing that wrong.
Whoops, um because discussion of the outstanding issues.
So, really um, the dependent is sort of discussion.
Um, okay. We go from heads to dependence.
And usually, it's convenient to serve in addition to the sentence to
sort of have a fake root node that points to the head of the whole sentence.
So, we use that as well.
Okay. Um, so to build a dependency pauses or to indeed build
any kind of human language structure
finders including kind of constituency grammar pauses,
the central tool in recent work,
where recent work kind of means the last 25 years has been this idea of tree banks.
Um, and the idea of tree banks is to say we are going to get
human beings to sit around and [NOISE] put grammatical structures over sentences.
So, here are some examples I'm showing you from
Universal Dependencies where here are some um, English sentences.
I think Miramar was a famous goat trainer or something.
And some human being has sat and put
a dependency structure over this sentence and all the rest.
Um, and with the name Universal Dependencies,
this is just an aside.
Um, Universal Dependencies is actually project I've been strongly involved with.
But precisely what the goal of universal dependencies
was is to say what we'd like to do is have
a uniform parallel system of
dependency description which could be used for any human language.
So, if you go to the Universal Dependencies website,
it's not only about English.
You can find Universal Dependency analyses of you know, French,
or German, or Finish,
or Carsac, or Indonesian,
um, lots of languages.
Of course, there are um, even more languages
which there aren't Universal Dependencies analyses of.
So, if you have a- a big calling to say I'm gonna
build a Swahili Universal Dependencies um,
treebank, um, you can get in touch.
Um, but anyway.
So, this is the idea of treebank.
You know, historically, tree banks wasn't something that people thought of immediately.
This so- an idea that took quite a long time to develop, right?
That um, people started thinking about grammars
of languages even in modern times in the fifties,
and people started building parses for languages in the 19, early 1960s.
So, there was decades of work in the 60s,
70s, 80s, and no one had tree banks.
The way people did this work is that they wrote grammars,
that they either wrote grammars like the one I did for constituency of
noun phrase goes to determiner, optional adjective noun.
Noun goes to goat um,
or the equivalent kind of grammars and a dependency format,
and they hand built these grammars and then train,
had parsers that could parse these sentences.
Going into things, having a human being write a grammar feels more efficient.
Because if you write uh,
a rule like noun phrase goes to determiner optional adjective noun.
I mean, that- that describes
a huge number of phrases or actually infinite number of phrases.
Um, so that you know,
this is the structure of you know, the cat, the dog,
or cat or dog, or large dog all those things we saw at the beginning.
So, it's really efficient you're capturing lots of stuff with one rule.
Um, but it sort of turned out that in practice that wasn't such a good idea,
and it turned out to be much better to have
these kind of treebank supporting structures over sentences.
It's often a bit more subtle was to why that
is because it sounds like pretty menial work um,
building tree banks, and in some sense it is.
Um, but you know,
it turns out to be much more useful.
I mean, so one huge benefit is that treebanks are very reusable.
That effectively what they was in 60s, 70s,
and 80s was that every different you know,
people who started about building a parser invented
their own notation for grammar rules which got more and more complex,
and it was only used by their parser and nobody else's parser.
So, there was no sharing and reuse of the work those done by human beings.
Well, once you have a treebank,
it's reusable for all sorts of purposes that lots of people build parsers format.
But also other people use it as well like linguists now often used
tree banks to find examples of different constructions.
Um, but beyond that,
this sort of just became necessary once we wanted to do machine learning.
So that if we want to do machine learning,
we want to have data that we can build models on.
In particular, a lot of what
our machine learning models exploit is how common are different structures.
So, we want to know about the commoners and the frequency of things.
Um, but then treebanks gave us another big thing which is,
well, lots of sentences are ambiguous,
and what we want to do is build models that find the right structure for sentences.
If all you do is have a grammar you have no way of
telling what is the right structure for ambiguous sentences.
All you can do is say hey that sentence with
four prepositional phrases after it that I showed you earlier,
it has 14 different parsers.
Let me show you all of them.
Um, but once you have um,
treebank examples, you can say this is the right structure for this sentence in context.
So, you should be building a machine learning model which will recover that structure,
and if you don't that you're wrong.
[NOISE]. Okay. Um, so that's treebanks.
Um, so how are we gonna do build dependency parsers?
Well, somehow we want models that can kind of capture what's the right parse.
Just thinking about abstractly, you know,
there's sort of different things that we can pay attention to.
So, one thing that we can pay attention to is the sort of actual words, right?
Discussion of issues.
That's a reasonable thing.
So, it's reasonable to have issues as dependent of discussion um,
where you know, discussion of outstanding.
That sounds weird.
So, you probably don't want that dependency.
Um, there's a question of how far apart words are.
Most dependencies are fairly short distance.
They not all of them are.
There's a question of what's in between.
Um, if there's a semicolon in between,
there probably is an a dependency across that.
Um, and the other issue is sort of how many arguments do things take?
So, here we have was completed.
If you see the words was completed,
you sort of expect that there'll be a subject before of the something was completed,
and it would be wrong if there wasn't.
So, you're expecting an argument on that side.
But on the other side, hand it won't have object after it.
You won't say the discussion was completed the goat.
Um, that's not a good sentence, right?
So, you won't have ah, um, an object after it.
So, there's sort of information of that sort,
and we want to have our dependency parsers be able to make use of that structure.
[NOISE] Okay.
Um, so effectively what we do when we build a dependency parser is going to say,
for each word is- is going to be the dependent of some other word or the root.
So, this give here is actually the head of the sentence.
So, it's a dependent of root,
the talk is a dependent of give,
'll is a dependent of talk.
And so, for each word we want to choose what is
the dependent of and we want to do it in such a way that the dependencies form a tree.
So that means it would be a bad idea if we made a cycle.
So, if we sort of said, Bootstrapping, um,
was a dependent of, um, talk,
um, but then we had things sort of move around.
So,this goes to here,
but then talk is a dependent that,
and so I'm gonna cycle that's bad news,
we don't want cycles, we want a tree.
And there's one final issue,
um, which is we don't want things that,
um, is whether we want to allow dependencies to cross or not,
um, and this is an example of this.
So, most of the time, um,
dependencies don't cross each other.
Uh, but sometimes they do,
and this example here is actually an instance for that.
So, I'll give a talk tomorrow, um, on bootstrapping.
So, we're giving a talk that's the object,
and when it's being given is tomorrow,
but this talk has a modifier that's on bootstrapping.
So, we actually have another dependency here that crosses, um, that dependency.
And that's sort of rare,
that doesn't happen a ton in English,
but it happens sometimes in some structures like that.
And so, this is the question of whether, um,
what we say is that the positive sentence is projective if there
no crossing dependencies and it's non-projective if there are crossing dependencies,
and most of the time, English's projective and it's
parses of sentences, but occasionally not.
And when it's not is when you kind of have
these constituents that are delayed to the end of the sentence, right?
You could've said, I'll give a talk on bootstrapping tomorrow,
and then a [inaudible] have a projective parse, but if you want to,
you can kind of delay that extra modifier and say I'll give a talk
tomorrow on bootstrapping and then the parse becomes non-projective.
Um, okay.
So, that's that.
Um, there are various ways of,
um, doing dependency parsing,
but basically what I am gonna tell you about today is this one called
transition-based or deterministic dependency parsing,
and this is, um,
the one that's just been enormously influential in practical deployments of parsing.
So, when Google goes off and parses every web page,
what they're using is a transition based parser.
Um, and so, this was a notion of parsing that, um,
was mainly popularized by this guy,
walk him Joakim Nivre, he is a Swedish computational linguists.
Um, and what you do it's- it's sort of inspired by shift-reduce parsing.
So, probably in- in our CS103 or compilers class or something,
you saw a little bit of shift-reduce parsing.
And this is sort of like a shift-reduce parser,
apart from when we reduce,
we build dependencies instead of constituent.
Um, and this has a lot of very technical description that
doesn't help you at all to look at in terms of understanding what,
um, a shift-reduce parser does.
And here's a formal description of a
transition-based shift-reduce parser and which also doesn't help you at all.
Um, so, instead we kinda look at this example,
uh, [LAUGHTER] because that will hopefully help you.
So, what I wanna to do is parse the sentence "I ate fish".
And yet formally what I have is I have a why I start,
there are three actions I can take and I have
a finished condition for formal parse, parse.
Um, and so here's what I do.
So, I have a stack which is on this side and I have a buffer.
Um, so, the stack is what I have built,
and the buffer is all the words in the sentence I haven't dealt with yet.
So, I stop the parse,
and that's the sort of instruction here, by putting route,
my root for my whole sentence onto my stack,
and my buffer is the whole sentence,
and I haven't found any dependencies yet.
Okay, and so then,
the actions I can take is to shift things onto the stack
or to do the equivalent of a Reduce where I build dependencies.
So, starting off, um,
I can't build a dependency because I only have root on the stack,
so the only thing I can do is shift,
so I can shift I onto the stack.
Um, now, I could at this point say,
let's build a dependency,
I is a dependent of root,
but that would be the wrong analysis,
because really the head of this sentence is I ate.
So, I'm a clever boy and I shift again.
And now I have root I ate on the stack.
Okay, and so, at this point,
I'm in a position where,
hey, what I'm gonna do is reductions that build structure, because look,
I have I ate here and I want to be able to say
that I is the subject of dependency of ate,
and I will do that by,
um, by doing a reduction.
And so, what I'm gonna do is the left-arc reduction, which says, look,
I'm gonna treat the second from top thing on the stack
as a dependent of the thing that's on top of the stack.
And so, I do that,
and so, when I do that,
I create the second from the head thing as a subject dependent of ate,
and I leave the head on the stack ate,
but I sort of add this dependencies as other dependencies I've built.
Okay, um, so, I do that.
Um, now, I could immediately reduce again and say ate is a dependent of root,
but my sentence's actually I ate fish.
So, what I want to do is say, "Oh,
if it's still fish on the buffer," so what I should first do is shift again,
have root ate fish in my sentence,
and then I'll be able to say, Look,
I want to now build, um,
the thing on the top of this stack as
a right dependent of the thing that's second from top of the stack,
and so that's referred to as a Right-Arc move,
and so, I say Right Arc, and so,
I do a reduction where I've generated
a new dependency and I take the two things that are on top of the stack and say,
um, fish is a dependent of ate,
and so therefore, I just keep the head.
I always just keep the hit on the stack and the- and I generate this new Arc.
And so, at this point,
I'm in the same position I want to say that this ate is a right dependent of my route,
and so, I'm again going to do Right Arc,
um, and make this extra dependency here.
Okay. So, then my finished condition of having
successfully parsed the sentence is my buffer is
empty and I just have root left on my stack because that's what I sort of said back here,
that was, buffer is empty as my finished condition.
Okay. So, I've parsed the sentence.
So that worked well but, you know,
I actually had different choices of when to pa- when to shift and when to reduce.
And I just miraculously made the right choice at each point.
And well, one thing you could do at this point is say, well,
you could have explored every choice and,
um, seen what happened and gone different parsers.
And I could have,
but if that's what I'd done,
I would've explored this exponential size tree of different possible parsers.
And if that was what I was doing,
I wouldn't be able to parse efficiently.
And indeed that's not what people did in the 60s, 70s and 80s.
Uh, clever people in the 60s said,
uh, rather than doing a crummy search here,
we can come up with clever dynamic programming algorithms and you
can relatively efficiently explore the space of all possible parsers.
Uh, and that was sort of the mainstay of parsing in those decades.
But when Joakim Nivre came along,
he said "Yeah, that's true, um, but hey,
I've got a clever idea, uh,
because now it's the 2000s and I know machine learning."
Um, so, what I could do instead,
is say I'm at a particular position in the parse and I'm gonna build
a machine learning classifier and that machine learning
classifier is gonna tell me the next thing to do.
It's gonna tell me whether to shift,
um, with left arc or right arc.
So, if we're only just so talking about, well,
how to build the arrows,
they're just three actions,
shift, left arc or right arc.
Um, if we also wanted to put labels on the dependencies,
and we have our different labels, um,
there are then sort of 2R plus actions because she is
sort of left arc subject or left arc object or something like that.
But anyway, there's a set of actions and so you gonna build
a classifier with machine learning somehow which will predict
the right action and Joakim Nivre showed the sort of slightly surprising fact
that actually you could predict the correct action to take with high accuracy.
So, um, in the simplest version of this,
um, there's absolutely no search.
You just run a classifier at each step and it
says "What you should do next is shift" and you shift,
and then it says "What you should do is left arc" and you left arc
and you run that through and he proved, no,
he showed empirically, that even doing that,
you could parse sentences with high accuracy.
Now if you wanna do some searching around,
you can do a bit better,
but it's not necessary.
Um, and we're not gonna do it for our, um, assignment.
But so if you're doing this just sort of run classify,
predict action, run classify, predict action,
we then get this wonderful result which
you're meant to explain a bit honest on your assignment 3,
is that what we've built is a linear time parser.
Right? That because we are gonna be sort of- as we chug through a sentence,
where we're only doing a linear amount of work for
each word and that was sort of an enormous breakthrough.
Because although people in the 60s hadn't come
up with these dynamic programming algorithms,
dynamic programming algorithms for sentences were always cubic or worse.
And that's not very good if you want to parse the whole web,
whereas if you have something that's linear time,
that's really getting you places.
Okay. So this is the conventional way in which this was done.
Was, you know, we have a stack,
we might have already built some structure if we
hadn't working out something's dependent of something.
We have a buffer of words that we don't deal with and we want to predict the next action.
So the conventional way to do this is to say well,
we want to have features.
And well, the kind of features you wanted was so
the usually some kind of conjunction or multiple things so
that if the top word of the stack is good,
um, and something else is true, right,
that the second top word of the stack it has,
and it's part of speech is verb,
then maybe that's an indicator of do some action.
So ha- had these very complex binary indicator features
and you'd build- you literally have millions of
these binary indicator features and you'd feed them into
some big logistic regression or
support vector machine or something like that and you would build parses.
And these parses worked pretty well.
Um, but you sort of had these sort of very complex hand engineered binary features.
Um, so in the last bit of lecture I want to show you what people have done in the,
um, neural dependency parsing world.
But before I do that,
let me just explain how you,
um, how you evaluate, um, dependency parses.
And that's actually very simple, right?
So, what you do is well,
you assume because the human wrote it down,
that there is a correct dependency parse for a sentence.
She saw the video lecture like this.
And so these are the correct arcs and to evaluate our dependency parser,
we're simply gonna say,
uh, which arcs are correct.
So, there are the gold arcs,
so there's a gold arc,
um, from two to one,
She saw subject, and there's a gold arc from zero to two,
the root of the sentence,
these the gold arcs.
Um, if we generate a parse,
we're gonna propose some arcs as to what is the head of each word.
And we're simply going to count up how many of them are correct,
treating each arc individually.
And there are two ways we can do that.
We can either, as we're going to do,
ignore the labels and that's then,
uh, referred to as the unlabeled attachment score.
So here in my example, my dependency paths,
I've got most of the arcs right but it got this one wrong.
So I say my unlabeled attachment score is 80 percent or we can also
look at the labels and then my parser wasn't very good at getting the labels rights,
so I'm only getting 40 percent.
And so we can just count up the number of dependencies and how many we get correct.
And that's in our accuracy and in the assignment,
you're meant to build a dependency parser with a certain accuracy.
I forget the number now is saying,
some number 80 something or something that you're meant to get to.
Okay. Um, maybe I'll skip that.
Okay. Um, so, now I wanted to sort of explain to you just a bit
about neural dependency parses and why they are motivated.
So I'd mentioned to you already that the conventional model, uh,
had these sort of indicated features of, um,
on the top of the stack is the word good and the second thing on
the stack is the verb has or on
the top of the stack is some other word and the second top is of some part of speech.
And that part of speech has already been
joined with the dependency of another part of speech.
People hand-engineer these features.
And the problems with that,
was these features were very sparse.
Each of these features matches very few things.
Um, they match some configurations but not others so the features tend to be incomplete.
Um, and there are a lot of them,
they're are commonly millions of features.
And so it turned out that actually computing
these features was just expensive so that you had some configuration on
your stack and the buffer and then you wanted to know which of
these features were active for that stack and buffer configuration.
And so you had to compute features format.
And it turned out that
conventional dependency parsers spent most of their time computing features,
then went into the machine learning model rather than doing the sort of shifting and,
which you're are seeing, are just a pure parser operation.
And so that seemed like it left open the possibility that, well,
what if we could get rid of all of this stuff and we could run
a neural network directly on the stack and buffer configuration,
then maybe that would allow us to build a dependency parser which was
faster and suffer less from issues of sparseness than the conventional dependency parser.
And so that was a project that Dan Chi Chen and me tried to do in 2014,
uh, we used to build a neural dependency parser.
And, you know, effectively what we found,
is that that's exactly what you could do.
So, here's sort of a few stats here.
So these are these same UAS and LAS.
Uh, so MaltParser was Joakim Nivre's Parser that I sort of,
uh, we started showing before.
And they've got, um,
a UAS on this data of 89.8.
But everybody loved that.
And the reason they loved it is it could parse at 469 sentences a second.
There had been other people that have worked out
different more complex ways
of doing parsing with so-called graph-based dependency parsers.
So this is another famous dependency parser from the 90s.
So it was actually, you know,
a bit more accurate but it was a bit more
accurate at the cost of being two orders of magnitude slower.
And, you know, people have worked on top of that.
So, here is an even more complex graph-based parser, uh,
from the 2000s and well, you know,
it's a little bit more accurate again but it's gotten even slower.
Um, okay.
So, what we were able to show is that using the idea of instead using
a neural network to make the decisions of Joakim Nivre Style shift-reduce parser,
we could produce something that was almost
as accurate as the very best parsers available at that time.
I mean, strictly we won over here and we are a fraction behind on UAS.
Um, but, you know,
it was not only just as fast as Nivre's parser,
it was actually faster than Nivre's parser,
because we didn't have to spend as much time on feature computation.
And that's actually almost a surprising result, right?
It's not that we didn't have to do anything.
We had to do matrix multiplies in our neural network,
but it turned out, um,
you could do the matrix multiplies more quickly than
the feature computation that he was doing even though at the end of the day,
it was sort of looking at weights that went into a support vector machine.
So that was kind of cool.
And so the secret was we're gonna make use of
distributed representations like we've already seen for words.
So for each word,
we're going to represent it as a word embedding,
like we've all what already seen.
And in particular, um,
we are gonna make use of word vectors
and use them as the represent- the starting representations of words in our Parser.
But well, if we're interested in distributed representations,
it seem to us like maybe you should only have distributed representations of words.
Um, maybe it also be good temp distributed representations of other things.
So we had parts of speech like,
you know, nouns and verbs and adjectives and so on.
Well some of those parts of speech have more to do with each other than others.
I mean, [NOISE] in particular, um,
most NLP work uses fine-grained parts of speech.
So you don't only have a part of speech like noun or verb,
you have parts of speech like singular noun versus
plural noun and you have different parts of speech for, you know,
work, works, working, kind of the different forms of
verbs are given different parts of speech, um, as well.
So there's sort of sets of parts of speech labels that kind of clusters.
So maybe we could have distributed representations,
a part of speech that represent their similarity.
Why not? Um, well if we're gonna do that,
why not just keep on going and say the dependency labels.
They also, um, have a distributed representation.
And so, we built a representation that did that.
So the idea is that we have in our stack,
the sort of the top positions of the stack,
the first positions of the buffer and for each of those positions,
we have a word and a part of speech and if we've already built structure as here,
we kind of know about a dependency that's already been built.
And so we've got a triple for each position and we're gonna convert
all of those into a distributed representation,
um, which we are learning and we're gonna use those distributed representations,
um, to build our parser.
Okay. Now for- so,
you know starting from- starting from the next lecture forward,
we're gonna sort of s- start using a more complex forms of neural models.
But for this model, um,
we did it in a sort of a very simple straightforward way.
We said, well, we could just use exactly the same model,
exactly the same parser structure that Nivre used, right?
Doing those shifts and left arcs and right arcs.
Um, the only part we're gonna turn into
a neural network is we're gonna have the decision of what to do next,
um, being controlled by our neural network.
So our neural network is
just a very simple classifier of the kind that we are talking about last week.
So based on the configuration,
we create an input layer which means we're sort
of taking the stuff in these boxers and turn- and looking up
a vector representation for each one and concatenating them together to produce
a input representation that's sort of similar to when we were making
those window classifiers and then we can concatenate a bunch of stuff together.
So that gives us in our input layer.
[NOISE] Um, so from there,
we put things through a hidden layer just like last week.
We do Wx plus b and then put it through a ReLU or a non-linearity to a hidden layer.
And then on top of that,
we're simply gonna stick a softmax output layer.
So multiplying by another matrix,
adding another, um, bias term,
and then that goes into the softmax which is gonna give
a probability over our actions as to whether it's shift left arc or right arc,
or the corresponding one with labels.
And then we're gonna use the same kind of cross entropy loss to say how good a job did we
do at guessing the action that we should have
taken according to the tree bank parse of the sentence.
And so each step of the shift-reduce parser,
we're making a decision as what to do next and we're doing it by this classifier
and we're getting a loss to
the extent that we don't give probability one to the right action.
Um, and so that's what we did using the tree bank.
We trained up our parser, um,
and it was then able to predict the sentences.
And the cool thing- the cool thing was,
um, that this, um,
had all the good things of Nivre's parser but, you know,
by having it use these dense representations,
it meant that we could get greater accuracy and
speed than Nivre's parser at the same time.
So here is sort of some results on that.
I mean, I already showed you some earlier results, right?
So this was showing, um, the fact, um,
that, you know, we're outperforming these earlier parsers basically.
But subsequent to us doing this work,
um, people at Google,
um, these papers here by Weiss and Andor,
um, they said, "Well, this is pretty cool.
Um, maybe we can get the numbers even better if we make our neural network,
um, bigger and deeper and we spend a lot more time tuning our hyper-parameters."
Um, sad but true.
All of these things help when you're building
neural networks and when you're doing your final project.
Sometimes the answer to making the results better is to make it bigger,
deeper and spend more time choosing the hyper-parameters.
Um, they put in Beam search as I sort of mentioned.
Um, Beam search can really help.
So in Beam search,
you know, rather than just saying,
"Let's work out what's the best next action,
do that one and repeat over",
you allow yourself to do a little bit of search.
You sort of say, "Well, let's consider two actions and explore what happens."
Um, quick question.
Do humans always agree on how to build this trees and if they don't,
what will be the [inaudible] or agreement of humans relative to [inaudible] [OVERLAPPING] [NOISE]
So that's a good question which I haven't addressed.
Um, humans don't always agree.
There are sort of two reasons they can't agree fundamentally.
One is that, uh, humans,
um, sort of mess up, right?
Because human work is doing this aren't perfect.
And the other one is they generally think that there should be different structures.
Um, so, you know,
it depend- varies depending on the circumstances and so on.
If you just get humans to parse sentences and say,
"Well, what is the agreement and what they produced?"
You know, maybe you're only getting something like 92 percent.
But, you know, if you then do an adjudication phase and you say, "Um,
look at these differences,
um, is one of them right or wrong?"
There are a lot of them where, you know,
one of the person is effectively saying,
"Oh yeah, I goofed.
Um, wasn't paying attention or whatever."
Um, and so then,
what's the residual rate in which,
um, people can actually disagree about possible parses?
I think that's sort of more around three percent.
Um, yeah.
But there certainly are cases and that includes
some of the prepositional phrase attachment ambiguities.
Sometimes there are multiple attachments
that sort of same clause although it's not really
clear which one is right even though there are lots of
other circumstances where one of them is very clearly wrong.
Um, yeah.
[inaudible].
There's- there's still room to do better.
I mean, at the unlabeled attachment score,
it's actually starting to get pretty good.
But there's still room to do better. Um, yeah.
Um, yeah.
So Beam search,
the final thing that they did was- that we're not gonna talk about here,
is the sort of more global inference to make sure, um, it's sensible.
Um, and so, um,
that then led to Google developing these models that they gave silly names to,
especially the Parsey McPa- parseFace,
um, model of parsing.
Um, and so, yeah.
So that then- that's sort of pushed up the numbers even further so that they were sort of
getting close to 95 percent unlabeled accuracy score from these models.
And actually, this work has kind of,
you know, deep learning people like to optimize.
Um, this work [LAUGHTER] has continued along
in the intervening two years and the numbers are sort of getting,
um, a bit higher again.
But, you know, so this actually, um,
led to ah sort of a new era of sort of better parsers because so effectively this was the
90's- the 90's era of parsers that was sort of where
around 90 percent and then going into this sort of new generation of,
um, neural transition based dependency parsers.
We sort of have gone down that we've halve that error- error rate.
And we're now down to sort of about a five percent error rate.
Yeah. I'm basically out of time now but, you know,
there is further work including, you know, at Stanford.
Um, another student, Tim Dossad has some sort of more recent work.
It's more accurate than 95 percent, right?
So we- we're still going on but I think I'd better stop here today,
um, and that's neural dependency parsing. [NOISE]
 Hi, everyone. I'm Abby,
I'm the head TA for this class
and I'm also a PhD student in the Stanford NLP group.
And today I'm gonna be telling you about
language models and recurrent neural networks.
So, here's an overview of what we're gonna do today.
Today, first, we're going to introduce a new NLP task, that's language modelling,
and that's going to motivate us to learn about a new family of neural networks,
that is recurrent neural networks or RNNs.
So, I'd say that these are two of
the most important ideas you're going to learn for the rest of the course.
So, we're going to be covering some fairly cool material today.
So, let's start off with language modeling.
Language modeling is the task of predicting what word comes next.
So, given this piece of text the students opens their blank,
could anyone shout out a word which you think might be coming next?
Purpose. [NOISE].
[OVERLAPPING] Mind, what else? I didn't quite hear them,
but, uh, yeah, these are all likely things, right?
So, these are some things which I thought,
students might be opening, uh,
students open their books, seems likely.
Uh, students open their laptops,
students open their exams,
Students open their minds, incredibly,
someone came up with one, that one just now,
uh, it's kind of a metaphorical meaning of opening.
So, you are all performing language modeling right now.
And thinking about what word comes next,
you are being a language model.
So, here's a more formal definition of what a language model is.
Given a sequence of words X1 up to Xt,
a language model, is something that computes
the probability distribution of the next word, Xt plus 1.
So, a language model comes up with the probability distribution,
the conditional probability, of what X t plus 1 is given the words it found.
And here we're assuming that, Xt plus 1
can be any word w from a fixed vocabulary V.
So we are assuming that there is
a pre-defined list of words that we're considering.
In this way, you can view language modeling
as a type of classification task,
because there's a predefined number of possibilities.
Um, we call a system that does this a language model.
There's an alternative way of thinking
about a language model as well.
You can think of a language model
as a system which assigns probability to a piece of text.
So, for example, if we have some piece of text,
X up to X capital T,
then, the probability of this text
according to the language model can be broken down.
So, just by definition,
you can say that the probability is equal to,
the product of all of these conditional probabilities.
And, uh, the form inside,
the products is exactly what a language model provides.
So, you can think of these things as somewhat equivalent.
Predicting next words, gives you a system,
that can give the probability of a given piece of text.
So, in fact, you, use language models every day.
For example, when you're texting on your phone and you're writing a message,
then most likely if you have a smartphone,
it will be predicting what word you might be about to say.
So, if you say, um, I'll meet you at the-
your phone might suggest perhaps you mean airport or cafe,
or office, for example.
Another situation which you use language models every day
is when you search for something on the internet, for example, Google,
and you start typing your query,
then Google tries to complete your query for you, and that's language modeling.
It's predicting what word or words might come next.
So, that's what a language model is,
and the question is, how would you learn a language model?
So, if I was to ask that question in the pre- deep learning era,
which was really only a few years ago,
the answer would be, you would learn a n-gram language model.
So, today first we're going to learn about n-gram language models.
So, before I can tell you what a n-gram language model is,
you need to know what an n-gram is.
So, by definition an n-gram is a chunk of n consecutive words.
So, for example, a one gram or unigram,
is just all of the individual words
in the sequence that would be "the students open the-"
A two gram or bigram would be all of the consecutive chunks of pairs of words,
"the students", "students opened", "opened their"
and so on for trigrams and four-grams, etc.
So, the core idea of an n-gram language model
is that in order to predict what word comes next,
you're going to collect a bunch of statistics,
about how frequent different n-grams are,
from some kind of training data,
and then you can use those statistics
to predict what next words might be likely.
Here is some more detail.
So, to make an n-gram language model,
first you need to make a simplifying assumption,
and this your assumption.
You say that the next word Xt plus 1
depends only on the preceding N-1 words.
So, what we're assuming,
is that the probability distribution,
the conditional probability of Xt plus 1 given all of the words they follow,
we're just going to simplify that,
and say it only depends on the last N-1 words, and that's our assumption.
So, by the definition of conditional probability,
we can say that this probability,
is just the ratio of two different probabilities.
So, on the top, you've got the probability of
a particular n-gram and on the bottom we've
got the probability of a particular N-1 gram
This is a little hard to read because of all the superscripts
but I'm gonna give an example with words on the next slide.
Okay. So, that's the definition of the probability of the next word,
but the question remains, how do we get all of
these n-gram and N-1 gram probabilities?
So, the answer is, we're going to get them by
counting them in some large corpus of text.
So, we're going to approximate,
these probabilities just by the count of the number of times that
these particular n-grams and N-1 grams appeared in our training corpus.
Okay. So, here's an example with some words.
Suppose we are trying to learn a 4-gram language model,
and suppose that we have a piece of text, that says,
"As the proctor started the clock,
the students opened their blank",
and we're trying to predict what word is coming next.
So, because we're learning a 4-gram language model,
a simplifying assumption is that the next word depends only on the last three words,
last N-1 words.
So, we're going to discard all of the context so far except for the last few words,
which is, "Students opened their."
So, as a reminder, n-gram language model says that,
the probability of the next word being,
some particular word W in the vocabulary is equal to the number of times we saw
students opened their W divided by the number of
times we saw students opened their, in the training corpus.
So, let's suppose that in our training corpus,
we saw the phrase "students open their" 1,000 times.
And suppose that, we saw "students opened their books" 400 times.
This means that the probability of the next word being books is 0.4.
And uh, similarly, let's suppose that we saw students open their exams 100 times,
this means that the probability of exams given students
open their is 0.1. Is there a question?
[inaudible].
The question is, does the order of the words matter?
And the answer is yes, the order of students open there does matter.
It's different to "the students opened."
So, the question I want to raise now is,
was it a good idea for us to discard the proctor context?
If you look at the actual example that we had,
the example was as the proctor started the clock,
the students opened their blank.
So, do we think that books or exams is more likely given the actual context,
the full context? Yep.
Exams.
Right. Exams is more likely because the proctor and
the clock heavily implies that it's an exam scenario, so
they're more likely to be opening the exams than the books,
unless it's an open book exam.
Uh, but I think, overall, it should be exams.
So, the problem that we're seeing here is that in the training corpus,
the fact that students were opening
something means that it's more likely to be books than exams
because overall, books are more common than exams.
But if we know that the context is,
the proctor and the clock, then it should be exams.
So, what I'm highlighting here is a problem with our simplifying assumption.
If we throw away too much context,
then we are not as good as predicting the words as we would be if we kept the context.
Okay. So, that's one problem with n-gram, uh, language models.
Uh, there are some other problems as well.
So, uh, here again is the equation that you saw before.
One problem which we're gonna call
the sparsity problem is what happens if the number on top,
the numerator, what if that count is equal to zero.
So, what if for some particular word W,
the phrase students opened their W never occurred in the data.
So, for example, let's suppose students opened their petri dishes,
is fairly uncommon and it never appears in the data,
then that means our probability of the next word being petri dishes will be zero.
And this is bad, because it might be uncommon but it is,
a valid scenario, right?
If you're a biology student for example.
So, this is a problem and we call it the sparsity problem,
because the problem is that if we'd never seen an event happen in the training data,
then our model assigns zero probability to that event.
So, one partial solution to this problem is that maybe we should add a small delta,
small number delta to the count,
for every word in the vocabulary.
And then this way, every possible word that come next,
has at least some small probability.
So, petri dishes will have some small probability,
but then so, will all of the other words which are possibly bad choices.
So, this, uh, technique is called smoothing, because the idea is,
you're going from a very, uh,
sparse probability distribution, which is zero, almost everywhere,
with a few spikes where there's,
uh, being n-grams that we've seen,
it goes from that to being a more smooth probability distribution
where everything has at least a small probability on it.
So, the second sparsity problem which is possibly worse than the first one is,
what happens if the number in the denominator is zero?
So, in our example, that would mean,
what if we never even saw the trigram "students opened their" in the training data.
If that happens, then we can't even calculate this probability distribution at
all for any word W because we never even saw this context before.
So, a possible solution to this is that
if you can't find "students open their" in the corpus,
then you should back off to just conditioning on the last two words,
rather than the last three words.
So, now you'd be looking at times when you'd seen,
uh, "open their" and seeing what what's come next.
So, this is called back-off because in this failure case,
for when you have no data for your 4-gram language model,
you're backing off to a trigram language model.
Are there any questions at this point?
Okay. So, um, another thing to note is that these sparsity problems
get worse if you increase N. If you make N larger in your n-gram language model,
and you might want to do this, for example,
you might think, uh, I want to have a larger context,
so I can pay attention to words that
happened longer ago and that's gonna make it a better predictor.
So, you might think making N bigger is a good idea.
But the problem is if you do that then the sparsity problems get worse.
Because, let's suppose you say,
I want a 10-gram language model.
Then the problem is that you're going to be counting,
how often you seen process in 9-grams and 10-grams.
But 9-grams and 10-grams, there's so many of them,
that the one you are interested in probably never occurred,
in your training data which means that the whole thing becomes dysfunctional.
So, in practice, we usually can't have N much bigger than five.
Okay. So, that was, uh,
two sparsity problems with n-gram language models.
Here is a problem with storage.
So, if we look at this equation, uh,
you have to think about what do you need to
store in order to use your n-gram language model.
You need to store this count number,
for all of the n-grams that you observed in
the corpus when you were going through the training corpus counting them.
And the problem is, that as you increase N,
then this number of n-grams that you have to store and count increases.
So, another problem with increasing N is that the size of your model,
or your n-gram model, uh, gets bigger.
Okay, so n-gram Language Models in practice. Let's look at an example.
You can actually build a simple trigram Language Model over a 1.7 million word corpus,
uh, in a few seconds on your laptop.
And in fact, the corpus that I used to do this
was the same one that you met in assignment one.
It's Reuters' corpus which is,
uh, business and financial news.
So, if you want to do this yourself,
you can follow that link at the bottom of the slide later.
So, uh, this is, uh,
something which I ran on my laptop in a few second.
So I gave it the context of the bigram today the,
and then I asked the trigram Language Model what word is likely to come next.
So, the Language Model said that the top next most likely words are
company, bank, price, Italian, emirate, et cetera.
So already just looking at these probabilities that are assigned to these different words,
uh, you can see that there is a sparsity problem.
For example, the top two most likely words have
the exact same probability and the reason for that is,
that this number is 4 over 26.
So these are quite small integers, uh,
meaning that we only saw, uh,
today the company and today the bank four times each.
So, uh, this is an example of
the sparsity problem because overall these are quite low counts,
we haven't seen that many different, uh,
versions of this event,
so we don't have a very granular probability distribution.
But in any case ignoring the sparsity problem,
I would say that overall,
these, uh, top suggestions look pretty reasonable.
So you can actually use a Language Model to
generate text and this is how you would do it.
So let's suppose you have your first two words already, uh,
you condition on this and you ask your Language Model what's likely to come next.
So then given this probability distribution over the words,
you can sample from it, that is,
select some words with, you know, the associated probability.
So let's suppose that gives us the word price.
So then price is your next word, and then you just condition on the last two words,
which in this ex- example is now the price.
So now you get a new probability distribution and you can continue this process,
uh, sampling and then conditioning again and sampling.
So if you do this long enough,
you will get a piece of text,
so this is the actual text that I got when
I run this generation process with this trigram Language Model.
So it says, "Today the price of gold per ton,
while production of shoe lasts and shoe industry,
the bank intervened just after it considered and rejected
an IMF demand to rebuild depleted European stocks,
September, 30th end primary 76 counts a share.''
Okay. So, uh, what do we think about this text?
We think it's good? We, uh, surprised?
Um, I would say that in some ways it is good,
it's kind of surprisingly grammatical, you know,
it mostly, uh, kind of pauses,
uh, but you would definitely say that it,
it doesn't really make any sense.
It's pretty incoherent.
And we shouldn't be surprised that it's incoherent I
think because if you remember this is a trigram Language Model,
it has a memory of just the last well,
three or two words depending on how you look at it.
So clearly we need to consider
more than three words at a time if we want to model language well.
But as we already know, increasing n makes the sparsity problem worse,
n-gram Language Models, and it also increases model size. Is that a question?
How does it [inaudible] [NOISE]
So the question is, how does the n-gram Language Model know when to put commas.
Uh, so you can,
[NOISE] decide that commas and other punctuation are just another kind of word,
is that well or token,
and then, to the Language Model it doesn't really make much difference.
It's just used that as another possible world that can be, um, predicted,
that's why we've got the weird spacing around the,
the commas is because it was essentially viewed as a separate word.
[NOISE] Okay.
So this course is called NLP with Deep Learning.
So you probably thinking how do we build a neural Language Model?
So let's just recap, uh, in case you forgot.
Remember that a Language Model is something that takes
inputs which is a sequence of words X1 up to Xt,
and then it outputs a probability distribution of what the next word might be Xt plus 1.
Okay, so when we think about what kind of neural models we've met in this course so far.
Uh, we've already met window-based neural models.
And in lecture three, we saw how you could apply
a window-based neural model to a named entity recognition.
So in that scenario you take some kind of window around the word that you
care about which in this example is Paris, and then, uh,
you get the word embeddings for those, concatenate them put them through
some layers, and then you get your decision which is that Paris is a location not,
you know, a person or organization.
So that's a recap of what we saw in lecture three.
How would we apply a model like this to language modeling? So here's how you would do it.
Here's an example of a fixed-window neural language model.
So, again, we have some kind of context
which is, as the proctor started the clock the students opened their,
um, we're trying to guess what word might come next.
So we have to make a similar simplifying assumption to before.
Uh, because it's a fixed size window, uh,
we have to discard the context except for the window that we're conditioning on.
So let's suppose that our fixed window is of size four.
So what we'll do is similarly to the, ah, NER model.
We're going to represent these words with one-hot vectors,
and then we'll use those to look up the word embeddings for these words using the,
uh, embedding lookup matrix.
So then we get all of our word embeddings E,1, 2, 3, 4,
and then we concatenate them together to get e. We put this through
a linear layer and a nonlinearity function f to get some kind of hidden layer,
and then we put it through another linear layer and
the softmax function and now we have an output probability distribution y hat.
And in our case because we're trying to predict what word comes next, ah, ah,
vector y hat will be of length v where v is
the vocabulary and it will contain
the probabilities of all the different words in the vocabulary.
So here I've represented that as a bar charts where if you suppose
you've got all of the words listed alphabetically from a to z,
and then there's the different probabilities of the words.
So if everything goes well,
then this language model should tell us that
some likely next words are books and laptops, for example.
So none of this should be, um,
unfamiliar to you because you saw it all last week.
We're just applying a Window-based model to a different task, such as language modeling.
Okay, so what are,
some good things about this model compared to n-gram language models?
So one, ah, advantage I'd say is that there's no sparsity problem.
If you remember an n-gram language model has a sparsity problem
which is that if you've never seen a particular n-gram in training then,
you can't assign any probability to it.
You don't have any data on it.
Whereas at least here you can take any, you know, for example,
4-gram you want and you can feed it into the, ah,
the neural nets and it will give you
an output distribution of what it thinks the next word would be.
It might not be a good prediction but at least it will, it will run.
Another advantage is you don't need to store
all of the observed n-grams that you ever saw.
So, uh, this an advantage by, uh,
comparison you just have to store
all of the word vectors for all the words in your vocabulary.
Uh, but there are quite a lot of problems with this fixed-window language model.
So here are some remaining problems: Uh,
one is that your fixed window is probably too small.
No matter how big you make your fixed window, uh,
you're probably going to be losing some kind of
useful context that you would want to use sometimes.
And in fact, if you try to enlarge the window size,
then you also have to enlarge the size of your,
uh, weight factor, sorry,
your weight matrix W. Uh,
so the width of W because you're multiplying it
by e which is the concatenation of your word embeddings.
The width of W grows as you increase the size of your window.
So in inclusion really your window can never be large enough.
Another problem with this model which is more of a subtle point is that
X1 and X2 and really all of the words in the window they're,
uh, multiplied by completely diffe rent weights in
W. So to demonstrate this you could draw a picture.
So the problem is that if you have
your weight matrix W and then you have
your concatenation of embeddings e and we have, uh, four embeddings.
So we have e_1, e_2, e_3,
e_4, and you multiply, uh,
the concatenated embeddings by the weight matrix.
So really you can see that there are essentially
kind of four sections of the weight matrix,
and the first word embedding e_1 is only
ever multiplied by the weights for it in this section,
and that's completely separate to the weights that multiply by e_2 and so forth.
So the problem with this is that what you
learn in the weight matrix in one section is not shared with the others.
You're kind of learning a lot of similar functions four times.
So the reason why we think this is a problem is because there should be a lot of
commonalities in how you process the incoming word embeddings.
So what you learn about how to process, you know,
the third embedding, some of it at least should be shared with all of the embeddings.
So what I'm saying is it's kind of inefficient that we're learning, uh,
all of these separate weights for these different words
when there's a lot of commonalities between them. Is there a question?
So that's why [inaudible] [NOISE].
Okay-
Yeah, hopefully- hopefully the verbal description is on.
So, in conclusion, I'd say that the biggest problem that we've got with
this fixed-size neural model is that clearly we
need some kind of neural architecture that can process any length input,
because most of the problems here come from the fact that we had to make
this simplifying assumption that there was a fixed window.
Okay. So this motivates, uh,
us to introduce this new family of neural architecture,
it's called recurrent neural networks or RNNs.
So, this is a simplified diagram that shows you the most important,
um, features of an RNN.
So we have again an input sequence of X1, X2,
et cetera, but you can assume that this sequence is of any arbitrary length you like.
The idea is that you have a sequence of hidden states instead of just having,
for example, one hidden state as we did in the previous model.
We have a sequence of hidden states and we have as many of them as we have inputs.
And the important thing is that each hidden state ht is computed based
on the previous hidden state and also the input on that step.
So the reason why they're called hidden states is because you could think of
this as a single state that's mutating over time.
It's kind of like several versions of the same thing.
And for this reason, we often call these time-steps, right?
So these steps that go left to right,
we often call them time-steps.
So the really important thing is that
the same weight matrix W is applied on every time-step of this RNN.
That's what makes us able to process any length input we want.
Is because we don't have to have different weights on every step,
because we just apply the exact same transformation on every step.
So additionally, you can also have some outputs from the RNN.
So these y hats,
these are the outputs on each step.
And they're optional because you don't have to compute them
or you can compute them on just some steps and not others.
It depends on where you want to use your RNN to do.
Okay. So that's a simple diagram of an RNN.
Uh, here I'm going to give you a bit more detail.
So here's how you would apply an RNN to do language modeling.
So, uh, again, let's suppose that we have some kind of text so far.
My text is only four words long,
but you can assume that it could be any length, right?
It's just short because we can't fit more on the slide.
So you have some sequence of tags, which could be kind of long.
And again, we're going to represent these by some kind of one-hot vectors and
use those to look up the word embeddings from our embedding matrix.
So then to compute the first hidden state H1,
we need to compute it based on the previous hidden state and the current input.
We already have the current input, that's E1,
but the question is where do we get this first hidden state from?
All right, what comes before H1?
So we often call the initial hidden state H0, uh, yes,
we call the initial hidden state and it can either be something that you learn,
like it's a parameter of the network and you learn how to initialize it,
or you can assume something like maybe it's the zero vector.
So the formula we use to compute the new hidden state based on the previous one,
and also the current inputs is written on the left.
So you do a linear transformation on the previous hidden state and on
the current input and then you add some kind of
bias and then put it through a non-linearity,
like for example, the sigmoid function.
And that gives you a new hidden state.
Okay. So, once you've done that,
then you can compute the next hidden state and you
can keep unrolling the network like this.
And that's, uh, yeah,
that's called unrolling because you're kind of
computing each step given the previous one.
All right. So finally, if you remember,
we're trying to do language modeling.
So we're trying to predict which words should come next after the students opened their.
So on this fourth step over here,
we can use, uh,
the current hidden state, H4,
and put it through a linear layer and put it through a softmax function and then we get
our output distribution Y-hat 4 which is a distribution over the vocabulary.
And again, hopefully, we'll get some kind of
sensible estimates for what the next word might be.
Any questions at this point. Yep?
Is the- the number of hidden state or is it gonna be the number of words in your input?
The question is, is the number of hidden states the number of words in your input?
Yeah, in this setting here, uh, yes,
or you could say more generally the number of hidden states is the number of inputs. Yep.
And just as with the n-gram model,
we could use the output as the input from the tasks mutation in transformational model?
Yeah, so the question is,
as with the n-gram language model,
could we use the output as the input on the next step?
And the answer is yes, and I'll show you that in a minute.
Any other questions? Yeah.
Are you learning the embedding?
The question is, are you learning the embeddings?
Um, that's a choice.
You could have the embeddings be for example,
pre-generated embeddings that you download and you use those and they're frozen,
or maybe you could download them,
but then you could fine-tune them.
That is, allow them to be changed as parameters of
the network or you could initialize them to,
you know, small, uh, random values and learn them from scratch.
Any other questions? Yeah.
So you said you use the same delta matrix,
like you do back propagation,
does that you only update like WE,
or do you update both WH and WE?
So the question is, you say we reuse the matrix, do we update WE and WH, or just one?
So you suddenly learn both WE and WH.
I suppose I was emphasizing WH more, but yeah,
they're both matrices that are applied repeatedly.
There was also a question about back-prop,
but we're going to cover that later in this lecture.
Okay, moving on for now. Um, so,
what are some advantages and disadvantages of this RNN language model?
So here are some advantages that we can see in comparison to the fixed window one.
So an obvious advantage is that this RNN can process any length of input.
Another advantage is that the computation for
step t can in theory use information from many steps back.
So in our motivation example,
which was as the proctor started the clock,
the students opened their.
We think that proctor and maybe clock are
both pretty important hints for what might be coming up next.
So, at least in theory,
the hidden state at the end
can have access to the information from the input from many steps ago.
Another advantage is that the model size doesn't increase for longer inputs.
So, uh, the size of the model is actually fixed.
It's just WH and WE,s
and then also the biases and also the embedding matrix, if you're counting that.
None of those get bigger if you want to apply it to more,
uh, longer inputs because you just apply the same weights repeatedly.
And another advantage is that you have the same weights applied on every time-step.
So I said this thing before about how the fixed-sized window neural model,
it was less efficient because it was applying
different weights of the weight matrix to the different,
uh, words in the window.
And the advantage about this RNN is that it's
applying the exact same transformation to each of the inputs.
So this means that if it learns a good way to process one input,
that is applied to every input in the sequence.
So you can see it as more efficient in that way.
Okay, so what are the disadvantages of this model?
One is that recurrent computation is pretty slow.
Uh, as you saw before,
you have to compute the hidden state based on the previous hidden state.
So this means that you can't compute all of the hidden states in parallel.
You have to compute them in sequence.
So, especially if you're trying to compute an RNN over a pretty long sequence of inputs,
this means that the RNN can be pretty slow to compute.
Another disadvantage of RNNs is that it tuns out,
in practice, it's quite difficult to access information from many steps back.
So even though I said we should be able to remember about
the proctor and the clock and use that to predict exams and our books,
it turns out that RNNs,
at least the ones that I've presented in this lecture,
are not as good as that as you would think.
Um, we're gonna learn more about both of these disadvantages later in the course,
and we're going to learn something about how you can try to fix them.
Have we gotten any questions at this point? Yep.
Why do we assume that WH are the same?
Sorry, can you speak up?
Why do we assume that the WH should be the same?
So the question is, why should you assume that the WH are the same?
I suppose, it's not exactly an assumption,
it's more a deliberate decision in the design of an RNN.
So, an RNN is by definition,
a network where you apply the exact same weights on every step.
So, I suppose the question why do you assume maybe should be,
why is that a good idea?
Um, so I spoke a little bit about why it's a good idea,
and this list of advantages,
I suppose, are the reasons why you'd want to do that. Does that answer your question?
Open their books, right? If you assume that WH are the same,
you mean that like, uh,
Markov chain, it's like a Markov chain.
Uh, the trans- transmit, uh,
trans- transfer probability for the human moods open,
they are the same,
but actually the Markov chain.
The model, [inaudible] the transfer probability for that is the same,
so [inaudible] probability,
it- it's just an approximation but it's another test.
Okay. So I think that [OVERLAPPING]
If you assume WH could be the same,
it's good because you used a number of parameters,
but this is just an, this is just an approximation.
The underlying transfer, uh,
probability, it shouldn't be the same. Especially [OVERLAPPING]
Okay. Um, so I think the question is saying that given the- these
words the students opened their
are all different and they're happening in different context,
then why should we be applying the same transformation each time?
So that's a- that's a good question.
I think, uh, the idea is that you are learning a general function, not just, you know,
how to deal with students,
the one-word students in this one context.
We're trying to learn a general function of how you
should deal with a word given the word so far.
You're trying to learn a general representation of language and context so far,
which is indeed a very difficult problem.
Um, I think you also mentioned that something about an approximation.
Uh, another thing to note is that all of
the hidden states are vectors, they're not just single numbers, right?
They are vectors of lengths, I don't know, 500 or something?
So they have quite a large capacity to hold lots of information about
different things in all of their different, um, positions.
So, I think the idea is that you can
store a lot of different information in different contexts,
in different parts of the hidden state,
but it is indeed an approximation and there is
some kind of limit to how much information you can store.
Okay, any other questions? Yes.
Since you kinda process any single length frame,
what length do you use during your training?
And does the length you use for training affect WH?
Okay, so, the question is, given that you can have any length input,
what length is the input during training?
So, I suppose in practice,
you choose how long the inputs are in
training either based on what your data is or maybe based on,
uh, your efficiency concerns so maybe you make it artificially
shorter by chopping it up. Um, what was the other question?
Uh, does WH depend on that?
Okay. So the question was, does WH depend on the length you used?
So, no, and that's one of the good things in the advantages list.
Is that the model size doesn't increase for longer input,
because we just unroll the RNN
applying the same weights again and again for as long as we'd like.
There's no need to have more weights just because you have a longer input.
[NOISE] Yeah.
So how the ratios that you mentioned are [inaudible] the number of words.
[NOISE] Are you asking about capital E or the lowercase E?
Uh, lowercase E.
Okay. So, the question is,
how do we choose the dimension of the lowercase Es?
Uh, so, you could, for example,
assume that those are just pre-trained word vectors like the ones that you,
uh, used in assignment one.
More like word2vec.
Yeah. For example, word2vec,
and you just download them and use them,
or maybe you learn them from scratch, in which case,
you decide at the beginning of training how big you want those vectors to be.
[NOISE] Okay. I'm gonna move on for now.
[NOISE] So, we've learned what an RNN language model is and we've learned how you would,
uh, run one forward, but the question remains,
how would you train an RNN language model?
How would you learn it? [NOISE]
So, as always, in machine learning,
our answer starts with, you're going to get a big corpus of text,
and we're gonna call that just a sequence of words X1 up to X capital T. So,
you feed the sequence of words into the RNN language model, and then,
the idea is that you compute the output distribution Y-hat T for every step T. So,
I know that the picture I showed on the previous, uh,
slide [NOISE] only showed us doing on the last step,
but the idea is, you would actually compute this on every step.
So, this means that you're actually predicting
the probability of the next word on every step.
[NOISE] Okay.
So, once you've done that, then you can define the loss function,
and this should be familiar to you by now.
Uh, this is the cross-entropy between [NOISE]
our predicted probability distribution Y-hat T and the true, uh,
distribution, which is Y-hat- sorry, just YT,
which is a one-hot vector, uh,
representing the true next [NOISE] words,
which is XT plus one.
So, as you've seen before, this, uh,
cross-entropy [NOISE] between those two vectors can be written
also as a negative log probability.
And then, lastly, if you average this cross-entropy loss across every step, uh,
every T in the corpus time step T, then,
uh, this gives you your overall loss for the entire training set.
[NOISE] Okay.
So, just to make that even more clear with a picture,
uh, suppose that our corpus is,
the students open their exams,
et cetera, and it goes on for a long time.
Then, what we'd be doing is,
we'd be running our RNN over this text, and then,
on every step, we would be predicting the probability [NOISE] distribution Y-hats,
and then, from each of those,
you can calculate what your loss is,
which is the JT, and then, uh, on the first step,
the loss would be the negative log probability of the next word,
which is, in this example,
students, [NOISE] and so on.
Each of those is the negative log probability of the next word.
[NOISE] And then, once you've computed all of those,
you can add them [NOISE] all up and average them,
and then, this gives you your final loss.
[NOISE] Okay. So, there's a caveat here.
Um, computing the loss and gradients across the entire corpus,
all of those words X1 up to X capital T is too
expensive [NOISE] because your corpus is probably really big.
[NOISE] So, um, as a student asked earlier,
uh, in practice, what do you actually regard as your sequence?
So, in practice, you might regard your sequence as, uh,
something like a sentence or a document,
some shorter unit of text.
So, uh, another thing you'll do [NOISE] is, if you remember,
stochastic gradient descent allows you to compute gradients
for small chunks of data rather than the whole corpus at a time.
So, in practice, if you're training a language model,
what you're actually likely to be doing is computing the loss for a sentence,
but that's actually a batch of sentences, and then,
you compute the gradients with respect to that batch of sentences,
update your weights, and repeat.
Any questions at this point? [NOISE] Okay.
So, uh, moving onto backprop.
Don't worry, there won't be as much backprop as there was last week,
but, uh, there's an interesting question here, right?
So, the, uh, characteristic thing about RNNs
is that they apply the same weight matrix repeatedly.
So, the question is,
[NOISE] what's the derivative of our loss function,
let's say, on step T?
What's the derivative of that loss with respect to the repeated weight matrix WH?
So, the answer is that the derivative of the loss, uh,
the gradient with respect to the repeated weight is
the sum of the gradient with respect to each time it appears,
and that's what that equation says.
So, on the right, the notation with the vertical line and the I is saying, uh,
the derivative of the loss with respect to WH when it appears on the Ith step.
Okay. So, so, why is that true?
[NOISE] Uh, to sketch why this is true,
uh, [NOISE] I'm gonna remind you of the multivariable chain rule.
So, uh, this is a screenshot from a Khan Academy article on the multivariable chain rule,
and, uh, I advise you check it out if you
want to learn more because it's very easy to understand.
Uh, and what it says is,
given a function F [NOISE] which depends on X and Y,
which are both themselves functions of some variable T, then,
if you want to get the derivative of F with respect to T,
then you need to do the chain ru- rule across X and Y separately and then add them up.
[NOISE] So, that's the multivariable chain rule,
[NOISE] and if we apply this to our scenario with trying to take
the derivative of the loss JT with respect to our weight matrix WH,
then you could view it as this kind of diagram [NOISE] where WH has, uh,
a relationship with all of these individual appearances of WH,
but it's a [NOISE] simple relationship,
it's just equality, and then,
each of those appearances of WH affect the loss in different ways.
So, then, if we apply the multivariable chain rule,
then it says that the derivative of the loss with respect to
WH is the sum of those chain rule things,
but the expression on the right is just one because it's an equality relation,
[NOISE] and then, that gives us the equation that I wrote on the previous slide.
So, this is a proof sketch for why the derivative of the loss with
respect to our recurrent matrix is the sum of the derivatives each time it appears.
Okay. So, suppose you believe me on that, that is,
how you compute the, uh,
gradient with respect to the recurrent weight.
So, a remaining question is, well,
how [NOISE] do we actually calculate this in practice?
[NOISE] So, the answer is that you're going to calculate this sum by doing backprop,
uh, backwards, kind of right to left, um,
through the RNN, and you're going to accumulate this sum as you go.
So, the important thing is,
you shouldn't compute each of those things separately, uh,
you should compute them by accumulating, like,
each one can be computed in form- in terms of the previous one.
[NOISE] So, this algorithm of computing each of these,
uh, each of these gradients with respect to
the previous one is called backpropagation through time.
And, um, I always think that this sounds way more sci-fi than it is.
It sounds like it's time travel or something,
but it's actually pretty simple.
Uh, it's just the name you give to
applying the backprop algorithm to a recurrent neural network.
Any questions at this point? Yep. [NOISE]
So, it seems that how you break up the batches matter your end result.
[inaudible].
So, if you break it into much more [inaudible].
Okay. So the question is, um, surely,
how you decide to break up your batches affects how you learn, right?
Because if you choose, uh,
one set of data to be your batch, right, then,
you will make your update based on that, and then,
you only update the next one based on [NOISE] where you go from there.
So, if you decided to put different data in the batch,
then you would have made a different step.
So, that's true, [NOISE] and that is why
stochastic gradient descent is only an approximation of
true gradient descent because the gradient that you compute with
respect to one batch is just an approximation of the true gradient with respect to the,
uh, the loss over the whole corpus.
So, yes, it's true that it's an approximation
and how [NOISE] you choose to batch up your data can matter,
and that's why, for example, shuffling your data is a good idea,
and shuffling it differently, each epoch, is a good idea.
Uh, but the, the core idea of SGD is [NOISE] that, um,
it should be a good enough approximation that over many steps,
you will, uh, minimize your loss.
[NOISE] Any other questions? [NOISE] Yeah.
[NOISE] So, is, uh, is the question,
as you compute forward prop,
do you start computing backprop before you've even, like, got to the loss?
Is that the question? [NOISE]
Yes.
I didn't think so, right? Because you need to know what the loss is in
order to compute the derivative of the loss with respect to something.
So, I think you need to get to the end.
So, if we assume simplicity,
that there is only one loss which you get at the end of several steps,
then you need to get to the end,
compute the loss before you can compute the derivatives.
But I suppose you, you, you could compute the derivative of two,
kind of, adjacent things of one with respect to the other.
[OVERLAPPING] But, yeah. [NOISE]
As you're going forward, do- you need to sort of keep a track of what,
what you would have [inaudible] the one you eventually get the loss. [inaudible]
Yes. So, when you forward prop,
you certainly have to hang on to all of the intervening factors.
[NOISE] Okay. I'm gonna move on for now.
Uh, so, that was a maths-heavy bit but,
um, now, we're getting on to text generation,
which someone asked about earlier.
So, um, just as we use the n-gram language model to generate text,
you can also use an RNN language model to generate text,
uh, via the same repeated sampling technique.
Um, so, here's a picture of how that would work.
How you start off with your initial hidden state H0, uh,
which, uh, we have either as a parameter of
the model or we initialize it to zero, or something like that.
So, let's suppose that we have the first word my,
and Iet's suppose I, um, supply that to the model.
So, then, using the inputs and the initial hidden state,
you can get our first hidden state H1.
And then from there, we can compute the, er,
probability distribution Y hat one of what's coming next,
and then we can use that distribution to sample some word.
So let's suppose that we sampled the word favorite.
So, the idea is that we use the outputted word as the input on the next step.
So, we feed favorite into the second step of the RNN,
we get a new hidden state,
and again we get a new probability distribution,
and from that we can sample a new word.
So, we can just continue doing this process again and again,
and in this way we can generate some text.
So, uh, here we've generated the text,
My favorite season is Spring,
and we can keep going for as long as we'd like.
Okay, so, uh, let's have some fun with this.
Uh, you can generate,
uh, text using an RNN language model.
If you train the RNN language model on any kind of text,
then you can use it to generate text in that style.
And in fact, this has become a whole kind of
genre of internet humor that you might've seen.
So, uh, for example,
here is an RNN language model trained on Obama speeches,
and I found this in a blog post online.
So, here's the text that the RNN language model generated.
"The United States will step up to the cost of a new challenges of
the American people that will share the fact that we created the problem.
They were attacked and so that they have to say that
all the task of the final days of war that I will not be able to get this done."
[LAUGHTER] Okay.
So, if we look at this and
especially think about what did
that text look like that we got from the n-gram language model,
the one about the, the price of gold.
Um, I'd say that this is kind of recognizably better than that.
It seems more fluent overall.
Uh, I'd say it has a more of
a sustained context in that it kind of makes sense for longer stretches at a time,
and I'd say it does sound totally like Obama as well.
So, all of that's pretty good,
but you can see that it's still pretty incoherent overall,
like i- it was quite difficult to read it because it didn't really make sense, right?
So I had to read the words carefully.
Um, so, yeah, I think this shows
some of the progress you can get from using RNNs to generate text but still,
um, very far from human level. Here are some more examples.
Uh, here's an RNN language model that was trained on the Harry Potter books.
And here's what it said. "Sorry." Harry shouted, panicking.
"I'll leave those brooms in London." Are they?
"No idea." said Nearly Headless Nick,
casting low close by Cedric,
carrying the last bit of treacle Charms from Harry's shoulder.
And to answer him the common room perched upon it,
four arms held a shining knob from when the Spider hadn't felt it seemed.
He reached the teams too."
So, again, I'd say that this is fairly fluent.
It sounds totally like the Harry Potter books.
In fact, I'm pretty impressed by how much it does
sound like in the voice of the Harry Potter books.
You even got some character attributes,
I'd say that Harry the character does often panic in the book so that seems right.
Um, [LAUGHTER] but some bad things are that we have,
for example, a pretty long run-on sentence in the second paragraph that's hard to read.
Uh, you have some nonsensical things that really make no sense.
Like, I don't know what a treacle charm is.
It sounds delicious but I don't think it's real,
uh, and overall it's just pretty nonsensical.
Here's another example. Here is an RNN language model that was trained on recipes.
So, uh, [LAUGHTER] this one's pretty bizarre,
the title is 'chocolate ranch barbecue',
It contains Parmesan cheese,
coconut milk, eggs, and the recipe says place each pasta over layers of lumps,
shape mixture into the moderate oven and simmer until firm.
Serve hot in bodied fresh,
mustard orange and cheese.
Combine the cheese and salt together the dough in a large skillet;
add the ingredients and stir in the chocolate and pepper.
[LAUGHTER] Um, so, one thing that I think is
even more clear here in the recipes example than the prose example,
is the inability to remember what's [NOISE] what's happening overall, right?
Cuz a recipe you could say is pretty challenging because you need to remember
the title of what you're trying to make which in this case is chocolate ranch barbecue,
and you need to actually, you know, make that thing by the end.
Uh, you also need to remember what were the ingredients
in the beginning and did you use them.
And in a recipe, if you make something and put it in the oven,
you need to take it out later, a- and stuff like that, right?
So, clearly it's not really
remembering what's happening overall or what it's trying to do,
it seems to be just generating kind of
generic recipe sentences and putting them in a random order.
Uh, but again, I mean, we can see that it's fairly fluent,
it's grammatically right, it kind of sounds like a recipe.
Uh, but the problem is it's just nonsensical.
Like for example, shape mixture into
the moderate oven is grammatical but it doesn't make any sense.
Okay, last example.
So, here's an RNN language model that's trained on paint-color names.
And this is an example of a character-level language model because
it's predicting what character comes next not what word comes next.
And this is why it's able to come up with new words.
Another thing to note is that this language model was
trained to be conditioned on some kind of input.
So here, the input is the color itself I think represented by the three numbers,
that's probably RGB numbers.
And it generated some names for the colors.
And I think these are pretty funny.
My favorite one is Stanky Bean,
which is in the bottom right.
[LAUGHTER] Um, so, it's pretty creative,
[LAUGHTER] and I think these do sound kind of
like paint colors but often they're quite bizarre.
[LAUGHTER] Light of Blast is pretty good too.
So, uh, you're gonna learn more about
character-level language models in a future lecture,
and you're also going to learn more about how to condition a language model
based on some kind of input such as the color, um, code.
So, these are pretty funny,
uh, but I do want to say a warning.
Um, you'll find a lot of these kinds of articles online,
uh, often with headlines like,
"We forced a bot to watch, you know,
1000 hours of sci-fi movies and it wrote a script," something like that.
Um, so, my advice is you have to take these with a big pinch of salt, because often,
uh, the examples that people put online were
hand selected by humans to be the funniest examples.
Like I think all of the examples I've shown today were definitely hand selected
by humans as the funniest examples that the RNN came up with.
And in some cases they might even have been edited by a human.
So, uh, yeah, you do need to be a little bit skeptical when you look at these examples.
[OVERLAPPING] Yep.
So, uh, in the Harry Potter one,
there was a opening quote and then there was a closing quote.
So, like do you expect the RNN,
like when it puts that opening quote and keeps putting more words,
do you expect the probability of a closing quote to like increase as you're going or decrease?
That's a great question. So, uh,
the question was, uh,
we noticed that in the Harry Potter example,
there was some open quotes and some closed quotes.
And it looks like the model didn't screw up, right?
All of these open quotes and closed quotes,
uh, are in the correct places.
So, the question is, do we expect the model to put
a higher probability on closing the quote given that is inside a quo- quote passage?
So, I should say definitely yes and
that's most- mostly the explanation for why this works.
Um, there's been some really interesting work in trying
to look inside the hidden states of, uh,
language models to see whether it's tracking things like,
are we inside an open quote or a close quote?
And there has been some limited evidence to show that
maybe there are certain neuron or neurons inside the hidden state,
which are tracking things like,
are we currently inside a quote or not?
[NOISE]. Yeah.
So, so, like do you think the probability would increase as you go more to the right [OVERLAPPING]?
So, the question is as the quote passage goes on for longer,
do you think the priority or
the probability of outputting a closed quote should increase?
Um, I don't know.
Maybe. Um, that would be good, I suppose,
because you don't want an infinite quote,
uh, but I wouldn't be surprised if that didn't happen.
Like I wouldn't be surprised if maybe some other worse-trained language models,
just opened quotes and never closed them.
Uh, any other questions? Yeah.
What are the dimensions of the W metric?
Okay. So, the question is what are the dimensions of the W metric?
So we're going back to the online stuff.
Uh, okay. You're asking me about W_h or W_e or something else?
Yeah.
So, W_h will be,
uh, if we say that the hidden size has size n,
then W_h will be n by n. And if we suppose that the embeddings have size d,
then W_e will be, uh,
d by n, n by d, maybe.
Does that answer your question? [NOISE] Uh,
any other questions about generating or anything? Yep.
So, you said that there was a long sentence in the Harry Potter-related text?
Yeah.
Is it ever sort of practical to combine RNNs with like in this hand written rules?
Sorry. Is it ever practical to combine-
RNNs with a written list of hand-written rules.
[OVERLAPPING]
Okay. Yeah. That's a great question.
So the question was, is it ever practical to
combine RNNs with a list of hand-written rules?
For example, don't let your sentence be longer than this many words.
Um, so yeah.
I'd say it probably is practical maybe especially if you're interested in, uh,
making sure that certain bad things don't happen,
you might apply some hacky rules like yeah forcing it to end, uh, early.
I mean, okay. So there's this thing called Beam Search
which we're going to learn about in a later lecture,
which essentially doesn't just,
um, choose one word in each step and continue.
It explores many different options for words you could generate.
And you can apply some kinds of rules on that
where if you have lots of different things to choose from,
then you can maybe get rid of
some options if you don't like them because they break some of your rules.
But, um, it can be difficult to do. Any other questions?
Okay. Um, so we've talked about generating from language models.
Uh, so unfortunately, you can't just use
generation as your evaluation metric for the language models.
You do need some kind of, um, measurable metric.
So, the standard evaluation metric for language models is called perplexity.
And, uh, perplexity is defined as
the inverse probability of the corpus according to the language model.
So, if you look at it you can see that that's what this formula is saying.
It's saying that for every, uh,
word xt, lowercase t, in the corpus, uh,
we're computing the probability of that word given
everything that came so far but its inverse is one over that.
And then lastly, when normalizing this big,
uh, product by the number of words,
which is capital T. And the reason why we're doing that is because if we didn't do that,
then perplexity would just get smaller and smaller as your corpus got bigger.
So we need to normalize by that factor.
So, you can actually show you that this, uh,
perplexity is equal to the exponential of the cross-entropy loss J Theta.
So if you remember, cross-entropy loss J Theta is, uh,
the training objective that we're using to train the language model.
And, uh, by rearranging things a little bit,
you can see that perplexity is actually the exponential of the cross-entropy.
And this is a good thing, uh,
because if we're training the language model to, uh,
minimize the cross-entropy loss,
then you are training it to optimize the perplexity as well.
So you should remember that the lower perplexity is better,
uh, because perplexity is the inverse probability of the corpus.
So, uh, if you want your language model to assign high probability to the corpus, right?
Then that means you want to get low perplexity.
Uh, any questions? [NOISE] Okay.
Uh, so RNNs have been pretty successful in recent years in improving perplexity.
So, uh, this is a results table from a recent,
uh, Facebook research paper about RNN language models.
And, uh, you don't have to understand all of the details of this table,
but what it's telling you is that,
on the, uh, top where we have n gram language model.
And thessssssssssssn in the subsequent various,
we have some increasingly complex and large RNNs.
And you can see that the perplexity numbers are decreasing,
because lower is better.
So RNNs have been really great for
making more effective language models in the last few years.
Okay. So to zoom out a little bit,
you might be thinking, uh,
why should I care about Language Modelling?
Why is it important? I'd say there are
two main reasons why Language Modelling is important.
Uh, so the first one is,
that language modelling is a benchmark task that
helps us measure our progress on understanding language.
So, you could view language modeling as
a pretty general language understanding task, right?
Because predicting what word comes next to given any,
any kind of, uh, generic text.
Um, that's quite a difficult and general problem.
And in order to be good at language modelling,
you have to understand a lot of things, right?
You have to understand grammar,
you have to understand syntax,
and you have to understand,
uh, logic and reasoning.
And you have to understand something about,
you know, real-world knowledge.
You have to understand a lot of things in order to be
able to do language modelling properly.
So, the reason why we care about it as
a benchmark task is because if you're able to build a model,
which is a better language model than the ones that came before it,
then you must have made some kind of progress on at
least some of those sub-components of natural language understanding.
So, another more tangible reason why you might
care about language modelling is that it's a sub-component of
many many NLP tasks especially those which involve
generating text or estimating the probability of text.
So, here's a bunch of examples.
Uh, one is predictive typing.
That's the example that we showed at the beginning of the lecture
with typing on your phone or searching on Google.
Uh, this is also very useful for people who have movement disabilities, uh,
because they are these systems that help people communicate using fewer movements.
Uh, another example is speech recognition.
So, in speech recognition you have
some kind of audio recording of a person saying something
and often it's kind of noisy and hard to make out what they're saying and you need to,
uh, figure out what words did they say.
So this an example where you have to estimate the probability of different,
uh, different options of what, what it is they could have said.
And in the same way, handwriting recognition,
is an example where there's a lot of noise
and you have to figure out what the person intended to say.
Uh, spelling and grammar correction is yet
another example where it's all about trying to figure out what someone meant.
And that means you actually understand how
likely it is that they were saying different things.
Uh, an interesting, an interesting application is authorship identification.
So suppose that you have a piece of text and you're trying to
figure out who likely wrote it and maybe you have,
uh, several different authors and you have text written by those different authors.
So you could, for example,
train a separate language model on each of the different authors' texts.
And then, because, remember,
a language model can tell you the probability of a given piece of text.
Then you could ask all the different language models,
um, how likely the texts and the question is,
and then if a certain author's language model says that it's likely then that
means that text the texts and the question is more likely to be written by that author.
Um, other examples include machine translation.
This is a huge, uh,
application of language models,
uh, because it's all about generating text.
Uh, similarly, summarization is
a task where we need to generate some text given some input text.
Uh, dialogue as well,
not all dialogue agents necessarily are RNN language models but you can
build a dialogue agent that generates the text using an RNN language model.
And there are more examples as well.
Any questions on this? [LAUGHTER] Yep.
So, I know that [inaudible]
Great question. So, the question was,
uh, for some of these examples, uh,
such as speech recognition or maybe [NOISE] image captioning,
the input is audio or image or something that is not text, right?
So, you can't represent it in the way that we've talked about so far.
Um, so, [NOISE] in those examples,
you will have some way of representing the input,
some way of encoding the audio or the image or whatever.
Uh, the reason I brought it up now in terms of language models is that that's the input,
but you use the language model to get the output, right?
So, the language model, [NOISE] uh, generates
the output in the way that we saw earlier, uh,
but we're gonna learn more about those conditional language [NOISE] models later.
[NOISE] Anyone else?
[NOISE] Okay.
[NOISE] So, uh, here's a recap.
If I've lost you somewhere in this lecture, uh, or you got tired,
um, now's a great time to jump back in
because things are gonna get a little bit more accessible.
Okay. So, here's a recap of what we've done today.
Uh, a language model is a system that predicts the next word,
[NOISE] and a recurrent neural network,
is a new family, oh, new to us,
a family of neural networks that takes sequential input
of any length and it applies the same weights on every step,
and it can optionally produce some kind of output on
each step or some of the steps or none of the steps.
[NOISE] So, don't be confused.
A recurrent neural network is not [NOISE] the same thing as a language model.
Uh, we've seen today that an RNN is a great way to build a language model, but actually,
it turns out that you can use RNNs for,
uh, a lot of other different things that are not language modeling.
[NOISE] So, here's a few examples of that.
[NOISE] Uh, you can use an RNN to do a tagging task.
So, some examples of tagging tasks are
part-of-speech tagging and named entity recognition.
So, pictured here is part-of-speech tagging, and this is the task.
We have some kind of input text such as, uh,
the startled cat knocked over the vase,
and your job is to, uh,
label or tag each word with its part of speech.
So, for example, cat is a noun and knocked is a verb.
So, you can use an RNN to do this task in,
in the way that we've pictured, which is that you, uh,
feed the text into the RNN, [NOISE] and then,
on each step of the RNN,
you, uh, have an output,
probably a distribution over what, uh,
tag you think it is, and then, uh, you can tag it in that way.
And then, also for named entity recognition,
that's all about, um,
tagging each of the words with what named entity type they are.
So, you do it in the same way. [NOISE] Okay.
Here's another thing you can use RNNs for,
uh, you can use them for sentence classification.
So, sentence classification is just a general term to mean
any kind of task where you want to take sentence or other piece of text,
and then, you want to classify it into one of several classes.
So, an example of that is sentiment classification.
Uh, sentiment classification is when you have some kind
of input text such as, let's say, overall,
I enjoyed the movie a lot, and then,
you're trying to classify that as being
positive or negative or [NOISE] neutral sentiment.
So, in this example, this is positive sentiment.
[NOISE] So, one way you might use an RNN to tackle this task is, uh,
you might encode the text using the RNN, and then,
really what you want is some kind of sentence encoding so that you
can output your label for the sentence, right?
And it'll be useful if you would have a single vector to
represent the sentence rather than all of these separate vectors.
So, how would you do this?
How would you get the sentence encoding from the RNN?
[NOISE] Uh, one thing you could do [NOISE] is,
you could use the final hidden state as your sentence encoding.
So, um, the reason why you might think this is a good idea is because,
for example, in the RNN,
we regard the, the final hidden state as,
um, this is the thing you use to predict what's coming next, right?
So, we're assuming that the final hidden state contains
information about all of the text that has come so far, right?
So, for that reason, you might suppose that this is a good sentence encoding,
and we could use that [NOISE] to predict, you know,
what, uh, what sentiment is this sentence.
And it turns out that usually, a better way to do this,
usually a more effective way,
is to do something like maybe take an element-wise max or
an element-wise mean of all these hidden states to get your sentence encoding,
um, [NOISE] and, uh,
this tends to work better than just using the final hidden state.
[NOISE] Uh, there are some other more advanced things you can do as well.
Okay. [NOISE] Another thing that you can use RNNs for
is as a general purpose encoder module.
Uh, so, here's an example that's question answering,
but really this idea of RNNs as
a general purpose encoder module is very common [NOISE] and use it in lots of different,
um, deep learning [NOISE] architectures for NLP.
[NOISE] So, here's an example which is question answering.
Uh, so, let's suppose that the, the task is,
you've got some kind of context,
which, in this, uh, situation,
is the Wikipedia article on Beethoven, and then,
you have a question which is asking,
what nationality was Beethoven?
Uh, and this is actually taken from the SQuAD Challenge,
which is the subject of the Default Final Project.
So, um, if you choose to do- to do the Default Final Project,
you're going to be building systems that solve this problem.
So, what you might do is, you might use an RNN to process the question,
what nationality was [NOISE] Beethoven?
And then, you might use those hidden states that you get from this, uh,
RNN of the question as a representation of the question.
And I'm being intentionally vague here [NOISE] about what might happen next, uh,
but the idea is that you have [NOISE]
both the context and the question are going to be fed some way,
and maybe you'll use an RNN on context as well,
and you're going to have lots more neural architecture in order to get your answer,
which is, uh, German.
So, the point here is that the RNN is acting as an encoder for the question,
that is, the hidden states that you get from running
the RNN over the question, represent the question.
[NOISE] Uh, so, the encoder is part of a larger neural system,
[NOISE] and it's the, the hidden states themselves
that you're interested in because they contain the information.
So, you could have, um, taken,
uh, element-wise max or mean,
like we showed in the previous slide,
to get a single vector for the question, but often, you don't do that.
Often, you'll, uh, do something else which uses the hidden states directly.
So, the general point here is that RNNs are quite powerful as a way to represent,
uh, a sequence of text,
uh, for further computation.
Okay. Last example. So, going back to RNN language models again, [NOISE] uh,
they can be used to generate text,
and there are lots of different, uh, applications for this.
So, for example, speech recognition, uh, you will have your input,
which is the audio, and as a student asked earlier,
this will be, uh, represented in some way,
and then, uh, maybe you'll do a neural encoding of that, [NOISE] and then,
you use your RNN language model to generate the output,
which, in this case, is going to be a transcription
of what the audio recording is saying.
So, you will have some way of conditioning,
and we're gonna talk more about how this works, uh,
in a later lecture, but you have some way of
conditioning your RNN language model on the input.
So, you'll use that to generate your text, [NOISE] and in this case,
the utterance might be something like, what's the weather,
question mark. [OVERLAPPING] [NOISE]
Yeah. [NOISE]
In speech recognition, [inaudible].
Okay. So, the question is, in speech recognition,
we often use word error rates to evaluate,
but would you use perplexity to evaluate?
[NOISE] Um, I don't actually know much about that. Do you know, Chris,
what they use in, uh,
speech recognition as an eval metric? [NOISE]
[inaudible] word error rate [inaudible].
The answer is, you often use WER,
uh, for eval, but you might also use perplexity.
Yeah. Any other questions?
[NOISE] Okay. So, um,
this is an example of a conditional language model,
and it's called a conditional language model
because we have the language model component,
but crucially, we're conditioning it on some kind of input.
So, unlike the, uh, fun examples like with the Harry Potter text where we were just, uh,
generating text basically unconditionally, you know,
we trained it on the training data, and then,
we just started [NOISE] with some kind of random seed,
and then, it generates unconditionally.
This is called a conditional language model
because there's some kind of input that we need to condition on.
Uh, machine translation is an example [NOISE] also of a conditional language model,
and we're going to see that in much more detail in
the lecture next week on machine translation.
[NOISE] All right. Are there any more questions?
You have a bit of extra time, I think.
[NOISE] Yeah.
I have a question about RNNs in general.
[NOISE] Do people ever combine the RNN,
uh, patterns of architecture,
um, with other neural networks?
Say, [NOISE] you have, um, you know,
N previous layers that could be doing anything,
and at the end of your network,
you wanna run them through,
uh, five recurrent layers.
Do people mix and match like that,
or these, uh, [inaudible]. [NOISE]
Uh, the question is,
do you ever combine RNN for the other types of architecture?
So, I think the answer is yes.
[NOISE] Uh, you might, [NOISE] you know, uh,
have- you might have other types of architectures, uh,
to produce the vectors that are going to be the input to RNN,
or you might use the output of your RNN
[NOISE] and feed that into a different type of neural network.
So, yes. [NOISE] Any other questions?
[NOISE] Okay.
Uh, so, before we finish, uh, I have a note on terminology.
Uh, when you're reading papers,
you might find often this phrase vanilla RNN,
and when you see the phrase vanilla RNN,
that usually means, uh,
the RNNs that are described in this lecture.
So, the reason why those are called vanilla RNNs is
because there are actually other more complex kinds of RNN flavors.
So, for example, there's GRU and LSTM,
and we're gonna learn about both of those next week.
And another thing we're going to learn about next week
[NOISE] is that you can actually get some multi-layer RNNs,
which is when you stack multiple RNNs on top of each other.
[NOISE] So, uh, you're gonna learn about those,
but we hope that by the time you reach the end of this course,
you're going to be able to read a research paper and see a phrase like
stacked bidirectional LSTM with residual connections and self-attention,
and you'll know exactly what that is.
[NOISE] That's just an RNN with all of the toppings.
[LAUGHTER] All right. Thank you. That's it for today.
[NOISE] Uh, next time- [APPLAUSE] next time,
we're learning about problems [NOISE] and fancy RNNs.
[NOISE]
 Hi, everyone. I'm Abby.
If you weren't here last week,
I'm the head TA of this course.
And this is the second [NOISE] of three lectures that I'm
going to be giving on RNNs and related topics.
Okay. So, welcome to week four.
Today, we're going to be learning about vanishing gradients,
and some more complex types of RNNs.
So, before we get started,
I've got a few announcements.
Uh, the first announcement is that assignment four is released today, uh,
it's due Thursday of next week, not Tuesday,
so that means you have two days more to do it than you did for all the other homeworks.
And the reason for that is assignment four is
probably more work than the other homework so far,
so don't be surprised by that.
Uh, assignment four is all about Neural Machine Translation.
Uh, we're gonna learn about NMT on Thursday's lecture this week.
And, uh, this is really exciting,
because actually CS 224 has never had an NMT assignment before,
so this is all new this year,
and you're gonna be the first year students who are going to be doing an NMT assignment.
Uh, something else that's different about
assignment four is that you're going to be using Azure, which is, uh,
a cloud computing service,
in order to train your NMT systems on a virtual machine with a GPU.
And, uh, this is necessary in order to be able to do it in a reasonable amount of time.
So, I have a warning which is,
if you're a person who perhaps doesn't have, ah,
learnt- a lot of experience working on remote machines,
so for example if you're not very familiar with SSH,
or tmux, or remote text editing,
then I advise you to budget some extra time for assignment four,
because that's probably gonna take you a little while to set up and get used to.
So, again, I'm going to emphasize,
do get started early on assignment four, because, uh,
the NMT system takes about four hours to train on your virtual machine,
so you really can't start it the night before and expect to get it in on time.
Uh, and assignment four is really quite a lot more complicated than assignment three.
So, uh, don't get into a false sense of security if you found assignment three easy.
Um, so Thursday's slides on NMT are ready on the website today,
so you can even start looking at it today if you
want- if you wanna get started on assignment four early.
Uh, so, I have a few more announcements, uh,
on the subject of projects, uh,
next week's lectures are going to be all about projects.
So, you're going to hear about, uh, question answering,
and the default final projects,
and then you're also gonna get some tips about how you might,
uh, choose and define your own custom projects.
So, it's fine if you're not thinking about a project this week, that's okay.
You can delay until next week to start thinking about it for the first time.
But if you are a person who is already thinking about your projects,
for example, if you're trying to choose your custom projects, uh,
then you should check out the website's project page,
because it has quite a lot of information about, uh,
how to choose your projects, and also some inspiration.
And that includes- we've collected some, uh,
project ideas from various members of the Stanford AI Lab.
So, these are faculty and PhD students and postdocs,
who have ideas for, uh,
NLP deep learning projects that they would like
CS224n students such as yourself to work on.
So, especially, if you're looking to maybe get into research later,
this is a really great opportunity, uh,
to work with someone in the Stanford AI Lab,
and maybe get some mentorship as well.
Okay. So here's an overview.
Uh, last week, we learned about Recurrent Neural Networks,
um, we learned about why they're really great for Language Modeling.
And today, we're gonna learn about some problems with RNNs,
and we're gonna learn about how to fix them.
And this is gonna motiva- motivate us to learn about some more complex RNN variants.
And then, uh, next lecture on Thursday,
we're going to, uh, have some more application-based, uh, contents,
so we are going to be learning about Neural Machine Translation,
which is a really important task in, uh,
NLP and deep learning, and in particular,
we're gonna learn about this architecture called sequence-to-sequence with attention.
But in more detail,
today's lecture, uh, first,
we are going to learn about the vanishing gradient problem.
And this is gonna motivate us to learn about two new types of
RNN called Long Short-Term Memory,
and Gated Recurrent Unit.
We're also going to learn about some other kind of
miscellaneous fixes for the vanishing gradient problem,
or the exploding gradient problem.
Uh, so in particular,
we're going to learn about gradient clipping,
which is, uh, fairly simple, but quite important.
Uh, we're also going to learn about skip connections,
which is a fairly new neural architecture,
which tries to, uh,
fix the vanishing gradient problem.
[NOISE] And then, at the end of the lecture,
we're gonna learn about some more fancy RNN variants such as, uh,
bidirectional RN- RNNs, those are the ones which go not just left to right,
but also right to left,
and we're going to learn about multi-layer RNNs.
And that's when you stack multiple RNNs on top of each other.
So, there's a lot of important definitions today.
Um, so, you're gonna find that the information in
this lecture is pretty important for
assignment four and probably for your project as well.
Okay. So, let's get started thinking about the vanishing gradients.
Uh, so here we have an RNN,
with, let say, ah, four steps,
and suppose that we have some kind of loss that's, uh,
J4, and that's computed based on the four hidden states.
So, let's suppose we're interested in asking what is the derivative of this loss J4,
with respect to the hidden states,
uh, h1, the first hidden state?
So, I'm representing that with this, uh,
blue arrow notation to kind of represent how we have
to make the gradients flow backwards in order to complete this.
So, if we're interested in what this gradient is,
we can apply the chain rule and say, "Well,
it's the product of the, uh,
gradient of the loss with respect to h2,
and then gradient of h2, with respect to h1."
And then, similarly, we can decompose that
again using the chain rule, and we can do it again.
So, what we've done here is we've decomposed the gradient that we were interested in,
into the products of these various intermediate gradients.
And in particular, we're seeing all these ht by ht minus 1,
uh, adjacent gradients of the hidden states.
So, the thing I want to ask you is,
what happens if these gradients are small?
Given that there's a lot of them,
uh, what happens if they're small in magnitude?
So, the overall problem of the vanishing gradient problem,
is that when these gradients are small,
then our overall gradient is gonna get smaller and smaller,
as it back propagates further.
Because the accumulated gradient is the product of all of these intermediate gradients.
And when you multiply something by something small,
then the whole thing gets smaller.
So, that's what I'm representing here with these, uh,
smaller and smaller blue arrows going backwards.
So, that's the general idea of the vanishing gradient problem.
Here's a slightly more formal definition.
So, if you remember from last time,
uh, if we have a null RNN,
then the hidden state ht is
computed as a function of the previous hidden state ht minus 1,
and the current input xt.
Uh, so you might remember in the previous lecture we
said that xt were one-hot vectors representing words,
and then ET is the embedding.
Uh, this lecture we're going to be,
uh, getting rid of that detail,
and we're just gonna be thinking very abstractly about
an RNN that has some kind of input xt,
and xt is just any kind of vector.
Probably a dense vector,
but you know, it could be words or not.
It could be one-hot or dense.
Uh, but that's just the input.
So, that's the, uh,
the definition that we learned last time for Vanilla RNNs.
So, this means that the derivative of ht,
hidden state on step t with respect to the previous hidden state,
uh, is this expression here.
Uh, so this is just an application of the chain rule, and, uh,
if you looked long enough or refer back to
the backprop lecture you'll see, uh, that that make sense.
So, in particular, we're, um,
multiplying by Wh at the end, uh,
because we have the multiplication of Wh and ht minus 1 on the inside.
Okay. So, if you remember, on the previous slide,
we were thinking about what's the gradient of the loss on some step,
step i I'd say,
with respect to a hidden state hj,
on some previous step j.
And maybe J is quite a few steps before i.
So, we can now write this,
uh, in the following way.
So just by applying the chain rule,
now on the first line we're saying that this derivative that we're interested in
can be decomposed into the derivative with respect to step i,
which is kind of the last step,
and then do all of those intermediate gradients of the adjacent hidden states as well.
So, that- that first slide is just exactly the same thing as we were looking at on the,
uh, the picture, uh, the diagram on the previous slide.
Okay. And then, given that we figured out what is, uh,
dht by dht minus one,
ah, further on the slide,
then we can just substitute that in.
So, what we're finding is that this overall gradient that we're
interested in, in particular,
has this term, uh,
Wh, the weight matrix, and it's, uh,
multiplied by itself, i minus j times,
because there's i minus j many steps between, uh,
step j and step i,
which is the- the distance that we're traveling with this gradient.
So, the big problem here is,
if this weight matrix Wh is small,
then this term is gonna get vanishingly small,
exponentially small, as i and j get further apart.
So, to give this a little more detail, uh,
we can think about the, uh,
L2 matrix norms of all of these matrices, right?
And, uh, as a- as a- uh, as a- sorry.
I'm- it's a known fact of,
uh, L2 norms that you have this, um,
inequality that's the, uh,
norm of the products of
some matrices is less and equal to the product of the norms of the matrices.
So, in particular, we're seeing that the norm of this gradient that we're interested in,
is less than or equal to, uh,
the product i minus j many times of the norm of the weight matrix Wh.
So, this is what we mean when we say we're concerned about Wh being small,
because if it's small, then the thing on the left has to be exponentially small.
So in particular in this,
uh, paper that, uh,
you can take a look at the bottom if you're interested, um, uh,
Pascanu et al showed that if
the largest eigenvalue of the weight matrix Wh is less than one,
then this gradient on the left is going to shrink exponentially.
And you can probably see intuitively why this is true.
So if, you know, as a simplifying assumption,
we suppose that Wh was not a matrix,
but simply a scalar that was just a single number,
then you can see why if that number was greater than one,
then the whole thing is gonna explode.
And if that number is less than one,
then it is going to shrink
exponentially as you multiply by the same number again and again.
Uh, so you can check out the paper for more details,
but here, uh, the bound is one,
partially because we have the sigmoid nonlinearity.
And that's, uh, based on the bounds of what we know as the,
uh, norm of the sigmoid function to be.
So, uh, this shows you why if the, uh,
Wh matrix is small,
or if its largest eigenvalue was small,
then we're going to have vanishing gradients.
And similarly, if you check out the paper,
you can see that there's a similar proof, uh,
relating if the largest eigenvalue is greater than one,
to having exploding gradients.
So that's when the gradients get bigger and bigger,
as you backprop further.
Okay. So hopefully I've convinced you that
vanishing gradients is a phenomenon that happens in our norms.
But I haven't yet said why this is a problem.
So, why should we view this as a bad thing,
if the gradients are getting larger and larger,
or smaller and smaller as you backprop?
So here's, uh, here's a picture that might illustrate why it's a bad thing.
So, uh, as before,
suppose that we're thinking about,
what's the derivative of the loss on
the fourth step with respect to the first hidden state?
And we have this situation where
the gradient is getting smaller and smaller as it goes backwards.
But then, think about what is the gradient of let's say
the loss in the second step also with respect to the first hidden state.
So I'm representing that with the orange arrows.
And what my point is here,
is that the magnitude of the gradient signal from close by,
is a lot bigger than the magnitude of the gradient signal from far away.
And this means that when you update your model weights,
the signal that you're getting from close by is gonna
be so much bigger than the signal from far away,
that essentially you're only going to learn,
you're only going to optimize with respect to
these nearby effects and not the long-term effects.
So you're gonna, you're gonna lose the long-term effects, er, inside the,
the nearby effects. Any questions about this, yeah?
So, uh, where they say there that you do actual updates.
You know, there are actually some that are multiple chains, not just one chain.
So the nearer term should cover it.
Sorry, what's the last part?
The nearer term should have a larger effect considering you're
updating the sum of the weights over different chains.
Okay. So I think, ah, the observation was that,
given that, for example,
in Language Modeling you might be summing over multiple losses.
There is a loss in every step and you sum all of them and that's your overall loss.
Then you do want to update more with respect to the nearby losses than the far losses.
So I think, uh, yeah,
so if the design of your objective function
is that it's the sum of the loss in every step,
then you do want to, uh,
weight all of them equally.
I think, uh, my point was more about,
what is the influence of, uh,
the action of the weight matrix at this early stage.
What is its influence on a loss that's nearby?
And what is its influence on a loss that's far away?
Um, and due to, uh,
the dynamics of how the vanishing gradient, uh,
problem works, then, uh,
the influence on the loss that's far away
is gonna be much less than the influence nearby.
And I'm gonna give some more linguistics examples later of why you might want to learn,
uh, the connections that are farther away.
So essentially the problem is,
in situations where you do want to learn the connection
between something that happens early and something that happens later,
then you're going to be unable to learn that connection.
Uh, so we'll see some motivating examples in a minute.
Any other questions on this? Yeah?
Um, I'm getting confused like, why are you talking about like dh, dj dh.
Uh, it's like H parameter, like, are we going-
Yeah.
from-
Okay. That's a great question.
So you're asking why are we interested in some kind of dj by
dh given that we're not updating H. H is an activation not a weight.
Um, so the reason why we're thinking about that,
is because when you think about what is dj by dw,
which is a thing that we're going to update.
That's always gonna be in terms of dj by dh at some point, right?
So if we're thinking about W, you know,
and how it acts on, uh,
the transmission from h_1 to h_2,
then dj4 by W in that position is going to have to go through dj4 by dh_2.
So if we're getting vanishing gradients,
uh, as we back propagate further,
then it's kind of like a bottleneck.
Then you're certainly going to have vanishing gradients as they affect, uh,
the recurrence matrix there,
and indeed the matrix that's applied to the inputs.
Okay. I'm gonna move off now.
Uh, so another way to explain why vanishing gradients is a problem,
is you can think of it as, uh, a gradient.
You can think of it as a measure of the effect of the past on the future.
So we've already talked about this little bit.
Uh, gradient is like saying, if I change, uh,
this weight or this activation a little bit,
then how much and how does it affect this thing in the future.
So in particular, if our gradient is becoming vanishingly small over longer distances,
let say from step T, step T to step T plus N,
then we can't tell whether in one of two situations.
So the first situation is maybe there's no dependency between
step T and step T plus N in the data.
So perhaps we're learning on a task where,
in the task there truly is no collect, uh,
connection or relationship to be
learned between what happens on step T and what happens on
step T plus N. So there truly is nothing to be
learned and it's actually correct that there should be,
you know, small gradients with respect to those two things.
But the second possibility is that, yes,
that is a true connection between those two things in the data and in the task.
And really ideally we should be learning that connection.
Um, but we have the wrong parameters in our model to capture this thing,
and therefore that is why the,
the gradients are small.
Because the model doesn't see them as connected.
So we are not learning the true dependency between these two things.
And the problem with the vanishing gradient problem is that it's,
we're unable to tell in this situation,
which of these two situations we're in.
Okay. So this is all pretty theoretical.
I think this example should make it a little more,
more clear why the vanishing gradient problem is bad.
So, uh, last week we learned about RNN-Language Models.
And if you remember Language Modeling is a task where you have some kind of
text and then you're trying to predict what word should come next.
So, uh, here's a piece of text.
It says, um, ''When she tried to print her tickets,
she found that the printer was out of toner.
She went to the stationery store to buy more toner.
It was very overpriced.
After installing the toner into the printer,
she finally printed her,'' and
can someone shout out what word you think should come next?
Tickets.
Tickets. Yes, exactly.
So that was easy for you to do because, uh,
it makes sense logically that if that was the thing she was trying to do,
that's the thing she's gonna do once she's gone the whole detour for the, for the toner.
Um, so the question is,
can RNN-Language Models easily answer this question.
Would they do well at this particular Language Modeling example?
So for an RNN-Language Model to do well at this kind of example,
then they need to learn from this kind of example in the Training Data.
So if it solves the example in the Training Data,
then the RNN-Language Model will need to model the dependency.
Learn the connection between the appearance of
the word tickets early on on the 7th step,
and the target word tickets at the end.
But if we have the vanishing gradient problem,
then these gradients, uh, if they know the step,
the, the last step with respect to the early step,
it's gonna be very small because it's,
it's a fairly long distance, right?
And this means that the model is going to be unable to
learn this dependency, easily or at all.
So if the model can't learn this kind of dependency during training,
then the model is going to be unable to predict
similar kinds of long distance dependencies at test-time.
Okay, here's another example.
Um, here's a piece of text.
Uh, the text says and this isn't a full sentence.
This is just a partial sentence.
It says, the writer of the books, blank.
And I'm gonna give you two options.
It's either, the writer of the books is or the writer of the books are.
So, uh, again shout out which one do you think it is, is or are?
Is.
Is, that's right. So, uh, the correct answer,
a correct possible continuation of the sentence would be,
uh, the writer of the books is planning a sequel.
I can't think of a continuation that goes the writer of the books are,
that would be, uh, grammatically correct.
So the reason why I'm bringing up this example,
is because this shows a kind of tension between, uh,
two things called, uh,
syntactic recency and sem- uh, sequential recency.
So syntactic recency is the idea that in
order to correctly predict the next word should be more is than are,
is that the word writer is the kind of syntactically close word here.
So we say the writer of the books is because it's the writer is.
So you can see this as the word writer and is,
are, uh, syntactically close.
Because if you looked at the dependency paths for example,
then there would be a short path in that tree.
So by contrast, se- sequential recency is the,
uh, simpler concepts of how close words are just in the sentence as a sequence of words.
So in this example,
books and are, are very sequentially recent because they're right next to each other.
So the reason I'm bringing this up is because,
the second one would be incorrect but it's kind of a tempting option.
Because if you're mostly only paying attention to things that happened recently,
um, then you might get distracted and think,
"Oh, the books are, that sounds right."
So the problem here is that RNN-Language Models
are better at learning from sequential recency than sicta- syntactic recency.
And this is partially due,
due to the vanishing gradient problem.
Because especially perhaps, if your syntactically,
uh, related word is actually kind of far away,
then it might get really hard to use the information from the syntactically recent word,
especially if there's a lot of strong signal from the sequentially recent word.
So, uh, there are some papers that show that RNN-Language Models make this kind of error,
of saying are, rather than is.
Uh, they make this kind of error more often than you would like, uh,
especially if you have multiple of these distracting words such as books, uh,
in between, uh, the word you're trying to predict
and the true word that you should be, uh, referring to.
Okay, any questions on this? All right, moving on.
So, we briefly mentioned that exploding gradients, uh, is a problem.
So, I'm briefly going to justify why is exploding gradients a problem,
and why does it, uh, what does it look like?
[NOISE] So, the reason why exploding gradients are a problem,
is if you remember this is how SGD works.
Uh, we say that the new parameters of the model,
which we represent by Theta,
is equal to the old premises,
and then you take some step in the direction of
negative gradients because you're trying to minimize the loss of J.
So, the problem is if your gradient gets really big, uh,
then your SGD update step is going to become really big too.
So, you're going to be taking a very big step,
and you're going to be drastically changing your model parameters, Theta.
And this means that you can end up with some bad updates.
We end up taking too large a step.
And we're changing the parameters too much.
And this means that, uh,
we kind of take a big step,
and we end up in some, uh,
area where the parameters are actually very bad.
Uh, with example the- for example,
they might have a much larger loss than they had before.
So, in the worst case,
this can often manifest as seeing, uh,
infinities or NaNs, not a number in your network when you're training it in practice.
[NOISE] So, this can happen because if you take such a big step
that maybe you update your parameters so much that now they're infinity,
or minus infinity, something like that,
then you're gonna have all of these infinities within your activations as well,
and then all of your losses are going to be infinity,
and the whole thing just isn't going to work, at all.
So, it's very annoying when this happens,
and unfortunately it happens, uh, fairly often.
And if it does then you have to essentially
restart training from some earlier checkpoint before you
got the NaNs and the infinities because there's
no kind of salvaging it from its new state.
[NOISE] So, what's the solution to this exploding gradient problem?
[NOISE] Uh, the solution is actually pretty
simple and it's this technique called gradient clipping.
So, the main idea of gradient clipping,
[NOISE] is that if the norm of your gradient is
greater than some threshold and the threshold is a hyperparameter that you choose.
uh, then you want to scale down that gradient,
um, before you apply the SGD update.
So, the intuition is yo- you're still gonna take a step in the same direction.
But you're gonna make sure that it's a smaller step.
[NOISE] So, here, um,
I've got a screenshot of some pseudocode from, uh,
the related paper that, uh,
proposed gradient clipping, or at least some version of gradient clipping.
[NOISE] And, um, it's pretty simple as you can see.
Uh, g hat is the vector which is the, uh,
derivative of the error with respect to the premises,
and it's saying that if the norm of
this gradient is greater than the threshold's, then you just scale it down.
But the important thing to note is that it's still pointing in the same direction,
it's just a smaller step.
So, here's a picture to show how that might work out in practice.
And, uh, this is a diagram from the, uh,
deep learning textbook which is also linked on [NOISE] the website.
So, what's going on here, is that, uh,
the picture here is the loss surface of a simple RNN.
So, they made a very simple RNN that instead of having, uh,
a sequence of vectors as the hidden states,
it just suppose that each hidden state is simply just a single scalar.
So, this means that instead of having a weight matrix, w,
and the bias vector, b,
you have a scalar w and a scalar b.
So, that's why in the picture, you just have this like two-dimensional parameter space.
And then the, the z-axis is your, is your loss.
So here, high loss is,
is bad and low loss is good in what you're trying to get.
So, uh, here in this picture,
you've got this kind of cliff, right, where you have this very steep cliff face,
uh, where the loss changes very quickly.
[NOISE] And this cliff is really dangerous because it has steep, steep gradients.
And you might be in danger of taking a really big,
[NOISE] uh, update step because you're on the area with a really steep gradient.
[NOISE] So, on the left,
you've got a possible scenario of what might happen if you don't have gradient clipping.
[NOISE] So, on the left, uh,
you can see that you start kind of at the bottom of the cliff,
and you have a f- a si- a few small updates.
And then, in particular makes a bad update because you
see there's a small kind of dip before it goes off the cliff.
So, th- the true local minimum,
the optimal you're trying to get to is that the bottom of that small kind of ditch.
And, um, it starts off kind of near the edge of that ditch,
and then there's a negative gradient going into it.
But unfortunately, the, the update kind of overshoots,
and it ends up going a long way off the cliff.
So now, it's in this bad situation where it's taken a bad update,
and now it's got a much bigger loss than it had [NOISE] before.
So now that it's on the cliff.
Again it, it measures the gradient,
and the gradient is very steep, right?
The gradient is very large.
So, when it takes a, uh,
update with respect to that gradient,
then because the gradient is so big,
it takes a really huge step.
And that's, um, the, the one to the right.
You can see the step going to the right.
So, that's also a very bad update because it's just throwing
it really far to some probably fairly random,
uh, configuration of w and b.
So, on the left, you can see what can go wrong if you're taking
these really big steps because you were in areas with a very steep gradient.
So, by contrast on the right,
you can see what might happen if you do have a gradient clipping.
[NOISE] [NOISE] And, um, it's much less drastic, right?
You've got a similar kind of pattern where it takes a few steps into the ditch,
and then ends up going off the cliff a little bit,
but not too much because the gradient was clipped.
And then, it's on the cliff and there's again a really steep gradient,
but it doesn't take such a big step because again the gradient was clipped,
so that it kind of comes back down.
So, you can see that plausibly by using this gradient clipping method,
you've got a, a kind of safer update rule,
where you're not gonna take any,
any big crazy steps and you're more likely to kind of find the,
the true minimum which is at the bottom of the ditch.
[NOISE] I think there was a question earlier.
Was there a question over here? [NOISE]
I just want to see the value. [NOISE] [NOISE]
Okay. Anyone else?
[NOISE]
Yeah?
[NOISE] [inaudible]
So, the question is, in assignment three,
y- you saw the atom optimization algorithm which, uh,
has this thing called momentum,
which essentially says that kind of like physical momentum in,
in the real world, that if you've been traveling in the same direction for a while,
then you can take bigger steps,
I think, and if you've recently kind of changed direction,
then you should take smaller steps.
And I think there's another element as well, where you divide by some factor.
[NOISE] So, it is a similar kind of idea.
I suppose it's a different criterion, right?
So, what they both have in common is it's a kind of criterion for when to
scale up or scale down the size of your update step.
Um, and I think they're based on different notions
of when should you take bigger steps and when should you take smaller steps.
When should you be cautious or less cautious?
So, I guess here the criterion is different.
It's kind of a simple criterion saying, like if it's really steep,
then be careful. Yeah. Another question?
Uh, so the [inaudible]. [NOISE]
Okay. So the question is,
is this similar to regularization of some kind, right?
So, I suppose, yeah, there is- there are some things in common.
Say for, example, L2 regularization says that you want, for example,
your weight matrices to have a small L2 norm, right?
And the idea is that you're trying to prevent
your model from over-fitting the data by, um,
having some kind of constraint that says you have to keep your weights fairly simple,
that is keep them, you know, small.
So, I suppose the relationship is that here we're
saying that we don't want the norm of the gradients to be too big.
Ah, I don't know if this is related to overfitting.
Um, I guess I have to think more carefully about that,
but I guess it's a similar kind of constraint that you're placing.
Okay. I'm gonna move on for now.
Uh, so we've talked
about how you might fix the exploding gradient problem with gradient clipping,
but we haven't talked about how we might fix the vanishing gradient problem.
So, um, to recap,
I think one way to characterize the problem with the- the vanishing gradients in RNNs is
that it's too difficult for the RNN to learn to preserve information over many timesteps.
So, in our example with printing
the tickets and re- remembering that it's the tickets that she wants to print,
you could think of it as it's hard for the RNN language model to correctly
predict tickets because in a way, it's too hard for the RNN language model to,
uh, learn to retain the tickets information and use it later.
So, um, if you look at the equation
for vanilla RNNs and how we compute the hidden state, uh,
based on the previous hidden state and- and the inputs,
you can see that the hidden state is in a way constantly being rewritten.
It's always computed based on these, uh,
linear transformations and the,
you know, the non-linearity.
So, it's not all that easy to
preserve the information from one hidden state to the other,
in particular, because we are putting it through this non-linearity function.
So, this motivates us to ask what about an RNN with some kind of separate memory?
If we have some kind of separate place to store information that we want to use later,
then would this make it easier for our RNN
to learn to preserve information over many timesteps?
So, this is the motivating idea behind LSTMs or Long Short-Term Memory RNNs.
So, the idea here is that an LSTM is a type of RNN and it was proposed back in, uh, 1997.
And the idea is that this is, uh,
this was proposed as an explicit solution to the vanishing gradients problem.
[NOISE] So, one of the main differences here is
that on each step T instead of just having a hidden state h_t,
we have both the hidden state h_t and the cell state which we denote c_t.
And both of these are vectors of some same length,
n, and the idea there is that the cell is meant to
sto- store our long-term information that, that's on memory units.
Another super important thing is that the LSTM can
erase and write [NOISE] and read information from the cell.
So, you kind of think of this a bit like memory in a computer,
in that you can do these operations, reading and writing and erasing,
um, and that's how you're gonna keep your information.
[NOISE].
Another super important thing is that the way the LSTM decides,
whether it wants to erase, write, read,
information and decide how much and which information,
uh, that's all controlled by these [NOISE] gates.
So, the idea is [NOISE] that the gates are themselves also vectors of length n,
and the idea there is that on each timestep,
each element of these gates which are vectors are somewhere between zero and one.
So here, uh, one represents an open gate and zero represents a closed gate,
and you can have values anywhere in between.
So, the overall idea, which we're gonna firm up on the next slide,
but the overall idea is that if the gate is open,
that represents some kind of information being passed through,
and if the gate is closed,
it [NOISE] means that information does not pass through.
Okay. So, the last really important thing is that the gates are dynamic.
They're not just set at some constant value for the whole sequence.
[NOISE] Um, they're dynamic,
which means that they're different on each timestep T,
and the value that is the decision of whether they're open or closed and in which ways,
[NOISE] um, that is computed based on the current context.
Okay. So here's, um,
here's the- the equations for the LSTM which might make it clearer.
So, uh, suppose we have some sequence of i- inputs x_t and we
want to compute a sequence of hidden state h_t and cell states c_t.
So, this is what happens on timestep t. Uh,
this process equation shows you the three gates that I talked about before.
So, the first one is called the Forget Gates.
And the idea is that this one is controlling what is kept versus what is forgotten,
um, from the previous cell state, the previous memory.
And you can see that this forget gate is computed based on, uh,
the previous hidden state h_t minus one and the current input x_t.
Um, so that's what I meant when I said that it's
dynamic and it's computed based on the- the current context.
[NOISE] Um, you can also see that it's computed using,
uh, the sigmoid function,
which means that it is somewhere between zero and one.
Okay. The next gate is called the input gate,
and this one controls what parts of the new cell contents are written to the cell.
So, the idea there is that you have this- this memory cell and this is kind of, um,
controlling like ho- how and what you get to write to the memory cell.
Okay. And the last one is called the upper gate.
So, this one is controlling, uh,
what parts of the cell are outputs to the hidden state,
[NOISE] so you could view this as kind of like the read function, right?
We're going to read some information from
our memory cell and that's gonna get put into our hidden states,
and this gate is gonna control that.
[NOISE] Okay.
[NOISE] Uh, yeah, that's just the sigmoid function as we noted before.
All right. So, the next set of equation shows how we use these gates.
[NOISE] So, the first line, uh,
you could regard this, uh,
c_tilde as the new [NOISE] cell content.
So, uh, this is the new content that you want to write to the cell,
[NOISE] and this is also computed based on, uh,
your previous hidden state and your current inputs,
and this goes through your tan h non-linearity.
So, uh, this is kind of the- the main contents that
you are computing based on the context and you want to write this into memory.
So, on the next line what's happening is that we're going to use
the forget gate to selectively forget some of the information from the previous,
[NOISE] uh, memory cell.
And you can see that we're doing these element-wise products,
that's what the little circle is.
So, the idea is that if you remember that f_t is
a vector full of values between zero and one,
when you do an element-wise product between f_t and
the previous cell state c_t minus one,
then what you're essentially doing is you're kind of masking
out some of the information from the previous hidden state.
Sorry, no. Previous cell state.
So, when f is one,
then you're copying over the information,
but when f is zero, then you're getting rid of that information,
you are erasing it or forgetting it.
Okay. And then the other half of this equation,
um, i_t times c tilde t, uh,
that's the input gate controlling
which parts of the new cell contents are gonna get written,
written to the, to the cell.
Okay. And then the last thing we do is we, uh,
pass the cell through a tan h,
that's just adding another non-linearity,
and then you pass that through
the output gates and that gives you [NOISE] the hidden state.
So, in LSTMs, we often think of the hidden states as being,
uh, like the outputs of the RNN.
And the reason for this is that you kind of view
the cell states as being this kind of
internal memory that's not generally accessible to the outside,
but the hidden states are the parts that you're
gonna pa- pass on to the next part of the model.
So, that's why we view it as kind of like the output of the model.
[NOISE] Uh, and this is, yeah,
x just to remind the- there is- circles are
element-wise products and that's how we apply the gates.
Uh, did anyone have any questions about this?
[NOISE].
Okay. [NOISE] Um, so as a reminder,
all of these are vectors of some same length n.
[NOISE] Okay.
So, some people learn better from diagrams than equations,
and here's a diagram presentation of the same idea.
So, this is a really nice diagram from a blog post,
uh, by Chris Olah about LSTMs,
and that was a good place to start if you want to
get an intuitive understanding of what LSTMs are.
So, in this diagram, uh,
the green boxes represent timesteps,
um, and let's zoom in on the middle one and see what's happening here.
So, within one timestep,
you can see that this diagram is showing exactly the same thing as
those six equations showed on the previous slide.
So, uh, the first thing we do is we use the, uh, the current input x_t,
which is at the bottom and the previous hidden state h_t minus the one on the left,
and we can use that to compute the forget gate.
[NOISE] And you can see f_t is on that arrow there.
And then you apply the forget gate to the previous, uh, cell,
and that's the same thing as forgetting some of the- the cell content from last time.
[NOISE] Okay.
And then after that, you can compute the input gate, uh,
and that's computed in much the same way as the forget gate.
And then you use the input gate to decide which parts of this,
uh, new cell content get written to the cell,
and that gives you the cell c_t.
So, here you can see that you computed the impu ga- input gates and
the new content and then you use that to gate that and write it to the cell.
So, now we've got our new cell c_t,
and then the last things we need to do is to compute our new output gate, that's o_t.
And then lastly, use the output gate to select which parts of
the cell contents you're gonna read and put in the new hidden state h_t.
So, that's, that's, uh, that's
the same thing as the equations we saw on the previous slide.
Okay. So, that's LSTMs.
Um, is there a question?
What's the importance [NOISE] [inaudible]
The question is, why are we applying a tan h
on the very last equation on this, on this slide?
Why we're planning a tan h to the cell before applying the output gate?
Let's see. Um.
Yeah. So, your question is, the- the cell,
the new cell content already went through a tan h. Um, I'm not sure.
So, I suppose a- a- a general answer is that it must
be giving some kind of more expressivity in some way,
and that it's not just applying
tan h's sequentially because you do have the gates in between.
Um, so I suppose there must be a reason,
kind of similarly to when you apply- apply
a linear layer you won't have a non-linearity before the next linear layer.
I suppose maybe we're viewing these cases as a kind of linear layer?
I'm not sure. I'll look it up.
[NOISE] Okay.
So, uh, that's LSTMs.
And, um, re- if you recall,
we were- oh, question?
Yeah. Why is it that in the forget gate,
you don't look at the previous cell state but you just look at the new hidden state?
Like it seems like if you're this- instead of
deciding what to forget from the cell state, you should look at it.
So the question is, why is the forget gate
computed only for the previous hidden state and the current input,
why is it not computed based on ct minus one itself, right?
Because surely you want to look at the thing to figure
out whether you want to forget it or not?
Um, that's a pretty good question.
Uh, so, I suppose one reason why you might think that this- this works fine is that
the LSTM might be learning a general algorithm
for where it stores different types of information in the cell, right?
So, maybe it's learning that in this particular position in the cell,
I learn information about this particular semantic thing and then in this situation,
I want to use that or not use that, forget it or keep it.
But, yeah, I haven't entirely convinced myself why you don't want to
look at the contents of the cell itself in order to decide.
I suppose another thing to notice is that ht minus one was read from ct minus one.
So, I suppose there is some information there but not necessarily all of the information.
Ah, yeah.
I'm not sure, that's another thing I need to look up I guess.
[NOISE] Any other questions?
Okay. Ah, so, that's LSTMs and,
um, LSTMs were introduced to try to solve the vanishing gradient problem.
So, the question is, ah,
how exactly is this architecture making the vanishing gradient problem any better?
So, you could, ah, see that the LSTM architecture
actually makes it easier for RNNs to preserve information over many time steps.
So, while it w as kind of difficult for
the vanilla RNN to preserve the information over all of the hidden states,
there's actually a fairly easy strategy that makes
it simple for the LSTM to preserve the information.
So, namely, if the forget gate is set to remember everything on every step, um,
that's a fairly simple strategy that will ensure that
the information in the cell is going to be preserved indefinitely over many time steps.
So, I don't know if that's actually a good strategy for whatever task you're trying to do,
but my point is that there is at least, um,
a fairly straightforward way for the LSTM to keep the information over many steps.
And as we noted that's relatively harder for the vanilla RNN to do.
So, you can think of this as the key reason why LSTMs are more able,
ah, to preserve the information
and thus are more robust to the vanishing gradient problem.
Ah, however, I think you should still know that LSTMs don't
necessarily guarantee that we don't have a vanishing or exploding gradient problem.
You could still have that problem,
but the thing to remember is that it's easier to avoid it anyway.
Okay. So, um, LSTMs, ah,
have been shown to be more robust to the vanishing gradient problem,
ah but I'm going to tell you a little about how they've
actually been more successful in real life. You have a question?
Yeah, [inaudible]
Okay. So it's a great question.
The question is, why is it that just because you
have these LSTM defined forward equations,
why do you not have the vanishing gradient problem?
Why does the- the logic about, ah,
the chain rule kind of getting smaller and smaller or bigger and bigger not apply?
So, I think the key here is that, um,
in the vanilla RNN,
the hidden states are kind of like a bottleneck, right?
Like all gradients must pass through them.
So, if that gradient is small then,
all downstream gradients will be small,
whereas here you could regard the cell as being kind of like
a shortcut connection at least in
the case where the forget gate is set to remember things,
um, then that's kind of like a shortcut connection where
the cell will stay the same if you have the forget gate set to remember things.
So, if the cell is staying mostly the same,
then you are not going to be,
ah, having the vanishing gradient via the cell.
So, that means that to get a connection from
the gradient of something in the future with respect to something in the past,
there is a potential route for the gradient to
go via the cell that doesn't necessarily vanish.
So in that, I have one more question.
Um-uh.
Since we have a shortcut [inaudible]
So I think the question was how do you check that your gradients are correct given that
there are now multiple routes for information to travel?
Right.
So, I suppose this somewhat relates to what we talked about last time with
the multivariable chain rule about what is
the derivative of the loss with respect to a repeated weight matrix and we saw that,
if there are multiple routes then
the multivariable chain rule says that you add up the gradients.
So, if your question is how do you do the calculus correctly and make sure it's correct,
I guess you just kind of apply
the multi-variable chain rule and it's more
complicated than assessing with the LSTMs.
Ah if you're using PyTorch 14 you do not have to do that yourself,
if you're going to implement it yourself then,
you might have a more difficult time.
Um, yeah. So, I guess, yeah.
Okay. All right, so, what do we get to. All right.
So, let's talk about LSTMs and how they work in the- in the real world.
So, in the pretty recent past,
2013-2015 um LSTM started achieving a lot of state of
the art results on a variety of different tasks including for example,
handwriting recognition, speech recognition,
machine translation, parsing, image captioning.
So, over this period,
LSTMs became the dominant approach in a lot of
these application areas because they worked convincingly a lot better than vanilla RNNs.
However, today in 2019,
things changed pretty fast in deep learning.
So, other approaches for example,
transformers which you're going to learn about later in the class.
Ah, in some of these application areas,
they seem to have become,
ah, the dominant approach.
So, to look into this,
I had a look at WMT which is a machine translation conference and
also competition where people submit their MT systems to be evaluated.
And I looked at the report,
the summary report for WMT 2016 and in this report,
I did a quick Ctrl+F,
and I found the word RNN appeared 44 times.
So, it seems that most people entering this competition were building
their MT systems based on RNNs and in particular LSTMs.
And then I looked at the report from 2018,
just two years later and I found that the RNN,
the word RNN only appeared nine times and the word transformer appeared 63 times,
and in fact the organizers noted that everyone,
well, most people seem to using transformers now.
So um, this shows that things change pretty fast in deep learning.
The thing that was hot and new just a few years ago um,
is- is now being passed by perhaps by other kinds of approaches.
So, you're going to learn more about transformers
later but I guess that gives you a kind of
idea of where LSTMs are currently in applications.
Okay. So, the second kind of RNN we're going to learn about is gated recurrent units.
So, these fortunately are simpler than LSTMs,
in fact that was the motivation for them being proposed.
They were proposed in 2014 as a way to try to retain
the strengths of LSTMs by getting rid of any unnecessary complexities.
So, in a GRU,
we don't have a cell state.
We again just have a hidden state.
But the thing it has in ah in common with LSTMs is that we're going to be
using gates to control the flow of information.
So, here are the equations for GRU.
We start off with two gates.
So the first gate is called the update gate and this
controls what parts of the hidden states are going to be updated versus preserved.
So, you can kind of view this as playing
the role of both the forget gate and the input gate in
the LSTM and it's computed in much the same way as the gates in the LSTM were.
The second gate is called the reset gate rt,
and this gate is controlling which parts of
the previous hidden state are going to be used to compute new contents.
So, you can think of the- the reset gate as kind of selecting
which parts of the previous hidden states are useful versus not useful.
So, it's going to discard some things and select some other things.
Okay. So, here's how those gates get used.
Um, h tilde here.
This is you can think of it as the new hidden state contents and what's
going on in that equation is that we are applying
the reset gate to the previous hidden state ht minus
one um and then putting all of that through some linear transformations and
a tan H and then this gives us the new content
which we want to write to the hidden cell.
And then lastly our new hidden cell is going to be a combination
of ah this new content and the previous hidden state.
So, the important thing to notice here is that we have this one minus u and u term.
So um, it's kind of like a balance right?
U is ah is setting the balance between
preserving things from the previous hidden state versus writing new stuff.
So, whereas in the LSTM,
those were two completely separate gates that could be whatever value.
Here we have this constraint that U is being uh, balanced.
So, if you have more of one, you have to have less of the other.
So, this is one way in which the creators of the GRU sought to make LSTMs more simple.
Was by having a single gate play both of these roles.
Okay. So, that's GRUs and I think it's a little less obvious just looking at it.
Why GRUs help the vanishing gradients problem because there is no explicit ah memory
cell, like there is in LSTMs.
So, I think the way to look at this here is um GRUs,
you can view this as also being a solution to
the vanishing gradient problem because like LSTMs,
GRUs make it easier to retain information ah long-term.
So, for example here,
if the update gate ut is set to zero,
then we're going to be ah keeping the hidden state the same on every step.
And again that's maybe not a good idea but at least that is a strategy you can easily
do in order to retain information over long distances.
So that's kind of like- like the same explanation of how GRUs make it
potentially easier for RNNs to retain information long-term.
Okay. So, we've learned about these two different types of RNNs. Yes.
[inaudible]
I think the question was,
if we view the two gates in the GRU, as being, uh,
a precise, um, analogy to the gates in the LSTM or are they more of a fuzzy analogy.
I'd say probably more of a fuzzy analogy
because there are other changes going on in here, like,
for example, the fact that there's no separate, um,
memory cell, it means they're not performing exactly the same functions.
Yeah. Okay. So, we've learned about LSTMs and GRUs which are both,
um, more complicated forms of RNNs,
more complicated than Vanilla RNNs.
And they are both,
uh, more robust to the vanishing gradient problem.
So, um, it would be useful to know which of these should we be using in practice?
Which one is more successful,
the LSTM or GRU?
Uh, so, I- I did a little reading and it looks like researchers have
proposed a lot of different types of gated RNNs.
So, it's not just GRUs and LSTMs,
there's many other papers with lots of other different variants.
Uh, but these are definitely the two that are most widely used.
And, ah, you can probably say that the biggest difference between the two, um,
for sure is the fact that GRUs are simpler
and quicker to compute and they have fewer parameters.
So, this makes an actual practical difference to you as, uh,
a deep learning practitioner because if you build your net based on GRUs,
then it's gonna be faster to run forwards and,
you know, faster to train and so on.
So, other than that, there appears to be
no very conclusive evidence that one of these LSTM or GRUs,
uh, is consistently outperforming the other on lots of different tasks.
Uh, it seems that often, uh,
sometimes GRUs do perform as well as LSTMs,
but there are cases where one of them performs better than the other.
So, as a rule of thumb,
it seems like LSTM is often a good default choice to start with, uh,
especially if your data has
particularly long dependencies because there's evidence to think
that LSTMs might be slightly better at keeping information over very long distances.
And also, if you have a lot of training data,
you might think that LSTMs are a better choice because they
have more parameters which means that,
um, maybe you need more train data to learn them.
So, a rule of thumb is that maybe you want to start with LSTMs
and if you're happy with their performance and you're
happy with how long it takes to train, then you stick with that.
But if you feel like you need it to be more efficient,
then maybe you should switch to GRUs and see how that goes with the performance
and if it's faster. All right.
So, um, we've talked so far about how
the vanishing/exploding gradients are a problem that occur a lot in RNNs.
But, um, the question is,
is it only an RNN problem?
Does this occur in other kinds of neural networks as well?
And the answer is,
uh, no, it's not just an RNN problem.
In fact, vanishing and exploding gradients are a
pretty significant problem for
most neural architecture such as feed-forward and convolutional,
especially when they're deep.
And this is a really serious problem because there's no point having
a really cool neural architecture if you can't learn it efficiently because of the,
uh, vanishing gradient problem.
So, in particular, uh, in these feed-forward and convolutional networks, uh,
you often have a gradient becoming vanishingly
small over back-propagation, uh, because of the Chain Rule,
because of this multiplying by
all these different intermediate gradients or
sometimes due to your choice of non-linearity function.
So, if this happens, this means that your- the lower layers of your, let's say,
convolutional or feed-forward network,
they have a much smaller,
uh, gradient than the high levels.
And this means that they get changed very slowly during SGD.
So, this means that, overall,
your network is very slow to train because when you take updates,
then your lower layers are changing very slowly.
So, one solution, uh,
the kind of like a family of solutions that we've seen in
recent years is that there's been lots of
proposals for new types of deep feed-forward or convolutional architectures.
And what they do is, they add more direct connections in the network.
And the- the idea,
kind of as we talked about before,
is that if you add all of these direct connections between layers,
like maybe not just adjacent layers but further apart layers,
then it makes it much easier for the gradients to flow,
and you're going to find it easier to train your network overall.
So, I'm going to show you some examples of these in
particular because it's fairly likely you're going to
run into these kinds of architectures when you're doing your projects and reading papers.
So, one example is something called residual connections or,
uh, the network itself is sometimes referred to as ResNet.
And here we've got a figure from the related paper.
So, what's going on in this diagram is that you have, uh,
the usual kind of you've got weight layer and
a non-linearity which is ReLU, and another weight layer.
So, if you regard that function as being f of x, ah,
what they're doing is instead of just, ah,
transforming x to f of x,
the- they're taking f of x plus x.
So they're adding this identity skip connection where
the input x is skipped over those two layers and then,
um, added to the output of the two layers.
So, the reason why this is a good idea,
uh, also known as skip connections,
is that the identity connection is going to preserve information by default, right?
So, if you imagine perhaps if you, um,
initialize your network and you
initialize your weight layers to have small random values,
then if they're small and kind of close to zero,
then you're going to have something like a noisy identity function, right?
So you're going to be preserving information by default through all of your layers.
And if you have a very deep network,
that means that even often many,
um, many layers, you're still gonna have something like your original input.
So, uh, the- the people who wrote this paper, they show that, uh,
if you don't have something like skip connections then
actually you can find that deep layers- uh,
deep networks perform worse on some tasks than shallow networks.
Not because they're not expressive enough,
but because they're too difficult to learn.
So, when you attempt to learn deep networks,
it just doesn't learn effectively and you end up
getting worse performance in the shallow network.
So, the people who wrote this paper,
they show that when they add these skip connections,
then they made the deep networks, uh,
much more effective and they managed to get good performance.
Uh, so another example which kinda take this- this idea
further is something called dense connections or DenseNet.
And again, this was, uh,
something proposed I think in a feed-forward or or convolutional setting.
And, ah, it's just kind of the same as skip connections but except ,
um, connects everything to everything.
So, add more of these skip connections kind of
from all layers to all layers and they showed that this,
uh, performs even better.
And, uh, the last one I want to talk about which I don't have a picture
for is something called highway connections.
So, this is similar to the residual or skip connections.
Ah, but the idea is that instead of just adding your x,
adding your identity, uh, connection,
the idea is that you're gonna have a gate that controls the balance between, um,
adding the identity and computing, ah, the transformation.
So, instead of f of x plus x, you're gonna have, you know,
gate times f of x plus, you know,
one minus gate times x, something like that.
Um, so, this work was actually inspired by LSTMs,
but instead of applying it to a recurrent setting,
they were seeking to apply it to a feed-forward setting.
Okay. I'm gonna keep going for now.
Um. So, overall the question was,
you know, how much uh,
vanishing and exploding gradients a problem outside of the setting of RNNs?
And I think uh, the important takeaway is that it is a big problem
but you should notice that it is particularly a problem for RNNs.
So, um, RNNs are particularly unstable and
this is essentially due to the repeated multiplication by the same weight matrix.
If you remember from last time, um,
the characteristic thing about RNNs that makes them recurrent is
the fact that you are applying the same weight matrix over and over again.
So, this is actually the core reason
why they are so prone to the vanishing and exploding gradients,
and ah, you can see some more information about that in the paper.
Okay. So, I know there's been a lot of dense information today,
a lot of um, lot of notation.
So, here's a recap, if I've lost you at any point.
Now's a good time to jump back in because it's gonna
get a little easier to understand perhaps.
So, okay, recap. What have we learned about today?
Um, the first thing we learned about was the vanishing gradient problem.
We learned uh, what it is.
We learned why it happens and we saw why it's bad for RNNs,
for example, RNN language models.
Ah, and we also learned about LSTMs and GRUs which are
more complicated RNNs and they use gates to control the flow of information.
And by doing that, they are more resilient to the vanishing gradient problem.
Okay. So, if the remainder of this lecture,
I think we've got about 20 minutes left,
ah, we're going to be learning about two more advanced type of RNNs.
So, the first one is bidirectional RNNs and that's all
about information flowing left to right and right to left.
And then we're also going to learn about
multi-layer RNNs which is when you apply multiple RNNs on top of each other.
So, I'd say that both of these are pretty simple conceptually.
Um, so it shouldn't be too hard to understand.
All right, so let's start with bidirectional RNNs.
Um, this is a picture which you saw at the end of last lecture.
So, if you remember,
sentiment classification is the task when you have
some kind of input sentence such as the movie was
terribly exciting and you want to classify this as a positive or negative sentiment.
So, in this example, it should be seen as positive sentiment.
So, um, this is an example of how you might try to
solve sentiment classification using a fairly simple RNN model.
Ah, here we're using the RNN as a kind of encoder of
the sentence and the hidden states represent the sentence.
And we'll do some kind of combination of the hidden states to compute uh,
what we think the sentiment is.
So, my question is, if we look at let's say,
the hidden state that corresponds to the word terribly and we're regarding
this hidden state as a representation of the word
terribly in the context of the sentence.
So, for this reason we- we sometimes call hidden states in this kind of situation
a contextual representation because the idea is that it's
a representation of the word terribly in the context of the sentence.
So, thing to think about here is that this contextual representation,
it only contains information about the left context.
So, for terribly, the left context is the words um,
the movie was and this hidden state the one that's got
a blue box around it has only seen information to the left.
It hasn't seen the information of the words exciting or exclamation mark.
So, what we're asking is what about the right context?
The right context of terribly is- is what exciting and the exclamation mark.
And do we think that the right context is useful here?
Do we think that this is something we want to know about?
And I would argue that in this example,
it is actually kind of important because we've got the phrase terribly exciting.
And if you look at the word terribly in isolation,
terrible or terribly usually means something bad, right?
But terribly exciting, you can mean something good because it just means very exciting.
So, if you know about the right context,
the word exciting then this might quite significantly
modify your perception of the meaning of the word
terribly in the context of the sentence.
And especially given that we're trying to do sentiment classification,
this is- this is kind of important.
So this motivates why you might want to have information
from both the left and the right when you're making your representations.
Ah, if when you were a kid,
your parents told you to look both ways before you cross the street.
You might regard it as the same kind of idea that there's
useful information to the left and the right that
you'd like to know about ah, before you do anything.
Okay. So that's the motivation and um,
here is how a bidirectional RNN might work in practice.
I have a kind of accidentally festive color scheme here.
And so the idea is that you have two RNNs going on.
You have the forward RNN as before that encodes the sentence left to right.
And then separately, you also have a backwards RNN.
And this has completely separate weights to the forward RNN.
So, the backward RNN is just doing the same thing
except that it's encoding the sequence from right to left.
So, each of the hidden states is computed based on the one to the right.
And then finally, you just take the hidden states from
the two RNNs and then you concatenate them together and you've got your uh,
your final kind of representations.
So, in particular, if we now think about
this contextual representation of the word terribly in the context,
um, this- this vector has information from both the left and the right, right?
Because you had the forwards and backwards RNNs that
respectively had information from both left and right.
So the idea is that these concatenated hidden states,
those can be regarded as kind of like the outputs of the bidirectional RNN.
Like if you're going to use these hidden states for
any kind of further computation, then ah,
it's these concatenated hidden states that you are going to be
passing on to the next part of the network.
Um, here- here are the equations that just say the same thing.
So, you have your forward RNN and here we've got ah,
a notation that you might not have seen before
this kind of notation where it says RNN and then in brackets,
the previous hidden state and the input that's simply saying that you know,
HT is computed from the previous hidden state and the input.
And RNN forward could be a vanilla or a GRU or an LSTM.
It doesn't really matter, we're looking at it abstractly.
So, you have these two separate RNNs,
RNN forwards and RNN backwards and generally, these have separate weights.
Although I have seen some papers where they have shared weights.
So, it seems that sometimes that does work better,
perhaps maybe when you have enough training data.
And then finally, we regard these concatenated hidden states which you might just
notice ht as being like the hidden state of the bidirectional RNN.
So, um, the previous diagram is pretty unwieldy.
So here's a simplified diagram.
And this is probably the only kind of diagram you're going to
see from now on to denote bidirectional RNNs.
Um, so, what we've done here is you've just
made all of the horizontal arrows go left and right ah,
to represent that this is a bidirectional RNN.
So, the other thing you should assume is that the hidden states depicted here, you know,
these red- red trying- red rectangles with the dots.
You can assume that those are the concatenated forwards,
backwards hidden states from the bidirectional RNN.
[inaudible]
Okay. So the question is, um,
would you train your forwards and backwards RNNs kind of separately,
um, on some kind of task and then
maybe concatenate them together once they're separately trained networks,
or would you train them all together?
Um, it seems to me that it's much more common to train them together,
but I don- I don't think I've heard of anyone training them separately.
Uh, so yeah, it seems like the standard practice is usually
to train them together. Does that make sense?
[inaudible].
So, let's suppose that we were trying to build
a sentiment classification system using the bidirectional RNN.
Then what you do, which maybe I should have pictured but I didn't have space, is uh,
you would do the same thing that you were doing with the unidirectional RNN, uh,
which was, let's say an element y is min or max,
um, to get your sentence encoding.
Maybe you just do that but over the concatenated, um, n states.
Okay. So, an important thing to note is that, uh,
when talking about applying bidirectional RNNs,
we've assumed that we actually have access to the entire input sequence.
So, we assume that we have the full sentence,
uh, the movie was very exciting, and,
uh, that, that was a necessary assumption in order to
be able to run the forwards and the backwards RNN, right?
Um, so there are some situations where you can't assume this.
Like, for example, in Language Modeling,
you only have access to the left context kind of by definition of the task.
You only know the words that have come so far.
You don't know what's coming next.
So, you can't use a bidirectional RNN, uh,
to do Language Modeling, uh,
in the way that we've depicted here because uh,
you don't have the full sequence.
However, if you do have access to the entire sequence.
Uh, so, for example, if you're doing any kind of encoding
similar to the sentiment example,
uh, then bidirectionally- bidirectionality is pretty powerful.
And you should probably regard it as a good thing to do by default uh,
because it turns out that getting this information from
both the left and the right, uh,
makes it a lot easier to learn these more useful contextual representations.
So, in particular, as a preview of
something you're going to learn about later in the class, uh,
there's a model called BERT, B-E-R-T,
and that stands for Bidirectional Encoder Representations from Transformers.
And this is a pretty recently.
Like, a few months ago, uh, proposed system,
and it's this pre-trained contextual representation system.
Um, and it's heavily reliant on the idea of bidirectionality.
It turns out that the bidirectional, uh,
nature of BERT is pretty important to its success.
So, you're gonna learn more about that later,
but that's just an example of how bidirectionality can give you much
more uh, powerful contextual representations.
Okay. So the last thing we're going to talk about today is multi-layer RNNs.
Uh, so you could regard RNNs as already being deep
in some sense because you've already unrolled them over potentially very many timesteps,
and you could regard that as a kind of depth, right?
But there's another way that RNNs could be deep.
So, for example, if you applied multiple RNNs kind of one after another,
then this would be a different way to make your RNN deep,
and this is the idea between, uh,
behind a multi-layer RNN.
So, the reason why you would want to do this is because uh,
this might allow the network to compute more complex representations.
So, this is the logic betwe- behind deep networks in general.
So, if you're familiar with the idea of why
deeper is better for let's say convolutional networks,
then this is kind of the same logic.
It's saying that, uh, your lower RNNs might be computing lower-level features like,
let's suppose maybe it's keeping track of syntax,
and your higher level RNN's gonna compute higher-level features like maybe semantics.
And a note on terminology, these are sometimes called stacked RNNs.
So, this works much as you'd imagine.
So here's an example of how a multi-layer RNN might work.
Uh, if it's three layers.
So this is a unidirectional RNN,
but it could be bidirectional,
um, If you have access to the entire input sequence.
So, I guess the, the main thing is that the hidden states from one RNN layer are going to
be used as the inputs to the RNN layer that's coming next.
Um, any questions on this?
Yeah.
[inaudible].
That's a great question. So the question I think it's about the order of computation.
What order will you compute all of these hidden states in?
I suppose there's some flexibility, right?
But you could compute all of the step one ones,
like all of the V ones and then all of the movie ones,
or you could do all of RNN layer one and then all of RNN layer two.
So, it's- I think that, um, when you- you know,
call the PyTorch function to do a multi-layer RNN,
it will do all of RNN layer one, then two, then three.
That's what I think happens.
But it seems like logically,
there's no reason why you couldn't do it the other way.
Yep? [inaudible].
Yes, yes. That's a great point as well.
Um, so uh, someone pointed out that if they were bidirectional,
then you no longer have that flexibility.
You would have to do all of layer one before layer two.
Yeah, good point. Anyone else?
Okay. Uh, so mostly RNNs in practice,
um, this tends to perform pretty well,
uh, in that when I look at, um,
RNN-based systems that are doing very well on some kind of task,
they usually are some kind of multi-layer RNN, um,
but they certainly aren't as deep as
the deep convolutional or feed-forward networks you might have seen in,
for example, image tasks.
So whereas, you know, very deep convolutional networks,
I think hundreds of layers now, um,
you certainly aren't getting RNNs that are that deep.
So, for example, um,
in this paper from, uh, Google, uh,
they're doing this kind of large hyperparameter search for
neural machine translation to find which kinds of hyperparameters work well for NMT.
And in this paper, they found that um,
two to four layers was best for the encoder RNN,
and four layers was best for the decoder RNN.
Uh, you'll find out more about what encoder and decoder mean next time.
Um, but those are fairly small numbers.
Although they did find that if you add these skip
connections or these dense connections, um,
then it makes it much easier to learn some even deeper RNNs more effectively,
like, maybe up to eight layers,
but these certainly aren'tx hundreds of layers deep.
And one of the reasons why, uh,
RNNs don't tend to be nearly as deep as these other kinds of networks,
is that because as we commented before,
RNNs have to be computed, uh,
sequentially; they can't be computed in parallel.
This means that they're pretty expensive to compute.
If you have this depth in like, two-dimensions,
you have the depth over the timesteps and then the depth over the RNN layer is two,
then it beco- it becomes very,
very expensive to compute these, these RNNs.
So, that's another reason why they don't get very deep.
Uh, so again, we just mentioned transformers.
Uh, you gonna learn about transformers later.
But these, it seems, um,
can be deeper fro- from what I can tell of,
of what people are using these days.
Transformer-based networks can be pretty deep.
So, uh, but for example,
there's a 24-layer version and a 12-layer version, um,
and admittedly, that was trained by Google,
and they have a lot of computational power.
Um, but I think part of the reason why
these transformer-based networks can be quite deep,
is that they have a lot of these skipping like connections.
In fact, the whole um,
innovation of transformers is that they're built on a lot of, kind of,
skip connections. Okay, any questions?
We're almost done. Okay. All right.
So, uh, here's a summary of what we've learned today.
I know it's been a lot of information.
Um, but I think here are four practical takeaways from today that, uh,
are probably useful to you in your projects,
even if you, um,
uh, even if you
didn't find them very interesting in themselves they're probably pretty useful.
So, the first one is that LSTMs are very powerful.
They're certainly a lot powerful than,
uh, more powerful than Vanila RNNs.
Um, GRUs are also more powerful than, uh, Vanila RNNs.
Uh, and the only difference that is consistently the
same is that GRUs are faster than LSTMs.
The next one is that you should probably clip your gradients,
because if you don't clip your gradients,
you're in danger of walking off cliffs and then ending up with NaNs in your model.
Uh, the next tip is that bidirectionality is useful if you can apply it.
And, basically, anytime when you have access to the entire input sequence,
you can apply bidirectionality,
so you should probably do that by default.
And then the last tip is that multi-layer RNNs are pretty powerful.
And again, you should probably do that if you,
uh, have enough computational power to do so.
But if you're going to make your multi-layer RNN pretty deep,
then you might need skip connections.
All right. Thanks [NOISE]. 
 So welcome to the Machine [NOISE] Translation lecture,
which is kind of like a culmination [NOISE] of
this sequence of three lectures on RNNs and related topics.
So let's have a few announcements first.
Uh, the first thing is,
as you probably noticed when you came in,
we're taking attendance today.
Uh, so you need to sign in with the TAs who are outside the auditorium.
Uh, if you missed it,
don't get up now, it's fine.
There will be time to sign in after the lecture.
Uh, and then, if you have any kind of questions about
special cases with the attendance policy, uh,
you should check out a Piazza post that we put up last night with some clarifications.
[NOISE] Uh, you have the reminder that Assignment 4 content is going to be covered today.
So you're gonna have everything you need to do Assignment 4 at the end of today.
[NOISE] And do get started early because the model takes 4 hours to train.
The other announcement is that we're going [NOISE] to be sending out
our mid-quarter feedback survey sometime in the next few days probably,
uh, so please do fill it out.
You'll get 0.5% credit,
and you're also gonna help us to make the class better for the rest of the quarter.
[NOISE] Okay.
So here's the overview of what we're going to do today.
[NOISE] Uh, today, first,
we're going to introduce a new task in NLP,
which is machine translation, [NOISE] and then,
we're going to introduce a new neural architecture called sequence-to-sequence.
And the connection here is that machine translation
is a major use case of sequence-to-sequence.
[NOISE] After that, we're going to introduce a new neural technique called attention,
and this is something that improves sequence-to-sequence a lot.
Okay. So Section 1 of this is gonna be about, uh,
a bit of machine translation history, pre-neural machine translation.
[NOISE] So machine translation or MT,
uh, is the task of translating a sentence x, uh,
which we call the source language,
whatever language you're translating from,
into a sentence y, which is in another language,
which we call the target language.
Uh, so here's an example. Let's suppose x is this French sentence.
Um, [NOISE] could anyone in the audience,
a French speaker, translate this to English for us? [NOISE] [BACKGROUND] Yeah.
Um, the man is born free, and, uh, everywhere, he is in irons.
Great.
So that was something like,
the man is born free, but everywhere, he's in irons.
That was a fairly literal translation.
It's usually translated, this quote by Rousseau is usually translated as,
man is born free, but everywhere, he is in chains.
But there's an ambiguity: [NOISE] should fers be,
um, literally irons or chains?
Also, you could choose to, uh,
translate l'homme as man or maybe humankind.
Uh, so this is an example of machine translation,
and there's already, you know,
quite a few choices you can make.
[NOISE] So the beginning of machine translation as an AI task began in the early 1950s.
So, um, in particular,
there was a lot of work translating Russian to English, uh,
because the West was very interested in listening
to what the Russians were saying during the Cold War.
And we've got a fun video here,
[NOISE] which shows the state of machine translation in 1954.
[MUSIC] They haven't reckoned with
ambiguity when they set out to use computers to translate languages.
A $500,000 simple calculator,
most versatile electronic brain known,
translates Russian into English.
Instead of mathematical wizardry,
a sentence in Russian is to be fed [OVERLAPPING].
One of the first non-numerical applications of computers,
[BACKGROUND] it was hyped as the solution to
the Cold War obsession of keeping tabs on what the Russians were doing.
Claims were made that the computer would replace most human translators.
[inaudible] you're just in the experimental stage.
When you go in for full-scale production,
what will the capacity be?
We should be able to do about,
with the help of a commercial computer, uh, about one to two million words, uh,
an hour, and this will be quite an adequate speed to cope with
the whole alphabet of the Soviet Union in just a few hours' computer time a week.
When do you have to be able to achieve this feat?
If our experiments go well,
then perhaps within, uh, five years or so.
So in this video, I think there's a number of interesting things.
Uh, firstly, we can see an example of about how,
uh, AI hype is nothing new.
Even in 1954, [NOISE] they were talking
this machine translation system as if it was an electronic brain,
which I think, uh, overstates maybe how general it is.
Uh, they were also, at least some of them,
fairly optimistic that this [NOISE] machine translation system
was going to be replacing humans, uh, anytime soon.
Um, so yeah, that's, that's pretty interesting.
And, um, [NOISE] the thing is that these systems actually were mostly rule-based, uh,
by which I mean that they were mostly using
a bilingual dictionary between Russian and English,
and they were essentially mostly just looking up the Russian words, uh,
looking up their English counterparts,
and they were storing these big bilingual dictionaries on these large magnetic tapes.
Um, so certainly, it was a [NOISE] huge technical feat at the time, uh,
but they, uh, some people were probably too
optimistic [NOISE] about how quickly it would replace humans.
So jumping forward several decades in time,
uh, now I want to tell you about statistical machine translation.
So the core idea of statistical machine translation is that you're going to
learn a probabilistic model from the data in order to do the translation.
So as an example, uh, as before,
suppose that we're translating from French to English.
The idea is that you want to find the best English sentence y,
given the French sentence x,
[NOISE] and, uh, mathematically,
you can formulate this as finding argmax y of this conditional probability of y, given x,
[NOISE] and the model that you're learning is
this probability distribution P. [NOISE] So what we usually do is,
we break down this probability into,
uh, its two components using Bayes' Rule.
[NOISE] So this means that finding the y that maximizes,
uh, probability of y, given x,
is equivalent to finding the y that maximizes the probability of x,
given y, times the probability of y.
So the two components here, on the left,
we have a translation model,
and this is keeping track of how words and phrases should be translated.
Uh, so the idea is that it knows, uh, how, uh,
French words and an English word might be translated to each other or maybe small,
small phrases and chunks of words should be translated.
And this is learned from a lot of parallel data,
and I'll be telling you later how we do that.
The second compo- component P(y),
[NOISE] this is just a language model.
[NOISE] We learned about this last week.
A language model is a system that can predict the next word,
but it can also be thought of as a system [NOISE] that tells
you the probability of a sequence of words.
So here, if we're translating from French to English,
P(y) is an English language model.
[NOISE] So the idea is the,
the reason why we want to break down
this single conditiona- conditional probability distribution into the,
the pr- product of two different ones is that this is a kind of division of labor.
The idea is that instead of, uh,
a single conditional probability distribution needing to understand how to translate,
and how to write good English text,
and understand sentence structure,
and everything at once, the idea is that you separate it so that [NOISE]
the translation model on the left in blue mostly just knows about a
local translation of small chunks of words and phrases,
whereas the language model on the right more takes care of writing good English,
good sentence structure, word order, and so on.
[NOISE] So you already know
how to learn a language model [NOISE] because we learned about that last time.
You just need lots of monolingual data,
in this case, English data.
[NOISE] So I'm going to tell you more about how we would learn
this translation model that needs to be learned from parallel data.
[NOISE]
So we need a large amount of parallel data in order to learn this translation model.
And an early example of a parallel corpus,
is the Rosetta Stone.
So this is a stone that has the same text written in three different languages.
And this is a hugely important artifact
for the people who were trying to understand ancient Egyptian.
So in the 19th century,
uh, scholars discovered this stone,
and it helped them to figure out ancient Egyptian because there was
this parallel text that had the the same text in other languages that they did know.
So this is, this is a really important parallel corpus,
and if you're ever in London,
you can go to the British Museum,
and see this in person.
So the idea is that you get your parallel data.
Obviously, you need a larger amount that is on the stone,
and hopefully it shouldn't be written on a stone either.
But you can use this to learn your statistical machine translation model.
So the idea is that, you are trying to learn
this conditional probability distribution of x given y.
So what we do is we actually break this down even further.
We actually want to consider the probability of x and a given y.
Where a is the alignment.
So the idea of alignment,
is this is how the words in
the English sentence and the French sentence correspond to each other.
So I'm gonna, uh, demonstrate this by an example.
So in this example,
while we're translating the sentence 'Japan shaken by two new quakes' to French.
Then you can see there is a pretty simple one-to-one alignment here,
uh, of English words to French words,
and also they appear in the exact same order.
The only thing that doesn't conform to that is the word 'Le' in French,
which we call a spurious word because it doesn't
have a direct counterpart in the English sentence,
and that's because in English we just say,
'Japan', but in French we say, 'Le Japon'.
So alignment can be a bit more complicated than that.
For example, alignment can be many-to-one.
In this example, you have, uh,
several French words that have multiple English words that correspond to them.
So this is what we call many-to-one alignment.
Uh, it can go in the other direction too.
Alignment can be one-to-many.
So here we have a single English word implemented,
which has a one-to-many alignment because there is
a three-word French phra-phrase that corresponds to it.
So on the left and the right,
we have two ways of depicting the same alignments.
It's either a kind of chart or it can be a, a graph.
So here's another example, um,
of a one-to-many, well, sorry, right.
So we call, uh, this word implemented, that is one-to-many.
We call it a fertile word because the idea is that it has many children in the,
in the target sentence.
So in fact, there are some words which are very fertile.
Here's an example where the source sentence,
'il m'a entarte', means,
'he hit me with a pie',
and here in French,
this verb, 'entarte' means, uh, to hit someone with a pie,
[LAUGHTER]
and this word has no single word equivalent in English.
We don't have a single verb that means to hit someone with a pie.
[LAUGHTER]
Which I think that is really fun, that French has a word.
You wonder, maybe they do it so
often that they need a single word for that. I don't know.
[LAUGHTER]
So this is an example of a fertile word, right?
Because it needs to have several corresponding English words to translate it.
So we can have one-to-many, and many-to-one.
You can also have many-to-many alignments.
You could call that kind of phrase level translation, or phrase-to-phrase.
So here, uh, the English sentence says,
'The poor doesn't- don't have any money',
and here don't have any money corresponds to the French phrase,
'sont demunis', and this is a many-to-many alignment because there is
no obvious way to break down this phrase-to-phrase alignment into,
um, smaller word-to-word alignments.
Okay. So that's what alignment is.
And if you remember, we were thinking about how would you
learn this probability distribution of what the alignment is,
uh, in order to do statistical machine translation.
So the idea is that you learn probability of x and a,
given y as a combination of many factors or many features.
So you consider for example,
what's the probability of a particular word aligning to another particular word?
Like you know, this English word and this French word,
how often do they align?
But then, this also depends on for example,
what's their position in the sentence?
Like if they both appear near the end of the sentences,
then it's more likely that they align, whereas,
if one's at the beginning and one's at the end, that's less likely.
You would also consider things like, uh,
what's the probability of this particular French word having this particular fertility?
Like, what's the probability of this word having
three corresponding English words and so on?
So all of these statistics are learned from your parallel data,
and there's many other things that you would take into consideration.
So we're looking at a kind of overview of statistical machine translation today.
You're not going to understand it in full detail,
but we're understanding an overview of how it works,
because we're going to be, uh,
comparing it to neural machine translation.
Okay. So we're learning this SMT system,
and so far, we've broken it down into these two main components.
We've got the translation model,
and we've got the language model,
and we understand a little bit about how you might
learn this translation model by breaking it down into alignments.
So the question remains, how do you do the argmax over y?
How do you find your French sentence y that maximizes this probability?
So one kind of brute force solution is you could say,
"'let's enumerate every possible y."
That's kind of every possible sequence of French words,
maybe up to some length, and, uh,
we'll calculate this probability for all of them,
and it should be pretty clear that that is just a no go.
That's way too expensive,
and we're not going to be able to get anywhere with that.
So the answer for how you actually do this in practice is,
you are going to use some kind of heuristic search algorithm,
to search for the best translation, y.
Uh, but along the way, you're going to discard hypotheses that are too low probability.
So you're gonna search,
you're going to discard,
and prune the trees as you go to make sure that you're not
keeping too many hypotheses, uh, on each step.
So this process of finding your best sequence is also called decoding.
So here is an overview of how that works for SMT.
This an example where you have this German sentence that translates to,
'He does not go home',
and you can see that there is some kind of phrase-to-phrase alignment here.
So, uh, an overview of how this decoding would work in SMT,
is that you kind of consider lots of different hypotheses,
for how you might translate these individual words, uh,
and then you build it up to consider how you might translate,
uh, individual phrases, and the phrases get bigger.
So for example, you can see that on the top right,
if it's not too small, you can see that the, uh,
the German word for house, uh,
could be translated into the English word,
'house' or 'home', or 'chamber', and so on.
Uh, so we consider all of these different hypotheses,
and look into how we might put those together to
translate phrases but you don't keep all of them all the time.
You get rid of the ones that are too low probability.
So this can also be depicted as a kind of a tree,
where you are exploring different options.
You are searching through the space of options,
but then you prune the tree as you go.
So I know this is a very,
very high level, uh,
description of how decoding might walk.
And in fact, later in this lecture,
you're going to see a detailed explanation
of how this kind of decoding works for neural machine translation.
Okay. So what's our, um,
overview of statistical machine translation,
uh, was it effective?
Uh, so SMT was a huge research field,
uh, from the 1990s to about, maybe, uh, 2013.
And the best systems during this time were extremely complex.
They were extremely sophisticated and impressive systems and, uh,
SMT made the best machine translation systems in the world.
But they were very complex.
So for example, you know,
there were hundreds of important details that we haven't mentioned here at all.
There were many, many techniques to make it, uh,
more complex and more,
um, sophisticated than what I've described today.
In particular, the systems had to have many separately designed, uh, sub-components.
So we already saw how you, uh,
break down the translation model into two separate parts.
Uh, but there was, you know, many more sub-components than that,
and often they had to be learned separately.
This meant the engineers had to do a lot of feature engineering.
Uh, you have to design features to capture
the particular language phenomena that you were interested in.
So this meant that they had to require a lot of
compiling and maintaining of extra resources,
and in fact, you had to have, uh,
different resources for different languages.
So the work kind of multiplied the more languages you had.
An example of this, is you had to have,
uh, tables of equivalent phrases.
So for example if you're doing French and English translation, then, uh,
they would be collecting these phrases of, uh,
sorry these tables of phrases that they considered similar,
and those were learned from the data.
But this was a lot of information that had to be stored and maintained.
So overall, this was just a lot of human effort to maintain.
Uh, and again, yes,
you had to put more human effort in if you wanted to
learn an SMT system for a new language pair.
Okay, are there any questions here about, uh, SMT?
[NOISE] Okay.
Uh, so moving on, that's SMT.
[NOISE] Now, we're gonna move on to,
uh, section two of this lecture.
So I want to take you back to the year 2014,
for a dramatic re-enactment of what happened in
the world of machine translation research.
So in 2014, something very dramatic happened and
that thing that happened is called neural machine translation,
and [LAUGHTER] I think it looks a little
bit like this [NOISE] if I'm not being too dramatic.
So what is neural machine translation?
The idea is that NMT is a way to do
machine translation but using just a single neural network.
[NOISE] And the neural network architecture that they use
is called Sequence-to-Sequence or sometimes just called seq2seq,
uh, and involves two RNNs.
So, uh, it's called sequence-to-sequence,
because you're mapping one sequence to the other.
The source sentence [NOISE] to the target sentence and you need two RNNs,
basically to handle those two different sentences.
All right, lets look at the diagram to see what sequences-to-sequence is in detail.
So we start off with our source sentence,
and we're gonna use our example from before
il a m'entarte, which means, he hit me with a pie.
So we, uh, feed this into our encoder RNN,
and this is as you've seen before,
I've drawn a uni-directional RNN,
but this could be bi-directional.
It also could be multi-layer.
It could be vanilla, or it could be LSTM, and so on.
Uh, another thing to note is that [NOISE] we are
passing word embeddings into this encoder RNN,
but I'm just not explicitly depicting that step.
[NOISE] Okay.
So the idea of the encoder RNN is that it's going to
produce some kind of encoding of this source sentence.
So for now, let's assume that the encoding of the source sentence is going to be,
uh, the final hidden state of this encoder RNN.
So what happens next is we pass this encoding of the source sentence.
We pass it over to the decoder RNN,
which is going to translate into English.
So the decoder RNN is a language model.
In particular, it's a conditional language model,
like we talked about last time.
So it's conditional because it's going to produce the target sentence,
but conditioned on this encoding,
and the encoding is that vector that has the orange box around it.
So how does this work? Uh, we start off by feeding, uh,
the start token into the decoder, and then, uh,
we can get the first state of the decoder,
because we're using, uh,
the encoding of the source sentence as the initial hidden state for the decoder.
So then we get our first output from the decoder,
which is a probability distribution of what word might come next,
and let's suppose that we take the argmax over that,
and then that gets us the word, uh, he.
Which is in this case is correct,
because that's probably the word you should start with.
Okay, so then we just take the word,
he and then we feed it back into the decoder on the next step,
and then we do the same thing again.
We take argmax and we get a new word and we get he hit.
So the idea here is you can co- uh, continue doing this operation and in that way,
you're going to generate, uh,
your target sentence, uh,
which will be something like he hit me with a pie,
and you stop once your decoder produces the end token.
So an important thing to note here,
is that this picture is showing you what happens at test time.
This shows you how to generate text.
Uh, this isn't what happens during training.
I'll show you what happens [NOISE] during training later.
Uh, but this thing with the,
the pink dotted arrows where you feed the word back in.
This is what you do to generate text at test time.
Any questions on this? Uh, oh,
another thing I should note is that you need two separate sets of word embeddings, right?
You need word embeddings for French words, and you need English word embeddings,
so that's kind of two separate sets,
two separate vocabularies. Um, yeah.
Okay. So as a side note, uh,
this architecture called sequence-to-sequence is actually pretty versatile.
It's not just a machine translation architecture.
Uh, you can, uh, uh,
phrase quite a few NLP tasks as sequence-to-sequence tasks.
Uh, so for example a summarization is
a sequence-to-sequence task because in goes your long text and out comes your short text.
Uh, dialogue can [NOISE] be seq2seq because in
goes the previous utterance and out comes your next utterance, uh,
parsing can even be thought of as a sequence-to-sequence task,
because you could say in goes the input text and
the output parse is going to be expressed as a sequence.
This might not be the best way to do parsing but it is a way you can try.
Lastly, you could even do something like code generation.
So suppose you want to build a system that takes some kind of,
uh, natural language input,
such as sum up the numbers from 1-10 and then it outputs,
let's say some Python code that says,
sum open brackets range 10 or something like that.
Uh, so if you wanted to train,
um, an assistant to do this.
You could in a way view that as a translation task,
where you're translating from English to Python.
It's a pretty challenging translation task.
It probably requires a lot more logic than just uh, you know,
French to English [NOISE] but you can try and people have tried.
There are research papers where people have used seq2seq to do this kind of task.
Okay. So to recap, uh,
seq2seq is an example of a conditional language model.
Uh, it's a language model because the decoder
is a language model that's predicting the next target words.
But it's a conditional language model because it's also conditioning on
your source sentence which is represented by the encoding of the source sentence.
So you could look,
you could view it like this.
NMT is directly calculating the probability
of the target sentence y given the source sentence x.
So if you look at this, you see that this is just, uh,
breaking down the probability of the sequence y,
which we suppose is of length, uh,
capital T. You can break it down into the being the probability of
the first word of y given x and then the probability of the second word of y given,
uh, the words that came before, and x, and so on.
So in fact, you can see that each of the terms in this product on the right,
those are probabilities of the next target word
given all the ones so far, and also the source sentence,
and that's exactly the conditional probability that your language model produces.
So the reason I'm highlighting this is because if you remember in SMT, uh,
we didn't directly learn the translation model p of y given x,
we broke it down into,
uh, uh, smaller components.
Whereas here in NMT,
we are directly learning this model.
And this is in some ways an advantage because it's simpler to do.
You don't have to learn all of these different systems and optimize them separately.
It's, uh, kind of, simpler and easier.
So, uh, this is,
this is the model that we're learning.
Uh, the question is, how do we train this NMT system?
So hopefully, you should already have a good idea of how this would work,
given that we've already seen how you would train a language model.
But here are the details just in case.
So you get your big, uh, parallel corpus.
Uh, and then, uh,
let's say you have your sentence pair from your parallel corpus.
Uh, so this is what happens during training.
Uh, you feed your source sentence into the encoder RNN, uh,
and then you feed your target sentence into the decoder RNN,
and you're going to pass over that
final hidden state to be the initial hidden state of the decoder.
And then, uh, for every step of the decoder RNN,
you're going to produce the, uh,
probability distribution of what comes next,
which is the, the y hats.
And then from those,
you can compute your loss.
And the loss is just the same as we saw for,
u h, unconditional language models.
It's, uh, the cross entropy or you could also
say negative log-likelihood of the true next word.
So for example, on those selected ones, uh,
the loss is the negative log probability of the correct next word.
And then as before, we're going to average all of
these losses to get the total loss for the example.
Uh, so a thing you might notice people saying in,
for example, research papers is this phrase end-to-end.
And this is an example of learning a system end-to-end.
And what we mean by this is that the backpropagation is happening end-to-end, one end is,
is losses, the loss functions,
and the other end I guess is kind of like the,
the beginning of the encoder RNN.
The point is that you, uh,
backpropagation, uh, flows throughout the entire system,
and you learn the entire system with respect to this single, uh, loss. Yep?
[inaudible]
The question is,
if the decoder RNN outputs the end token too early,
then how can you measure the loss on,
uh, the words that came after that?
So this is the difference between training time and test time,
which is pretty confusing.
So, uh, during training,
we have this picture where you feed the token back in.
So in this scenario,
once you produce end,
then you have to stop because you can't feed end in as the initial next step.
But in training, you don't feed the thing that you produced into the next step.
During training, you feed the target sentence from the corpus.
So like the gold target sentence into the model.
So no matter what the, uh,
the decoder predicts on a step,
you kind of, you don't use that for anything other than computing loss.
Any other questions? Yeah.
Is there a reason why you would, uh, backpropagation end-to-end instead of maybe
training an encoder like [inaudible] model and then [inaudible] together?
The question is, is there a reason why you would want to train end-to-end when,
for example, you might want to train the encoder and the decoder separately?
Uh, so I think, uh, people view training end-to-end as favorable
because the idea is that you can optimize the system as a whole.
You might think that if you optimize the part separately,
then when you put them together,
they will not be optimal together necessarily.
So if possible, directly optimizing the thing that you care abou-
about with respect to all of the parameters is more likely to succeed.
However, there is a notion of pre-training.
And as you said, maybe you'd want to learn your, um,
decoder RNN as a kind of a language model,
an unconditional language model by itself.
And that's something that people do.
You might, uh, learn a very strong language model,
and then use that to initialize your decoder RNN,
and then fine-tune it on your task.
That's a, a valid thing you might try to do.
Yep.
Are you always [inaudible]
The question is, is the length of
the source sentence and the length of the target sentence fixed?
So for example, is the source sentence always length 4?
Uh, no.
That's definitely not true because in your parallel corpus,
you're going to have sentences of all lengths.
Uh, so this is more kind of an implementation or a practicality question.
Uh, the idea is that this is what you mathematically want
to be computing during training for each example,
and you're going to have batches of examples.
But the question is, how do you actually implement them in, uh, in practice?
So what you usually do just because it's easier to assume that your
batch is this kind of even-sized tensor where everything is the same length,
is you pad any short sentences up to some predefined maximum length,
or maybe the length of the maximum example in your batch, uh,
and then you make sure that you don't
use any hidden states that came from the padding. Yep.
I believe two languages together [inaudible]
possible to have a system
[inaudible] that will be kind of universal with similar languages or something like that?
Okay. So the question I think is,
uh, it seems like sometimes you wouldn't want to train things end-to-end,
and there are circumstances in which you might want to train things separately,
and you mentioned, for example,
having, uh, different languages mapped to each other.
So this is a totally valid point,
and in fact, uh, so far we've, kind of,
assumed that you want to learn language A to language B as a pair, right?
And that's different to language A to language C or even language B to language A.
And, um, that does mean you have kind of n-squared many systems in the number of,
uh, languages you're considering.
So, yeah, that's actually a valid idea,
and this is something that people have researched.
The idea that maybe you could have a, kind of,
mix and match with your encoders and decoders.
And you could try to, uh, train a kind of general purpose,
let's say English decoder and then match it up with your different encoders.
Uh, but this is, I think fairly complex to train to,
to make sure that they all work together.
But that, that is certainly something that people have done.
Let me just check on the time.
Okay. Let's take one more question. Yep.
So does the word embedding also come from the same corpus that we are training on?
The question is, does the word embedding
also come from the corpus that you're training on?
So I think there's a few options just as we saw with language models; you could download,
uh, pretrained word vectors like Word2Vec or GloVe,
and you could use those.
And then you can either, kind of, freeze them or you could
fine-tune them as part of the end-to-end training,
or you could just initialize your word vectors as,
uh, you know, close to zero random and then learn them from scratch.
All right. Okay, moving on.
Uh, so now we understand how you would train a neural machine translation system.
And we talked briefly about how you might,
uh, do decoding or generation.
So what I showed you before is something called, uh, greedy decoding,
which is this idea that on each step,
you just choose the argmax,
the top one best word,
and then you feed that in on the next step.
So this is called greedy decoding because you're just taking the best, uh,
the best option that you can see right now,
and then you really don't have a way to go back.
So can anyone see a problem with this method? Maybe I've kind of given it away but, uh, yeah.
[inaudible].
You said too expensive.
Um, I guess I mean it is expensive in that you have to do
a sequence and the sequence is usually worse than something you can do in parallel.
But I suppose, um, maybe what's wrong with the greediness?
Can anyone suggest what's wrong with the greediness, yeah?
[inaudible] [NOISE] That's not
necessarily gonna give you the argmax over the entire sentence.
That's exactly right. That's, uh,
kind of what, uh, what greediness means.
So in practice, this might give you something like this.
Uh, we're trying to translate
our running example sentence and let's suppose on the first step we say,
"He," and then we say,
"He hit," and then we say,
"He hit a," oh no, that wasn't right.
That wasn't the best thing to choose but we kinda have no way to go back now, right.
We just have to continue and try to make the best of it after saying,
"He hit a," which isn't gonna work out well.
So that's the main problem with greedy decoding.
There's kind of no way to backtrack, no way to go back.
So how can we fix this?
And this relates back to, uh,
what I told you earlier about how we might use, uh,
a kind of searching algorithm to do decoding and SMT.
Uh, but first, you might,
uh, think exhaustive search is a good idea.
Well, probably not because it's still a bad idea for the same reasons as before.
So if you did want to do exhaustive search,
and search through the space of all possible French translations, uh,
then you would be again,
trying to consider which Y maximizes,
uh, this product of all of these individual probability distributions.
So as before, if you try to do this,
uh, then on each step T of the decoder,
you're gonna be having to track V to the power of T possible partial translations,
uh, where V is your vocabulary size.
So here when I say partial translation,
I just mean, uh, a kinda,
you know, like, half of a sentence so far, or something like that.
So, of course, this, uh,
exponential in V complexity is just far too expensive.
So yes, we're gonna use some kind of search algorithm,
and in particular, we're gonna use a beam search decoding.
So the core idea of beam search decoding is that on each step of the decoder,
you're gonna be keeping track of the K most probable partial translations,
and we call partial translations hypotheses,
because we're kind of tracking multiple of them and we're not sure which one is best.
So we're thinking about several.
Here K is an integer and we call this the beam size,
and in practice for NMT this is usually maybe 5-10.
So you can think of, uh,
K kind of as how big is your search space at any one time.
So if you increase K, then you're going to be
considering more different options on each step
and you might hope that this will mean that you get
the best quality solution in the end though of course it'll be more expensive.
So I said that we want to keep track of
the K most probable partial translations, that is, hypotheses.
So this means that we need some kind of notion of, you know,
how probable is this hypothesis or what's its score.
So the score of a hypothesis and, uh,
we're representing that as Y_1 up to Y_T,
um, is just its log probability.
So, uh, the log probability of this partial translation, uh,
according to the language model can be broken down as we saw before into the sum of
the individual log probabilities of the words given everything that came before.
So it's, if it's not obvious, uh,
these scores are all negative because we're taking log of,
uh, of a number between 0 and 1.
Uh, and a higher score is better.
Yes, because you want a higher probability of,
uh, of the hypothesis according to the language model.
So the idea is that we're gonna use this score, uh,
and the search algorithm to search for
high-scoring hypotheses and we're gonna track the top K on each step.
So I'm gonna show you a detailed example in a moment,
but the important thing is to know
that beam search is not guaranteed to find an optimal solution.
Uh, exhaustive search, the one where you enumerate,
enumerate all V to the T possible translations, that is guaranteed to find
the optimal solution but it is just completely infeasible because it's so expensive.
So beam search is not guaranteed to find the optimal solution,
but it is much more efficient than exhaustive search of course.
Okay. So, um, here's an example of beam search decoding in action.
Uh, so let's suppose the beam size equals K, uh,
is 2 and then as a reminder, we have, uh,
this is the score that you apply to a partial, uh,
hypothesis, uh, a partial translation,
which is a hypothesis.
So we start off with our starting token,
and the idea is that we're going to compute
the probability distribution of what word might come next.
So having computed that probability distribution using our seq2seq model,
then we just take the top K, that is top two possible options.
So let's suppose that the top two are the words "He" and "I".
So the idea is that we can compute the score of these two hypotheses,
uh, by using the formula above.
It's just the log probability of this word given the context so far.
So here, let's say that "He" has a score of -0.7 and "I" has a score of -0.9.
So this means that he is currently the best one.
Okay. So what we do is, uh,
we have our two, uh, K hypotheses,
and then for each of those,
we find the top K words that could come next.
And we calculate their scores.
So this means that for both "He" and "I" we find the top two words that could come next.
And for each of these four possibilities, uh,
the score of the hypothesis is equal to, uh,
the log probability of this new word given the context so far plus
the score so far because you can accumulate the sum of low probabilities.
You don't have to compute it from scratch each time.
So here you can see that we have these four possibilities and that
the top two scores are -1.6 and -1.7.
So this means that hit and was are the two best ones.
So the idea is that of these K squared equals 4 hypotheses,
we're just gonna keep the K equals 2 top ones.
And then we just keep doing the same thing.
For these two, we expand to get the two next ones.
And then of those we compute the scores,
and then we keep the two best ones and discard the others and then of those, we expand.
So we keep doing this again and again,
expanding and then just keeping the top K and expanding like this until,
uh, you get some kinda, uh, finished translation.
I'm going to tell you more in a moment about what exactly the stopping criterion is.
But let's suppose that we stop here.
Uh, looking at the four hypotheses that we have on the far right,
the one with the top score is, uh,
the top pie one with -4.3.
So let's suppose that we are gonna stop now when we
decide that this is the top hypothesis,
then all we need to do is just backtrack
through this tree in order to find the full translation,
which is "He hit me with the pie."
All right. So, um, let me tell you more detail about how exactly we decide when to stop.
So if you remember in greedy decoding,
usually we just keep decoding until the model produces the END token.
So for example, this means that your model is actually producing the sequence, uh,
I guess it doesn't produce START,
you give it START but then it produces the sequence "He hit me with a pie" END.
So the problem in beam search decoding is
that you're considering all these different hypotheses,
K different hypotheses at once and the thing is
those hypotheses might produce END tokens at different times.
So there's no one obvious place to stop.
So what we do in practice,
is when a hypothesis produces the END token,
then we regard this hypothesis as complete and we kind of place it aside.
We have a collection of completed hypothesis.
So we kind of take it out of beam search,
we no longer keep exploring it because it's finished,
uh, and we, yeah, place it aside.
And you continue exploring other hypotheses with beam search.
So the remaining question is when do you stop doing beam search?
When do you stop iterating through this algorithm?
So there's, uh, uh, multiple possible stopping criterion
but two common ones are you might say, uh,
we're gonna stop doing beam search once we reach time step T,
where T is some, uh,
predefined threshold that you choose.
So you might say, uh,
we're gonna stop beam search after 30 steps because we don't want
any output sentences that are longer than 30 words for example,
or you might say, "Uh,
we're gonna stop doing beam search once we've collected at least N completed hypotheses."
So you might say, "Uh, I want
at least 10 complete translations before I stop doing beam search."
Okay. So what's the final thing you have to do?
Uh, we finished doing beam search, um,
we have this collection of completed hypotheses.
Uh, we want to choose the top one.
Uh, the one that we're going to use is our translation.
So, uh, how do we select the top one that has the highest score?
Uh, you might think this is simple given that all of
these hypotheses already have scores attached.
But if we just look at this, uh,
formula again, uh, for what the score is of each hypothesis.
Uh, can anyone see a problem with this?
If we have our sets of hypotheses,
and then we're choosing the top one
based on the one that has the best score, can anyone see a problem?
Yeah. [NOISE] So the answer was you're gonna end up choosing the shortest one.
The problem here is that longer hypotheses have lower scores in
general because you're multiplying more probabilities so you're getting a smaller,
a smaller overall value or I guess if we're adding
low probabilities we're gonna get more negative values.
So it's not quite that you will definitely choose
the shortest hypothesis because if you could overall have,
uh, a lower score but there's definitely going to be a bias towards shorter translations,
uh, because they'll in general have lower scores.
So the way you can fix this is pretty simple,
you just normalize by length.
So instead of using the tools we have above, you're going to use, uh,
the score divided by [inaudible].
And then you use this to select the top one.
Any questions on this? [NOISE].
Yeah.
Can we train with the END token so that it is possible to [inaudible]
I didn't quite hear that, can you train with the END token?
Yeah, like we had an END token.
Yes. So you train with the END token, if that's your question.
Um, because the whole point is you're relying on your language model,
your decoder to produce the END token in order to know when to stop.
So you need to train it to produce the END token by giving it examples of
training sentences with END tokens. Yeah.
Why don't we use this score being changed [inaudible]
Great question. The question is,
why don't we use this normalized score,
the one at the bottom of the screen during beam search in the first place?
So the reason why that's not necessary,
you could, but it's not necessary,
is because during beam search,
we only ever compare the scores of hypotheses that have the same length, right?
So in each of these steps, the way we look at,
let's say the top k squared and we want to choose which ones are the top k,
we're comparing the scores of four different hypotheses that are of length,
one, two, three, four, five.
So, um, it's true that these scores are getting lower and lower,
but in the same way because they're all length five right now.
[NOISE] Okay.
So we now understand how you would train an NMT system and how would you- you
would use your trained NMT system to generate your translations using,
let's say, beam search.
So let's all take a step back and think about,
what are the overall advantages of NMT in comparison to SMT?
Uh, so the first advantage is just better performance.
Uh, NMT systems tend to give better output than SMT systems in several ways.
One is that the output often tends to be more fluent.
Uh, this is probably because NMT, uh,
this is probably because RNNs are particularly good at
learning language models as you learned last week.
Uh, another way that they're better is they often use,
uh, the context better, that is,
uh, they're better at conditioning on
the source sentence and using that to change the output.
Another way they're better is they often, uh,
are more able to generalize what they learn about phrases and how to translate them.
So for example, if it sees an example of how to translate a certain
source phrase and then later it sees a slightly different version of that source phrase,
it's, uh, more able to generalize what
it learned about the first phrase than SMT systems will.
Another big advantage of NMT systems compared to SMT that we talked about
before is that it's a single neural network that can be optimized end-to-end.
And the- the advantage here I suppose is primarily simplicity and convenience.
So there's no subcomponents that need to be individually optimized.
Another big advantage is that it requires much less human engineering efforts.
When I told you earlier about all the different things that
people had to do to build, uh, big,
uh, powerful SMT systems, uh,
there's relatively less engineering effort for NMT.
And NMT is certainly not easy,
but it's- is less complicated than SMT.
In particular, there's no feature engineering.
You don't have to define what features of,
uh, linguistic phenomena that you want to capture.
You can mostly just view it as a sequence of words although,
uh, there are different views on that.
Uh, lastly, a great thing about NMT is that you can
use pretty much the same method for all language pairs.
So if you've, uh, you know,
built your French-to-English translation system
and now you want to build a Spanish-to-English one, [NOISE] uh,
you can probably use basically the same architecture and the same method
as long as you can go find a big enough parallel corpus of Spanish-to-English.
All right. So what are the disadvantages of NMT, uh, remaining?
So compared to SMT,
there are some disadvantages.
One is that NMT is less interpretable.
Uh, what I mean by this is you
feed in your source sentence into the neural network and then it feeds
out some target sentence and you didn't
really have any way to figure out why that happened, right?
So in particular, if the target sentence con- contains some kind of error,
um, you can't really look at the neurons and understand what happened.
It's pretty hard to attribute errors.
So this means that, uh,
NMT systems are pretty hard to debug.
So by comparison, SMT systems were more
interpretable in that you had all of
these different sub-components that were doing different jobs.
And, uh, you were more able to look at those.
They weren't, you know, neurons often would be, uh,
you know, probabilities of certain words given other words and so on.
And, you know, that's by no means easy to
interpret but it was at least more interpretable than NMT.
Uh, another disadvantage is NMT is pretty difficult to control.
So, uh, for example,
if your NMT system is,
uh, doing a particular error,
it's not very easy for you, the, uh,
programmer to specify some kind of
rule or guideline that you want the NMT system to follow.
So for example, if you want to say,
I want to always translate this word in this way.
Um, when- when this other thing is present,
like that's not particularly easy to, uh,
to impose as a rule on the NMT system, uh,
because you can't, uh,
easily control what it's doing on a step-by-step basis.
So sometimes you have some kind of, uh,
post-processing rules you might try to do,
but overall you can't.
It- it's- it's harder than you'd expect to try to, um,
impose a fairly simple form.
[NOISE] So this means it has some kind of safety concerns in fact.
Because, uh, let's say, you know,
you don't want your NMT system to say bad things, right?
It's- it's pretty hard to actually put, um,
these, uh, controls in
place to stop it from saying these things that you don't want it to say.
I mean, on the level of maybe just never saying particular bad words,
then sure you can remove them from the vocabulary.
But overall they're pretty hard to control,
and we're actually gonna see some examples of NMT systems being,
you know, doing things that their
uh, designers certainly didn't intend.
Okay. So, uh, how do we evaluate MT?
Uh, every good NLP task needs to have
an automatic metric so that we can, uh, measure our progress.
So the, uh, most commonly used evaluation metric for MT is
called BLEU and that stands for Bilingual Evaluation Understudy.
So the main idea is that BLEU is gonna
compare the translation that's produced by your machine translation system.
It's gonna compare that to
one or maybe several human written translations of the same sentence.
And then it's gonna compute a similarity score that's based on n-gram precision.
So when I say n-gram precision,
I mean you're gonna look at all the one, two, three,
and four grams that appear in your, uh,
machine written translation and your human written translation.
And then n-gram precision is basically saying,
for all of the n-grams that appeared in the machine-written translation,
how many of those appeared in, you know,
at least one of the human-written translations?
Another thing that you need to add to BLEU is a brevity penalty.
Uh, so you're saying that you get a lower BLEU score if
your system translation is significantly
shorter than all of the human-written translations.
And the reason why you need to add this is because
n-gram precision alone doesn't [NOISE] really punish using, uh, fewer words.
So you might try to maximize n-gram precision by being very conservative and writing,
uh, short sentences that only contain words that you're really sure about,
and then you get a good precision score.
But this doesn't make a good translation because you're probably missing a bunch of
information that you needed to translate from the source sentence.
So that's why you need to add the brevity, uh, penalty.
So overall, um, BLEU is very useful because, uh,
we need an automatic metric in order to, uh, measure progress,
you can't measure progress on human evaluation
alone because it takes too long [NOISE] to compute.
Um, but of course it's pretty and perfect.
So for example, you can think about how there
are many ways- many valid ways to translate a sentence.
At the very beginning of this lecture,
I asked how do we translate that sentence, uh,
by Rousseau and there were at least a few different options that came up.
Uh, so if there's many valid ways to translate a sentence,
how does BLEU recognize that?
BLEU is [NOISE] rewarding sentences that have a high n-gram overlap with,
uh, one or some of the human-written translations.
But, if, uh, you write one,
if your model writes one valid translation and the humans wrote
a different valid translation and they don't have high n-gram overlap,
then BLEU is going to, uh, give you a low score.
So, um, you're going to learn about BLEU in detail in Assignment 4,
and in fact Assignment 4 has
a full description- mathematical description of what the BLEU score is.
So I'm not gonna tell you about that now, uh, yes,
so you're gonna think about BLEU and the- the ways in which it's
imperfect but useful. Yeah.
So would one- one n-gram, be a one to one equivalency?
What?
Would a one n-gram be a one to one equivalency?
The question is, would a one n-gram be a one-to-one equivalency?
I'm not sure I understand the question. You're asking about alignment or something else?
Uh, just trying to get an idea about how they're doing n-gram checks,
is it doing all n-gram permutations or is it doing like window size of one?
Well, I guess one- one n-gram it doesn't
make a difference because you can't permute a one-gram.
Okay. So you're asking for examples,
for four grams are they checking, uh,
whether this exact sequence of four paired or
any permutation of it, its exact sequences?
So by definition, n-grams are sequences where the order matters.
Okay. All right.
So, uh, that's how you evaluate machine translation.
So now you can understand this metric of how we
evaluate our progress on machine translation,
um, I can show you this graph and you might understand what it means.
So this is a, uh,
bar plot which shows in a nutshell how NMT changed the machine translation,
uh, landscape in just a few years.
So in this plot, we've got BLEU score is the Y-axis.
Uh, and you have two different types of SMT
which is the red and the dark blue, uh, bar plots.
And what's happening is,
uh, in 2015, uh,
Neural MT enters the scene for the first time and it isn't doi- doing as well as SMT,
and then the next year it's suddenly outperforming SMT.
And here these are BLEU scores on some particular fixed dataset like,
uh, a shared task that many people were,
um, submitting systems for.
[NOISE] So the main thing to notice here is
that the progress that was being made by SMT systems was,
you know, a fairly gentle increase in BLEU year-by-year.
And then in just one year,
NMT arrives and is suddenly doing,
uh, much more rapid progress.
So I think this justifies why the picture of the meteor maybe isn't too Jurassic here.
So you could in fact call NMT the biggest success story of NLP in deep learning.
Uh, because if you think about the history of this,
NMT went from being a fringe research activity in 2014 to being actually
the leading standard methodfor machine translation in the world in 2016.
In particular, in 2014,
the first seq2seq paper was published.
And in 2016, Google Translate switches from SMT to NMT.
This is a pretty remarkable turnaround for just two years.
So this is amazing,
not just because it was a quick turnaround,
but also if you think about the level of human effort involved.
Uh, these SMT systems, for example
the Google Translate SMT system was built by
doubtless hundreds of engineers over many years.
And this, uh, this SMT system was outperformed by an NMT system that was trained by,
uh, you know, relatively few like a handful of engineers in a few months.
So I'm not- I'm not diminishing how difficult it is to,
um, build NMT systems,
and certainly I'm sure Google's NMT system
today is built by more than a handful of engineers in a few months.
I'm sure it's a very big operation now.
Uh, but when NMT,
uh, began to outperform SMT,
it was pretty remarkable how it was able to do that,
uh, based on the amount of effort involved. Yeah.
Given the [inaudible] cons of NMT has there been research on combining the two and if there is, what does that look like?
Yeah, great. The question is given that we know that there are
some disadvantages of NMT even in comparison to SMT,
is there any work on combining the two?
So, yes. I think there is.
Uh, there's a lot of NMT research ongoing and in particular,
people sometimes focus on these particular shortcomings.
And, uh, there's a lot of work in kind of taking techniques
and ideas and wisdom from the many decades of SMT research,
and then integrating them into the new NMT paradigm.
So yes. [NOISE]. Okay.
So is machine translation solved?
Can we all go home? I think the answer is clearly no.
Uh, NMT definitely is not doing machine translation perfectly.
So, um, just to highlight some of the difficulties that remained with NMT.
Uh, one is out-of-vocabulary words.
Uh, this is a kind of basic problem but it is pretty tricky.
You know, what do you do if you're trying to translate
a sentence that contains a word that is not in your source vocabulary,
or what if you're trying to produce a word that's not in your target vocabulary?
Um, there's certainly been lots of work on doing this,
and you're going to hear later in the class how you
might try to attack this with for example,
uh, sub-word modeling can make it easier.
Uh, but this is a significant problem.
Another one is domain mismatch.
So let's suppose that you train your machine translation system on a bunch of fairly,
uh, formal text, like let's say,
uh, Wikipedia or something like that.
Uh, but then you try to deploy it to translate
informal text, like people chatting on Twitter or something.
Then often, you'll find that it doesn't perform very well
on this different domain because you've got a domain mismatch.
Uh, so that's quite a big problem.
Another one is maintaining context over longer text.
So everything we've talked about so far has assumed that you were
just translating a single sentence to a single sentence,
and there's no other wider context.
Uh, but, you know if you want to use
a machine translation system to translate a whole news article and maybe even a book,
then you're probably going to want to use the context that came in
previous sentences in order to translate things correctly in the current sentence.
So, uh, this is an active area of research,
how can you get an NMT system to condition on
larger pieces of context without it becoming too expensive and so on?
Another difficulty is low-resource language pairs.
Um, everything we've talked about so far has assumed that you have
access to a very large parallel corpus, but what if you don't?
What if you are trying to translate to or from a language that
has relatively little text available,
um, online for example?
So that can be pretty difficult.
Here a few examples of machine translation screwing up,
uh, with specific errors.
So, here's an example of how common sense is really difficult for NMT systems.
On the left, we have the English phrase paper jam,
which means when your printer
gets jammed up with paper and it's all, uh, tangled inside.
And then on the right,
we have a very literal translation of that into Spanish,
and it's essentially saying jam,
edible jam made of paper,
which clearly isn't the right interpretation.
So here, we have an NMT system that's just doing
very literal translation and clearly doesn't have any notion of common sense.
You can't make jam from paper. Uh, here's another example.
NMT can pick up biases in the training data.
We already talked about this at the,
uh, the word embedding level,
the representation of words.
Uh, but it can also be a problem at the you know,
the sentence level when you're translating things.
So here in this example,
uh, on the left,
we have two sentences in Malay that roughly mean,
uh, they work as a nurse, and they work as a programmer.
The point is on the left,
there is no information about gender in the pronouns.
But when it gets translated to English,
then we've suddenly got gender coming out of nowhere,
she works as a nurse, and he works as a programmer.
This is likely happening because in our training data,
we had more examples of female nurses and male programmers.
So you can understand why from a machine learning, uh,
maximizing the objective point of view the,
uh, English language model has learned to do that.
But the problem here is this isn't good machine translation.
Uh, here the system is making up information that was not present in the source sentence.
So this is certainly an error that
the machine translation shouldn't be doing because it's just simply inaccurate.
And even worse, it's propagating, uh, gender roles.
Here's another pretty weird example.
[LAUGHTER] What is happening here?
Uh, on the left,
we have a nonsense sentence,
this is just kind of a syllable repeated.
And we're supposedly translating from Somali.
Uh, and then we're asking to translate this into
English and then we're getting this out of nowhere.
Um, as the name of the Lord was written in the Hebrew language,
it was written in the language of the Hebrew nation,
and you might be thinking "Where on earth did that come from?"
And in fact, this got reported in the media as you know,
Google Translate wants to convert you to its religion or whatever.
[LAUGHTER] Um, so for sure,
it is very startling.
But the thing is there's actually quite a reasonable explanation.
So what's going on here is that,
um, often for low resource languages,
such as for example Somali, um,
one of the best resources of parallel text is the Bible.
So you train for example Somali to English using the Bible as a training text,
maybe among other texts.
Okay, that's the first puzzle piece.
But the other puzzle piece is the nonsensical input.
So when the input isn't really Somali or any kind of text, right?
It's just the same syllable over and over.
Then the NMT system doesn't really have anything sensible to condition on.
Its basically nonsense, it's just noise.
So what does the NMT system do?
Right? It can't really use, it can't really condition on the source sentence.
So what it does, is it just uses the English language model, right?
You can think of it as like the English language model of the decoder RNN
just kind of goes into autopilot and starts generating random text,
kind of like we saw last week when we saw, uh,
a language model trained on Obama's speeches or
Harry Potter would just generate texts in that style.
That's kind of what's happening here with the Bible,
because we don't have any useful information,
um, from the sentence on the left.
Um, so, this is an example why, uh,
neural machine translation in particular makes these kinds of errors,
uh, because the system is uninterpretable.
So you don't know that this is going to happen until it happens,
and perhaps Google didn't know this was going to
happen until it happened and it got reported.
Um, so this is one downside of uninterpretability is that really weird effects can
happen and you don't see them coming and it's not
always even easy to explain why they happened. Yeah?
[inaudible].
Ah, the question is what happens if you did translate from Irish?
I suppose that's the part where Google tries to autodetect
the language, maybe it thinks that ag ag ag is more like Irish than Somali,
[LAUGHTER] I imagine if you did put Irish to English,
there's probably more, uh,
training data for Irish to English.
So maybe it wouldn't be so Bible-focused.
Um, yeah, and there's a lot of examples of these online where
you do different kinds of nonsense syllables in different languages.
So there's a lot of, uh, challenges remaining in NMT.
And, uh, the research continues.
So NMT, I think remains one of the flagship tasks for NLP Deep Learning.
In fact, NMT research has pioneered many of
the successful innovations of NLP Deep Learning in general.
Uh, so today in 2019, uh,
NMT research continues to thrive, there's still many,
many papers, uh, published all the time on NMT.
And in fact, uh, researchers have found lots of
improvements to the fairly vanilla seq2seq models that I've shown you today.
Uh, but in fact,
there is one improvement that is so integral
to seq2seq that you could regard it as the new vanilla.
And that's the improvement we're going to learn about today,
and it's called attention.
Okay. So section three is on attention. What is attention?
First, I'm going to motivate why we need this thing called attention.
So let's look at this diagram that we saw before of sequence-to-sequence.
And remember when we assumed that this,
uh, encoding of the source sentence,
the, t he one in the orange box is going to represent the whole sentence.
Uh, can anyone volunteer a problem you can see with this architecture?
In particular perhaps, a problem with this idea that that single vector is
the encoding of the source sentence. Yeah?
[inaudible]
Okay, so the answer is something like, um, you're only looking at
one word, you mean like the last word in the source sentence?
And you're not seeing more information.
Yeah some- it's, it's something like that.
Any other ideas? Yep.
We might have lost information in
the beginning of the sentence by the time you get to the end.
Yeah. You might have lost information from [NOISE] the beginning of the sentence by,
by the time you get to the end, especially if it was longer than four words.
Right. I think these are different ways of saying a
similar idea [NOISE] which is that we have a kind of informational bottleneck.
Uh, we're forcing all of the information about the source sentence to be captured
in this single vector because that's the only thing that gets given to the decoder.
If some information about source sentence isn't in our vector,
then there's no way the decoder is gonna be able to translate it correctly.
So this is the, yeah,
this is an informational bottleneck.
[NOISE] It's putting kind of too much pressure on
this single vector to be a good representation [NOISE] of the encoder.
So this is the motivation for attention.
Attention is a neural technique and it provides a solution to the bottleneck problem.
The core idea is that on each step [NOISE] of the decoder,
you're gonna use a direct connection to the encoder
to focus on a particular part of the source sequence.
So first I'm gonna show you what attention is
via a diagram so that's kind of an intuitive explanation.
And then I'm gonna show you the equations later.
So here's how seq- sequence-to-sequence with attention works.
So on the first step of our decoder,
uh, we have our first decoder hidden state.
So what we do, is we take the dot-product between
that decoder hidden state and the first [NOISE] encoder hidden state.
And then we get something called
an attention score which I'm representing by a dot. So that's a scalar.
[NOISE] And in fact,
we take the dot-product between the decoder hidden state
and all of the encoder hidden states.
So this means that we get one attention score or one scalar for each of these,
uh, source words effectively.
So next what we do, is we take those four number scores and we apply the softmax,
uh, distribution, uh, the softmax function to
them and then we get a probability distribution.
So here, I'm going to represent that probability distribution as a bar chart.
Um, and we call this the attention distribution and this one sums up to 1.
So here, you can see that most of the probability mass is on the first word.
And that kinda makes sense because our first word essentially means "he" and,
uh, were gonna be producing the word "he" first in our target sentence.
So once we've got this attention distribution,
uh, we're going to use it to produce something called the attention output.
So the idea is that the attention output is a weighted sum of
the encoder hidden states and the weighting is the attention distribution.
So I've got these dotted arrows that go from
the attention distribution to the attention output,
probably there should be dotted arrows also from
the encoder RNN but that's hard to depict.
[NOISE] But the idea is that you're summing up these encoder RNN, uh,
hidden states, [NOISE] but you're gonna weight each
one according to how much attention distribution it has on them.
So this means that your attention output which is a single vector is going to be
mostly containing information from the hidden states that had high attention.
In this case, it's gonna be mostly information from the first hidden state.
So after you do this,
you're going to use the attention output to influence your prediction of the next word.
So what you usually do is you concatenate
the attention output with your decoder hidden state and then,
uh, use that kind of concatenated pair in the way
you would have used the decoder [NOISE] hidden state alone before.
So that way you can get your probability distribution,
uh, y hat 1 of what's coming next.
So as before, we can use that to sample your next word.
[NOISE] So on the next step,
you just do the same thing again.
You've got your second decoder hidden state.
Again, you take dot-product with all of the encoder hidden states.
You take softmax over that to the get attention distribution.
And here, you can see the attention distribution is different.
We're putting more attention on, uh, the,
the word entarté because we're about to produce the word hit.
Uh, but we're also attending a little bit to
the second word a because that's telling us that hit is a past tense.
So a cool thing that's happening here is we're getting [NOISE] a soft alignment.
If you remember when we looked at alignment in SMT systems,
it was mostly this, uh,
hard binary thing with on or off, either these words are aligned or they're not.
Here, you have a much more flexible soft notion of alignments where,
uh, each word kind of has a distribution over
the corresponding words in the source sentence.
So another thing to note kind of a side note,
is that sometimes, uh,
we take the attention output from the previous hidden state, uh,
and we kind of feed it into the decoder again along with the usual word.
So that would mean you take the attention output from the first step and kind of
concatenate it to the word vector for he and then use it in the decoder.
Uh, the reason for this is sometimes it's useful to have this, uh,
information from the, the attention on the previous step on the next step.
So I'm telling you this because this is something we do in Assignment 4
and it's a fairly common technique but also sometimes people don't do it.
Okay. So, um, the theory is,
that you just do this attention,
uh, computation on every step.
And on each step, you're going to be attending to different things.
So in our example on this third step,
we look at m' which means me when we
produce me and then on the last
three [NOISE] we're probably mostly just gonna be looking at this,
uh, fertile word entarté to produce hit me with a pie.
[NOISE] I'm gonna keep going because we don't have a lot of time.
Uh, so here are the equations to describe attention.
Uh, I think it's probably easier to look at these in
your own time later rather than look at them in the lecture now.
But these are the equations that essentially
say the same thing as what the diagram just said.
So you have your encoder hidden states h_1 up to h_N.
And then on timestep t of the decoder,
we also have a decoder hidden state, uh, S_t.
So we're gonna get the attention score which we're gonna call et by taking
the dot-product of your decoder hidden state with each of the encoder hidden states.
[NOISE] And that gives you, uh,
a vector of same length as the, uh,
encoder [NOISE] sentence because you've got one score per source word.
Next you take softmax over these scores
to get attention distribution that sums up to 1,
and we call that alpha.
And then you use alpha to take a weighted sum of
the encoder hidden states and that gives you your attention output.
So [NOISE] the attention output which we call a is a vector that's
the same size as your encoder hidden states.
Lastly, you take your attention output a and then you, [NOISE] uh,
concatenate it with your decoder hidden states and then
proceed with that as you were taught before in the no attention model.
So attention, if it's not clear, it's pretty cool.
It has a number of advantages.
So one advantage is that attention just significantly improves NMT performance.
And the main reason why it improves it,
is because it turns out it's super useful to allow the decoder [NOISE] to
focus on certain parts of the source sentence when it's translating.
And you can see why this makes sense, right?
Because there's a very natural notion of alignment,
and if you can focus on the specific word or words you're translating,
you can probably do a better job.
Another reason why attention is cool is that [NOISE] it solves the bottleneck problem.
Uh, we were noting that the problem with having
a single vector that has to represent the entire source sentence [NOISE] and that's
the only way information can pass from encoder to decoder
means that if that encoding isn't very good then, uh, you're not gonna do well.
So by contrast in, uh, with attention,
the decoder can look directly at the encoder and
the source sentence and translate without the bottleneck.
[NOISE]
Another great thing about attention is that it helps with the vanishing gradient problem,
especially if your sentences are quite long.
Uh, the reason why attention helps is because you have
these direct connections between the decoder and the encoder,
kind of over many time steps.
So it's like a shortcut connection.
And just as we learned last time about, uh,
skip connections being [NOISE] useful for reducing vanishing gradient.
Here it's the same notion.
We have these, uh, long distance
[NOISE] direct connections that help the gradients flow better.
Another great thing about attention is it provides some interpretability.
Uh, if you look at the attention distribution,
often you've produced your translation.
Uh, you can see what the decoder was focusing on on each step.
So for example if we run our system and we translate our,
our running example here,
then we can produce a plot,
kind of like this that shows the attention distribution.
So here, dark means high attention and white means low attention.
So you might see something like this where,
um, it was, it was focusing on the different words and different steps.
And this is basically the same kind of
plot that we had earlier with a hard notion of alignment,
uh, in SNT except that we, uh,
we have more flexibility to have a more soft version of alignment
like for example when we produce the English word hit,
perhaps we were mostly looking at entarte, but we're also looking at a little bit of A.
So this, uh, means that we're getting,
uh, alignment for free.
And the reason I say for free is because when you remember the SNT systems,
the whole point there is that you had to learn
an alignment system deliberately and separately.
You had to define the notion of alignment,
you had to define the model of calculating,
what the probability of different alignments were and train it.
Whereas here, we never told the NMT system about alignments.
We never explicitly trained an alignment system.
We never had a loss function that tells you how good your alignment was.
We just gave the NMT system the apparatus to
do something like alignments and told it to maximize the,
uh, the cross-entropy loss for doing machine translation.
And then the network just learned alignment by itself.
I think this is the coolest thing about attention,
is that it's learned some structure in a somewhat unsupervised way.
Okay, so in the last few minutes,
I'm just going to, uh, generalize the notion of attention.
Because it turns out that attention is actually a very general, uh,
deep learning technique that you can apply in lots of different circumstances.
So you've seen that attention is a great way to improve
the sequence-to-sequence model for MT,
but you can actually use attention for other architectures that aren't
seq2seq and also tasks that aren't MT.
So to understand this,
I'm going to somewhat redefine attention to a more general definition.
So here's our more general definition.
Suppose you have a set of values,
each of which is a vector,
and you also have a single vector in which you're calling the query.
Then attention is a way, uh,
to compute a weighted sum of the values.
But the way you weight it is dependent on the query.
[NOISE] So we often phrase this,
uh, as saying that the query is attending to the values.
The idea being that you have all this information that's in the values and
the query is somehow determining how it's going to pay attention to the values.
So for example in seq2seq, uh,
the decoder hidden state is the query.
Uh the decoder hidden state on a particular time step is the query
and is attending to all the encoder hidden states which are the values.
All right, here's our definition again.
So here's a way to kind of understand this intuitively, two alternative ways.
One is to think of it like this.
You could think of it as the weighted sum is like
a selective summary of the information in the values.
And I say selective because your choice of how much you choose
to draw from each value depends on the attention distribution.
Uh, so the distribution, uh, depends on the query.
So the query is determining how much you're going to select from different, uh, values.
And this is kind of similar to LSTM that learned about earlier this week.
LSTMs rule based on the idea of a gate that, uh,
[NOISE] that defines how much information shou-
should [NOISE] come from different elements.
And the gate depends on the context.
So the strength of LSTMs came from the idea that based on the context,
you decide where you're going to draw information from.
And this is kind of like the same idea.
The second way to think about attention is you could say that it's a way to obtain
a fixed-size representation from an arbitrary set of representations.
So when I say arbitrary sets,
I'm saying we have this set of vectors called the values, right?
And you could have 10 values.
You could have 100 values.
You can have, uh,
any [NOISE] arbitrary number of these vectors.
But attention gives you a way to get a single vector,
um, summary of that which is the attention output, uh, using your query.
Okay, uh, so the last thing, uh,
is that there's actually several variants of
attention and this is something were are going to look at a little in Assignment 4.
So in our more general setting,
we've seen that we have some values in the query.
Doing attention always involves computing the attention [NOISE] scores,
and then you apply softmax to get the attention distribution.
And then you use that attention distribution to take a weighted sum.
So this is, uh, always the outline of how attention works.
The part that can be different is this, uh, number one.
There are multiple ways you can compute the scores.
So, uh, last slide,
here all the different ways you can repeat the scores.
So the first one which you've already seen today is basic dot-product attention.
And the idea here is that [NOISE] the score for a particular, a particular value,
HI, is just the dot-product of the query and that particular value.
[NOISE] And, uh, in particular this assumes that the size
of your query vector and the size of
your value vector has to be the same because you're taking dot-product.
[NOISE] Another, uh, version of,
uh, attention is called multiplicative attention.
And here, the idea is that the score of your, uh,
value HI, is going to be this, uh,
bi-linear function of your query and that value.
So in particular, we're pushing this weight matrix in
the middle and that's a learnable parameter.
You're learning the best way matric- ma- weight matrix in order to get the scores,
the attention scores that are useful.
The last one is called additive attention.
So what's happening here is that the score of the value HI is, uh,
you get it by applying
a linear transformation to both the value and the query and then you add them together.
And then you put them through a non-linearity like tanh.
And then lastly, uh,
you take that vector and you take the dot-product with
a weight vector to give you a single number that is the score.
[NOISE] So here, you've got
two different weight matrices and [NOISE]
also a weight vector which are the learnable parameters.
One thing that's different here is that there's kind of an additional hyperparameter,
which is the attention dimensionality.
So [NOISE] that's kind of, uh, the,
I think it's the heights of the W1 and W2 and this is the length of V, right?
You can choose what size that dimension is.
It's kind of like a hidden layer in the computation.
So, um, you can decide how big you want that intermediate representation to be.
Okay, so I'm not going to tell you any more about that because that's
actually one of the questions in the assignment, uh,
Assignment 4 is to think about
the relative advantages and disadvantages of these models.
[NOISE] Okay.
So here's a summary of today.
[NOISE] It really is the last slide,
[BACKGROUND] second last, last time,
but this was the last slide.
[BACKGROUND] So we learned about the history of MT.
[NOISE] We learned about how in 2014 [NOISE] Neural MT revolutionized MT.
[NOISE] We learned about how sequence-to-sequence
is the right architecture for NMT and it uses two RNNs.
And lastly, we learned about how attention [NOISE] is a way to focus
on particular parts of the input. All right, thanks.
 [NOISE] Okay everyone, let's get started for today.
Okay. So, we're into week five of CS224n.
And so, this is the plan for today.
Um, in some sense a lot of this class is gonna be
an easy class because I'm gonna talk about things like,
um, final projects and tips for what you're meant to do,
and finding a topic,
and writing up your work,
and things like that.
Um, so for, um, so,
two-thirds of the class there isn't a lot of,
um, deep technical content.
But I hope they're actually
just some useful stuff and stuff that would be good to know about.
One way you can think about this is until,
until this year we had a midterm in this class.
So, you know, if we weren't doing this class should instead be doing the
the mid-term based on all the material that we've covered, um, so far.
So, this should be really pleasant by comparison.
Um, but that isn't gonna be quite the entire class.
So, for this piece here in the middle I'm gonna
spend a while back on some of the topics of last week.
So, I wanted to have one more look at some of these gated recurrent models,
um, that Abby introduced last week.
And I guess my hope is that now that you've
had a bit more time to look and read about things,
and hopefully even have started working on homework for that.
Maybe it starts to make a bit more sense or else even if it's more confusing then before,
you've got some idea of what your confusions are and questions.
And so, hopefully it's, um,
good to think about those one more time because I think they are quite a complex notion,
and it's not so obvious what they're doing and why they're doing anything useful,
or whether they're just this big complex blob of mystery.
And then also to touch on a couple of machine translation topics that have um, come up
in the final project that we didn't really get m- time to say much about last week.
[NOISE] Okay.
So, let's get started.
Um, so, this is our coursework in grading that we showed at the beginning.
And so, the main thing I wanna do today is talk about this final project.
Um, but before tha- I do that,
let's just save one minute on participation.
Um, so, I guess we started into one aspect of the participation policy, um,
last Thursday when we took attendance,
and that makes it sound draconian,
but I wanted to say, um,
the positive viewpoint of,
um, the participation points.
I mean, obviously this is a big class.
There are lots of people.
Um, our hope is just that people will variously,
they're sort of engaged and involved in the class,
and the participation points,
ah, are our way of doing that.
I mean, basically the way this is set up.
I mean, if you do much of anything
you should just get three percent for the participation points.
It shouldn't be hard.
I mean, I will bet you that there will be some people who at the end,
will have gotten seven points in the participation category.
And unfortunately we cap you, we'll only give you
three percent for the participation category, but you know,
providing you usually come to class,
or usually write the,
um, what we've got to [NOISE] the invited speakers
the reaction paragraphs if you are an SCPD student.
Sometimes, um, write a helpful answer on Piazza, right.
You're already gonna be there on three percent.
Um, yeah.
And so, one, but one other thing, um,
that's a way to get some parti- participation points that's out today.
So, um, today we're putting up our Mid-quarter feedback survey.
And we'd love to have you fill that in.
I mean, we'd like to get your thoughts on the course so far.
And, you know, for you guys,
there are two ways that you can win.
First if you give us some feedback that can help the rest of your quarter be better,
but we've also got a simple bribe built into this, um,
which is you get half a participation point simply for filling in,
um, the, um, Mid-quarter survey,
but it'd be really good to get your feedback on that.
Okay. So, then the main thing I want to get to
today is to talk about [NOISE] the final project.
Okay. And so, I'll jump right ahead, um, into that.
So, for the final project there are two choices.
Um, you, you can either do our default final project,
which I'll say a little bit about, it's doing SQuAD question answering,
or you can propose a final,
a custom final project,
which we then have to approve.
And in the course of that,
um, if you have some outside mentor, um,
you can say who they are and your project proposal,
but otherwise, um, we'll attempt to assign you a mentor somewhere out of the course staff.
Um, so, for all the assignments,
through assignments one through five,
you have to do them by yourself.
Um, for the final project in either form of that,
you can do it as a team.
So, you can do it as one,
two, or three people.
And how does that work?
Um, well, it works like this, um,
if you're a bigger team,
we do expect you to do more,
and there are actually two ways you can be a bigger team that I'll point out.
One way is having more people being two or three people.
And the other thing that comes up is, um,
sometimes people wanna do a final project for more than one class at the same time.
In particular for this quarter I know there are
at least a couple of people who are hoping to do,
um, a joint project with Emma's reinforcement learning class.
And we allow that as well.
But we sort of do multiplication because if you're two people using it for two classes,
that means it should be four times as great as
what one person is doing for one class, right?
So, how, how it works with larger teams, you know,
in all honesty it's a little bit subtle because, you know,
the truth is if something is just bad, um,
your model was broken, um,
or you, your experiment failed,
um, and you don't know why.
Um, you know. If, if there's just obvious ways in what you've done as bad as it's sort of,
it's sort of bad whether you're one person or four person.
Um, and if you've written it up beautifully,
you've written up beautifully regardless of whether
you're one person or four per- people,
that you know nevertheless the expectation is that if you're one person will be pleased,
that if you put together one model and gotten it to work well, um,
but if you're three people will say, "Well,
that wasn't such a big effort, um,
running this one model against this task."
Surely if there are three people,
they could have investigated
some other model classes and seeing whether they perform better or worse on this task.
And we'll feel a sense of lightweight.
So, we are expecting that sort of both more ambitious projects,
and more thorough exploration of them if you're
being a bigger team or you're using it for multiple classes.
Um, for the final project,
you are allowed to use any language or deep learning,
um, framework that you choose to.
We don't insist on what you use,
though in practice in past years.
Basically everyone keeps on using what they've learned,
um, in the assignments.
I expect that will be true, um,
this time as well. [NOISE]
Okay. So, um, let me just mention quickly the default final project,
so that you've got, um,
some sense of context.
So, the materials of that will be released this Thursday.
And so, for the tasks for it is,
a textural question-answering task which is done over the,
the Stanford Question Answering Dataset, SQuAD,
which was a dataset put together, um,
by Percy Liang and the department and the student .
Um, so, we've used this as a default final project,
um, before but we're mixing up a couple of things this year.
I mean, firstly, the starter code we're providing this year is in pytorch,
to fit in with what we've done to the rest of the class.
But secondly, the SQuAD team,
released a new version of SQuAD,
SQuAD 2.0 and we're going to use that for the class this year.
And the essential difference in SQuAD 2.0,
is in SQuAD 1.1 or 1.0,
every question had an answer in the passage of text whereas in SQuAD 2.0,
a lot of questions don't have answers.
So, there's this extra significant thing that you need to do which is working out,
um, whether a question has an answer.
So, th- this is just one example,
um, which just gives you a sense of the SQuAD, what SQuAD is like.
So, there's a paragraph of text.
I've just put a subset of it here, um, Bill Aken,
adopted by Mexican movie actress, Lupe Mayorga, um,
grew up in the neighborhood town, neighboring, sorry,
neighboring town of Madeira and his song chronicled
the hardships faced by the migrant farm workers he saw as a child.
Right, there's then a question, um,
in what town did Bill,
right, actually I misspelled that sorry,
it should have been Aken without an I. I got confused with our former department chair,
Alex Aiken, I guess when I was typing.
Um, Bill Aken grow up?
And the answer you are meant to give is Madeira.
Um, so, just incidentally,
it's a random fact.
Um, so, quite a few of you know about something that was
recently in the kind of tech news, tech
news and we're going to talk about later in the class.
Um, that people, um,
from Google produced this very strong New Natural Language
Understanding representation model called BERT.
And which is one of several kind of models that are in a class of,
models that contextually model words that have come into prominence in 2017 and 18.
And in general, BERT has sort of produced very good performance for very many tasks.
Indeed, if you look at the SQuAD 2.0 leader board online, um,
at this URL, what you'll find is that
all of the leading systems use BERT in some way or another, these days.
Um, but nevertheless, this was actually a question that BERT got wrong.
Um, that BERT said,
"No answer to this question,
" rather than getting the correct answer.
Even though it looks kind of straightforward reading it as a human being.
It doesn't really look a human tricky reading comprehension question.
Um, so, that's the default final project.
So, on Thursday, I'm going to talk more about the default final project.
I'm going to talk about how people build textual question answering systems.
And the details on the default final project should all be posted by then,
but that's just to give you a bit of context of what the other choice is.
And today, I'm sort of more going to be aiming at people,
um, doing the custom final project.
But let me just sort of say a bit first about the choice between the two of them.
So, um, why might you want to choose the default final project?
So, if you have limited experience with research,
you don't have any clear idea of a research project you want to do this quarter,
you're just really busy with other classes that, uh,
you're enrolled in CS140 and you're just really loade- loaded
[LAUGHTER] now with other classes you're doing this quarter.
Um, you'd be happy to have just a clear goal towards, to work towards.
A leaderboard of your fellow students that you can compete against.
Um, do the default final project.
Um, I think for many people it's actually the good right choice.
And I mean, for what it's worth, I mean,
typically, slightly over half of people have done the default final project.
It's normally that, so 55 percent have done
the default final project and the rest the custom final project.
So, if you do the default final project,
you'll get lots of guidance.
You get lots of scaffolding.
There are clear things to aim at in what you do.
Um, the course staff are in general most prepared and most able to help you.
Um, and in particular,
I mean, the, for the bottom bullet here.
I mean, you know, something to think about in making
the choices that some of it comes down to how committed,
organized, and keen are you to be wanting to do your own custom final project.
If you've got a, something you really want to do for a custom final project, great.
We love to see interesting custom final projects.
But, you know, if you're going to end up doing something that just looks
worse like [LAUGHTER] not done as well [LAUGHTER] as you would've done a, done a project.
If you'd just done the fin-, default final project,
then you should probably choose the default final project [LAUGHTER].
Um, okay.
But even if you are doing,
think you'll do the default final project.
I hope that some of this lecture will still, um, be useful.
While the part in the middle, when I talk back about
MT and Gater or current networks are definitely useful.
But, you know, beyond that, um,
some of the tips on doing research and discussions of,
sort of looking at how to make neural networks work and error analysis, paper writing.
These are all good topics that apply to the default final project as well.
So, in the other direction, um,
if you have some research project that you're excited about.
Possibly, it's one you are already working on or possibly,
that you've just always wished to do.
Something exciting with neural networks and rap music.
Um, well, you know, that custom final project is an opportunity to do that.
Um, so, it's a chance for you to do something on your own.
Um, it, you know, obviously,
if you're not interested in textural question-answering
but do you think you might like machine translation.
Well, it's an opportunity, um,
to choose any topic of your own.
It's also a way to sort of experience much more of the research pro- process because,
you know, for the default final project, it's a bigger,
more open-ended thing than any of our assignments.
But, you know, nevertheless,
the default final project is still
sort of a pre-setup thing that you don't have to find your own problem,
find your own data,
work out a good approach to it.
A lot of that's sort of been done for you.
So, that, for a custom final project it's much more
your own job to sort of define and execute a mini research project.
And so, if all of that stuff seems appealing or some of it seems appealing,
um, then aim at the custom final project.
Um, doing this just reminded me about a fact about assignments one to five.
You know, for assignments one to five,
we are hoping that they can be a set of stepping
stones for learning how to build deep learning systems.
But, you know, one of our goals in that is to give you less hand holds as time goes by.
So, you know, assignment one was really easy and assignment three,
we tried to make it really handholdy,
so people could start to learn PyTorch.
But, you know, we're actually hoping for assignments
four and five that they're actually harder,
so that you're getting more experience of working
out how to build and do things by yourself
because if the only thing you ever see is completely scaffolded assignments.
It's sort of like when you do CS106A that you have to do a great job on
the CS106A assignments but you don't really know how to write a program by yourselves.
And that's sort of what we want to, um,
sort of get you beyond,
um, in the latter two assignments.
So, I hope you have started on assignment four.
If not, you really should start and get underway soon as Abby was emphasizing.
Okay. So, this year for the,
um, final project, whichever one you're doing.
Um, we're actually putting more structure in than we have
in previous years to encourage people to get going.
And so, in particular,
there are early on components which are worth points in the grading.
So, the first part of that is a project proposal,
um, which is, um,
we want from each team.
So, one per team, um,
you can just do a joint one,
um, which is worth five percent.
Um, so, it's, we're releasing the details on Thursday which is when
assignment four is due and it'll be due the following Thursday.
So, we're actually having an interruption in the sequence of current assignments, right.
So, for the next week, um,
what the thing to do is project proposal.
And then the week after that, um,
we're back to assignment five and then we go full time into final project.
So, what we're wanting for
the project proposal is we're actually wanting you to do a little bit
of starting off research and the fine ter- terms of reading some paper.
So, find some paper that's, um,
relevant to your research,
um, that you are going to do.
Um, read it, write a summary of what it does.
Um, write down some thoughts on how you could adapt or extend ideas in it,
in your own final project.
Um, and then say something about what your plan is for
what you're goi- hoping to do for your final project.
And especially, if you're doing a custom final project
there's more to write there because we'll want to make
sure that you have some idea as to
what data you can use and how are you going to evaluate it.
Whereas a couple of those things are actually sort of
determined for you if you're doing the default final project.
Um, and so then after that we're going to have a project milestone, um,
which is the progress report where we're hoping that you can
report that you're well along in your final project.
That you've run at least some experiment and have
some results on some data that you can talk about.
So the default- the project milestone is due on,
um, Thursday, March seven.
So it's actually more than halfway through
the period that's sort of dedicated to the final project.
So, if you are not- we sort of put it past
halfway because the fact of the matter is it always takes people time to get going,
um, but nevertheless, you know,
what you should have in your head is unless you're halfway
through by the time you're handing in your,
um, project milestone, then you're definitely behind.
And you'll be doing that typical Stanford thing of having a lot of late nights
and lack of sleep in the last week [LAUGHTER] of class trying to catch up for that.
Um, okay. So, um,
so now I've sort of, um,
want to sort of just start saying a bit of- for
custom final projects of some of the sort of
thinking and types of things that you could do about that.
Um, so you have to determine some project,
um, for- if you're doing a custom final project.
So, in philosophy of science, you know,
there are basically two ways for any field you can have a project.
You either start with some domain problem of interest.
You're [NOISE] just got something you're interested in or say,
"Gee, I'd like to do better machine translation."
And then you work out some ways to address it with technology,
or you start with some, um,
technical approach of interest.
And you say, "Oh well,
those LSTMs seemed kind of neat,
but I didn't understand why there's
that extra 10H and I think it'd be better if it changed in this other way.
And you start exploring from a technical direction to try and come up with a better idea.
And then you're wanting to prove that it works.
So in kinds of the projects that people do for this class,
this isn't quite an exhaustive list,
but this is sort of in general what people do.
So, the first category and really I think this
is the bulk of projects over half is people find
some task replication of interest and they build
some neural network models to try and do it as effectively as possible.
Um, there's a second category where people sort of concentrate on implementing,
so re-implementing some complex neural architecture and getting it to work on some data.
And so let me just say a couple of sentences on this.
Um, so, it's certainly okay for you to,
um, start by re-implementing some existing model.
Um, and some people that's as far as they get.
And then the question is, um, is that okay?
And the answer to whether that's okay sort
of largely depends on how complex your neural model is.
Um, so if what you think is okay I'm going to, um,
re-implement something like we've seen already,
like a window-based classification model and you
just re-implement that and run it on some data and get some results and stop.
That's definitely a bad project.
Um, but there are lots of very complicated and sophisticated neural,
um, architectures out there.
And if you're trying to do something complicated well then that can be a fine project.
Um, so, I actually sort of stuck in a few examples of projects.
So, I mean, here's one that was actually from a couple of years ago.
Um, so this was in the 2017 class.
And so, shortly before the 2017 class,
"Deep Mind" who's one of the um, organizations producing
the most complicated neural models had just released
a paper about the differentiable neural computer model,
which was a model of how to have something like
a differentiate- differentiable Turing machine-like
architecture inside a neural network, um,
and thought, um,
this would be a great challenge to try and, um,
re-implement the differentiable neural computer which Deep Mind hadn't released
any source code for because they're not the kind of
place that generally releases their source code.
Um, and, you know, this was actually an extremely ambitious project because it
was, it's a very complex architecture which is hard to get to train.
And so, you know, at the end,
at the end she hadn't been able to sort of train as
big a model or get as good results as they report in the paper that,
you know, frankly we thought it was pretty
miraculous that she managed to get it working at all.
In the period of time we had in the class and she did successfully do an open-source
re-implementation of this model which basically worked the same as in their paper.
Though not quite as well.
So, you know, that seemed a huge achievement.
So, you certainly can do something of that sort.
Right. So, um, so you- you can sort of from
a technical direction have some ideas for variant model and explore,
um, how to make a different kind of model class and then look
at how it works on some problem that works well.
Another kind of project you can do is an analysis project,
so that you might be interested in something in
natural language or something on the behavior of neural networks,
and just think that you want to analyze them more closely.
So, you might think, "Oh,
maybe these neural machine translation systems work great
providing the word order is the same in the source and target language,
but can they really do a good job of reordering phrases for different language types?
How much does their performance vary based on
the amount of reordering between the source and target language?"
And you could do some experiments to try and
investigate that as an analysis problem that looks at a model,
and we sometimes get projects like that.
Down at the bottom is the rarest kind of project,
which is when some people try to do something
theoretical which is to prove some properties of a system.
So if- this is easiest to do in simple systems for something like word vectors,
that if you might want to prove something about
the kind of spaces that are induced by word vectors,
and what properties you need to have in
models for word analogies to work or something like that.
Um here are just another couple of examples that so- shows some of the other classes.
So, this one is an example of find a problem and build some models.
So, these three people um, looked at Shakespearean Sonnet generation and then they considered
several different models for Shakespearean Sonnet generation and
got the best results from this sort of- you'd probably can't really see all the details,
but they have a sort of a mixture of word level and
character level gated model that feeds into
a word level LSTM and produces sonnets and the output wasn't totally bad.
"Thy youth's time and face his form shall cover.
Now all fresh beauty my love there.
Will ever time to greet forget each like ever decease,
but in a- in a best at worship his glory die."
Okay. It's maybe not perfect,
[LAUGHTER] but it sort of sounds like a Shakespearean sonnet.
Um, okay.
Yeah. So, I showed you that one already.
Um, here's, um, an example of someone who designed a different kind of network,
and this was a project that came out of this class that was then continued with,
and the- they got a conference paper out of it,
the ICLR 2017 paper.
So, this was looking at doing a better job at building a neural language model.
And essentially, they had two ideas,
both of which seem useful for building better neural language models.
And so, one is that in the stuff that we've presented so far,
whether it was the early word vectors,
or what Abby presented last week in the neural language model,
there are effectively two vectors for each word: there's one for the word encoding
on the input and then when you have the softmax on the other side effectively,
the rows of that matrix that go into the softmax are also
word vectors for determining how likely you are to produce different words.
And so, um, these two people had the idea that maybe if we actually in the model
tied those two word ve- vectors together that would help and produce a better model and,
um, and so this was actually done
several years ago when that was a novel idea which hadn't actually been done.
So, this was done in the 2016 class,
and then they had this second idea which was,
well maybe doing the kind of,
cross entropy one, zero,
sort of you look at the correct word that you are meant to
produce and sort of work out a loss based on that.
Maybe that's not very good because you don't get
partial points if you produce a different word that's semantically similar.
And so, that they had this idea that they could use
word vector similarity and then you'd be giving a score for any word that was
produced next based on how similar it was
according to word vector similarity to the word that you are
meant to produce next and that was also
a useful idea that they're able to produce improved language models with.
So, that was a cool project.
Um, here's an example of, um,
somebody from last year,
um, who did an analysis project.
So, their idea was,
um, that they- well,
they were going to, um,
evaluate on some task,
they actually did several tasks, um,
word similarity, analogy, and the SQuAD,
um, question answering system.
But the question was, okay,
a lot of neural network models are big and so aren't very suitable for phones, um,
could we get away with compressing the models a lot so that rather than having doubles,
or 32-bit floats, or even 16-bit floats,
that are now used quite a bit in neural networks, could we,
um, compress a lot more and quantize, um,
numeric values so that we can only be, say,
using two bits fo- per parameter so they'll literally need four bits per parameter?
And if you do that naively, it doesn't work.
But if you explore some cleverer ways of doing it and see how to make things work,
you can actually get it to work, um, really well.
Um, in fact, it actually seems like sometimes you can improve
your performance doing this because the quantization acts as a form of regularizer.
Um, you can find lots of other projects, um, online,
if you look at the CS224n pages and you should.
Um, okay.
So, if you want to do a final project you have to find someplace to start.
You know, one place is to start looking at papers there's
online anthology of most of the NLP conference papers.
You can look at M- ML conferences have lots of relevant papers as well.
You can look at past CS224n papers that cover lots of topics.
Um, though, you know, I- I sugge- don't also forget, um,
the advice down the bottom, um,
which is look for an interesting problem in the world.
Um, so, our Stanford's CS emeritus professor
Ed Feigenbaum likes to quote the advice of his,
um, advisor, Herb Simon, um,
of "If you see a research area where many people are working, go somewhere else."
Um, well, you know,
in the context of this class don't go so far away that you're not using
neural networks or NLP because that won't work for project for this class.
But, you know, nevertheless, I mean,
in some sense it's a bad strategy of
saying let's look at all the papers that were published last year,
and let's wo- start working on one of their problems,
or lots of people are working on question-answering, I'll do it too.
You know, there are lots of interesting different problems
in the world and if you know of some, you know,
cool website that somehow does something interesting related to language,
you know, maybe you can make a final project out of that.
Um, other ways to find final projects.
Um, so the person who's first put together most of
the CS231n content was And- Andrej Karpathy, um,
who now works at Tesla and among his other- things
he did for the world he put together this site Arxiv Sanity Preserver, um,
which is a way to find online archive papers which is
a major pre-print server and if you say a few papers you're interested in,
it'll show you other papers that you're interested in.
It'll show you papers that are currently trending.
So, that can be a good way to look.
Um, if you think it'd be just good to be in
some competition where you're wanting to
build a system that's better than other people's,
um, you can look at leaderboards for various tasks.
So, there's this brand new site which is pretty good though
not completely error free and correct, of
paperswithcode.com, and it collects a whole lot of
leaderboards for a whole lot of machine learning tasks including tons of language ones.
So, it gives leaderboards for question answering,
machine translation, named entity recognition,
language modeling, part of speech tagging.
All sorts of tasks you can find there,
and find out what the current states of the art and datasets are.
Okay. Um, so, you know,
different projects are different,
but often for a lot of projects the things you need to be making sure of is
that something that you can get a decent amount of data about so you can train a model.
It's a feasible task,
it's not so enormous you can't possibly do it in four weeks.
Um, you'll want to have some evaluation metric and
normally for deep learning you have to have-
even if you hope to do some human evaluation,
as well, you have to have some automatic evaluation metric.
Because unless there's just some code that you can run
that gives you a score for how well you're doing,
then unless you have that,
you just sort of can't do the deep learning trick of saying, "Okay,
let's, um, do backpropagation to optimize our scores according to this metric."
And pretty much you'll want to do that to be able to do neural network optimization.
Um, and we do require that there is an important part of NLP in your class project.
I mean, it doesn't have to be only thing,
you can be doing reinforcement learning as well,
or you could do images to caption, say you're
doing joint vision and NLP,
but there has to be NLP in it.
Okay. Ah, last bit before I get back onto the content from last week.
Ah, so, something that you'll need to do is have data for your project.
Um, so some people collect their own data for a project and, you know,
it's not impossible to collect your own data
especially if there's something you can do with unsupervised data.
You might be able to get it by just sort of crawling an interesting website.
You can annotate a small amount of data yourself.
If you have any site that has some kind of, you know,
ratings annotation stars on it,
you can treat those as a form of, ah, annotation.
Right? So, if you want to predict something like, um, you know,
which descriptions on product review websites
or which reviews on product review websites do people like?
Well, they get star ratings at the bottom from people and
then you can try and fit to that as your supervision.
Um, sometimes people have data from an existing project for a company.
You can use that.
But nevertheless for most people, um,
given that classes are short and things like that,
the practical thing to do is use
an existing curated dataset that's been built by previous researchers.
That normally gives you a fast start and lets you get to work building models, um,
there's obvious prior work,
there are baselines and previous systems
that you can compare your performance on, et cetera.
Okay. Um, so, where can you find data?
I'll just mention a couple of places here and there are lots more.
So, traditionally the biggest source of
linguistic data used by academics was this place called
the Linguistic Data Consortium and they have lots of
datasets for treebanks and named entities and coreference,
parallel machine, translation data,
et cetera, et cetera.
And so, um, the Linguistic Data Consortium licenses their data,
Stanford pays that license so you can use any of it.
Um, but if you want to use it, um,
you go to that, um, linguistics.stanford.edu page.
And there's a sign-up, um, ah,
piece on how to sign up where you basically, um, say,
"I will use this data only for
good Stanford purposes and not as the basis of my startup."
And, um, then you can have access to that data
and it can be made available by NFS or otherwise.
Um, but as time has gone by,
there's a ton of curated NLP data that's available on various websites.
In fact, if anything the problem is it's just sort of
spread over the web and that's sort of hard to find different things.
But there are some, some sites that have a lot of data for various purposes.
So, anything related to machine translation or just parallel,
um, data across different languages.
The statistical MT statmt.org site has a great amount of
data and that organization runs shared tasks every year,
the Workshop on Machine Translation,
WMT which Abby already mentioned in her class.
And they've got datasets that we use for
those tasks and then there are leaderboards for those tasks.
And you can find data for that.
Um, if you thought dependency parsing was cool, um,
there's the Universal Dependencies site which has parallel, not parallel site,
which has treebanks in the same annotation scheme for
about 60 different languages and you can work on
parsers for different languages and things like that.
Um, I'm not gonna bore you with going through all of them but, you know,
there are just tons and tons of other datasets that
Facebook has released datasets, Google's released datasets,
I said Stanford have released several other datasets including
the Stanford Sentiment Treebank and the Stanford Na- Natural Language, um,
Inference corpus, uh, new question-answering datasets and
including HotPotQA and conversational question answering.
Other groups at different universities have released datasets.
There are just tons of them.
You can find data on sites like Kaggle where it has machine-learning competitions.
There are sites with lists of datasets.
You can look at research papers and see what datasets they used.
And of course, you can ask the course staff or on Piazza
to try and find suitable datasets for a project.
Okay. Um, so that's a fair bit about
the projects that I've got a bit more to say later about doing projects.
Does anyone have any questions up until now on projects?
Okay. Um, well, so now we're gonna sort of, um,
flip a switch in our brains and go back and have one more look,
um, at gated recurrent units,
um, and what happens and what they mean.
Um, and, you know,
this is sort of,
sort of the same material that Abby presented,
presented a little bit differently but, you know,
I hope it might just sort of give one more way of
sort of thinking a bit about what's happening about
these gated recurrent units and why they might be doing
something useful and what are the alternatives to them.
So, if you remember the problem we started with is that we
wanted to understand sort of derivatives backward in time.
And so, the idea of that is well,
if we twiddle this a little bit at time T,
how much effect is that going to have so we make some adjustment here.
How much effect is that going to have n time steps later?
Um, and well, we sort of looked at the derivatives and we sort of saw we got these,
um, terms for each successive time step.
And so as Abby discussed the problem is that for the derivatives that we got,
we kind of got this matrix form for each time step.
And so that if we're going through a lot of time steps,
we got a lot of matrix multiplies and as the result of those matrix multiplies,
pretty much either things disappeared down to
zero or exploded upward depending on what was in the matrix.
And so that- and so that's sort of means we,
When the gradient goes to zero,
we kind of can't know what's happening there.
Whether there isn't any conditioning or just we can't measure it.
And so that's sort of made people think that maybe this naive, um,
recurrent neural network transition function just isn't a good one to use.
And that sort of leads into these ideas of gated recurrent units.
Right? Because if we have
the simple recurrent neural network where we're
sort of feeding forward for each step in time.
Well, what happens is when we backpropagate.
We have to backpropagate through
every intermediate node and that's where we sort of have our gradients disappear.
And so an idea of how you could fix that is to say well,
suppose we just put in direct connections that were longer distance, um,
then we'd also get direct backpropagation signal
and so then we wouldn't have this same problem of vanishing gradients.
And effectively, we've sort of looked at two ways in which you can achieve that effect.
Because one way of you can achieve that effect which Abby looked at
in the end part of the last lecture was this idea of attention.
So, when you've got attention,
you're actually are creating these shortcut connections,
oops, they're the blue ones, um,
from every time step and using it to calculate an attention distribution.
But the way the attention was done that we looked at,
it was sort of mushing together all previous time steps into some kind of an average.
But the idea of the gated recurrent units is in some sense we want to
achieve this same kind of ability to have shortcut connections.
But we want to do it in
a more controlled and adaptive fashion where we still do remember the position of things.
So, how can we create an adaptive shortcut connection?
And so that's, um,
what we start to do with the gates that are put into a gated recurrent network.
So, if- so first off we sort of say let's have
a candidate update which is exactly the same
as the one that's used in a simple recurrent neural network.
But what we can do is add a gate.
And so, the gate will calculate a value from zero to one.
And so what we're going to do here is mix together
using our candidate update which is just like
a simple recurrent neural network which will be then mixed together with simply
directly carrying forward the hidden state from the previous time step.
So, once we're doing that we are sort of then adaptively-
we're adaptively partly using a computation from one time step back,
um, done as a recurrent neural network.
And we're partly just inheriting the,
we're just part- sorry, we're partly inheriting
the hidden state from the previous time step.
So, it's sort of like a shortcut connection but we're waiting as to
how much we're short cutting and how much we're doing our computation.
And we control that adaptive choice by using a calculation to set the gate.
And we do that with a sigmoid, um,
computed over the import and the hidden- previous hidden state and using it again,
an equation kind of like a simple recurrent neural network.
Okay. Um, but, you know,
if you wanted to go a bit further than that,
um, you could think well,
maybe sometimes we sort of might actually
just want to get rid of the stuff that was in the past.
That maybe the stuff in the past sometimes becomes irrelevant, like,
maybe sometimes we start a new sentence or a new
thought and we just want to get rid of the stuff that's in the past.
And so, that can lead into this idea of having a second gate,
a reset gate and so the reset gate calculates a value from 0 to 1, um,
just like the other gates,
and then we're doing this element wise dot-product between
the reset gate and the previous hidden state and that's then sort of saying well,
maybe we want to keep some parts of what was stored
previously and some parts that we now want to throw away.
And so we put that into the model as a second gate.
Um, and so an interesting way to think about that is to sort of think
about this as if this recurrent neural network is like
a little tiny computer as the kind of little tiny computers you
might do in a sort of simple architecture class and if you think about it that way,
um, for the basic simple recurrent neural network
the way the tiny computer works is that you've got a bank of registers h,
your hidden state, and at each time step you have to
read- whoops, at each time step you have to read the entirety of your bank of registers,
you do some computation and then you write
the entirety of your bank of registers and, you know,
if in terms of thinking about computer architecture,
that sounds like a pretty bad way to implement a simple computer.
Um, so precisely what a gated recurrent unit is doing is saying,
"Well, maybe we can have a slightly more sophisticated little baby computer."
Instead of that, we could select a subset of the registers that we want to read.
And so, the reset gate can control that because it can say,
"We'll just ignore a bunch of the other registers."
Um, it then will compute a new value based on just these, um,
stored registers and then the update gate which is also adaptive can say, "Well,
I want you to write
some registers but the rest of the registers will just keep their previous value."
That seems a useful idea to have in a computer.
And so, that's what we're doing here.
And so, this model here is, um,
what was- Abby presented second as the gated recurrent unit.
So, this is sort of a much more realistic model
and it sort of in some sense overlaps with the ideas of attention.
Okay. Um, so gated recurrent units are actually a quite new model.
Um, the model that was done way earlier and has had huge impact
is these LSTM long short-term memory units and they are a bit more complex.
Um, but, you know,
a lot of it is sort of the same, right?
So, the hidden state of
a gated recurrent unit is kind of equivalent to the cell of the LSTM.
So, both of them are using the same idea of summing together,
a mixture of just directly interpret- directly inheriting
what you had from the previous time step together with, um,
something that you've calculated for the current time step and the way you count-
calculate it for the current time step is exactly the same in both cases.
Whoops, sorry. Both cases again that you're calculating
the current update using this sort of simple RNN equation.
So, those parts are exactly the same.
Um, but the LSTM is a little bit more complicated.
It now has three gates, um,
and it's got this extra, um,
hidden state that's then worked out with a bit more complexity.
So, in terms of my LSTM picture, you know,
the LSTM picture looks as if you sort of pull apart all of its math pretty
complex but so there are three gates so that you can forget or ignore everything.
So, you can forget or ignore the input,
you can forget or ignore parts of
your previous hidden state and you can forget or ignore parts of the cell
when calculating the output and each of these
is produce- when I say forget or ignore parts of,
what that's meaning is you're calculating a vector which is then going to be element-wise
multiplied by the import of the previous hidden state or the cell.
And so, that's why you have this effective now an addressable bank of
registers where you can use some of them but not others of them.
Okay. So, the bottom part of the LSTM is just
like a simpler simple recurrent neural network,
um, which then calculates,
um, a candidate update.
And so, for both of the GRU and the LSTM the real secret is
that rather than just keeping on multiplying
stuff what you do is you add two things together.
Um, and so this adding is why you don't
get the same vanishing gradient evil effects because you're calculating a
new candidate update and you're adding it to stuff that was
previously in the cell and that gives you
a simple gradient when you backpropagate that- that you have
direct linear connection between the cell at time t and the cell at time t minus one.
And so, really that simple addition there is sort of
the secret of most of the power of LSTMs and
this same idea of adding two things together has also been a
secret of many of the other advances in deep learning recently.
So, envision in the last couple of years the sort of standard model
that everybody uses as ResNets, residual networks and they use
exactly the same secret of allowing these adaptive updates where you add
together a current layer's value with directly inheriting a value from the layer below.
Um, other things that use similar ideas are things like highway networks and so on.
So, that's proven to be an extremely powerful idea.
Um, the LSTM is slightly different from
the GRU because when we look back at its equations
that the- the GRU kind of does a linear mixture where you have one gate value,
UT, and one minus UT,
where the LSTM adds values controlled by two different gates,
a forget gate, and an input gate.
Theoretically, having the adding of
two separate gates rather than than a mixture is theoretically more powerful.
Um, depending on the application,
sometimes it doesn't seem to make much difference, um,
but there's definitely a theoretical advantage to the LSTM there.
Okay. Um, just, I hope that's maybe a little bit more helpful to have seen those again,
um, any questions on gated recurrent units?
Still look confusing?
I think it's useful to have some kind of idea as to why the people come up with
these things and why do they make sense but,
you know, nevertheless, the reality is in the sort of era of
2015 plus any deep learning package you use whether it's PyTorch,
TensorFlow, MXNet whatever, you know,
it just comes with LSTM and GRUs and you don't have to program your own.
In fact, you're at disadvantage if you
program your own because if you are using the built-in one,
it's using an efficient CUDA kernel from
Nvidia whereas your custom built one won't and/or run three times slower.
Um, so, you know, essentially don't have to know how to do it,
you can just take the attitude that an LSTM is just like
a fancy recurrent network which will be easier to train and that's true.
Um, but you know, these kind of architectural ideas have actually been
central to most of the big advances that have come in deep learning in the last couple of years,
so there's actually good to have an ID,
to have some sense of what were
these important ideas that made everything so much better because they had
the same kind of component building blocks you might also want
to use in custom models that you design for yourself.
Okay, two bits of machine translation.
Um, so a bit of machine translation that we
sort of didn't cover next week but lots of people have been seeing
and getting confused by in the assignments so I thought I'd explain
a bit about is UNKs and explain where do UNKs
come from and why are there UNKs and the reason why
there are UNKs is effectively kind of for efficiency reasons.
So, if you sort of think about producing output in a neural machine translation system
and really this is the same as producing output
in any natural, neural natural language generation system,
so that's really the same for neural language model, um,
that if you have a very large output vocabulary is just a expensive operation.
So you have a big matrix of softmax parameters where you have a row for every word, um,
and then you have what,
[NOISE] then we have an animation that is not working for me.
Oh, all right there, there we go.
Um, so then we have some hidden state that we've
calculated in our recurrent neural network.
And so, what we gonna do is sort of multiply, um,
that vector by every row of the matrix,
put it through a softmax and then get probabilities without putting every word.
Um, and you know,
this seems pretty simple but the problem is that
to the extent that you have a humongous vocabulary here,
you just have to do a humongous number of rows
of this multiplication and it actually turns out that
doing this is the expensive part of
having a neural machine translation or neural language model system, right?
The LSTM might look complicated and hard to understand, but you know,
it's relatively small vectors that you multiply or dot-product once,
and it's not that much work whereas if you have a huge number of words,
this is a huge amount of work.
So, just for instance sort of for the pion- pioneering sequence to sequence,
um, neural machine translation system that Google first did,
they ran it on an eight GPU machine because they have lots of GPUs but
the way they set it up to maximize performance was of those eight GPUs,
three of them were running
a deep multi-layer neural sequence model and the other five GPUs,
the only thing that they were doing was calculating softmaxes because that's
actually the bulk of the computation that you need to be able to do.
Um, so the simplest way to make this, um,
computation not completely excessive is to say,
"Hey, I'll just limit the vocabulary."
Yeah I know that you can make
a million different words in English and if you look at Spanish inflections of verbs,
there are a lot of them and there's gonna be huge number of words, um,
but maybe I can just make do with a modest vocabulary and it'll be near enough.
Surely 50,000 common words,
I can cover a lot of stuff and so,
that was sort of the starting off point of neural machine translation that you,
people use the modest vocabulary like around 50,000 words.
And well, if you do that, um,
well, then what happens is you have UNKs.
So UNK means, this is an unknown word,
that's not in my vocabulary and so there are two kinds of UNKs,
they can be UNKs in the source language and you know,
they're sort of optional because, you know,
it's not actually a problem having a large source language vocabulary,
but the fact of the matter is if you've sort of trained
a model on a certain amount of data,
there are some words you aren't going to have seen,
so you are going to have words that you just didn't
see in your training data and you won't have
any pre-trained or trained word vector
for them and you can deal with that by either just treating them as UNK,
so giving them a new word vector when you encounter them.
But the tricky part is on the translation that you're wanting to
produce these rare words but they're not in your output vocabulary,
so your system is producing UNK, UNK to UNK, which is not a very good translation really.
Um, yeah, and so that was sort of what the first,
um, machine, neural machine translation systems, um, did.
And so, you know, obviously that's not
a very satisfactory state of affairs and so there's been a whole bunch of work,
um, as to how to deal with this,
so you can use methods that allow you to deal with a larger output vocabulary,
um, without the computation being excessive.
So one method of doing that is to have what's called a hierarchical softmax,
so that rather than just having a huge matrix of words,
you sort of have a tree structure in your vocabulary
so you can do calculations with hierarchical,
um, multiple small softmaxes and you can do that more quickly.
Um, I'm not gonna go through all these exam,
all these things in detail now,
I'm just sort of very quickly mentioning them and if anyone's interested, they can look.
People have used the noise-contrastive estimation idea that we
saw with Word2vec in this context as well.
So this is a way to get much faster training which is important,
it's not really a way to solve, um,
speed at translation time but, you know,
if this means you can train your system in six hours instead of
six days that's a big win and so that's a good technique to use.
Um, people have done much smarter things, so really, um,
the large vocabulary problem is basically solved
now and so the kind of things that you can do is you can produce
subsets of your vocabulary and train on particular subsets of
vocabulary at a time and then when you're testing,
you adaptively choose kind of a likely list of words that might
appear in the translation of particular sentences or passages and then
you can effectively work with sort of an appropriate subset of
a vocabulary and that's sort of an efficient technique by which you can
deal with an unlimited vocabulary but only be using
a moderate sized softmax for any particular paragraph that you're translating,
there's a paper that talks about that method.
Um, another idea is you can use attention when you do translation,
the idea talked about at the end of last time.
So if you have attention, that sort of means that you can,
you're pointing somewhere in the source and you
know what you're translating at any point in time.
So, if that word is a rare word that's not in your vocabulary,
there are things that you could do to deal with that.
I mean, firstly, if it's a rare word,
its translation is much more likely to be constant,
so you might just look it up in a dictionary or word list, um, and,
um, stick in its translation,
sometimes it's appropriate to do other things.
I mean, turns out that, you know,
quite a lot of things that unknown words turn out to be other things like, you know,
hexadecimal numbers, or FedEx tracking IDs,
or GitHub shards, or things like that.
So for a lot of things like that,
the right thing to do is just to copy them across.
And so, another thing that people have looked at is copying models,
um, in machine translation.
Okay, um, there are more ideas that you can,
we can get into to solve this and actually, um,
next week we're gonna start dealing with
some of the other ways that you could solve this, um,
but I hope there to have given you sort of a sense of,
um, sort of what these UNKs are about,
why you see them and, uh,
that there are sort of some ways that you might
deal with them but you're not expected to be doing that,
um, for assignment four.
Okay, then I just wanted to give a teeny bit more on evaluation.
Um, so Abby said a little bit about
evaluation with blue and that then comes up in the assignment,
so I just thought I'd give you a little bit more context on
that since they're being quite a few questions about it.
So, um, so the general context here is, you know,
how do you evaluate machine translation quality and sort of to this day,
if you wanted to do a first rate bang up evaluation of machine translation quality,
the way you do it is you get human beings to assess quality,
you take translations and you send them to
human beings with good bilingual skills and get them to score things.
And there are two ways that are commonly used.
One is sort of rating on
Likert scales for things like adequacy and fluency of translations,
um, but another way that often works better is asking for comparative judgments.
So here are two translations of this sentence which is better, um.
And so that's, you know,
sort of still our gold standard of translation.
Um, another way you can evaluate translation is
use your translations in the downstream task.
So, you could say "I'm gonna build
a cross-lingual question answering system and inside that system I'm,
gonna use machine translation.
I'm gonna translate the questions um,
and then try and match them against the documents.
Um, and then my score will be how good my question answering system is,
and so the machine translation system is better
if my question-answering score um, goes up."
I mean, that's kind of a nice way to do things because you're kinda then taking them in, run around needing,
needing human beings, and yet you do have
a clear numerical measure that's coming out the back end.
But it sort of has some catches because, you know,
often there will be a fairly indirect connection between
your end task and the quality of the machine translation,
and it might turn out that there certain aspects of
the machine translation like whether you get agreement endings,
right on nouns and verbs or something.
They are actually just irrelevant to your performance in the task and say you're
not assessing all aspects of um, quality.
Um, and so then the third way to do it is to come up with
some way to score the direct tasks.
So, here, um, the direct task is machine translation,
and this has been a valuable tool.
For, you know, really the last so
25 years when people are doing machine learning models,
because as soon as you have an automatic way to score things,
you can then run automated experiments to say "Let me try out these 50 different options.
Let me start varying these hyper-parameters and work out which way to do things is best."
And that importance has only grown in the deep learning era,
when all the time what we want you to do is as Abby discussed, um,
build end-to-end systems and then back
propagate throughout the entire system to improve them,
and we're doing that based on having
some objective measure which is our automatic metric.
And so, that led into the development of
automatic metrics to try and assess machine translation quality,
and the most famous and still most used one is this one called BLEU.
And so, as Abby briefly mentioned,
we have a reference translation done by human beings.
At some time a human being has to translate each piece of source material once,
but then you take a machine translation and you
score it based on the extent to which there
are one or more word sequences that appear in
the reference translation and also appear in the machine translation.
And so you are working out n-gram preci-precision scores for different values of n. So,
the standard way of doing it is you do it for one grams,
bigrams, trigrams, and four-grams.
So, word sequences of size one to four,
and you try and find for ones of those in the machine translation,
whether they also appear in the reference translation,
and there are two tricks at work here.
Um, one trick is you have to do a kind of a bipartite matching um,
because it just can't be that um,
there's a word um,
in the, in the reference translation somewhere.
Um, [NOISE] I don't know if there's.
I've got a good example here [NOISE].
Um, maybe I can only do a silly example,
but I'll do a silly example.
Um, that it's- it doesn't seem like you wanna say "Okay.
Because there's a "the" in the reference,
that means that this "the" is right and this "the" is right,
and this "the" is right and every other "the" is also right."
That sort of seems unfair.
So, you're only allowed to use each thing in the reference once in matching n-grams,
but you are allowed to use it multiple times for different order n-grams.
So, you can use it both in the uh unigram,
bigram, trigram and 4-gram.
The other idea is that although you're measuring
the precision of n-grams that are in the machine translation,
you wouldn't want people to be able to cheat by
putting almost nothing into the machine translation.
So, you might wanna game it by no matter what the source document is.
If the target language is English,
you could just um say,
"My translation is the,
because I'm pretty sure that will be in
the reference translation somewhere and I'll get 0.3 unigram,
and that's not great but I'll get something for that and I am done."
And so you wouldn't want that and so,
you're then being penalized by something called the brevity penalty if
your translation is shorter than the reference translation,
and so this BLEU metric is um forming
a geometric average of n-gram precision up to some n. Normally,
it's sort of up to four,
is how it's done.
Where it's a weighted geometric average,
where you're putting weights on the different n-grams.
Um, for the assignment, we're only using unigrams and bigrams.
So, you could say that means we're putting a weight of zero on um,
the trigrams and 4-grams.
Okay. Um, and so that's basically what we're doing.
I-I've just mentioned um couple of other things.
You might think that this is kind of random,
and so people have um,
used this idea of rather than just having one reference translation,
we could have multiple reference translations,
because that way we can allow for there being
variation and good ways of translating things,
because in language there's always lots of good ways that you can translate one sentence.
Um, people have done that quite a bit,
but people have also decided that even if you have one translation,
provided it's independent and on a kind of statistical basis,
you're still more likely to match it if your translation is a good translation.
So, it's probably okay.
Um, so when BLEU was originally um, introduced,
BLEU seemed marvelous and people drew graphs like this showing how
closely BLEU scores correlated um,
with human judgments of translation quality.
However, um, like a lot of things in life,
there are a lot of things that are great measures,
providing people aren't directly trying to optimize it,
and so what's happened since then um,
is that everybody has been trying to optimize BLEU scores,
and the result of that is that BLEU scores have gone up massively but the correlation
between BLEU scores and human judgments of
translation in quality have gone down massively,
and so we're in this current state that um, the BLEU scores,
the machines, um are pretty near the scores of human translations.
So, you know, according to BLEU scores,
we're producing almost human quality machine translation,
but if you actually look at the real quality of the translations,
they're still well behind
human beings um and because you could say the metric is being gamed.
Okay. I'll hope those things help for giving more sense um for assignment four.
Um, so now for the last um,
about 12 minutes, um,
I just now wanna um,
return to um final projects and say a little bit more um about final projects.
Um so, there many,
many different ways you can do final projects,
but just to sort of go through the steps.
I mean, you know, for a simple straightforward project,
this is kind of the steps that you want to go through.
So, you choose some tasks,
summarizing text um, producing a shorter version of a text.
You work out some dataset that you can use.
So, this is an example of the kind of tasks that there
are academic data sets for that other people have used,
and so you could just use one of those,
and that's it, you're already done or you could think "Oh no!
I'm much too creative for that.
I'm gonna come up with my own dataset [NOISE] um and get some online source and do it."
Um, and you know,
summaries of the kind of things you can find online and produce your own dataset.
Um [NOISE] I wanna say a bit in,
in just after this,
about separating off um data sets for
training and test data, so I'll delay that, but that's important.
Then, you want to work out a way to evaluate your um,
system including an automatic evaluation.
Um, normally, for summarization,
people use a slightly different metric called
ROUGE but it's sort of related to BLEU hence its name.
Um, it's the same story that it sort of works,
but human evaluation is much better.
Um, but you need- so you need to work out some metrics you can use for the project.
Um, the next thing you should do is establish a baseline.
So, if it's a well-worked on problem there might already be one,
but it's not bad to try and calculate one for yourself anyway,
and in particular what you should first have is
a very simple model and see how well it works.
So, for human language material,
often doing things like bag of words models,
whether they're just a simple classifier over
words or a new bag of words, averaging word vectors.
It's just useful to try that on the task and see how it works,
see what kinds of things it already gets right,
what kind of things it gets wrong.
You know, one possibility is you will find that
a very simple model already does great on your task.
If that's the case, um,
you have too easy a task,
and you probably need to find a task that's more challenging to work on. Um, yes.
So after that, you'll then sort of think about what could be a good kind
of neural network model that might do well, implement it,
test it um, see what kind of errors that makes and you know,
that's sort of if you've gotten that far,
you're sort of in the right space for a class project.
But, you know, it's sort of hoped that you could do more than that.
But after you've seen the errors from the first version,
you could think about how to make it better and come up with a better project,
and so I would encourage everyone,
you know, you really do want to look at the data, right?
You don't just wanna be sort of having things and files and run and say "Okay, 0.71.
Let me make some random change 0.70.
Oh, that's not a good one," repeat over.
You actually want to be sort of looking at your dataset in any way you can.
It's good to visualize the dataset to understand what's
important in it that you might be able to take advantage of,
you want to be able to look at what kind of
errors are being made because that might give you
ideas of how you could put more stuff into the model that would do better.
Um, you might wanna do some graphing of the effect of hyper-parameters,
so you can kind of understand that better.
And so, the hope is that you will try out
some other kinds of models and make things better.
And sort of one of the goals here is,
it's good if you've sort of got a well-setup experimental setup,
so you can easily turn around experiments because then you're just more
likely to be able to try several things in the time available.
Okay. Um, couple of other things I wanted to mention.
Um, one is sort of different amounts of data.
So, it's really, really important for all the stuff that we do,
that we have different sets of data.
So, we have trained data,
we have dev test data,
we have test data at least,
and sometimes it's useful to have even,
um, more data available.
So, for many of the public datasets, they're already split into different subsets like this,
but there are some that aren't.
There are some that might only have a training set,
and a test set.
And what you don't want to do is think,
"Oh, there's only a training set and a test set.
Therefore I'll just run every time on the test set."
That- that's a really invalid way to go about your research.
So, if there aren't
dev sets available or you need to do some more tuning,
and you need some separate tuning data,
you sort of have to, um,
make it for yourself by splitting off some of the training data,
and not using it for the basic training and using it for tuning,
and fo- as dev data.
Um, yes.
So, to go on about that, um, more, more.
So, the basic issue is this issue of fitting and overfitting to particular datasets.
So, when we train a model, um,
on some training data,
we train it and the error rate goes down.
And over time, we gradually overfit to the training data because we sort of
pick up on our neural network f- facts about the particular training data items,
and we just sort of start to learn them.
Now in the old days,
the fact that you overfit to the training data was seen as evil.
In modern neural network think,
we don't think it is evil what we overfit to the training data
because all neural nets that are any good overfit to the training data,
and we would be very sad if they didn't.
I'll come back to that in a moment.
But nevertheless, they're overfitting like crazy.
So, what we, but and what we want to build is something that generalizes well.
So, we have to have some separate data,
that's our validation data,
and say look at what performance looks like on the validation data.
And commonly we find that training up until some point,
improves our performance on separate validation data,
and then we start to overfit to
the training data in a way that our validation set performance gets worse.
Um, and so, then,
further training on the training data isn't useful because we're starting
to build a model that generalizes worse when run on other data.
But there's- the whole point here is,
we can only do this experiment if our validation data is separate from our training data.
If it's the same data or if it's overlapping data,
we can't draw this graph.
Um, and so, therefore, we can't do valid experiments.
Um, now you might think, "Oh, well,
maybe I can, um,
do this and just use the test set of data."
Um, but that's also invalid,
and the reason why that's invalid is,
as you do experiments,
you also start slowly over fitting to your development data.
So, the standard practice is you do a run and you get a score on the development data.
You do a second run.
You do worse on the development data,
and so you throw that second model away.
You do a third experiment.
You do better on the development data,
and so you keep that model and you repeat over 50 times.
And while some of those subsequent models you keep,
are genuinely better because you sort of worked out something good to do.
But it turns out that some of those subsequent models only sort of just happened.
You just got lucky and they happened to score better on the development data.
And so, if you kind of keep repeating that process 60 or 100 times,
you're also gradually [NOISE] overfitting on your development data,
and you get unrealistically good dev scores.
And so, that means two things.
You know, if you want to be rigorous and do a huge amount of hyper-parameter exploration,
it can be good to have a second development se- test set,
so that you have one, that you haven't overfit as much.
And if you want to have valid scores on te-
on as to what is my actual performance on independent data,
it's vital that you have separate test data that you are
not using at all in this process, right?
So, the ideal state is that,
for your real test data, um,
that you never used it at all until you've finished training your data, uh,
training your model, and then you run your final model once on the test data,
and you write up your paper and those are your results.
Now, I will be honest and say the world usually isn't
quite that perfect because after you've done that,
you then go to sleep [NOISE] and wake up thinking.
"I've got a fantastic idea of how to make my model better."
and you run off and implement that,
and it works great on the dev data,
and then for you, run it on the test data again and the numbers go up.
Um, sort of everybody does that.
Um, and you know,
in modicum it's okay,
you know, if that means you occasionally run on the test data it's not so bad, um,
but you really need to be aware of the slippery slope because,
if you then start falling into, "I've got a new model.
Let me try that one on the test data.
I've got a new model. Let me try this one on the test data."
Then you're just sort of overfitting to the test data,
and getting an unrealistically high score.
And that's precisely why a lot of the competitions like Kaggle competitions,
have a secret test dataset that you can't run on.
So, that they can do a genuine,
independent test on the actual test data.
Okay. Um, let's see, um, a couple more minutes.
So, yeah, getting your neural network to train.
Um, my two messages are, you know,
first of all, you should start with a positive attitude.
Neural networks want to learn.
If they're not learning,
you're doing something to stop them from learning.
And so, you should just stop that,
and they will learn because they want to learn.
They're just like little children.
Um, but, if the follow up to that is the grim reality that there are just tons
of things you can do that will cause
your neural networks not to learn very well or at all,
and this is the frustrating part of
this whole field because you know, it's not like a compile error.
It can just be hard to find and fix them.
And, you know, it is just really
standard that you spend more time dealing with trying to find,
and fix why it doesn't work well and getting it to work well than
you- than the time you spent writing the code for your model.
So, remember to budget for that when you're doing your final project,
it just won't work if you finish the code a day or two before the deadline.
Um, so, you need to work out what those things are,
"That can be hard," but you know experience,
experimental care, rules of thumb help.
So, there are just lots of things that are important.
So, you know, your learning rates are important.
If your learning rates are way too high, things won't learn.
If your learning rates are way too low,
they will learn very slowly and badly.
Um, initialization makes a difference.
Having good initialization often determines how well neural networks, um, learn.
Um, I have a separate slide here that I probably haven't got time to go
through all of on sort of for sequence [NOISE] models,
some of the tips of what people normally think are
good ways to get those models, um, working.
But I'll just say this one last thing.
Um, I think the strategy that you really want to
take is to work incrementally and build up slowly.
It just doesn't work to think,
"Oh I've got the mother of all models,
and build this enormously complex thing,
and then run it on the data,
and it crashes and burns."
You have no idea what to do at that point,
that the only good way is to sort of build up slowly.
So [NOISE] start with a very simple model,
get it to work,
add your bells and whistles,
extra layers and so on.
Get them to work or abandon them.
And so, try and proceed from one working model to another as much as possible.
One of- another way that you can start small and build up is with data.
The easiest way to see bugs and problems in your model,
is with the minutest possible amount of data.
So, start with a dataset of eight items.
Sometimes it's even best if those eight items are ones that are
artificial data that you designed yourself
because then you can often more easily see problems,
and what's going wrong.
So, you should train on that,
um, because it's only eight items,
training will only take seconds,
and that's really, really useful for being able to iterate quickly.
And you know, if you can't have your model get
100 percent accuracy on training and testing on those eight examples,
well, you know, either the model is woefully under powered or the model is broken,
and you've got clear things to do right there.
Um, when you go to a bigger model, um,
the standard practice with modern neural networks is,
you want to train your models.
You want models that can overfit massively on the training set.
So, in general, your models should still be getting
close to 100 percent accuracy on the training set after you've
trained it for a long time because powerful neural network models are
just really good at over-fitting to, and memorizing data.
Um, if that's not the case well, you know,
maybe you want a bigger model.
Maybe you want to have higher hidden dimensions or
add an extra layer to your neural network or something like that.
You shouldn't be scared of overfitting on the training data.
But once you've proved you can do that,
you then do want a model that also generalizes well.
And so, normally the way that you're addressing that is then by regularizing the model,
and there are different ways to regularize your model,
but we talked about in the assignment, doing dropout.
I mean, using generous dropout is
one very common and effective strategy for regularizing your models.
And so, then you've, what you want to be doing is regularizing
your model enough that the curve no longer looks like this,
but instead that your validation performance kind of levels out,
but doesn't start ramping back up again,
and that's then a sort of a sign of a well regularized model.
Okay. I will stop there,
and then we'll come back to the question-answering project on Thursday.
 Okay. Hi, everyone. Um, so let's get started again today.
So today's lecture what I'm going to do,
is be talking about, um,
question answering over text.
Um, this is another of the big successes
in using deep learning inside natural language processing,
and it's also a technology that has some really obvious commercial uses.
So it's an, it's an area that has attracted
a lot of attention in the last couple of years.
So this is the overall plan.
Um, just a couple of reminders and things at the beginning about final project stuff,
and then we'll, basically all of it is talking about question-answering starting with, um,
motivation history, um, talking about the SQuAD data,
uh, a particular simple model, our Stanford Attentive Reader.
Then talking about some other more complex,
um, stuff into the most modern stuff.
Um, yeah, so in a census, um,
lecture serves a double purpose because if you're going to do the,
the default final project, well,
it's about textual question-answering,
and this is your chance to learn something about the area of textual question-answering,
and the kinds of models you might want to be thinking about and building.
Um but the content of this lecture pretty much is in
no way specifically tied to the default final project,
apart from by subject matter that really it's telling you about
how people use neural nets to build question-answering systems.
Okay. So first just quickly on the reminders,
um, mid-quarter survey.
I mean, a huge number of people,
um, have actually filled this in already.
Uh, we already had over 60 percent, um, um,
filling-it-in rate by which by the standards of people
who do surveys they come as a huge success already.
But if you're not in that percent, um,
we'd still love to have your feedback and now's the perfect time to do it.
Um, yeah.
I just wanted to sort of have a note on custom final projects.
Um, so in general, um,
it's great to get feedback on custom final projects.
There's a formal mechanism for that which is
the project proposal that I mentioned last time.
It's also great to chat to people,
um, informally about, um, final projects.
And so I'm one of those people and I have
been talking to lots of people about final projects,
and, uh, very happy to do so.
But there's sort of a problem that there's only one of me.
Um, so I do also, um,
encourage you to realize that among the various TAs that
really lots of them have had experience of different deep learning projects,
and in particular on the office hours page,
there's a table that's like this but you can read it if you look at it on your own laptop,
which talks about the experience of different TA's.
And many of them have experience in different areas,
and many of them are also good people to talk to about final projects.
Okay. Um, so for the default final project, the textual question-answering.
So um, draft materials for that app today,
um, right now on the website actually.
Um, we're calling them draft because we think that there are still
probably a few things that are gonna get changed over the next week,
so um, don't regard as completely final in terms of the code that,
you know, it's sort of 90 percent final.
So in terms of deciding whether you're going to do, um,
a custom final project or a default final project,
and working out what you're putting into your project proposal.
Um, it should be, you know,
well more than, um,
what you need for this year.
Okay. The one other, um,
final bit I just wanted to say that I didn't get to
last time is so for the final projects,
regardless of which kind you're doing,
um, well, part of it is, um,
doing some experiments, of
doing stuff with data and code,
and getting some numbers and things like that.
But I do really, um,
encourage people to also remember that an important part of
the final project is writing a final project report.
And this is no different to any research project of the kinds that,
um, students do for conferences or journals and things like that, right?
You spend months commonly working over your code and experiments.
But in most cases,
the main evaluation of your work is from people reading,
a written paper output version of things.
So it's really important that,
that paper version sort of reflects the work
that you did and the interesting ideas that you came up with,
and explains them well and present your experiments,
and all of those things.
And so we encourage you to sort of do a good job at writing up your projects.
Um, here is just sort of a vague outline of, you know,
what a typical project write-up is likely to look like.
Now, there isn't really one size completely fits all
because depending on what you've done different things might be appropriate.
But, you know, typically the first page,
you'll have an abstract for the paper and the introduction to the paper.
You'll spend some time talking about related prior work.
Um, you'll talk about what kind of models you built for a while.
Um, there's probably some discussion of what data you are using for your projects.
Um, experiments commonly with some tables and figures about the things that you're doing.
Um, more tables and figures talking about the results as to how well your systems work.
Um, it's great to have some error analysis to see
what kind of things that you got right and wrong,
and then maybe at the end there's sort of
plans for the future, conclusions, or something like that.
Okay. Um, that's sort of it for my extra administrative reminders.
Um, are there any questions on final projects that people are dying to know?
[NOISE] Okay. Good luck.
I just meant to say good luck.
Yeah. Good luck with your final projects. [LAUGHTER] Okay.
So now moving into,
um, yeah, the question answering.
Okay. So, I mean- so question answering is
a very direct application for something that human beings,
um, want to do.
Um, well, maybe human beings don't in general want to know this.
Um, here's my query of "Who was Australia's third prime minister?".
Um, maybe, yeah, that's not really the kind of
thing you're gonna put into your queries but,
you know, maybe you query,
"Who was the lead singer of Big Thief?"
or something like that. I don't know.
Um, you're, uh, but you know,
lots- a large percentage of stuff [NOISE] on the web
is that people actually are asking for answers to questions.
And so, if I put in this query into Google,
it actually just works.
It tells me the answer is John Christian Watson.
And, um, so that's sort of question answering working in the real world.
Um, if you try different kinds of questions in Google,
you'll find that some of them work and lots of them don't work.
And when they don't work,
you're just sort of getting whatever kind of information retrieval, web search results.
Um, there is one fine point that I just wanted,
um, to mention down here.
So another thing that Google has is the Google Knowledge Graph,
which is a structured graph representation of knowledge.
And some kinds of questions,
um, being answered from that structured knowledge representation.
And so, I mean,
quite a lot of the time for things like movies,
it's coming from that structured graph.
If you're sort of saying, "Who's the director of a movie?"
or something like that.
But this answer isn't coming from that.
This answer is a genuine,
the kind of stuff we're gonna talk about today.
It's textual question answering from a web page where
Google's question and answering system has
extracted the answer and is sticking it up there.
Um, if you're, um,
wanting to explore these things, um,
if you get one of these boxes sort of down here where I've cut it off,
there's a little bit of gray that says,
"How did I get this result?".
And if you click on that,
it actually tells you what source it's getting it from and you can see if it's doing it
from the textual question answering system or from something like the Knowledge Graph.
Okay. Um, so the- in general,
the motivation for question answering is that these days there's
just these sort of massive collections of full text documents,
i.e., there's the web.
Um, so that there are sort of billions of documents of information.
And traditionally, when people first started
thinking about search information retrieval as a field,
you know, nothing of that kind of quantity and size existed, right?
That when people first started building search systems,
it was sort of unthinkable to index
whole documents because no one had hard disks big enough in those days, right?
That really- they were indexing titles or titles and abstracts or something like that.
And so, it seemed perfectly adequate in those days to say, "Okay.
We're just gonna send you- give you your results."
as to "Here's a list of documents."
because the documents are only a hundred words long.
But that's clearly not the case now when we have the sort of, you know,
ten minute read, Medium posts um, which might have the answer to a question.
And so, there's this need to sort of say, "Well,
can we just have systems that will give us answers to questions?".
And a lot of the recent changes in technology have hugely underlined that need.
So, returning documents works okay if you're sitting at your laptop,
but it works really terribly if you're on your phone and it works even more
terribly if you're trying to work with speech on a digital assistant device,
something like an Alexa system.
And so, we really want to actually be able to produce
systems that can give the answers to people's questions.
And so typically, doing that is factored into two parts.
That the first part of that is we still do information retrieval.
We use stand- normally quite standard information retrieval techniques to
find documents that quite likely to con- maintain- contain an answer.
And the reason that this is normally done by quite traditional techniques is because
the traditional techniques are extremely scalable over billions of documents,
whereas current neural systems actually
aren't really scalable over billions of documents.
But that's an area in sort of which research is ongoing.
But then once we have sort of some candidate likely documents,
we want to find, uh,
do they contain an answer,
and if so, what is the answer?
And so at that point,
we have a document or a paragraph,
and we're saying, "Can we answer this question from there?"
And then that problem is often referred to as the Reading Comprehension problem.
And so that's really what I'm gonna focus on today.
Um, Reading Comprehension isn't a new problem.
I mean it- you can trace it back into the early days of artificial intelligence and NLP.
So, back in the 70's,
a lot of NLP work was trying to do Reading Comprehension.
I mean one of the famous strands of that, um, was, um,
Sir Roger Shank was a famous,
um, early NLP person.
Though not a terribly nice man.
I don't think, actually.
Um, but the Yale School of AI was a very well-known,
um, NLP approach and really,
it was very focused on Reading Comprehension.
Um, but it's sort of,
you know, I think it was sort of the time, it was too early in any way.
It sort of died out. Nothing much came out of that.
Um, but then in- right just before the turn of the mil- millennium,
Lynette Hirschman revived this idea and said, "Well,
maybe a good challenge would be to find the kind of
Reading Comprehension questions that elementary school kids do,
and let's see if we could get,
um, computers to do that.
And some people tried that with fairly simple methods,
which only work mediocrely.
Then sort of somewhat after that, um,
Chris Burges who was a guy who was at
Microsoft Research and he wasn't really an NLP person at all.
He was a machine learning person,
but he got it into his head, um,
that while really a big problem that should be being worked on is
Machine Comprehension and he suggested that you sort of could codify it like this.
And this is a particular clean codification
that has lived on and we'll look at more today.
All right. So, a machine comprehends a passage of text.
If there's any question regarding that text that can be
answered correctly by a majority of native speakers,
that machine can provide a string,
which those speakers would agree both answers
that question and does not contain information irrelevant to that question.
Um, and he sort of proposed this as sort of a challenge problem for
artificial intelligence and set about collecting a corpus,
the MCTest corpus, which was meant to be a simple Reading Comprehension challenge.
Um, so they collected, um,
stories, um, which, um,
were meant to be kids' stories, you know.
"Alyssa got to the beach after a long trip.
She's from Charlotte. She traveled from Atlanta.
She's now in Miami".
Sort of pretty easy stuff.
And then there were questions.
"Why did Alyssa go to Miami?"
Um, and then the answer is,
"To visit some friends".
And so you've got there this string that is coming from the passage.
That's the answer to the question.
Um, so the MCTest is a corpus of
about 600 such stories and that challenge existed, and a few people worked on it.
But that never really went very far either for the next couple of years.
But what really changed things was that in 2015,
and then with more stuff in 2016,
um, deep learning people got interested in this idea of,
"Could we perhaps build neural question answering systems?"
And it seemed like if you wanted to do that, um,
something like MCTest could only be a test set
and the ways to make progress would be to do what had been done
in other domains and to actually build just- hand build a large training set of passages,
questions, and answers in such a way that would be able to train neural networks using
the kind of supervised learning techniques that we've
concentrated on so far in this class.
And indeed, the kind of supervised neural network learning techniques,
which is [NOISE] actually the successful stuff that
powers nearly all the applications of deep learning,
not only in NLP,
but also in other fields like vision.
Um, and so the first subs- the first such dataset was built by
people at DeepMind over CNN and Daily Mail news stories.
Um, but then the next year, um,
Pranav Rajpurkar is a Stanford PhD student
working with Percy Liang and a couple of other students, um,
produced the SQuAD dataset,
which was actually a much better designed dataset and proved to be
sort of much more successful at driving this forward.
And then following along from that,
other people started to produce lots of other,
um, question answering datasets which, you know,
many of them have interesting advantages
and disadvantages of their own including MS MARCO,
TriviaQA, RACE, blah, blah, blah, lots of them.
Um, but for today's class,
I'm gonna concentrate on SQuAD,
because SQuAD is actually the one that has been by far the most widely used.
And because it - it was just a well-constructed clean dataset,
that it sort of just proved a profitable one for people to work with.
[NOISE]
Okay. Um, so, that was reading comprehension.
I'll also just quickly tell you the, um,
the history of open domain question answering.
So, the difference here for the- the field of
Open-domain Question Answering that we're saying, okay,
there's an encyclopedia or there's a web crawl,
I'm just going to ask a question,
can you answer it?
So, it's this bigger task of question answering.
And, you know, that was something that again was thought about,
um, very early on.
So, there's this kind of early, um,
CACM paper by Simmons who sort of explores how you could
do answering questions as textual question-answering, um, and yet, you know,
he has the idea that what's going to
happen is you're gonna dependency parse the question,
and dependency parse sentences of the text,
and then sort of do tree matching over the dependency parses,
um, to get out the answers.
And, you know, that's in some sense
actually prefigured work that people actually were then attempting to do 35 years later.
Um, getting a bit more modern, um, Julian Kupiec,
she was working at Xerox PARC at the time,
um, came up with this system called MURAX,
and so at this stage in the 90s there started to be the first, um,
digitally available encyclopedias available,
so he was using the Grolier's Encyclopedia,
and so he said about trying to build a system that could answer
questions over that encyclopedia using,
in general, fairly sort of shallow, um,
linguistic processing methods, i.e, regular expressions.
Um, for, after [LAUGHTER] having, um,
done information retrieval search over that.
But that started to evoke more interest from other people,
and so in 1999 the US National Institutes of Standards and Technology, um,
instituted a TREC question-answering track where the idea was,
there was a large collection of News-wire documents,
and you could be asked to provide the question of them,
and lots of people started to build question answering systems.
Indeed, if in some sense that was
this competition which was where people at IBM started,
um, working on textual question-answering,
and then, um, sort of a decade later, um,
IBM rejigged things into the sexier format of,
um, let's build a Jeopardy contestant rather than let's answer questions from the news,
and that then led to their DeepQA system in 2011.
Which I presume quite a few of you saw,
these people saw Jeopardy IBM?
Yeah, some of you.
Okay. So, that they were able to successfully, um,
build a question answering system that could compete at Jeopardy, um, and win.
Um, and, you know, like a lot of these demonstrations of
technological success there are things you can quibble about the way it was set up,
um, that really the kind of computer just had
a speed advantage versus the human beings that had to buzz in to answer the question.
But, you know, nevertheless, fundamentally,
the textual question-answering had to work,
that this was a system that was answering questions mainly based on textual passages,
and it had to be able to find the answers to those questions correctly,
for the system to work.
Um, so then, more recently again, um,
and really the first piece of work that did this with a neural system was,
um, work that was, um,
done by a Stanford PhD student,
that I'll get to later,
was then the idea of well,
could we replace traditional complex question answering systems
by using a neural reading comprehension system,
and that's proved to be very successful.
So, to, to explain that a little bit more, um,
if you look at the kind of systems that were built for TREC question-answering,
um, they were very complex multi-part systems.
And really, if you then look at something like,
IBM's Deep QA system it was sort of like this
times 10 because it both had very complex systems like this,
but it ensembled together sort of six different components in every place,
and then did sort of,
um, classify a combination on top of them.
But so far, the current-.
This is sort of around a sort of a 2003 question answering system,
and so the kind of things that went through is,
so when there was a question,
it parsed the question with a parser
kind of like the ones we saw with our dependency parsers.
It did some sort of
handwritten semantic normalization rules to try and get them into a better semantic form.
It then had a question type classifier which tried to
work out what kind of semantic type is this question looking for,
is it looking for a person name,
or a country name,
or a temperature, or something like that.
Um, it would, um, then, um,
have an information retrieval system out of the document collection,
um, which would find paragraphs that were likely to contain the answers.
Um, and then it would have a method of ranking
those paragraph choices to see which ones are likely to have the answers.
Um, it would then,
um, over there somewhere, um,
run Named Entity Recognition on those passages to find entities that were in them.
These systems depended strongly on the use of
fine matching entities because then it could look for
an entity which corresponded to the question type.
Um, then once it had candidate entities,
it had to actually try and determine whether
these entities did or didn't answer the question.
So, these people, this is the system from LCC by,
um, Sanda Harabagiu and Dan Moldovan.
They actually had some quite interesting stuff here,
where they had a kind of a loose theorem prover that would try and prove that, um,
the semantic form of a piece of text,
um, gave an answer to what the question was.
So, you know, that was kind of cool stuff with an Axiomatic Knowledge Base,
um, and eventually out would come an answer.
Um, so, you know, something that is,
I do just want to emphasize, you know,
sometimes with these deep learning courses you get these days,
the impression you have is that absolutely nothing worked before 2014,
uh, when we got back to deep learning,
and that's not actually true.
So, these kind of factoid question on,
these kind of question answering systems within
a certain domain actually really worked rather well.
Um, so, I started saying the word Factoid Question Answering,
and so let me explain that because that's the secret.
So, people, at least in NLP,
use the term "Factoid Question Answering" to mean
the case that your answer is a named entity.
So, it's sort of something like, you know,
what year was Elvis Presley born,
or what is the name of Beyonce's husband, or, um,
you know, which state,
um, has the most pork or something, I don't know.
Right, anything that's got,
anything that's sort of the answer is sort of some clear semantic type entity,
and that's your answer.
I mean, so, within the space of those kind of questions,
which actually is a significant part of the questions you get in web search, right?
Lots of web search is just, you know,
who was the star of this movie,
or what year was somebody born, right?
There's zillions of those all the time.
These systems actually really did work quite well
that they could get about 70 percent of those questions right,
um, which wasn't bad at all, um,
though that they really sort of didn't really
extend it out to other kinds of stuff beyond that.
But whatever virtues they had, um,
they were extremely complex systems that people spent years put togeth- putting together,
which had many components with a huge amount of hand-built stuff.
And most of the stuff was sort of built quite separately and tied together,
and you just sort of hope that it worked,
um, well, when put together in composite.
And so we can contrast that to what we then see later,
um, for neural network-style systems.
Okay. Um, so let me now say some more stuff about, um,
the Stanford Question Answering Dataset or SQuAD that I just mentioned a little bit ago,
and as this is the data for the default final project as well.
Um, so what SQuAD has is,
questions in SQuAD have a passage,
which is a paragraph from Wikipedia.
And then there is a question,
here it's, "Which team won Super Bowl 50?"
And the goal of the system is to come up with the answer to this question.
Um, human reading comprehension.
What is the answer to the question?
[NOISE]
Broncos.
Broncos. [LAUGHTER] Okay.
Yeah. Um, so that's the answer to the question.
Um, and so by construction for SQuAD,
the answer to a question is always a sub-sequence of words from the passage which is,
normally, it ends up being referred to as a span,
a sub-sequence of words from the passage.
So that's the only kind of questions you can have.
You can't have questions that are counting questions,
or yes, no questions, or anything like that.
You can just pick out a sub-sequence.
Um, okay.
But, um, so they created in the first version about 100,000 examples.
So there are a bunch of questions about each passage.
So it's sort of something like, um,
I think it's maybe sort of about five questions per passage,
and there are 20,000 different bits that Wikipedia uses, used.
Um, and this sort of must be a span form,
as often referred to as extractive question answering.
Okay. Um, here's just one more example
that can give you some more sense of some of the things that are there,
and it illustrates a couple of other factors.
Um, so, you know,
even this one, I guess the previous one wasn't, um,
completely obvious what your answers should be because
maybe you could say the answer should just have been Broncos,
or you could have said it was Denver Broncos.
Um, and in general,
even if you're answering with a span,
there's gonna be variation as to how long a span you choose.
Um, so what they did, um,
and so this was done with, on Mechanical Turk,
gathering the data, or building questions,
and getting answers, is that they got answers from three different people.
So here's this question,
"Along with non-governmental and non-state schools,
what is another name for private schools?"
And three human beings were asked the answer based on this passage.
And one said independent,
and two said independent schools.
Um, this one, all three people gave the same answer.
This one, again, you get two different answers,
so that they sample three answers.
And basically, then, you can be correct if you're going with any of the answers.
And so that sort of at least gives you a bit of robustness to variation in human answers.
Okay. And that starts me into the topic of evaluation.
Um, yeah.
And these slides here are entitled
SQuAD version 1.1 because that means in five minutes time,
I'm gonna tell you about SQuAD version 2,
which adds a bit more stuff into it,
but we'll just get 1.1 straight first.
All right. So there are three answers that col- were collected.
And so for evaluation metrics,
they suggested two evaluation metrics.
The first one is exact match.
So you're going to return a span.
If the span is one of these three,
you get one point,
and if the scan,
span is not one of these three,
you get zero for that question.
And then your accuracy is just the percent correct,
so that's extremely simple.
But the second metric, and actually,
the one that was favored as the primary metric,
was an F1 metric.
So what you do for this F1 metric
is you're matching at the word level for the different answers.
So you've treat each,
you treat the system span and each gold answer as a bag of words,
and then you work out a precision, which is,
um, the percent of words in the system's answer that are actually in a span,
i- in a gold span, the recall,
which is the percent of words in a gold span that are in the system's span.
And then you calculate the harmonic mean of those two numbers
and the harmonic mean is sort of a very conservative average.
So it's close to the mean of those two numbers,
and that gives you a score.
And what you then do is, for each question,
you'd return, you say its score is
the maximum F1 over the three different answers that were collected from human beings.
And then for the whole, um, dataset,
you then average those F1 scores across questions and that's then your final F1 result.
So that's a more complicated thing to say.
Um, and we provide there sort of a val code,
um, for you that does that.
Um, but it sort of seems that F1 is actually
a more reliable and better measure because if you use exact match,
you know, even though there's of,
a bit of robustness that comes on three people's answers,
three is not a very large sample,
so there's sort of a bit of guessing as to whether you get
exactly the same span some human being got,
whereas you're sort of going to get a reasonable score
in the F1 even if your boundaries are off by a little.
So the F1 metric sort of, um,
is more reliable and avoids various kinds of artifacts as to how big
or small an answer human beings tend to choose in some circumstances.
Um, and so that's sort of being used as
the primary metric that people score people on in the leader boards.
Um, final detail, both metrics, um,
ignore punctuation and the English articles a, an, the.
Okay. Um, so how did things work out?
Um, so for SQuAD version 1.1, um.
A long time ago,
at the end of 2016,
um, this is how the leaderboard looked.
Um, this is the bottom of the leaderboard at this point in
time because that allows me to show you a couple of things.
So down at the bottom of the leaderboard, um,
so they tested how well human beings did, um,
at answering these questions because you know,
human beings aren't perfect at answering questions either.
Um, and so the human performance that they measured,
um, had an F1 score of 91.2.
And I'll come back to that again in a minute.
Um, and so when they built the dataset,
they built a logistic regression baseline which was sort of a conventional NLP system.
So, they dependency parsed the question and sentences of the answer.
They looked for dependency.
So dependency link matches,
so a word at both ends with the dependency relation in
between and count and matches of those and sort of pointing to a likely answer.
Um, so as sort of a fairly competently built traditional NLP system of it's
not as complex as but it's sort of in
the same vein of that early question answering system I mentioned.
And it got an F1 of about 51.
So not hopeless, um,
but not that great compared to human beings.
And so, very shortly after that, um,
people then started building
neural network systems to try and do better at this task on this dataset.
And so, one of the first people to do this quite successfully,
um, were these people from Singapore Management University,
maybe not the first place you would have thought of but, um,
they were really sort of the first people who showed that, yes,
you could build an end-to-end trained neural network
for this task and do rather better.
And so, they got up to 67 F1.
Um, and well, then they had a second system.
They got 70 and then things started,
um, to, um, go on.
So that even by,
um, the end of 2016,
um, there started to be systems that really worked rather well on this task.
Um, so here, this time was the,
um, top of the leaderboard.
So I'll talk later about this BiDAF system from, uh,
the AI to,
Allen Institute for Artificial Intelligence and the University of Washington.
So, it was getting to 77 as
a single system that like in just about all machine learning,
people pretty soon noticed that if you made
an ensemble of identically structured systems,
you could push the number higher and so if you ensemble those,
you could then get another sort of whatever it is about four points
and get up to 81, um, F1.
And so this was sort of around the situation when in the, uh, 2017, um,
224N class, we first used SQuAD version one as jus- as a default final project.
And at that point, you know,
actually the best students got almost to the top of this leaderboard.
So our best, um,
CS224N Final Project in winter 2017 made it into,
um, the equivalent of fourth place on this leaderboard,
um, with 77.5 as their score.
So that was really rather cool.
Um, but that's a couple of years ago and since then,
people have started building, um,
bigger and bigger and more and more complex, um, systems.
And, um, so essentially,
you could sort of say that SQuAD version one is basically solved.
So the very best systems are now getting
F1 scores that are in the low 90s and in particular,
you can see that the best couple of, um,
systems have higher F1s and
well higher exact matches than what was measured for human beings.
Uh, but like a lot of the claims of
deep learning being better and performing from human being,
than human beings, there's sort of some asterisks you can put after that.
I mean, in particular for this dataset,
the way they measured human performance was a little bit
unfair because they only actually collected three human beings' answers.
So, to judge, um, the human performance,
the hu- those hu- each of those humans was being scored versus only two other humans.
And so, that means you only had two chances to match instead of three.
So, there's actually sort of a systematic underscoring of the human performance.
But whatever, systems got very good at doing this.
Um, so the next step, um,
was then to introduce, uh,
the SQuAD vers- version 2 task.
And so many people felt that a defect of SQuAD version
1 was that in all cases, questions had answers.
So, that you just had to find the answer in the paragraph,
um, and so that's sort of turned into a kind of a ranking task.
You just had to work out what seems the most likely answer.
I'll return that without really having
any idea whether it was an answer to the question or not.
And so, for SQuAD version two,
for the dev and test sets,
half of the questions have answers and half of
the questions just don't have an answer in the passage,
um, it's slightly different distribution, the training data.
Um, and the way it works for scoring is the sort of, like,
the no answer kind of counts as like one word as a sort of a special token.
So, if it's, if it should be a no answer and you say no answer,
you get a score of one on the either exact match or the F-measure.
And if you don't do that,
you get a score of zero.
Um, and so, the simplest way of approaching SQuAD 2.0 would be to say, well,
rather than just always returning the best match in my system,
I'll use some kind of threshold and only if the score is above a threshold,
our counters and answer.
You could do more sophisticated things.
So another area that we've worked on quite a bit at Stanford is
this natural language inference task that I'll talk about later in the course.
Um, but that's really about saying whether one piece of,
um, text is the conclusion of another,
um, piece of text.
And so that's sort of a way that you can try and see whether, uh,
a piece of text actually gives you a justification and answer to what the question was.
But at any rate, this trying to decide whether
you've actually got an answer or not is a quite difficult problem in many cases.
So here's an example from SQuAD, um, 2.0.
So Genghis Khan united the Mongol and Turkic tribes of
the steppes and became Great Khan in 1206.
He and his successors expanded the Mongol Empire across Asia,
blah, blah, blah, blah.
And the question is,
when did Genghis Khan kill Great Khan?
And the answer to that is,
you know, uh, there isn't an answer because actually,
Genghis Khan was a person named Great Khan and he didn't kill a Great Khan.
It's just not a question with an answer.
Um, but it's precisely what happens with systems is, you know,
even though these systems get high scores in terms of points,
they don't actually understand human language that well.
So they look at something that says,
when did Genghis Khan kill Great Khan?
Well, this is something that's looking for a date and there are
some obvious dates in this passage there's 1206, 1234,
1251 and well, there's kill,
and kill looks a little bit similar to destroyed.
I can see the word destroyed.
So that probably kind of matches.
And then we're talking about, um,
Genghis Khan and there,
I can see Genghis and Khan in this passage.
And so it sort of puts that together and says
1234 is the answer when that isn't the answer at all.
And that's actually kind of pretty typical of the behavior of these systems.
And so that, on the one hand, they work great.
On the other hand, they don't actually understand that much,
and effectively asking whether there's,
this question is actually answered in the passage is a way of
revealing the extent to which these models
do or don't understand what's actually going on.
Okay. So, at the time, um,
they built SQuAD version 2.0.
They took some of, um,
the existing SQuAD version one's systems,
and, um, modified them in a very simple way.
I put in a threshold, um,
score as to how good the final match was deemed to be,
and said, Well, how well do you do on SQuAD 2.0?
And the kind of systems that we saw doing well before,
now didn't do that well,
so something like the BiDAF system that we mentioned before was now scoring about 62 F1,
so that that was sort of hugely lowering
its performance and reflecting the limits of understanding.
Um, but it turned out actually that this problem didn't prove to
be q- quite as difficult as the dataset authors,
um, maybe thought either.
Um, because it turns out that um,
here we are now in February 2019,
and if you look at the top of the leaderboard,
we're kind of getting close again to the point
where the best systems are almost as good as human beings.
So, um, the current top rate system there you can see is getting 87.6 F1,
which is less than two points behind where the human beings are.
Um, the SQuAD version 2 they also co- corrected the,
um, scoring of human beings,
so it's more of a fair evaluation this time, um,
so there's still a bit of a gap but, you know,
the systems are actually doing, um, really well.
And the interesting thing there is,
you know, on the one hand these systems are impressively good.
Um, you can go on the SQuAD website and look
at the output of several of the good systems,
and you can see that there are just a ton of things that they get right.
They're absolutely not bad systems.
You have to be a good system to be getting five out of six of the questions right.
Um, but, you know, on the other hand they still
make quite elementary Natural Language Understanding Errors.
And so here's an example of one of those.
Okay, so this one,
the Yuan dynasty is considered both a successor to
the Mongol Empire and an imperial Chinese dynasty.
It was the khanate ruled by the successors of
Mongke Khan after the division of the Mongol Empire.
In official Chinese histories the Yuan dynasty bore the Mandate of Heaven,
following the Song dynasty and preceding the Ming dynasty.
Okay. And then the question is,
what dynasty came before the Yuan?
And that's a pretty easy question,
I'd hope, for a human being.
Everyone can answer that question?
Okay, um, yeah, so it says in official Chinese histories Yuan Dynast- uh,
sorry the next sentence.
Um, yeah followed- right the Yuan Dynasty following
the Song dynasty and preceding the Ming dynasty.
But, you know actually um,
this sort of the leading um,
Google BERT model says that it was the Ming dynasty that came before
the Yuan Dynasty which you know is sort of elementarily
wrong that reveals some of the same kind of it's
not really understanding everything but it's doing a sort of a matching problem still.
Okay. So, this SQuAD dataset has been useful and good.
It still has some major limitations and I just thought I'd
mentioned what a few of those are so you're aware of some of the issues.
So one of them I've already mentioned, right,
that you're in this space where all answers are a span from the passage.
And that just limits the kind of questions you can
ask and the kind of difficult situations there can be.
So, there can't be yes-no questions counting
questions or even any of the sort of more difficult implicit questions.
So, if you think back to when you were in middle school and did reading comprehension,
I mean, it wasn't typically um,
the case um, that you're being asked
questions that were just stated explicitly in the text of,
you know, Sue is visiting her mother in Miami.
And the question was,
who was visiting in Miami?
That wasn't the kind of questions you were asked you were normally asked questions um,
like um, you know,
um, Sue is going to a job interview this morning,
um, it's a really important job interview for her future.
At breakfast she um,
starts buttering both sides of her piece of toast um,
and you are asked a question like, um,
why um, is Sue buttering both sides of her piece of toast?
And you're meant to be able to answer,
"She's distracted by her important job interview coming up later in the day."
Which isn't the- something that you can answer um,
by just picking out a sub span.
Um, a second problem which is sort of actually a bigger problem is um,
the way SQuAD was constructed for ease
and not to be too expensive and various other reasons was um,
paragraphs of Wikipedia were selected and then,
Mechanical Turkers were hired to say,
"Come up with some questions um,
that can be answered by this this passage in version 1.1."
And then in version two they were said- told,
"Also come up with some questions that
look like they're related to this passage but aren't actually answered in the passage."
But, in all cases people were coming up with
the questions staring at the passage and if you do that,
it means that your questions are strongly
overlapping with the passage both in terms of the,
the words that are used and even the syntactic structures that are
used for your questions tending to match the syntactic structures of the passage.
And so that makes question answering um, naturally easy.
What happens in the real world,
is this human beings think up questions and
type something into a search engine and the way
that they type it in is completely distinct
from the way something might be worded on a website.
So that they might be saying something like,
you know, "In what year did the price of hard disks drop below a dollar a megabyte?"
Um, and the webpage will say something like
the cost of hard disks has being dropping for many years um,
in I know whenever it was 2004 prices eventually crossed um,
the dollar megabyte barrier or something like that.
But there's a quite different discussion of the ideas.
And that kinda matching is much harder and that's one of
the things that people have done other datasets have tried to do differently.
Um, another limitation is that these questions and
answers are very much, find the sentence that's addressing the fact,
match your question to the sentence,
return the right thing,
that there's nothing sort of more difficult than involves multi sentence,
combine facts together styles of inferencing,
that the limits of cross sentence stuff there is pretty much limited to
resolving co-reference which is something we'll talk about later in the class,
that means that you see a he or she or an it,
and you can work out who that refers to earlier in the, this course.
Um, nevertheless, despite all those disadvantages,
it sort of proved that SQuAD was, you know,
well-targeted in terms of its level of difficulty, well-structured,
clean dataset, and it's just been
sort of everybody's favorite for a question answering dataset.
It also seems to have proved that actually for
people who work in industry and want to build a question answering system,
starting off by training a model in SQuAD,
actually turns out to work pretty well it turns out.
I mean, it's not everything you want to do.
You definitely wanna have relevant in domain data and be using that as well,
but you know, it turns out that it seems to actually be a quite useful starting point.
Okay. So, what I wanted to show you now was a- is a concrete,
simple, neural question answering system.
Um, and this is the model that was built by here and I guess she was
sort of an Abby predecessor since she was the preceding head TA for CS 224N.
Um, so this system,
um, Stanford Attentive Reader it kind of gets called now.
I mean, this is sort of essentially
the simplest neural question answering system that works pretty well.
So, it's not a bad thing to have in mind as
a baseline and it's not the current state of the art by any means.
But you know, if you're sort of wondering what's the simplest thing that I can build
that basically works as a question answering system decently,
this is basically it.
Um, okay. So how does this work?
So the way it works is like this.
So, first of all,
we have a question which team won Super Bowl 50?
And what we're gonna wanna do is build a representation of a question as a vector.
And the way we can do that is like this,
for each word in the question,
we look up a word embedding.
So, in particular it used GloVe- GloVe 300 dimensional word embeddings.
Um, we then run an LSTM
forward through the question and then kind of like Abby talked about,
we actually make it a bi-LSTM.
So, we run a second LSTM backwards through the question.
And so then, we grab the end state of both LSTMs
and we simply concatenate them together into a vector of dimension 2D if,
if our hidden states of the LSTM are dimension
d and we say that is the representation of the question.
Okay. So, once we have that,
we then start looking at the passage.
And so, for the start of dealing with the passage,
we do the same thing.
We, um, look up a word vector for every word in
the passage and we run a bidirectional LSTM,
now being represented a bit more compactly um, across the passage.
But then we have to do a little bit more work because we actually
have to find the answer in the passage.
And so what we're gonna do is use
the question representation to sort of work out where the answer is using attention.
So this is a different use of attention to machine translation.
That kind of attention equations are still exactly the same.
But we've now got this sort of one question vector that we gonna be trying to
match against to return the answer.
So, what we do is we, um,
work out an attention score between
each word's bi-LSTM representation and the question.
And so the way that's being done is we're using this bi-linear attention,
um, that um, Abby briefly discussed and we'll see more of today.
We've got the question vector,
the vector for a particular position in the passage
to the two concatenated LSTM hidden states.
So they're the same dimensionality.
We have this intervening learn W matrix.
So, we work out that quantity,
um, for each position,
and then we put that through a softmax which will give us
probabilities over the different words in the passage.
Um, and those give us,
um, our attention weights.
And so at that point we have attention weights,
um, for different positions, um,
in the passage and we just declare that,
um, that is where,
um, the answer starts.
Um, and then to get the end of the answer,
we simply do exactly the same thing again apart from we train a different W matrix here,
and we have that,
um, predict the end token.
And there's something a little bit subtle here.
Um, because, you know, really we're asking it to sort
of predict the starts and the ends of the answer,
and you might think, but wait a minute.
Surely, we need to look at the middle of the answer as well because maybe the,
the most indicative words are actually going to be in the middle of the answer.
Um, but, you know, really really what we're,
we're sort of implicitly telling the model of well,
when you're training, if there's stuff in the middle that's useful,
it's the bi-LSTM's job to push it to the extremes of the span,
so that this simple bi-linear attention
will be able to get a big score at the start of the span.
And you might also think there's something
funny that this equation and that equation are exactly the same.
So, how come one of them is meant to know it's picking up beginning, um,
and the other at the end?
And again, you know, we're not doing anything to impose that.
We're just saying, neural network.
It is your job to learn.
Um, you have to learn a matrix here and a different one over there,
so that one of them will pick out parts of the representation that
indicate starts of answer spans and the other one ends of answer spans.
And so, that will then again pressure
the neural network to sort of self organize itself in
such a way that there'll be some parts of
this hidden representation that will be good at learning starts of spans.
You know, maybe there'll be carried backwards by
the backwards LSTM and and some parts of it will be good at
learning where the spans end and then
the W matrix will be able to pick out those parts of the representation.
Um, but yeah, uh,
that's the system. Um, yeah.
So, um, so this is
the basic Stanford Attentive Reader model and it's just no more complex than that.
Um, and the interesting thing is, you know,
that very simple model actually works nicely well.
Um, so this is going back in time.
Again, this was the February 2017 SQuAD version 1 leaderboard.
Um, but at that time, that provide- like,
it always in neural networks quite a bit of your success
is training your hyperparameters and optimizing your model really well.
And some time, you know,
it's been repeatedly proven in neural network land that often you can get
much better scores than you would think from
very simple models if you optimize them really well.
So there have been multiple cycles in sort of
deep learning research where there
was a paper that did something and then the next person says,
"Here's a more- more- more complex model that
works better," and then someone else published a paper saying,
"Here's an even more complex than that model that works
better," and then someone points out, "No.
If you go back to the first model and just really train its hyperparameters well,
you can beat both of those two models."
And that was effectively the case about what
was happening with the Stanford Attentive Reader.
That, you know, back in- back in February 2017,
if you just train this model really well,
it could actually outperform most of the early SQuAD systems.
I mean, in particular,
it could outperform, um, the BiDAF,
the version of BiDAF that was around in early 2017 and,
you know, various of these other systems from other people.
But it was actually, at that time,
it was pretty close to the best system that anyone had built.
Um, as I've already pointed out to you,
um, the numbers have gone up a lot since then.
So I'm not claiming that, um,
this system is still as good as the best systems that you can build. But there you go.
Um, so that's the simple system that already works pretty well,
but of course you want this system to work better.
Um, and so Danqi did quite a bit of work on that.
And so here I'll just mention a few things for, um,
Stanford Attentive Reader++ as to
what kind of things can you do to make the model better.
And so here's a sort of a picture of, um,
the sort of the improved system and we'll go through
some of the differences and what makes it better.
Um, there's something I didn't have before that I should just mention, right?
Sort of this whole model, all the parameters of this model are just trained end to end,
where your training objective is simply, um,
working out how accurately you're predicting
the start position and how accurately you're predicting
the end position so that the attention gives
you a probability distribution over start positions and end positions.
So you're just being asked what probability estimate
are you giving to the true start position and the true end position.
And to the extent that though,
you know, those aren't one,
you've then got loss that is then being sort of summed in terms of log probability.
Okay. So how is this model, um,
more complex now than what I showed before?
Essentially in two main ways.
So the first one is looking at the question,
we still run the BiLSTM as before.
Um, but now what we're going to do is it's a little bit crude
just to take the end states of the LSTM and concatenate them together.
It turns out that you can do better by making use of all states in an LSTM.
And this is true for most tasks where you
want some kind of sentence representation from a sequence model.
It turns out you can generally gain by using
all of them rather than just the endpoints or that.
Um, so but this is just an interesting general thing to know again because, you know,
this is actually another variant of how that- how you can use attention.
There are, you know, a lot of sort of the last two years of neural NLP can be summed
up as people have found a lot of clever ways to use
attention and that's been pairing just about all the advances.
Um, so what we wanna do is we want to have attention over the positions in this LSTM.
But, you know, this- we're processing the query first.
So it sort of seems like we've got nothing to calculate attention with respect to.
So what we do is we just invent something.
So we just sort of invent.
Here is a vector and it's sometimes called a sentinel or some word like that,
but, you know, we just in our PyTorch say,
"Here is a vector.
Um, we're going to calculate, um,
we initialize it randomly,
and we're gonna calculate attention with respect to that vector,
and we're going to use those attention scores, um, to, um,
work out where to pay attention, um,
in this BiLSTM, and then we just sort of train that vector so it gets values.
And so then we end up with a weighted sum of the time
steps of that LSTM that uh, then form the question representation.
Um, second change, uh,
the pictures only show a shallow BiLSTM but, you know,
it turns out you can do better if you have a deep BiLSTM and say
use a three-layer deep BiLSTM rather than a single layer.
Okay. Then the other changes in
the passage representations and this part arguably gets a little bit more hacky,
um, but there are things that you can do that make the numbers go up, I guess.
Um, okay.
So- so firstly for the representation of words rather than only using
the GloVe representation that the input vectors are
expanded so that- so a named entity recognizer and a part of speech tagger is run.
And since those are sort of small sets of values,
that the output of those is just one-hot encoded and concatenated onto
the word vector, so it represents if it's
a location or a person name and whether it's a noun or a verb.
Um, word frequency proves to be a bit useful.
So there's your concatenating on sort of a representation of the word frequency as,
um, just sort of a float of the unigram probability.
Um, and then this part is kind of key to getting some further advances which is, well,
it turns out that we can do a better job by doing some sort
of better understanding of the matching between the question and the passage.
And, um, this feature seems like it's
very simple but turns out to actually give you quite a lot of value.
So you're simply saying for each word in the question,
uh, so for each word- well, I said that wrong.
For each word in the passage,
you were just saying, "Does this word appear in the question?"
And if so you're setting a one bit into
the input and that's done in three different ways: exact match,
uncased match, and lemma match.
So that means something like drive and driving, um,
will match, and just that sort of
indicator of here's where in the passage that's in the question.
In theory, the system should be able to work that out
anyway that explicitly indicate and it gives quite a bit of value.
And then this last one does a sort of a softer version of that where it's using word
embedding similarities to sort of calculate
a kind of similarity between questions and answers,
and that's a slightly complex equation that you can look up.
But effectively, um, that you're getting the embedding of words and the question answers.
Each of those, you're running through a single hidden layer,
neural network, you know,
dot producting it, and then putting all that through a Softmax,
and that kind of gives you a sort of word similarity score and that helps as well.
Okay. So here's the kind of just overall picture this gives you.
So if you remember, um, um,
there was the sort of the classical NLP
with logistic regression baseline, there's around 51.
So for sort of a fairly simple model,
like the Stanford Attentive Reader,
it gives you an enormous boost in performance, right?
That's giving you close to 30 percent performance gain.
And then, you know, from there,
people have kept on pushing up neural systems.
But, you know, so this gives you kind of in some sense three quarters of
the value over the traditional NLP system and in the much more,
um, complex, um, neural systems that come after it.
Um, yeah.
In terms of error reduction,
they're huge but it's sort of more like they're giving you the sort of,
um, 12 percent after that.
Why did these systems work such a ton better um, than traditional systems?
And so we actually did some error analysis of this and, you know,
it turns out that most of their gains is because they can just do
better semantic matching of word similarities
or rephrasings that are semantically related but don't use the same words.
So, to- to the extent that the question is where was Christopher Manning born?
And the sentence says Christopher Manning was born in Australia,
a traditional NLP system would get that right too.
But that to the extent that you being able to get it right,
depends on being able to match,
sort of looser semantic matches so that we understand the sort of um,
you know, the place of birth has to be matching was born or something.
That's where the neural systems actually do work much much better.
Okay. So, that's not the end of the story on question-answering systems.
And I wanted to say just a little bit about um,
more complex systems to give you some idea um,
of what goes on after that.
Um, but before I go further into that,
are there any questions on uh,
up until now, Stanford Attentive Reader?
[NOISE] Yeah.
I have a question about attention in general.
Every example we've seen has been just linear mapping with a weight matrix.
Has anybody tried to convert that to a deep neural network and see what happens?
Um, so yes they have.
Well, at least a shallow neural network.
Um, I'll actually show an example of that in just a minute.
So maybe I will um, save it till then.
But yeah absolutely, um,
yeah people have done that and that can be a good thing to um, play with.
Anything else? Okay. Um, okay.
So, this is a picture of the BiDAF system,
so this is the one from AI2 UDub.
And the BiDAF system is very well known.
Um, it's another sort of classic version of
question-answering system that lots of people have used and built off.
Um, and, you know,
some of it isn't completely different to what we saw before but it has various additions.
So, there are word embeddings just like we had before,
there's a biLSTM running just like what we had before,
and that's being done for both the um,
passage and the question.
Um, but there are some different things that are happening as well.
So one of them is rather than just having word embeddings,
it also processes the questions and passages at the character level.
And that's something that we're going to talk about coming up ahead in the class.
There's been a lot of work at doing character level processing in recent neural NLP,
but I don't want to talk about that now.
Um, the main technical innovation of the BiDAF model
is this attention flow layout because that's in its name bidirectional attention flow.
And so, there was a model of attention flow where you have attention
flowing in both directions between the query and the passage.
And that was their main innovation and it was quite useful in their model.
Um, but beyond that,
there's you know, sort of more stuff to this model.
So after the attention flow layer there's again
multiple layers of bidirectional LSTMs running.
And then on top of that their output layer is more
complex than the sort of simple attention version that I showed previously.
So let's just look at that in a bit more detail.
Um so, for the attention flow layer.
So, the motivation here was in the Stanford Attentive Reader,
we used attention to map from
the representation of the question onto the words of the passage.
But, you know so as questions are whole mapping onto the words of the passage.
Where their idea was well,
presumably you could do better by mapping in both directions at the word level.
So you should be sort of finding passage words that you can map onto question words,
and question words that you can map onto passage words.
And if you do that in both directions with attention flowing,
and then run another round of sequence models on top of that,
that you'll just be able to do much better matching between the two of them.
And so the way they do that is, um,
that they- they've got the bottom- so at
the bottom layers they've sort of run these two LSTMs.
So they have representations in the LSTM for each word and um,
word and passage position.
And at this point I have to put it in a slight apology because I just
stole the equations and so the letters that are used change.
Sorry. But, so these are the um,
question individual words and these are the passage individual words.
And so, what they're then wanting to do is to say for each passage word,
and each question word, I want to work out a similarity score.
And the way they work out that similarity score is they build a big concatenated vector.
So there's the LSTM representation of the passage word, the question word,
and then they throw in a third thing where they do a Hadamard product,
so an element-wise product of the question word and the context word.
Um, you know, for a neural net purist, throwing in
these kind of Hadamard products is a little bit of a cheat because
you kind of would hope that a neural net might just learn that
this relation between the passage and the question was useful to look at.
But you can find a lot of models that put in
these kind of Hadamard product because it's sort of
a very easy way of sort of having a model that knows that matching is a good idea.
Because essentially this is sort of looking for each question and passage word pair.
You know, do the vectors look similar in various dimensions?
You can sort of access very well from looking at that Hadamard product.
So that- so you take that big vector,
and you then dot-product that with a learned weight matrix,
and that gives you a similarity score
between each position in the question and the context.
And so then what you're gonna do is use that to
define attentions that go in both directions. Um-
So for the, um, context,
the question attention, this one's completely straightforward.
So, you put these similarity scores through a soft-max.
So for each of the i positions in the passage or sort of,
having a softmax which is giving you a probability distribution,
over question words and then you're coming up with
a new representation of the i-th position which is then the attention weighted,
um, version, the attention weighted average of those question words.
Um, so you're sort of,
having attention weighted view of the question mapped onto each position in the passage.
Um, you then want to do something in the reverse direction.
Um, but the one in the reverse direction is done subtly differently.
So you're again starting off, um,
with the- the same similarity scores but this time they're sort of wanting to, sort of,
really assign which position,
in which position in the question is the one that's, sort of,
aligning the most so that they're finding a max and so that they're finding
which is the most aligned one and so then for each of,
for each of the i's,
they're finding the most aligned question word.
And so then they're doing a softmax over these m scores and then those are being
used to form a new representation of the passage by,
sort of, summing over these attention weights.
Okay. So you build these things up and this then
gives you a new representation where you have,
um, your original representations of the passage words.
You'd have a new representation that you've built from
this bidirectional attention flow and you
look at these sort of Hadamard products of them and
that then gives you kind of the output of the BiDAF layer and that output of
the BiDAF layer is then what's sort of being fed as
the input into these nick- next sequence of LSTM layers.
Okay. Um, and so yeah,
um, so then that's the modeling layer.
You have another two BiLSTM layers and so the way they do the,
um, suspense selection is a bit more complex as well.
Um, so that they're then, um,
sort of taking the output of the modeling layer and putting it through a sort of
a dense feed-forward neural network layer and then softmaxing over that,
um, and that's then getting a distribution of
a start and you're running yet another LSTM kind of a distribution finish.
Um, yeah. So, that gives you some idea of a more complex model.
Um, you know, in some sense,
um, the summary if you go further forward than here is that, sort of,
most of the work in the last couple of years,
people have been producing progressively more complex architectures with
lots of variants of attention and effectively that has been giving good gains.
Um, I think I'll skip since time is running,
out, showing you that one.
But, um, let me just mention this FusionNet model
which was done by people at Microsoft because this relates to the answer,
the attention question, right?
So p- so people have definitely used different versions of attention, right?
So that in some of the stuff that we've shown we tend to emphasize
this bi-linear attention where you've got two vectors mediated by a matrix.
And I guess traditionally at Stanford NLP,
we've liked this, um,
version of attention since it seems to very directly learn
a similarity but other people have used a little neural net.
So this is, sort of, a shallow neural net to
work out attention scores and there's, sort of,
no reason why you couldn't say, maybe it would be even better if I
make that a deep neural net and add another layer.
Um, and some of, you know,
to be perfectly honest, um,
some of the results that have been done by people including Google
argue that actually that NLP version of attention is better.
Um, so there's something to explore in that direction.
But actually, um, the people in FusionNet didn't head that direction because they said,
"Look, we want to use tons and tons of attention.
So we want an attention computation that's pretty
efficient and so it's bad news if you have to
be evaluating a little dense neural net at
every position every time that you do attention."
So this bi-linear form is fairly appealing
but they then did some playing with it so rather than having a W matrix
you can reduce the rank and complexity of
your W matrix by dividing it into the product of two lower rank matrices.
So you can have a U and a V matrix.
And if you make these rectangular matrices that are kind of skinny,
you can then have a sort of a lower rank factorization and,
that seems a good idea.
And then they thought well,
maybe really you want your attention distribution to be symmetric.
So we can actually put in the middle here,
we can have the U and the V, so to speak,
be the same and just have a diagonal matrix in
the middle and that might be a useful way to think of it.
And that all makes sense from linear algebra terms but then they thought,
"Oh, non-linearity is really good in deep learning.
So why don't we, sort of,
stick the left and right half through a ReLU and maybe that will help.
[LAUGHTER] Which doesn't so much make sense in your linear algebra terms, um,
but that's actually what they ended up using as their, um, attention forms.
There are lots of things you can play with when doing your final project.
Um, yeah.
And, but, you know, their argument is still, you know,
that doing attention this way is actually much much
cheaper and so they can use a lot of attention.
And so they build this very complex tons of attention model, um,
which I'm not going to try and explain, um,
all of now, um,
but I will show you this picture.
Um, so a point that they make is that a lot of
the different models that people have explored in different years you,
that, you know, they're sort of,
doing different kinds of attention.
That you could be doing attention right,
lining up with the original LSTM,
you could run both sides through some stuff and do attention,
you can do self attention inside your layer that there are a lot of
different attentions that different models have explored.
And essentially what they are wanting to say is,
let's do all of those and let's make it deep and do it all
five times and the numbers will go up. And to some extent the answer is,
yeah they do and the model ends up scoring very well.
Okay, um, so the one last thing I just wanted to mention but not explain is,
I mean in the last year there's then been
a further revolution in how well people can do these tasks.
And so people have developed algorithms which produce contextual word representation.
So that means that rather than a traditional word vector,
you have a representation for each word in a particular context.
So here's the word frog in this particular context and the way people build
those representations is using something
like a language modeling tasks like Abby talked about,
of saying putting probabilities of words in
context to learn a context-specific word representation.
And ELMo was the first well-known such model.
And then people from Google came up with BERT,
which worked even better.
Um, and so BERT is really in some sense is
super complex attention Architecture doing a language modeling like objective.
We're going to talk about these later, um,
I'm not going to talk about them now, um,
but if you look at the current SQuAD 2.0 Leaderboard,
um, you will quickly,
um - sorry that's- oh I put the wrong slide and that was the bottom of the leaderboard.
Oops, slipped at the last minute.
If you go back to my slide which had the top of the leaderboard, um,
you will have noticed that the top of the leaderboard,
every single one of the top systems uses BERT.
So that's something that you may want to
consider but you may want to consider how you could
use it as a sub-module which you could add other stuff too as many of these systems do.
Okay. Done for today.
 The plan for today is what I am gonna talk about
is the topic of convolutional neural networks.
So essentially, um, there's actually quite a lot of
content in this lecture of different things that's good to know about,
since essentially this is going to be learn about
convolutional neural networks in one large bite for NLP.
So, um, bit on announcements,
explain the general idea of convolutional neural networks,
and then for quite a bit of it,
I want to go through in sort of some detail to particular papers that made
use of convolutional neural networks for
text classification, sentence classification tasks.
Um, the first is a sort of a pretty simple,
um, CNN that was done in 2014,
and then the second one is a
way more complex CNN that was done much more recently in 2017.
Okay. But first, a couple of announcements.
Um, firstly, the last reminder on the mid-quarter feedback survey.
So tons of you have done the- this already.
Thank you, thank you very much.
Um, but if you'd still be putting it off till the very last minute, um,
tonight at midnight is your last chance, um,
to fill in the mid-quarter survey to get your,
um, to give us feedback and to get your half-a-point.
Um, okay. And then the other thing that you should be thinking about,
and I know lots of you are thinking about
since I spent three hours talking to people yesterday,
um, is about final projects.
Um, and so make sure you've got some plans from that, um,
in place for, um,
04:00 p.m, uh, 04:30 p.m. Thursday.
I mean, in particular as we've discussed, um,
your- part of what you're meant to do this year is to have found some research paper,
have read it, and have a summary and thoughts as to how it can inform your work.
Um, and then just make sure you have in your calendars, um,
the final project poster session for CS224n,
which is gonna be in the evening of Wednesday March 20th,
and we're holding it at the Alumni Center.
Okay. Um, one more sort of announcement or just general stuff to cogitate.
Um, so we're now officially in the second half of the class.
Congratulations.
Um, and, you know,
there's sort of still a few things that we want to teach you that are sort of basic,
and actually convolutional neural networks is one of them.
But, I mean, nevertheless in the second half of the class, I mean,
things start to change and we're hoping to much more, um,
prepare you for being real deep learning NLP researchers or practitioners.
And so what does that mean concretely?
Well, the lectures start to be less
giving every detail of how to build a very basic thing,
and more giving you some ideas
to sort of some of the work that's been done in different areas.
And so to the extent that there's something of interest or
rele- relevant to a project or things like that.
Um, the hope is that while you can take some initiative to
find out more about some of the things that are being talked about.
Um, also would really welcome any questions about things that people,
um, would want to know more about.
And the other thing that you should know about
deep learning is that once we get past the fundamentals,
a lot of the stuff we teach just isn't
really known science or things that people are sure of that,
you know, most of what I'm teaching in the second half of the course is pretty
much what people think is good practice in 2019.
But, you know, the fact of the matter is what people think is
good practice in deep learning has been changing really rapidly.
So if you go back even two years or definitely if you go back four years, right?
There's just a lot of different things that people used to believe,
and now people have some different ideas as to what works best.
And it's perfectly clear that come 2021 or 2023,
there will be some different ideas again as to what,
um, people think is best.
So you sort of just have to accept that this is, um,
a nascent rapidly emerging field
and it's good to understand the fundamentals and how things fit together.
But after that, quite a bit of the knowledge is this is what people
think is good at the moment and it keeps evolving over time.
And if you want to stay in the field, or doing things with deep learning,
you kind of still have to keep up with how it changes.
It's called lifelong learning these days.
It's a very trendy concept.
Um, and so as well as the lectures,
this is also true for the assignments.
Um, and, you know,
we've been trying to make the assignments so that they started off very introductory,
and gradually started to use less scaffolding,
and we're going to hope to, um,
continue that, um, with the sort of less hand holding in assignment five.
And, you know, I guess what we're hoping to do is prepare you
both for the final project and for real life.
I guess I was making an analogy this morning,
um, comparing this to the sort of intro CS sequence,
so when there's CS106A and B that have tons of scaffolding,
and then in CS107,
you're meant to learn how to diagnose and solve problems
for yourself in a debugger that is kind of the same,
um, for neural networks that, you know,
for the early assignments, um, you know,
we've given you every bit of handholding here, all of
these tests to make sure every little bit of it is okay,
and here's exactly how to structure things.
But, you know, in the real world,
um, you're only going to be able to build and use neural networks.
If you can figure out why they're not working
and what you have to change to make them work.
And, you know, the truth is as I talked a bit about last week, you know,
that's often well more than half of the job that it seems easy enough to stick down.
Here's my neural net and the pieces that make sense to me,
and then you can spend the remaining 80 percent of the time
scratching your head wondering why it doesn't actually work well,
and how you could change it to make it to work well.
Um, so, um, I confess that debugging neural nets can often be hard, but, you know,
the goal is that you should actually learn something about doing it,
and that's kind of one of the learning goals of the course when it comes down to it.
Um, final little advertisement.
If you feel like you'd like to read a book,
um, just out this week,
there's a new book on natural language processing with PyTorch
by Delip Rao and Brian McMahan.
Delip actually lives in San Francisco.
Um, so, um, if you want to,
you can buy a copy of this, of course.
But if you don't want to, um,
buy it and you feel like having a bit of a look through it, um,
the Stanford library is actually has a license to the O'Reilly's Safari Books collection.
So you can start off at library.stanford.edu and read it for free.
There's one catch to this which is the library only has
16 simultaneous licenses to Safari Books.
So if you'd also like your classmates to be able to read it for free,
it really helps if you remember to log out of Safari Books Online,
um, when you're done looking at it.
Um, yes, so this is sort of a,
I mean, in some sense,
I hope you will feel if you look at this book,
"Boy, I already know most of that stuff already.
It's not a super advanced book.
But it's a good well-written tutorial of how to do things with PyTorch and NLP."
If you don't feel like you know most of the stuff in this book,
you can let me know but I will be a little sad.
Um, okay, um, yeah.
So, let, so starting into today.
Um, so, we spent a lot of time on
recurrent neural networks and they are great for many things.
Um, but there's sort of some things that they're not so good at.
So, you know, we kind of might like to know about a phrase like my birth,
or a bigger phrase like of my birth,
and there's sort of no independent, um,
representation of those spans in a recurrent neural network.
We kind of get sort of prefixes of a whole sentence.
And while we did, um, bidirectional, um,
recurrent neural networks, and you could say, 'Well,
wait a minute you could use it in both directions' and to some extent that's true.
We can get stuff from this direction and stuff from this direction,
but we still kind of have sort of
whole sequences that go to one end of the sentence or another.
We don't just have pieces of sentences.
And often, we'd like to sort of work out meanings of pieces of sentences,
and so, we sort of have two problems here.
We only have sort of initial and final sub-sequences.
And also, if you look at these representations, like if you say,
take this last state as the representation of the meaning of this text.
What you find out,
is it's very dominated by the meaning of
the most recent words and what they are trying to predict as to what comes after them,
and that's part of the reason why I mentioned
last time in the question answering, um, lecture,
the idea that well you can do better by having a sentinel and training
something that has attention over the whole, um, LSTM structure.
Okay. But today we're going to look at
a different alternative which is convolutional neural nets,
which are often abbreviated as either CNN's or ConvNets.
Um, and the idea of these is, well,
look maybe we could just take
every sub-sequence of a certain length and calculate a representation for it, um,
so that, you know, if we have some piece of text like,
tentative deal reached to keep government open,
and we could sort of just say, well,
let's just take all three words sequences,
tentative deal reached, deal reached to,
reached to keep et cetera,
and we're going to calculate some kind of representation for each of those sequences.
So, this is an- isn't a strongly linguistic idea.
Right? We're not worrying about whether it's a coherent phrase,
that's grammatical linguistically valid,
cognitively plausible, we're just taking every sub-sequence of a certain length.
And then, once we've calculated representations of those,
we're going to look at how to group them.
Okay. So, let's get into more detail as to what CNN's are and how they work.
Um, yeah, so, there's this general idea of a convolution which you may or may
not have seen in some math or electrical engineering class.
And then, there's the particular version of convolutions,
the discrete convolutions, which you can means that
you can use the friendly summation symbol rather than an integral.
Um, and that's a,
that's a discrete convolution.
I find that that notation as completely unhelpful.
So, I won't even try and explain it.
But I've got lots of examples,
and convolutions are really easy for neural nets in terms of what they do for examples.
All right, so the classic case of where convolutional neural networks are used,
is in vision applications.
So, if you do CS231N next quarter,
essentially you know, the first four weeks is just all doing
convolutional neural networks in all their variants and glory.
Um, and the sort of essential idea of, um,
convolutions for a vision,
is that you want to recognize things no matter where they appear in an image.
So, you have a sort of property of translation and variance,
and the idea of a convolution as a way
of finding something in different places in the image,
regardless of where it appears.
Um, so this is the vision example which I stole from Andrew Ng's UFLDL website.
And so, what a convolution is,
is it's here a patch,
but you can think of it as just as a vector,
and the patch has weights which are these little numbers in red,
and what you're gonna do,
is slide that patch over the image as this as this animation does.
Um, and so at each position,
you're going to multiply each of the red numbers by the black number in that position,
and then you're just going to sum them up.
So, that's what a discrete convolution does,
which is what that notation at the top is saying,
right? You're multiplying things together and then you're summing them up,
and so you're doing this,
and then you're filling in the pink with the products,
um, the sum products.
So, it's sort of like, you're taking these sort of
patch dot products and putting them into the pink matrix,
and that's then your convolved feature.
So, that's a 2D convolution,
which for the rest of today,
we're not going to look at anymore.
So, this is all you're learning about vision.
Um, and so we're now going to go back and look at 1D convolutions,
which is what people use when they're using convolutional neural networks for text.
So, the starting point of a convolutional neural network for text,
is we have an input.
So, here's my sentence and for each word
in the sentence I have here got a dense word vector,
I made it a 4D, want to keep it small in my example but usually as you know, it's more.
So, our starting point is we have some input, you know,
input could just be a one-hot encoding that's not forbidden here,
but normally we'll have these kind of dense word vectors.
And so, then it's sort of the same as the 3D as the 2D one,
apart from we've only got one dimension.
So, we have a filter.
Um, so here is our filter,
and so our filter is gonna do three steps and time, three words.
And that's going to work across the dimensions.
So, these different dimensions in
the convolutional neural network often get referred to as channels.
So, we're kind of working across the input channels,
and so we have a patch like this.
And we're going to take this patch and put it on top of the first three words.
I don't have as good an animation as the previous slide.
Sorry. And we're going to work out the dot product,
um, between those, and I did that at home by putting this into Excel.
And the answer [LAUGHTER] to that,
is that the product is minus 1.0.
And then at that point, we slide our,
We slide this, um,
matrix which gets referred to as a kernel or
a filter which is the patch that we're using for our convolutional neural network.
We slide it down one and do the dot product of those terms again.
And that comes out as minus a half and we keep on sliding that down and we get what,
um, gets what's shown on the right as our output.
So at this point,
we've just reduced the sentence,
um, to a single vector.
Um, and that seems like we might want to do more than that.
Um, but the other thing that you will have noticed is that
our sentence is sort of shrunk because before, you know,
we had a seven word sentence but because I've just sort of slid this three word,
um, kernel down here,
I ended up with only five positions to put it in.
So it's become a five word thing.
Um, so to first of all address that problem,
commonly when people do convolutional neural networks, they add padding.
Um, so what I can do is I can add zero padding at
both ends and then sort of do the same trick and say run a convolution on that.
And now, I'll be able to put my size three filter in seven different places as I
slide it down and so I'm getting out a vector that's the same length of my input.
Um, that, you know, there are different way,
so this is the most common way of doing things.
And it's kind of seems logical because it maintains size.
I mean, you know, there's always more than one way to do it.
Um, if you really wanted to,
you, oops, I don't want you, yeah,
there, oops, I made, uh,
I made a slight mistake on my slide because this
turns out which I was about to get to in a minute
but I'll just explain this bit here anyway [LAUGHTER].
Um, you know, if you wanted to,
you could have two steps of padding on both ends here.
So that your first convolution we'll be looking at zero, zero,
10 to the of and then the convolution would actually grow the size of your input.
Yeah. But, yes. So I mean,
so what we've done so far,
we've started with these word vectors which in
convolutional neural networks terms were of length four.
So our kind of input had four channels.
But when we were back here, um,
we were just producing from this, um,
kernel, one column of output.
So our output has only a single channel.
So we've sort of shrunk things in the columns direction from four to one.
And that might seem bad.
And for many purposes, it is bad.
Um, and so, a lot of the time,
what you want to do is to say,
well, rather than have only one filter,
instead of that, why don't I have several filters?
So here I've got three different filters and each of
these filters is just sort of the same size three,
three the size, the kernel size times the input,
number of channels for the matrix.
So I have three different filters and I'm going to run
each one down the text and get a column here.
So now, I'm ending up with three columns of output.
And so I have this sort of a three channel output.
And the way to intuitively think of this is for these filters,
well, you know, for what we do in neural networks,
we're going to learn them by backpropagation like everything else.
But our hope is that these filters could somehow specialize in different things.
So maybe this filter could specialize on,
is this language polite?
And it will produce a high value whenever it sees polite words.
And maybe, um, this, um,
filter could specialize on, I don't know,
eating and it will have a high value whenever it sees words
about food and you know this filter will do a third thing.
And so that's the sense in which people sometimes talk about, um, the, um,
what you're getting is output of different features because your hope is that
you'll kind of gain different latent features coming out of the text.
Okay. So that gives us a representation and that's sort of
a useful sort of having found learn features in our text.
That quite often though, what we'll want to do is just
summarize the text with re- with respect to those features.
So you might just have the question of, well,
in this piece of text, um,
is it polite and does it talk about food?
So another operation that we'll quite often
do is wanna summarize the output of a convolutional network.
And the simplest way to do that,
is for 1D convolutions,
is called max pooling over time.
So if we max pool over time,
that each of the channels or otherwise known as features,
we're just simply going to look down and see what is its maximum value, 0.3, 1.6, 1.4.
Um, and so, you know,
if I use my story about the first two, um,
filters, it's sort of saying, well,
it's not very polite text but it's really about food, right?
That we're sort of summarizing,
um, what we've detected there.
Um, so the concept of max pooling in some sense captures,
does, is this thing being activated anywhere, right?
So if we have things like politeness and about food,
that the output of max pooling will have a high value.
If somewhere in the sentence there was a clear marker of
politeness or something clearly about food.
And that's often a useful notion because often what you want to know is,
you know, is there some discussion of food in this sentence or is there not?
There's another thing, there are other things that you could do.
Instead of, ah, max pooling,
you can instead do average pooling.
So here you just take these numbers and find the average of them.
That then has the different semantics which is sort of
what's the average amount of politeness of this, um,
text or on average how much, you know, how,
what percent of the sentence is about food or something like that.
Um, for some purposes,
this is better because, you know,
it takes in all of the important builds to an average.
I mean, a lot of the time,
people have found that actually max pooling is better because,
you know, a lot of signals in natural language are sparse.
You know, no matter how polite you are trying to be,
you're not going to be being polite in every word.
You're going to say nouns and articles like that and a,
and prepositions and conjunctions,
none of which are inherently polite, right?
Um, so that if there's some politeness showing up prominently,
then the sentence becomes polite and max pooling is actually better for capturing that.
Um, of course the one other kind of thing that you can do as
min pooling and find the least [LAUGHTER] active thing.
Um, it doesn't get used much but you could do that as well.
Okay. So, um, so if you're in PyTorch,
this is all pretty easy stuff to do.
So there's a handy dandy Conv1d.
There's also a Conv2d as you might guess for vision.
But there's a Conv1d, um,
where you're specifying how many input channels there are.
That was our word embedding size.
How many output channels there are?
We have three.
What the size of the convolutional kernel is?
So the ones that we were showing were also
three and then there are various other parameters you can have.
Like you can say that you want a padding of one and things like that.
And then once you've got one of those,
you can just sort of run
your convolutional filter on the input to get a new hidden state.
And then if you wanna max pool,
you can just max,
um, through the output of that and then you've got a max pooled output.
Okay. So that gives us the basics of building a kind of a convolutional neural network,
um, for, um, NLP.
Does that sort of makes sense up until there?
Yeah. Okay. So next bit is to sort of show
you three or four other things that you can do.
Um, I started off typing these slides
other less useful notions because I
kinda thought, oh, at least they don't really come up much in NLP.
But, you know, actually it turned out when I got on to that second paper,
when I say the complex convolutional neural network, actually,
in that paper they try out just about all of these things that I say no one uses.
So it's sort of good to know what they are for looking at various papers.
So here, when we did things so far then we were calculating these convolutions,
that we're sort of trying them out at every position.
So we had one for zero, tentative deal.
Then for tentative deal reached then deal reached to.
And so we were just walking down one step at
a time which is referred to as a stride as, of one.
And that's by far the most common thing to do.
But you could observe,
look wait a minute,
since the first convolution concerns zero tentative deal.
I've got all those three words in there.
Even if I skip down to a next did, deal reach to and then I did to keep government,
I'd still have in one or other of the convolutions every word of the sentence
so I can do half as much computation and I've
still got everything in there in some sense.
And so that's referred to as using a stride of two.
And so then I get something with half as many rows out.
So it's one way to sort of compactify your representation and produce
something shorter from a longer sentence and we'll see that use of it coming up later.
There's other ways to compactify what cut representation that comes out of your sentence.
And so there's a different notion of pooling which is local pooling.
Now, if if you've seen any of
the vision world when people talk about max pooling and vision,
they normally mean local pooling as opposed to
the max pooling through time that I showed you first.
So here we're sort of back to where we started and we've done
our size three stride one convolution which is producing output as before.
But now, what I'm gonna do is local pool with a stride of two.
Which means I'm gonna take each two rows and I'm gonna pool them together into
one row and I could do that again by
either maxing or averaging or whatever appeals to me.
So I take the first two rows,
I max pool them I get this.
I take the next two rows,
I max pool them I get this.
Next two, next two and I sort of pad it
on the bottom so I have two rows at the bottom.
And so that's then give me a local max pooling of a stride of two.
And that sort of had exactly the same effect in the sense but
with a different result as using a stride of two in
my convolution because I have again reduced it to
something of four rows that used to be eight rows.
Yeah, picture that.
Okay so that's that one.
What else can you do.
There are more things you can do to make it complex.
Another thing that people have sometimes done is k-max pooling.
And so this is a more complex thing and it's sort of saying well,
rather than just keeping the max over time,
if a feature is being kind of activated two or three times in the sentence,
maybe it'd be good to record all the times that it's
activated in the sentence while throwing away the rest.
So in k-max pooling,
and I'm doing two max here,
you look down this column and you find the two highest values for that column.
But then you put the two highest values not in the order of highest to lowest,
but in the order in which they are in these columns.
So it's minus 0.2,
0.3 for this one and it's 1.6,
0.6 for this one because it reflects the orders of the columns up above.
Okay. Almost done, one more concept.
This is another way of compressing data which is a dilated convolution.
So if you have a dilated convolution,
so dilated convolution doing it over here doesn't really make sense but where you can use
a dilated convolution is if I take this and put it through another convolutional layer,
we can kind of have deep convolutional networks that have multiple convolutional layers.
So the idea of a dilated convolution issue is you're gonna skip some of the rows.
So if you use a dilation of two starting at the top,
you're going to take the first, third,
and the fifth row and multiply them by my fil- sorry,
I have different filters.
Multiply them by my filters and then get the values that appear here.
And then if stride as one,
you'd then use, you would go on and sort of do the next spread out rows.
And so this allows you to have convolutions that see
a bigger spread of the sentence without having many parameters.
So you don't have to do things this way.
You could have said, look,
I could just instead have convolutions with a kernel size of five.
And then they'd say five,
see five words in a row but then I'd be having
sort of bigger matrices to specify my feature.
Whereas, this way I can keep the matrices small but still
see a bigger range of the sentence in one operation.
Yeah and that concept of how much of a sentence you
see is kind of an important notion in convolutional neural networks.
Because, you know, if you start at the beginning of a sentence
and you're just running three-by-three convolutions, um,
you're sort of seeing these three word patches of the sentence.
And it turns out in natural language that's
already actually quite a useful representation.
Because sort of having those kind of n-grams as features is
just good for many purposes including text classification.
But if you want to sort of understand more of the semantics of a sentence,
somehow you wanna see more of that at once.
And you've sort of got several tools you can use to see more of it once,
you can use bigger filters,
you could use, uh,
kernel size five, seven,
nine or something convolution.
You could do something like dilated convolution so you can see spread out pictures.
And the third thing that you can do is you
can have depth of a convolutional neural network.
Because as you have greater depth of a convolutional neural network, you see more.
So at this first layer,
the rows now have sort of info about three words in them.
And if you sort of just stuck a second layer of
convolutional neural network with
the same general nature on top of it and you sort of take
the first three rows and convolve it again then and
then the next ones that those then know about five words of your original input sentence.
So as you kind of have a deeper ConvNet stack you
start to know about bigger and bigger patches of the sentence.
Okay. All good?
Any questions?
No, that's good, okay. So, um, the next piece is essentially shows you this stuff again,
um, in the context of a particular paper.
So this was, um,
a paper by Yoon Kim who was a Harvard student,
maybe still is a Harvard student, um, in 2014.
So this was sort of a fairly early paper.
Um, and he wanted to show that you could use convolutional neural networks to do
a good job for doing
text classification when what you want to classify is a single sentence.
So, the kind of thing you might want to do is look at the kind of
snippets of movie reviews that you see on the Rotten Tomatoes site and say,
"Is this a positive or is this a negative sentence description?"
And the model he built is actually kind of similar
to the convolutional neural networks that Collobert and Weston,
um, introduced in their 2011 paper that we
mentioned before when we were talking about window-based classifiers.
So, in their paper they actually use
both window-based classifiers and the convolutional classifier.
Okay. Um, so yeah,
I sort of already said this.
So their tasks are sentence classification, could be sentiment.
It could be other things like,
is this sentence subjective or objective?
So objective is what the main news articles are meant
to be and subjective is what the opinion pieces are meant to be.
Um, and then other things like question classification.
Is this a question asking about a person,
location, number, or whatever?
Okay, so here is what he did.
And it's sort of the- these slides sort of, um,
use the notation of his paper which is sort of a little bit different the
way the math gets written down to what I just showed
you, that it's really doing exactly the same thing.
So we start with word vectors of length k. Um,
the sentence is made by just concatenating all of those word vectors together and then,
when we- so we have a range of words,
it's a subpart of that sentence vector.
And so, the convolutional filter is just being represented as a vector because
here he's flattened everything out into one long vector for the entire sentence,
whereas I'd sort of stepped into a matrix.
Um, so a size three convolution is just a real vector of length hk,
the size of the convolutional filter times the dimensionality of the words.
Um, and so, what he's gonna do to build
his text classifier is use convolutions made out of different sizes.
So you can have size two convolutions,
size three convolutions as shown here, and bigger convolutions.
And so, um, so to compute a feature one channel for our CNN, we're
then doing a dot product between the weight vector of
the feature times this sub-sequence of the same terms,
and he sort of also put in a bias which I sort of omitted.
Um, and then putting it through a non-linearity,
um, which I wasn't doing either.
Um, but as sort of we've seen a ton of.
Um, and so, what we're wanting to do is that's our,
um, feature and we want to, um,
do it through all this- for a feature of kernel size three,
we're gonna go all the way through the sentence.
The other thing he did though was slightly funnel funny is,
his windows were sort of lopsided in the notation, right.
There's a word and th- the,
um, h minus 1 words to the right of it.
So he has padding here just on the right end whereas
most people do their convolutions symmetrically in both directions around things.
Okay. And so, we're going to do that for a bunch of features or
channels Ci and therefore compute
our convolved representations just as we've talked about.
Okay. Um, then he does just what we talked about.
Um, there's max over time pooling in the pooling layer to capture
the most relevant things and is giving us a single number for each channel.
Um, and we have features that look at different that have different kernel sizes.
Um, here's one other idea he used which is possibly a neat idea.
Um, he knows one of the things that you could even think about in various ways,
um, for say a question answering system among other things.
Um, and so he used pre-trained word vectors.
Um, but what he did was he actually kind of doubled the word vectors.
So, for each word he had two copies of the word vector,
and so you have sort of two channel sets and one set he
froze and the other one he fine tuned as he trained.
So it's sort of he tried to get the best of both worlds of sort of fine tuning
and not fine tuning and all that went into the max pooling operation.
Okay. Um, so, after the max pooling we get out one number for each channel and so,
um, he has something of three size convolutions, three,
four, five, 100 features for each size.
So we're getting out a vector of size,
um, 300 at that point,
and at that point you're taking that final vector and just sticking it
through a softmax and that's then giving your classification of the classes.
Um, so all of that can be summarized in this picture if it's big enough to sort of read.
So, here's our sentence.
I like this movie very much,
which has you know, our word embedding dimension is five,
and so then doing it in this example,
we are having two channels for each kernel size and
we consider kernels of size two, three, and four.
Um, and so and then we are getting two different ones.
Um, so we're getting, um, six.
This is showing six of our filters.
Um, so we apply those.
When we- when we apply those filters without any padding,
we are then getting out these outputs of the filters which are of sizes four,
five, and six respectively.
Um, and so then once we've got these
for each of these sets of numbers we're doing one max pooling.
So, we're just taking the max of each of these,
um, output features which gives us these six numbers.
Um, we can concatenate them all together into one vector which we feed into,
um, a softmax over two classes as to whether sentiment is positive or negative.
Um, so that's basically the model.
So something- so this is sort of really actually a very simple,
very computationally efficient, uh,
model as to how to build a text classifier.
[NOISE] Um, yeah, just a couple more things to get through,
um, so in one of the assignments,
we talked about Dropout [NOISE] and you used it.
So, um, you know,
hopefully you're all masters of Dropout at this point.
Um, so he was using Dropout, um,
and this being 2014 and the,
um, Dropout paper only coming out in 2014.
I guess, there'd been an earlier version that came out a couple of years earlier.
This was sort of still fairly early,
um, to be taking advantage of Dropout.
So that while training,
you've got this sort of Dropout vector, um,
where you sample your Bernoulli random variables and you're, sort of,
um, sort of, designed to drop out some of the features each time you are doing things.
At testing time, you don't do the dropout,
but because before you were sort of dropping out a lot of stuff,
you're scaling your weight matrix by the same probability that you use for dropping out,
so that you get, sort of,
vectors of the same scale as before.
Um, so as we sort of discussed in the assignment,
Dropout is a really effective form of regularization,
widely used in neural networks.
Um, he didn't only do that, he actually did,
a kind of another sort of funky form of regularization.
So that's for the softmax weight vector,
he constrained the L2 norms,
so the squared norms of the weight vectors and the softmax, [NOISE] um,
matrix, um, to a fixed number S,
which was sort of set of the hyper-parameters,
actually set to the value three.
Um, and if your weights were getting too large,
they were being rescaled,
um, so they didn't blow up.
Um, this isn't a very common thing to do.
I'm not sure it's very necessary, um, but, um,
I guess it gives you some- I mean,
I guess by showing you a few of the details of this one,
my hope is, sort of,
gives you some ideas about how there are lots of things you can play
around with and muck with if you wanna try different things,
um, for your final projects.
Um, okay.
So here are some of his final hyperparameters.
So he's using ReLU nonlinearities,
um, window sizes of three, four, and five,
the convolutions, hundred features or channels for each size,
um, Dropout of a half as usual.
Um, you get several percentage improvements from dropout,
which is quite common actually.
Um, the sort of L2 constraint, s equals three,
mini batch of 50,
300 dimensional word vectors,
train to maximize dev set performance.
Okay. And here is the big table,
you know, I was too lazy, um,
to redo of performance on these different text classification data sets.
Um, there are lots of different ones.
So these two are both Stanford Sentiment Treebank.
This is the Subjective Objective Language.
This is the Question Classification, of is it asking for a person name and location,
a company or whatever.
Um, this is, um,
talking about, sort of, a perspective,
which is another classification thing.
Consumer Reports is another sentiment one.
Um, so lots of data sets and then here are lots of models.
So the model- some of the models down here or here,
are traditional feature-based, um, classifiers.
Um, so in particular,
um, sort of Wang and me back in 2012,
had sort of pointed out that by taking certain steps
with n-gram features and other forms of normalization,
that you could actually get quite good results with
just the traditional feature, um, based classifiers.
So many people use that as a baseline for showing that you can do better things.
Um, the ones up here,
were tree structured neural networks that my group was very fond
of in the early 2010s and then up at the very top,
uh, his CNN models.
And as you can see,
it's sort of a mix.
Sometimes the CNN model wins,
like in this column and this column,
sometimes it doesn't win like in these columns.
Um, but in general, um,
what you didn't see from this is that, you know,
this is an extremely simple, um,
convolutional neural network model and it actually does,
um, kind of well on this system.
Um, you can quibble with this results table,
and again in terms of like writing your propos- project proposal, um,
one thing that you should do is kind of think about what you're reading, um,
because, you know, a lot of papers aren't perfect
and there are reasons to quibble with what they claim.
And sometimes if you think about what they're claiming and whether it's reasonable, um,
there are reasons why it's not or there are ideas
of how you could do things differently or show something different.
I mean, the main reason why you could quibble with,
um, Yoon Kim's results table is, well,
he already said, as I had a couple of slides back, um,
that the statement that Dropout gives you
two to four percent accuracy improvement in this neural nets.
[NOISE] Um, but most of these systems because they
are older and were done before Dropout was invented,
um, didn't make use of Dropout.
But, you know, any of these sort of neural net systems up here
could have used Dropout and presumably it would have given them a couple of,
um, percent gain as well.
So arguably, this is sort of a biased, unfair comparison.
And the right thing would have been to be comparing all the systems, um, using Dropout.
Um, but, you know,
despite that, you know,
this was still a prett- a lot of people noticed
this paper because it showed that using this sort of very simple,
very fast convolutional architecture,
could give you strong results for text classification.
Um, that's that.
Yes. So in summary,
you know, something that you should be thinking about for projects and otherwise,
we're effectively building up a bigger toolkit of different tools you could be using,
um, for projects or future work or whatever it is.
So starting off with,
we had word vectors and then we could build bag of
vector models by just taking the word vectors and averaging them.
And, you know, that's actually a surprisingly good baseline to start with.
We suggest to you in many cases for things like projects,
you should use that.
See how well it does,
make sure you're working better.
I mean particularly, you can do even better with that,
if you sort of add some extra ReLU layers on top,
which is an idea that's been explored in deep averaging networks.
Um, then we looked at window models which were very simple.
You're just taking these sort of
five word windows and computing a feed-forward network on them,
and they work very well for word classification problems that only need local context.
Things like, part of speech tagging or NER.
But then we've gone ahead and looked at some other models.
And so, um, CNN's are very good for text classification, um,
and they're very good because they parallelize really well on GPUs,
which is something I'll come back to again later.
So they, they just sort- the general sort of representing sentence meaning.
They're actually a efficient,
versatile, good method, which has been used quite a bit.
And then they sort of contrast with recurrent neural networks.
Recurrent neural networks have some advantages.
They're sort of more cognitively plausible,
because you're sort of reading through the text and,
um, getting its meaning.
Um, recurrent neural networks are good for
things like sequence tagging and classification,
building language models to predict what's coming next.
Um, they can do really well when combined with attention.
Um, but they also have some disadvantages.
They're way slower than convolutional neural networks and if what you wanna
do is get out some kind of overall meaning representation of a sentence,
you know, "What does this mean?
Are these two, um,
phrases paraphrases with each other?"
There are now many results that show that people
don't get better results with recurrent neural networks.
They can get better results using techniques like convolutional neural networks.
Okay. [NOISE] So in the next step then [NOISE] is to,
sort of, head towards our com- our complex,
um, convolutional architecture example.
So before getting to that,
I just wanna sort of introduce a few concepts that we haven't seen,
all of which, um, start to turn up when we do this.
So we spent a lot of time in the sequence models part,
talking about gated models or the gated recurrent units and the LSTM units.
But the idea of a gate is general that we can
sort of have this idea that we can calculate something,
put it through, um,
a sigmoid nonlinearity and gets a value between zero and one,
um, or a vector of values between zero and one.
And then do a Hadamard product with a vector
and sort of gate it between its value and zero.
So that suggests the idea that you could also apply
gates vertically when you're building multilayer networks.
And after the successive LSTMs had been proven,
that was, um, an idea that really took off,
was people start exploring,
how can we have, use these ideas of skip connections and gating in a,
in a vertical direction?
And here are two versions of it.
This one is a very simple one,
but a very successful one that's basically just about a skip connection.
So and this is referred to as a residual block and- which is used in residual networks,
otherwise known as ResNets.
Um, so in a residual block, for each block,
you allow a value just to skip ahead to the next, um, layer.
Or you can stick it through a conv block,
and the typical conv block is you go through a convolutional layer,
you then go through a ReLU nonlinearity,
another convolutional layer, and then when you come out,
you just sum these two values.
So this is the same idea that sort of
summing values is magical in the same way as an LSTM.
And then you put the output of that through another ReLU,
and this thing here is called a residual block
and then commonly you'll stack residual blocks on top of each other.
Um, there's one little trick here,
um, which is you need to use padding, right?
Um, because at the end of the day since you want to sum these two pathways,
you want them to be the same size.
And if you, sort of,
have them shrinking in the conv blocks you wouldn't be able to sum them.
So you want to, sort of, have a padding at each stage so they stay the same size here,
and so that you can add them together.
Um, here's, um, a different version of a block which is
sort of more LSTM-ish and indeed
this block was developed by Jürgen Schmidhuber and students,
who's the same guy who's behind LSTMs and you can see the same thinking.
It's called a highway block.
So in a way it's sort of similar.
You've got, you know, kind of thinking of moving an identity x that skips
a nonlinear block or you can have it go through exactly the same stuff conv, relu, conv.
The difference is that unlike this one,
this time there's explicit gates so there's,
um, and this T-gate and the C-gate.
And so you're multiplying both of the path through here and the path through here
by a gate just kinda like the sort of
the get input gates that we saw before and then summing them together.
So that sort of feels more
powerful but it's not actually clear that it is more powerful.
I mean, this one actually has a very simple
semantic because if you think of the semantics of this one
is the default is just you walk
this way and you just sort of carry forward your value and do nothing.
Um, so, what this block's job to- is to do,
is to learn a delta that is meant to learn
what kind of deviation you have from doing nothing.
Um, so that's a nice simple semantic which, um,
seems to work well in neural networks to learn things.
Um, this sort of has
more complicated apparent semantics because you're taking, you know,
some parts of the identity multiplying by this sort of gate in a Hadamard product
and some parts of this conv block multiplied by this other gate T in a Hadamard product.
So that sort of feels more powerful as that
gives me a lot more control because I can take pieces of the different ones and so on.
If you think about it for a bit longer, I mean,
mathematically it's actually not any more powerful that you
can represent anything you can do with this one with that one.
And the way to think about that is well, um,
you know, here you're kind of keeping only part of the identity,
um, but what you could do is keep the whole of the identity and see it as your job
to subtract off the bits that this one isn't keeping
over here in the conv block which you can do theoretically.
Um, and so, you can sort of anything you can compute with this as a function,
you can actually compute with a, um, ResNet block.
Um, and so then as quite often in neural network land,
the question isn't sort of, um,
some kind of proof of compute- can be computed or not.
It sort of comes down to learning and regularization questions as to
whether one or the other of these actually proves
better as something to use in a learning architecture.
Okay. Second concept.
Um, batch normalization.
So when people are building deep convolutional neural networks,
um, in the 2015 pluses,
um, they almost always use batch normalization layers because
this makes your life a lot better and if they're not using batch normalization layers,
they're normally using one of the other variant ideas that people have suggested
such as layer normalization which is sort of meant to do about the same thing.
Um, so what batch normalization does?
I mean, I think many of you will have seen somewhere in steps or
otherwise the idea of doing a Z-transform which means you take your data,
you work out its mean and you work out its
standard deviation and then you rescale by subtraction and
multiplication so that you have a set of data which
has a mean of zero and a standard deviation of one.
Most people see that, right?
Yeah? Um, so batch normalization is effectively doing exactly that but in a weird way.
So what you're doing is that you're taking each mini batch.
So whatever just random 32 examples you've stuck in a mini batch,
you're running them through a layer of
your neural network like a ConvBlock that we saw before
and you take the output of that mini batch and then you do a Z-transform on it.
Um, and then it goes forward into the next ConvBlock or whatever,
and the next time you have a different mini batch,
you just Z-transform it.
So it seems a little bit weird.
You're just doing it on the output of these mini batches.
Um, but that's proven to be a very effective thing to do.
So that it sort of means that what comes out of
a ConvBlock sort of always has the same kind of scale.
So it doesn't sort of fluctuate a lot and mess things up and it tends to
make the models just much more reliably trainable because,
you know, you just have to be much less fussy about a lot of things.
Because, you know, a lot of the things we've talked about,
about initializing your parameters and
setting your learning rates is sort of about, well,
you have to keep the scale of things about right so they don't get
too big or too small and things like that.
Whereas, if you're doing this batch normalization,
you're sort of forcing scale,
um, to being the same size each time.
And s o therefore, you kind of don't have to do
the other stuff as well and it still tends to,
um, work pretty well.
So that's a good technique to know about.
Okay. Um, one last thing to learn about.
Um, there's a concept of,
um, size one convolutions.
Um, and actually, I guess I really sort of, um,
renamed it- I named this wrong because I wrote down
one by one convolutions because that's the term you normally see.
But that's, um, the vision world where you have 2D convolutions.
So I guess I should have just called this one convolutions.
So you can have convolutions, um,
with a kernel size of one and when you first see that,
it seems like that makes no sense whatsoever because the whole idea
of a convolution was I was taking this patch and calculating something from it.
If I'm not looking at any other words,
surely I'm calculating nothing.
But what actually happens in the size one convolution,
is if you have a number of channels that
sort of in a previous layer if you'd calculated whatever it was,
32 channels or something like that.
What the one by one convolution is doing is acting as
a tiny little embedded fully-connected network over those channels.
And so you're sort of doing a
position specific fully-connected network,
um, in- for each row of your data.
And so you can do that,
um, for various reasons.
You can do it because you want to map down from having
a lot of channels to having fewer channels or
you can do it just because you think another non-linearity
will help and this is a really cheap way to do it.
Because the crucial thing to notice is that if you sort
of put fully-connected layers over everything,
they involve a lot of parameters whereas putting in these size
one convolutions involve very few parameters
because you're just doing it at the level of a single word.
Um, okay.
Um, two random things and then I'll go onto my complex model.
Um, this is just a sort of
almost a bias- aside but it just shows
something different that you could do and it's something that you could play with.
I mean, when we talked about machine translation,
we talk about the SIC to SIC architecture that was introduced in
2014 and has been very successful for machine translation.
But actually, the year before that came out,
um, there was a paper, um,
doing neural machine translation by Nal Kalchbrenner and Phil Blunsom in the UK.
And this sort of was actually essentially
the first neural machine translation paper of the modern era.
If you dig back far enough,
there are actually a couple of people that tried to use
neural networks for machine translation
in the '80s and '90s but this was sort of the first one that restarted it,
and they didn't actually use a SIC to SIC architecture.
So what they used was for the encoder,
they used the convolutional neural networks.
And so that they had a stack of convolutional neural networks that progressively shrunk
down the input and then finally pulled it to get a sentence representation,
and then they used a sequence model as the decoder.
Um, so, um, that's sort of something that you could
try in some other applications that for encoders,
it's really easy to use convolutional neural networks.
There has been work on using convolutional neural networks as decoders as well,
though that's a little bit harder to get your brain around and isn't used nearly as much.
Then the second thing I want to mention because we'll turn to it in just a minute is so,
so far we've done Convolutional models over words so that
our kernels are effectively picking up
these word n-gram units of two-word or three word sub-sequences.
And the idea that then developed fairly soon was well maybe
it would also be useful to use convolutions over characters.
So, you could run a convolutional neural network
over the characters of the word to try and,
um, generate a word embedding, um,
and this idea has been explored quite a lot, um,
it's part of what you guys are gonna do for assignment
five is build a character level ConvNet,
um, for your improved machine translation system.
I'm not going to say sort of a huge amount about the foundations of this today, um,
because Thursday's lecture is then talking about subword models
and we'll go through all the details of different subword models.
But, I wanted to show you a con- a complex
convolutional neural network which is also used for text classification.
So, essentially, the same task as Yoon Kim's model
and this model actually is built on characters,
it's not built on words.
So, we are at the foundation of it,
um, having a word-like model.
Um, so, this is a paper from 2017,
um, by, um, the four authors shown here, um,
people working at Facebook AI Research,
um, in France, um, and so,
they kind of had an interesting hypothesis for
this paper which was essentially to say, that, you know,
by 2017 people who are using deep learning for vision were building really,
really deep networks and fi- finding that they work much,
much better for vision tasks.
So, essentially to some extend,
the breakthrough was these guys that once these ideas that emerged,
it then proved that it wasn't just that you could build a six layer or an eight layer,
um, Convolutional Neural Network for vision tasks.
You could start building really,
really deep networks for vision tasks which had tens or even hundreds of
layers and that those models when trained on a lot of data proved to work even better.
So, um, if that's what's in your head and you then looked,
look at what was and indeed is happening in natural language processing,
the observation is, you know,
these NLP people are kind of pathetic,
they claim they're doing deep learning but they're still working with three layer LSTMs.
Surely, we can make some progress, um,
by building really deep networks that kinda look like vision networks and using them,
um, for natural language processing goals.
And so, that is precisely what they said about doing.
So, that they designed and built really deep network which sort of looks like a vision stack,
um, as a convolutional neural network that is built over characters.
Um, so, I've got the picture of it here but sufficiently deep that it's fitting it on
the slide and making it readable [LAUGHTER] is a little bit
of a challenge but we can try and look at this.
So, at the bottom,
we have the text, um,
which is a sequence of characters and so, um,
for the text, um, so,
when people do vision object recognition on
pictures normally all the pictures are made the same size.
Right. You make every picture 300 pixels by 300 pixels or something like that.
So, they do exactly the same for NLP, um,
they have a size, um,
for their document which is 1024 characters.
If it's longer than that they truncate it and keep the first part.
If it's shorter than that they pad it until it's of
size 1024 and then they're gonna stick it into their stack.
So, the first part is that for each character,
they're going to learn a character embedding now and
their character embeddings are of dimensionality 16.
So, that the piece of text is now 16 by 1024, um, so,
they're going to stick that through a convolutional layer where
you've got kernel size of three and 64 output channels.
So you now have something that's 64 times of 1024 in size.
You now stick this through a convolutional block.
I'll explain the details of that convolutional block on the next slide but,
you should be thinking of that ResNet picture I showed earlier where you
can either be going through some convolutions or taking this optional shortcut.
Another ResNet, another residual block
where you can be going through convolutions are an optional shortcut,
um, they're then doing local pooling in the same way people typically do envision.
So, commonly what people do in vision systems
is you are sort of shrinking the size of the images, um,
by doing pooling that halves the dimensions in each direction.
But, at the same time,
you do that in your neural network,
you expand the number of channels,
and so you make it deeper in terms of the number of
channels at the same time as you make it smaller in the x,
y size of the image.
So, they do exactly the same apart from these one-dimensional convolutions.
So, before we had 64 channels in our 1024 character,
um, embedding, um, document.
So, now we pool it, um, so,
we're going to have 512 positions which are sort of like pairs of characters,
um, but we now have 128 channels
and then they kind of repeat that over and over again, right?
So, there are two more convolutional blocks which I'll
explain more but they're sort of residual blocks.
They pool it again and they do exactly the same thing.
So, now there are 256, um,
positions which are like four character blocks and they have 256 channels,
um, I can't point high enough but they repeat that again and they pool again.
So, now they've got, um,
128 positions which are about eight characters
each and they have 512 channels representing that.
They pool again, they have convolutional blocks again, um,
then lo and behold because I said that even the
weird ideas are going to turn up, right up there,
they're doing k max pooling and they're keeping the eight strongest values,
um, in each channel.
Um, and so at that point,
they've got something of size 512 by eight, um, so,
sort of like eight of the eight character sequences
have been deemed important to the classification and they're
kept but they sort per channel and there are 512 of them
you're then putting that through three fully connected layers.
So, typically vision systems at the top
have a couple of fully connected layers at the end,
um, and the very last one of those,
is effectively sort of feeding into your Softmax.
So, it's size 2,048 times the number of
classes which might just be positive negative two class unlike the topical classes.
Um, so, yeah, so it's essentially like
a vision stack but they're going to use it for language.
Um, okay.
So, the bit that I hand quite explained was
these convolutional blocks but it sort of looks like the picture that we had before or,
um, departments slightly more complicated.
So you're doing, um,
a convolutional block of size three
convolutions some number of channels depending on where you are in the sequence.
You're then putting it through a batch norm as we just
talked about putting it through a ReLu non-linearity,
repeating all those three things again or remember there
was this sort of skipped connection that went right around the outside of this block.
And so this is sort of a residual style block, um, so,
that's the kind of complex architecture you can put together and
try in your final projects if you dare in PyTorch.
Um, yeah, um, so,
for experiments so- so one of
the things that they were interested in and wanted to make a point of is well some
of these traditional sentence and
text classification datasets have been used in other papers
like Yoon Kim's paper are effectively quite small.
So, something like that Rotten Tomatoes dataset is actually only 10,000 examples, 5,000,
positive 5,000 negative and they sort of have
the idea that just like ImageNet was needed for
deep learning models to really show their worth and vision
that probably does show the value of a huge model like that.
Um, you need to have really big datasets.
So, they get some much bigger,
um, text classification datasets.
So, here's an Amazon review positive-negative dataset, um,
with which they have sort of 3.6 million documents,
um, Yelp reviews 650,000 documents.
So much bigger datasets,
um, and here are their experiments.
Okay. So, the numbers at the top, uh,
for the different datasets of the best previous result printed in the literature,
and then if you read the, um,
footnotes, um, there are a few things that they want to sort of star.
So, the ones that have a star next to them use
an external thesaurus which they don't use. [NOISE]
And the Yang method, um,
use some special techniques as well that I cut off.
Um, and the other thing to mention is these numbers,
they're error rates, so low is good.
Um, so the lower you get them, the better.
And so then these are all of their results.
Um, and so what can you get out of these results?
Um, well, the first thing that you can notice is basically with these results,
the deeper networks are working better, right?
So, the one I showed you,
uh, well, no, I think the one that I have the picture of this isn't the full thing.
Um, but they have ones with depth 9, 17,
and 29 in terms of the number of convolutional layers,
and the deepest one is always the one that's working best.
So, that's a proof of deep networks.
Um, that didn't keep on working, um,
so an interesting footnote here is,
um, I guess they thought,
oh, this is cool.
Why don't we try an even deeper one that has 47 layers and see how well that works?
And, I mean, the results were sort of interesting for that.
So, for the 47 layer one,
it worked a fraction worse than this one.
Um, so in one sense you,
they showed the result of sort of residual layers work really well.
So, they did an experiment of let's try to train
a 47-layer network without using residual connections.
And, well, it was a lot worse.
The numbers went down about two percent.
And they trained one with residual connections,
and the fact of the matter is the numbers were just a teeny weeny bit worse.
They were sort of 0.1 of a percent worse.
So, you know, they sort of work just about as well.
But, nevertheless, that's kind of different to the situation in vision,
because for the sort of residual networks that people are using in vision,
this is sort of like the very minimum depth that people use.
So, if you're using residual networks in vision typically,
you might use ResNet-34.
If you're really short on memory and want to have a small model,
but you just know you'd get better results if you used ResNet-50,
and in fact, if you used ResNet-101 it'd work even better again.
Um, and so that somehow, you know,
whether it's got to do with the different nature of
language or the amounts of data or something,
you haven't yet gone to the same depth that you can in vision.
Um, but other results, um,
so the other thing they're comparing here is that they're comparing
three different ways of sort of stringing things down.
So, you could be using, um,
the stride in the Convolution,
you can be using local MaxPooling,
and you could be using KMaxPooling.
Um, and they're general,
they're slightly different numbers as you can see.
Each one, um, wins and one, uh,
at least one of these datasets or actually at least two of these datasets.
But not only does MaxPooling win for four of the datasets,
if you sort of look at the numbers,
MaxPooling always does pretty well.
Because MaxPooling does pretty well here,
whereas the convolutional stride works badly,
and over here MaxPooling works pretty well,
and the, um, KMaxPooling works kind of badly.
So, their recommendation at the end of the day is you should always use, um,
just MaxPooling of a simple kind,
that that seems to be fine,
um, and nothing else.
Um, it's actually worth the trouble of thinking about doing.
Okay. Um, was there any other conclusions I wanted to say?
Okay. Um, I think that was most of that.
I guess their overall message is you can build super good, um,
text classification systems using ConvNets,
and you should take away that message.
Okay. So, there are just a couple of minutes left.
There was sort of one other thing that I wanted to mention,
but I think I'll just sort of mention it very quickly,
and you can look in more detail if you want to.
So, we sort of have this situation
that re- recurrent neural networks are a very standard building block for NLP,
but they have this big problem that they just don't parallelize well.
And the way we get fast computation deep learning is we find
things that parallelize well so that we can stick them on GPUs.
GPUs only are fast if they can be simultaneously doing the same computation many times,
which is sort of trivial for a convolutional neural network,
because precisely, you're doing the same comput- computation every position.
But that's not what's happening in the recurrent neural network because you have to
work out the value of position one
before you can start to calculate the value of position two,
which is used for the value of position three.
Um, so this was a piece of work, um,
done by sometimes CS224N co-instructor
Richard Socher and some of his people at Salesforce Research
on saying, how can we get the best of both worlds?
How can we get something that's kind of like a
recurrent neural network, but doesn't have the bad computational properties?
And so the idea that they had was, well,
rather than doing the standard LSTM style thing where you're calculating, you know,
an updated candidate value and your gates in terms of the preceding time slice,
maybe what instead we could do is we could stick a relation between time
minus 1 and time into the MaxPooling layer of a convolutional neural network.
So, we're sort of calculating a candidate and a forget gate and an output gate.
But these, these candidate and the, um,
gated values are done inside the pooling layer via compute,
um, via, um, uh, uh, convolutional operation.
So, it sort of get,
it doesn't, it, you know,
if there's no free lunch you can't get true recurrence and not pay the penalty.
This is giving you sort of a pseudo-recurrence because you are
modeling an association between adjacent elements at each time slice,
but it's sort of just worked out locally rather than being carried forward,
um, in one layer.
But sort of what they found is,
if you made your networks deeper using this idea,
well then, you sort of start to, again,
expand your window of influence.
So, you got a certain amount of information being carried forward.
Um, so, their conclusions was that you could sort of
build these kind of models and get them to work,
you know, not necessarily better actually on this slide,
um, it says often better.
Um, you can get them to work kind of as well as an LSTM does,
but you could get them to work much faster because you're avoiding
the standard recurrent operation and keeping it as something that you can parallelize,
um, in the MaxPooling operations.
Um, yes, so that was a kind of
an interesting alternative way of sort of trying to get some of the benefits.
I think long-term this isn't the idea that's going to end up winning out.
And so next week we're going to talk about transformer networks,
which actually seems to be the idea that's gained the most steam at the moment.
Okay. I'll stop there for today. Thanks a lot.
 Okay. Hi, everyone.
Let's get started again.
Okay, so first of all let me just say a bit about Assignment 5.
So Assignment 5 is coming out today.
Um, it's a brand new assignment,
so you guys are the guinea pigs for that.
Um, and so what it's going to be, it essentially builds on Assignment 4.
Um, so it's okay if you didn't do perfectly on Assignment 4,
but I think actually most people did.
Um, and what we're gonna be doing is adding, um,
convolutional neural networks and subword
modeling to the neural machine translation system,
seeking it to make it better.
Um, so this assignment is coding heavy, written questions light.
Um, so I mean the coding that you have
to do sort of isn't actually really more difficult than Assignment 4,
it's kind of like Assignment 4.
But what we're hoping is that this time you will be able to do it on your own.
Now what I mean by that,
um, is for Assignment 4.
Well, there was tons of scaffolding telling you what everything should be,
and there were all of these auto-grader checks and you could keep on working on
your code until they passed all the autograder checks, and everybody did.
Um, and so it was very kind of coddled, shall we say.
Um, but I mean,
I guess what we're really wanting to achieve is to have a more- sorry, question.
[inaudible]?
Yes. So what we're hoping is that this can be, uh, useful.
Um, it'll be short-term pain but useful as being
a more effective ramp to doing
the final project and indeed for the rest of your life, right.
And the- the reality is that in the rest of your life,
you sort of if you're going to be doing things with deep learning,
you kind of have to work out what kind of
model to build and which pieces to stitch together,
and how to write some tests to see if it's doing something sensible.
And if it's not doing something sensible, um,
to figure out how you could change things and try different things,
and get it to work sensibly.
And so that's what we're hoping,
um, that people, um,
can do in Assignment 5,
so you've got to,
um, figure things out.
Um, should write your own testing code.
Um, we don't have a public autograder,
so you should- that's part of working out your own sanity checks,
trying to do things like what I talked about last week of sort of getting
simple bits working, confirming that they work on
minute amounts of test data and so on, and doing things more sensibly.
I mean in particular,
um, the one particular part of that,
that we were planning to do, um, for,
um, this assignment, I was looking for it,
um, but it's on the next slide.
Um, so, um, for this assignment and beyond, um,
we're going to enforce rules like more like they are in CS107,
for those of you who are undergrads,
meaning that the TAs don't look at and debug your code for you.
Um, and so, you know,
of course we still want TAs to be helpful, come to them with your problems, um,
talk about how you're meant to use different things,
um, in the PyTorch library,
um, but you shouldn't be regarding it as the TA's job of, here's a big Python file.
Um, can you tell me what's wrong with it,
and fix it up for you.
Okay. Um, the precise policy for that's,
um, written up on Piazza.
Okay. So after- any questions about that or do I go straight on in?
Okay. Um, yes so today's lecture,
um, in some sense today's lecture is an easy lecture.
Um, so last time's lecture,
there was really sort of a ton of new stuff
of other stuff on neural networks that you haven't seen before,
and we did Convnets and we did pooling layers,
and we did highway and residual connections,
and batch norms, and I don't know, whatever else we did.
Um, size one convolutions I guess.
So there are tons of new stuff really
in this lecture in terms of sort of neural network machinery,
there isn't any new stuff at all.
So this is really easy.
Um, and this is also really a new lecture but it was sort of put in for a reason.
And the reason for this relates to a kinda remark I made
last time about how lots of stuff keeps changing in neural network land.
So at the time we first designed this class and
the way as- that a lot of the structure of it still is.
Um, that sort of around 2014-2015 when we designed this class,
it was basically axiomatic that
all deep learning models for natural language processing worked off words.
And therefore it completely made sense that we start with word vectors,
and then we start looking at things like recurrent models over words.
Whereas the fact of the matter is in the last approximately three years,
there's been a ton of new work including some of the most influential new work.
There's building language models that aren't- isn't -isn't aren't, um,
being built over words that they're building, built over pieces of words or characters.
And so this lecture is sort of meant to give you
some sense of these other ways of doing things and,
um, some orientation to some of the things that's going on.
But the actual kind of models that we're looking at, uh,
sort of using all of the building blocks that we've already looked at, things like,
um, RNNs and ConvNets and things like that.
So let's get into this.
Um, so I'm going to start off with a teeny bit
of linguistics of learning about the structure of language, um,
at first sort of lower level units of language and then we'll see how that pans out,
um, for things like character level models.
So in linguistics, um,
if you start at the bottom of the totem pole,
the first-level of linguistics is phonetics,
which is sort of understanding the sounds and the physiology of human speech.
So that's sort of like physics, or physiology,
or something, right, there are mouth parts that move,
there are ear parts that act as filters,
and there's, um, audio waves in between the two of them.
So that's kind of uncontroversial in some sense.
Um, but above that level,
the standard thing that people do for the analysis of human languages is to say,
well human languages may seem to make use of a relatively small set of
distinctive units which are then commonly called phonemes which are actually categorical.
And the idea here is that well,
uh, our mouths are continuous spaces, right.
That they've got these various bits of their mouths like, you know,
tongues and pharynges and so on,
but it's a continuous space.
So actually, um, we can make an infinite variety of sounds, right.
So if I open my mouth and apply voicing and just wiggle my tongue around, I can go [NOISE].
And I can make an infinite variety of different sounds.
But the reality is that human languages aren't like that,
that out of that infinite variety of sounds,
we distinguish a small space of sounds.
Um, and something that happens when languages change is, um,
that the space of sounds that are seen as
important and distinguished in a language change.
And that happens even with inside one language as- as English.
And I'm about to give an example of that.
Um, so people in cog psych talk about the phenomenon of categorical perception.
And what that means is that really there's something continuous,
but that humans perceive it as belonging to fairly sharp categories.
Um, and, you know,
you can use that for sort of, you know,
styles of clothing or whether someone counts as fat or not.
Um, but the most famous examples of categorical perception are in language,
where we can make an infinite variety of sounds but
people per- perceive them as categories.
And so effectively what that means is when you have categorical perception,
the differences within a category are sort of perceived to have shrunk.
You barely notice them at all where differences
across categories are expanded and very clear.
And so one of the cases that sort of studied
a lot is what's called- referred to as sort of,
um, voice onset time.
So lots of languages including English have pairs of sounds like p and b, uh,
pah and bah, and they differ based on when voicing starts.
So buh, it has a voice sound like a vowel with an r in it.
And well that's a continuous parameter,
you can sort of make any point along a spectrum between a p and a b but, um,
human beings, um, who speak English,
um, perceive just two points on that spectrum.
And you don't sort of really notice the fine differences between them.
Um, some languages distinguish more points on the spectrum.
So Thai distinguishes three different consonant sounds,
um, in depending on the voice onset time.
Um, something that might be, um,
more accessible to you is,
um, this is an example of language change.
So for a speaker like me, um,
there was caught and cot and those are different vowels,
and I hear them as different vowels.
But if you are someone who grew up in the southwest of the United States,
um, then these are exactly the same vowel and you don't distinguish them.
Then- you thought I said the same thing twice even though I'm saying two different vowels.
And so that's where in even at a dialectal issue- level that people develop
categorical perception as to
which- which distinctions and sounds they're sensitive to or not sensitive to.
Okay. And summing- and,
I mean why I'm mentioning this is in some senses these sound distinctions of
categorical sound distinctions that are what a lot of
our language writing systems that we'll come to in a minute record.
Okay. Um, so in traditional linguistics, um,
you have sounds, but sounds don't have any meanings in language.
So pah and bah don't have meanings,
and a and e don't have meanings.
And so people then normally distinguish as
the next level up morphology is parts of words.
And this is seen as the minimal level that has meaning.
And so the idea is lots of words are complex and can be made
u- made up of pieces but these pieces do have meanings.
So fortune has a meaning, um, fortunate,
you end in this ate ending,
um, which sort of gives the- gives fortune to somebody.
So that means you know, having fortune, um,
that has a meaning,
un has a meaning which means to reverse that.
So unfortunate means that you don't have fortune, and ly, um,
then has a meaning of turning this all into an adverb,
and you can say unfortunately not having,
um, gotten fortune, something happened.
And so these sort of pieces of, um,
words are then the minimal things that have meaning.
Um, almost no work in deep learning has tried to
make use of this sort of morpheme level of structure.
Actually me and a couple of students six years ago did actually
try and build a system where it built these tree structured neural networks,
that put together meanings of words out of their pieces.
Um, but that really isn't an idea that's taken on widely.
There's sort of a reason why it hasn't taken on widely,
which is doing this and working out the semantically meaningful pieces of words,
is kind of hard and a lot of the time in NLP what people have found
is you can just about get the same kind of
results if you just work with character n-grams.
The kind of units that you put into the convolutional neural net.
Because if you just have a model that uses, um,
character trigrams and you have sort of start of word, un, and nfo, and so on.
For going through the ly end of word,
that those different units.
There's different character trigrams,
in a distributed way will pick up
all the important meaning components of the word pretty well,
and that that's just good enough.
And that's actually a very classic idea that's sort of been revived.
Um, so back in the second coming of
neural networks in the mid-80s into the early 90s, um,
there was qu- um,
there was quite a bit of sort of controversial work on
the structure of language and in particular Dave Rumelhart and Jay McClelland.
So Jay McClelland's still in the psych department here,
if you want to look him up in your spare time.
Um, they proposed a model of how to model generating past tense forms in English.
So this was sort of a cog-psych experiment of can we build
a system that can learn past tenses of English verbs?
And the difficult part there is some, many verbs are regular,
you add the kinda -ed ending,
but some words are irregular and you had to sort of learn about the irregular patterning.
Um, but the way they did that.
I mean partly because this was sort of early days with respect to, um, sequence models,
is that they used a representation where they represented
words precisely with these sort of character trigrams.
And that was the representation of words that they used and fed forward in their model.
And that, um, idea was met with a lot of controversy by
linguists, philosophers, and other people with their ideas of
language and so there was a lot of debate in those days about that.
But from a- as a purely engineering solution that
sort of proved to be a pretty good way to do things.
And so this decade there's been other work which includes the model, um,
developed at Microsoft of a sort of a deep, um,
semantics model where what they're using as these kind of
character n-grams to put meaning over words.
Okay so, um, so now we might be interested in building models that aren't over words.
So we're going to have a word written as characters
and we're going to do something with it such as build character n-grams.
And so something that is just useful, um,
to know is there's actually
a fair amount of variation between languages when you do this.
So it's not all the same stuff, right?
So the first problem is, um,
there are some languages that don't put spaces between words.
The most famous example is Chinese.
Um, but an interesting fact for those people of European ancestry, um,
is that you know if- for- when the ancient Greeks wrote ancient Greek,
um, they didn't put spaces between words either.
It was actually a later invention of
medieval scholars who are recopying their manuscripts,
who they decided [NOISE] maybe that'll be easier to read if we
put spaces in and then they started doing it.
Um, [NOISE] most languages these days do put spaces in between words but even,
then there's sort of a lot of fine cases.
So in particular, a lot of languages have some sort of little bits
of stuff which might be pronouns, or prepositions,
or various kind of joining words like and, and so,
and which sometimes they write together and sometimes separately.
So in French, um,
you get these kind of prepositional, I'm sorry,
pronominal, um, markers for you, I, you, have brought.
Um, and you know, these kind of little words and
pronunciation just sort of run together as je vous ai,
and arguably it's almost one word,
but it's written as separate words.
Um, where there are other languages which sort of stick things together,
where arguably they're separate words.
So in Arabic, you get pronominal clitics and some of these sort of joining words like so,
and and, and they are sort of written together as one word,
where arguably that they should really be four words.
Another famous case of that is with compound nouns.
Um, so in English,
we write compound nouns with spaces between them,
so you can see each noun.
Um, even though in many respects compound nouns
something like whiteboard behaves like it's one word, or high-school.
Um, whereas other languages,
German is the most famous case,
but also other Germanic languages,
just write them all as one word and you get very long words like that.
So we can get different words if we just use spaces and don't do much else. Um, good.
Okay. Yes. So for dealing with words,
there are these practical problems.
Um, and we sort of already started to
touch on them that if you're trying to build word-based models,
there's this huge space of words,
and well, strictly there's an infinite space of words
because once you allow in things like numbers,
let alone FedEx routing numbers, or, um,
or if you allow just morphology,
when you can make those ones like unfortunately.
Yeah, sort of, you can just expand the space of words,
so you get this large open vocabulary.
Um, English, you know, a bit problematic.
It gets way more problematic than a lot of other languages.
So here's a lovely Czech word, um,
to the worst farmable one, um,
where you can make sort of much more complex words in lots of other languages.
Um, many Native American languages,
other European languages like Finnish have these sort of very complex words,
Turkish has very complex words.
Um, so that's bad news.
Um, there are other more reasons we'd like to be able to
look at words below the word level,
to know things about them.
So when you're translating,
there's a wide space of things,
especially names, where translation is essentially transliteration,
that you're going to rewrite the sound of somebody's name as just roughly, you know,
perhaps not perfectly correctly but roughly correctly
according to the sound systems of the different language.
And well, if we want to do that,
we essentially want to work- operate at the letter level,
not the word level.
But another huge modern reason why we'd like to start modeling below the word level is,
we live in this age of social media and if you're in the social media land,
there's a lot of stuff that's written not
using the canonical words that you find in the dictionary,
and somehow we'd wanna start,
um, to model that.
So in some sense this is the, um,
easy case.
Um, good vibes.
Um, but nevertheless this is spelled with one,
two, three, four, five, six,
seven O's, and one, two,
three, four, five, oh and also seven S's, they match.
I don't know if that's deliberate or not [LAUGHTER]. Um, okay.
So this sty le of writing is very common, um, and well,
you know, we kind of sunk if we're
treating things at the word level and we're trying to model this right.
That's clearly not what human beings are doing, we're sort of looking at
the characters and recognizing what goes on.
Um, in some sense that's kind of the easy case that you could imagine preprocessing out.
Um, there's a lot of harder stuff that then turns up.
Um, I guess there's sort of the abbreviation speak, like I don't care.
Um, but then you sort of get a lot of creative spellings, um,
that come off of kind of reduced pronunciations like I'mma go, sumn.
Um, and it seems like somehow we need something other than
canonical words if we're going to start to deal better with a lot of this text.
Oops. Okay. So that suggests we sort of want to start doing that with our models.
And so, that's led to a lot of interest in using character level models.
Um, and I mean there are sort of two extents to which you can do this,
and we'll look at them both a bit.
Um, one level is to say,
look we're still gonna have words in our system.
Basically, we're going to build a system that works over words,
but we want to be able to create
word representations for any character sequence and we'd like to
do it in a way that takes advantage of being able to
recognize parts of the character sequence that look familiar,
so that we can probably guess what vibes means.
Um, and so that sort of then solves the problems
with unknown words and we get similar words,
similar embeddings for words with similar terms, spellings, et cetera.
But the other alternative is to say,
oh no, just forget about these words altogether, who needs- um,
why don't we just do all of our language processing on sequence of characters,
it'll work out fine.
Um, both of these methods have been proven to work very successfully.
Um, and I just wanted to dwell on that for one moment,
and that sort of goes back to my,
um, morphology slide here.
When people first started proposing that they are going
to build deep learning models over characters.
I mean, my first feeling was oh, that is never going to
work because it sort of seemed like,
okay, words have a meaning it makes sense,
um, that you can do something like build
a word2vec model and that's going to really be able to
sort of see words and their distribution and learn
the meanings of the words because words have a meaning.
The idea that you're going to be able to say, well,
I'm going to come up with a vector representation of h,
and a different vector representation of a,
and a different vec- vector representation of t,
and somehow that'll be useful for representing what a hat
means once I put it through enough neural network layers,
um, frankly it sounded pretty unconvincing to me.
Um, but, um, I guess, you know-
But it, it, totally works so I'm convinced now, empirical proof.
And I think what we so essentially need to realize,
is that with going- that yes,
at some level we just have these characters that don't mean much.
But we then have these very powerful combinatory models with a lot of parameters in them,
things like recurrent neural networks and
convolutional neural networks and that they are respectively able to, sort of, build,
store and build representations of meaning from multi-letter groups,
in such a way that they can model the meanings of
morphemes and larger units and therefore put together word meanings.
Um, yeah. So, um,
one more detail on using characters,
um, from writing systems.
So if you're a linguist you tend to think of sounds as primary.
Those were the phonemes that we- I mentioned beforehand.
You know, um, essentially,
um, deep learning hasn't tried to use phonemes at all.
Traditional speech recognizers often did use phonemes,
but in the deep learning land,
you want to have a lot of data and the way you get a lot of data is you just use, um,
written stuff because, you know,
it's the easily found data where you can get millions and billions of words of stuff.
Um, so that sort of makes sense from a data point of view.
But the thing that ends up as a little weird about that,
is that when you're then building a character level model,
what your character level model is,
actually varies depending on the writing system of the language.
And so you, kind of,
have these quite different writing systems.
So you have some writing systems which are just completely phonemic,
that there are letters that have a particular sound and you say that sound.
Something like Spanish is pretty much phonemic.
Sometimes it's a teeny bit complicated.
So you might have a digraph.
So this digraph, ngabulu, is, kind of, like,
the N-G of English that is used for "ng" sound like at the end of seeing,
but, you know, basically this is just 'jiyawu',
each letter is a sound,
you can read it, um,
and it's just, um, phonemic.
Um, that then contrasts from something like English where
all the non-native speakers know the spelling is terrible.
It's got this, sort of, highly fossilized,
once upon a time,
phonemic system in the tenth century or something.
Um, but now we have this system that words have
fairly arbitrary spelling that doesn't actually represent the sounds, um, very clearly.
But it's sort of a phonemic system.
But then there are languages that use larger units.
Um, this is, um,
Canadian and Inuktitut which I just put in there because it's such a pretty writing system.
Um, but there are a lot of languages that represent syllables by their characters.
Um, so you'd have something like this in Korean for example,
with Korean Hangul, that each, um,
letter is then being a syllable of a sort of consonant vowel combination like bar.
Um, if you can then go up a level from that and if we get back to Chinese again,
well, um, this is sort of also a syllabic system, you could say.
But really, the Chinese characters are much more than just the sound.
They also have a meaning.
That this is really then an ideographic system
where there are characters with particular meanings attached to them.
So they're, sort of, uh,
whole morphemes in- written as one letter.
And, you know, another example of such language,
um, was Egyptian hieroglyphs, if you've seen those.
That they're, sort of, ideographic systems where you have letters with meanings.
Um, and then you have language systems that sort of mix several of those.
So Japanese is sort of a mixture of partly moraic,
partly ideographic systems mixed together.
So if you just,
sort of, start off and say,
"Okay, I'm gonna build a character-based system." That's fine.
But effectively, your character units like
letter trigrams are just very different in a language like Chinese,
where commonly a letter trigram will be, sort of,
a word and a half,
three morphemes with meaning.
Whereas if you're in something like English,
your character trigram will be something like T-H-O
which is still sort of much too small a unit to have any meaning.
So moving right ahead.
So these two kind of approaches, um,
one was just do a completely character level model and then the other one was,
sort of, you make use of characters
to build bigger things that you're then gonna put something,
like, into a more word level model.
So I'll do this one first and the other one.
So for purely character level models,
I actually showed an example of that last time. Do you remember?
So there was that very deep convolutional network from
the Conneau et-al word for text classification at the end, um,
and that just started with a big line of
characters and built these convolutional layers on top of that,
in the vision like network and classified the documents.
Um, so that was, sort of, a completely character-level model.
Um, but here's a bit more work on this.
So people for machine translation have built, um,
machine translation systems that just read characters and write characters.
And when people first tried to do that,
um, it, sort of, didn't work, right?
The people thought it might help to build
character-level models especially for languages like Chinese.
But people just weren't able to build models that worked as
well as word-based models and either the pre-neural,
the non-neural or the neural world.
But gradually that started to change.
So people started to have successful character-level decoders and then,
sort of, around 2015 and '16,
um, people started to show,
look you could- can actually do machine translation very
well at just a character level with a few asterisks.
And so, um, here's a bit of work, um, that we did.
Um, the Luong and Manning one, from, um,
2015 on the last slide.
So this is looking at English to Czech translation and Czech's
a good language to use if you want to motivate doing things at the character level,
because it had those big horrible words with lots of
morphology like the example I showed you before and I'll show you some more later.
So people had built word-level models for Czech.
Um, and, you know, they didn't work great,
partly because of some of these vocab problems.
So, um, the, sort of,
word-level state of the art was at this time was 15.7 BLEU,
which as you know is much less than we will accept for full grades in your homework.
[LAUGHTER] Um, but, you know,
what counts as a good BLEU score depends on how difficult the language pair is.
Uh, um, and so you're not doing Czech.
Um, but, um, so this was, sort of, the,
kind of, neural MT model that we've talked about.
So it was a Seq2Seq model,
with attention and then it had extra stuff for substituting UNKs with either,
uh, single word translation or by copying stuff from the source.
So it was, sort of, basically,
state of the art neural MT of 2015, got 15.7 BLEU.
And the difference isn't big,
um, but we were able to show,
look we can build this completely, um,
character-level model and then actually, it did fractionally better.
Um, so this, sort of,
showed that in terms of translation quality, um,
character, purely character-based models were completely viable at
capturing the meaning of text as well as word-based models.
Um, was this a great result?
Um, in many, in some ways,
yes, in another way, no.
I mean, this model was truly terrible to train, right?
So it took about three weeks for us to train this model and at run-time,
it also worked very slowly.
And so the problem with character-level models,
if you're putting them into something like an LSTM,
is your sequences get way longer, right.
So you've got about seven times as long sequences as you used to have.
And since there's not much information, the characters,
you have to do back propagation through time much further back.
And so we were running back propagation through time for
600 steps before we were trun- truncating it.
And so this, sort of, made,
maybe that was excessive,
but it made the models, um, very slow.
But we were able to show that it was able to get some of these good effects, right.
So here's a Czech,
um, translating to Czech,
her 11 year-old daughter,
Shani Bart, said it felt a little bit weird.
And, um, I don't know, probably.
Does anyone speak Czech, any Czech speakers?
Um, no Czech speakers?
Okay, um, I don't speak Czech either, um,
but we can see that the [LAUGHTER] we can see that this does interesting things, right.
So the second line is the human translation
into Czech which we can use for some guidance.
And so in particular, um,
in Czech there's a word for 11 years old,
um, which you can see is that blue word on the second line.
And you can see that despite 11-year-old was, um,
that for 11-year-old it's just able to perfectly, um,
produce letter by letter, um,
the Czech word for 11 years old and that works beautifully.
In contrast, for the word-level model, um,
11 year-old was an unknown word because that wasn't in the vocabulary.
And so then it had two mechanisms to try and deal with, um, unknown words.
It could either do, uh,
unigram translation of them or it could just copy them.
And for whatever reason, it decided here that the best strategy
was to copy and so that was a complete fail.
Um, and if we go along for the character-level model,
another thing that it gets right that's really cool,
um, is the name Shani Bart.
It's able to do this transliteration tasks that I mentioned just perfectly.
And it turns that to Shani Bartova
which is exactly what the human translator did as well.
And so, you know, it's actually doing some really kind of nice, um,
human translator, um, like things.
I mean, in fact,
as best I can tell from spending a bit of time on Google Translate,
it actually does a pretty good job in this sentence, period.
All right, this part here starts to be different,
um, from the human translator.
But it's not actually bad.
It's sort of a more literal translation.
So this citi um,
actually translates feel like in the English texts.
Whereas the human, sort of,
didn't actually use the word feel in the Czech version that they just went,
um, was a little bit,
um, weird or strange. So that's cool.
Okay. So here are a couple more results from this.
So here's another system that was built the next year.
By these people Jason Lee,
Kyunghyun Cho and Thomas Hoffman.
So they wanted to do something that was, I don't know,
much more complex and neural and
understanding the meaning of the text on the source side.
And so they were more using the kind of technologies we saw last time.
So on the encoder side you started off with a letter sequence of character embeddings.
And then you're sort of using convolutions of four,
three and five of characters to get representations up here.
You're then doing a max pooling with a stride of five.
So you're getting a max pooled representation of pieces of the text for each of the three,
four and five convolutions.
You're then feeding that through multiple layers of
highway network and feeding that through
a bidirectional gated recurrent unit and that's giving you your source representation.
On the decoder side,
it was sort of the same as our decoder,
it was just running a character level sequence model.
So overall, so they were doing the opposite task.
This is Czech to English.
But, so they are starting to get better scores.
But I mean actually if you're sort of looking at these different numbers,
where I'll explain this system more in a minute,
I mean it sort of seems like the place where they get a lot of value is that using
the character level decoder gives them a lot of value by
this very complex model on the source side is giving them almost no value at all.
One even more recent paper,
so this is Colin Cherry and fellow researchers at Google.
So they last year did one more exploration of
doing LSTM sequence to sequence
style models of comparing word and character-based models.
And this is English to French and this
is um, Czech to English which is just what we were doing.
And so in both cases when you have a big model,
the character model wins for them.
The blue model comes out on top but the sort of interesting thing as you
sort of see these different effects depending on
the morphological complexity of the language.
So for a language like Czech,
it's a really good idea,
if you want to build a good model,
to use character level that they're getting about a BLEU point of difference there,
whereas for a model without putting French or English there's
actually a tiny but very little gain from using a character level model.
Okay so let me just explain these models,
so these models are models of different sizes.
So these models are using bidirectional LSTM encoders and one-directional LSTM decoders.
So the simplest model just has
a shallow bidirectional LSTM encoder and a two layer LSTM decoder.
The middle model has a three deep stack of
bidirectional LSTM encoders and a four deep stack of LSTM decoders.
And the most complex model has a six deep stack of
bidirectional LSTM encoders and an eight deep stack of LSTM decoders.
This is where it helps to work at Google.
Probably for your projects,
you don't want to go beyond three or four. Stay over here.
Okay yeah so, so these are the results.
So basically what you're finding is if you're making
sort of smaller models you're better off with words,
but as you go to big models especially if you're in a morphologically rich language,
you clearly start to win from the characters.
But there is still a loss which is essentially
exactly the same loss that we were suffering from in 2015, right?
This is the time graph and so these are the same three models as over here,
it's just the axis is changed to sort of sum the total number of LSTM layers.
And so essentially, if you're at the word level,
you can run any of these three models and they are fast that you can be translating in
sort of not much time but for the character level models your slope is much higher.
So it starts to get quite expensive to run the deep character level models.
Okay, so that's that section.
So then chugging along.
I then wanted to look at other ways of doing things.
And so these are models that in some sense still do have words but where
we're going to want to sort of build word representations out of pieces.
And there are essentially two families of ways that people have explored doing this.
One way of doing it is to say look we just want to use
exactly the same architecture as we use for a word model
except our words aren't really going to
be words at least sometimes they're going to be pieces of words.
And so those are often called word piece models.
And in particular, there's one commonest way of doing it.
It's called BPE, which I'll go through in some detail.
The other alternative is to say,
well, we're gonna kind of make a mixture or a hybrid.
So our main model is going to work in terms of words but we're
going to have some kind of facility where we can construct a representation,
for otherwise unknown words,
by doing things that at a character or a lower level.
And I'll show you a bit of that as well.
Okay, so this is BPE.
BPE is actually a pretty simple idea which has nothing to
do with deep learning but the use of BPE has sort of become
pretty standard and successful for representing pieces of words to allow you to
have an infinite vocabulary
while an infinite effective vocabulary while actually working with a finite vocabulary.
So the origins of Byte Pair Encoding and
the name byte pair has nothing to do with natural language processing or neural nets,
we're just writing a compression algorithm.
So this is something like compressing your documents with gzip.
So what basic Byte Pair Encoding is,
that you've got a collection of stuff with bytes and you are
looking for the most frequent sequence of two bytes and you say,
okay, I'm going to add that sequence of two bytes as
a new element to my dictionary of possible values.
And that means I can have 257 different values for bytes so to
speak that I can shrink the length of
my sequence and I can repeat over and do that again.
And so essentially, this work suggested,
well we can apply this kind of compression algorithm and use it as
a way of coming up with pieces of words that were
useful, doing it not strictly with bytes despite
the name but instead with characters and character n-grams.
And so the most common way to do this with
characters and character n-grams and if you're up with modern times,
you know that means there's unicode and you can represent
all of these lovely letters like Canadian Inuktitut's
syllabics and stuff like that.
But there's actually a problem with Unicode,
which is there actually a lot of Unicode characters.
I forget the number theoretically.
I think there's about 200,000 possible Unicode characters.
But at any rate, if you want to handle a bunch of languages which include East Asian languages,
maybe you need something like 20,000 characters and that's sort of a lot.
So there are actually some people who've literally gone back to bytes and said,
"You know 200,000, that's a really big vocabulary.
I don't want to deal with anything."
Sorry, 200,000 is a really big vocabulary.
I don't even want to deal with anything that large.
So why don't I actually just do these kind of algorithms over bytes?
And so that means that in UTF-8 encoding,
Chinese characters take three bytes each.
And so you're actually have to- you only get whole characters if you
actually merge together several bytes that are common sequencers.
Okay. So more concretely,
um, how does this work?
So we're sort of doing this bottom-up clustering of short sequences.
So we start with a unigram vocabulary,
which is all of the Unicode characters and some data.
We then sort of ask, what's the most frequent ngram here?
Um, initially it will be a bigram pair and we add that to our vocabulary.
So if we start off, you know,
we can take our text that's- um,
I'll come back to this in a minute.
Let's assume we have a text that has been divided into words so we do have word tokens.
And so we can represent as a dictionary and say here are some words with their frequency.
Um, and so now we look for a common letter sequence and we say, "Oh, es."
That occurs nine times, um,
in this data because we have the counts for the words on the left side.
So, um, we start with our vocabulary being all the individual letters.
We find a commonest letter sequence like es,
and so we say, "Let's clump that together and make that a new thing in our vocabulary."
So now we've got an extra thing in our vocabulary.
And now what's the commonest ngram sequence that clumped something?
Well, actually all of these es's are followed by t,
so we also have es,
t with frequency nine,
and so we can add that to our vocabulary.
And then we ask again, well,
what's another common letter sequence?
Let's see, there are seven cases of o double- well,
I guess there are seven cases of either l o or o w,
so we can lump those and then we can lump
again and make an lo w. So if we sort of run this,
we start to build these clumps of common letter sequences,
and so common bits like est,
but also just common words,
something like that in English will very quickly be
clumped together and be a unit of our vocabulary.
Um, and so we do that for a while.
So normally what we do is we decide
a vocabulary size that we want to work with. We say, "Okay.
I want to work with a vocabulary size of 8,000 words."
That'll mean my model will be fast,
and we just sort of keep doing this until we have 8,000 things in our vocabulary.
And that means that our vocabulary will have in it
all single letters because we started with them and it'll
have common subsequences of words like the es and the est that are now in our vocabulary,
but also have whole words whenever there're common words, like, you know,
the, and too, and with,
and so on, will become parts of our vocabulary.
Um, and so then when we have a piece of text we can do
a deterministic longest piece segmentation of words,
and we will say that is our eeset of word pieces.
And so for an input piece of text,
we turn into word pieces,
and then we just run it through our MT system as if we were using words,
but really it's pieces of words,
and then on the output side,
we just concatenate them back together as needed.
Okay. So we get this sort of automatic word-based system.
And that's proved to be a very successful system.
So this idea of using byte pair encoding sort of really
emerged in 2015 and then in 2016, uh,
workshop on machine translation which has been the main sort of annual competition for
MT systems that the several top systems were built using byte pair encoding.
If you look at last year's competition,
there's a bit more variety,
but really a number of the top systems are still using byte pair encoding,
that's just been a good way to do things.
So for Google's Neural Machine Translation,
they effectively use of- a variant of byte pair encoding.
So they don't use exactly the same algorithm, um,
they use a slightly different algorithm where they're
using a language model and they're saying,
what- what- rather than just using pure counts,
they're saying, "What clumping together would maximally reduce
the perplexity of my language model and clump those things and repeat over?"
And so they did- they've done two versions of this model.
So the first version, the wordpiece model kind of like, um,
byte pair encoding assumed that you have an initial tokenization
to words and then you're just sort of having pieces of words,
um, using this algorithm.
And then they did a second version, um,
the sentencepiece model which you can find at this GitHub site which said, "Well,
it's problematic if we need to tokenize into words first
because then we need to have a tokenizer for
every language and that's a lot of work."
Um, so maybe instead of that,
we could just sort of treat,
go from a character sequence,
retain whitespaces and regard that as something that's part of the clumping process,
and so that, um,
you just build your word pieces which
commonly will have spaces on one side or the other of them, um,
because often things inside a word are the
commoner- more common clumps and you build those up,
and that's proven to be quite successful.
Um, in particular, one place where some of you might see this,
um, is, um, we've yet to get to describing it in the class really,
but there's been this recent work which we actually talk about next week in class, are
building these transformer models, in particular,
Google has released this BERT model which gives you
very good, um, word representations.
And if you download BERT and try and use it,
what you will find out is it doesn't operate over words,
it operates over word pieces.
Um, and so it has a large vocabulary.
It's not a vocabulary of like 8,000 words.
I forget the number,
but the models have a large vocabulary,
but they're still not a huge vocabulary and it's using word pieces.
So lots of words are in the vocabulary.
So if you look at the English model,
it not only has words like f in it,
but it even has words like Fairfax and 1910s,
which aren't that common.
Um, but it's nevertheless to cover all words,
it's again using this wordpiece idea.
So if I want a representation for the word hypatia, um,
that's not in the vocabulary,
and so I'm making it up of pieces.
There's an h representation,
and then in the BERT version,
which is different to the Google NMT version,
the non- the non-initial word pieces are represented with two hashes at the start,
so I can put that together with h##yp etc.,
and this would be my representation of hypatia.
So effectively, I have word vectors, um,
for four word pieces,
and then I have to work out what to do with them.
The simplest and quite common way is I just average the four of them.
And there are obviously other things you could do.
You could ConvNet and maxpool or you could run
a little LSTM or something to put together a representation.
Okay. Yeah. So- so those were the models that, um,
sort of, worked with pieces of words to give you
infinite vocabulary and ran them through a normal system.
The other possibility is to say, "Well,
we wanna work with characters so we can deal with an infinite vocabulary,
but we're gonna sort of incorporate those into a bigger system."
And a whole bunch of work has done this
and in some sense it's a fairly obvious thing to do.
Um, so this work in 2014 was one of the early ones.
So they said, "Well,
we could start with characters.
We can do a convolution over the characters to generate word embeddings,
and then we can use those word embeddings for something in a higher level model."
Um, this was actually sort of a fixed window model for doing part of speech tagging.
Um, that makes sense.
Instead of a convolution,
you could use LSTM.
So this was work from a year later,
and they said, "Well,
we're also gonna build up, um,
word representations from characters.
And the way we're gonna do it is we're gonna run character level Bi-LSTMs,
concatenate the two final states,
and we're gonna call that outward representation,
and then we're gonna put that word representation into a
language model which is then a higher level LSTM that works along a sequence of words."
And I thought I'd just- Oh, yeah.
Words, are they training uh, like character-
Yeah. Oh yeah, that's very important to realize.
Yes so yeah so if you're learning- you'll learn- I mean this is the hidden layer.
I guess I'm not actually showing the input layer but for the input layer
you're learning a vector for each character.
So effectively you're doing the same kind of thing we saw
before that you're starting with random representations for each character.
You've got this embedded inside a word sequence LSTM,
your goal is to minimize the perplexity of the higher level LSTM as,
um, as a language model and so it filters back its gradients.
So it's wanting to come up with character vectors such that if it
produces good word vectors which produces low, um, perplexities.
Good question. Um, so here's, um,
a slightly more complex version of trying to
do this that's a bit more recent where again the idea is can we build
a good language model by starting out from characters
and wanting to exploit sort of related sub words and rare words.
And so they built sort of this kind of
this more stacked complex model that we'll go through the stages of wherefore
we start with a word represented as characters.
We have character embeddings which we build
into a convolutional network and then we head upwards.
So if we take that one piece at a time,
um, so you have a character embedding for each character.
Um, you'll then have a convolutional layer which
then sort of rep, has various filters that work over those,
um, character sequence of two, three and four grams of characters.
So you're getting representations of parts of words.
Um, then from those convolutional networks you're then doing max pooling over time which
is effectively sort of like choosing which of
these n-grams best represents the meaning of a word.
Um, then what they do after that is so at that point
they've got an output representation for character n-grams,
and so then they feed that into a highway network like we talked about a bit last time.
And then the output of that then at the word level,
um, goes into an LSTM network,
and this LSTM network is now word-level LSTM network,
and you're trying to sort of maxim- minimize
perplexity like for the neural language models we saw earlier.
Um, so what could they show with this?
Well, the first thing they could show with it is that it
actually again just works well as a language model despite that skepticism
that I hadn't told you of about the fact of
the matter is you could build these kind of character level models and
train them and they work to a first approximation as well as word-level language models.
But one of the observations that they make is that you can be
getting as good results but with much smaller models.
So up at the top here are
the character level LSTM models and word ones that the models they built.
And here are a whole bunch of models over this data-set.
Um, and so as time went by perplexities have been going down,
gone to 78.4 and their point was well we can
build pretty much as good a character model with 78.9 perplexity but
our model is actually much smaller, this model here has
52 million parameters whereas our model that
works on a character level has only 19 million parameters.
So it's about 40% of the size.
And that seems,um, kind of interesting.
But perhaps what's more interesting is to sort of peek inside it and see what
happens with the representation of words when
built out of characters and this part is sort of actually a bit cool.
Um, so what this is showing is for words that are up to the top while,
his, you, Richard, trading.
It's asking what other words are most
similar to it according to the word representations that's computed.
And the top part is the output of a word level LSTM model and that's sort of okay.
Richard comes out as similar to Jonathan,
Robert, Neil and Nancy et cetera.
While although letting though minute mainly okay.
But it's sort of interesting what happens with their character level models,um,
and so in particular, um,
what's kind of interesting is like first of all you remember they sort of had
the character embeddings that went through the convolutional layer and the max pooling.
And if at that point you ask what things are most
similar that basically it's still remembering things about characters.
So the most similar words to while,
chile, whole, meanwhile and white.
So at least for the sort of first ones they all end in LE.
And you see that pattern elsewhere right close to Richard,
hard, rich, richer, richter that hard ends in ARD, rich.
So you're just sort of getting this character sequence similarity,
it's not really doing meaning at all.
But interestingly when they're then putting it through the highway layers,
that the highway layers are suc- successfully learning how to
transform those character sequence representations
into something that does capture meaning.
So if you then say at the output of
the highway layers what words are most similar then it seems to be working pretty well,
While was similar to meanwhile, Richard is similar to Edward, Gerard, Edward with Carl.
They're sort of now working much more like
a word level model in capturing semantic similarity.
So that seems kind of cool.
Um, so then they say well what about if we ask about
words that aren't in the vocabulary of the model.
Well, if they're not in the vocabulary of the model,
the word level model can't do anything and so that's why you get those dashes there.
And what they're wanting to show is that
the character level model still works pretty well.
So if you give it look with seven O's in the middle of
it that it's correctly deciding that look,
look, look, looking are actually
the most similar words to that which is actually working very nicely.
And some of the other examples are similar, computer-aided,
is seen as most similar to computer-guided,
computer-driven, computerized, computer, you're getting pretty similar sensible results.
And then the little picture on the,
um, right is sort of, um,
showing, um, one of these 2D visualizations of the units that have been learned.
And so the red,
the red things are word character prefixes,
the blue things are character suffixes,
the orange things are hyphenated things
like in the middle of computer-guided and gray is everything else.
And so there's some sort of sense,
with which it's picking out different important parts of words.
Okay. Um, and that's why also I guess just another good example of how you can sort of
compose together different kinds of building blocks to
make more powerful models that you might
also want to think about for your final projects.
Okay.
Um, so here's
back to one other example from a neural machine translation system
of doing this hybrid architecture that has word-level and character level.
I showed you earlier a purely character level model.
I mean we built that out of interest to see
how well it did but we were sort of really wanting to build
a hybrid model because that seemed like it would be much more practical
to build something that translated relatively quickly and well.
Um, so the idea was we'd mainly build
a word-level neural machine translation system but we'd
be able to work with character level stuff when we had rare or unseen words.
Um, and that turned out to work pretty,
um, successfully at improving performance.
So the idea of that model is this.
Um, that we're going to run a pretty standard, um,
sequence to sequence with attention LSTM neural machine translation system.
In my pic- I mean, it's actually a four-level deep system but in my picture I
showed less than four levels stacked to make it easier to see things.
And we're going to run this with a reasonable vocabulary of 16,000 words.
So for common words we just have word representations that we're feeding into
our neural machine translation model but for words that aren't in the vocabulary we're
going to work out a word representation for them by using a character level LSTM,
and conversely, when we start to generate words on
the other side we have a soft max with a vocabulary of 16,000.
It could just generate words like [NOISE] but one of those words is the UNK symbol.
And if it generates the UNK symbol we then run a- we take
this hidden representation and feed it in as
the initial input into a character level LSTM and then we have the character level
LSTM generate a character sequence until it generates
a stop symbol and we use that to generate words. Um-
Okay. So we end up sort of with this sort of
hybrid composed stack of eight LSTM layers. Uh, yeah.
[inaudible] and you always get some probability for the UNK symbol.
So if you wanted to get the- the proper gradient,
you- you'll always have to run it for every word but what- what do you-?
I would often say, you only run during training,
you only run the character level LSTM when the UNK symbol receives the highest likelihood.
So we-
What is that?
So at training, at training time,
there's a determinant piece of tech, right?
You know the source and you know the target,
and so we're, and at training time,
we've already decided our vocabulary, right?
That we've just decided what are the 15,999 most common words,
those and UNK are our vocabulary.
So for both the input and the output side,
we know which words aren't in our vocabulary.
And so if it's not in our vocabulary,
we're running this one.
If if what was the output is not in our vocabulary,
we're running that one, and otherwise we're just not running it at all, yeah.
So and and the bit that I didn't explain but is actually important perhaps
related like when we're calculating a loss that we can back
propagate, that sort of up here,
there are sort of two losses.
There's a loss at the word level that you know you'd like to in this position,
give probability 1 to generating UNK but really,
this model we'll softmax, we'll say UNK is you know probability 0.2 or whatever.
So there's a loss there and then secondarily,
there's a particular sequence of characters you wanna generate and you've also got
a loss because you've met the probabilities you put over the characters.
Um, So then, um,
I think we- I think Abby sort of briefly mentioned this.
Commonly, the decoders do some kind of
beam search to consider different possibilities before deciding,
um, the highest probability one over a sequence of words.
And so this was doing a slightly more complex version of that.
So there's a word-level beam search when running it and then
also doing a character level beam search to consider different possibilities.
And so if you wanna integrate the the two of those together.
Um, but essentially, um,
this worked pretty well.
Um, so, um, this was the winning system at WMT 2015 which
used 30 times as much data and ensembled together
three other systems compared to the data that was provided for the task.
This was the system I showed before, they got 18.3.
Um, and if you remember our character,
purely character level system got 18.5.
Um, then by building this hybrid system,
that we were able to build a much better system that was about 2.5 BLEU points better,
um, than after- than either this word level or the character level system.
So that was kind of nice, um,
and in particular that was the state of the art at the time.
Now of course, if you were paying very close attention,
that's now nowhere near the state of the art.
Because when I showed you that slide way earlier of the Google system,
you will have noticed that they have
much higher numbers in the 20s, but that's what happens as the years go by.
Um, okay.
But here's an example that shows
these different systems working and some of the mistakes they make.
Um, here's a cherry picked example, um,
where our system, the hybrid system,
works perfectly because what- that's what you expect to see.
Um, and so, you know,
you can see some of the defects of things that can go wrong.
Um, so in this case,
you know the character level system didn't work here because it just sort
of starting with the Steph, it sort of seemed to free associate,
um, a completely made up name that doesn't really have anything to do with the source.
So that one isn't very good.
Um, the word level system went bang here,
so you remember when it generates an UNK,
the word level system would have when it generates, it's using attention.
So when it wants to generate, um,
it has attention back to words and the source.
And when it generates UNK has two strategies.
It can either do unigram translation of the word that it's maximally
putting attention on or it could copy the word that it's maximally putting attention on.
Um, so in this case,
it chose to translate the word that it was maximally putting attention on but the word it
was maximally putting attention on was after rather than diagnosis.
And so you just get this po po coming out of after,
after and we've completely lost the word.
Um, and in this example, in this example,
how a hybrid system, um,
just ends up working beautifully and gives you exactly the right translation.
Yeah. Um, of course,
it's not always that good in the real world.
Um, so here's a different example.
So this is the example I showed before with the 11-year-old daughter.
Um, and in this example,
the hybrid model has the same strength of the character model.
It correctly generates 11 years old at a character level in its translation,
but you know this time, for whatever reason,
it's the hybrid model that goes bang in
generating the names and it translates Shani Bart as Graham Bart.
Um, whereas the character level model gets it right.
Um, actually, I think this is one of the weaknesses of
this hybrid model compared to the character level model.
That because of the character level generator is
kind of this sort of second level.
For the purely character level model,
it's able to use the character sequence as conditioning context very effectively.
Whereas our hybrid model,
although we feed the hidden representation of
the word level model in as
the starting hidden representation of the character level model,
it doesn't have any representation further
back than that of what's in the word level model.
And so it tends to not always do as good a job at representing,
of capturing the context that allows it to do translation of things like names.
Okay. Um, very- almost finished but there's
just sort of one thing I wanted to mention before
the end which is almost a practical thing.
Um, So we started off with word embeddings,
but now we've been talking a lot of character level models.
So surely, just for word embedding, you should be able to do useful things with them,
with characters or pieces of words.
And that's something that people start to play with.
So in this Cao and Rei paper they said well
let's train a Word2vec model using exactly the same, um,
loss as Word2vec uses but let's,
um, rather than having word representations,
let's start with character sequences and run
a bidirectional LSTM to work out word representations,
and we'll then sort of be effectively
training this more complex model where we're learning
character embeddings and LSTM parameters and that will give us our word representations.
And that's an idea that people have continued to play with,
and so in particular I just wanted to mention these FastText embeddings.
Um, so a couple of years ago,
um, people now at Facebook,
the same Tomas Mikolov who did the original Word2vec,
brought out a new set of embeddings,
the FastText embeddings and their goal was to sort of
have a next-generation Word2vec, um,
which is sort of an efficient fast, um,
word vector learning library, um,
but it was better for rare words and languages with lots of morphology.
And the way they did it was that they sort of essentially took the Word2vec skip
gram model but they augmented it to put in character n-grams.
So more precisely, this is what they did.
So, um, when you had a word,
my example word is where,
for some n-gram size you represent it as a set of n-gram.
So this is kind of just about like those, we called phonemes I mentioned
right at the beginning where you have a kind of a boundary symbol,
so you know the beginning of the word.
So if the length is three you have beginning of word WH, WHE, HER,
ERE, RE end of word,
as pieces of representation.
And then you have an additional one for just the whole word.
So you do still have whole word representations in this model.
So where is represented by six things and so
then you're going to use all six of those things in your computation.
Um, so if you sort of remember the guts of
Word2vec that what you were doing was you were doing
these vector dot products between your context representation
and your center word representation.
So they're going to do exactly the same thing but for
the center word they're gonna use all six of these vectors.
All the vectors corresponding to all six of
these representations and they're going to sum them.
And so you're just doing a simple summing operation,
and that's sort of then giving you your representation of similarity.
Um, very precisely, they don't quite do that
because there's a hashing trick but I'll leave that out.
But what they're able to show is that that model actually works pretty successfully.
So these are words similarity scores,
skip gram, they're all CBOW,
and then this is the sort of new model,
um, that, um, uses these kind of n-grams.
And in this, um,
you know at least for one of the English data sets,
it doesn't get any better.
Um, but what they especially notice this is for languages that have more,
morp- more morphology that you're sort of getting some fairly clear gains.
70, 69 onto 75,
59, 60 on to 66 in the right column,
so then these wordpiece models do give them a better model of
words and just practically FastText, um,
library now has sort of word embeddings for about 60 or 70 different languages,
so it's sort of a good source of word embeddings for multilingual applications.
Okay, I think I am done.
So thanks a lot and see you again next week.
 Okay hi everyone.
Let's get started again.
Um. Okay. So, first of all for a couple of announcements.
Um, first of all thanks to everyone, um,
who filled in our mid-quarter survey we've actually gotten,
um, great participation in that.
Here are my two little Pac-Man figures.
So, the Pac-Man figures thinks,
means that almost everyone thinks the lectures are at
the right pace and those that don't are pretty much evenly divided.
Um, if we go for how challenging was Assignment three,
slightly more people thought it was too easy than too hard.
So, I guess we're setting about rectifying that with
assignments four and five, um, [NOISE].
So, though there are a whole bunch of other questions and we've
been trying to absorb all the feedback.
I mean one of the questions was what people wanted most from the remaining lectures.
I guess the good news here is really we're very good at predicting, um,
what people wanted, that or else everybody
just looked ahead in the syllabus and wrote down what it said was
ahead in the syllabus but I guess the most popular four answers to
topics that they wanted in the remaining lectures were Transformers and BERT,
both of which are gonna be covered this week.
Uh, question-answering which we talked about last week, um,
and then text generation and summarization
and you guys get Abby back next week to talk about that.
Um, there are also a lot of people also answered this question
a different way as to what kind of style of stuff,
um, some people emphasized new research and the latest updates from the field.
I guess we'll get some of that today as well,
some people are more interested in
successful applications in industry or trying to do a bit of that,
um, cool new neural architectures.
Um, the bottom answer wasn't the most popular one,
I'll admit but at least a few people, um,
wish that we were teaching more linguistic stuff.
Um, I mean that is something that I actually feel
a bit awkward about the way things were merged with CS224N,
with this deep learning,
I mean the truth of the matter is that sort of seems
like in the early part of the course,
there's so much to cover with,
um, neural networks, backpropagation,
different, um, neural net architectures and so on that the reality is that we
teach rather less linguistic stuff than we used to in the class.
I mean, for the last four weeks of the class we really do try and
cover some more linguistic stuff topics.
Um, so look forward to that.
Um, announcements.
Okay. So we've made a couple of deadline changes.
Um, firstly, a number of people have
mentioned that they think assignment five is a bit tough.
And so, we're giving people one extra day,
um, to do assignment five.
Um, I'm realizing in one sense that one extra day is not a ton
but you know there's sort of this complex balance here because on the other hand,
we don't really want to undermine time that people have available for final projects.
And if you're one of the people who hasn't yet started assignment five,
um, we do really encourage you to get underway on it.
Um, yeah, in the reverse direction
we decided that the project milestone was really too late.
If we are going to be able to give you feedback on it that you could usefully make use
of, so we're moving the project milestone date two days earlier.
And so, we've also gotten everyone's project proposals and our
planned hope is to get them back to everybody on Friday.
Yes, so, a lot of things moving.
Um, and finally on other announcements I guess, um, on
this Thursday is our first invited speaker, um, and so,
if you're in person student you're meant to be here,
um, and if you're not able to be here,
you should know about our reaction paragraph policy and
I actually stuck up on the Piazza pinned posts about, um,
reaction pieces and attendance, an example of a reaction piece, um,
from a past class to make it a little bit more concrete what's expected there.
But, you know, the idea is what we're hoping for something that isn't a ton of work.
You can just write 100, 150 words, a few sentences,
but wanting you to pick out a specific thing that was
interesting and write a couple of sentences
about what it was and what your thoughts are about it.
I, not just some very generic statement of this was a lecture about transformers.
He talked about transformers and it was interesting,
that is not what we want for the reaction piece. Um, okay.
So, here's the plan for today.
So, for today's, what I want to talk about is,
um, the exciting recent work about contextual word representations.
I mean I, I was thinking of what I was gonna say I was wanting to say, oh, this is
the most exciting thing in deep learning for NLP in
the last five years then something's just completely wrong,
because really this is the most exciting thing in deep learning that happened in 2018.
I mean, I guess things move very quickly, um,
in deep learning at the moment and it's sort of I don't think it's
really fair to say that you know it's got 5 years of life.
But there's a very exciting thing that happened last year,
and we'll talk about that.
Okay. So, we'll talk about early stuff,
the ELMo, ULMfit,
transformer architectures briefly and then go on to
talk about the BERT model that's being quite prominent lately.
So, let's just recap,
let's just go backwards a bit first to think about, um,
where we've been and where we are now and why we might want something more.
So, up until now,
we've sort of just had,
one representation for words which is what we learned at the beginning of class,
there was a word, you trained a word vector for it and that's what you used in your model.
Um, and you could do that, with algorithms like Word2vec,
GloVe, or fastText that I mentioned last week.
Um, so some on this sort of progression of ideas in deep learning,
when deep learning for NLP or the general
just the resurgence of neural networks for NLP
came about sort of at the beginning of this decade.
Um, these pre-trained word vectors.
So, pre-trained unsupervised over a large amount of text.
They were completely seen as the secret sauce,
and they were the thing that transformed
neural networks from NLP to something that didn't really work,
to something that worked great.
Um, so, this is actually an old slide of mine.
So, this is a slide I guess I first made for
2012 ACL tutorial and then sort of used in lectures.
Sort of in 2013, 2014. Um-.
And so this was sort of the picture in those years.
So this was looking at two tasks,
part of speech tagging and named entity recognition which I'll use quite a bit today.
And, you know, the top line was showing a state of the art which was
a traditional categorical feature based classifier of the kind
that dominated NLP in the 2000s decade, in their performance.
And what then the next line showed is that if you took the same data set
and you trained a supervised neural network on it and said how good is your performance?
Um, the story was, it wasn't great.
Um, part-of-speech tagging has very high numbers always for various reasons.
So perhaps the more indicative one to look at is these named entity recognition numbers.
So, you know, this was sort of neural net sucked, right?
The reason why last decade everybody used, um,
categorical feature based, you know,
CRF, SVM kind of classifiers.
Well, if you look, it worked eight percent better than a neural network.
Why wouldn't anybody?
But then what had happened was people had come up with this idea that we could
do unsupervised pre-training of word representations,
um, to come up with word vectors for words.
And, you know, in those days,
this was very hard to do the alg- both because of
the kind of algorithms and the kind of machines that were available, right?
So Collobert and Weston, 2011,
spent seven weeks training their unsupervised word representations.
And at the end of the day,
there are only 100 dimensional, um, word representations.
But this was the miracle breakthrough, right?
You've put in this miracle breakthrough of unsupervised word representations.
And now, the neural net is getting to 88.87.
So it's almost as good as the feature-based classifier,
and then like any good engineers,
they did some hacking with some extra features,
because they had some stuff like that.
And they got a system that was then slightly better than the feature based system.
Okay. So that was sort of our picture that,
um, having these pre-trained,
unsuper- and unsupervised manner of word representations,
that was sort of the big breakthrough and
the secret sauce that gave all the oomph that made,
um, neural networks competitive.
Um, but, you know,
it's a sort of a funny thing happened which was after people had sort of had
some of these initial breakthroughs which were
all about unsupervised methods for pre-training,
it was the same in vision.
This was the era in vision,
where you were building restricted Boltzmann machines and doing
complicated unsupervised pre-training techniques on them as well.
Some- somehow, after people had kind of discovered that and started to get good on it,
people sort of started to discover, well,
actually we have some new technologies for non-linearities,
regularization, and things like that.
And if we keep using those same technologies,
we can just go back to good old supervised learning.
And shockingly, it works way better now inside neural networks.
And so if you sort of go ahead to what I will call,
sort of 2014 to 2018 picture,
the, the picture is actually very different.
So the picture is, so this,
the results I'm actually gonna show you this is from the Chen and Manning,
um, neural dependency parser that we talked about weeks ago.
The picture there was, um,
and you could- despite the fact that
this dependency parser is being trained on a pretty small corpus,
a million words of supervised data,
you can just initialize it with random word vectors,
um, and train a dependency parser.
And to a first approximation,
it just works fine.
You get, get sort of a 90 percent accuracy,
E- um, English dependency parser.
Now, it is the case that instead,
you could use pre-trained word embeddings and you do a bit better.
You do about one percent better.
And so this was sort of the,
the new world order which was yeah, um,
these pre-trained unsupervised word embeddings are useful because you can
train them from a lot more data and they can know about a much larger vocabulary.
That means they are useful.
They help with rare words and things like that and they give you a percent,
but they're definitely no longer the sort of night and day,
uh, thing to make neural networks work that we used to believe.
I'm, I'm just gonna deviate here to,
from the main narrative to just sort of say, um,
one more tip for dealing with unknown words with word vectors,
um, just in case it's useful for some people,
building question answering systems, right?
So, um, so for sort of word vectors on unknown words, you know,
the commonest thing historically is you've got your supervised training data,
you define a vocab which might be words that occur
five times or more in your supervised training data.
And you treat everything else as an UNK.
And so you also train one vector per UNK.
Um, but that has some problems which you have no way to
distinguish different UNK words either for identity or meaning.
And that tends to be problematic for question answering systems.
And so one way to fix that is what we talked about last week,
you just say, "Oh, words are made out of characters.
I can use character representations to learn word vectors for other words."
And you can certainly do that.
You might wanna try that.
That adds some complexity.
Um, but especially for things like question answering systems,
there are a couple of other things that you can do
that work considerably better and they've been
explored in this paper by Dhingra et al., um, from 2017.
Um, the first one is to say, well, um,
when you at test-time encounter new words, probably your unsupervised word,
pre-trained word embeddings have a much bigger vocabulary than your actual system does.
So anytime you come across a word that isn't in
your vocab but is in the pre-trained word embeddings,
just use, get the word vector of that word and start using it.
That'll be a much more useful thing to use.
And then there's a second possible tip that if you
see something that's still an unknown word,
rather than treating it as UNK,
you just assign it on the spot,
a random word vector.
And so this has the effect that each word does get a unique identity.
Which means if you see the same word in the question,
and a potential answer,
they will match together beautifully in an accurate way which you're
not getting with just UNK matching and those can be kind of useful ideas to try.
Okay, end digression. Okay, so up until now,
we just sort of had this representation of words,
we ran Word2vec and we got a word vector,
um, for each word.
Um, so, um, that, that was useful.
It's worked pretty well.
Um, but it had, um,
some big problems. So what were the big problems of doing that?
The problems when we,
of having a word vector in each word, yes.
A lot of words have like one spelling, but a whole bunch of meanings.
Right, so, a word can have- So, typically,
you have one string of letters which has a whole bunch of meanings.
So, words have a ton of senses.
Um, and yeah, so that's
the biggest and most obvious problem that we're
collapsing together all the meanings of words.
So, we talked about a bit where
one solution to that was you could distinguish
word senses and to have different word vectors for them.
Um, and I then said something about also you could think of
this word vector as a sort of a mixture of them and maybe your model could separate it.
But it seems like we might want to take that more seriously.
And one way, um,
that we could take that more seriously is we could start to say, well,
really, you know, traditional lists of word senses are themselves a crude approximation.
What we actually want to know is the sense of the word inside a particular context of use.
And sort of what I mean by that is, you know,
we distinguish different senses of a word, right?
Say for the word star there's the astronomical sense and
there's the Hollywood sense and they're clearly different.
But you know, if we then go to this what I'm calling the Hollywood sense,
I could then say, well, wait a minute.
There are movie stars and there are rock stars,
and there, uh, are R&B stars,
and there are country stars.
Now, all of those different senses, um,
in certain contexts, though, one or other of them would be evoked.
And so, you know,
it's very hard if you're trying to actually enumerate
senses of a word as to which ones count as different or the same.
So, it's really you sort of wanna know what a word means in a context.
There's a second limitation of these word vectors which is,
we haven't really talked about and is less obvious,
but it's also something that we might want to fix, and at least one of
the models we discussed today takes some aim at that,
and that is, we just sort of have one vector for a word.
But there are sort of different dimensions of a word.
So, words can have different meanings,
some sort of real semantics or words can have
different syntactic behavior like different parts of speech or grammatical behavior.
So, in some sense, arrive and arrival,
their semantics are almost the same,
but they're different parts of speech.
One is a, um, a verb and one is a noun,
so they can kind of appear in quite different places.
And you know, you'd wanna do different things with them in a dependency parser.
And there are even other dimensions.
So, words also have register and connotation differences.
So, you can probably think of lots of different words for a bathroom,
and a lot of those words all means semantically the same,
but have rather different registers and
connotations as to when they're appropriate to use.
And so, we might want to distinguish words on that basis as well.
And so these are the kinds of soluti- things we want to
solve with our new contextual word embeddings.
Um, so I've said up until now, you know,
oh, we just had these word vectors that we use,
words just had one vector.
Um, but if you actually think about it, maybe that's wrong.
I mean, maybe we never had a problem, or at any rate, we solved it six classes ago.
Because if you remember back, [NOISE] um,
to when we started talking about neural language models,
well, what did a neural language model do?
At the bottom, you fed into it the word vectors.
But then you ran across that one or more recurrent layers,
something like a LSTM layer,
and it was calculating these representations that sit above each word and,
you know, the role of those hidden states is a bit ambivalent.
They are used for prediction.
And they are used for next hidden state and output states and so on.
But in many ways you can think huh,
these representations are actually representations of a word in context.
And if you think about what happened with, uh,
the question answering systems,
that's exactly how they were used, right?
We ran LSTM's backwards and forwards,
over a question in the passage, and then we say,
okay those are a good representation of a word's meaning and context.
Let's start matching them with attention functions et cetera.
So, it sort of seemed like we'd already invented a way to have,
um, context-specific representations of words.
And effectively, you know,
the rest of the content of this lecture is sort of basically no more complex than that.
Um, that it took a while but sort of people woke up and started to notice, huh,
really when you're running any language model,
you generate a context-specific representation of words.
Maybe, if we just took those context-specific
representation of words, they'd be useful for doing other things with them.
And that's sort of, you know,
there are a few more details,
but that's really the summary of the entire of this lecture.
Um, so one of the first things to do that was a paper that Matt Peters wrote in 2017,
um, the year before last.
Um, and this was sort of a predecessor to the sort of modern, um,
versions of, um, these context-sensitive word embeddings.
So, um, together with co-authors,
he came up with a paper called TagLM,
but it essentially already had all the main ideas.
So, what, um, was wanted was okay.
We want to do better at tasks such as named-entity recognition.
And what we'd like to do is know about the meaning of a word in context.
Um, but you know, standardly if we're doing named-entity recognition,
we just train it on half a million words of supervised data.
And that's not much of a source of
information to be learning about the meaning of words and context.
So, why don't we adopt the semi-supervised approach and so that's what we do.
So, we start off with a ton of unlabeled data.
Um, and from that unlabeled data,
we can train a conventional word embedding model like Word2vec.
But we can also at the same time train a neural language model.
So, something like a bi-LSTM language model.
Okay. So, then for step two when we're using our supervised data,
um, actually, I guess that's step three.
Okay. Um, so for then when we want to learn our supervised part-of-speech tagger at the top,
what we're gonna do is say, well,
for the input words New what York is located,
we can not only use the word embedding which is context independent,
but we can use our trained recurrent language model and also run it over this import,
and then we'll generate hidden states in our bi-LSTM language model and we can also
feed those in as features into ou- our sequence tagging model,
and those features will let it work better.
Here's a second picture that runs this through in much greater detail.
So, so, we're assuming that we have trained, uh,
bi-LSTM language model, um,
on a lot of unsupervised data.
Then what we wanna do is we want to do named entity recognition for New York is located.
So, the first thing we do is say,
let's just run New York is located through our separately trained neural language model.
So, we run it through a forward language model.
We run it through a backward language model.
We get from that, um,
a hidden state representation,
um, for each word,
we concatenate the forward and backward ones,
and that's going to give a set, a concatenated language model embedding
which we'll use as features in our named entity recognizer.
So, then for the named entity recognizer itself that we're gonna
train supervised while we have the same sentence,
so we can both look up a Word2vec-style token embedding for it.
We can use what we learned about with character level CNNs and RNNs and we can build
a character level representation for it which we also
concatenate to have two representations.
So, we feed these representations into a bi-LSTM layer.
But then when we get the output of the, this bi-LSTM layer,
as well as this normal output,
we can concatenate with each output what was- what we get from our,
um, neural language model.
So, each of these things becomes a pair of states.
One that's spit up from the first bi-LSTM layer and
then it's concatenated with something from the neural language model.
And so that concatenated representation is then fed into a second layer of bi-LSTM.
And then from the output of that,
we do the usual kind of softmax classification
where we're then giving tags like beginning of location,
end of location, say New York is a location and then is, we'll get
another tag to say it's not a location. Does that makes sense?
Yeah so, um, so the central thing is
sort of having seen that these sort of representations that we get from Bi-LSTMs are useful.
We're just going to feed them into supervised models as we train them,
and the idea is that will give us better features of words.
Some kind of representation of their meaning and context,
which will allow us to learn better named entity recognizers or what it- whatever it is.
Maybe I should put this slide earlier,
but this slide was meant to remind you what a named entity recognizer is.
I hope you remember that,
something where are we going to find and label
entities for things like person, location, date, organization.
So anyway, doing this worked.
So, here's a little bit of a history.
So the most famous Named Entity Recognition dataset is this CoNLL 2003 dataset,
which actually exists in multiple languages.
But whenever people say CoNLL 2003 and don't mention a language,
they mean the English version of it.
That's the way the world works.
Um, okay so on this dataset- yeah.
So, it's sort of been around for whatever, 15 years roughly now.
So, in the- so it was originally a competition, right?
So, this is in 2003 was the original bake-off.
My group actually took place in that.
Took part in it. I think we got third or fourth place or something,
and our F1 score was 86.
The people who won were from IBM Research Labs,
and they got 88 almost 89.
But a difference between these two things is our system was
a single clean machine-learning model categorical,
whereas the IBM one was not only an ensemble
of four different machine learning models, plus gazetteers.
It also fit in the output of
two other old NER systems that IBM people were trained years ago on different data.
So it was- I guess it worked for them but,
it was a fairly complex system.
Here's another system from Stanford.
So this was our classic Stanford NER system that is widely used.
So, this was then using a conditional random field model which generally dominated
sort of the second half of the 2000s and the first half of the 2010s for doing NER,
and it was sort of, you know, a bit but not usually better than the 2003 system.
This system here was sort of the best ever built categorical CRF system.
But rather than only using the training data to build the model as this system did,
it threw in Wikipedia and other stuff to make it work better,
and that got you to about 90.8 F1.
So, essentially, once sort of BiLSTM style models started to be known and used in NLP.
That was when people were able to train, build training
just on the training data systems that worked a lot better.
Because essentially you're going from the same data from this system to that system.
So, you're getting about 4 percent gain on it,
because it's not- wasn't making use of Wikipedia and things like that;
and so this Ma and Hovy system is pretty well-known getting about 91.21.
Okay, but if we then go to this TagLM system, um,
that Matt Peters and Co have a system that
was sort of similar to the Ma and Hovy system that is a little bit worse.
But the point is that this BiLSTM uses sorry- using the neural language model,
is just a useful oomph giver which sort of takes the results up.
Yeah, not night and day but,
slightly over a percent and then gives them the best NER system that was then available.
So that sort of proved these sort of
contextual word representations really had some power and started to be useful,
and then there's a white space at the top because we'll get back to more of this later.
Um, there's some details on their language model.
Some of their details are that it's useful to have
a bidirectional language model, not unidirectional.
It's useful to have a big um,
language model to get much in the way of gains,
um and, you need to train this language model over much more data.
It doesn't work if you're just sort of training it over your supervised training data.
Another model that was around was CoVe,
but I think I'll skip that.
Okay. So, then the next year, um,
Matt Peters and a different set of colleagues
then came up with an improved system called ELMo,
and effectively this was the breakthrough system.
That this was sort of just the system that everybody
noticed and said "Wow these contextual word vectors are great.
Everyone should be using them,
not traditional word vectors." Yes?
I have a simple question, imagine re-training a system, what exactly
what measure [inaudible]
It's pre-trained because this piece over here;
a big neural language model is trained first,
and there's an important thing I forgot to say.
So, thank you for the question.
The main reason why it's- in some sense pre-trained,
is this was trained first.
But the main reason why people think of this as pre-training
is after you've trained this, it is frozen.
So, this is just something that you can run with parameters which will give
you a vector which is your contextual word representation each position,
and then that's just going to be used in this system.
So, when you're training this system,
there's no gradient flowing back into
this neural language model that's changing and updating it; it's just fixed.
And so that's sort of the sense when people are talking about pre-training.
It's sort of normally a model that you trained
somewhere else and that you're using to give features,
but isn't part of the model that you are now training. Yeah?
[inaudible]
Well, I guess that's, I wouldn't quite call it reconstruction.
Yeah, it's unsupervised in the sense that this is a language model,
you're training it to predict the next word.
So here are words one to k. What is the k plus oneth word during a cross entropy loss,
and repeat over for each position.
[NOISE] Yes, so I mean,
having gone through TagLM in some detail, I mean,
in some sense, the difference between TagLM and ELMo is kind of small,
it's sort of in the details.
So I mean, to a first approximation,
they're doing exactly the same again,
but a little bit better.
Um, so, um, I sort of hope it made sense the last time,
I mean, what are the things that are different?
Um, they do the bidirectional language model a bit differently,
and actually one of their concerns was to try and come up with
a compact language model that would be easy for people to use,
um, in other tasks even if they don't have the beefiest computer hardware in the world.
And so they decided to dispense with having
word representations altogether and just use, um,
character CNNs to build word representations,
because that lessens the number of parameters you have to store,
the big matrices you have to, um, use.
Um, they expanded the hidden dimension to 4,096,
but then they project it down to
512 dimensions with a sort of feed-forward projection layer,
and that's a fairly common technique to again reduce
the parameterization of the model so that you have a lot of
parameters going in their current direction but you
need much smaller matrices for including,
um, the input at the next level.
Um, between the layers,
they now use a residual connection and they do a bit of parameter tying.
So it's sort of all in the little details there.
Um, but there's another interesting thing
that they did which was an important innovation of ELMo,
so we should get this bit.
So in TagLM,
what was fed from the pre-trained LM into
the main model was just the top level of the neural language model stack,
and that was completely standard de rigueur in those days,
that you might have had three layers of
neural language model that you regard at the top-level as your sort
of one that's really captured the meaning of
the sentence and the lower layers for processing that led up to it.
Um, and they had the idea that maybe
it would be useful to actually use all layers of the,
biLSTM of the neural language models.
So maybe not just the top layer but all layers would be kind of useful.
So, um, there are these kind of complex equations,
uh, but essentially the point of it over here is,
we going- for a particular position,
word seven in the language model,
we're going to take the hidden state at each level of our,
our neural language model stack,
we're going to give- learn a weight for that level,
we go in to sort of sum them,
so this is sort of a weighted average of the hidden layers at each position,
and that will be used as our basic representation.
Um, and so, they found that that gave quite a bit
of extra usefulness for- and different tasks could prefer different layers.
There's one other bit here which is,
they learn a global scaling factor Gamma for a particular task.
And this allows them to control that for some tasks, the, um,
contextual word embeddings might be really
useful and for other tasks they might not be so useful,
so you're just sort of learning a specific,
um, usefulness for the entire task.
Okay. So, um, that's the sort of new version of language model.
But this, this is allowing this idea of well,
maybe there's sort of more syntactic meanings
of a word and more semantic meanings of a word,
possibly those could be represented at different layers of
your neural language model and then for
different tasks you can differentially weight them.
Um, so that's the basic model.
So you run your biLSTM before to g et representations of each word.
And then the generic ELMo recipe was,
well, with that frozen language model,
you want to feed it into some supervised model depending on what the task was,
and they sort of say in the paper, well,
how you do this maybe depends on the task.
You might want to kind of concatenate it to the intermediate layer,
just as the TagLM did,
that might be fine.
But you know it might also be useful to make use of
these ELMo representations when producing outputs,
so if you're doing something like a
generation system or you might just sort of feed in the ELMo representation again,
be- before you sort of do the softmax to find the output,
they sort of left it flexible as to how it was used,
but the general picture,
you know, was kinda like we saw before.
Indeed I'm reusing the same picture that you've calculated
an ELMo representation for each position as a weighted average,
and then you're sort of concatenating that to the hidden state of
your supervised system and generating your output.
And anyway, um, one way or another,
um, they were able to do this, uh,
and that with the little improvements that gave them about an extra
0.3 percent in Named Entity Recognition.
Um, now, that sort of sounds like not very much.
And you might conclude from this why the excitement [LAUGHTER] and,
you know, in some sense, um,
that's right because sort of to the extent that there was an interesting idea here really
that come up with it for the TagLM paper which gave a much better gain.
But, you know, why everyone got really excited was that in the ELMo paper,
they then showed this isn't something that you can
do one-off to improve a Named Entity Recognizer,
you can take these ELMo representations and use them for pretty much any NLP task,
and they can be very useful and give good gains.
And so, essentially why people got excited was because of the data that's in this table.
So here we're taking a whole bunch of very different tasks,
so there's SQuAD question-answering, uh,
there's natural language inference,
there's semantic role labeling,
there's co-reference, the Named Entity Recognition, doing sentiment analysis,
so a wide range of different NLP tasks,
and they have a previous state of the art system.
They produced their own baseline um, which is,
you know, commonly sort of similar to the previous state of the art,
but usually actually a bit worse than
the current state of the art because it's
whatever simpler cleaner system that they came up with,
but then they could say in each case,
oh, just take this system and add
ELMo vectors into the hidden representations in the middle,
and have those help you predict.
And in general, in all cases,
that's giving you about a three percent or so gain absolute
which was then producing this huge performance increase,
which in all cases was moving the performance well above the previous,
um, state of the art system.
So you know, this sort of then made it seem like magic pixie dust,
because, you know, in the stakes of NLP conference land, you know,
a lot of people use to try and to come up
with a paper for the next year that's one percent better
on one task and writing it up and that's
their big breakthrough for the year to get their new paper out.
And the idea that there's just well this set of
this way of creating context sensitive, um,
word representations and you just use them in any task,
and they'll give you around three percent and take you past the state of the art,
this seemed like it was really great stuff.
And so people got very excited about this and that won
the Best Paper Award at the NAACL 2018 conference.
Ah, and then, a- as I sort of vaguely mentioned,
um, so the model that they actually used wasn't a deep stack,
there were actually only two layers of biLSTMs,
but they do show this interesting result that the lower level better captures
low-level syntax word properties
and its most useful things like part-of-speech tagging, syntactic
dependencies, NER, where the top layer of
their language model is better for
higher level semantics that is more useful for things like sentiments,
semantic role labeling and question answering.
Um, so that seemed interesting,
though it'll actually be interesting to see how that panned
out more if you had sort of more layers to play with.
Okay. ELMo, done.
Um, so I'm moving right ahead.
Um, here's something else that I just thought I should mention a little bit about,
another piece of work that came out around the same time,
a few months later maybe or maybe not,
came out around the same time, uh,
in, in 2018, was this work on
Universal Language Model Fine-tuning for text classification,
um, or ULMfit, by Howard and Ruder.
And essentially this had the same general idea of saying, Well,
what we want to do is transfer learning where we could learn a big language model, um.
A big language model,
and then for our target task which might be named entity recognition.
But here's text classification,
we can transfer this language model information and help us to do better with the task.
And so, they proposed an architecture to do that.
And so, their architecture was,
you have a big unsupervised corpus from which you train a neural language model.
They used the deeper neural language model with three hidden layers.
Um, you then fine tune
your neural language model on the actual domain that you're interested in working in.
So, this was sort of an extra stage that they did.
And then finally, um,
you now introduce your classification objectives.
So, what they're going to be doing is making text classifiers.
So, we're now wanting to,
take this model and turn it from a language model into a text classifier.
Um, but there's something that they did differently, um,
which is in some sense,
foreshadows the later work in transformers.
So, rather than just feeding features from this into a completely different network,
they keep using the same network but they introduce a different objective at the top.
So, one thing you could do with this network is use
it to predict the next word as a language model.
And so at this point,
they freeze the parameters of that softmax at the top,
that's why it's shown in black.
Um, but instead, they could stick on
a different prediction unit where it's predicting stuff for a particular task.
So, it might be predicting
positive or negative sentiment in a text classification task or something like that.
So, in their model,
they're sort of reusing the same network but sticking on the top of that,
a different layer, to do the new classification task.
Um, they were also interested in something small,
the sort of one GPU model of research, um,
the paper has a lot of detail, the sort of tricks
and care and feeding of your neural models to maximize performance.
If you're interested in that, you could sort of look up some of the details about that.
Um, but what they were able to show again,
was making use of this language model pre-training was
a very effective way to improve performance,
this time for text classification.
So, these are text classification datasets,
IMDb is for sentiment,
um, TREC is for topical text classification, and again,
there are preceding systems that other people have developed and they
are showing that by making use of this language model pre-training,
they're able to significantly improve on the state of the art of these error rates,
so that low is good.
They also showed another interesting result which is kind of,
um, what you would expect or hope from doing this kind of transfer learning,
that what they were able to show is,
if you can train this neural language model on a big amount of data,
that that means you will then be able to do well on
your supervised task even when trained on pretty little data.
Um, so, here this is error rate,
so low is good.
So, what the- and here's the number of
training examples which has being done on a log scale.
And so the blue line is if you're just training
a text classifier from scratch on supervised data.
So, you need a lot of data to start to do pretty well.
Um, but if you're making use of this transfer learning, um,
from a pre-trained language model,
you can get to that you're sort of doing pretty
well with way less, um, training examples.
Essentially, an order of magnitude,
less training examples will give you the same amount of performance.
And the difference between these two lines corresponds to the extra,
um, phase that they had in the middle of theirs, um, which is,
whether you're doing this sort of extra fine tuning on your target domain,
um, it's part of your process and they found that to be pretty helpful.
Okay. So, that, um, is another precursor.
Um, and so, one big part of what has happened since then,
is effectively people said this is a good idea, uh,
maybe it'll become a really really good idea if we just make things way bigger.
Um, so, ULMfit, um,
was something that you could train in one GPU day,
sounds appealing for CS224N final projects,
remember that, um, and but well,
then the people at OpenAI decided, well,
we could build a pretrain language model and train it on
a much larger amount of data on a much larger amount of compute,
and use about 242 GPU days and that will get a lot better, and it did.
Um, and then the people at Google said,
well we could train a model, um,
in to 256 TPU days,
which means maybe about double the amount of computation.
It's hard to figure out exactly,
and that might be able to do exciting things,
and that was the BERT model, and it did.
Um, and then if you're following along these things, um,
just last week, um,
the OpenAI people said,
well we can go much bigger again and we can train a model, um,
for approximately 2,000 TPU version three days.
Um, and it will be able to,
um, do much bigger again,
a bit much better again,
um, and so, this is this GP2,
GPT-2 language model, um,
which OpenAI released last week.
Um, and they're, they're actually very impressive results, um,
when they're showing that if you're sort of building a really,
really huge language model over a very large amount of data.
And then you say language model go off and generate some text,
on this particular topic,
that it can actually just do a great job of producing text.
So, the way this was being do- done,
was a humanist writing a couple of sentences;
in a shocking finding,
scientists discovered a herd of unicorns,
living in remote previously unexplored valley in the Andes Mountains.
Um, and so, we then,
using our neural language model and chugging through that,
so that gives us context,
and then say generate more text,
and it starts to generate the scientist
named the population after their distinctive horn,
Ovid's Unicorn, these four-horned,
silver-white Uni four corns were previously unknown to science.
Um, it produces remarkably,
um, good text or at least in the,
in the hand-picked examples [LAUGHTER] that they showed in the tech news,
um, it produces extremely good text.
Um, yeah so, I think one should be a little bit cautious about, um,
that and sort of some of its random outputs actually
aren't nearly as good but nevertheless you know,
I think is is actually dramatic
how good language models are becoming once you are training
them on long contexts as we can do with modern models on vast amounts of data, um-.
So then, um, the OpenAI people decided
this language model was so good that they weren't gonna release it to the world, um,
which then got transformed into headlines of,
Elon Musk's OpenAI builds artificial intelligence so powerful,
it must be kept locked up for the good of humanity.
[LAUGHTER] Um, with the suitable pictures that always turn off at
these moments down the bottom of the screen, um, and,
um, yeah I guess that was the leading even Elon Musk to be wanting to clarify and say
that it's not actually really that he's directing what's happening at OpenAI anymore.
Um, anyway, moving right along.
Um, so, part of the story here is
just a scaling thing that these things have been getting bigger and bigger,
um, but the other part of the story is that all three of
these are then systems that use the transformer architecture.
And transformer architectures have not only being very powerful,
but technically had allowed scaling to much bigger sizes.
So to understand some of the rest of these, um,
we should learn more about transformers.
And so, I'm sort of gonna do that, um,
but I mean, um, in mix of orders,
um, our invited speaker coming Thursday uh, is, um,
one of the authors of the transformer paper,
and he's gonna talk about transformers.
So I think what I'm gonna do is, um,
say a little bit about transformers quickly,
but not really dwell on all the details, um,
but hope that it's a bit of an introduction,
and you can find out more on Thursday about the details and
then talk some more about the BERT model before finishing.
So the motivation for transformers is essentially
we want things to go faster so we can build bigger models,
and the problem as we mentioned for these, um,
LSTM or in general any of the recurrent models is the fact that they're recurrent.
You have to generate sort of one to n status time chugging through,
and that means you just can't do the same kind of parallel computation, um,
that GPUs love that you can do in things like convolutional neural networks.
But, you know, on the other hand,
we discovered that even though, um,
these gated recurrent units like LSTMs and GRUs are great,
that to get really great performance out of these recurrent models,
we found that we wanted to- we had a problem within these long sequence lengths,
and we can improve things by adding attention mechanisms.
And so that led to the idea of- well,
since attention works so great,
maybe we can just use attention,
and we can actually get rid of the recurrent part of the model [NOISE] altogether.
And so that actually then leads to the idea of these transformer architectures,
and the original paper on this is actually called attention is all you need,
which reflects this idea of we're gonna keep the attention part,
and we're getting- going to get rid of the, um,
recurrent part, and we'll be able to build a great model.
So in the initial work,
what they're doing is machine translation kind of like
the Neural Machine Translation with attention we described,
but what they're wanting to do is build
a complex encoder and a complex decoder that works non-recurrently,
and, um, nevertheless is able to translate sentences
well by making use of lots of attention distributions.
And so, I wanted to say a little bit more quickly about that,
and hopefully we'll get more of this on Thursday.
Um, first as a- as a recommended resource,
if you wanna look at, um,
home and learn more about, um,
the transformer architecture, there's this really great, um,
bit of work by Sasha Rush called The Annotated Transformer that goes through
the entire transformer paper accompanied by PyTorch code in a Jupyter Notebook,
and so that can actually be a really useful thing,
but I'll go through a little bit of the basics now of how we do things.
So the basic idea, um,
is that they're going to use attention everywhere to calculate things.
And, um, we talked before about the different kinds of
attention of the sort of multiplicative by linear attention and the little,
um, feed-forward network additive attention.
They kind of go for the simplest kind of attention,
where the attention is just dot-products between two things.
Um, but they sort of do the more comp- for various purposes,
they do the more complicated version of dot-product between two things where they have,
um, when the- the things that they're looking up are
assumed to be key-value pairs, keys and values,
and so you're calculating the similarity as a dot-product between a query and the key,
and then based on that,
you're going to be using the vector for the corresponding value.
So our equation here for what we're calculating is where you are
looking using the softmax over query, um,
key similarities and using that to give
the weightings as an attention based weighting over the corresponding values.
Um, so that's the basic attention model.
Um, so that add- saying it that way, um,
adds a little bit of complexity,
but sort of for the simplest part for their encoder.
Actually, all of the query keys and values are exactly the same.
They are the words, um,
that they're using as their source language, um, things.
So, it sort of adds some complexity that isn't really there.
Um, okay. Um, I'll skip that.
Um, so, there are a couple of other things that they do.
One thing that they note is that, um,
the- the values you get from, um, QTK, um,
very, in variances the dimension gets large
so that they sort of do some normalization by the size of the hidden state dimension,
but I'll leave that out as well for details, right.
So in the encoder, um,
everything is just our word vectors,
there are the queries, the keys, and the values.
Um, and we're gonna use attention everywhere in the system.
Oops. Okay. So the second new idea is, well,
attention is great but maybe it's bad if you only have one attention distribution,
because you're gonna only attend to things one way.
Maybe for various users it would be great
if you could attend from one position to various things.
So, if you're thinking about syntax and what we did with dependency parsers.
If you're a word, you might want to attend to your headword,
but you might also wanna attend- attend to your dependent words.
And if you happen to be a pronoun,
you might want to attend to what the pronoun refers to you.
You might want to have lots of attention.
So they introduced this idea of multi-head attention.
And so what you're doing with multi-head attention is you have,
um, your hidden states,
um, in your system,
and you map them via projection layers, um,
which are just multiplications by different W matrices as
linear projections into sort of different lower dimensional spaces,
and then you use each of those to calculate dot-product attention,
and so you can attend to different things at the same time.
And this multi-head attention was one of
the very successful ideas of transformers that made them a more powerful architecture.
Okay. Um, so, then for our complete transformer block,
it's sort of then starting to build complex architectures like we sort of started seeing,
um, the other week.
Um, so- okay.
Yeah. So, starting,
um, from our word vectors,
we're kind of going to do attention to multiple different things,
um, and we're simultaneously gonna have
a residual connection that short-circuits around them.
Um, we're then going to sort of sum the two of these,
and then they're going to do a normalization at that point.
Um, I talked previously about batch normalization,
they don't do batch normalization,
they do another variant which is layer normalization,
which is a different way of doing normalization,
but I'll skip that for now.
And then they sort of for one transformer block,
you then go after the multi-head attention,
you put things through a feed-forward layer which also has a residual connection,
you sum the output of those,
and you then again do another, um, layer normalization.
So this is the basic transformer block that they're gonna use everywhere.
And to make their complete architectures,
they're then gonna sort of start stacking
these transformer blocks to produce a very deep network.
And in some sense,
what has been found is that transformers performed very well.
But, you know, there's no free lunch,
um, you kind of can't.
You're- now, no longer getting
recurrent information actually being carried along a sequence.
You've got a word at some position which can be casting attention,
uh, on other words.
So if you'd like to have information carried along in a chain,
you've sort of first of all gotta walk the first step of the chain,
and then you need to have another layer
vertically which can walk the next step of the chain,
and then you need to have another layer vertically that walks the next step of the chain.
So, you're getting rid of the recurrence along the sequence,
but you're substituting some depth to allow things to walk along multiple hops.
But nevertheless, that's highly advantageous in GPU architectures
because it allows you to use parallelization to calculate everything at each,
um, depth at the same time. Um.
Maybe I'll go light on explaining this as well.
Um, so they use byte-pair encodings.
But if you do nothing else,
you just have words fed in this word vectors and you have
no idea whether you're at the beginning of the sentence or at the end of the sentence.
Though, they have a message of- method of doing positional encoding which gives
you some ideas to pro- position your word has in the sentence.
Okay. Um, so that's sort of the, um, encoder system.
So from the words,
they have an initial word embedding,
you add in their positional encoding,
you go into one of these transformer blocks,
and you then repeat it n times.
So you'll have a stack of these transformer blocks.
So you're multiple times doing, um,
multi-head attention to other parts of the sentence, calculating values,
feeding forward a value,
putting it through a fully-connected layer,
and then you just sort of repeat, do attention to different places in the sentence.
Get all your information,
put it through a fully connected layer,
and go up, um, proceeding up deeply.
And and that sounds a little mysterious,
but it turns out to work just great.
And the way to think about,
I think is that at each stage,
you can look with your multi-headed attention and various other places in the sentence,
accumulate information, push it up to the next layer.
And if you do that sort of half a dozen times,
you can be starting to progressively push information along
the sequence in either direction to calculate values that are of interest.
Um, and the interesting thing is that these models turn out to work
really well at sort of learning to attend the interesting things in linguistic structure.
Um, so these are just sort of suggestive diagrams,
but this is looking at layer five of the transformer stack and
seeing what words are being attended to by different attention heads.
So these different colors correspond to different attention heads.
And so the sentence is,
um, it is, "In this spirit,
that a majority of American governments have passed new laws since
2009 making the registration or voting process more difficult."
And so what we see is sort of most of the attention heads,
uh, looking from making to making more difficult and that seems to be useful.
One of the attention heads seems to be looking at the word itself might be okay.
Um, then the other ones are sort of looking a bit at laws and at 2009.
So it's sort of picking out the arguments, um,
and modifiers and making in a syntax kind of like way.
Um, interestingly, for pronouns,
attention heads appear to learn to be able to look back to reference.
So the law will never be perfect,
but its application should be just that one attention head it for its,
is looking at what its is modifying in the application.
But another attention head,
the its is looking strongly at what its refers back to as the law.
So that seems kind of cool.
Um, yeah.
Um, okay.
And so then, for the rest of the model, um,
there's then some more complexity for how to use
the transformers decoder to give you a full neural machine translation system.
But I think maybe I will skip that and go
on and say a bit about BERT in my remaining minutes.
Okay. So, um, the latest and greatest contextual
word representations to help you flow your tasks have been these BERT vectors,
where BERT is Bidirectional Encoder Representations from Transformers.
And so essentially, it's using the encoder from a transformer network.
Uh, this deep multi-headed attention stack to calculate, um,
a representation of a sentence and saying,
"That's a great all-purpose representation of a sentence that you can use for tasks.
Be it named entity recognition or SQuAD question answering."
And so there's actually an interesting new idea that these people had.
And that well, their idea was well standard language models are
unidirectional and that's useful
because it gives you a probability distribution of a language model.
But it's bad because you'd like to be able to do
prediction from both sides to understand word meaning and context.
There's a second choice, um,
which is you can kind of do bidirectional models when you incorporate,
um, information in both ways.
But that sort of has problems as well,
because then you get crosstalk.
Um, and so if you run a BiLSTM,
and then you merge the representations by
concatenation and then feed them into the next layer.
When you're running the next layer,
the forward LSTM will have already gotten
information about the future from the first layer.
Um, so it sort of, um,
ends up with words that have already seen the future themselves.
So you have this sort of complex non-generative model.
Um, so somehow, they wanted to do things a bit differently,
so they can have bidirectional context without words being able to see themselves.
And the idea that they came up with is well,
we're gonna train things with a transformer encoder.
But what we're gonna do is mask out some of the words in the sentence,
like, maybe we'll mask here store and gallon.
And then, so our language mod- our language modelling like
objective will no longer be
a true language model that's sort of generating a probability of a sentence,
um, which is standardly done by working from left to right,
but it will instead be a Mad Libs style fill in the blank objective.
So you'll see this context,
which will be literally,
"The man went to the mask to buy a mask of milk."
And your, what's your training objective is to say,
try and predict what this word is,
which you can do with a cross entropy loss to the extent that you don't guess store.
And then, it will be trying to guess what this word is and you want to let guess gallon.
So you're training a model,
um, to fill in these blanks.
Um, and the rate at which they blank words is essentially one word in seven,
and they discuss how this is a trade-off.
Because if you blank too few words,
it gets very expensive to train.
And if you blank many words,
well you've blanked out most of the context of a word,
and that means it's not very useful for training,
and they found about sort of one in seven seemed to work pretty well for them.
But what they want to argue is, um,
that for the OpenAI's GPT,
which is also a transformer model.
It's a sort of a classic language model working from
left to right and so you only get left context.
Um, for the BERT language model,
sorry, the ELMo language model that's shown up at the top.
Um, well, they're running a left to right language model and they're running,
um, right to left language models.
So in some sense, um,
they have context from both sides.
But these two language models are trained completely independently
and then you're just sort of concatenating their representations, um, together.
So there's no sense in which we're actually kind of having a model that's jointly
using context from both sides at the time though that the pre-trained,
um, contextual word representations are built.
So their hope is using inside a transformer model
this trick of blanking out words,
and predicting it using the entire context will allow them to use two-sided context,
and be much more effective.
And that's what they seem to show, um.
There's one other complication and,
I mean, I'll show later.
Um, this last complication is a bit useful,
but it's sort of not really essential to their main idea,
was that they thought,
one of the, one of the goals in their head was clearly to be able to
have this be useful for things like question answering,
um, tasks, or, um,
natural language inference tasks,
and their relationships between, um, two sentences.
So, their idea was, well,
one good objective is this fill in the blank word objective which is,
sort of, like language modeling objective.
But they thought it would be useful to have a second objective
where you're predicting relationships between sentences.
So, they secondly have a loss function which is, um,
let's have two sentences where
the sentences might be two successive sentences in the text,
or a sentence followed by a random sentence from somewhere else.
And we want to train the system to predict when you've,
seeing an- a correct next sentence versus a random sentence.
And so you're also training a loss based on this next sentence prediction task.
And so it'll be something like: The man went to the store.
He bought a gallon of milk.
You're meant to predict true is the next sentence,
um: The man went to the store.
Penguins are flightless.
You're meant to say false.
This isn't the next sentence.
And so they're simultaneously also,
um, training with this representation.
So, what they end up looks, looks like this.
Um, so, they have,
um, for the input,
they'll have a pair of sentences.
My dog is cute.
Um, separator.
He likes playing.
Um, the words are represented as word pieces like we talked about last week.
Um, so there's a token embedding for each word piece.
Um, then there's a positional embedding for
each word piece which is gonna be summed with the token embedding.
And then finally, there's a segment embedding for each word piece which is simply
whether it comes from the first sentence or
the second sentence before or after the separator.
So, you're summing those three things together to get the token representations.
And then you're going to use those in a transformer model
where you will have losses to the extent that you can't predict the masked words.
And then your binary prediction function as to whether there's
a correct next sentence or not which is the training architecture.
Okay. So, it's a transformer as before,
it's trained on Wikipedia plus the BookCorpus.
And they built two models.
Um, the Base-BERT model was a twelve layer transformer.
And so this corresponded to what the previous transformer paper had used, right?
Those two layer transformer blocks repeated six times gave you 12 layers with 768 hidden,
um, dimension hidden states and 12 heads for the multi-head attention.
And then they went bigger,
um, and trained BERT-Large which is,
sort of, double the number of layers,
bigger hidden states, even more attention heads.
Um, and training these on,
um, pods of TPUs.
Um, so, first of all, you're training, um,
on this basis for masked words and,
um, next sentence or not.
Um, so then what they wanted to say was this pre-trained model,
um, evaluated on these losses and masked language model and next sentence prediction.
Um, we could then take this model,
fr- freeze most of its what weak. No, sorry, that's wrong.
We could take this model, um,
pre-trained and it would be incredibly useful for various different tasks.
We could use it for named entity recognition,
question answering, natural language inference et cetera.
And the way we're going to do it, is kind of,
doing the same thing as the ULMFit model did.
We're not just going to say here's our,
here's a contextual word representation like ELMo did.
Instead, what we're gonna say is just keep on using this,
keep on using this um,
transformer network that we trained as a, sort of,
language model, but fine tune it for a particular task.
So, you're now going to run this transformer
calculating representations for a particular task.
And what we're going to change is we're going to remove the very top-level prediction.
The bits that predict the mass language model and next sentence prediction.
And we're going to substitute on it,
on top, um, a final prediction layer that's appropriate for the task.
So, if our task is SQuAD question answering,
our final prediction layer will be predicting start of span and end of span,
kind of, like when we saw DrQA a couple of weeks ago.
If what we're doing is the NER task,
our final prediction layer will be predicting
the net- named entity recognition class of each token just like a standard NER system.
Okay, um, and so they built this system and tested it on a whole bunch of data sets.
Um, one of the main things they tested on was
this GLUE data set which has a whole bunch of tasks.
A lot of the tasks, they're,
uh, natural language inference tasks.
And I've kept saying that phrase all of this lecture but I haven't really defined it.
So, with a natural language inference you're given two sentences
like: Hills and mountains are especially sanctified in Jainism.
And then you can write a hypothesis on: Jainism hates nature.
And what you're meant to say is,
whether the hypothesis, um,
follows from the premise,
contradicts the premise, or has no relation to the premise.
So, that's a three-way classification.
And so here it contradicts the premise.
Um, there are various other tasks such as this linguistic acceptability task.
Um, but if we look at these, um, GLUE tasks.
Um, these are showing the Pre-OpenAI State Of The Art.
How well, um, ELMo works.
How well OpenAI GPT works,
and then how well do small and large BERT models work.
And effectively, what you're finding is,
um, that the OpenAI GPT was so,
you know, pretty good.
It showed actually good advances on most of these tasks.
For many, but not all of them that broke the previous state of the art,
showing the power of these contextual language models.
But the bidirectional form of BERT's prediction just seemed much better again.
So, going from this line to this line you're getting depending on
the task about two percent better performance.
And so the BERT people actually did their experiments carefully.
So, these models are pretty comparable in terms of size,
but the bidirectional context seems to really help.
And then what they found was,
well, by going to just a bigger model,
again, you could get another big lift in performance.
And so you're getting for many of the tasks about
another two percent lift in performance going into the bigger model.
So, this really produced super-strong results.
And in general, um, people have found,
um, that BERT continues to give super strong results.
So, if I return back to my ConLL NER task,
we had ELMo giving you 92.2,
um, and you, sort of,
continue to get gains.
So, BERT Base gets you to 92.4 and BERT Large takes you to 92.8.
Though in, um, truth in, truth in description,
there is now a system of beats BERT Large on NER which is actually a character-level,
um, transformer language model from Flair.
Um, but, you know,
this continued over to a lot of other things.
So, on SQuAD 1.1, um,
BERT immediately just outperformed
everything else that people have been working on for SQuAD for ages.
In particular, what was especially dramatic, um,
was the sing- a single BERT model, um,
beat everything else that had been done previously on SQuAD version 1.1,
even though they could also show that an
ensemble of BERT models could give further good, um, performance gains.
Um, and as I've mentioned before,
essentially if you look at the SQuAD 2.0, um,
leaderboard, all of the top ranked systems,
um, are using BERT one place or another.
Um, and so that,
sort of, led into this,
sort of, new world order, um, that, okay,
it seems like the state of NLP now is to,
if you want to have the best performance,
you want to be using
these deep pre-trained transformer stacks to get the best performance.
And so this is, sort of, making,
um, NLP more like vision.
Because really vision for five years has had
these deep pre-trained neural network stacks, um, like ResNets.
Where for most vision tasks what you do is you take a pre-trained ResNet,
and then you fine tune a layer at the top to
do some classification tasks you're interested in.
And this is, sort of, now, um,
starting to be what's happening in NLP as well.
That you can do the same thing by downloading
your pre-trained BERT and fine tuning it to do some particular performance task.
Okay, um, that's it for today and more on
transformers on Thursday [NOISE].
 Okay. So I'm delighted to introduce,
um, our first lot of invited speakers.
And so we're gonna have two invited speakers, um, today.
So starting off, um,
we go and have Ashish Vaswani who's gonna be
talking about self attention for generative models and in particular,
um, we'll introduce some of the work on
transformers that he is well-known for along with his colleagues.
Um and then as a sort of, um,
a special edition then we're also going to have
Anna Huang talking about some applications of this work.
There are actually at least a couple of people in the class who are
actually interested in music applications.
So this will be your one chance in the course to see music applications of deep learning.
Okay, um, so I'll hand it over to Ashish.
Thanks, Chris and, uh, thanks, Evie.
Uh, Anna is actually here to make the class less dull.
So [LAUGHTER] she's the highlight on this one.
So uh, so, uh, hi everyone.
Um, um, uh excited to be here.
This is a very large class.
Uh, first invited speaker,
no pressure, so hopefully this will all go well.
Uh, so yes, so the talk is going to be about, uh, self attention.
Um, and so the purpose is,
is not going to be just to talk about a particular model, but, as,
as, as, as empiricists and, and,
like, well, I'm an empiricist and I
consume machine learning to apply it to various tasks.
And, and, and, well, starting point always is to ask this question, you know,
what are the- what's the structure in
my dataset or what are the symmetries in my dataset,
and is there a model that exists that that's a very good- that,
that has the inductive biases to model these properties that exist in my dataset.
So hopefully, over the course of this, uh,
this, this lecture Anna and I will convince you that, uh,
self attention indeed does have some- has
the ability models and inductive biases that
potentially could be useful for the problems that you care about.
Um, so, um, this talk is going to be our learning representations primarily of,
uh, variable length data where we have images but,
uh, most of it is going to be variable length data.
And, uh, and, and,
and all of us care about this problem because we- in
deep learning, and deep learning is all about representation learning.
And if- and building the right tools for learning representations as,
as, as, as sort of- is an important factor in,
in achieving empirical success.
Um, now, uh, the models of choice,
the primary workhorse for
perhaps even now and or up to this point had been recurrent neural networks.
Um, um, how, how many people here are familiar with RNNs?
[LAUGHTER] Okay.
So definitely up to this point,
the primary workhorse have been recurrent neural networks,
and some of the more, uh, some, uh,
some gated variants that explicitly add multiplicative interactions like LSTMs,
they also, they also have mechanisms that allow for better gradient transfer.
And some recent variants like gated, uh,
recurrent units that are simplification,
they're kind of the- they're- they dominate this, this recurrent landscape.
Um, and typically how did recurrent neural networks, uh,
learn or, um, produce representations?
They consume a string or a sentence, um,
even an image, imagine, you know,
in a particular- in sequentially and, uh, at each,
at each, uh, position,
at each timestep they produce, they produce a,
a continuous representation that's
summarization of, of everything that they've actually crunched through.
Um, now, so in, in, in the,
in the realm of large data, uh,
par- having parallel models is,
is quite, is quite beneficial.
In fact, I was actually reading Oliver Selfridge.
Uh, he was a,
he was a professor at MIT and, uh, he had this,
uh, sorry, he wrote the precursor to deep nets its it's called Pandemoniums.
I would recommend everybody to read it.
And he has this fascinating note that, you know,
if you give me more parallel computation,
I'll just add more data and make it slower.
So you can consume more data.
Um, and, and recurrence, uh, recurrence sort of just by construction, um,
limits parallelization because you have to,
you have to wait until- your wait un-
for a particular time point to produce a representation.
Um, but if there's any questions,
please raise your hands, I'll
hopefully look around and, and,
uh, be able to attend to your question.
Um, and again, and, and now because we're actually producing these representations,
we're sort of summarizing,
you know, if you want to pass information,
if you want to pass co-reference information,
then we kind of have to shove all of this inside
this fixed size vector, so it could potentially be difficult to model.
And, uh, while they have been successful in language, uh,
explicit they don't have- the architecture
doesn't have a very clear explicit way to model hierarchy which is,
which is something that's very important in language.
Um, now, um, so they have been devin- it has been excellent work of,
a precursor to self attention that actually surmounted some of these difficulties.
And what were these difficulties basically is a convolutional sequence models
where you have these limited receptive field convolutions that,
again, consumed the sentence now not,
not sequentially but in depth.
And they produce representations for every-
they produce representations of your variable length sequences.
Um, and, uh, they're trivial to
parallelize because you can apply these convolutions simultaneously at every position.
Each layer is trivial to parallelize.
Uh, the, the, the serial dependencies are only in the number of layers.
Um, you can get, uh,
you can- you can get
these local dependencies efficiently because that a single application of
a convolution can consume all the information inside its local receptive field.
Um, now if you want to have
these really long distance interactions while you
don't have to pass through a linear number of steps,
you still because these,
because these receptive fields are local you might need something like linear
and depth or logarithmic if you're doing something like dilated convolutions.
So there's still need- the number of layers that are needed are
still a function of the length of the of, of your string.
Uh, but they're a great development and they
actually pushed a lot of research like WaveRNN, for example,
is a classic sort of success story of
convolutio- convolutional sequence models even by net.
Um, now, so far attention has been like one of the most important components,
the sort of content-based,
you know, memory retrieval mechanism.
And it's content-based because you have your decoder that attends to all this content,
that's your encoder and then just sort of decides what to wha- what,
what information to absorb based on how similar
this content is to every position in the memory.
So this has been a very critical mechanism in,
uh, in neural machine translation.
So now the question that we asked was, like, why,
why not just use attention for representations and, uh,
now here's what sort of a rough framework of this,
this representation mechanism would look like, uh,
the way- just sort of repeating what attention is essentially.
Now imagine you have- you want to represent the word,
re-represent the word representing, you want to construct its new representation.
And then first, uh, you, you attend or you,
you compare yourself, you compare your content,
and in the beginning it could just be a word embedding.
Your compare content with all your words, and with all,
with all the embeddings and based on these,
based on these compatibilities or these comparisons,
you produce, uh, you produce a weighted combination of your entire neighborhood,
and based on that weighted combination you,
you summarize all that information.
So it's, like, you're re-expressing yourself in certain terms
of a weighted combination of your entire neighborhood.
That's what attention does,
and you can add feed-forward layers to basically sort of compute new features for you.
Um, now, um so the first part is going to be about how, like,
some of the properties of self attention actually help us in text generation, like,
what inductive biases are actually useful,
and we empirically showed that indeed they,
they move the needle in text generation.
And this is going to be about machine translation,
but there were other work also that we'll talk about later.
So [NOISE] now with this, uh,
with this sort of, uh,
with this attention mechanism you get this- we get a constant path length.
So all pairs or a word can in-
position can interact with any position, every position simultaneously.
Um, hopefully if the number of positions is not too many.
Uh, attention just by virtue of, like,
it's a construction, you have a softmax,
you have these gating and multiplicative interactions.
And again, I'm not gonna be able to explain why,
but it's, it's interesting, like,
you've seen these models, like,
even, even the, uh,
even Pixel, PixelCNN, uh, or, um,
when it was actually modeling images,
they explicitly had to add these multiplicative interactions inside the model to,
to basically beat RNNs,
and attention just by construction gets this because you're,
you're multiplying the attention probabilities with your, with your activations.
It's trivial to parallelize, why?
Because you can just do attention with matmuls, especially the variant that we use in our paper,
uh, in our work.
And, uh, so now the question is
convolutional sequence to- convolutional sequence models have been very successful in,
in, in, in ge- generative tasks for text.
Can we actually do the same or achieved the same with, uh,
with, uh, attention as our primary workhorse for representation learning.
Um, so just to sort of add some context and there's been some,
there's been some- up to- up to the transformer there have been a lot of
great work on using self attention primarily for classification within.
There was, there was work on self attention within the confines of,
like, recurrent neural networks.
Um, perhaps the closest to us is the,
is the memory networks,
uh, by Weston, Sukhbaatar,
where they actually had a version of recurrent attention,
but they didn't have, uh,
but they didn't actually- empirically,
they didn't show it to work on sort of conditional modeling, like,
uh, translation and their mechanism was, uh, like,
they were using sort of a fixed- they were using a fixed query at every step.
So there's- it, it leaves something to be desired.
They still had this question, is it actually going to work, um, on,
on, on large scale machine translation systems or large-scale text generation systems.
So this is sort of the, the culmination of, um,
of the, the self attention, our self attention work.
This is the tran- the- and we put it together in the transformer model.
And, uh, so how does this look like?
So we're going to use attention pri- we're going to use
attention primarily for computing representations so- of your input.
Imagine you're doing English to German translation.
So you have your words, and notice that,
uh, attention is, uh, permutation invariant.
So you just change the order of your positions.
You change the order of your words and, and,
uh, it's not going to affect the actual output.
So in ord- in order to maintain order we add,
we add position representations.
And, uh, there's two kinds that we tried in the paper,
these, these fantastic sinusoids with no entropy invented.
And we also use learned representations which are
very plain vanilla both of them work equally well.
Um, and, uh, so,
so first we have- so the encoder looks as follows, right?
So we have a self attention layer that just recomputes the representation, uh,
for every position simultaneously using attention,
then we have a feed-forward layer.
And we also have residual,
residual connections and I'll,
I'll sort of give you a glimpse of what these residual connections
might be bringing that is between every,
every layer, and the input we have a skip connection that just adds the activations.
Uh, and then this tuple of, uh,
self attention and feed-forward layer just essentially repeats.
Now, on the decoder side, uh,
we've- we, we have a sort of standard encoder decoder architecture.
On the decoder side, we mimic a language model using self attention,
and the way to mimic a language model using self attention is to impose
causality by just masking out the positions that you can look at.
So basically, uh,
the first position it's- it can't look forward, it's illegal to look forward.
It can look at itself because we actually shift the input.
Um, so it's not copying, uh.
It's kind of surprising that parti- with these models,
it's very easy to copy at one point,
when early on it was even harder to ge- you know,
do copying with recurrent models.
But now, at least, you can copy really well,
which is a positive sign, I think overall.
Um, but, uh, so now on the decoder side, uh,
we have, uh, we have
this causal self attention layer followed by encoder-decoder attention,
where we actually attend to the, uh,
last layer of the encoder and a feed-forward layer, and this tripled,
repeats a mul- a few times,
and at the end we have the standard cross-entropy loss.
Um, and, um, so, um, sort of,
staring at the- at,
at our parti- at the particular variant of the self-
of the attention mechanis- mechanism that we use,
we went for both- we went for simplicity and speed.
So, um, so how do you actually compute attention?
So imagine you want to re-represent the position e2.
And, uh, we're going to first linearly,
linearly transform it into, uh, a query,
and then we're gonna linearly transform
every position in your neighborhood
or let's say every position at the input because this is the,
uh, uh, the encoder side,
to, uh, a key.
And these linear transformations can actually be thought as features,
and I'll talk more about it later on.
So it's like- it's, it's basically a bilinear form.
You're projecting these vectors into a space where dot product is
a good- where just a dot product is a good proxy for similarity.
Okay? So now, you have your logit,
so you just do a so- softmax computer convex combination.
And now based on this convex combination,
you're going to then re-express e2 or in
terms of this convex combination of all the vectors of all these positions.
And before doing- before doing the convex combination,
we again do a linear transformation to produce values.
And then we do a second linear transformation just to
mix this information and pass it through a- pass it through a feedforward layer.
And this is- um,
and all of this can be expressed basically
in two- in two- in two-matrix multiplications,
and the square root factor is just to make sure that these,
these dot products don't blow up.
It's just a scaling factor.
And, uh, and, and,
wha- why is this particular- why is
this mechanism attractive? Well, it's just really fast.
You can do this very quickly on a GPU,
and simul- you can do it simultaneously for
all positions with just two matmuls and a softmax.
Um, on the decoder side it's,
it's exactly the same,
except we impose causality by just adding 10 e- minus 10 e9 to the logits.
So it basi- it's just- you just get zero probabilities on those positions.
So we just impose causality by, by adding these,
uh, highly negative values on the attention- on the attention logits.
Um, is, is everything-
[LAUGHTER]
I thought that was a question.
So, um, [LAUGHTER] okay so attention is really, uh, attention is cheap.
So because it's- because this variant of
attention just involve two- involves two matrix multiplications,
it's quadratic in the length of your sequence.
And now what's the computational profile of RNNs or convolutions?
They're quadratic in the dimension.
Because, basically, you can just think of a convolution just flattening
your input or just applying a linear transformation on top of it, right?
So- and when does this actually become very attractive?
This becomes very, very attractive when your dimension is,
uh, much larger than your length.
Which is the case for machine translation.
Now, we will talk about cases when there's- when the- when this is not true,
and we have to- we have to do a- we have to make other model developments.
Um, but, uh, but for
short sequences or sequences where
your length does- where your dimension dominates length,
attention is a very- has a very favorable computation profile.
And as you can see, it's about four times faster than an RNN.
Um, um, and, and faster than
a convolutional model where the- you have a kernel of- like filter with, uh, three.
So, so there's still one problem.
Now, here's something- so in language,
typically, we want to know, like,
who did what to whom, right?
So now, imagine you applied a convolutional filter.
Because you actually have
different linear transformations based on let- relative distances,
like this, this, this, this,
linear transformation on the word who, uh, o- o- on the concept,
we can have- can learn this concept of who and, and, and,
pick out different information from this embedding of the word I.
And this linear transformation,
the lre- the red linear transformation can pick out different information
from kicked and the blue linear transformation can pick out different,
different information from ball.
Now, when you have a single attention layer, this is difficult.
Because all- because they're just a convex combination
where you have the same linear transformation everywhere.
All that's available to you is just a- is just mixing proportions.
So you can't pick out different pieces of information from different places.
Well, what if we had one attention layer for who?
So you can think of an attention layer as something like a feature detector almost,
like, because a particular- it,
it might try to- it might- because it carries with it a linear transformation,
so it's projecting them in a space that- which starts caring maybe about syntax,
or it's projecting in this space which starts caring about who or what.
Uh, then we can have another attention layer for or attention head for what,
did what, and other- another attention head for,
for, for whom- to whom.
And all of this can actually be done in parallel,
and that's actually- and that's exactly what we do.
And for efficiency, instead of actually
having these dimensions operating in a large space,
we just- we just reduce the dimensionality of all these heads
and we operate these attention layers in parallel, sort of bridging the gap.
Now, here's a, uh,
perhaps, well, here's a little quiz.
I mean, can you actually- is there
a combination of heads or is there a configuration in which you can,
actually, exactly simulate a convolution probably with more parameters?
I think there should be a simple way to show that if you
had mo- more heads or heads are a function of positions,
you could probably just simulate a convolution,
but- although with a lot of parameters.
Uh, so it can- in, in,
in the limit, it can actually simulate a convolution.
Uh, and it also- we can al- we can continue to enjoy the benefits of parallelism,
but we did increase the number of softmaxes
because each head then carries with it a softmax.
But the amount of FLOPS didn't change because we-
instead of actually having these heads operating in very large dimensions,
they're operating in very small dimensions.
Um, so, uh, when we applied this on, on,
on machine translation, um,
we were able to drama- uh, dramatically outperform,
uh, previous results on English-German and English-French translation.
So we had a pretty standard setup: 32,000-word vocabularies,
WordPiece encodings, WMT14-, uh,
WMT 2014, uh, was our test set,
2013 did the dev set.
And, uh, and some of these results were much stronger than even our previous ensemble models.
And, um, and on English-French also,
we had some- we had some very favorabl- favorable results.
Uh, and we- and we are,
we, we, we achieved state of the art.
Now, ste- stepping back a bit, uh,
I- I'm not claiming that we,
we arrived at an architecture that has better expressivity than an LSTM.
I mean, there's, there's, there's,
there's theorems that are- that say that LSTMs can model any function.
Um, perhaps, all we did was just build an architecture that was good for SGD.
Because stochastic gradient descent could just train this architecture really well,
because the gradient dynamics and attention are very simple attentions,
just a linear combination.
And, uh, um, I think that's- I,
I think that's actually favorable.
But hopefully, uh, as we- as we go on,
but the- well, I'd,
I'd also like to point out that, you know,
we do explicit mo- we do explicitly model all,
all path connection, all, all,
all pairwise connections and it has its adva- advantage of a very clear modeling,
very clear relationships directly between, between any two words.
Um, and, like, hopefully we'll be able to also
show that there are other inductive biases.
That it's not just like building more architectures that,
that are good for- that are good inductive biases for SGD.
So frameworks, a lot of our work was initially pushed out in tensor2tensor.
Maybe that might change in the future with the arrival of JAX.
There's ano- there's a framework also from Amazon called Sockeye.
There's also Fairseq, uh, the se- the
convolutional sequence-to-sequence toolkit from Facebook that the,
they prob- I'm actually not sure if it has a transformer implementation,
but they have some really good sequence-to-sequence models as well.
Um, okay.
So the importance of residuals.
So, uh, we have these resil- residual connections, uh, between, um,
so we have these residual connections that go from here to- here to here,
here to here, like between every pair of layers, and it's interesting.
So we, um, we- so what we do is we just
add the position informations at the input to the model.
And, uh, we don't infuse- we don't infuse
or we don't inject position information at every layer.
So when, uh, we severed these residual connections and we loo- stared at these,
uh, stared at these attention distributions, this is the center or,
sort of, the middle map is this attention distribution.
You actually- basically, it- it's been unable to pick this diagonal.
It should have a very strong diagonal focus.
And so what has happened was these residuals
were carrying this position information to every layer.
And because these subsequent layers had no notion of position,
they were fi- finding it hard to actually attend.
This is the encoder-decoder attention which typically ends up being diagonal.
Now, so then we, uh, we said okay.
So then we actually continued with- continued to sever the residuals,
but we added position information back in at every layer.
We injected position information back in.
And we didn't recover the accuracy,
but we did get some of this,
sort of, diagonal focus back in.
So the residuals are doing more, but they're certainly,
definitely moving this position information to the model there.
They're pumping this position information through the model.
Um, okay.
So, so that was- that was- so, so now we saw that,
you know, being able to, sort of,
model both long- and short-,
short-term relationships, uh, sh- uh, long and,
long- and short-distance relationships with,
with attention is beneficial for, for text generation.
Um, what kind of inductive,
inductive biases lay- actually, uh, appear, or what,
what kind of phenomena appear in images and something that we constantly see- constantly
see in images and music is this notion of
repeating structure that's very similar to each other?
You have these motifs that repeat in, in different scales.
So, for example, there's a b- it's another artificial but beautiful example of
self-similarity where you have this Van Gogh painting where this texture or these,
these little objects just repeat.
These images are- these different pieces of the image are very sa- similar to each other,
but they might have different scales.
Uh, again in music,
here's a motif that repeats, uh,
that could have- it could have, like,
di- various, like, spans of time between in, in, between it.
So, um, so, so this,
so we, we, we,
we attempted after this to see, well,
to ask this question: can self-attention help us in modeling other objects like images?
So the, the path we took was, sort of,
standard auto-regressive image modeling the- or probabilistic image modeling, not GANs.
Because it was- well, one, it was very easy.
We had a language model almost.
So this is just like language modeling on images.
Uh, and also training at maximum,
likely, it allows you to, sort of,
measure, measure how well you're doing on,
uh, on, on your held-out set.
Uh, and it also gives you diversity,
so you hopefully are covering all possible, uh,
different kinds of images you- So, um,
and to this point there's al- we had
an advantage that's also been- there are- there've been
good work on using recurrent models like PixelRNN and PixelCNN,
that, that we're actually getting some very good compression rates. Um-
And, um, again here,
originally the argument was that, well, you know,
in images because there- because you want symmetry,
because you want like if you have a face,
you want, you want one ear to sort of match with the other.
If you had a large receptive field,
which you could potentially get with attention at a lower computational cost,
then it should benefit- then it should be quite beneficial for, for images,
for images and you wouldn't need many layers like you do in
convolutions to actually get dependencies between these far away pixels.
So it seem like self-attention would have been a- what, what,
what was already a good computational mechanism, right?
But this sort of- but it was actually interesting to see
how it even modeled- naturally modeled self-similarity,
and people have used self-similarity in image generation like, you know, uh,
there's this really cool work by Efros where they actually see, okay,
in the training set, what are those patches that are really,
that are really similar to me?
And based on the patches that are really similar to me,
I'm going to fill up the information.
So it's like actually doing image generation.
Uh, there is this really classic work called
non-local means where they do image denoising,
where they want to denoise this sort of,
this patch P. And they say,
I'm going to- based on my similarity between all other patches in my image,
I'm going to compute some function of content-based similarity,
and based on the similarity I'm going to pull information.
So as- and exploiting this fact that images are very self-similar.
And, uh, uh, this has also been sort of,
uh, applied in some recent work.
Now if you just took this encoder self-attention mechanism
and just replace these word embeddings with patches,
and that's kind of exactly what it's doing.
It's, it's computing this notion of content-based similarity
between these elements and then based on this content-based similarity,
it constructs a convex combination that essentially brings these things together.
So it's, it's a very ni- it was,
it was quite- it was very pleasant to see that,
oh, this is a differentiable way of doing non-local means.
And, uh, and we took the transformer architecture and replaced words with pixels.
Uh, there was some- there were some architecture adjustments to do.
And, uh, so this was but- this was
basically the kind of- it was very similar to the original work,
and here the position representations instead of being, you know,
one-dimensional, they were- because we are not dealing with sequences,
we have two-dimensional position representations.
Um, okay.
So I pointed out before,
attention is a very com- very favorable computational profile
if your length- if your dimension dominates length,
which if- which is absolutely untrue for,
absolutely untrue for images.
Uh, because even for like 32 by- even for 32 by 32 images,
when you flatten them and you- and you flatten them, you have 30- you get 30,
72 positions, uh, so it's your standard CFIR image.
Um, so simple solution, uh,
because like convolutions of- I mean,
you get- convolutions are basically looked
at local windows and you get translational equivariance.
We said, "Okay. Let's adopt the same strategy."
And also there's a lot of spatial locality and images.
Uh, but now, we will still have a better computational profile.
If your- if your receptive field is still smaller than your dimension,
you can afford- you can actually still do
much more long distance computation than a standard convolution because you're,
uh, because you're quadratic in length.
So as long as we didn't increase our length beyond the dimension,
we still had a favorable computational profile.
And so the way we did it was, uh,
we essentially had, uh,
two kinds of rasterizations.
So we had a one-dimensional rasterization where you had a sort of single query block,
uh, which was, uh,
which was then attending or to the- into a larger memory block,
uh, in this rasterized fashion along the- along, along the rows.
Um, then we tried another form of rasterization,
falling standard two-dimensional locality,
where you had- where we actually produced the image in,
uh, in blocks and within each block we had a rasterization scheme.
Um, again, these- the image transformer layer was very similar.
We had two-dimensional position representations along
with query- with the same- with a very similar attention mechanism.
Um, and we tried
both super-resolution and unconditional and conditional image generation.
Uh, this is- this is Ne- Niki Parmar,
I and a co- and a few other authors from Brain,
um, and we presented it at ICML.
And, uh, we were able to achieve better perplexity than existing models.
So PixelSNAIL is actually another model that used- mixed
both convolutions and self-attention and they- they outperformed us on,
on, on, on, on, bits per dimension.
So we were measuring perplexity because these are
probabilistic- these are probabilistic models.
It's like basically a language model of images and,
and it just- and your- and the factorization
of your language model just depends on how you rasterize.
In the- in this- in the one-D rasterization,
we went first rows and then columns.
In the two-D rasterization,
we went blockwise and inside each block we rasterized.
On ImageNet, we achieved better perplexities, and,
uh, so yeah, I mean we're at a GAN level, right?
I mean this weird- this is- I think probabilist auto-regressive Image generation,
uh, by this point had not reached GANs.
At ICLR 2019, there's a paper by Nal that actually uses self-attention and gets very,
very good quality images.
But what we, what we observed was,
we were getting structured objects fairly well.
Like can people recognize what the second row is?
Cars. [OVERLAPPING]
I heard- I said- most- almost everyone said cars.
I'm not going to ask who said something else, but yes, they're cars.
yeah. And, uh, so the- and the last row is another vehicles like,
uh, so essentially when structured jo- structured objects were easy to capture.
Um, like frogs and sort of,
you know, objects that were camouflaged just turned into this mush.
Um, and- but on super resolution,
now super-resolution is interesting because
there's a lot of conditioning information, right?
And, uh, when you have a lot of conditioning information, the,
the sort of possible- you break- you,
you actually lock quite a few of the modes.
So there's only a few options you can have at the output.
And super- our super resolution results are much better.
We were able to get better facial orientation and structure than previous work.
And these are samples at different temperatures and, uh, and, uh,
and we wou- when we quantify this with actual human evaluators,
we- like we flash an image and said,
is this real, is this false?
And we were able to, uh,
we were able to fool humans like four
times better than previous results in super resolution.
Again, these are not- these results like I, I guess the,
the latest GAN result from Nvidia makes us look like a joke.
But, I mean this is,
I mean, we're starting later than GAN.
So hopefully we'll catch up.
But, but the point here is that this is an interesting inductive bias for images,
so very natural inductive bias for images.
Um, and, uh, and,
and there is hope to apply it- for applying in classification and other such tasks also.
Um, so one interesting thing,
just to sort of both out of curiosity and
asking how good is maximum or like does maximum likelihood.
Well, one, does the model actually capture some interesting structure in the role?
Second, do you get diversity?
Well, maximum likelihood should get diversity,
by, by virtue, by virtue of what it does.
Uh, so then we just- we did image completion.
And why is- why image completion because as soon as you
lock down half the image to the goal truth,
you're actually shaving off a lot of the possible modes.
So you have a much easier time sampling.
So, uh, so the first is,
uh, first is what we supply to the model.
The, the, the right row- the right most column is,
is gold, and we were able to generate different samples.
But what was really interesting is the third row.
Uh, so the rightmost column is- the rightmost column is gold.
Uh, now if you look at the third row, this horse.
So actually there's this sort of glimpse or a suggestion of a pull,
but the model hallucinated a human in some of these,
in some of these images,
which is interesting like in- it does capture at least
the data teaches it to capture some structure about the world.
Um, the dog is just cute and I guess it also shows that, you know,
there was this entire object,
this chair, that the model just completely refused to imagine.
So there's a lot of difficulty.
And I guess Anna is gonna talk about
[NOISE] the another way to exploit self- self-similarity.
Thank you.
[APPLAUSE]
So thank you Ashish for the introduction.
Uh, so there's a lot of self-similarity in images.
There's also a lot of self-similarity in, in music.
So we can imagine, transformer being a, a good model for it.
Uh, we- we're going to show how,
uh, we can add more to,
to the self attention, to think more about kind of
relational information and how that could help, uh, music generation.
[NOISE] So, uh, first I want to
clarify what is the raw representation that we're working with right now.
So analogous to language,
you can think about there's text and somebody is reading out a text,
so they add their kind of own intonations to it,
and then you have sound waves coming out of that speech.
So for music there's a va- very similar kind of, uh,
line of a generation where you say the composer has an idea,
uh, writes down the score and then,
a performer performs it and then you get sound.
So what we're going to focus on today is mostly, uh,
you can think of the score but it's actually,
er, a performance, um,
in that it's a symbolic representation where MIDI pianos were used and,
uh, um, professional amateur, uh,
musicians were performing on the pianos.
So we have the recorded,
uh, information of their playing.
So in particular, um,
at each time se- step modeling music as this sequential, uh,
process, what is being output are, okay,
turn this note on, ah,
advance the clock by this much,
and then turn this note off.
And also there is, uh, dynamics information,
so when you turn the note on, you first say like,
how loud it's going to be.
Uh, so traditionally, uh, modeling, uh,
music as kind of a language,
we've been using, uh, recurrent neural networks.
And, um, because as Ashish introduced and, and talked about,
there is a lot of compression that needs to happen,
like a long sequence has to be embedded into like a fixed length vector.
And that becomes hard when, uh,
in music you have- you have repetition coming,
um, at a distance.
So, uh, I'm first going to show you,
um, samples from, from the RNNs,
from a transformer and then from a music transformer that has
the relative attention and kind of let you hear
the differences and then I'll go into how we,
uh, what are, what are the, uh,
modifications we needed to do on top of the, uh, transformer model.
Uh, so here, uh,
this task is kind of the image completion task.
So we give it an initial motif and then we ask the model to do continuations.
So this is the motif that we fed.
[MUSIC] How many people recognize that?
Awesome. Okay. [LAUGHTER] Yeah,
so this is a, uh,
kind of a fragment from a Chopin Etude piece.
And we're going to ask, uh,
the RNN to do a continuation.
[NOISE]
[MUSIC]
So in here, like in the beginning, it was trying to repeat it.
But very fast, it, er,
wandered off into, its other different ideas.
So that's one challenge because it's, uh,
not able to directly look back to what happened in the past, uh, and,
and can just look at kind of a blu- blurry version,
and that blurry version becomes more and more blurry.
Uh, so this is what the transformer does.
Uh, so so, uh, a detail is, uh,
these models are trained on half the length that you're hearing.
So we're kinda asking the model to generalize beyond the length that it's trained on.
And you can see for this transformer,
it, it deteriorates beyond that.
But it can hold the motif pretty consistent.
[MUSIC] Okay. You, you,
you ge- you get the idea.
[LAUGHTER] So initially, it was able to do this repetition really well.
Uh, so it was able to copy it very well.
But beyond the length that was trained on,
it kinda didn't know how to cope with, like longer contexts.
And, uh, what you see,
uh, the, the last one is from the music transformer.
I think so that kind of [NOISE] the relational information.
And you can just see visually how it's very consistent and kinda
repeating these [NOISE] these larger, uh, arcs.
[MUSIC]
Yeah. So that was, uh, music transformer.
And so in music,
the, the self similarity that we talked about, uh,
so we see, uh,
the motif here, and so,
so there we primed the model with a motif,
and this is actually a sample,
unconditioned sample from the model.
So nothing, er, there was no priming that the, uh,
model kinda had to create its own motif and then,
uh, do, uh, continuations from there.
And here, uh, if we kinda look at it and analyze it a bit, you see,
uh, a lot of repetition,
uh, with gaps in between.
And if you look at the self attention structure,
we actually do see the model,
uh, looking at the relevant parts.
Even if, if it was not immediately, uh, preceding it.
So, so here, uh,
what I colored shaded out is where the motif, um, occurs.
Uh, and you can, uh, see the different colors,
there's a different attention heads and they're kinda focusing,
uh, among those, uh, grayed out sections.
[NOISE] So I'll play the sample and we also have
a visualization that kind of shows you as the music is pa- uh,
is being played or what notes it was attending to as it was predicting that note.
And, uh, this was generated from scratch.
And, uh, so the self attention is, um,
from, from kind of note to note level or event to event level.
So it's, it's quite low level.
Uh, so when you look at it, it's,
it's ki- a little bit overwhelming.
It has like multiple heads and,
er, a lot of things moving.
Uh, but there's kind of these structural moments
where you would kind of see more of this, uh,
clean, uh, kind of,
uh, sections where it's attending to.
[MUSIC]
VOkay. So, um,
how, how did we do that?
And so starting from kind of the the regular attention mechanism,
we know it's, uh, a weighted average of the past history.
Uh, and the nice thing is, uh,
however far it is, we have direct access to it.
So if we know, uh,
there are kind of motifs that occurred,
uh, in in early on in the piece,
we're still able to based on, uh,
the fact that things that are similar,
uh, to be able to retrieve those.
Um, but, uh, it also becomes,
all the past becomes kind of a bag of words,
like there is no structure of which came,
uh, before or after.
So there's the positional sinusoids that Ashish talked about.
That, uh, basically in this, uh,
indices indexes into a sinusoids that are moving at different speeds.
And so close-by positions would have, uh,
a very similar kind of, uh,
cross section into those multiple sinusoids.
Uh, in contrast for, er,
for convolutions, you kinda have this, uh,
fixed filter that's moving around that captures the relative distance.
Like 1B4, 2B4.
And these are kind of, uh,
in some ways like a rigid structure that allows you to be, uh,
a kind of, uh, bring in the,
the distance information very explicitly.
Um, you can imagine relative attention, um,
with the multiple heads, uh, at play,
uh, to be some combination of these.
So, uh, on one hand,
you can access, uh,
the the history very directly.
On the other hand, you also know, er,
how you rel- relate to this history.
Uh, capturing for example,
like translational invariance and, er,
and we, uh, and for example,
we think one of the reasons why in the beginning, uh,
priming samples that you heard that the, uh,
music transformer was able to generate
beyond the length that it was trained on at a very coherent way,
is that it's able to kind of rely on this translational invariance to to carry,
uh, the relational information forward.
So, if we take a closer look at how how how the,
how this works is, uh,
the regular transformer you have,
you compare all the queries and keys,
so you get kind of this, uh, square matrix.
You can think of it as like a self similarity,
uh, matrix, so it's, uh, a square.
Uh, what relative attention does is,
to add an additional term that thinks, uh,
that thinks about whenever you're comparing two things,
how far are you apart?
And also based on the content, do I,
do I care about things that are two steps away or
three steps away or I maybe care about things that are recurring,
at kind of a periodical distance.
And, uh, with that information gathered,
that influences, uh, the the similarity between positions.
And in particular, uh,
this extra term is based on, um, the distance.
So you wanna, uh,
gather the embeddings, uh,
that's irrelevant to the, uh,
the query key distances,
uh, on the [NOISE] on the logits.
So, in translation, this,
uh, has shown, uh,
a lot of improvement in,
um, for example English to to German translation.
Uh, but in translation,
the sequences are usually quite short.
It's only a sentence to sentence.
Uh, a translation for example,
maybe 50 words or 100 words.
But the music, er, samples that you've heard are in the range of 2,000 time-steps.
So it's like 2,000 tokens need to be able to fit in memory.
So this was a problem, uh,
because the original formulation relied on building this 3D tensor that's,
uh, that's very large in memory.
Um, and and why this is the case?
It's because for every pair,
uh, you look up what the,
what the re- so you can compute what the relative distance is,
and then you look up an embedding that corresponds to that distance.
So, um, for like this there's a length by length, like L by L, uh, matrix.
You need like, uh,
to collect embeddings for each of the positions and that's, uh,
depth D. So that gives us the 3D.
What we realized is,
you can actually just directly multiply the queries and the embedding distances.
[NOISE] And they, uh,
come out kind of in a different order,
because now you have the queries ordered by a relative distance,
but you need the queries ordered by keys, uh,
which is kind of a absolute by absolute, uh, configuration.
So what we could do is just, uh,
do a series of skewing, uh,
to to put it into the right, uh, configuration.
And this is, uh, yeah.
Just a, just a quick contrast to,
to show, um, the difference in memory requirements.
So, er, a lot of the times the challenge is in, uh,
being able to scale, uh, you know,
being able to be more memory efficient so that [NOISE] you can model longer sequences.
So with that, uh, this is,
um, I can play you one more example if we have time.
But if we don't have time, we can, go ahead.
We'll see more of that.
Okay. [LAUGHTER] So this is,
this is, uh, maybe a one, uh,
about a one-minute sample and I- I hope you like it.
Thanks. [MUSIC]
Thank you for listening.
[APPLAUSE].
[LAUGHTER] Thanks, Anna. Um, um, great.
Um, so to sort to, um,
so relative attention has been a powerful mechanism for,
um, a very powerful mechanism for music.
It's also helped in machine translation.
Um, one really interesting, uh,
consequences of, uh, of, um,
one really interesting consequence of relative attention in,
uh, images, is that,
um, like convolutions achieve,
uh, convolutions achieve translational equivariance.
So if you have,
let's say, you wa- uh, you have this,
this red dot or this feature that you're computing at this red dot,
it doesn't depend on where the image of the dog is in the image,
is in the the larger image. It just doesn't depend on its absolute location.
It's going to, it's going to produce the same activation.
So you have- convolutions have this nice, uh, translation equivariance.
Now, with, with relative,
uh, positions or relative attention,
you get exactly the same effect because you don't have any- once you just
remove this notion of absolute position that you are injecting [NOISE] into the model,
uh, once you've, once you've removed that,
then your attention computation,
because it actually includes I mean, we've,
we've- Niki and I couple of others have actually,
and Anna were actually working on images and seems-
and it seems to actually show, uh, better results.
Um, this actio- this now satisfies this,
uh, uh, the- I mean, it,
it can achieve translation equivariance which is a great property for images.
So there's a lot of- it seems like this might be
an interesting direction to pursue if you want to push,
uh, Self-Attention in images for a self-supervised learning.
Um, I guess on, on self-supervised learning so the geni- generative modeling work that,
that I talked about before in,
in itself just having probabilistic models of images is, I mean,
I guess the best model of an image is I,
I go to Google search and I pick up an image and I just give it to you,
but I guess generative models of images are useful because,
if you want to do something like semis-, uh, uh,
self supervised learning where you just pre-train a model on
a lot of- on a lot of unlabeled data then you transfer it.
So hopefully, this is gonna help and this is gonna be a part of that machinery.
Um, another interesting, uh,
another indus-interesting structure that relative attention allows you to model,
is, uh, is, is kind of a graph.
So imagine you have this, uh,
you have this similarity graph where these red edges are,
are this notion of companies,
and the blue edge is a notion of a fruit, uh,
and um, an apple takes these two forms.
And, uh, and you could just imagine
relative attention just modeling this- just being able to model,
or being able to- you, you,
yourself being able to impose these different notions of similarity uh,
between, uh, between, uh, different elements.
Uh, so if you have like, if you have graph problems, um,
then relative self-attention might be a good fit for you.
Um, there's also, there's also a simi- quite a position paper by Battaglia et al from
Deep Mind that talks about relative attention and how it can be used, um, within graphs.
So while we're on graphs,
I just wanted to- perhaps might be interesting to connect,
um, uh, of- some, uh,
excellent work that was done on, uh,
on graphs called Message Passing Neural Networks.
And it's quite funny, so if you look at,
if you look at the message passing function, um,
what it's saying is you're actually just passing messages between pairs of nodes.
So you can just think of self attention as imposing a fully connect- it's
like a bipe- a full, a complete bipartite graph,
and, uh, you're, you're passing messages between,
you're passing messages between nodes.
Now message passing, message passing neural networks did exactly that.
They were passing messages between nodes as well. And how are they different?
Well, the only way that when- well, mathematically,
they were only different in that message passing was,
was, uh, forcing the messages to be between pairs of nodes,
but just because of the Softmax function where you get interaction between all the nodes,
self attention is like a message passing mechanism,
where the interactions are between all, all nodes.
So, uh, they're, they're like,
they're not too far mathematically,
and also the me- the Message Passing Paper introduces
an interesting concept called Multiple Towers that are similar to multi-head attention,
uh, that, that Norman invented.
And, uh, it's like you run k copies of these message passing neural networks in parallel.
So there's a lot of similarity between existing, you know,
this connects to work that existed before but these connections sort of came in later.
Um, we have a graph library where we kind of connected these both,
both these strands message passing and, uh, we,
uh, we put it out in tensor2tensor.
Um, so to sort of summarize, um,
the properties that Self-Attention has been able to help
us model is this constant path length between any two,
any two positions, and it's been,
it's been shown to be quite useful in,
in, in, uh, in sequence modeling.
This advantage of having unbounded memory not having to pack information in finite,
in, in sort of a finite amount of- in a,
in a fixed amount of space,
uh, where in, in our case our memory essentially grows with the sequences is,
is helps you computationally, uh, it's trivial to parallelize.
You can, you can crunch a lot of data, it's uh,
which is useful if you wanna have your large data sets.
We found that it can model Self-Similarity.
Uh, It seems to be a very natural thing, uh,
a very, a very natural phenomenon if you're dealing with images or music.
Also, relative attention allows you to sort of, gives you this added dimension
of being able to model expressive timing and music,
well, this translational equivariance,
uh, it extends naturally to graphs.
Um, so this part or everything that I talked so far was about sort of parallel training.
Um, so there's a very active area of research now using the Self-Attention models for,
for, for less auto-regressive generation.
So notice a- at generation time,
notice that the decoder mask was causal,
we couldn't look into the future.
So when we're, when we're generating we're still
generating sequentially left to right on the target side.
Um, so, um, and, and,
and, and why, why is generation hard?
Well, because your outputs are multi-modal.
I f you had- if you want to translate English to German,
there's multiple ways and,
and, and your, your second word that you're translating will depend on the first word.
For example, if you, if you first- the first word that you predict was danke,
then that's going to change the second word that you predict.
And if you just predicted them independently,
then you can imagine you can just have all sorts of
permutations of these which will be incorrect.
Uh, and the way we actually break modes is
just- or we make decisions is just sequential generation.
Once we commit to a word that makes a decision,
and then that nails down what's the next word that you're going to predict.
So there's been some, there's been some work on, it's an active research area, uh,
and you can kind of categorize some of these papers like
the non-autogressive transformer of the fast- the third paper, fast decoding.
Um, the fourth paper towards a better understanding
of all Vector Quantized Auto-encoders into this group,
where they're actually make- doing the decision making in a latent space,
that's being, uh, it's e- either being learned using word alignments,
uh, fertilities, or that's being learned using Auto-encoders.
So you make- you do the decision making in latent space,
and then you- once you've made the decisions in latent space,
you assume that all your outputs,
are actually conditionally independent,
given that you've made these decisions.
So that's how they actually speed up.
There's also- there's ano- there's another paper.
The second one is a
paper that does Iterative Refinement.
There is also a Blockwise Parallel Decoding paper by Mitchell Stern,
uh, Noam Shazeer, and Jakob Uszkoreit, uh,
where they essentially just run multiple models like, uh,
and rescore using a more- a decode using a faster model and score,
using the more expensive model.
So that's how it sort of it speeds it up.
Um, [NOISE] transfer learning has had the- Self-Attention has been beneficial in transfer
learning, GPT from OpenAI and BERT are two classic examples.
There's been some work on actually, scaling this up,
like add a factor as, uh, efficient optimizer.
Um, there's a, there's a recent paper by Rohan Anil and Yoram Singer.
Um, there's also Mesh-Tensorflow,
which actually they've been able to train models
of just several orders of magnitude larger than the original models have been trained.
So there's, I mean, when you're working this large data regime you would probably want to
memorize a lot of- you want to memorize
a lot of things inside your parameters used to train a larger model.
Uh, Mesh-Tensorflow can uh, can let you do that.
Um, there has been a lot of interesting work, universal transformers,
sort of recurrent neural networks can actually count very nicely.
There's these cute papers by Schmidhuber where he actually shows
that recurring neural, the count- the cell mechanism just learns a nice counter,
like if you're- you can learn kind of a to the n,
b to the n, uh, with LSTM.
So then, uh, universals transformers
brings back recurrence in depth inside the transformer.
Uh, there is a really cool Wikipedia paper,
um, simultaneously with the image transformer paper that also uses local attention.
Transformer-XL paper that sort of combines recurrence with Self-Attention,
so they do Self-Attention in chunks,
but they sort of summarize history by using recurrence, it's kinda cute.
It's been used in speech but I don't know if there's been
some fairly big success stories of Self-Attention in speech.
Uh, again, similar issues where you have very large, uh,
um as positions to,
uh, to do Self-Attention over.
So yeah, um, self supervision is a- if it works it would be,
it would be, it would be very beneficial.
We wouldn't need large label datasets, understanding transfer,
transfers is becoming very succe- becoming- is becoming
a reality in NLP with BERT and some of these other models.
So understanding how these, what's actually happening is a-
is an interesting area of ongoing research for me and a couple.
And a few of my collaborators and uh, multitask learning and surmounting this,
this quadratic problem with Self-Attention is
an interesting area of research that I- that I'd like to pursue. Thank you.
 So today we're gonna be learning about Natural Language Generation.
And uh, this is probably going to be a little different to
my previous lectures because this is going to be much more of a kind of survey,
of lots of cutting edge, uh,
research topics that are happening in NLG right now.
So before we get to that, uh,
we've got a few announcements.
Uh, so I guess the main announcement is just,
thank you all so much for your hard work.
I know, um, the last week or two have been pretty tough.
Uh, assignment five was really quite difficult,
I think, and it was a challenge to do it in eight days.
So we just really appreciate all the hard work you've put in.
Um, we also understand the project proposal was,
uh, sometimes a bit difficult to understand the expectations for some people.
Um, so, yeah, these are both new components of
the class this year that were not present last year.
Um, so you know,
we have to go through some learning curves as well as the teaching staff.
So just we really want to say thank you so much, uh,
for putting everything into this class.
And please do continue to give us your feedback both right
now and in the end of quarter feedback survey.
Okay, so here's the overview for what we're going to be doing today.
So today we're going to learn about what's happening in the world
of neural approaches for Natural Language Generation.
Uh, that is a super,
super broad title, Natural Language Generation.
Um, NLG encompasses a huge variety of research areas
and pretty much each of those could have had
their own lectures and we could have taught a whole,
a whole quarter- quarter's worth of classes on, ah, NLG.
Uh, but we're going to try to cover a selection of things today.
And, um, uh, it's mostly going to be, uh,
guided by the things which, uh,
I've seen that I think are cool or interesting or exciting.
So it's by no means going to be comprehensive but
I hope you're going to enjoy some of the stuff we're going to learn about.
Okay, so in particular we're going to start off by having a recap of what we
already know about Natural Language Generation to make sure we're on the same page.
And we're also going to learn a little bit extra about decoding algorithms.
So we learned a bit before about, uh,
greedy decoding and beam search decoding,
but today we're going to learn some extra information about
that and some other types of decoding algorithms.
After that we're going to go through, um,
a pretty quick tour of lots of
different NLG tasks and a selection of neural approaches to them.
And then after that we're gonna talk about probably the biggest problem in NLG research,
which is NLG evaluation and why it is such a tricky situation.
And then lastly, we're going to have some concluding thoughts on NLG research.
What are the current trends and where are we going in the future?
Okay. So, uh, section one, let's do a recap.
Okay, so Natural Language Generation to define it just
refers to any setting in which we are generating some kind of text.
So for example, NLG is an important sub-component
of lots of different tasks such as, uh, machine translation,
we've already met, uh, abstracted summarization,
we'll learn a bit more about that later, um,
dialogue both chit-chat and task-based.
Uh, also creative writing tasks such as writing stories and writing poems even.
NLG is also a sub-component of,
uh, free-form question answering.
So I know a lot of you are doing the SQuAD project right now, uh,
that is not an NLG task because you're just extracting the answer from the,
uh, the source document.
But there are other question answering tasks
that do have a Natural Language Generation component.
Uh, image captioning is another example of,
uh, a task that has an NLG sub-component.
So NLG is a pretty cool component of a lot of different NLP tasks.
All right, let's go into our recap.
So the first thing I want to recap is,
uh, what is language modeling?
Um, I've noticed that some people are a little bit confused about this, I think it, uh,
might be because the name language modeling sounds like it might mean
just simply encoding language like representing language using embeddings or something.
So as a reminder language modeling,
uh, has a more precise meaning.
Language modeling is the task of predicting the next word given the word so far.
So any system which produces
this conditional probability distribution that does this task is called a Language Model.
And if that language model,
uh, system is an RNN,
then we often abbreviate it as RNN-Language Model.
Okay, so I hope, uh, you'll remember that.
Uh, the next thing we're going to recap is do you
remember what a Conditional Language Model is?
Uh, the task of Conditional Language Modeling is when you're predicting, uh,
what word's going to come next but you're also conditioning on
some other input x as well as all of your words so far.
So to recap some examples of conditional language modeling include, uh,
machine translation where you're conditioning on the source sentence x,
uh, summarization you're conditioning on your input text that you're trying to summarize.
Dialogue, you're conditioning on your dialogue history and so on.
Okay, uh, next we're going to quickly recap how do you train an RNN-Language model?
I guess, it could also be a transformer-based language model or a CNN-based language model,
now that you know about those, uh, and it could be conditional or it could be not.
So the main thing I want to remind you about is that when you are training the system,
then you feed in the target sequence that you're trying to
generate so where it says target sentence from corpus, uh,
that's saying that you have some sequence that you're trying to
generate and you feed that into the decoder, the RNN-Language model.
And then it predicts what words are going to come next.
So the super important thing is that during training,
we're feeding the gold, that is the reference target sentence into the decoder,
regardless of what the decoder is predicting.
So even if let's say this is a very bad decoder that isn't predicting the correct words,
uh, it's not, you know, predicting them high at all,
um, that doesn't matter we still just, um,
input the targets- the gold target sequence into the decoder.
And, um, I'm emphasizing this because it's going to come up later,
uh, this training method is called Teacher Forcing.
Which might be a phrase that you have come across elsewhere.
So, yeah, it refers to the fact that the teacher,
that is kind of like the gold input is- is forcing, uh,
the language model to use that on every step
instead of using its own predictions on each step.
So that's how you train a RNN-Language model which might be conditional.
Uh, okay.
So now a recap on decoding algorithms.
So, uh, you've got your trained language model which might be conditional.
The question is how do you use it to generate a text?
So the answer is you need a decoding algorithm.
A decoding algorithm is an algorithm you use to
generate the text from your trained language model.
So, uh, in the NMT lecture
a few weeks ago we learned about two different decoding algorithms.
We learned about greedy decoding and beam search.
So let's quickly recap those.
Uh, greedy decoding is a pretty simple algorithm.
On each step you just take what's
the most probable words according to the language model.
You could deal with the argmax and then use that as the next word,
you feed it in as the input on the next step.
And you just keep going until you produce some kind of END
token or maybe when you reach some maximum length.
And I think you're all quite familiar with this because you did it in assignment five.
So uh, yes this diagram shows how greedy decoding would work to generate the sentence.
So as we learned before,
due to a kind of lack of backtracking and
inability to go back if you made a wrong choice, uh,
the output from greedy decoding is generally, uh,
pretty poor like it can be ungrammatical, or it can be unnatural, kind of nonsensical.
Okay, let's recap beam search decoding.
So beam search is a search algorithm which aims to find a high probability sequence.
So if we're doing translation that sequence is the sequence of translation words,
um, by tracking multiple possible sequences at once.
So the core idea is that on each step of the decoder,
you're going to be keeping track of
the K most probable partial sequences which we call hypotheses.
And here K is some hyper- hyper parameter called the beam size.
So the idea is by um,
considering lots of different hypotheses we're going to try to search effectively for
a high probability sequence but there is
no guarantee that this is going to be the optimal,
most high probability sequence.
So, uh, at the end of beam search, uh,
you reach some kind of stopping criterion which we talked
about before but I won't cover in detail again.
Uh, and once you've reached your stopping criterion,
you choose the sequence with the highest probability,
um, factoring in some adjustments for length and then that's your output.
So just to do this one more time.
Here's the diagram that we saw in the NMT lecture of beam search decoding um,
once it's completed and in this scenario we have a beam size of two.
So this is what it looks like after we've done this exploration problem,
this shows the full tree that we explored,
and then we've come to some kind of stopping criterion and we identify the top,
uh, hypothesis and, uh,
that's highlighted in green.
So on the subject of beam search decoding,
I was watching TV the other day,
and I notice something in Westworld.
I think the hosts- [LAUGHTER] the AI hosts in Westworld maybe used beam search.
Which is something I wasn't expecting to see on TV.
[LAUGHTER] So there's this scene,
uh, Westworld is, by the way,
a sci-fi series that has these, um,
very convincing humanoid AI systems.
Um, and there's a scene where one of
the AI systems is confronted with the reality of the fact that,
um, she, I suppose is,
um, not human because she sees the generation system of words as she says them,
and I was looking at the TV and I thought,
is that beam search?
Because that diagram looks a lot like this diagram here,
um, but maybe with a bigger beam size.
So, I thought, that was pretty cool because, you know,
AI has hit the mainstream when you see beam search on TV.
And then if you zoom in really hard you can see
some other exciting words in this screenshot like knowledge base,
forward chaining and backward chaining,
identifies the same thing as forward prop and backward prop,
um, and also fuzzy logic algorithms and neural net.
Um, so yeah, beam search,
I think, has hit the mainstream now,
um, so it's good enough for Westworld,
maybe it's good enough for us.
Uh, so with beam search, right?
We've talked about how you have this hyperparameter k or the beam size.
And one thing we didn't talk about in the last lecture,
so now we're leaving the recap portion, um,
is what's the effect of changing that beam size k. So, uh,
if you have a really small k,
then you're gonna have similar problems to greedy decoding.
And in fact, if k equals one,
then you are actually just doing greedy decoding.
So those same problems are, you know, ungrammatical,
maybe unnatural, nonsensical, just kind of plain incorrect output.
So once if we get larger k,
if you have a larger beam size,
then you're doing your search algorithm but considering more hypotheses, right?
You're, you're having a larger search space and
you're considering more different possibilities.
So if you do that, then we often find that this reduces some of the problems above.
So you're much less likely to have this ungrammatical,
uh, you know, disjointed output.
But there are some downsides to raising k. So of course,
larger k is more computationally expensive
and that can get pretty bad if you're trying to, um,
for example, generate your, uh,
outputs for a large, you know,
test set of NMT examples.
Um, but more seriously than that,
increasing k can introduce some other problems.
So for example, it's been shown that in NMT,
increasing the beam size too much actually decreases the BLEU score.
And this is kind of counter-intuitive, right?
Because we were thinking of beam search
as this algorithm that tries to find the optimal solution.
So surely, if you increase k,
then you're only going to find a better solution, right?
Um, so I think maybe the key here is the difference between optimality
in terms of the search problem that is finding
a high probability sequence and BLEU score,
which are two separate things,
and there's no guarantee that they actually, um, correspond, right?
And I mean, there's a difference, again, between BLEU score and actual translation,
uh, quality as we know.
So if you look at the two papers which I've linked to
here which are the ones that show that,
uh, increasing beam size too much decreases the BLEU score.
They explain it by saying that the main reason why this
happens is because when you increase the beam size too much,
then you end up producing translations that are too short.
So I mean, that kind of explains it to a degree that translations are too short,
therefore they have low BLEU because they're
probably missing words that they should contain.
But the question is, why does large beam size gives you short translations?
I think that's harder to answer.
Wherever, in these two papers, I didn't see an explicit explanation of why.
Um, I think it's possible larger kind of passing,
we see sometimes with beam search which is when you really increase your, uh,
search space and make the search much more
powerful so that it can consider lots of different alternatives.
It can end up finding these high probability,
um, sequences which aren't actually the thing that you want.
Sure, they're high probabili- probability
but they're not actually the thing that you wanted.
Um, so another example of that is
that in open-ended tasks like for example chit-chat dialogue
where you're trying to just, um,
say something interesting back to your conversational partner,
if we use a beam search with a large beam size,
we find that that can give you some output that is really generic.
Um, and I'll give you an example here to show you what I mean.
So these are examples from a chit-chat,
uh, dialogue project that I was doing.
So here you've got, uh,
your human chit-chat partner said something like I mostly eat a fresh and raw diet,
so I save on groceries.
And then here's what the chat bot said back depending on the beam size.
I will let you read that.
So I would say that this is fairly characteristic of what you see
happening when you raise and lower the beam size [NOISE].
When you have a low beam size,
um, it might be more kind of on topic.
Like here, we can see that eat healthy, eat healthy,
I am a nurse so I do not eat raw food and so on,
that kind of relates to what the user said,
uh, but it's kind of bad English, right?
There's some repetition and,
uh, it doesn't always make that much sense, right?
Um, [NOISE] but then,
when you raise the beam size,
then it kind of converges to
a safe so-called correct response but it's kind of generic and less relevant, right?
And it's kind of applicable in all scenarios, what do you do for a living.
Um, so the, the,
the particular dataset I was using here is, uh,
one called Persona-Chat,
that I'll tell you more about later.
Um, but it's a,
it's a chit-chat dialog dataset where each,
uh, conv- conversational partner has a persona which is a set of traits.
Um, so the reason it keeps talking about being a nurse,
I think is because it was in the persona.
[NOISE] But the main point here is that, um,
we kind of have an unfortunate trade off with no,
with no Goldilocks zone that's very obvious.
I mean, there's, there's a, yeah,
kind of an unfortunate trade-off between having kind of bad,
bad output, bad English and just having something very boring.
So this is one of the problems that we get with beam, beam search.
Okay. So we've talked about, uh,
greedy decoding and beam search. Yes.
So beam size depending on the [inaudible]
The question is, can we have
an adaptive beam size dependent on the position that you're in?
You mean like in the sequence?
Yeah. That is in [inaudible].
Yeah. I mean, I think I- I might have heard of a research paper that does that?
That adaptively like raises the capacity of the, the hypothesis space.
I mean, it sounds awkward to implement, uh,
because, you know, things fitting into a fixed space in your GPU.
Um, but I think that might be possible,
I suppose you'd would have to learn the criterion on which you increase beam,
beam size, yeah. Seems possible.
Okay. So we've talked about, uh,
beam search and greedy decoding.
So here's a new family of decoding
algorithms which are pretty simple, uh, sampling-based decoding.
So something which I'm calling pure sampling because I didn't know what else to call it.
Um, this is just the,
the simple sampling method that says that on each, uh,
timestep of your decoder t,
you just want to randomly sample from the probability distribution,
uh, to obtain your next word.
So this is very simple.
It's just like greedy decoding.
But instead of taking the top words,
instead just sample from that distribution.
So the reason I call this pure sampling was to differentiate it from top-n sampling.
And again, this is actually usually called top-k
sampling but I already called k the beam size,
and I didn't want to be confusing, so I'm gonna call it top-n sampling for now.
Um, so the idea here is also pretty simple.
On each step t,
you want to randomly sample from your probability distribution but
you're gonna restrict to just the top n most probable words.
So this is saying that it's,
it's like the simple, you know,
pure sampling method but you want to truncate your probability distribution just to be,
you know, the, the top most probable words.
So, uh, the idea here kind of like how beam search, um,
gave you a hyperparameter is kind of go between greedy decoding and,
you know, uh, a very exhaustive search.
In the same way here, you've got a hyperparameter n
which can take you between greedy search and pure sampling.
If you think about this for a moment,
if n is one, then you would truncate it the top one.
So you're just taking arg max which is greedy.
And if n is vocab size,
then you don't truncate it at all.
You're sampling from everything,
that's just the pure sampling method.
So here, um, it should be clear, I hope,
if you think about that if you increase n,
then you're gonna get more diverse and risky output, right?
Because you're, uh, giving it more,
more to choose from and you're going lower into the probability distribution,
going lower into less likely things.
And then, if you decrease n,
then you're gonna get more kind of generic safe output because you're
restricting more to the most high probability options.
So both of these are more efficient than
beam search which I think is something important to note,
uh, because there are no multiple hypotheses to track, right?
Because in beam search, on every step t of the decoder,
you've got k different, you know,
beam size, many hypotheses to track.
Uh, whereas here, at least if you're only generating one sample,
there's only one thing to track.
So it, it's a very simple algorithm.
So that is one advantage of these sampling-based algorithms over beam search.
Okay. So, the last thing I want to tell you that's kind of related to decoding is,
uh, softmax [NOISE] temperature.
So, if you recall on timestep t of your decoder,
your language model computes some kind of probability distribution P_t, uh,
by applying the softmax function to a vector of scores that you got from somewhere.
Like from your transformer or from your RNN or something.
So, there's the softmax function again.
It's saying that the probability of a word W is this softmax function,
uh, given, given the scores.
So, the idea here of a temperature on the softmax is that you have some kind of
temperature hyperparameter tau and you're going to apply that to this, uh, softmax.
So, all that we're doing is we're div- dividing all of the scores,
or logits you might call them,
by the temperature hyperparameter.
So again, if you just think about this a little bit,
you'll see that raising the temperature,
that is increasing, uh,
the hyperparameter, this is going to make your probability distribution more uniform.
And this kind of comes down to the question about when you,
when you multiply all of your scores by a constant,
um, how does that affect the softmax, right?
So, do things get more far apart or less far apart once you take the exponential?
So, this is something you can just work up by yourself on paper,
but as a, uh,
a kind of a memory shortcut,
a good way to think about it is that if you raise the temperature,
then the distribution kind of melts and goes soft and mushy and uniform.
And if you, uh,
lower the temperature, like make it cold then,
the probability distribution becomes more spiky, right?
So, like the things which are rated as high probability become like even more,
uh, disproportionately high probability compared to the other things.
Um, I think that's a easy way to remember it.
Today I had to work it out on paper and then, uh,
I realized that just the, the,
the temperature visualization thing usually gets me there quicker.
So, um, one thing I want to note is that softmax temperature is not a decoding algorithm.
I know that I put it in the decoding algorithm section,
uh, that was just because it's kind of a thing, a
simple thing that you can do at test time to change how the decoding happens, right?
You don't need to train, uh,
with the, the softmax temperature.
So, it's not a decoding algorithm itself.
It's a technique that you can apply at test time
in conjunction with a decoding algorithm.
So, for example, if you're doing beam search or you're doing some kind of sampling,
then you can also apply a softmax temperature, um, to change,
you know, this kind of risky versus safe, um, trade-off.
Any questions on this? Okay. So, here's
a summary of what we just learned about decoding algorithms.
Um, Greedy decoding is a simple method.
It gives kind of low quality output in comparison to the others, at least beam search.
Beam search, especially when you've got a high beam size, uh,
it searches through lots of different hypotheses for high-probability outputs.
And this generally is gonna deliver better quality than greedy search, uh,
but if the beam size is too high, then you can have these,
uh, kind of counter-intuitive problems we talked about before.
Where you've retrieved some kind of high-probability but unsuitable output.
Say, something is too generic or something is too short.
And we're gonna talk about that more later.
Uh, sampling methods are a way to get more diversity,
uh, via, via randomness.
Uh, well, getting randomness might be your goal in itself.
Um, so, this is good if you want to have some kind of, for example,
open-ended or creative generation setting like,
uh, generating poetry or stories,
then sampling is probably a better idea than
beam search because you want to have a kind of source of randomness to,
uh, write different things creatively.
And top-n sampling allows you to control the diversity by,
uh, changing n. And then lastly,
softmax temperature is another way to control diversity.
So there's quite a few different knobs you can turn here.
And it's not a decoding algorithm,
it's just a technique that you can apply alongside any decoding algorithm.
Although it wouldn't make sense to apply it with
greedy decoding because even if you make it more spiky or more flat,
the argmax is still the argmax, so it doesn't make sense.
Okay. Cool. I'm going to move on to section two.
So, uh, section two is NLG tasks and neural approaches to them.
Uh, as mentioned before, this is not going to be an overview of all of NLG.
That will be quite impossible.
This is gonna be some selected highlights.
So, in particular, I'm gonna start off with
a fairly deep dive into a particular NLG task that I'm a bit more familiar with,
and that is, uh, summarization.
So, let's start off with a task definition for summarization.
Um, one sensible definition would be: Given some kind of input text x,
you want to write a summary y which is shorter than
x and contains the main information of x.
So, summarization can be single-document or multi-document.
Uh, single-document means that you just have a summary y of a single document x.
In multi-document summarization, you're saying that you want to write
a single summary y of multiple documents x_1 up to x_n.
And here typically x_1 up to x_n will have some kind of overlapping content.
So, for example, they might all be different news articles
from different newspapers about the same event, right?
Because it kind of makes sense to write a single summary that draws from all of those.
Um, makes less sense to summarize things that are about different topics.
There is further, uh,
subdivision of, uh, task definitions in, in summarization.
So, I'm gonna describe it via some datasets.
Uh, here are some different really common datasets especially in, uh,
neural summarization, um, and they kind of correspond to different,
like, lengths and different styles of text.
So, a common one is,
uh, the Gigaword dataset.
And the task here is that you want to map from
the first one or two sentences of a news article to write the headline.
[NOISE] And you could think of this as sentence compression,
especially if it's kind of one sentence to headline because you're going from
a longish sentence to a shortish headline style sentence.
Uh, next one that I, um,
wanted to tell you about is this, uh,
it's a Chinese summarization dataset but I,
I see people using it a lot.
And it's, uh, from a micro-blogging,
um, website where people write summaries of their posts.
So, the actual summarization task is
you've got some paragraph of text and then you want to,
uh, summarize that into,
I think, a single sentence summary.
Uh, another one, uh, two actually,
are the New York Times and CNN/Daily Mail, uh, datasets.
So, these ones are both of the form,
you've got a whole news article which is actually pretty long like
hun-hundreds of words and then you want to summarize that into,
uh, like, maybe a single-sentence or multi-sentence summary.
Uh, The New York Times ones are written by, I think, uh,
librarians or people who, who,
um, write summaries for, for library purposes.
Uh, and then, uh,
one I just spotted today when I was writing this list is there's a new,
fairly new like last six months dataset from wikiHow.
So, from what I can tell this seems to be,
you've got a full how-to-article from wikiHow and then you want to boil this down to
the summary sentences which are kind of cleverly
extracted from throughout the wikiHow article.
They are kind of like headings.
So, um, I looked at this paper and it seems that, um,
this is kind of interesting because it's a different type of text.
As you might have noticed most of the other ones are news-based and this is,
uh, not, so that kind of poses different challenges.
Uh, another kind of division of summarization is sentence simplification.
So, this is a related but actually different task.
In summarization, you want to write something which is shorter and contains
main information but is still maybe written in just as complex language,
whereas in sentence simplification you want to rewrite the source text using simpler,
uh, simpler language, right?
So, like simpler word choices and simpler sentence structure.
That might mean it's shorter but not necessarily.
So, for example, uh,
simple Wiki- Wikipedia is a standard dataset for this.
And the idea is you've got, um, you know,
standard Wikipedia and you've got a simple Wikipedia version.
And they mostly align up,
so you want to map from
some sentence in one to the equivalent sentence in the [NOISE] other.
Another source of data for this is Newsela which is a website that,
uh, rewrites news for children.
Actually, at different learning levels I think.
So, you have multiple options for how much it's simplified.
Okay. So, um, so
that's the definition or the many definitions of summarization as different tasks.
So, now I'm gonna give an overview of, like,
what are the main, uh,
techniques for doing summarization.
So, there's two main strategies for summarization.
Uh, you can call them extractive summarization and abstractive summarization.
And the main idea as I had hinted out earlier,
is that in extractive summarization you're just selecting
parts of the original texts to form a summary.
And often this will be whole sentences but maybe it'll be more granular than that;
maybe, uh, phrases or words.
Whereas abstractive summarization, you're going to be
generating some new text using NLG techniques.
So the idea is that it's, you know, generation from scratch.
And my visual metaphor for this is this kind of like the difference between highlighting
the parts with a highlighter or writing the summary yourself with a pen.
I think the high level things to know about these two techniques are that
extractive summarization is basically easier,
at least to make a decent system to start,
because selecting things is probably easier than writing text from scratch.
Um, but extractive summarization is pretty restrictive, right?
Because you can't really paraphrase anything,
you can't really do any powerful sentence compression
if you can only just select sentences.
Um, and, of course, abstractive summarization as a paradigm
is more flexible and it's more how humans might summarize,
uh, but as noted it's pretty difficult.
So, I'm gonna give you a very quick view of what pre-neural summarization looks like.
And here we've got, uh,
this is a diagram from the, uh,
Speech and Language Processing book.
So, uh, pre-neural summarization systems were mostly extractive.
And like pre-neural NMT,
which we learnt about in the NMT lecture,
it typically had a pipeline which is what this picture is showing.
So, a typical pipeline might have three parts.
First, you have content selection which is, uh,
essentially choosing some of the sentences from the source document to include.
And then secondly, you're going to do some kind of information
ordering which means choosing what order should I put these sentences in.
And this is particularly a more nontrivial question if you were
doing multiple document summarization
because your sentences might come from different documents.
Uh, and then lastly,
you're going to do a sentence realization that is actually, um,
turning your selected sentences into your actual summary.
So, although we're not doing, kind of,
free-form text generation, uh,
there might be some kind of editing for example like, uh, simplifying, editing,
or removing parts that are redundant,
or fixing continuity issues.
So for example, you can't refer to
a person as she if you never introduced them in the first place.
So maybe you need to change that she to the name of the person.
So in particular [NOISE] uh,
these pre-neural summarization systems, uh,
have some pretty sophisticated algorithms of content selection.
Um, so, for example,
uh, you would have some sentence scoring functions.
This is the most simple, uh, way you might do it,
is you might score all of the sentences individually
and you could score them based on features such as,
um, are there, you know, topic keywords in the sentence?
If so, maybe it's an important sentence that we should include.
Um, and you could compute those, uh,
keywords using, uh, statistics such as tf-idf for example.
[NOISE] You can also use pretty basic but powerful features such as,
uh, where does the sentence appear in the document?
If it's near the top of the document,
then it's more likely to be important.
Uh, there are also
some more complex content selection algorithms such as for example, uh,
there are these graph-based algorithms which kind of view the document as
a set of sentences and those sentences are the nodes of the graph,
and you imagine that all sentences, er,
sentence pairs have an edge between them,
and the weight of the edge is kind of how similar the sentences are.
So, then, if you think about the graph in that sense,
then now you can try to identify which sentences are
important by finding which sentences are central in the graph.
So you can apply some kind of general purpose
gla- graph algorithms to figure out which [NOISE] nodes are central,
and this is a way to find central sentences.
Okay. So um, [NOISE] back to summarization as a task.
Um, we've, I can't remember if we've talked about ROUGE already.
We've certainly talked about BLEU.
But I'm gonna tell you about ROUGE now which is
the main automatic metric for summarization.
So ROUGE stands for Recall-Oriented Understudy for Gisting Evaluation.
I'm not sure if that was the first thing they came up with
or if they made it like that to match BLEU.
Um, and here's the,
here's the equation, uh,
for, well, I suppose one of the ROUGE metrics.
I'll tell you more about what that means later and you can
read more in the original paper which is linked at the bottom.
So, uh, the overall idea is that ROUGE is actually pretty similar to BLEU.
It's based on n-gram overlap.
So, some main differences with BLEU are ROUGE doesn't have a brevity penalty.
Um, I'll talk more about that in a minute.
Uh, the other big one is that ROUGE is based on recall while BLEU is based on precision.
So you can see it's there in the title.
[NOISE] Um, so, if you think about this a little bit,
I think you can say arguably precision is more important for machine translation.
That is, you only want to generate text that appears in one of your reference, uh,
translations, and then to avoid taking
a really conservative strategy where you only generate
really safe things in a really short translation.
That's why you add the brevity penalty to make sure
that [NOISE] it tries to write something long enough.
And then by contrast,
recall is more important for
summarization because you want to include all the information,
the info- the important information in your summary, right?
So the information that's in the reference summary is,
uh, assumed to be the important information.
So recall means that you captured all of that.
Um, and I suppose i- if you assume that you have
a maximum length constraint for your summarization system,
then those two kind of give a trade-off, right?
Where you want to include all the information but you can't be too long as a summary.
So I think that's the kind of justification why you have
recall and precision for these two different tasks.
However, confusingly, often an F1,
that is combination of precision and recall version of
ROUGE is reported anyway in the summarization literature.
And to be honest, I'm not entirely sure why this is, uh,
maybe it's because of the lack of,
uh, explicit max length constraint.
Um, anyway, I, I tried to search that but I couldn't find an answer.
So here's some more information on ROUGE.
Um, if you remember,
BLEU is reported as a single number, right?
BLEU is just a single number and it is
a combination of the precisions for the different n-grams
which is usually 1-4 whereas
ROUGE scores are usually reported separately for each n-gram.
So, the most commonly reported ROUGE scores are ROUGE-1, ROUGE-2 and ROUGE-L.
So, ROUGE one, not to be confused with Rogue One: A Star Wars Story.
Um, I feel like since that film came out,
I see so many people mistyping this, and I think it's related.
Um, so, ROUGE-1 is, uh,
based on unigram overlap,
um, [NOISE] and ROUGE-2 based on bigram overlap.
It's kind of an analogy to BLEU really except,
uh, recall-based, not precision-based.
The more interesting one is ROUGE-L which is longest common subsequence overlap.
Um, so, the idea here is that you are interested not only in, uh,
particular n-grams matching up but in,
you know, how many, uh, how,
how long a sequence of words can you find that appear in both.
So you can, uh, read more about these metrics
in the paper that was linked on the previous page.
And another really important thing to note is there's [NOISE] now
a convenient Python implementation of ROUGE, and um,
maybe it is not apparent why that's exciting,
but it's actually pretty exciting because for a long time,
there was just this Perl script, um,
that was quite hard to run and quite hard to set up and understand.
So um, someone out there has been a hero and has, uh,
implemented a pure Python version of ROUGE and checked that it
really does match up to the Perl script that people were using before.
So if any of you are using ROUGE or doing summarization for your projects, uh,
make sure that you, uh,
go use that because it will probably save you some time.
[NOISE] Okay.
So we're gonna re- return to ROUGE a little bit later.
Um, I know that in assignment 4 you thought about
the shortcomings of BLEU as a metric and um,
for sure ROUGE has some short- shortcomings as well as a metric for summarization.
Um, we're gonna come back to that later.
Okay. So, we're gonna move on to neural approaches for summarization.
[NOISE] So uh, going back to 2015,
I don't have another dramatic reenactment, I'm afraid.
[NOISE] Um, Rush et al.
published the first seq2seq summarization paper.
[NOISE] So uh, they were viewing this as,
you know, NMT has recently been super successful,
why don't we view abstractive summarization as a translation task and
therefore apply standard translation seq2seq methods to it.
So that's exactly what they did and they applied,
uh, a standard attention model,
and then they did a pretty good job at, uh, Gigaword summarization.
That's the one where you're, um,
converting from the first sentence of the news article to the headline.
So it's kind of like, uh, sentence compression.
So crucially, this is kind of the same order of magnitude of length as NMT, right?
Because NMT is sentence to sentence and this is kind of sentence to sentence,
maybe at most two sentence two sentence.
So this works pretty well and you can get pretty decent, um,
headline generation or sentence compression using this kind of method.
[NOISE] Okay.
So after that, since 2015,
there have been lots more developments in neural abstractive summarization.
And you can kind of um,
group together these developments in,
uh, a collection of themes.
So one theme is make it easier to copy.
Uh, this seems pretty obvious because in summarization, you know,
you're gonna want to copy every, quite a few words and even phrases,
but don't copy too much.
Uh, that's the other thing is that if you make it
too easy to copy, then you copy too much.
So, then there's other research showing how to prevent too much copying.
[NOISE] Uh, the next thing is some kind of hierarchical or multi-level attention.
So as I just showed,
the attention has been pretty key to, um,
abstractive neural summarization so far.
So there's been some work looking at, you know,
can we kind of make this attention work at a more kind of high-level,
low-level cost fine version so
that we can kind of maybe do our selection at the high-level and at low-level.
Another thing which is kind of related is having
some more kind of global content selection.
So if you remember when we were talking about the,
the pipelines pre-neural summarization,
they had these different content selection algorithms.
And I think you can say that,
um, kind of naive attention,
attention seq2seq is not necessarily
the best way to do content selection for summarization,
maybe you want a more kind of global strategy where you choose what's important.
It's not so apparent here when you're doing this small-scale summarization,
but if you imagine that you're summarizing
a whole news article and you're choosing which information,
kind of deciding on each decoder step,
what to choose doesn't seem like the most global strategy.
Er, what else have we got?
Uh, there's using, uh, Reinforcement Learning to directly maximize
ROUGE or other discrete goals you might
care about such as maybe the length of the summary.
Um, and I say discrete here because ROUGE is a non-differentiable,
uh, function of your generated outputs.
There's no, you know, easy way to differentiably
learn that during training in the usual way.
Uh, my last point on this list is the kind of theme of
resurrecting pre-neural ideas such as those graph algorithms that I mentioned
earlier and working them into
these new seq2seq abstractive neural systems and I'm sure there is more as well.
So, I'm gonna show you a few of these, um,
especially because even if you're not particularly interested in summarization,
a lot of the ideas that we're gonna explore here are actually kind of applicable
to other areas of NLG or just other areas of NLP deep learning.
So, the first thing on the list is making it easier to copy,
which seems like probably the first thing you want to fix,
if you've just got basic seq2seq with attention.
So, um, a copy mechanism,
which can exist outside of summarization.
The reason, why you want this is that basic seq2seq with attention,
they're good at writing fluent output, as we know,
but they are pretty bad at copying over details like rare words correctly.
So a copy mechanism is just the kind of sensible idea of saying,
um, let's have an explicit mechanism to just copy over words.
So for example, you could use
the attention distribution to- to kind of select what you're going to copy.
Um, so, if you are allowing both copying
over words and generating words in the usual way with your language model,
then now you've got a kind of hybrid extractive/abstractive approach to summarization.
So, there are several papers, which are- which propose
some kind of copy mechanism variants and I think,
the reason why there is multiple is because there's
kind of a few different choices you can make about how to implement this,
and that means that there's a few different versions of how to implement copy mechanism.
So, uh, yeah, there are several papers here which you can look at.
I'm going to show you a diagram from a paper that um,
I did a few years ago with Chris.
So, this is just one example of how you can do a copying mechanism.
So, the - the way we did it,
is we said that on each decoder step,
you're going to calculate this probability Pgen and that's
the probability of generating the next word rather than copying it,
and the idea is that this is computed based on your current kind of context,
your current decoder hidden state.
So, then once you've done that,
then the idea is you've got your attention distribution as
normal and you've got your kind of output,
you know, generation distribution as normal and you're going to use this Pgen,
which is just a scalar.
You can use that to kind of, uh, combine,
mix together these two probability distributions.
So, what this equation is telling you,
is that saying that the uh,
final output distribution for uh,
what word is gonna come next,
it's kind of saying, you know,
it is the probability of generating
times your probability distribution of what you would generate
but then also the probability of copying
and then also what you're attending to at that time.
So, the, the main thing is, you're using attention as your copying mechanism.
So, attention is kind of doing double-duty here.
It's both uh, being useful for the generator to,
you know, uh, maybe choose to rephrase things
but it is also being useful as a copying mechanism.
And I think that's one of the several things that these different papers do differently.
I think, I've seen a paper that maybe has like two separate uh,
attention distributions, one for the copying and one for the attending.
Um, other choices you can make differently are for example,
D1 Pgen to be this kind of soft thing that's between zero and
one or do you want it to be a hard thing that has to be either zero or one.
Um, you can also make decisions about like
do you want the Pgen to have supervision during training?
Do you want to kind of annotate your data set saying these things are copied, things,
these things are not, or do you want to just like learn it end-to-end?
So there's multiple ways you can do this and um,
this has now become pretty, pretty standard.
Okay, so copy mechanism seems like,
seems like a sensible idea but there's a big problem with them,
which is what I mentioned earlier and that problem is,
that they copy too much.
Um, so, when you- when you run these kind of systems on summarization,
you find that they end up copying a lot of
long phrases and sometimes even whole sentences and uh,
unfortunately your dream of having an abstractive summarization system,
isn't going to work out because your, um,
you know, copy augmented seq2seq system has just
collapsed into a mostly extractive system, which is unfortunate.
Another problem with these uh,
copy mechanism models is that they are bad at
overall content selection especially if the input document is long,
and this is what I was hinting at earlier.
Um, let's suppose, that you are summarizing something that's quite
long like a news article that's hundreds of words long and you,
you want to write a several sentence summary.
It doesn't seem like the kind of smartest choice to
on every step of writing your several sentence summary,
but you're choosing again what to attend to,
what to select, what to summarize.
It seems better to kind of make a global decision at the beginning and then summarize.
So, yeah, the problem is, there's no overall strategy for selecting the contents.
So, uh, here's a paper that I found. Nope, not yet.
Okay. So, how might you do better content selection for neural summarization?
So, if you remember in this pre-neural summarization we looked at,
you had completely separate stages in the pipeline, right?
You had the content selection stage and you had
a surface realization that is the text generation stage.
But in our seq2seq attention systems,
these two stages are just completely mixed together, right?
You're doing your step-by-step surface realization that is text generation,
and then on each of those, you're also doing content selection.
So, yeah, this doesn't make sense.
So, I found a paper, which is,
uh, published I think last year,
which gives a quite nice kind of
simple solution to this problem and it's called bottom-up summarization.
So, in this paper if you look at the- if you look at the figure,
uh, the main idea is pretty simple.
It says that, first you're going to have a content selection stage and this is
just uh, thought of as a neural sequence tagging model problem, right?
You run through your source documents and
you kind of tag every word as include or don't include.
So, you're just kinda deciding like what seems important,
what seems like it should make it into the summary and what
doesn't and then the bottom-up attention stage says that,
now you'll seq2seq with an attention system,
which is gonna generate the summary.
Are you're gonna kind of apply a mask?
You know, apply a hard constraint that says,
that you can't attend to words that were tagged don't-include.
So, this turns out to be pretty simple but effective um,
because it's a better overall content selection strategy because by doing
this first content selection stage by sequence-tagging you're kind of just,
just doing the selection thing without also at the same time doing the generation thing,
which I think turns out to be a better way to make
better decisions about what to include and then separately,
this also means as a great side effect,
you have less copying of long sequences in the generation model.
Um, because if you are not allowed to attend to things,
which you shouldn't be including,
then it's kind of hard to copy a really long sequence, right?
Like if you want to copy a whole sentence but the sentence has
plenty of don't include words in it,
then you can't really copy a long sequence, you have to break it up.
So, what the model ends up doing,
is it kind of has to skip,
skip around the parts that is meant to include and then it's forced to
be more abstractive to put the parts together. Yep.
How did they backpropagate the masking decision because it seems like-
Because during training [inaudible] masking decision.
Yeah, I think it might be trained separately.
I mean, you can go and check the paper.
I've, I've read a lot of papers in the last days, I can't quite remember.
I think, it might be trained separately but they might
have tried training it together but it didn't work as well.
I am not sure. You can check it out.
Okay. So, another paper I want to tell you about is a paper which uh,
used reinforcement learning to directly maximize ROUGE for neural summarization.
So this was a paper from two years ago.
And the main idea is that they can use RL to
directly optimize in this case ROUGE-L, the metric.
So by contrast, the standard maximum likelihood of training that is
the training objective we've been talking about for
the whole class so far for language models uh,
that can't directly optimize ROUGE-L because it's a non-differentiable function.
So they uh, they use this RL technique
to compute the ROUGE score during training and then uh,
use a reinforcement learning to backprop to the model.
So, the interesting finding from this paper is that if they just used the RL objective,
then they do indeed get higher ROUGE scores.
So they can successfully optimize
this ROUGE-L metric that they were aiming to
optimize but the problem is that when you do that,
you get lower human judgment scores.
So, on the right we're seeing that the RL only model has
actually pretty pretty bad readability relevance human judgment scores.
It's worse than just the maximum likelihood supervised training system.
So, this is a quote from their blog post that says,
"We have observed that our models with the highest ROUGE scores
also generated barely readable summaries."
So, this is- this is,
um, I suppose a problem, right?
If you try to directly optimize for the metric,
then you might start finding that you're kind of gaming
the metric and not optimizing for the true task, right,
because we know, just as we know that BLEU was not really a perfect analogy to
actual translation quality so is ROUGE
not a perfect analogy to uh, summarization quality.
But they did do something cool, which is that they found that if
you combine the two objectives,
so they kind of, uh, you know,
predict the language model sequence objective and then they also like produce
an overall summary that gets a high ROUGE score objective and you combine them together,
then you can get a better human uh, judgment score,
which in the end is the closest thing we have to uh,
a measure of actual summarization quality.
[NOISE] Okay.
So, I'm gonna move on to uh, dialogue,
which is um, a different NLG, kind of family of tasks.
Uh, so, really dialogue encompasses a really large variety of settings.
And we are not going to cover them all,
but here is a kind of overview of all the different kinds
of tasks that people might mean, when they say dialogue.
Um, so, there's task-oriented dialogue and this kind of refers to any setting,
where you're trying to kind of get something done in the conversation.
So, if for example, you've got kind of
assistive tasks where it's assumed that you have, you know, maybe the uh,
the dialogue agent is trying to help a human user to do
something like maybe giving customer service or recommendations,
answering questions, helping a user,
you know, accomplish a task like buying or booking something.
Uh, these are the kinds of tasks, which the virtual systems on
your phone can do or can kind of do.
Um, another family of task-oriented dialogue tasks are cooperative tasks.
So, this is kind of anything where you've got two agents who are
trying to solve a task together via dialogue.
Um, and the opposite of that would be adversarial.
So anything where you have two agents who are trying to compete in a task and that uh,
competition is conducted through dialogue.
[NOISE] So uh, the opposite to task-oriented dialogue is, uh, social dialogue.
So that's something where there is no explicit task other than to, I suppose socialize.
So chit-chat dialogue, um,
is just dialogue where you're just doing it for social fun or for company.
Um, I've also seen some work on kind of like therapy or mental well-being dialogue,
I'm not sure if this should go in task or social,
it's kind of a mix, uh,
but I suppose these are the ones where the goal is to
maybe offer kind of emotional support to the human user.
Um, so as a very kind of brief overview of how,
uh, the deep learning, uh,
renaissance has kind of changed dialog research, um,
I think you can say that in kind of pre-deep learning,
uh, the difficulty of open-ended,
free-form natural language generation, meant that, uh,
dialogue systems were often,
uh, not doing free-form NLG.
They might use predefined templates meaning that you have
a template where you just fill in some slots with the content, uh,
or maybe you retrieve an appropriate response from
a corpus of responses that you have in order to find,
you know, an appropriate response for the user.
And these are by no means simple systems,
they had some very complex things going on like deciding, you know,
what their dialogue state is and what template you should use and so on and the-
all the natural language understanding components of understanding the context so far.
But, uh, one effect that,
that deep learning had is that, uh,
since again kind of 2015 which is when NMT, uh,
became standard, there's been, uh,
just like summarization, lots of papers applying seq2seq methods to dialogue.
And this has kind of led to a renewed interest in open-ended, free-form dialogue systems.
So uh, if you wanna have a look at what did
those early seq2seq dialogue papers look like,
um, here's two kind of early ones like maybe the first ones to apply seq2seq.
Okay. So uh, people quickly applied seq2seq, uh,
NMT methods to dialogue but it quickly became
very apparent that this kind of naive application of
standard NMT methods has
some serious pervasive deficiencies when applied to a task like chitchat dialogue.
And this is even more true than it was for summarization.
So what are some examples of these serious pervas- pervasive deficiencies?
Uh, one would be genericness or boring responses,
and I'll go into more detail about these in a moment.
Another one is irrelevant responses.
So that's when, uh,
the dialogue agent kind of says something back
that's just kind of unrelated to what the user says.
Um, another one is repetition,
this is pretty basic but it,
uh, it happens a lot.
Um, so that's also repetition within the utterance and maybe repetition across utterances.
Ah, another difficulty is,
uh, kind of lack of context,
like not remembering the conversation history.
Obviously, if you do not condition on the whole conversation history,
there's no way your dialogue agent can use it but it is a challenge especially if you
have a very long dialogue history to figure out how to condition on it effectively.
Another problem is the lack of consistent persona.
So if you kind of, uh,
naively as in maybe those two papers that I referenced on the previous slide,
if you naively train a kind of standard seq2seq model to maybe take the, uh,
you know the user's last utterance and then say something back,
or maybe even the whole dialogue history and say something back.
Often your dialogue agent will have this completely inconsistent persona,
like one moment they will say that it lives in Europe and then it'll say it lives in,
I don't know, China or something and it just doesn't make sense.
So I'm gonna go through, uh,
some of these problems and give you a bit more detail on them.
So first, this irrelevant response problem.
So in a bit more detail, your problem is that seq2seq often
generates some response that's kind of unrelated to the user's utterance.
So it can be unrelated because it's simply generic,
which means that this is kind of like an overlap with
a generic response problem or it can be
kind of unrelated because the model's choosing to kind of change,
to change the subject to something unrelated.
So one solution of many, there,
there are a lot of different papers which, uh,
kind of attack this irrelevant response problem, uh, but just one,
one for example is, uh,
that you should tr- change the training objective.
So instead of trying to optimize, um,
mapping from input S to response T such that
you're maximizing the conditional probability of T given S,
instead you should maximize the maximum mutual information.
So that's why this is here.
So maximum mutual information, uh,
you can kind of rewrite the objective like this,
and if you want to see some more detail you can go look at this paper here.
But the idea is that you're trying to find your response T that kind of, uh,
maximizes this thing which is kind of like saying,
it needs to be probable given the inputs but
kind of like as a ratio of its probability in itself.
So if T is very high likelihood,
then it gets penalized and it's kind of like about the ratio
of the probability given the input and it's just the stand-alone probability.
So the idea is that this is meant to discourage, um,
just saying generic things that just have a high PT by themselves.
Um, so that's the irrelevant response problem.
And as I just hinted at, there's, uh,
definitely a strong link between
the irrelevant response problem and the kind of generic or boring response problem.
So to look at the genericness or boring response problem.
[NOISE] So I think
there are some pretty easy fixes that you can make to,
to a degree ameliorate the boring response problem.
Whether you're really getting to the heart of the issue is a different question.
But some kind of easy test-time fixes that you can certainly do are for example,
you can just directly up-rate, up-weight rare words during beam search.
So you can say, all rare words kind of get a boost to their, uh,
log probabilities and then now we're
more likely to produce them during beam search.
Another thing you could do is you could use for example,
a sampling decoding algorithm rather than beam search and we talked about that earlier,
um, or you could use, oh yeah,
you could use softmax temperature as well.
That's another thing. So those are
kind of test-time fixes and you could regard those as a kind of late intervention, right?
So an earlier intervention would be maybe training your model differently.
So I'm calling these kind of conditioning fixes because these fixes kind of relate to,
uh, conditioning your model on something that's gonna help it be less boring.
So one example is maybe you should condition
the decoder on some kind of additional context.
Uh, so for example, there's some work showing that, you know,
if you're doing chitchat dialogue, then maybe you should, uh,
go and sample some related words that are related to
what the user said and then just kind of attend to them when you
generate and then you're more likely to say something that's kind of content
full and interesting compared to the boring things you were saying before.
Ah, another option is you could train
a retrieve-and-refine model rather than a generate-from-scratch model.
So by retrieve-and-refine, I mean, uh,
you've- supposing you have some kind of corpus of,
of just general kind of utterances,
things that you could say and then maybe you sample one, uh,
from that test set,
th- the training set,
and then you edit it to fit the current situation.
So it turns out that this is a pretty strong method to produce
much more kind of diverse and human-like and interesting utterances, um,
because you can get all of that kind of fine grain detail from the sampled,
ah, utterance and then you edit it as necessary to fit your current situation.
So I mean, there are downsides to these kinds of methods like maybe it can be
hard to edit it to actually appropriately fit the situation,
um, but it's certainly a way to effectively get like
some more diversity and, um, interest in that.
So on the subject of the repetition problem,
that was another kind of major problem we noticed for,
um, applying seq2seq to, uh, chitchat.
Um, again, there are kind of simple solutions and more complex solutions.
Um, so a simple solution is you could just block repeating
n-grams during beam search and this is usually really quite effective.
And what I mean by that is, uh,
during beam search when you're kind of considering,
you know, what are my K hypotheses?
Which is just kind of the top K in the probability distribution, you say,
well, anything that would constitute a repeating n-gram just gets thrown out.
So when I say constitutes a repeating n-gram,
I mean if you did take that word,
would you now be creating a repeating let's say two-gram, bigram and, um,
if we're deciding that we're banning all repeating bigrams or trigrams or whatever,
then you essentially just have to check for every possible word that you might
be looking at in beam search and whether that would create a repeating n-gram.
So this works pretty well, I mean,
it's by no means a kind of principled solution, right?
If feels like we should kind of have a better way to learn not to repeat, um,
but as a kind of, uh,
effective hack, I think that works, that works pretty well.
So the more complex solutions are,
for example, you can train something called coverage mechanism.
Um, so in seq2seq, and this is mostly, uh,
inspired by the machine translation setting, uh,
a coverage mechanism is a kind of objective that prevents
the attention mechanism from attending to
the same words multiple times or too many times.
And the intuition here is that, uh,
maybe repetition is caused by repeated attention.
So if you attend to the same things many times,
then maybe you're gonna repeat,
you know, the same output many times.
So if you prevent the repeated attention,
you prevent the repeated output.
So this does work pretty well but it's definitely,
um, more of a complex thing to implement,
it's less convenient and,
um, I don't know,
in some settings, it does seem like the simple solution is,
uh, easier and works just as well.
Uh, so other complex solutions
might be you could define a training objective to discourage repetition.
Uh, this cou- you could try to, um,
define something differentiable but one of the,
the difficulties there is that because you're training with a teacher forcing, right?
Where you're always like looking at the,
the gold inputs so far,
then you never really do the thing where
you generate your own output and start repeating yourself.
So it's kind of hard to define the penalty in that situation.
So maybe this needs to be a kind of non-differentiable function.
So kind of like how,
um, the Paul et al paper was, uh,
optimizing for ROUGE, maybe we kind of, uh,
optimize for not repeating which is a discrete function of the input.
Uh, I'm going to skip ahead to storytelling.
So in storytelling, uh,
there's a lot of interesting neural storytelling work going on right now.
And most of it uses some kind of prompt to write a story.
So for example, uh,
writing a story given an image or given a writing prompt
or writing the next sentence of the story given the story so far.
So, uh, here's an example of generating a story from an image.
And what's interesting here is that we have this image which
is a picture of what appears to be an explosion and
then here you have
a story about the image but written in the style of Taylor Swift lyrics.
So it says, you have to be the only light bulb in the night sky I thought,
oh god, it's so dark out of me that I missed you, I promise.
And what's interesting here is that there wasn't any straightforward, supervised,
you know, image-captioning data set of explosions and Taylor Swift lyrics.
Um, they kind of learned this, uh, separately.
So how they did this is that they used a kind of common sentence encoding space.
So they used this particular kind of sentence encoding called
skip-thought vectors and then they trained,
um, this COCO image-captioning, uh,
system to go from the image to the encoding of
the sentence and then separately they also trained,
uh, a language model, a conditional language model to go from
the sentence-encoding to the Taylor Swift lyrics.
And then because you had this shared encoding space,
you can now put the two together and then go from the picture,
to the embedding, to the Taylor Swift style output,
which I think is pretty, pretty amazing.
Wow, I've really lost, lost track of the time.
So I, I think I have to hurry up quite a lot.
So, um, we've got some really impressive story,
generation systems, recently, um,
and this is an example of,
uh, a system which,
uh, prepares a new data set,
where you write a story given a prompt,
and they made this very impressive,
very beefed-up, uh, convolutional language model,
seq-to-seq system that generates the story given the input.
I'm not gonna go through all these details,
but I encourage you if you want to check out, uh,
what's the state of the art in story generation, you should check this out.
There's a lot of different interesting things going on with
very fancy attention and convolutions and so on,
and they managed to generate some really interesting, um, impressive stories.
So here, if you look at this example,
we've got some really interesting, um, kind of,
uh, story generation that's kind of diverse, it's non-generic,
it's stylistically dramatic which is good,
and is related to the prompts.
Um, but I think you can see here kind of the limits of what
the state of the art story generation system can do which is that- um,
although it's kind of in style,
it's mostly kind of atmospheric and descriptive.
It's not really moving the plot forward.
There's no kind of events here, right?
Um, so the problem is it gets even worse when you generate for longer.
When you generate a long, a long text,
then it will mostly just stay on the same idea without moving forward with new ideas.
Okay. So I'm gonna skip forward a lot and,
uh, sorry, ought to have planned better.
There's a lot of information here which you wanna check
out about poetry generation and other things.
I'm going to skip ahead because I want to get to
the NLG evaluation section because that's pretty important.
So, um, we've talked about Automatic Evaluation Metrics fr NLG,
and we know that these words overlap based metrics, such as BLEU,
and ROUGE, and METEOR, uh,
we know they're not ideal for machine translation.
Ah, they're kind of even worse for summarization mostly
because summarization is even more open-ended than machine translation.
And that means that having this kind of rigid notion,
if you've got to match the N-grams,
is even less useful.
And then for something even more open-ended like dialogue,
then it's just kind of a disaster.
It's not even a metric that gives you a good signal at all,
and this also applies to anything else open-ended, like story generation.
So it's been shown, and you can check out the paper at the bottom,
that word overlap metrics are just not a good fit for dialogue.
So the orange box is showing you, uh,
some plots of the correlation between human score on a dialog class and BLEU-2,
some variation of BLEU.
And the prob- the problem here is you're not seeing much of a correlation at all, right?
It seems that particularly on this dialogue setting, ah,
the correlation between the BLEU metric and
the human judgment of whether it's a good dialogue response is,
uh, the correlation is- I mean,
it looks kind of non-existent.
It's at least very weak.
So that's pretty unfortunate and there's some other papers that show much the same thing.
So you might think, "Well,
what other automatic metrics can we use?
"What about perplexity?
Um, so perplexity certainly captures how powerful your language model is,
but it doesn't tell you anything about generation.
So for example, if your deca- decoding algorithm is bad in some way,
then perplexity is not gonna tell you anything about that, right?
Because decoding is something you apply to your trained language model.
Perplexity can tell if you've got a strong language model or not,
but it's not gonna tell you, um,
necessarily how good your generation is.
So some other thoughts you might have about automatic evaluation are,
well, what about word embedding based metrics?
Uh, so the main idea with word embedding based metrics,
uh, you want to compute the similarity of the,
the word embeddings or maybe the average of the word embeddings across a sentence,
not just the overlap of the words themselves.
Um, so the idea is that rather than just being
very strict and saying only the exact same word counts,
you say, "Well, if the words are similar and in word embedding space, then they count."
So this is certainly more flexible, but unfortunately, uh,
the same paper I showed before shows that this doesn't
correlate well either with human judgments of quality,
at least for the- the dialogue task they are looking at.
So here, the middle column is showing the correlation between human,
judgments, and some kind of average of word embedding based metric.
So, um, yeah, that doesn't look great either,
not a great correlation.
So if we have no automatic metrics to adequately
capture overall quality for natural language generation,
um, what, what can we do instead?
So I think often the strategy is,
you end up defining some more kind of focused automatic metrics to
capture the particular aspects of the generated text that you might be interested in.
Um, so for example, you might be interested in, uh, fluency,
and you can compute that by just kind of running
a well-trained language model over your text and generating the probability,
and that's kind of a proxy for how well it's written, you know, good, fluent, grammatical text.
Um, if you're particularly interested in maybe generating text in a particular style,
then you could ta- take a language model that's
trained on the corpus representing that style,
and now the probability tells you not only is it a good text,
but is it in the right style.
Um, there are some other things as well that are like,
you know, diversity, um,
and you can can that pretty easily by just having some statistics about,
you know, how much you're using rare words.
Um, relevance to input,
you can kind of compute a similarity score with the input,
and there are just some simple things like, you know,
length and repetition that you surely can count, and yes,
it doesn't tell you overall the overall quality,
but these things are worth measuring.
So I think my main point is that yes,
we have a really difficult situation with NLG evaluation.
There's no kind of overall metric.
Often, they capture this overall quality.
Um, but if you measure lots of these things,
then they certainly can help you track some important things that you should know.
So we talked about how automatic evaluation metrics for NLG are really tough.
So let's talk about human evaluation.
Uh, human judgments are regarded as the gold standard, right?
But we already know that human evaluation is slow and expensive,
uh, but are those the only problems with human eval?
Let's suppose that you do have access, uh, to,
let's say, the time or money you need to do human evaluations.
Um, does that solve all your problems?
Suppose you have unlimited human eval,
does that actually solve your problems?
And my answer is, uh, no.
And this is kinda from personal experience.
Um, conducting human evaluation in itself is very difficult to get right.
It's not easy at all, and this is partially because humans do a lot of weird things.
Humans, uh, unlike a metric, uh,
an automatic metric, they're inconsistent,
they could be illogical.
Sometimes, they just get bored of your task,
and they don't really pay attention anymore.
Uh, they can misinterpret the question you asked,
and sometimes they do things they can't really explain why they did it.
So, um, as a kind of case study of
this I'm going to tell you about, um,
a project I did where I was,
uh, building some chatbots,
and it turned out that the human evaluation was kind of the hardest part of the project.
So I was trying to build these chatbots for the Persona-Chat data
set and in particular investigating controllability.
So we're trying to control aspects of the generated texts such as, you know,
whether you repeat itself,
how generic you are,
kind of these same problems that we noted before.
So we built these models that control, you know,
specificity of what we're saying and
how related what we're saying is to what the user said.
So here you can see that,
you know, uh, our partner said something like, "Yes,
I'm studying law at the moment," and we can kind of control-
turn this control knob that makes us say something very generic like,
"Oh," and then like 20 dots or something
just completely bonkers that's just all the rare words you know.
And there's like a sweet- a sweet spot between what you say,
"That sounds like a lot of fun. How long have you been studying?"
And then similarly, we have a knob we can turn to,
uh, determine how semantically related what we say is to what, what they said.
So, um, you know, that's kind of interesting.
It's, it's a way to control the output of the, uh, NLG system.
But actually, I want to tell you about how the human evaluation was so difficult,
so we have these systems that we wanted to generate using human eval.
So the question is, how do you ask for the human quality judgments here?
Uh, you can ask kind of simple overall quality questions,
like, you know, how well does the conversation go?
Was- was the user engaging?
Um, or maybe comparative,
Which of these users gave the best response? Uh, questions like this.
And, you know, we tried a lot of them,
but there were just major problems with all of them.
Like, these questions are necessarily very subjective and also,
the different respondents have different expectations,
and this affects their judgments.
So for example, if you ask, do you think this user is a human or a bot?
Then, well, that depends entirely on
this respondents' knowledge of bots or opinion of bots and what they think they can do.
Another example is, you'd have kind of catastrophic misunderstanding of the question.
So for example, if we ask,
was this user- was this chatbot engaging?
Then someone responded saying, "Yup,
it was engaging because it always wrote back",
which clearly isn't what we meant.
We meant like are they an engaging conversation partner,
but they took a very literal assumption,
uh, of, of what engaging means.
So the problem here is that overall quality depends on many underlying factors,
and it's pretty hard to kind of find a single,
overall question that captures just overall quality.
So we ended up doing this, we ended up breaking this down
into lots more kind of factors of quality.
So, uh, the way we saw it is that,
you have maybe these kind of overall measures of quality of the chatbot,
such as how engaging was it,
how enjoyable was it to talk to,
and kind of maybe how convincing was it that it was human.
And then below those,
we kind of broke down as these more low level, uh,
components of quality such as,
you know, uh, were you interesting?
Were you li- showing that you were listening?
Were you asking enough questions and so on?
And then below that, we had these kind of controllable attributes which
were the knobs that we were turning and then the goal was to figure out,
um, how these things affected the output.
Um, so let's see.
Um, so we had a bunch of findings here, and I think,
maybe the ones which I will highlight were,
uh, these two kind of in the middle.
So the overall metric engagingness,
which means enjoyment, that was really easy to maximize.
It turned out, uh,
our bots managed to get near human performance in terms of engagingness.
Um, but the overall metric humanness,
that is the kind of Turing test metric,
that was not at all easy to maximize.
All of our bots were way,
way below humans in terms of humanness, right?
So we were not at all convincing of being human,
and this is kind of interesting, right?
Like, we were as enjoyable as talk to as humans,
but we were clearly not human, right?
So like, humanness is not the same thing as conversational quality.
And one of the interesting things we found in this,
um, study, where we not only evaluated our chatbots,
we also actually got humans to evaluate each other,
was that, um, humans are sub-optimal conversationalists.
Uh, they scored pretty poorly on interestingness, fluency, listening.
They didn't ask each other enough questions,
and this is kind of the reason why we managed to like approach
human performance in kind of enjoyableness to talk to you because we just,
for example, turned up the question asking knob, asked more questions,
and people responded really well to that because people like talking about themselves.
So, um, yeah.
I think this is kind of interesting, right?
Because it shows that there is no obvious just one question to ask, right?
Because if you just seemed, "Oh,
the one question to ask is clearly engagingness or it's clearly humanness,
then we would have gotten completely different reads on how well we were doing, right?
Whereas asking these multiple questions kind of gives you more of an overview.
I am going to skip this just because there's not a lot of time.
Okay. So, here's the final section.
Uh, this is my kind of wrap-up thoughts on NLG research,
the current trends and where we're going in the future.
So, here's kind of three exciting current trends to identify in NLG.
And of course your mileage may vary,
you might think that other things are more interesting.
So, uh, the ones which I was thinking about, are
firstly incorporating discrete latent variables into NLG.
Um, so, you should go check out
the slides I skipped over because there were some examples of this.
But the idea is that with some tasks such as for example
storytelling or task oriented dialogue
where you're trying to actually get something done.
Um, you probably want a more kind of
concrete hard notion of the things that you're talking about
like you know, entities and people and events and negotiation and so on.
So, uh, there's, there's mentioning what kind of modeling
these discrete latent variables inside these continuous, uh, NLG methods.
The second one is alternatives to strict left to right generation.
And I'm really sorry [LAUGHTER] I skipped over so many things.
Um, so, there's some interesting work recently in trying
to generate text in ways other than left to right.
So, for example there's some kind of
parallel generation stuff or maybe writing something and iteratively refining it, uh,
there's also the idea of kind of top-down generation, um, for
especially longer pieces of text like maybe tried to decide the contents
of each of the sentences separately before uh, writing the words.
And then a third one is like
alternatives to maximum likelihood training with teacher forcing.
So, to remind you, a maximum likelihood training with teacher forcing is
just the standard method of training
a language model that we've been telling you about in the class so far.
Um, so, you know,
there's some interesting work on looking at more kind of holistic,
um, sentence level rather than word level objectives.
Uh, so, unfortunately I ran out of time with
this slide, and I didn't have time to put the references in but I will
put the references in later and it
will be on the course website so you can go check them out later.
Okay. So, as a kind of overview, NLG research, where are we and where are we going?
Um, so my metaphor is I think that
about five years ago NLP and deep learning research was a kind of a Wild West.
Right? Like everything was new and um, we were unsure,
NLP research weren't sure what kind of what
the new research landscape was because uh, you know,
uh, neural methods kind of changed machine translation a lot,
looked like they might change other areas but it was uncertain how much.
Um, but these days you know five years later,
um, it's a lot less wild.
I'd say, you know things are settled down a lot kind of
standard practices have emerged and sure there's still a lot of things changing.
Um, but you know there's more people in the community,
there's more standard practices,
we have things like TensorFlow and PyTorch.
So, you don't have to take up gradients anymore.
So, I'd say things are a lot less wild now
but I would say NLG does seem to be one of the wildest parts
remaining and part of the reasons for that is because of
the lack of evaluation metrics that makes it so difficult to tell what we're doing.
It's, uh, quite difficult to identify like what are the main methods that are
working when we don't have any metrics that can clearly tell us what's going on.
So, another thing that I'm really glad to see is that
the neural NLG community is rapidly expanding.
Um, so, in the early years, uh,
people were mostly transferring successful NMT methods to various NLG tasks.
Uh, but now I'm seeing you know, increasingly more inventive NLG techniques
merging which is specific to the non-NMT generation settings.
Um, and again I urge you to go back into the slides that I skipped.
Um, so, I'm also saying there's increasingly more kind of
neural NLG workshops and competitions especially
focusing on open-ended NLG like those tasks that we
know are not well suited by the automatic metrics that work for NMT.
So, there's a neural generation workshop, a storytelling workshop uh,
and various challenges as well where people enter their for example, um,
conversational dialogue agents to be,
um, evaluated against each other.
So, I think that these different, um,
kind of community organizing workshops and
competitions are really doing a great job to kind of organize a community,
increase reproducibility and standard evaluate, standardized evaluation.
Um, so, this is great but I'd say
the biggest roadblock to progress is definitely still evaluation.
Okay. So, the last thing that I want to share with you
is eight things that I've learned from working in NLG.
So, the first one is the more open-ended the task,
the harder everything becomes.
Evaluation becomes harder, defining what you're doing becomes harder,
telling when you're doing a good job becomes harder.
So, for this reason constraints can sometimes make things more welcome.
So, if you decide to constrain your task then sometimes it's easier to, to complete it.
Uh, the next one is aiming for a specific improvement can
often be more manageable than aiming to improve overall generation quality.
So, for example, if you decide that you want to
well for example increase diversity for your model, like say
more interesting things that's an easier thing to achieve and measure than just
saying we want to do overall generation quality because of the evaluation problem.
The next one is if you're using your language model to do NLG,
then improving the language model that is getting better with perplexity will give you
probably better generation quality because you've got
a stronger language model but it's not the only way to improve generation quality,
as we talked about before, uh,
there's also other components that can affect generation apart from just language model,
and that's part of the problem is that that's not in the training objective.
Um, my next tip is that you should look at your output a lot,
partially because you don't have any single metric that can tell you what's going on.
It's pretty important to look at your output a lot to form your own opinions.
It can be time consuming but it's probably worth doing.
I ended up talking to these chatbots
a huge amount during the time that I was working on the project.
Okay. Almost done, so, five you need an automatic metric, even if it's imperfect.
So, I know you that already know this because we
wrote it all over the project instructions.
Uh, but I'd probably amend that to like maybe you need several automatic metrics.
I talked earlier about how you might track
multiple things to get an overall picture of what's going on,
I'd say the more open-ended your NLG task is,
the more likely you need probably several metrics.
If you do human eval, you want to make the questions as focused as possible.
So, as I found out the hard way if you
define the question as a very kind of overall vague thing,
then you're just opening yourself up to, um,
the respondents kind of misunderstanding you and, uh,
if they are doing that then it's actually not their fault,
it's your fault and you need to take your questions and that's what I learned.
Uh, next thing is reproducibility is
a huge problem in today's NLP and deep learning in general,
and the problem is only bigger in NLG,
I guess it's another way that it's still a wild west.
So, I'd say that, uh, it would be really great,
if everybody could publicly release all of
their generated output when they write NLG papers.
I think this is a great practice because if you released your generated outputs,
then if someone later let's say comes up with a great automatic metric,
then they can just grab your generated output and then compute the metric on that.
Whereas if he never released your output or you
released with some kind of imperfect metric number,
then future researchers have nothing to compare it against.
Uh, so lastly, my last thought
about working in NLG is that it can be very frustrating sometimes,
uh, because things can be difficult and it's hard to know when you're making progress.
But the upside is it can also be very funny.
So this my last slide, here are some bizarre conversations that I've had with my chatbot.
[LAUGHTER] Thanks.
[NOISE] [LAUGHTER] All right, thanks.
 Hi everybody, time to get started.
Okay. Um, so, so today what we're gonna talk about is a topic that's, um,
coreference resolution and I'll explain in just a minute what that is,
um, but before getting on to that just a,
uh, couple of words on the announcements.
Um, so the TAs are feverishly working on getting homework five grades worked out,
so we hope that we can deliver those to you, um,
tomorrow just in case you're anxious to know
them before you make your final decisions about things.
And then, the other thing that you should be remembering
is that the milestone for the final project is this Tuesday.
Now, I will confess that even to me it seems like,
"Boy, boy this milestone came around really quickly."
So you probably feel that doubly, I realize.
And so you know, I do apologize for that a little bit,
but you know, really our hope was that we could actually use this to be helpful,
and to give you feedback on what you're doing and suggestions,
and it just really seemed like, well,
the only chance in which we can kind of, um,
turn around giving more feedback on the projects, um,
before it goes into the final week of the quarter is if we can kind of get stuff,
um, Tuesday, and hope to be then,
sort of turning it around again by the end of the week.
So the hope is to help you not to just,
um, create obstacles and roadblocks in your life.
Okay. So today what we're gonna do, um, is, uh,
learn more about a linguistic topic for a change and learn
some more stuff about what goes on in coreference resolution.
So first of all, I'm gonna talk about the task,
and then go on to some of the kinds of models that people,
um, do for coreference resolution.
So first of all, what is it?
Um, so the idea of coreference resolution is what we do, which we have a text,
"Barack Obama nominated Hillary Rodham Clinton as his Secretary of State on
Monday," and this text like most texts are about entities,
where entities are commonly human beings,
but they can be other things like God saw talking giraffes or whatever it is.
So it seems like we want to make,
find where entities are mentioned.
So my entities are mentioned,
they're referred to as mentions.
So things like Barack Obama and Secretary of State,
he, her, they are mentions of entities.
And then, when we talk about coreference resolution,
the task that we're wanting to do is say,
which of these mentions refer to the same entity,
the same real thing in the world.
So well, one entity that's mentioned in this text is Barack Obama,
and then he's referred to later in the text as his and he,
and so these three red noun phrases are all coreferent to each other.
And that then, refers to this real-world entity.
Um, and then, we have these references Hillary Rodham Clinton,
Secretary of State, her,
she, First Lady, they're all references to a different entity.
And so they all refer to this person.
And so those are examples of our coreference.
Um, in a way this is triv- sort of seems obvious to a human being,
um, looking at things, um,
but it can actually be kind of tricky and hard.
Um, so, um, I thought we could spend a few minutes doing
interactive working out coreferents together so that you guys can,
um, think about it all for a few minutes.
Um, so here's part of a little story.
Um, it's a story by Shruthi Rao called The Star.
Um, now, I confess that since this is a CS class,
um, not a literature class,
I did a little bit of, um,
helpful editing of this text to make it shorter,
so I could fit more of,
what was going on, um,
onto the page, um, but, um,
everything that is a sort of a linguistic
[inaudible] is something that comes from the original text.
Okay. So, um, in this text,
um, who is the first entity that's mentioned?
Vanaja, okay.
Okay. So it's Vanaja.
Now, where, let's do it forward.
Where else is Vanaja mentioned in this text?
Her son, right?
So this her not the son,
but this her is a reference of Vanaja, right?
Um, she resigned.
Okay. After that?
She bought.
Okay. So there's another she.
Was there another reference before that?
Herself, right? So herself is also a reference to Vanaja.
Um, okay. So then, it's again,
she made this, she, okay.
So we've done Vanaja.
Okay, that's a good start.
Okay. So then, um, we've got Akhila.
Okay. Um, where's Akhila next referred to?
As Akhila. Okay, there we go.
Um, are there other references, um, to Akhila?
Maybe not. Okay. What's the next entity that's mentioned?
Prajwal.
Okay. So what other references are there to Prajwal?
They.
They? Okay. So here's a tricky one, right?
So this they, I mean,
who does that refer to?
It occ- refers to Prajwal and Akash.
Yeah, so this they refers both to Prajwal and this Akash.
So that's, that's something that happens in human languages.
This is referred to as split antecedents,
where you have one thing that they,
that's sort of referring to two distributed things that came before it.
Um, so here's one of my first sad admissions of natural language processing technology.
None of the NLP systems that we're gonna talk about later
today or in general that have been built deal with split antecedents.
They automatically lose as soon as there's split antecedents.
Um, so that's a bit sad,
um, but that's the state of technology.
So it's something, um,
we could still work to improve,
but okay there's this sort of they that's kind of half Prajwal. Um, okay.
So there's directly Prajwal here,
but was there another place early in the text that Prajwal is effectively mentioned?
Yeah. So Akhila's son is really another mention of Prajwal, right?
Okay. Um, okay.
Um, any other mentions of Prajwal? Maybe not.
Okay. Then we go on.
Okay. Who's the next entity?
Akash. So we have Akash here,
and that then again,
we have that her son referring to Akash.
Um, and here was Akash.
Okay. What other, what other mentions of Akash are there?
Okay so there's another Akash here, um, fourth him.
Okay. Uh, there's another Akash.
Okay, um, but, so, um, here.
Okay. So are the obvious Akash's.
There's sort of a tricky case here which
you could wonder what the right treatment of this, right?
You know, it's sort of says Akash was to be a tree, all right.
So in some sense the tree is Akash.
Um, so really in terms of reference in this story,
the reference of the tree is the same as Akash.
And you could think, um,
that means you should treat the instances of,
um, the tree, the,
the instances here of the tree,
and later on when the nicest tree right that really,
that's sort of this Akash as well.
That doesn't quite feel right,
but this is something that comes up in coreference, right?
So here we have a sort of a predictive construction um,
with, you know, B.
And when you set,
when you have sentences such as like, um, you know,
my child is the smartest kid in the class or something like that, in some sense,
you're sort of saying that the smartest kid in
the class has the same reference as my child.
And some systems count links over that kind of predication,
and say that is coreference whereas
other ones don't and think that that's not quite reasonable.
So different things go on.
Okay. So, um, those,
those are fair number of entities.
I mean, so there are obviously lots of other things that are mentioned,
um that sort of, um, right?
So there's the local park, right,
that's a mention of some entity.
Um, there's, um, the school, um right?
So there's this school here and so that the school is coreferent with pre,
the preschool right here, right?
Um, and then there's,
um, again this sort of tricky one,
of how to treat the naughty child Lord Krishna because,
you know, in some sense Prajwal is representing that.
And then there are lots of other entities that are mentioned, right?
There's a t-shirt, and there's trousers,
um, and, um, things like that.
Another tricky thing that turns up here when you get later on into
the story is you can have entities that have parts.
So we not only have a tree,
but that tree then has a lot of parts, right?
So the tree has a trunk,
and the tree has foliage,
um, and things like that.
And there are these red balls that are representing fruits, right?
So there's a lot of stuff that's somehow connected together and somehow separate.
And that sort of, that doesn't fit terribly well with the kind of models we
use with coreference either because really we make our coreference,
um, reference models basically out of this notion of entities.
Um, but somehow there's this complexity that,
you know, human beings have parts too, right?
We have hands and faces,
and we can't say, oh, that's a separate entity,
but they're somehow in, um, involved with the other entity.
Okay. Um, hope that's sort of useful to give some idea.
Why is coreference resolution useful?
Um, so there are all kinds of things that we'd like to do well
in natural language processing that you really can't do well unless,
uh, you know how to do coreference resolution.
So anything that we want to do in terms of question-answering, summarization,
extracting facts from texts or anything like that,
there are places we are gonna fail unless we can do coreference resolution.
Because if we're reading a piece of text,
and it says he was born in 1961, um,
we can get a fact out or answer a question,
if we can work out who he was,
but we probably can't otherwise.
Um, there are, there's sort of another place that where
this is very useful is in machine translation,
so that lots of languages drop pronouns.
So you don't have to give explicit pronouns,
but you need to be able to work out how to fill them in.
And this is making coreference decisions about,
um, arguments of verbs.
And so here are a couple of examples,
um, that, um, covering from Spanish to English.
So in Spanish, you can freely drop the subjects of verbs and in these sentences,
in the because clause,
there's no overt subject.
And so he gets Alicia likes Juan because he's smart.
And so Google Translate is stuck in a he and that is right.
And to stick in that he,
it's implicitly making a coreference decision and saying, "Okay well,
the subject of this, um,
adjective smart should be Juan who's male,
and therefore, I should say he."
But, you know, the reality is Google Translate knows
nothing about coreference and making these coreference decisions.
And as has been um,
covered quite a bit in the media now and I think came up earlier in an earlier class,
that, um, Google Translate mainly just defaults to male default.
Um, so if you sort of swap- sweep it, uh,
if you flip it around and say,
Juan likes Alicia, it also says because he's smart.
Uh, whereas probably it should be because she's smart in that case.
And indeed you notice the bad effects of that everywhere.
So many languages, um, Turkish, Indonesian, um,
don't actually have gender,
so that they're much less sexist languages than English,
French or Germany is.
But what happens, um,
when you then translate where you just have
a generic pronoun that means third person pronoun, um,
that Google Translate is essentially using its language model,
which means that reconstructs, um,
the worst of stereotypes of she is a cook,
and he is an engineer, he is a doctor.
And well, in a connected piece of this course,
if you'd like Google Translate to be able to do better than that,
well again, what would be required is that you could actually do
coreference resolution and track along the actors in the text as you go along.
Um, one final example we haven't really talked about yet,
but we'll get back to soon now because the class is almost over
is doing things with dialogue agents or chat systems.
That, as soon as you are going to do anything more than a single turn,
um, dialog, that you need to start dealing with reference.
So if you've got something like, um,
booked tickets to see James Bond,
um, then you want to say something like,
"Spectre is playing near you at 2:00 and 3:00 today.
How many tickets would you like?"
Um, two tickets for the showing at three.
That as shown in the color,
there are various kinds of reference going on here where things have related reference,
but it's kind of complicated here.
And this is something that we'll come back to in a moment.
So James Bond and Spectre aren't obviously the same thing,
but in a context like, um, booking movies,
they are the same thing because one is the name of a character in a movie series,
and the other is the name of a movie that's currently showing that belongs to that,
so that they're sort of associated, um,
in a sort of subtle way that isn't exact identity,
but is relevant to a lot of the things that we want to do.
I'll come back to that in a little bit when we
talk a bit more about the linguistics of this.
Okay. So if we want to do the task of coreference resolution,
there are essentially two steps.
So the first step is gee, we want to work out
what mentions there are in the text that we should be doing something with.
And this one is effectively pretty easy,
but I'll have just a few slides on that immediately.
And then what the bulk of the class is gonna be on is,
um, working out coreference between mentions.
And if you think about this,
coreference is essentially a clustering task.
Because if you do the first task,
you have a set of mentions and then you want to be saying well,
how can I group these into clusters that have the same reference?
And so that's what we're going to look more at doing.
So quickly on mention detection.
So, um, for mention,
we wanna find all the spans that are candidates for,
um, referring to some entity.
And the answer to what these, um,
candidates are is basically they're all the noun phrases in the text.
And so normally people think of there being three types of mentions that we identify.
There are pronouns, I,
you, he, she, it,
etc., that are, um,
referring to different entities.
They're explicit names of people like that was
that Barack Obama and Hillary Clinton examples.
And then many of the tricky examples,
and then when we have common noun phrases
like a dog or the big fluffy cat stuck in the tree.
That the big fluffy cat stuck in the tree is a mention.
Um, it's actually a complex mention because it
also has embedded inside it other mentions.
Um, so the tree is also a mention.
Okay. So how can we detect mentions?
Well, one answer is to say,
well we've looked at, um,
various other NLP systems on and off.
And we can just use those NLP systems as preprocessing systems to find mentions.
So for pronouns, they're part of speech taggers that say what's a noun,
or a verb, or a pronoun,
and so we can run those and find all the pronouns and we're done.
From- for, um, the names of things like Barack Obama.
We've talked a couple of times about named entity recognizers,
so we can run those and find all the named entities.
Um, then for common noun phrases,
that's sort of where we need parsers to find
the structure of the sentence and find where the noun phrases are.
And we have talked about dependency parsers and well,
one choice is you can use a dependency parser to find the sort of nominal arguments,
and work with them.
That's sort of actually a little bit subtler than just sort of wanting to pick
out spans that refer to common noun phrases.
So the other notion of parsing which we come back to,
um, next week is constituency parsing.
In some sense, constituency parsers are
the simplest way to find mentions for this process.
Um, most of it seems and is easy,
um, there are sort of tricky cases as to what counts as a mention or not.
So, um, if it's kind of it is sunny,
I mean, is it a mention of something?
It's sort of seems like it's not really,
it's just it seems like it's an it that you stick at the start of the sentence,
um, that doesn't mean anything.
So that's maybe not a mention.
Um, every student.
Is every student a mention?
I mean, it's certainly, at best it's some kind of collective,
um, but it's not sort of a very clear concrete reference, um.
That goes further, if I sort of use different quantifiers,
so if it was like, every and no are called quantifiers.
I mean no student definitely doesn't have reference,
because it's not pointing at anything, right?
It's asserting a claim of nonexistence.
So that there's definitely, um,
no- it isn't a mention of anything.
Um, yeah, the best donut in the world.
Um, does that have reference?
Um, that's unclear.
This is the kind of thing that actual philosophers of language debate over, right?
So if there was agreement on what the best donut in the world is,
then maybe it has reference, um,
but I can say sentences like,
I'm searching everywhere to find the best donut in the world.
And then in that sentence,
it doesn't have any reference, right?
It's sort of an intentional description of what I'm hoping to find,
that there's no concrete thing it refers to.
Um, things like quantities, 100 miles.
That sort of behaves like a noun phrase,
but it is in- it's sort of really a quantity that doesn't really have reference.
Um, and so then there's the question of how can you deal with this stuff?
Um, well, um, our tool whenever we want to deal with stuff,
is we train classifiers,
as in they pick out things that are mentioned and things that aren't.
And so that's something that you could do is write a classifier that filters out,
um, these spurious things that you want to say aren't really mentions.
And people absolutely have done that.
But commonly actually people skip that step,
and you just sort of instead have your mention detector find all candidate mentions.
Because it turns out that that tends to work pretty well.
Because after we found all of our mentions, um,
we're then going to be doing this clustering process to find coreferent mentions.
And if there are just a few stray mentions like
no student and we don't cluster them wrongly with anything else,
it kind of doesn't do any harm because we are mainly involved in this clustering process.
Okay. Um, something you might be wondering is,
well I've sort of implied now,
we have a pipeline.
I'm saying we're going to run a part of speech tagger,
and we're going to run a named entity recognizer,
and we're going to run a parser.
And we're going to run a, um,
a named mention detector.
And then eventually, we're going to run this coref clustering system,
so we have a sort of a five-step pipeline.
Um, is that the only way you can do, um, coreference resolution?
And the traditional answer was yup,
that's the way you did coreference resolution.
That essentially, all systems for coreference resolution,
until approximately 2016 where a pipeline that went through about those stages.
Um, but just recently and I will dico- cover one such system,
um, later in the class, um,
that people in the neural world have started doing what's been
effective in a lot of places in the neural network world of saying,
can we just build an end-to-end coreference system
that starts with just plain text of a paragraph,
and feeds out coreference clusters without there being any intervening pipeline steps?
And I'll show you a bit more about how that works.
Um, but before we get into systems,
I just wanted to say a little bit more about the linguistics of coreference.
Um, there's actually quite a lot of interesting stuff here,
and to a fair degree,
it's not actually stuff that's been thought about
very much by people who build NLP systems, right?
I already mentioned, um,
from the Shruthi Rao story, um,
the example of split antecedents, right?
That that's just a clear linguistic phenomenon that happens,
and it's not even incredibly rare, right?
Um, that, you know, um,
people build these simple machine learning models that just can't deal with that.
And there's really quite a bit more structure
to what happens in the linguistics of coreference,
it isn't really being exploited in most of the systems people bui- build.
So I just wanted to show people a bit more of that.
And essentially, to sort of understanding, um,
more about how people see things linguistically,
there are two concepts that are related and commonly confused,
that are really different.
So one is coreference.
So we say that things are coreferent when there are
two mentions and they refer to the same entity in the world.
So if it's sort of,
um, Donald Trump and the current president, right?
They're two mentions and they refer to the same person in the world.
And so that is a relationship of coreference.
Um, and that's then contrasted, um, with anaphora.
And so the idea of anaphora is some terms in text don't have independent reference,
and you work out their reference by relating them back to another thing in the text.
So if we have the sentence,
Barack Obama said he would sign the bill.
He is an anaphor.
And if I just say, he,
what does he refer to in the abstract?
Well, you know, apart from saying something male, right?
You've got no idea, right?
Because you can't work out what he means just by knowing he.
You have to be looking at a text and interpreting it relative to the text.
And then if you're interpreting it,
um, relative to the text,
you're then in this situation of,
okay I see, this refers back to Barack Obama.
So he is another mention of Barack Obama,
then- and this then is this concept of anaphora.
So the picture we have is sort of like this,
that you can either have these independent mentions,
which do refer, um,
to the same thing in the world.
They're coreferent.
But in many cases,
such as when they're full mentions like President Obama,
versus Barack Obama, they don't have any textual relationship.
It's just they happen to refer to the same thing in the world.
And that then contrast with cases like Barack Obama said he would do something,
where the he has a textual relationship back to Barack Obama.
And that's an example of anaphora.
Um, this might up until now feel like an almost meaningless distinction.
But something that maybe gives you more of a sense that there's something useful here is,
um, these textual relationships exist even when there isn't coreference.
So we sort of mentioned before,
these cases like no dancer, right?
So no dancer doesn't have reference, right?
It refers to nothing.
Um, but if you have a sentence like,
"no dancer twisted her knee," well we have an anaphor here.
And that anaphor is referring back to "no
dancer" despite the fact that "no dancer" doesn't have reference.
So we can still have the anaphoric textual relationship.
And indeed, you know,
her knee is then a part of her.
And so these are the sort of part relationships again.
But her knee, in a sense that I'll just come back to,
is also an anaphor which is interpreted with respect, um, to the dancer.
So we have two anaphoric relationships here,
even though we have no reference.
There's another interesting case of
anaphoric relationships which aren't the same as reference,
which is you could have looser forms of anaphoric relationships.
So you get lots of sentences like this.
"We went to see a concert last night,
the tickets were really expensive."
So we have this mentioned here of the tickets.
Um, but really to interpret the tickets,
we have to interpret them with respect to this,
um, mention back here,
a concept, because really what this is saying,
the tickets for the concert were really expensive.
So this is also referred to as an anaphoric relationship,
where the meaning of the tickets has to be interpreted
textually based on another, um, noun phrase.
But it's not a coreference relationship that
the concert and the tickets are clearly two different entities.
So these kinda looser cases are referred to as bridging anaphora,
because you sort of have to supply for yourself the bridge,
the relation that connects together the antecedent and the anaphor.
Okay. So that's how- we then have these pictures,
that we have this sort of not in- not complete crossovers
between coreference and anaphora that we've sort of talked about.
Um, I have one other note on anaphora. Um,
Who- has anyone here ever done any Ancient Greek?
Any Ancient Greek? [LAUGHTER] Yes.
Okay. Um, so, um,
from- from the origins of the words anaphora,
anaphora is meant to be that you're finding your textual reference before you.
Um, and so there's actually a- a complementary, um,
term of art which is referred to as
cataphora where you're finding your reference after you.
Um, so here is a beautiful example of cataphora.
So this is from Oscar Wilde's,
The Picture of Dorian Gray.
"From the corner of the divan of Persian saddle-bags on which he was lying,
smoking, as was his custom,
innumerable cigarettes, Lord Henry Wotton could just catch
the gleam of the honey-sweet and honey-colored blossoms of a laburnum."
Um, right. So here we have this, um, mentioned,
Lord Henry Wotton and there are two anaphors,
um, that refer to Lord Henry Wotton.
Um, he and his,
and that they both come before,
um, Lord Henry Wotton.
And so these are referred to, um,
as instances of cataphora among a certain kind of classical scholar.
Um, and in case you don't know what a laburnum is,
um, this is a laburnum.
[LAUGHTER] Right. But, yeah,
so thi- this is cataphora.
Now- now there are two sad things to say.
Um, the first sad thing is in modern linguistics,
the term cataphora is completely disused.
And we mean- we just used the word um, anaphors everywhere as meaning
a word that gets referenced from some other mention in
the text and it doesn't matter what side it's on.
Um, so, um, that we go downhill one stage to
linguistics but then we get to NLP and we go downhill a second stage.
Because what you'll see is that in general,
the systems that people are building for,
um, reference resolution, they don't make any distinction of direction at all.
That once you find a mention,
you're always looking backwards for its reference.
Um, and you've got no idea that,
well, maybe sometimes you could look forwards.
So effectively, what it means,
that the systems end up doing is saying,
well, there's a he here,
there are various other things, there's a his, etc.,
and you'll eventually get to Lord Henry Wotton and you'll be able to
be trying to find its reference by looking backwards,
even though that's sort of ill-formed from any kind of linguistic sense
whereas really he and his that should have been looking for their reference forward.
Okay. Um, is everyone good up to there, any questions?
Okay. We'll move ahead and, um,
try and move on to kinds of coreference, um, models.
So I wanted to, um, tell you, um,
as much as I can and I have 45 minutes, um,
left about, so the kinda models people build with coreference.
And I hope to mention quickly four different ways that people have looked at coreference.
I wanna tell you a teeny bit about classical rule-based coreference.
Um, then, um, mention- mention pair coreference.
Spend the most time on mention ranking systems which have
tended to be the easiest simple systems.
And then just say a little bit about
clustering systems which should be the right way to do
it but in practice has been a way that's been hard to get the best performance from.
Okay. So here's a bit of history.
Um, this guy here is Jerry Hobbs.
He just had his retirement party from University of Southern California last month.
Um, so Jerry Hobbs,
way back when, um,
wrote a famous paper,
it was in 1976 on coreference resolution.
And in that paper, um, he proposed,
um, what's normally now referred to as the Hobbs Algorithm.
But actually, um, in his paper,
he refers to it as a naive algorithm.
Um, and I'll come back to that distinction in just a moment.
Um, but what the Hobbs algorithm was,
is if you have a sentence- so actually I should say this,
this algorithm is just for finding the reference of pronouns.
So one can extend out to other cases but the part I'm gonna show
you is just the part for doing the reference of pronouns.
So when you find out,
find a pronoun and you wanna say what is it, um, coreferent with?
What you're going to do is run this mechanical algorithm
that's looking at a parse of a sentence and is working out what to do with it.
Begin at the NP immediately dominating the pronoun,
go up the trees or the first NP or S. Call this X and the path p,
traverse along, ah, it goes on and on.
Um, there's more of it.
That was only the beginning of it.
There are a lot more stages.
Um, but, you know,
I'm not- I don't really wanna go into the details of this.
Um, but, you know, to try and explain the flavor of it,
here's a piece of text.
"Niall Ferguson is prolific,
well-paid, and a snappy dresser.
Stephen Moss hated him."
Um, and so if you can remember any of the steps of that algorithm,
here's our, um, pronoun him.
Um, and then, what it said to do was begin at the NP,
the noun phrase above the pronoun.
And then it said, to go up to the first noun phrase or S above that,
um, here is the S above that.
Um, and then what you're meant to do is, from there,
you're meant to go left to right through stuff that came before that.
So there's a lot of cleverness in this handwritten algorithm.
You know, this is in the space of clever handwritten algorithms.
And so what this is reflecting is that you might just think you
should go to the closest thing to find reference,
but actually if you have reference within the same sentence,
it's much more common for the sort of
highest syntactic roles to be what you're coreferent with.
So you're more likely to be coreferent with a subject than an object,
and you're more likely to be coreferent with an object than something like
a noun phrase and that's inside a prepositional phrase that follows the object.
So we're gonna start from the left here and we're gonna
say here's a noun phrase, Stephen Moss.
That's the first one we come to.
And then there's this clever bit of text that says,
um, traversal branches, um, below X,
that are to the left- left to right,
propose as antecedent and noun phrase, um,
that has a noun phrase or sentence between it's an ec- in the S. So it was saying,
this will be a candidate,
if and only if,
there's some other noun phrase or S in-between.
Um, and so what that's saying is Stephen Moss hated him.
It- this him cannot refer back to
Stephen Moss and that sort of pretty much a fact of English syntax.
But what it's wanting to do is distinguish between,
another thing that we could have had here was
a noun phrase that had another possessive noun phrase inside it.
Um, so if we had something like Stephen Moss's mother hated him, right?
Then the Stephen mother- Moss's mother hated him, then that would,
in that case, it would be perfectly okay for him to be coreferent with Stephen Moss.
And the algorithm allows that because relative to
this noun phrase is another noun phrase above it and between.
Okay. So that didn't work, um,
as an antece- as an antecedent,
so then we go onto the next step of the algorithm.
And then, the next step says,
we should proceed backwards through preceding sentences,
um, right to left.
And so that captures an important heuristic that proximity is actually
a good heuristic to find coreference
because coreference for pronouns is usually close by overall.
And so we go to the first sentence back.
And then in this sentence, again,
we go into within the sentence,
go left to right because there's the same kind of subject prominence role.
And so we're gonna start in this sentence,
and we're gonna say okay,
here's a noun phrase.
And now because we're in a different sentence,
there's nothing wrong with this one.
So we say, aha,
we have a candidate, Niall Ferguson,
um, is a possible antecedent and it's the first one we found.
And therefore, we say that him refers back to Niall Ferguson.
And this algorithm actually gives the right answer,
if you could follow along all of that.
Um, though that sounds like, um,
horrible handwritten stuff.
But, um, so Jerry Hobbs was aware of that this was horrible handwritten stuff,
but he was interested in this algorithm for a couple of reasons.
I mean, reason one is, you know,
this is actually one of the first places in natural language processing,
that someone produced the baseline, right.
In for final projects and elsewhere,
um, and stuff we gave you, right,
it's seen now in NLP and other areas,
that anything you are doing,
the first thing you should do is have a baseline,
a simple system and see how well it works.
And this was his simple rule-based system for doing coreference,
um, and he wanted to observe that actually this baseline was pretty good.
It actually gave the right answer a lot of the time.
And so the challenge was how to build a system that did better than this baseline.
And so he was well aware of it,
you know, it was a dumb algorithm,
but he proposed that as a good baseline for doing coreference resolution.
So what he was interested in,
um, remember that we're back in the 1970s here,
was how to do knowledge-based pronominal coreference resolution.
And so, um, essentially what he was noticing is well,
these kinds of syntactic factors that I was mentioning prefer subjects,
prefer close by, etc,
they're all useful predictors.
But there are lots of cases where they don't give the right answer,
and to know when they give, when,
to know what's really the coreferent thing,
you have to actually understand what's being described in the world.
So if I have this sentence,
she poured water from the pitcher into the cup until it was full.
What is it coreferent with?
Cup.
[NOISE] The cup.
Thank you. [LAUGHTER] Okay.
So that, it refers to the cup.
But then let's look at this example.
She poured water from the pitcher into the cup until it was empty.
What does it refer to?
The [OVERLAPPING].
The pitcher. [LAUGHTER] Okay.
So the crucial thing to notice in these two sentences is,
these sentences have identical syntactic structure, right.
So Jerry Hobbs's algorithm can't possibly work,
um, for both of these sentences.
It's gonna work for one of them,
but not the other one.
Um, since it's working from left to right within a sentence,
it's gonna say the pitcher both times actually, right.
So you can't get the answer right by Jerry Hobbs' algorithm and Jerry believed,
and still believes, um,
that the only way to get these kind of examples right,
is actually if you understand the world,
and you actually know what's going on in the world,
so you can see what, what this is talking about.
And there are lots of examples like this.
Um, this is another very famous example.
The city council refused the women a permit because they feared violence.
Um, who does that they refer to?
[inaudible].
The city councilors.
Um, but here's another sentence.
The city council refused the women a permit because they advocated violence.
Who does that they refer to?
The women.
The women. Okay. So this time it refers to the women.
Um, and again, you know,
identical syntactic structure, it couldn't possibly be done right by the Hobbs algorithm.
Um, so this particular pair of examples,
um, comes from Terry Winograd.
Um, how long ti- uh,
so Terry Winograd was originally an NLP faculty, um,
he sort of got disillusioned with NLP because there wasn't making much progress, um,
and ventured off into the land of HCI,
um, that became his career.
Um, but in his early work, um,
he was interested in these phenomena,
and came up with this example.
And so this example really stuck with people.
And so these kind of contrasts are referred to by
other people as Winograd sentences or Winograd schema.
And so this is actually something that's interesting that's revived recently.
Um, so Hector Le- Levesque, um,
wrote a paper, I guess five years ago now,
where he was trying to advocate for return to doing
more in the way of knowledge and world modeling and artificial intelligence,
and arguing that there are lots of problems that you just
can't solve by the kind of crude statistical methods,
that our machine learning systems are using.
And that you really needed to do more world understanding.
And so he proposed that
these Winograd schema would be a good te- alternative to the Turing test,
as a way of measuring intelligence.
And actually they're just coreference decisions, right.
So, um, so there's sort of a claim here that,
if you can do a coreference right 100 percent of the time,
you've solved artificial intelligence in that you're, sort of you can,
can code knowledge of the world into coreference problems.
Um, yes so people have then tried to work on these Winograd schemas,
and Levesque's feeling was, you know,
you just couldn't do these,
um, using kind of,
the kind of statistical factors, um,
that people put into their machine learning systems.
He was partly wrong about that because subsequent work, um,
both neural systems and otherwise has shown that actually you can
get f- a nontrivial distance with these kind of problems because, you know,
if it is the case,
um, that, you know,
you can somehow see enough examples,
where the city council refuses permits,
fearing violence, you know.
If you've go- if you're collecting
your neural language model over tens of billions of words,
you might have seen some instances of things like that,
and you could sort of predict it just on statistical patterning.
But the question is, you know,
how far can you actually get doing that,
without having a bit more of a world model?
And so that was, you know,
what Hobbs was interested in way back in 1978.
So he wrote, the naive approach is quite good,
computationally speaking it will be a long time before a semantically based algorithm,
is sophisticated enough to perform as well.
And these results set a very high standard for any other approach to aim for.
He was totally right about that, um,
that it really wasn't until the 2010s that anybody
managed to produce an algorithm for pronominal anaphora resolution,
that outperformed the Hobbs algorithm.
Even though it was just, uh,
what he called a naive algorithm,
or he might call a crude set of linguistic rules.
Um, but he says,
yet there is every reason to pursue a semantically based approach,
the naive algorithm does not work.
Anyone can think of examples where it fails.
In these cases it not only fails,
it gives no indication that it has failed,
and offers no help in finding the real antecedent.
Um, so food for thought there.
Um, but, um, notwithstanding that,
I'm gonna just rush ahead at this point,
and tell you about some of the, um,
statistical and neural algorithms,
um, that have been used for coreference resolution.
So the simplest form of algorithm that's commonly used,
is what is called mention pair models.
So what we mean by mention pair models is, um,
we are gonna take pairs of mentions,
and we're gonna train a binary classifier that says,
is coreferent or isn't coreferent.
And so then we're gonna proceed left to right through the text.
And every time we get to a new mention,
we're gonna then evaluate our classifier with respect to every preceding mention,
and we're gonna say, are they coreferent?
And it's gonna say yes or no.
And we're gonna find out that some of them.
It says yes for, um,
I voted for Nader because he was like, most aligned with my value.
She said, if we have a good classifier,
it will say yes to the two bu- blue ones and not to the rest of them.
Um, and so then we'll have at training time,
negative examples that Nader and he are negative examples.
[NOISE] So if you have data marked for coreference,
we have the sort of positive and negative examples,
and we can train a model.
And so for training a model,
we have a sort of the classifier outcome is one or zero,
based on whether two mentions are coreferent.
We're gonna have a coreference model that
predicts the probability of them being coreferent.
And we're gonna train it with the same kind of cross entropy loss,
we've used other places and, um,
try and learn a model that predicts coreference.
And so then when we get to test time, um,
and we have a piece of text with mentions, um,
we're gonna run this classifier and it's gonna say,
um, yes or no, with some probability.
And if we pick a threshold like 0.5,
we'll add certain coreference links.
And that sort of looks pretty good.
Um, but we're gonna sort of complete it off by saying well,
if A is coreferent to B and B is K coreferent to C. Then really
also A is coreferent to C. So we're gonna do a transitive closure,
and that will give us our clustering.
Um, note here that there's a certain danger in this.
Because this means, if we make,
since we're sor- with the transitive closure,
that's always adding clustering links.
And so that means the danger is that we're gonna over cluster,
because if we make a single mistake and we link things that should be kept separate.
So for example, if we wrongly said,
he and my are coreferent,
then everything of this, um,
discourse would collapse together into one cluster,
and everything would be deemed coreferent.
Okay, um, and this,
something that I haven't really emphasized, but comes up,
is well, there's some mentions that are coreferent to nothing, right.
In the Shruthi Rao story, there was a park,
which was just mentioned once in the text, and so on,
in this form of algorithm,
what we'd like the classifier to say is, no,
no, no, no, no,
for all of the decisions.
And so it's deemed coreferent to nothing.
And then it's just a singleton mention.
This sort of works,
but it hasn't proven to be the best way of doing coreference.
And a lot of the reason why it's not the best way to do coreference
is because we have this phenomenon of anaphora where we have textural dependence.
A lot of the time,
it seems that we're not really,
um, what- sort of wanting to make this all coreference decisions.
We'd like to make the anaphora decisions of textural dependence.
So we'd like to say that he is,
um, dependent on Nader and my is dependent on I.
These are anaphora relationships.
So we'd like to just choose one example of what is this anaphora relationship.
And so that's led to people then looking at what is called,
um, Mention Pair Models, right?
That the problem is that if we have a long document with lots of mentions,
um, that we want to not be saying- trying to find all of them and say, yes.
We just want to be saying there's a particular- we
just want to be saying that there's a particular one.
So for the he at the end here,
its anaphor relationship is back to Nader and you don't wanna be trying to say this
he is also coreferent back to all of these other things that are earlier in the text.
So it's not something that's been explored much.
But arguably, this is a case again,
where you should be separating coreference from anaphors because for anaphors it seems like
the right way to think is that they have
one prior thing in the text that they're textually dependent on.
Whereas true coreferents, when you just have various mentions in the text of Ralph Nader,
this Ralph Nader that,
Nader did that, those aren't textually dependent
and they should all be being grouped together as coreferents.
Um, but our models sort of don't normally try and do some one way and some the other way,
but you choose one of the models.
So in the other one,
we do it for- to do the other way,
you do what's mention rankings.
So for mention ranking,
the idea is for each mention,
we're going to find- try and find it
an antecedent that comes before- before it in the text,
that is- that it is, um,
coreferent with, and we're going to make a one of N decision.
So that when we see she here,
we're going to say,
"Okay, um, what is this coreferent with?"
And we're going to pick one thing that it's coreferent
with even though there might be others in the text.
Um, so if we're doing that,
we then have a problem with singleton mentions because if
we're trying to- for every mention we find say,
choose the thing that came before it in the text with which it's coreferent,
the right answer might be that there's no such thing.
So what we do is we add
one additional dummy mention right at the front here, the NA mention.
So one choice is you're gonna say there isn't anything preceding.
So effectively, when you get to I,
since this is, um, the first, um,
real mention in the text,
you're necessarily gonna choose as,
um, its antecedent NA.
You then go on to Nader and you have two choices.
You can either say it's coreferent to I or it's coreferent to NA.
I, it's a new mention- a new entity that's being mentioned in the text and
the right answer is it's a new mention in- a new entity being mentioned in the text.
Then you get to he and now you have three choices,
and the right thing is to say that it's coreferent to Nader.
Okay. Um, so this time,
it's- for training our models,
it's sort of the same, um,
apart from this, sort of this different one of semantics.
So now- previously, we wanted to say that for our, um,
mention pair classifier that is going to try and classify I and she,
and my and she,
and both of them had to get a high score,
where now it's sufficient that just one of them gets
a high score because that's sort of enough for us to do.
So what we're gonna use is our good old softmax and so for she,
we're gonna put a softmax over the antecedents.
And our hope is simply that we get a high probability with one of the antecedents,
if it has an antecedent or a high score with NA,
if it doesn't have any prior referents.
And so then when we're doing classification at run-time,
we're going to sort of add only the highest scoring coreference link.
So that means we train it just slightly
differently because now what we're going to do is that,
when we're- what we're wanting to say is,
we want a high score of coreference between at least one of the antecedents.
And so one possible model is,
we can maximize this probability.
So for the ones that are coreferent in the gold standard data,
we want the sum of their assigned probabilities to be high.
And so what that means is that it's sort of sufficient if we have,
um, one of them giving
a high probability and they don't all have to give a high probability.
So providing it's giving 0.9 probability,
say it a one of the correct antecedents,
we're getting a high score.
Okay. So we're gonna turn that into a loss function in the kind of
standard way we do in which we take log probabilities,
um, and then we want to, um,
or negative log probabilities to give us
a loss and then we're wanting to minimize that loss.
So with the mention ranking model,
um, at test time,
it's pretty much the same,
but our softmax classifier is just going to assign one antecedent for each mention.
And so we're then gonna hope that those sort of give us the kind
of clusters that we want and there's no subsequent clustering phase.
So there's a big part of this that I left out which was,
I've just said, "Okay,
we have this probability of MI and MJ as the- are they coreferent?"
But I've sort of said, zero as to
how you can determine whether they're coreferent or not.
Um, so briefly, um,
here- here's the classical way of doing it.
The classical way of doing it is,
you had a whole bunch of features and you had
a feature based statistical classifier which gave a score.
And these are the kind of features you could use.
So there are sort of strong features of person, number, gender agreement.
So if you have a masculine or feminine pronoun,
you wanna find an appropriate antecedent for it.
There are weaker, um, semantic compatibility features.
So the mining conglomerate, the company,
the conglomerate might be sort of similar to a company.
You could use something like word2vec similarity and assess that.
There are syntactic constraints.
So this is then kind of like, um,
what Hobbs's algorithm was all about us working out
how likely different syntactic configurations are gonna mean coreference.
And indeed it is the case, you know,
that a lot of these feature-based systems used Hobbs' algorithm as a feature inside
the system that was weighted and was normally a very strong feature to decide coreference.
Um, there are lots of other things you can put in as features.
Um, recency.
So John went to a movie,
Jack went as well,
he was not busy.
The most likely referent for he is the closer candidate Jack.
Um, I've mentioned subjects are more likely to be, um, the antecedent.
John went to a movie with Jack,
he was not busy.
Um, John seems a more likely antecedent.
So that's the sort of subject preference.
There's also a parallelism preference.
So John went with Jack to a movie,
Joe went with him to a bar.
I think it's sort of reasonable to think that him there is probably Jack,
and that's sort of for parallelism reasons as opposed to going with the subject.
So there are various kind of linguistic features and constraints and so on,
and you can throw these all into a statistical classifier and that's
sort of 2000s decade coref systems as to how they're built.
Um, more recently, people have built neural systems.
And so for these,
we are kind of normally using the same kind of embeddings.
So we'll have a candidate antecedent that will have embeddings,
we'll have a mention that has embeddings.
And this will be something like
average word vectors or something like that for the mention.
And we're gonna feed these into a neural network that will give us our score.
But what you find is that
most of these systems as well as having something like word vectors,
they also have additional features, um,
and these features still capture some of
the things that were in the feature-based statistical classifiers.
So there will be often features that reflect things like,
what grammatical relation does this mention have? Is it a subject?
Is it an object?
That's something you could put into the features of a mention.
But then, closer things are more likely to be coreferent.
So you might have additional features here which record how far apart dimensions are,
and those things get thrown in as well.
Um, and so these kind of features are still important even in neural systems.
And so I'll skip ahead now and show you a bit about, um,
what is the kind of current state of the art for coreference resolution,
and this was a system that was done at the University of Washington in
2017 by Kenton Lee and assorted other, um, authors.
Um, so the goal here was to produce an end-to-end coreference system that it was text in,
um, mention clusters that are coreferent out.
Um, and so they're wanting to use sort of a more complex
neural network that can do the whole thing end-to-end. So I'll go through,
um, the steps of that.
So the first step is we just start off with words.
And so for each word,
we're going to look up a word embedding for it and that's in other stuff we've seen.
We're also going to put in a character level CNN,
and the two of those concatenated are going to give the representation of each token.
That much should look familiar.
Okay. Then after that,
we're going to run a deep bidirectional LSTM back and forth across the sentence.
Again, that should look familiar from stuff that we've seen before.
Um, the next step gets us a bit into doing something more special, um,
For coreference.
So what they wanted to do after that is have a representation for spans.
And so by span,
we mean any contiguous subphrase of the word, of the sentence.
So this is a span.
This is a span.
This is a span.
Electric said the postal is a span, every sub-sequence.
Um, so I'll come back to that.
But, you know, they'll- in principle,
you're working this out for every sub-sequence.
So for every sub-sequence,
they want to come up with a span representation.
And so this span representation is going to be in three parts,
um, that represent one of these sub-sequences.
Um, so each of these will get its own representation.
And so the question is, what?
And so we have this span representation,
and it's gonna be in these three parts here.
Um, so what these parts are is,
well, first of all,
we're going to have a representation, um,
which is just looking at the first word of
the span and the last word of the span according to the BiLSTM.
So if we're looking at the span, the postal service,
we're going to take this BiLSTM and
this BiLSTM and use them as part of the representation of the span.
Um, that's a good start,
but then they actually do something a little tricky.
So kind of like when we're doing dependency parsing, the idea was,
well, phrases are going to have a headword,
um, so that if it's,
um, you know, my younger sister that the headword of that is sister,
and there- if it's something like the goat in the corner of the field,
the headword of that is going to be goat.
So they want to find a way of capturing headwords out of the text.
Um, and so what they're going to do for that is use attention.
So they're going to say we have this span, the postal service,
and we're going to use attention as
a span internal mechanism to sort of approximate a head.
So what we're going to do, uh, here,
what we're going to do is we're going to want to
learn attention weights, I'm just gonna, yeah.
Um, what we're gonna do is for this span, um,
we're going to be learning based on the hope,
the ends of the span which words to pay how much attention to.
So we're gonna put attention weights on the different words,
and then we're going to, in the usual attention way,
make this weighted sum of having put the word pair-
the bidirectional LSTM pairs through a feed-forward network and end
up with this new representation of a weighted representation.
And the hope is that in this case,
most of the weight will go on this final servers,
which will be the headword.
But there'll be sort of distributed across it.
And so that gives them a model of
sort of mentions that use both ends and hope to find the key word of the mention.
Okay. Um, so, um,
that's two-thirds of the span,
but they still have over here these additional features.
And so they still have some additional features.
They want to be able to mark speakers and addressees.
Um, they want to mark other things like the grammatical role.
But if things occur, you know,
it is still useful to have some additional features.
And so what they do is,
this is a representation of each span,
and then they're going to want to say are two spans coreferent.
And so they're going to have one score for the two, two split, each of two spans,
which is essentially saying,
is that a good mention?
And then you're going to have scores of,
do they look coreferent?
And so having calculated these representations for each span,
you're running three- through things through a fully connected feed-forward network,
multiplying by a weight factor,
and that's giving you, uh,
is that a good mention score?
And then for are they coreferent,
you're taking two spans,
the pointwise Hadamard product of two spans and
some extra features like distance apart in
the text and putting them through another neural network,
and that's then giving you, are
these two spans coreferent?
But all of these pieces,
um, give you an overall loss function.
So you can say that your model is, um, okay.
We're going to run these LSTMs,
we're going to take all spans,
we're going to score this,
and we know the gold answer for our coreference system.
And so we want to be predicting things that are coreferent and have
a loss based on the probability that we calculate with these scores,
um, as I had mentioned,
ranking model using a softmax loss like before.
So if you put all of this together and train it end to end,
you've got a whole coreference system that goes from words to coreference decisions.
Um, there's a huge problem with that,
um, which is if you actually applied this naively, well,
the problem is the number of spans in a piece of
text is the square of the length of the text in words.
And so therefore, if you're making coreference decisions,
which are between, um, pairs of spans,
you've then got an algorithm that's, um,
O- OT to the fourth,
where the length of the text is T words.
So that's sort of really, really computationally impractical.
So at this point,
they sort of say, well, actually,
we do want to use our mouths a little and we want to work out
how likely different things are to be mentions.
So effectively, um, then they're putting in a lot of pruning to
decide which spans are actually things that they want to consider in their model.
And so at this point, in some sense,
it's a little bit of a cheat, right?
Because really this pruning step here is okay,
we're going to stick in
a mention detection module,
um, just like a conventional system.
Um, but the prettiness of it is in terms of
the algor- in terms of the loss function that's defined.
The loss function is really defined end to end from just a sequence of
tokens through to the mention ranking decisions.
And so it is an end-to-end model,
even though in practice to make it practical,
you have to have something like a mention detector to get it to work.
Okay. Pause for breath. Um, yeah,
so there's one last.
So we've done sort of mention pair model and mention ranking model.
Um, and so for both of those,
you're just taking individual mentions and saying,
here's another mention, what,
what shall I do with it?
Let's look at mentions and see if we're coreferent to each other.
And that there's no real concept of entities which are clusters of mentions.
You're just making these sort of one-off decisions
between pairs of mentions, and somehow,
sort of the entities as clusters just
emerge as a consequence of those mention pair decisions.
So there's been this sort of long-standing feeling that,
oh that can't really be right,
the right way to do coreference must be really to do it as a clustering task,
and people often refer to this as saying,
we want entities as first-class citizens.
So we want to be,
sort of putting together mentions into clusters that represent the entities.
And the obvious way to do that is to do a kind of bottom-up agglomerative clustering.
So you start off by saying,
each mention is its own singleton cluster,
and then you're making decisions to merge clu- clusters which is initially,
um, saying two mentions are coreferent.
But as you go on with it,
you're then making decisions that two clusters are coreferent or not.
So the idea here is you'll have a piece of text,
Google recently blah blah blah blah,
the company announced Google Plus, blah blah blah blah,
the product features blah blah blah blah.
And so you have here some mentions.
And so what you're going to do is start off saying that okay,
there are these four mentions that each their own cluster.
And then what we're gonna do,
is we're going to make some decisions.
Um, so we might decide that these two clusters
are coreferent and merge them into one cluster.
And then we might decide that these two,
um, clusters are coreferent and merge them into one cluster.
And so we're progressively clustering.
And so then, we're going to look at these two clusters,
cluster one and cluster two, and say,
no we don't think those ones are coreferent,
and therefore we're going to keep them apart.
And so your, your coreference algorithm stops when there's nothing left to merge.
And the reason why people think that this is the right thing to do is,
the feeling is that if we sort of build partial clusters like this,
that you'll be able to do a better job.
Because if I just sort of say,
well here are two mentions,
Google and Google Plus,
should they be regarded as co- coreferent or not?
Um, well, since you're smart human beings,
and know what Google is and know what Google Plus is,
of course you'll answer no, of course not.
Um, but, you know,
if you're just a computer trying to make a decision,
it's sort of hard to know the right answer,
because there are lots of other cases when there are shortenings,
where the right answer is that they're coreferent, right.
Because if this is being Google and Google Corp,
then it would have been right to regard them as coreferent.
Or if it was sort of, um,
something like Hillary Clinton and Hillary,
it would have been right to regard them as coreferent.
So it can often be kind of hard to tell what's coreferent.
Um, but the hope is that,
if you've made some of the easy decisions first,
so if you decide Google and the company are coreferent
and Google Plus and the product are coreferent,
then it should be much easier to tell and to say,
well product and company,
they're definitely different things.
And therefore we should keep these things separate.
Um, and so that is the goal,
and so to follow that goal,
the kind of models people build.
And this was actually a model that Kevin Clark is one of the PhD students here,
um, and we did a couple of years ago.
The idea was well,
what we're going to do is,
we're initially going to consider mentioned pairs,
and build some kind of distributed, mention pair representation,
which is kind of similar to what we were doing previously with the previous models.
But we're then going to go beyond that and come up with cluster representations.
And then we can look at cluster pair representations.
And we would hope that by looking at these cluster representations,
we'll be able to make better decisions of what to merge or what next to merge.
Um, I have a few more slides that go through the Clark and Manning algorithm.
Um, but I also have just a few minutes left.
And so I think I'll skip the details.
Um, I think the main thing that's interesting here,
is the idea of clustering based coreference algorithms,
and why in principle,
it should give you extra oomph.
Um, and that's sort of the main useful thing to get through.
Because what I want to make sure we have covered in
the last few minutes that I've said nothing at all about,
is how do you evaluate coreference resolution and how well does it work?
So let me skip ahead to that.
Um, so if you look at coreference resolution papers,
or something like that,
um, there are many metrics that people have used to evaluate coreference,
and they have a long alphabet soup of names.
So there's MUC, and CEAF, and LEA,
and B- CUBED, and BLANC and,
um, things like that.
Um, so effectively part of it is that if you look in the clustering literature,
there are lots of ways that people try and evaluate clustering,
and essentially any of those metrics and some other ones, you can, um,
port over, um, to, um, coreference evaluation.
I mean, why it's kind of difficult is the situation you have,
is that you have a gold standard which picks out certain clusters,
and the system picks out certain clusters,
and you get some result like this and you have to decide how good it is.
So I'm going to show you just quickly one particular algorithm.
So the B-CUBED algorithm uses
precision and recall and F-measure like we thought of before.
So it looks at, uh,
cluster identified by the system.
And it says, well this cluster is four-fifths,
um, gold cluster one,
so the precision is four-fifths.
But actually, um, there are six things in gold cluster one.
So it only has a recall of four-sixth of that cluster.
And then it similarly does for the other one,
the same kind of calculation.
And then it's going to average across the precisions and recalls,
um, and it's going to come up with an overall, um, B-CUBED score.
Um, in- if you think about this from an algorithm's perspective,
this is actually tricky because I sort of said,
um, okay, this cluster is mainly gold cluster one.
So use that as its reference,
but that means you have to do a bipartite graph alignment
between system clusters, and gold clusters.
So hidden in- hidden inside this evaluation,
um, system is actually an NP-complete problem.
But in practice you can normally do it heuristically well enough,
that the evaluation method, um,
runs and works.
Um, okay.
And so the kind of thing to notice is that,
if you under cluster,
you automatically get great precision,
but you get bad recall.
And if you over cluster,
you get- get great recall because everything that should be in the same cluster is,
um, but you get terrible precision.
And so what you want to be doing is balancing those two things.
Okay. Last two minutes,
just to give you some idea of performance.
So these are results from the OntoNotes dataset which is about 3,000 documents.
Chinese, English, labeled for coreference.
Um, the scores I'm reporting is actually an average over three metrics.
One of which is the one I just showed you for B-CUBED,
um, here are some numbers.
Um, so Lee et al 2010 was the Stanford system.
So there- there was this shared task evaluation of coreference systems.
And we believe that Jerry Hobbs, um,
was still right, and you could do fine with rule-based coreference.
And so in 2010,
we managed to beat all machine learning systems with a rule-based coreference system,
and we were proud of it.
Um, and that's its performance right here.
Um, in subsequent years,
people did start to do a bit better, um,
with, um, with, uh, machine learning systems.
But as you see, not very much,
right for these 2012 systems that this one's somewhat,
better this one really wasn't better,
um, this, um, but making a bit of progress.
Starting in 2015, there started to be neural systems.
Um, so Wiseman et al was sort of the first neural system,
I vaguely mentioned this Clark & Manning system,
and the numbers are going up into the mid-sixties.
And this is the Kenton Lee system that has the end-to-end neural coreference,
and on English is getting about 67.
So something you'll notice from this,
is the numbers aren't great.
So coreference is still far from a solved problem.
Um, so if you want to have a bit of fun, um,
you can go out and try coreference systems for yourself.
Um, there's a Stanford one on the first link or the one from Hugging Face
is a good modern coreference system as well.
And if you just try these out with some pieces of text,
you'll notice they still get lots of things wrong.
Um, so there's still more work to do,
because this is just a harder language understanding task,
[NOISE] which is just kind of like, um,
Jerry Hobbs and Terry- Terry Winograd earlier observed.
Okay, um, but I'll stop there for now. Thanks a lot.
Um, oh yeah, I should have a reminder, invited speaker next Tuesday.
Um, so I'll be taking,
um, attendance for invited speakers.
 So today, we're very pleased to have as our second, um,
invited speaker, Richard Socher,
he is the chief scientist at Salesforce.
Um, Richard actually also has a lot more connection to this class,
um, because, um, for several years, um,
Richard was involved either as instructor or, um,
co-instructor in teaching this material at Stanford,
um, so he sort of knows the course, um, pretty well.
Um, and so today,
he's going to be talking about some of the challenges and recent work
in doing multitask learning in natural language processing. So welcome, Richard.
Thank you. Hello, everybody. I'm excited to be here.
Uh, yeah, I want to talk to you today about what we,
in short, called decaNLP.
I want to first give a big shout out to Bryan McCann.
He's the first author of this, uh, paper,
and I've pitched this idea to a lot of people in the last, like,
three to four years,
and most people were like,
"This is too much pre-processing because you're trying to
do 10 different tasks in one model."
That's sort of where the decathlon, uh,
wording comes in, uh, but he,
he really stuck to it, uh,
did all the pre-processing and all the things that you now know like tokenization,
and it turns out a lot of different data sets,
have a different conception of what a word is.
This wasn't two words,
uh, or one word,
and things like that, and that changes how you
write all your evaluation scripts and all of that.
So Bryan, uh, is,
is a really phenomenal researcher,
uh, with us in the group,
and Nitish has helped us a lot on the optimization side of this,
uh, and then Caiming Xiong,
the Director of Research, has done a lot of, uh,
really phenomenal work that's kind of helpful in pretty much all our projects.
So I'm going to tell you a couple of different, uh,
lines of reasoning that led us to,
uh, this idea of multitask learning.
And the first one was sort of trying to take a step back and looking at the field,
and I noticed not like that much of a historical class but basically pre-2010,
most natural language processing had kind of these very hand-designed features,
and we basically just had,
uh, machine learning kind of learned weights,
uh, in the optimization procedure for these human-designed features.
And so in 2010, Chris and I and others sort of started to work in deep learning for feature learning.
So everything was a word vector and now,
we can back-propagate into them and actually learn those representations.
And I think currently,
we're kind of in a state where we do a lot of
deep architecture engineering for specific tasks,
and you've seen this already.
You have like an NER model,
you have a question and answering model,
you have a translation model,
and we basically now,
each of these communities has at least, uh,
converged on is probably some kind of neural network,
but there's still a lot of different kinds of architectures of
these neural networks that you're working on for each different task.
And so the question is like, okay,
we're gonna probably do that for
another couple of years because we're making good progress,
but what's sort of next,
uh, on the research side?
And what I actually love about this class so much is that
you go from like maybe not knowing much about NLP at
all to you can basically understand
the state-of-the-art research papers as they come out now,
uh, and this, this is one of those.
Uh, so [NOISE] why,
why not continue to work in this multitask regime?
In some ways, I feel like, uh,
the community is a little bit, uh,
like this cute dog, where we, kind of,
randomly restart, uh, after every project.
And it's kind of clear to me that if you have a lot of training data, uh,
and you define a specific data set and task on that data set,
you start to architecture engineer in your model to hill-climb on a particular metric,
or leaderboard, or publications,
or products, or whatever it is, uh,
then as long as your data set has
roughly a good representative set of
1,000 times the number of output classes that you have,
you'll probably get it into a regi- regime where you're in the 80 to 90 percent accuracy,
or if one, where you're basically doing pretty okay.
And of course, now when you look at trends on ImageNet,
you have 1,000 different classes in computer vision,
1,000 different classes, each has 1,000 images.
So if you have roughly a million images, you do pretty well.
And in machine translation, ideally,
you know, I have many more, I have like hundreds of thousands of words,
so you want many millions of examples of each of the word in their,
uh, words in their context.
And of course, you know, that the caveat is
machine translation doesn't work to the level of humans,
but it works well enough to have it at least in products,
and even the best human translators use it as sort of a pre-translation and then,
uh, sort of, clean it up.
And so it's also clear to me that in this regime,
and if we want to get to, sort of,
more general AI features, uh,
we need to have some kind of more continuous learning of a single model.
Because if we keep restarting at every project,
we're never going to get to a single model that, kind of,
encompasses more and more of the complexity of natural language.
And, uh, when I say we start from random,
you of course know that that's not quite true
because we do have some things that we pre-train,
namely word vectors, and in computer vision,
we have even more things.
And so in some ways that is, ah,
an aspiring ideal for NLP,
because in computer vision, you would be, kind of,
crazy to not use some kind of
convolution neural network that has pre-train- has been pre-trained on some kind of
tasks like ImageNet when you start with your project and
try to classify objects or do object detection and a lot of other things.
And in some ways that the whole community could get behind it very quickly,
because I mean, you know, once it worked, uh,
reasonably well, because there was a, sort of,
single blocking task in computer vision.
If you can't even tell apart a dog from a cat from a house,
it doesn't really make sense to think of even larger, uh, vision projects.
And in NLP, we've had a lot of success with word vectors,
you know a lot of those now,
and it started for, sort of, just a small, uh,
window-based approach or Word2Vec and GloVe, uh,
then we had, uh, context vectors that were trained, uh,
on machine translation, but basically,
instead of just having a single set of words,
we actually pre-trained some of the NLSTMs that came on top of those word vectors,
and, uh, the way we train that, uh,
was also actually Bryan McCann's paper on
contextual vectors with machine translation and then ELMo,
kind of, replaced machine translation with, uh, language modeling,
which of course is even better because there's even more training data,
and it still tells you a lot, uh,
and kind of captures in some ways a more complex version of
distributional sort of hypotheses that we had in simpler word vectors,
and BERT, not quite a language model but also, kind of,
trying to predict words in their context, uh,
but pre-training a lot more layers and a lot deeper networks.
And so we see the success of pre-training a certain set of weights.
And so the question is,
why not try to pre-train the entire model?
As in including your output,
your softmax, your pointer mechanisms and everything,
and then just taking a completely pre-trained model and trying to do something,
and that is, kind of, the goal that we have.
And so, uh, we, sort of,
ask ourselves why hasn't this happened?
Why are we, you know,
the first to think about, like,
trying to pre-train the entirety of the model,
the encoders, and decoders,
and outputs, and everything.
Uh, and I think part of it is that NLP requires a lot of different kinds of reasoning.
You've seen many of them already.
You have some logical reasoning like 550 people in this room,
25 leave, are there still people in the room,
and you logically can answer that question,
and you have lots of different kinds of linguistic and emotional reasoning,
sentiment analysis, you know,
this is a typical Nicolas Cage movie and then you need to know that that's a
probably negative review unless you like Nicolas Cage movies.
Um, no judgment. And, uh,
you know, visual types of reasoning and so on.
And so I think partly because of that complexity in the beginning to feel,
didn't really make much progress and now and then kind of separate it.
And I think in some cases, kind of artificially separated into all these separate tasks,
like you have named entity recognition,
part of speech tagging, and semantic role labeling and, and so on.
And, and in some ways- and it sounds kind of snarky but,
you know, it made a lot of sense at the time,
and it allowed us to make a lot of progress in the community,
but basically we started chasing these benchmarks,
and all these different communities, kind of,
started going off in their own ways.
And we even have some communities that say,
"We do general question answering,
and there's literally workshops on general question answering, and when I asked,
uh, the organizers, "Can I ask your model what the sentiment is of this tweet?"
They're like, "No, that's sentiment analysis. Go to that different workshop.
It's down, down the hall."
But I'm like, "That's a- that's a question.
Why can't you answer it in the general question answering workshop?"
Um, and so a lot of people then say,
"Well, if you want to work on more general stuff,
it has to be an unsupervised, kind of,
task and the, the feature will not be supervised."
I don't think NLP will be completely unsupervised,
and we won't solve it, uh, completely unsupervised,
because in the end, language has a lot of supervision for people,
uh, and, uh, I think for, for systems also.
Uh, and you won't, you know,
if you have- there's a child and it's in a jungle,
it will probably develop a pretty good visual cortex by itself,
but it won't develop language by itself.
And then- and then also, like,
I think if you'll just allow AI's to talk to one another,
it makes very little sense for them to try to come up with as
inefficient of a communication protocol as humans have with, you know,
sequential processing of language because algorithms and computers could,
if there's no supervision of human language,
they could just communicate in much more efficient ways with one another.
So I think it's fairly clear,
we need a lot of supervision, uh, in NLP.
And so basically, all of this has led us, uh,
to trying to think about a unified multitask model for a lot of different NLP tasks.
By the way, if you have any questions, just raise your hand.
Okay, let's make this very interactive.
Um, basically, we want this unified model, uh,
to decide how to transfer knowledge,
uh, and not have it, sort of, be manually assigned.
Like in most cases,
when you assign your project you say, "Oh,
well I know that named entity recognition part of speech tagging help each other.
Because once you know something is a noun,
then it's more likely that it's also a named entity."
And in this case, we want to basically allow for the single unified model
to know itself how to do domain adaptation and wha- how to share the weights,
and that will hopefully then lead to a lot of,
uh, transfer learning and zero shot learning capabilities.
I also think that if we get to this, sort of,
hard goal of having a single fa- single unified multitask model,
then we'll easy- be able to more easily adapt it to
new tasks and we'll be also able to deploy it in production more quickly.
If nowadays you want to build
a little squirrel detector and connect it to your sprinkler system,
you can just download some off-the-shelf software,
and it will basically, kind of, work.
That is not the case if you try to do
a pretty complex language project where you
want to translate into some completely new language or,
you know, analyze some website and then do something else afterwards.
So, uh, you also,
when you actually try to deploy and use these kinds of tools and companies,
you'll realize that there are a lot of different kinds of groups.
There's the search group,
and the chatbot team,
and the translation team,
and, uh, and the social sentiment analysis team,
and they all use different models,
and they all deploy different models,
and they all have to build a lot of overhead into
the core of the- or around that core of an AI model.
So basically, um, lastly,
it was, sort of, what we had with, with this dog.
I think that once we have this unified model,
it will also be a first step to being able to
then continually learn this and just have a single model that just
gets better and better over time and starts
to capture more and more of the complexity of language.
All right, any questions around, sort of,
the motivation high level?
All right. So then, uh,
it's sort of the question, how do we actually make that happen?
And then we -- I first sort of sat down and looked at, like,
the general sort of formats of all the tasks that you may experience in
this class and that NLP sort of has as a field in
general and I think they can broadly classified,
be classified into these three different categories.
Sequence tagging, you already know.
Things like NER or aspect-specific sentiment or in
a specific context we want to classify if a word is positive or negative.
Uh, and then text classification,
just a single label for the entire piece of text
and then sequence the sequence a lot of different, you know,
problems fall into that and I actually personally love, uh,
these three particular tasks: machine translation, summarization, question answering.
Because they are immediately useful that you don't have to explain to somebody,
"Oh, but why do you need the semantic role labeller or parser? "
If you're a layman and you, you know,
on the Internet you understand immediately why it's
useful to do summarization, question answering,
or translation and an improvement in
those tasks kind of immediately translates in- into better products,
uh, and people being able to communicate better and more efficiently with language.
So, that, uh, kind of analysis led us to think,
uh, about these what I call three equivalent supertasks of NLP.
Uh, and basically they are
language modeling, question answer now- question answering and dialogue systems.
Uh, language modeling, basically trying to predin- predict the next word,
you've already worked on that.
Uh, and usually it's only used to rescore or basically to pre-train these days.
But really if you ask me a question and then you try to predict the next couple of words,
then that is also language modeling
and if you're able to predict the next couple of words after a question, like,
what were the named entities in the sentence and then you just generate, you know,
Dresden was a location,
Richard was a person and whatnot.
Uh, then you can kind of cast almost all of these tasks into language modeling.
Uh, similarly question answering,
you can ask any kind of question,
what is the translation,
what's the summary, uh, and so on,
and then with dialogue right now it's kind of tricky because there are
no really good dialogue datasets out there and a lot of times you want some interaction,
you have to run user studies and most of the existing NLP task would
basically be pretty short one-step dialogues like what are the named entity tags,
and you give them and that's it.
So it's a little bit overkill and because of that we basically converged,
uh, on question answering as our main formalism.
And here is now an overview of the 10 different tasks that we have,
uh, and we cast all of them as question answering.
These are literally the tr- the training,
uh, the format of the training dataset, uh,
and eventually also the way we formulate
the test set and you'll see basically for every single task,
you have a context as some kind of document.
It could be a Wikipedia article,
it could be a tweet, it could be a longer document,
whatever, and you ask a question about it and you want to generate an answer.
And I'm actually -- I'm curious if you can think of any task in NLP
that couldn't be formulated in this kind of structure.
Uh, so, let's go over some of these.
Uh, the first one is sort of the standard,
uh, task that all- you're all familiar with now.
The SQuAD, Stanford Question Answering Dataset.
Uh, where the answer is essentially a phrase somewhere in the context.
But then, uh, the second one is something that you would never see in most,
uh, generalized, uh, question answering workshops and that is, uh,
having a context of the single sentence asking what is the translation from
English into German and the output is again a sequence of words but in this case,
and we color them differently here.
Uh, this is blue because all these words are basically not in the context and not in
the question and we will just generate them
with a standard softmax to basically answer this question.
We can also ask what is the summary and you can see that those
two in some ways is artificial to make them into a natural language question.
You could just say translate or summarize and this is just like
one kind of task token in your network but actually half of these tasks.
It makes sense because the question also has ac- is different for every example.
So this one here is natural language inference, NLI, uh,
She covered also where we want to ask whether two sentences entail each other,
contradict each other or there's some neutral relationship between them.
You've seen a lot of sentiment.
And this here is kind of important.
We actually asked is this sentence positive or negative versus just what is the sentiment
and what- why that is important is that you see here in green,
this answer here actually comes from
a word into question and if we formulate it that way,
we can eventually do zero-shot learning where we ask a new question that was
never asked before for a new set of labels and magically, in some cases,
it still actually works and we'll, you know,
ask que- we can ask questions like is this story happy or sad and it will still
give us an answer even though we've never given
it a trained dataset of a bunch of happy and sad stories.
So, it's kind of zero-shot classification that you get to in
some cases if you formulate your questions in a way
that the answer is part as a word in the question.
Then we have semantic role labeling here.
So what has something experienced, kind of a random weird question.
Then we have a zero-shot relation extraction who is
the illustrator of Cycle of the Werewolf,
we also have some dialogue state tracking.
What is the current state in- in a dialogue and the context just keeps on
growing with the dialogue and then we also have SQL,
Wiki SQL translation tasks but not translating into
another natural language translating into a SQL database query.
It's actually a super-helpful task.
There's a, you know, a lot of data out there that is stored in databases.
If you can access it without having to ask
somebody who knows how to program SQL it will make
that data available to a lot more people so
they can analyze it and like business analytics and so on.
And then here, Winograd Schemas and anaphora resolution.
Uh, some people call this kind of common sense reasoning but it's kind of,
you know, mostly just anaphora resolution trying to understand in this context.
Uh, what -- who's, you know,
uh, the word like who had given help,
was it Susan or Joanne, and then based on this context,
you can kind of should be able to figure that out and again here,
the question is different for every single example. All right, yeah?
When you're testing it -- like when you ask,
is this sentence positive or negative,
does it sometimes, like, [inaudible]?
Great question. So, the question is when I ask,
is this sentence positive or negative will it sometimes eventually
accidentally switch to a different one of the task and, uh,
we actually have a slide on that and the answer is it's surprisingly good at
knowing how to go about doing the task and where to get the answer where it's from.
Um, and yeah, they'll make more sense in a couple of slides once we go over the model.
Any other questions about,
uh, the question answering formalism?
Are you able to formulate text generation in the question answer format as well?
Like, tell me a story.
Good question. So can we do text generation, uh,
like tell me a story, uh,
from a random kind of -- or in this kind of formalism.
Uh, we don't have that as a task because largely it's really hard to evaluate.
It'll tell you some random stuff and then is that a good story or not,
is it grammatical, you have to come up with a lot of,
uh, sort of, uh,
evaluation metrics which we actually are doing for
some of the dialogue systems and in case of dialogue,
why does -- why are they equivalent because
the context can just keep on growing and every time, uh,
the user said something, uh,
you basically try to then predict the next answer in that dialogue.
And so I think you could very easily [NOISE] use this to generate texts.
Uh, you basically just ask -- tell it like what is, you know,
what's a good ending of the story and you maybe start the context with like
two or three words and then you ask the model to generate more and more words,
uh, in the form of this network I'll describe in a second. Yeah?
I was wondering like, uh, when you're training
it and you're trying to research like a new task.
Uh, does it like learn with less data?
That is an amazingly thoughtful question
and it's- it's so important we'll have a bunch of slides on it.
So maybe we'll- we'll go -- we'll continue and we'll get to that question, uh,
in a lot of detail because it's sort of why we're doing it and, the short answer is yes.
But we'll get to more details. All right.
So these are basically the 10 tasks.
Uh, and again this is the actual format for it.
So if you have a problem,
and you can cast it in this format, uh,
you can just take, uh, the open source code and run it and,
uh, it'll- it'll work.
And so when you kind of analyze and think about what we've done here.
In some ways, we've taken the tasks that
usually is kind of in your head but it's not given to the model.
The model is just given an input x and an output y in almost all of
the supervised systems and instead we're actually including the task in the inputs,
uh, in the set of inputs to the model. So you can kind of call this meta-supervised learning.
So again the question, uh,
is kind of our task definition for each of these different tasks.
The model has to figure out itself when to ask the question
that way it can also figure out itself when to
transfer knowledge from these other tasks and y is again just the answer.
So, in some ways it's meta-supervised learning and I'm quite excited
because once you allow the task to be given to the model as input,
it can kind of decide itself how to go about
solving that particular task and now you can learn,
uh, a lot more powerful models.
So once we had the dataset,
we thought "Okay, how do we now solve this problem?"
The simplest way is you could just say, "Well,
I have a big if statement,
I have a classifier in the beginning and then I classify.
If this is a machine translation task,
then run my machine translation model."
And in general, in Python that would still be just like one big python,
uh, model with a bunch of if statements, right?
And that's not the goal because then we wouldn't get to any of
the transfer learning and zero-shot capabilities that we're hoping for.
So [NOISE] we want to have the model wanted
to have the capability to internally adjust
to these different tasks and make these decisions itself.
And basically, all of those considerations and all
of those thoughts led us, uh, to this model.
So before I go, uh,
into a little bit more detail.
I'll just like sort of give you the high-level overview.
Again, you start with the context.
Um, you start- you ask a question about, uh,
that context document, and then we're going to generate,
uh, the answer one word at a time by either pointing to the context,
and you've had pointers already, right?
Pointer networks, all that? Great. Um, pointing to a question word,
or choosing a word from an external vocabulary with your standard softmax classifier.
Uh, and we'll have a pointer switch mechanism that will kind
of choose how much to weight [NOISE] each of these three generation mechanisms.
So, uh, let's dig into a little bit into this model.
Fortunately, uh, in some ways it's kind of just taking the best, uh,
of the current sort of the state of the art techniques and putting them together in a way,
uh, that- that generalize well enough.
Uh, you can look at all the code on decanlp.com,
[NOISE] it has like thousands of, uh,
stars and, uh, and forks and stuff combined, uh,
and you can, you know,
basically run everything, uh,
in this, uh, on these experiments with just one command.
It'll double, you get all the datasets and everything and- and run everything,
you can really explore what it looks like but let's- let's
dive a little bit into the details of what this model told us.
In some ways again, it just kind of takes
all the best ingredients from deep learning [NOISE] NLP,
most of which you've already learned about and puts them together in a reasonable way.
So we start with fixed GloVe embeddings.
Eventually, we'll- we updated, uh,
the embeddings to CoVe embeddings, uh,
and probably it'll work even better if you update them to BERT embeddings.
Uh, but at some point we kind of have to move on and do other things.
Uh, but basically, you have a fixed set of word vectors,
and that is kind of important because in some of these,
uh, data sets, they're much smaller than others.
Uh, and as you know from SQuAD,
if you actually backpropagate into the word vectors,
you just do really, really well on your trained dataset,
but then you won't generalize because of most of the [NOISE] text,
uh, test documents will include words you've never seen before.
So if you change all the word vectors during training, uh,
it won't- it won't work very well at test time and won't generalize the unseen words.
So, uh, fixed GloVe embeddings,
if you don't have word vectors, uh,
for unseen words, we also have character n-gram embeddings.
Then we pipe them through a simple linear layer,
and then we have a shared, uh,
bidirectional LSTM with skip connections.
And so, uh, it's a deep- deep one so you skip to higher layers,
and it's shared between the context and the questions.
So they have basically the same [NOISE] set of weights.
[NOISE] Then, uh, we have a co-attention layer.
Uh, where we basically just have outer products, uh,
between all the hidden states of those two sequences,
and again, have skip connections, uh,
to circumvent, uh, those as well.
So now you have kind of context or question dependent, uh,
contextual representations [NOISE] or- or representations of that context.
[NOISE] Uh, then we feed those into our transformer layers,
uh, and we actually tried to use transformers for all the things,
with having no LSTMs or any of that.
Uh, unfortunately, transformer layers were still, uh,
very, uh, finicky and very hard to optimize,
and there's a lot of trickery with- of the learning rates,
and we could just not get them to perform really well,
uh, on- on these 10 different tasks.
Uh, [NOISE] sometimes you had one transformer layer, one transformer network,
that worked really well in one task,
but the only other transformer network that worked well
on the second task had like half the layers.
And once you tried to have one network with the same number of layers,
it just wouldn't work on either of the two tasks anymore.
Uh, and so- so yeah, unfortunately as nice as they
are because they're nicely paralyzable in GPUs,
uh, they weren't yet robust enough,
uh, to- to be used for this.
[NOISE] So we have to have these LSTMs,
uh, before and after the transformer layers.
[NOISE] And then we essentially just have a standard sort of autoregressive, uh,
decoder where given the last state,
uh, we generate the next word.
And then we have these three pointer mechanisms.
Uh, they're very similar to the pointer ne- mechanisms you already know.
But now on top of these very contextualized representations, uh,
at the end of this encoder, uh,
and it basically learns to either point to question words,
context words based on the hidden states,
or have also a standard softmax,
and then we just basically have a weighted sum,
convex sum, of these three different distributions of output words.
[NOISE] All right.
So I think these are mostly standard components that you've already saw,
uh, for you- already seen all their details.
But if you have any questions,
um, about how we put it together? Yeah?
[NOISE] So the output- the output has to be a word.
That's right. The output has to be a word and it's always either a word from the context,
a word from the question or a word from the softmax.
[NOISE]
That's- the data preprocessing I guess it's different with each task.
So the data preprocessing is different for each task,
but we basically had to normalize everything to have
the same tokenization and- and all of that. [NOISE]
Uh, so do the double arrows in the encoding just represent there's a bidirectional?
Yeah.
Okay.
Yeah. But the double arrows,
uh, here are just bidirectional.
So left to right and right to left for the LSTMs. All right.
So what datasets, uh, are we using?
Uh, I mentioned that that was a big headache in the beginning.
Uh, we definitely wanted to include a lot of the sequence to
sequence tasks that we felt like are very,
um, sort of high level and I- immediately useful, uh,
and in some ways what this also shows you is that
nowadays you don't have to work as much on some of the intermediate representations,
uh, in NLP anymore.
Uh, you can just directly go for the end tasks that that real users might care about,
and then have these end-to-end trainable systems,
uh, that really do quite well.
And, uh, I've myself worked a lot on parsing.
And so I don't wanna, you know,
say we- we don't need it.
There's certainly still tasks that you do need it for,
but it's kind of surprising that you can just go directly to translation or summarization
without having intermediate representations that
were sort of very specifically hand-designed.
Um, so we had those three really interesting, uh, and hard tasks.
Question answering, machine translation, summarization.
They actually also have the three biggest datasets,
uh, of all of these.
Uh, then we had NLI, and basically, um,
all of these, uh, 10 datasets [NOISE] were, uh,
publicly available, uh, and in several cases especially for translation,
you could actually find much larger, uh, translation datasets,
but we also tried to keep it, uh,
to a- to a size where normal people that don't work in gigantic companies with huge, uh,
GPU infrastructures could still run experiments, [NOISE] uh, themselves.
So universities and folks, uh, can still run it on.
Basically if you have just a single GPU,
it'll probably take about a week or so, uh,
to run an experiment.
If you have multiple GPUs on one large AWS machine,
you can kind of run an experiment in a day or two.
And so especially for translation, right,
you could get a lot more data, uh, than IWSLT.
And each of these, uh,
communities and datasets and- and tasks has their own metric.
We actually tried to, in the beginning,
we had a lot of discussion about how we should
define the measure of success for this project.
Uh, it doesn't make sense, uh,
to have a normalized F1 score for basically all the different tasks,
but then we basically realized that
these different communities have different metrics for a reason.
Uh, unfortunately at least all of these metrics are from 0-100 in theory.
Of course, in practice, you rarely ever see, uh,
a translation system of a 100, uh,
or even high 90s of a BLEU score,
uh, or these really, really high ROUGE scores.
But, you know, in theory they go from 0-100, and so, uh,
we kept basically intact the different evaluation metrics for each of these communities,
and we just said we're going to sum them up.
And, uh, when we first talked about this,
we have- had a lot of discussion,
uh, with- with others also like, oh,
but translation is so much more important because it's much
bigger and it's a much more useful task than you still,
you know, silly like pronoun resolution Winograd Schemas
which only have a couple hundred training samples.
And so you should have weighted translation more and
then literally five questions later somebody's like,
"Why didn't you weight pronoun resolution more?
That is a really hard task that captures sort of common sense reasoning and, you know,
the complexity of language and semantics,
and unlike all this, like, statistical pattern matching [NOISE] that you do in translation."
And I was like, I used to talk to that guy [LAUGHTER] and like,
uh, hopefully in the end,
we'll just all agree that like it's reasonable to sum them up, uh,
and of course, you also have to tackle when you run experiments in this.
Uh, a lot of the complexity that you have in machine learning and,
you know, stuff that very few people talk about like having very skewed distributions.
So you have translation which has, uh,
millions or hundreds of thousands of examples,
and you have Winograd Schemas,
uh, that only have a couple hundred.
How do you train that such that you don't just completely ignore the smaller dataset.
Uh, so we'll get to some of the optimization trickery,
uh, that Nitish spent several months on in a bit.
But I first wanna sort of give you the first set of experiments.
So as you can see from all the numbers,
there's a lot of experiments, uh,
that we ran to even get to this,
and so we'll walk through this, uh, quite carefully.
I think hopefully you'll get some ideas also for- for ablations,
or experiments that you might wanna run in your, um,
in your experiments and in your,
uh, problem- final- final projects.
So what are we looking at here?
So basically, uh, on the left side,
we have single task performance.
So here, each number comes from its different model that was trained,
um, separately on just one task.
Uh, each row- each column here is the same architecture, uh,
and [NOISE] on the right side here,
we basically have, uh,
for each column is basically the same architecture and the same exact model.
So here, we have four different models and here, uh,
we have 40 different models,
and each column again is the same architecture.
And so the simplest, uh,
first column here is just a standard sequence to sequence
model with very few bells and whistles and some pointers,
but nothing sort of major.
It's pretty deep, you know,
stack bidirectional LSTM skip connections,
all the standard good well-tuned stuff for sequence to sequence models.
And, uh, then we added self-attention.
Um, this- this sort of, uh,
basically, uh, transformer layers.
[NOISE] Then we have this co-attention layer of
the outer products that we mentioned in the beginning,
and then we also added the question pointer.
So having the ability to point to a word in a question.
All right. Any questions about this table?
We'll dig into some of the details.
Uh, okay. Well, we'll dig into
the details first and then maybe you can think of some questions.
So let's analyze, uh,
what's going on in this table because there are a lot of numbers, uh,
and you really want to carefully analyze and sort of distinguish.
I think my first, uh,
observation was, wow, we can have a single architecture.
Like, even, even this is not quite what we want, right?
We want a single model.
But even this kind of showed us, wow,
you can have a single architecture that actually does really well and somewhat randomly,
in some cases, it actually had gotten state-of-the-art results.
So Wiki SQL, for instance,
this architecture had the best model
to translate natural language English questions into SQL queries,
which was a surprise to us because it is the ninth dataset.
It was really not like a priority for us and when we designed
the model and thought about how to generate words and pointer mechanisms and so on.
We just kind of had the standard context of SQL words
and we asked the question what's the translation to SQL, and then, uh,
somewhat surprisingly to us this particular architecture had the state-of-the-art, uh,
on SQL generation and bunch of folks in that community kind
of picked it up more quickly because it had state-of-the-art.
And that's- uh, unfortunately,
it doesn't have that many other state-of-the-art numbers, uh,
which is why it's harder, uh,
it's actually a much harder task.
And what you also observe is that,
uh, in several of the cases, uh,
using the multitask model,
so having a single model for all the 10 tasks,
uh, actually hurts performance at first.
And this is also something you rarely read in papers because papers
have a strong selection bias to only publish positive results.
Uh, and when you look at most transfer learning and multitask learning papers,
they're sort of an outside of the actual model consideration of like,
well, let's only combine tasks that we know will work well with one another.
And if they don't work and hurt performance,
then we'd just exclude them from our experiments.
And so you don't see many negative task results, uh,
in the literature and there are a few papers here and there that, uh,
study basically the opposite side of transfer learning and that is,
uh, catastrophic interference and catastrophic forgetting.
So interference is when you train two different tasks in the same model,
and to interfere with one another next, you hurt each other's performance.
And catastrophic forgetting is if you train continually
your first train in one task then you train on a second task,
people used to think,
"Oh, well, you know,
basically the first task will be completely
forgotten," and you just work well on the second task.
If you train neural networks sort of in a sequential way one task and then
another and somewhat surprisingly, uh,
we- we found that things aren't actually
catastrophically being forgotten in these models,
turns out that if you train them sequentially and
you add a little bit of the original to the first task,
it comes back very, very quickly.
So while the performance is really bad,
you can get to the really good performance very,
very quickly in very few iterations.
So but it's one of the many interesting sort of tidbits that we found,
uh, in the course of this that we haven't even published yet. All right.
So, uh, focusing on, uh,
the transformer layers here we basically find transformers
do help the original sequence to sequence model a lot.
So if you tune them carefully and you combine them with, uh,
some bidirectional LSTMs and so on, uh,
they were very helpful and improved, uh,
across a bunch of different datasets, in some cases quite significantly.
Another observation is question-answering and semantic role labeling,
uh, actually can predict each other's performance quite well.
If one works well, the other works well,
uh, and- and vice-versa.
If they don't work well,
uh, both of them don't work very well.
Um, and it's also interesting because both of those tasks have different questions for,
uh, every training example.
Pointing. Uh, so the question pointing,
uh, is super important.
Uh, we actually have in some cases, uh,
twice the performance even for,
and this is kind of surprising to us,
a simple classification task where you could just have a standard Softmax.
But instead of saying you have a Softmax of entailment, contradiction, and so on,
you just basically, uh,
point to the word entailment in the question.
And that was also the case for Winograd Schemas that also benefited a lot,
uh, from this pointer mechanism.
[NOISE]
Can you explain that?
Sure. Um, can we explain it? Why-
[inaudible]
Why does it help so much?
Um, in some ways,
I think partly is the whole architecture
has been gotten- has gotten better and better at pointing.
And part of the reason we actually do very,
very poorly in translation,
which is the only task that hurt in the- our first experiments a lot, uh,
in the multitask setting is that that is the only task that now has to generate,
uh, results from a completely separate Softmax,
whereas the rest of the architecture got really,
really good at pointing to things to answer questions, any kind of question.
Uh, and so but in some ways,
I think that is one explanation,
but I- I don't think it's- it's all of it.
I think we still need to figure out more why this happens. All right.
Now, multitask learning is the most
helpful when it comes to zero-shot and I'm actually very excited about that.
So this is a zero-shot relation extraction where you have different kinds of, uh,
relations that you might wanna extract and you might have never
seen like the student-teacher relationship that you're trying
to identify in a certain context or
a product company relationship or something like that.
And so, uh, that one actually, uh,
benefited a lot and almost got twice, uh,
as high in terms of the accuracy, uh,
when you learned it with everything else.
So these were questions, it's never seen before,
relations that it's never seen before,
and it got twice as good, uh,
and benefited a lot especially from having seen other kinds of questions.
And in some ways, we have to give a lot of credit to SQuAD too,
uh, because SQuAD as a dataset,
uh, kind of pushed people into thinking about pointers as a mechanism to generate answers.
And pointers, we kind of see them like as a given and they don't get that much credit,
but they allow you to predict answers that you've never seen before at training time.
To generate words, you've never seen before at training time,
which is actually quite- quite amazing. All right.
Now, the main observation though
here is that you still if you had an Oracle that would tell you
exactly which task you're currently in
and you would be perfectly kind of separating these into 10 different models,
maybe they're all the same architecture but there's still 10 different models, then, uh,
you would actually still do slightly better,
uh, than the first version of this multitask learning model.
And that is largely because we
chose to include a bunch of different tasks that have nothing to do
with one another and we wanted the community to start
thinking about tackling catastrophic interference, right?
If you learn like a new language or, you know,
you learn how to understand social media on Twitter,
you don't replace all your language,
uh, you know, in- in your brain.
You have one brain, it keeps getting smarter,
you keep learning new skills,
even when that skills that are new to you are very,
very different from old skills.
So in some ways we may have made our lives too hard,
and now we're actually thinking, okay,
maybe if you wanna publish a nicer paper on multitask learning,
we'll just look at all the tasks that do help each other,
and then we'll just, you know, have groups of tasks,
and then I can very quickly publish,
uh, some, some nice state-of-the-art papers.
But basically here, uh, we're still, uh,
quite significantly away in the decaScore between 10 different models and a single model.
Now, this of course is kind of an oracle score,
that's why we put it in parentheses because you don't actually have this oracle.
And in some cases,
it's quite easy to build an almost perfect classifier.
So, you know, separating what is the summary
based on that question and what is the translation from English to German,
you can do with almost 100 percent accuracy.
Uh, but, uh, SQuAD, question-answering,
and zero-shot relation extraction,
and question-answering as a semantic role labeling,
those are actually easily confused in terms of how
to generate the answers and you wouldn't quite know,
uh, which into which model to route, uh, this.
So in some sense, this is kind of theoretical. All right.
Now, I mentioned that we have this prob- this
complexity in the optimization strategy and this is one of the many,
um, sort of problems that don't get that much, uh, coverage.
But when you have a very,
uh, imbalanced or skewed dataset,
it's easy to lose track and basically overpower the smaller dataset tasks.
And so, uh, the first, uh,
simplest training- we actually tried a ton of different training strategies,
but in the end, this fully joint one worked quite well.
But actually promised to ask go wait for questions, uh, on this table.
So any questions on all these results so far? Yeah?
So, uh, [NOISE] since you mentioned that if you had
an oracle that will tell you which task it is and
you have two better ways having 10 different ones.
So really try training a model on
like data meaning what task is interested in this particular version?
We did. And so it- it confused, you know,
SQuAD and- and those too the quest- the other- basically the other,
uh, two types of problems that were also cast, ask question answering.
So it confused those.
Um, but then a lot of the others, it was able to like, very perfectly do it.
But then you basically, as soon as you,
uh, were to try to then build a whole model and get a decaScore,
if your- if your classifier is even like 90 percent accurate,
you basically multiply this by 0.9 and
you get dinged so hard that it- it's not competitive anymore.
So it is actually hard if you try to just build
that whole system and keep adding sort of if-then else statements,
uh, to make that, uh,
into sort of a single system. Yeah?
Have you tried telling the model what kind of task this it's doing,
just giving that indicator of the kind of task quickly?
I mean, in some ways,
we did in this case,
because we only trained each model separately on it.
[inaudible]
Um, only through the question.
Yeah. Because I was thinking the
um, maybe it's not that important that the model figure out what we want it to
do in- in a practical [NOISE] application
if we could just tell it what we want it to do right now?
In some cases, you could tell.
Uh, so the question is sort of,
uh, and even in the multitask setting,
you could have like an extra kind of token to say,
"Now, you're doing summarization.
So, and that's another input."
Uh, in some ways,
whether you have a summarization token,
uh, or you ask what is the summary?
It actually I don't think makes that big of a difference.
It's just now you can query this model in
very natural language rather than having to know
kind of a special token to, to query the model.
Uh, and we'll see actually in a couple of slides that the model is not confused,
uh, when it comes to how to generate the answers.
So, for every of the task,
it knows very clearly how to generate the words to get to the right,
to get to, you know, a reasonably accurate answer.
[NOISE] Um, in the- [inaudible] does the model
see all of the data and then [inaudible] that class or does it only include a [inaudible]?
Oh, great question. So, how do we train, uh, the single task models?
They're only trained on that dataset.
So, the SQuAD number here is just a single model that has only seen SQuAD training.
[NOISE] So, your point about the,
um, the pointer exception for the, uh,
[inaudible] generally more helpful than [inaudible]?
Somewhat surprisingly, even, ah,
in the case here, uh,
where we had, um, this is MultiNLI,
this particular model, I mean,
if you just have the standard sequence to sequence,
it just generates, you know,
also with a softmax, uh, that label.
So in that sense, it's quite similar.
Uh, but yeah, it was actually better able to just point, which actually led us, uh,
for a while into thinking about maybe we should have a project where we just say point to
all the things and just get rid of softmax classifiers forever.
Um, the problem is when you then try to do translation also,
it's like okay wow,
what do you point to,
and then you kind of pre-train it and do
some alignment and it gets kinda very large and you point to a lot of different like,
you may have like- like tens of thousands of potential candidates.
So we kinda discarded it as like a single unifying model for all the things,
but you could point to a lot of different,
like a lot of these tasks,
you could actually point to and
I think it's another interesting side project that could spawn from this, yeah.
Just a quick question to how,
how sensitive [inaudible] how sensitive, uh,
the individual components [inaudible] was when you
slightly perturb the relative weights of them in the loss function?
So, we -- the question is, uh, how, um,
sensitive were the tasks if we were to,
um, add weights to the different tasks?
We [NOISE] did in the optimization kind of did a lot of trickery on
how to train it but we never said this task only matters like 0.5 or something.
So, we didn't do that analysis. Yeah?
Co-attention seems to be a burden a little bit.
In some cases, yeah.
Is it the [inaudible] co-attention and order but no co-attention or is that kind of like,
"Oh, you already saw the test data so, like, you can't use these."
I mean, these are all dep sets.
Um, but it's, you could definitely do even more architecture engineering.
In fact, there's this whole field which I don't think
you gotten to, right, neural architecture search?
Yeah. So like you can actually combine your reinforcement learning, um,
and you say the action space for the reinforcement learning agent
are trying to have a couple of
different modules of neural nets like maybe you want to have
like a CNN layer and then like
a memory layer and then an LSTM layer and maybe it's bidirectional and you
basically let a reinforcement learning agent figure out all of these decisions.
Uh, so I think it would be phenomenal to try to apply
neural architecture search not to what's
usually being done which is we already know how to do image classification,
we'll just do it slightly better with NAS, neural architecture search.
But we actually try to find
a single architecture for multi-task learning which we don't know.
The problem of course is that already getting to these.
All these numbers took a lot of compute time and a lot of
fiddling around with stuff and it is, I can,
I can only give you sort of an idea of like how often we'd say,
"Oh man, we got like this really amazing result
in this task but it needed this learning rate."
And it turns out the same model,
same set of hyperparameters everything,
but this other task to get to good performance needed a much higher learning rate.
And now, you try to combine those two tasks only together and you're like,
"Okay, how do you choose your learning rate now?"
You choose the, you know,
if you choose the task, the learning rate from the task that is, you know,
bigger than the smaller tasks just doesn't work
well at all because it needed this higher learning rate.
If you'd use the higher learning rate that the smaller task and the smaller dataset,
uh, did really well on then the large one just overfits and doesn't work well either.
If you try to do the average, neither of the two work.
Like there's a lot of complexity in trying to do multitask learning.
That's why, that's why it's such an interesting I think, uh, research challenge.
All right, any more questions about this first set of results?
They get, they will get better.
We, we have, we have had some ideas already,
uh, on, on how to improve them.
All right. So, uh,
how did we actually train this whole thing?
Um, we had tried a lot of different things but in the end, uh,
this very simple fully joint training strategy actually worked the best.
Uh, and that is you basically take a mini batch from each of
the different tasks and you just train on that mini batch from that task.
So basically just going through all the 10 tasks and then round robin,
uh, go through them.
Um, now it turns out, ah,
that that does not work,
uh, quite as well, uh,
as another training strategy and if you look into optimization,
uh, strategies in neural nets, uh,
there are actually a couple of papers on
so-called curriculum learning, where the idea is,
you start with training your model with simple pro- simple instances of your problems.
So, in translation, for instance you start training with
very short sentences and then you go to larger and larger,
uh, sentences, uh, or longer and longer sentences.
Uh, now it turns out for multi-task learning,
you actually want to do the opposite.
You wanna do anti-curriculum learning.
Uh, and that is you start with the hardest tasks and you iterate on
those for a while and then you add the simple tasks later on.
And to some degree, I think this is intuitive because when
you train this very gigantic and powerful model,
uh, on a very simple task like
sentiment and you just need to classify everything to be positive or negative.
You train all of these weights and you arrive at sort of, uh,
local optima that are quite deep and very
specific to just generating these two words and if you then try to get out of that,
out of this local optimum for that very simple task
and then try to generate all these other kinds of words and point to different,
you know, words it's never seen before then SQuAD,
it's very very hard to come out of that local optimum.
And that is sort of my intuition of why it actually makes more sense to say,
"Let's start with SQuAD and machine translation and a couple of these harder tasks.
We'll make the model very general purpose.
It has to generate a lot of different things,
create a softmax, German words,
it has to point to all kinds of
different words and be able to parse all kinds of different Wikipedia paragraphs."
And you do that a couple of times and then once you've finished,
uh, this sort of pre-training, uh,
stage or anti-curriculum, then you move on and add sort of the simpler smaller tasks.
So [NOISE] with that, uh,
relatively simple change that did take us,
uh, a lot of different experiments to get to.
Um, we actually, uh,
closed or, uh, um,
went closer to closing that gap and now, um,
we're only sort of, um, 14, uh, away.
Right, yeah, uh, 14 or so.
Uh, but there's still, uh,
a big gap and the biggest, uh,
nuisance and issue that we had was with a translation.
Basically, if you look at all of these,
most things are kind of similar,
get slightly better, um and it's sort of a toss up but then and,
and roughly similar, but translation was really bad.
It's almost only half, uh,
the performance in the multitask learning setup,
and part of that is because translation was the only task that had
a very large Softmax vocabulary of words that were in no other task.
And most of the other tasks,
actually were doing really well with pointing.
And so, uh, my interpretation of this was that the intermediate layers,
all these representations that we learned with
bi-directional LSTMs and transformers, they got really,
really good at being pointed to,
like creating hidden representations that the answer module can point to very accurately.
And then you have this one task that is like,
I don't point to almost anything,
I basically just generate other words and then different vocabulary.
And so those hidden representations became less useful for that task.
And so, that was one of the insights and that led
to one of the ways of trying to improve this.
Now, one of the interesting issues that we had is,
when we improved the model,
the multi-single model for all 10 tasks,
a lot of times we said, well,
but now we also have to go back and run
10 more experiments on all the single tasks to have a proper comparison, right?
Because if you tune the thing you care about,
and you stop tuning the thing you wanna show you can do better than,
then that's not fair.
Uh, so you always wanna give as much, uh,
TLC and focus and experiment time to your baselines.
And so, uh, in some cases we actually,
uh, improved some- improved something.
But then, we improve both the 10 separate models and our model,
and some cases like the 10 separate models improved, even more.
So the gap got even larger.
It's kind of the opposite of what we wanted to show, but in general,
it's better for both tests,
uh, for the architecture overall.
So basically, we started, uh,
with this fully joint training and we have
this sort of set of single models that we could,
in theory with some oracle,
kind of just sum up, uh,
in their scores, to get a decaScore.
So the gap started at 23.
And then, uh, we basically did this anti-curriculum training,
uh, which, uh, lowered the gap to 15.
So we're kind of excited,
uh, making good progress.
Then we switched, uh,
from GloVe and use CoVe.
So contextual vectors, um,
which actually increased the gap a lot again.
So everything got better, but the 10 separate models got
even better than the one single model that does the 10 tasks.
Um, so the gap got bigger,
but everybody's performance increased.
So it was still overall a good thing.
Uh, and then, uh, we basically figured,
especially with this machine translation issue,
we shouldn't just pre-train on SQuAD,
but we also should include machine translation in
this pre-training in the beginning so the model doesn't just start learning to point.
Um, and that helped us, uh,
to reduce the gap between the 10 separate models,
Oracle, and the single model to about five points.
And then, uh, we basically said,
okay, translation is still not that good.
We just keep oversampling.
So, every time we go through one of these round robin mini-batch sets,
we just always include machine translation.
And that basically allowed us to then reduce the gap,
uh, to just a single point.
So now, uh, we started, uh,
couple of, several months ago, uh, at 586.
And now the single, uh,
oracle with 10 different models,
if you were to sum them up,
get 618, uh, and the, you know,
better contextual vectors and tuning and adding a lot more translation,
and translation is still not as good as we would like it to be, uh,
but now, several of the other tasks benefited a bunch.
And now we're basically one decaScore away from
having a single model that does as well as 10 different ones.
And you can basically,
you could run even more experiments,
in some ways you could burn millions of dollars on AWS cost here,
because most of the time we kept the hyperparameters of these different models the same.
Like each of these, you could also say, well,
maybe this multitask model needs to have 50 more layers,
or maybe 19 more layers,
or maybe five more layers and maybe they should be 1000,
you know, wider in their hidden dimensions.
And you could basically run a lot more experiments.
Maybe hopefully, eventually, the community jointly does that,
and then we can kind of move, move towards that.
But we figured, okay, we're pretty close,
so we moved on to some other things which maybe I'll tell you about next year.
[LAUGHTER] But basically, um,
let's do some analysis of what happened in this project.
And this is kind of, I think something that I would encourage you all to do as well.
Like you, you can chase the numbers for a while and in some ways,
you should always be skeptical about your evaluations.
And in some cases,
you've seen- we've seen in the NLP community people
like basically just optimize BLEU scores for translation for years.
And then somebody came out with a paper and said, well,
it turns out BLEU metrics and human evaluations on how good of a translation is this,
aren't actually that correlated.
And you're like, ah, that that sucks,
we just spent years of our lives tuning that metric and publishing a bunch of papers.
Um, and so in some ways all of these metrics have flaws, uh, you know,
root scores summarization is a super,
uh, subjective kind of a task.
And summarization, for instance,
when you analyze the errors, uh,
you often realize that word vectors have problems too.
So, for instance, the word vector for Jason, John,
and Jeremy are all kind of the same, right?
They all have similar, uh,
distributions, similar contexts, windows, and so on.
And so word vectors of names are very similar.
And so in summarization errors, you realize, oh,
well, you know, this article, news article talked about Jeremy being kidnapped.
But the summary said that Jason was kidnapped.
And you like, well, you know, in the evaluation metric
that's just one word is off and like, all the rest is correct,
but it's a pretty important word.
And so, word vectors have like issues
for summarization that are pretty fundamental and I don't think,
uh, anybody's tackling really well right now.
Uh, and so all of these metrics have issues.
I would argue though that combining the 10 actually
makes it less problematic and more meaningful,
than looking at each one separately.
Uh, because now you can't use the idiosyncrasies of
one particular evaluation metric to just get like your score a little bit higher.
Um, because then, if you just tune with that particular thing in mind,
it will hurt some of the other tasks and you won't get to the sort of general,
uh, NLP model that much more easily.
All right. So now, let's do some analysis uh,
of this model and, uh,
look at, and this is the kinda thing that comes to one of the questions that was asked.
Uh, is this model able to kind of generate the right words for the right tasks?
And here, we basically looked at the distributions of how often, uh,
the model generated words in these differen- with these three different mechanisms,
Softmax vocabulary, context pointers, or question pointers.
And, uh, as you can see,
in the majority of cases it knows exactly how to generate.
So, uh, for, uh,
question, answering, and semantic role labeling,
and SQuAD and Wiki SQL and,
um, summarization, it basically uses the context pointer.
So it just points into the context document.
And we know for SQuAD,
that is basically [NOISE] how the data set was generated.
So that's the only thing that that really makes a lot of sense.
Uh, what's kind of cool is that in some cases like summarization,
it sometimes creates new words or, you know,
that weren't in the context document wherein pointed to.
Uh, and for zero-shot relation extraction,
also sometimes uses, uh,
this external vocabulary and in some cases the context pointer.
So for the most part, uh,
this model doesn't- is not confused how to execute on a task given, uh,
this question formalism rather than, uh, the,
uh, format of sort of this is the task,
just do this particular test.
Now, um, you might argue,
okay, I'm not that impressed by, you know,
having the performance be slightly the same with one model versus
10 separate models even though it's nice if you wanna deploy it right,
like, uses less RAM and all of that,
assuming they're the same size,
uh, while, you know, one-tenth the size.
But what I'm excited about is more like the next couple of results.
And namely, sort of this transfer learning,
domain adaptation, and zero-shot,
uh, these kinds of capabilities.
So here, uh, we chose two data sets that weren't included in the original 10.
And we basically trained a pre-trained model on this versus a random model.
And, uh, randomly here again,
they're the same architecture,
and pre-trained means the entirety of the model was pre-trained.
All the, you know,
encoders including the decoder in the Softmax and everything, uh,
and to two other tasks where another IWSLT language pair namely,
translating from English to Czech, uh,
and named entity recognition tasks that you all know very well.
So basically what we found is that,
uh, it converges much more quickly,
uh, in the beginning, uh, and then,
there's still a significant but not gigantic gap.
So this pre-training on these completely separate kinds of task had helped.
And, uh, I think that's,
that's pretty exciting, um,
especially sort of the quicker convergence, like,
learning more quickly, uh,
whatever new task you, you come up with,
which also means in some cases you can get away with
less training data on these new- on these new tasks.
Uh, now domain adaptation is kind of the simpler form of transfer learning,
where you basically just have a different,
uh, type of, uh,
you know, distribution for your words.
Uh, we mentioned we have the Stanford Sentiment Treebank for sentiment analysis.
Uh, and then we analyze this on different,
uh, sentiment data sets,
namely Amazon product reviews and Yelp restaurant reviews,
and out of the box without any training,
the model just got 80% accuracy on both of those data sets.
Uh, and I think for practitioners,
that is pretty exciting because you basically didn't have to train anything,
it just kind of worked out of the box,
download it from GitHub, and run it.
Uh, SNLI, that was slightly different.
It didn't quite work as well.
It's another natural language inference data set,
but has very different- a very different distribution, different, uh,
kinds of domains, uh, that,
uh, these entailment questions are asked over.
Uh, and here, out of the box it achieved 62.
Uh, but then, uh, once you fine tuned it and
similar to these experiments here continue to actually train on this data set,
it quickly uh, converged to 87 which was
still two percent gain over a randomlyor initialized McCann model. Yeah.
In that experiment, did you evaluate how much less data you can get away with?
Did we evaluate how much less data we can get away with? We didn't.
And in some ways, whenever you would run this experiment,
you'd basically be like, you'd still not do as well.
Like, everything- all these models will still do better with more training data.
So you just kind of, it would be a fuzzy kind of say,
like, cut- fuzzy sort of result, right?
Where you say, well, with one-tenth we might get
to 50 and the other model might get only to 40,
doing something like that.
Um, we don't- I don't have those numbers.
It would be kind of actually also a neat, neat, uh,
analysis to do. Yeah.
So if you wanted to like train on a new task [inaudible].
Yeah.
[inaudible] .
So, do we have the code to train a new task? Yes, we do.
Um, you can just, uh, edit,
make it into this format using context.
Here's a question, simple like CSV type format,
and then you add it and you can both like train the pre-trained model yourself.
You can download a pre-trained model and just add it. So I'll look it up, yeah.
Do you know how this compares to using other kinds of pre-trained representations like, say BERT?
So, um, it's a great question.
So how does this compare to other pre-trained representations like BERT?
So, in some ways,
people say BERT is kind of this model that does everything,
but when you actually read the paper, you realize, well,
it's a separate model for these different tasks, right?
If you wanted to have a classification task,
you have a little token in the beginning,
and you have a different top layer.
If you wanna do a sequence labeling task,
you have a different top layer.
If you wanted to do a sequence extraction task,
you have a different top layer.
So, BERT isn't actually a single model for all of these different tasks.
Ah, and then, on all the results,
there's a lot of extra tuning for each of the data sets,
and tasks, uh, that, you know,
different learning rate for this task, uh,
different size, or different sets of BERT, and so on.
So, we're also super excited, we're like maybe this is it,
we'll just run everything on BERT,
and then we looked into all the details,
and there's so much excitement in the beginning.
And then the more we dug through the details,
the less excited we became as this being like sort of the answer,
because it is not a single model.
Uh, in some ways, it's probably better to- for pre-training.
So instead of CoVe,
you can have kind of BERT at the very beginning,
and my hunch is everything will get slightly better,
but you still need to have, um,
a lot of the- a lot of the other sort of modeling architecture on top of it.
Uh, and then the sad thing is to really get the state of the art results,
there's a lot of very spec- task-specific tuning of those last top layers.
So, if you try to unify that task-specific tuning,
you lose a lot of the good performance of BERT.
Um, so, unfortunately, it's not quite the sort of,
"Oh, just use BERT for it,
and you'll just have state-of-the-art numbers and all the things."
Um, I could probably go like talk about it a lot more, but, uh,
I think it still makes sense to think about, um,
some of the ideas from BERT,
like basically, add as one of the tasks language modeling.
That would be very likely the task that helps the most for all the other tasks,
and we should include that, uh,
it also would be nice to have a faster model right now.
Um, it's hard to do language modeling is very, very large,
it benefits even more from,
you know, billions and billions of words.
It's hard to train the McCann model,
this current question answering model of the co-attention mechanism of the question
with like an increasingly large context.
So you'd have to kind of split it also like BERT,
works also reasonably well only for like at most I think 500 words or so,
and if you wanted to do summarization you'd basically have to cut
the original document to only 500 words, and then try to summarize it.
So, there are a lot of like devil in the details that they didn't have to figure out,
because they said, "Well, we'll just sort of just like word vectors,
we can take them in, and then we do a lot of other stuff that is task-specific,
um, with those- those word vectors,
or with the BERT architecture."
I still- I don't want to- this BERT is obviously amazing,
and we are looking into trying to use ideas from it.
But unfortunately, it wasn't just sort of a silver bullet to
solve multi-task learning. Mm-hmm?
Pre-training process to be considered, uh,
prioritized sampling based off of how much fewer group, how much loss there is?
Sorry, did we- say again?
Would you consider prioritizing sampling [inaudible]?
So, did we consider prioritizing the sampling?
So in some ways with this pre-trained strategy here, um,
that's kind of what we did by basically focusing on these really hard tasks.
And, uh, a lot of like the gap in the end was improved by really waiting for,
like four of the tasks at the very end,
uh, bef- unti- you know, uh,
until after you're gone through, uh,
sort of oversampling all of these,
uh, really hard tasks.
In the last 10 minutes, uh, basically, uh,
th- the most exciting thing, uh,
for- for last though I think you could also do a lot more work in this direction.
Uh, I mentioned the sole question pointer
and zero short learning in the beginning, and, uh,
we basically just tried to play around with that a little bit, um,
and found that in some cases,
it actually kind of magically works.
Uh, so here, we tried, uh,
a sentence John had a party,
but no one came, and he was all alone.
And then we asked, "Is this story sad, or happy?"
And while the model could've, you know,
generate some random German words,
or some random SQL words,
or it's just said whatever,
it actually pointed to, of all the words,
you could've pointed to in the context or the question that
pointed to "Sad", which is pretty cool.
Like- and it's just one small sample,
and, you know, you could do a lot more,
you could try to come up with a very large zero-shot kind of classification data set,
which is actually kind of hard too.
You have to be quite creative, it's not like you can just say, "Oh,
it would just take all these reviews,
and label them as these, you know, positive negative.
Ah, but so, I think we- we need to do more work in that direction.
Somebody will hopefully create a zero-shot kind of task data set,
that is not just zero-shot for, you know,
kind of new distributions or something with completely different, uh, outputs.
Uh, but we- we tried a couple,
and it doesn't always work, right.
You can be adversarial about it,
you can make this basically looks most similar to,
is the sentiment positive or negative?
Uh, is this sen- is this sentence positive or negative?
That was the formalism we had for sentiment analysis.
And so you could,
if you make the question more and more different,
eventually, it'll kinda get tripped up.
Ah, and it's clear that it's benefited, uh,
from the word vectors,
of sad being closer to negative,
and then understanding sort of through all these,
uh, correlations, and- and, uh,
deep representations that there are other sort of sad words in this context,
or- or whatever it is.
Uh, and so, it was able to point to this.
But you can be adversarial, it doesn't always work.
But even the fact that, uh,
it was sort of zero-shot classification based on word vectors, uh,
for new kinds of questions,
uh, personally, it was very exciting to me.
And we tried a couple of other things like,
uh, Bryan gave a talk and nobody clapped.
Was Bryan happy, or sad?
And it also got it right.
So, um, there are a couple- a couple of the,
the examples were, were at least as happy or sad thing worked.
And then, uh, a couple of other sort of adjective questions that we,
we tried but, um,
what I'm- what I would be most excited about is eventually actually
trying to have a zero-shot classification task,
uh, that combines the different tasks too.
So, uh, unfortunately, there's no data set for that,
so we didn't train it, so it doesn't happen with the model.
But in theory, if you ask what is the sum- you can summarize,
and you can translate from English into German,
why couldn't you ask the model for a German summary?
And if that worked, eventually,
that would be even more amazing,
but it, it doesn't work right now,
because we never ask it sort of for these
compositional task- these compositional task questions.
But is yet another interesting line of research that I think could spawn from this.
Uh, all right.
So, I hope I could show you that this sort of
decaNLP framework is an interesting new benchmark for generalized NLP.
Uh, I do think it's a reasonably good framework
for tackling a bunch of the really hard questions in the field.
Uh, more general language understanding,
and question answering of course,
uh, multitask learning, domain adaptation, uh,
which we sort of analyzed a little bit with the sentiment,
and SNLI versus multi NLI,
um, transfer learning, and then weight sharing.
I think it's clear, everybody loves weight sharing,
you wanna share as many weights as possible.
Uh, word vector started at, uh, ELMo,
CoVe, and now BERT basically share more and more,
deeper and deeper layers.
It would be great if we can unify that last bit also, uh,
and then share basically the entirety of the networks,
and then eventually hopefully get to zero-shot learning.
Now, there's a bunch of related work.
The original paper has over 100,
um, citations in it, uh, of,
of, you know, papers to other,
other, um, lines of, uh, work.
But, uh, this is actually zero- at least some of
the models and papers that influenced us the most,
uh, in, in our thinking and modelling.
Uh, one of them actually comes from,
uh, the two instructors of the class.
And so, um, hopefully, uh, we can,
you know, sort of think about what- what's next after all this architecture engineering.
And, uh, I think one potential answer to that, uh,
is single multitask learning for more generalized NLP models.
[NOISE] All right. Thank you. [APPLAUSE]
 Okay. Hi everyone.
Let's get started [NOISE] Okay.
So, so for today's lecture,
what we're gonna do is look at the topic of having Tree Recursive Neural Networks.
I mean, this is actually, uh,
a topic which I feel especially fond of and attached to,
because actually when we started doing deep learning for NLP here at Stanford in 2010,
really for the sort of period from 2010 to 2015,
the dominant set of ideas that we were working on was this topic of how you
could build a recur- recursive tree structure into neural networks.
So in a way, it's kind of funny that I'm only getting to it now.
I mean, there are sort of reasons for that,
but I think there are a bunch of interesting ideas
here which relate closely to linguistic structure,
and so it's good stuff to have seen.
But in practice, um,
these ideas have proven kind of hard to scale
and not necessarily to work better in practice than
the kind of things that we've spent more time on
meaning things like looking at LSTMs and looking at transformers,
and things like that.
And so that's kinda why we sort of shunted them towards the end of the curriculum.
But I want to sort of say something about the motivations,
and the ways you can build tree structures,
and neural networks, and look at some of the possibilities,
um, we explored um,
in during this class.
Um, another fact about this class is actually this is the last class I'm going to give.
Um, so two more classes next week.
Don't forget about next week, um,
CS224N classes, um,
but on Tuesday, um,
we've gotten the final invited speaker, ,
who's a great speaker and has tons of interesting stuff
to say about fairness and ethics in NLP and AI.
And then for the final lecture,
one of my- another of my PhD students is
gonna give that and talk about some of the recent,
what's been happening in deep learning in 2018, '19,
of some of the sort of recent developments in NLP and deep learning. Um,
so, um, let's- I'll say my farewells at the end of this one.
Um, so hopefully, everyone has submitted, um,
their, um, milestone for their final project.
If you haven't, you should really begin your milestone in- um,
you know, it's inevitable that somewhere around here,
there start to be problems that people have the situation that nothing works,
and everything is too slow, and you panic.
Um, and, um, this happens.
Um, I wish you luck, of course.
I mean, what can you do about it?
I mean, it can be really hard when you have things that don't work as to work out,
why they don't work,
and how to fix them.
I mean, I think often the best thing to do is really to go back to something
simple that you can get working and to work forward from there again.
It also really helps to have really small data sets.
I really recommend the strategy of sort of having a 10-item,
or 20-item data set and checking that your model works perfectly,
over-trains to 100 percent accuracy on that kind of data
set saves you huge amounts of time,
and it's sort of after you've gotten something simple working on a small amount of data,
that's the right time to sort of then,
um, expand forward again.
Um, you should definitely always make sure that
you can completely overfit on your training data set.
That's sort of, um,
not quite a proof,
but it's at least a first good requirement for your model being implemented properly.
Um, you, you know part of the trick of being
a successful deep learning researcher is actually
managing to get things done and not wasting a ton of time.
And so it definitely always helps just to be, you know,
plotting as you go along your training and
dev errors so that you can sort of tell if things are working,
or if things aren't working,
and you should abandon and start again with a new experiment,
tha- that just things like that save you hours and get you, uh, more done.
And so then once things are working,
there's sort of a whole bunch of things to make it work better.
There's regularization with L2 and Dropout,
there's time to do hyperparameter search,
um, and, you know,
often doing these things and make quite a lot of difference to what
your final results are and so it's good to have time to do those things.
But clearly, you want to get things, um,
working first before you go on to that, um,
and sort of really encourage people to still
stop by in office hours if you've got any problems,
and we'll try our best to help out here within
the limitations of what we can do from just being hit cold with problems.
Okay, um, yeah.
So, I wanted to sort of just say some general remarks about, um,
language and theories of language,
um, that, in the context that motivate these tree recursive networks.
Um, so this is an art installation at Carnegie Mellon University.
And as an NLP person,
I really love this art installation.
Um, so we need better art installations around the Stanford School of Engineering.
Um, so this is the bag-of-words art Installation.
There's the bag with a lot of words in it.
And you see down here,
there were the stop words, the the, and the us,
that had fallen out of the bag,
and are represented on the ground as the stop words.
Beautiful artwork, right? So, um,
one of the interesting things that has been found about NLP models of language,
and I think this is even more true in
the deep learning world than it used to be previously is,
boy, you can do a lot with bag-of-words models, right?
That you can just often get a lot of power by saying,
well, let's get our neural word vectors,
we're gonna average them or max pool them,
or something like this,
and do nothing more,
and that gives me a pretty good sentence representation or
document representation that I could use in a classifier or something.
And sometimes, you can do not much more than that and get even better.
So people have done things like deep averaging networks where you're taking
the output of a bag-of-words model and sort of
feeding it through a couple more layers and improving things.
So that is in complete distinction to
what's been dominant in linguistics of looking at language structure.
That typically in linguistics the emphasis has been on identifying kind of
huge amounts of structure of linguistic utterances through very complex formalisms.
I guess this is sort of a bit of a picture of a Chomsky minimalism syntactic tree,
and the one up at the top is a bit of a picture of head-driven phrase structure grammar.
Which was a theory that was predominantly, um,
developed at Stanford in the '90s.
Um, but sort of very complex data structures and
articulated structures used to describe linguistics.
And there's a huge gap between these two things.
And you might think that, you know, surely,
there's some good points in the middle where we have a certain amount of structure,
and that's going to help us do what we want.
And so in particular, um,
that if we're wanting to semantically interpret language,
it seems like we don't just want to have word vectors,
we want to have meanings of bigger phrases.
So here's the snowboarders leaping over a mogul,
and a person on a snowboard jumps into the air.
And what we'd like to be able to say is that the snowboarder
means basically the same thing as a person on the snowboard.
So we wanted to have these chunks of
language which in linguistics will be constituent phrases,
and say that they have a meaning,
and we'd like to be able to compare their meaning.
Now, we've looked at at least one tool that allows us to have chunks of language, right?
Because we looked at convolutional neural networks where you could take
three words and make a representation of the convolutional neural network,
but the fundamental difference is that in
human languages you have these chunks that have meaning,
that are of different sizes.
So we'd like to say the snowboarder
is pretty much semantically equivalent to a person on the snowboard,
but the top one is two words long,
and the bottom one is five words long.
And so if we're gonna be able to do that, um,
we somehow wanted to have these sort of constituent chunks and be
able to work with and represent them in neural networks.
And that's sort of, um,
the central idea of what
motivated some of the sort of tree structured neural networks that I'm about to show you.
There's another related thing that you might wanna think about is, you know,
a person on a snowboard,
how do human beings manage to understand what that means?
And then a person on a snowboard jumps into the air,
how does people manage to understand what that means?
And it sort of seems like the only possible answer to
this is what's normally referred to as the principle of compositionality.
That people know the word person,
they know the word on,
they know the word snowboard, therefore,
they can work out what on a snowboard means, um,
and they can work out what person on a snowboard means by knowing
the meanings of components and putting them together into bigger pieces.
There's a f- there's a famous,
um, applied mathematician statistician, um,
at Brown University, Stu Geman, and I guess the way he summarized this is,
either the principle of compositionality is true, or God exists.
Um, for [LAUGHTER] which he was, um,
well you can take that as- as you want but, you know,
um, I think what he meant was well, you know,
you can just make these infinite number of
infinitely long sentences and human beings understand them,
that it just has to be that people can know about
words and ways to combine meanings and-and make bigger meanings because,
you know, how else could it possibly work that people could understand sentences.
And so we want to be able to do that.
We want to be able to work out semantic compositions of smaller elements,
to work out the meanings of bigger pieces.
And that this obviously isn't only a linguistic thing,
compositionality, um, appears in other places as well, right.
So, um, if you want to understand how some piece of machinery works,
what you kind of wanna know is it has different sub-components.
And if you can understand how
the different sub-components work and how they're fitted together,
um, then you might have some understanding of how the whole scene works.
Um, and, um, compositionality seems to be wor- at work in vision as well.
So here is a scene and again it seems like this scene has parts.
So there are little parts that go together, right.
So there are people that go together into a crowd of people,
and there's a roof and a second floor and another bit of roof.
and a first floor that go together into a picture of this church.
And so this is also kind of a compositional scene in which pieces go together.
So it sort of seems like certainly for language understanding,
and then really for a lot of the other things that we use for intelligence,
that we somehow need to be able to understand
bigger things from knowing about smaller parts.
Um, yeah, so computational- so the most- I mentioned this earlier,
sometime the most famous, um,
linguist is Noam Chomsky at MIT and,
um, you know, really computational linguists,
a lot of the time haven't been that friendly to, um,
linguistics linguists and in particular some of Noam Chomsky's, um,
theories of language because really he's never been
sympathetic to the idea of machine learning.
Or in general does some of the empirical ability to learn from data.
He's sort of always been, um,
[NOISE] wanting to refuse to that exists.
But, um, if we nevertheless look for a little bit of,
um, insight on that.
Um, you know, this is a recent paper of Chomsky's with authors and that they're sort
of trying to give a version of what is unique about human language.
And essentially what they, um,
zero in on is that well,
if you're sort of looking at, you know,
humans versus other fairly intelligent creatures.
They suggest that the defining difference of human beings, um,
is that they have this ability to model recursion.
And so the- this paper argues that the- the singular distinction that allowed
language to develop in human beings was that we
could put together smaller parts to make bigger things,
in a recursive process and that that was the sort of defining new ability.
Um, not sure I- not sure I believe that or not,
um, [LAUGHTER] you can decide what you think.
But what I think, um,
is certainly the case is that- it's just incontrovertible that
the structure of human language sentences have these pieces,
um, constituents that then form together hierarchically or
recursively into bigger pieces as you go up in the tree.
And in particular you get this recursion where you get a little noun phrase meat,
which then appears in a bigger noun phrase like spaghetti with meat.
And you can repeat that several times,
giving you a recursive structure.
And I have an example of that in blue up at the top.
So the person standing next to the man from
the company that purchased the firm that you used to work at,
um, that whole thing is a big noun phrase.
Um, but inside that there's a noun phrase,
the man from the company that purchased the firm that you used to work at,
which is another big noun phrase.
And well inside that, um,
there are smaller noun phrases like,
the company that purchased the firm you used to work at.
But, you know, it's still got inside that noun phrases like,
the firm that you used to work at.
And actually even that's got it inside,
the smaller noun phrase,
which is just the word you.
So an individual pronoun is also a noun phrase.
Um, so just kind of structuring of
language where you get this sort of
hierarchical structure and the same kind of things inside them.
I think that's just sort of totally, totally correct.
Um, the- the claim then that,
you know, our language is recursive, I mean,
in a formal sense is not quite clear that that's,
uh, it's a clear thing.
And that's the reason- to say something is recursive,
it has to repeat out to infinity, right.
So as soon as you put any bound on something,
and you say, "Look that's a noun phrase you just gave me with five levels of nesting."
That's pretty implausible that someone is going to say that.
And so as soon as you sort of,
um, want to make an argument like,
okay even if they said that,
no one is going to say a noun phrase with 10 levels of nesting.
And if you put some hard limit on it like that, um,
then in some sense it's not truly recursive because it doesn't go out to infinity.
Um, but, you know,
regardless what you think about that,
that doesn't negate the basic argument that you get this hierarchical
structuring with the same kinds of things like noun phrases,
sentences, verb phrases, appearing inside each other in a way that has no clear bound.
Like to the extent that I show you a complex sentence,
you can say I can make that an even bigger, more complex sentence by putting it inside,
you said to me that, and then saying,
um, my sentence, right.
So that's the sense in which it does appear to be a recursive generative process,
even though practically there are limits to how complex sentences people say.
And so that's the kind of structure that gets
captured in these constituency, um, structure trees.
So before the early time when we talked about parsing and you guys did some of it,
I emphasized dependency parsing.
Um, but the other kind of parsing which is actually
the kind that the models I'm going to talk about today was using,
was this idea of what's often called constituency
parsing or linguists often call it phrase structure grammars,
um, or in sort of computer science formal language theory.
These are context-free grammars, where, um,
we're having, um, these,
um, non-terminals like noun phrase,
and verb phrase, and that's inside another noun phrases,
it's inside another verb phrase,
which is inside more verb phrases,
heading up the sentence.
And so these are our constituency grammars.
And when we've occasionally mentioned the Penn Treebank tree,
this was kind of an original Penn Treebank tree which is basically, uh,
phrase structure grammar like,
this with sort of various extra annotations,
um, put on the nodes.
Okay, so what did seem- what- what do you- to capture some of these properties,
it seems like we'd like to have a neural model
that can make use of some of this same kind of tree structure.
And so what we'd like to do for working out semantic similarity of constituents,
is we want to not only have
a word vector space like we started off with right at the beginning of the quarter,
but we'd like to be able to take bigger constituents like noun phrases,
the country of my birth,
and the place where I was born,
and also give them a meaning.
And so it seems like what we'd like to do is have a method of
computing the meaning of any phrase in a compositional manner,
such that the end result is also that
these phrases could be stuck inside our vector space models.
So we're still going to stick with our vector space semantics of phrases,
and we wanna comp- compute the meanings of phrases.
And so then the question is,
how could we go about doing that?
And well answer number one is we're gonna use the principle of
compositionality since we're sure it's right,
and so, well, what the principle of compositionality essentially says,
if you want to work out the meaning- or here it says of a sentence.
But the meaning of any phrase, any constituent is you're going to
build it by knowing the meanings of its words,
and then having rules that combine these meanings.
So starting off with the country of my birth,
I should be able to calculate a meaning of my birth,
and meaning of the country,
and meaning of of the- my birth and then a meaning of the country of my birth.
So we'd have meaning composition rules which will let us calculate
meanings upwards for larger constituents or sentences.
Um, so that seems kind of the right thing to do.
And so then the question is well, can we, um,
then build a model of how to do that?
Well, here's sort of a straightforward way of doing this, okay.
So we- we have word vectors for the words that we've calculated.
And what we'd like to do is work out, um-
Then a meaning representation of this sentence.
And at this point we sort of have two things to do.
We have parsing to do of working out what's the right structure of the sentence,
and then we have meaning computation to do of
working out what is the meaning representation of this sentence.
Um, so for parsing we'd sort of be building,
sort of noun phrase, prepositional phrase,
verb phrase, sentence kind of units, um,
to get "the cat sat on the mat",
and then will, what,
we, if we had that,
we could then run some kind of meaning computation program,
and give us sort of a vector space,
um, meaning of these sentences.
So that's kind of what we want,
is to do both of those,
and in a little bit I'll show you an example of the kind
of one way that you go about approaching that.
But before I do that, just sort of stepping back for
a moment as to what's different here, right?
That here we had our
recurrent neural network which in some sense has been
our workhorse tool in this class up to now,
and it gives you,
it gives you a representation of the meaning of the country of my birth sort of,
you could either say that's the meaning of,
um, the country of my birth,
or we talked about other tricks like,
doing max pooling across all of these,
or you could have a separate node out here,
which so does attention over these.
So it does give you a sort of representation, um,
of the meaning of this,
of any, um, sub-sequence of words as well.
Um, but they, they're sort of different, right?
That this what, the top,
the tree recursive neural network,
it requires a sentence or any kind of phrase to have a tree structure.
So we know what its component parts are,
but then we're working out meaning representations
for the phrase that is sensitive to what its syntactic structure is,
that how the words go together to build phrases.
Whereas for the recurrent neural network we're
just in an oblivious way running a sequence model along,
and say and compute things,
and in the obvious,
it doesn't in any obvious way give a meaning representation of,
of my birth, or my birth contained inside that.
We sort of only have a meaning representation for the whole sequence,
whereas if we're doing things this way, um,
we do have meaning representations for the different meaningful parts of the sentence.
Okay. That makes sense of what we're trying to do?
Okay. So how could we do,
go about doing that?
Um, well, the idea of how we could go about doing that is,
if we work bottom-up,
at the very bottom we have word vectors,
and so we want to recursively compute the meaning of bigger constituents.
So if we wanted to compute the meaning of "on the mat" what we can do is say,
well, we have, already have a meaning representation of, on and mat.
So if we could feed those into a neural network, because that's our
one tool, we could maybe get out of it two things.
We could get out of it a goodness score.
So this is what we're going to use for parsing.
We're going to say, "Do you belie- do you believe you can put together "on" and the
"mat" to form a good constituent that's part of a parse tree?
And this will be a big positive number if the answer is true,
and negative if it's not true,
and then we have a meaning composition device,
which says, "Okay, um,
if you put together these two things,
what would be the meaning representation of what we put together?"
And so this is the first model that we explored which
was doing this in a pretty simple way, right?
So here was our meaning composition, um,
device that we concatenated the two vectors of the constituents,
we multiply them by a matrix, add a
bias as usual,
put it through a tan h. Uh,
this work is old enough,
it's sort of before things, like,
ReLUs became popular,
but maybe it's better to have a tan h anyway, um, fit more like,
a recurrent neural network,
and so this was our meaning composition that gave the meaning of the parent.
And then to the side, what the score of it was as to whether this was a good phrase,
we were taking that parent vector representation,
and multiplying it by another vector,
and that was giving us out a number.
Um, if you think about it a bit while we're doing this,
you might think that this isn't quite a perfect model of meaning composition,
and later on in the class I'll talk about some more complex models,
um, that we then started to explore.
Um, but this is sort of enough to get us going,
and this gave us a way of building
a recursive neural network parser which both found parsers,
and worked out a meaning representation for them.
And so the way we did this was in the simplest possible way really,
which was to have a greedy parser.
So if we start off with the "cat sat on the mat",
what we could do is say,
well, maybe you should join "the" and "cat" together.
Let's try that.
Run it through our neural network,
it'll get a score and a meaning representation,
and while we could try doing that for "cat" and
"sat" we could try doing it for "sat" and "on".
We could try doing it for "on" and "the" we could try doing it for "the" and "mat".
And then at this point we'd say, okay, well the,
the best phrase that we can make combining these word vectors is the one for "the cat".
So let's just commit to that one,
and it has this semantic representation,
and at this point we can essentially repeat.
Now, all the work we did over there we can just reuse because nothing has changed,
but we can also consider now joining the "cat"
as a constituent with "sat" and get a score for that.
And so at this point we decide, okay,
the mat is the best constituent to build,
commit to that, calculate a meaning representation for "on the mat".
That looks good, commit to that,
and kind of keep on chugging up,
and so we've got a mechanism for sort of choosing a parse of a sentence in a,
in a greedy manner.
But, you know, when we looked at the dependency parsing,
we're also doing that greedily, right?
Um, and coming up with a meaning representation.
Okay. So that was our first model of having a tree recursive neural network,
and using it for parsing.
Um, there are a few more details here,
some of which probably aren't super,
um, important at this point, right?
So we could score a tree by summing the scores at each node,
um, for working out,
for the optimization we were working out,
we're using this kind of max-margin loss that we've looked at in other places.
Um, the simplest way to do things is completely greedily.
You just, um, find the best local decision at each point,
and make that structure,
and keep on going.
But if you wanna do things a bit better,
and we explored this,
um, you could say,
um, we could do beam search.
We could explore out several good ways of merging,
and then decide later higher up the tree as to which was the best way, um, to merge.
Um, we haven't talked about it in this class,
but just to mention, um,
something in case people have seen it is, um,
traditional constituency parsing where you have symbols here,
like, NP or VP.
Um, there exist efficient dynamic programming algorithms where you can
find the optimal parse of a sentence in polynomial time.
So in, in cubic time.
So if you have a regular context-free grammar, and well,
so regular probabilistic context-free grammar, um,
and if you want to know what is the best parse of
the sentence according to the probabilistic context-free grammar,
you can write a cubic time dynamic programming algorithm and you can find it.
That's good. And in the old days of CS224N,
um, before neural networks we used to have everyone do that.
The, the most, the most brain-breaking assignment of the old CS224N
was writing this dynamic program to do context-free grammar parsing of a sentence.
Um, the slightly sad fact is,
once you go to these kind of neural network representations,
you can't write clever dynamic programming algorithms anymore,
because clever dynamic programming algorithms only work when you have symbols
from a reasonably small set for your non-terminals because if that's the case,
you can, you kind of have collisions, right?
You have lots of ways of parsing stuff lower down,
which kind of, uh,
turn out to be different ways to make a noun phrase,
or different ways to make a prepositional phrase,
and therefore you can save work with dynamic programming.
If you've got a model like this,
since everything that you build is going through layers of neural network,
and you've got a meaning representation, some high-dimensional vector,
things are never going to collide,
and so you can never save work by doing dynamic programming.
And so, um, you're either doing exponential work to explore out everything,
or else you're using some kind of beam to explore a bunch of likely stuff.
Yeah. Um, we actually also applied this,
um, to vision at the same time.
So it wasn't just sort of completely
a vague motivation of, um,
visual scenes have parts that we actually started exploring that well you could take, um,
these pieces of scenes and then work out, um,
representations for scenes using a similar form of compositionality.
And so in particular,
um, there was sort of this dataset that was being used for, um,
multi-class segmentation in vision,
where you start off with very small patches and then you wanna combine them
up into parts of a scene of sort of
recognizing which part of the picture was the building,
the sky, the road, various other classes.
And we were actually at the time able to do this really rather well, um,
using one of these tree recursive structured neural networks better
than preceding work in vision had done in the late 2000s decade.
Okay. So how can we- how can we build neural networks,
um, that do this kind of stuff?
Um, so when- when we started off exploring these tree structured neural networks, um,
we thought that this was a cool original idea and no one
had worked on tree structured neural networks successfully before.
Um, but it turned out we were wrong, that there were a couple of Germans in the mid-1990s,
um, had actually started looking at tree structured neural networks and had worked out,
um, the math of them.
So corresponding to the backpropagation through time algorithm,
um, that Abby talked about when we were doing recurrent neural networks.
They worked out the tree structured case which they
called backpropagation, um, through structure.
Um, there are several slides on this in
the slides but I think I'm gonna sort of skip them.
If anyone wants to look at them,
they're on the web and you can look at them.
I mean, there isn't actually anything that's new.
So if you remember with- with
bad scarring or something that was early lectures of this class of working out,
um, the derivatives of neural networks and how it worked with recurrent neural networks.
It's sort of the same, right.
You have this recurrent matrix at different levels of tree structure.
You're summing the derivatives of everywhere it turns up.
The only difference is sort of because we now have tree structure,
you're sort of splitting things downwards.
Um, so yes.
So forward prop we kind of compute it forwards.
And then when we're doing back prop,
when we've had the backward propagation we have the error signal coming from above.
We then, um, combine it,
um, with the calculations at this node.
And then we're sort of sending it back in a tree structure
down to each of the branches underneath us.
So that was our first version of things and we got some decent results.
We got this good vision results that I showed you and it sort of seemed to do,
um, some good for, um,
language both for parsing and doing- We had
some results I haven't actually included here of sort of doing paraphrase,
um, judgment between sentences and it- it modeled things, um, fairly well.
But once we started thinking about it more it seemed like
that very simple neural net function couldn't possibly
compute the kind of meanings that we wanted to compute for sentence meanings.
And so we then sort of set about trying to come up with
some more complex ways of working out kind
of meaning composition functions and nodes that
could then be used to build a better neural network.
And sort of some- some of the essence of that is on this slide.
But, you know, for the first version we just
didn't have enough complexity of neural network, frankly, right?
So when we had two constituents we concatenated
them and multiply that by a weight, uh, weight matrix.
Um, and that was sort of essentially all we had.
And, um, as I hope you've gotten more of a sense of in this class.
If you just concatenate and multiply by a weight matrix,
you're not actually modeling the interaction between these two vectors, right.
Because you can think of this weight matrix as just sort of being
divided in two and half of it multiplies this vector,
and half of it multiplies this vector.
So the meanings of these two things don't act on each other.
And so somehow you have to make your neural network,
um, more complex than that.
But the other way in which this seemed too simple is in the first model,
we had just one weight matrix which we use for everything.
And, ah, at least if you're a linguist and you're
thinking about the structure of language you might start thinking of well,
wait a minute, sometimes you're gonna be putting
together a verb and an object noun phrase.
Um, hit the ball.
Sometimes you're gonna be putting together an article and a noun, uh, ball.
Sometimes you're gonna be doing adjectival modification blue ball.
These things are very different in their semantics.
Can it really be the case that you can just have one weight matrix that is
this universal composition function for putting together the meaning of phrases?
Could that possibly work?
And you sort of might suspect,
um, it doesn't work.
Um, and so I'm gonna go on and, um,
show, um, some of those different things.
But really, um, before I show the different things,
um, I'm gonna show one more version that's sort of related to the first thing,
which actually gave a pretty successful and good parser,
um, for doing, um,
context-free style constituency parsing.
And so this was another way of getting away from the parsing being completely greedy.
Um, which was to actually split apart the two parts of g. We
have to come up with a tree structure for our sentence from,
'Let's compute the meaning of the sentence'.
And so the thinking was, well,
in terms of deciding what's a good tree structure for a sentence,
that's actually something you can do pretty well with the symbolic grammar.
But the problems with symbolic grammars aren't
that they can't put tree structures over sentences.
The problems you have with those grammars is that,
they can't compute meaning representation and they're not
very good at choosing between alternative tree structures.
But we can divide up the two parts.
So what we can do is say, well,
let's just use a regular probabilistic Context-Free Grammar
to generate possible tree structures for sentences.
We can generate a k best list and say,
what are the 50 best, um,
context-free grammar structures for this sentence?
And that's something we can do very efficiently with dynamic programming algorithms.
And then we can work out a neural net,
um, that will work out the meaning representation of the sentence.
Um, and so that led to this, um,
what's called syntactically untied recursive neural network.
Um, so essentially what this is saying is that we
ha- for each node and the sentence it's got a category,
um, of a symbolic context-free grammar.
So they're category A and B and C. So when
we put things together we'll be able to say, okay.
We've got a rule that says, um,
X goes to BC,
so that licenses this node here.
So that part of the parsing is symbolic.
Then- then we want to, um,
work out the meaning of this phrase.
Um, and well, the second problem I talked about
was surely just having a one way of doing composition
is expecting a lot too much to be able to have
sort of verb and object versus adjective and noun composed the same way.
So we have this idea of well,
since we now know about the syntactic categories of
the children that we maybe know that this is an adjective and this is a noun.
What we could do is have different weight matrices for
composition depending on what the categories are.
So rather than where before there was
just this one universal weight matrix which was meant to do all meaning composition.
Here we can have,
this is the weight matrix for combining together
the meanings of an adjective and a noun and it will compute,
um, the meaning of this constituent.
But then we'll have a different weight matrix for combining
together the meanings of a determiner and a noun phrase or something like that.
Okay. Um, yes.
So I sort of always said this one I guess,
um, we wanted to be able to do things quickly.
And so our solution to be able to do that is we sort of
used a probabilistic context-free grammar to find likely parses,
um, and then only worked out our meaning for ones that were, um, quite probable.
And so we call this result a compositional vector grammar which was
a combination of a PCFG and a tree recursive neural network.
Um, and yeah.
So, um, essentially at the time,
this actually gave a pretty good constituency parser.
So there are sort of lots of results here.
The top ones are kind of our classic older, um,
Stanford Parser which is a PCFG, the kind of parsers that people had built.
This is our compositional vector grammar that the time of this being done in 2013,
it wasn't the very best parser available.
There had been some better work by Eugene Charniak at Brown.
But we actually had a pretty good parser coming out of that system.
But what was perhaps a bit more interesting was we,
we didn't only have a parser that was meant to give the right parse trees.
We are also computing meaning representations of nodes.
And as a kind of a consequence of that,
you can look at not only meaning representations of nodes.
You could learn about the weight matrices that these models were learning,
um, when they combine together meanings.
So remember we had these sort of category-specific W matrices,
that were going together with the children to work out the meaning.
Um, so these are a little bit hard to interpret.
But the deal is, when we load these matrices,
we initialize them as a pair of diagonal matrices.
So these are sort of two by one rectangular matrices because there are two children.
Um, so half of it is, um,
multiplying the left child,
the other half is multiplying the right child.
And we initialize them as sort of like a compi- two identity matrices next to each
other which would give us the sort of default semantics
of just averaging until something different was learned in the,
in the, in the weight vectors.
And to the extent that sort of nothing interesting has been learned by the model,
you'll get yellow along the diagonal and this sort of sky blue in the rest of the field.
And to the extent that it's learned something
interesting to take out of the semantics of a child,
you will then start to see reds and oranges on the diagonal,
and dark blues and greens and stuff in the rest of the field.
So what you find is that if you train this model,
it's learning about which children of a phrase are actually the important ones.
Um, so these ones are saying that if you're
combining together a noun phrase and the coordination,
so something like "the cat and",
that most of the semantics have to be found in "the cat"
and not much of the semantics is going to be found in "and".
Whereas if you are combining together a possessive pronoun,
something like her or his,
um, with a noun phrase inside it like,
um, her tabby cat or something like that.
Then most of the meaning is to be found inside the tabby cat constituent.
So it's actually learning where the important semantics of sentences is.
Um, and there're lots of examples of that. Um, yeah.
This one sort of- so this one shows a variety of
modification structures where adjectives or adverbs,
um, modify either a noun phrase or an adjective phrase or
just a single adjective is multiplying a noun phrase.
And the thing that you seem to notice is that there are particular dimensions which are
kind of capturing sort of modification meanings.
So dimension 6 and dimension 11 is sort of showing up in these different,
um, combinations here, as sort of capturing meaning components.
So that was kind of neat.
And so this slightly more complex model actually worked pretty
well at capturing a meaning of phrases and sentences.
So in this test here,
we were giving it- the system a test sentence and saying well,
what are the other- what are sentences that are most similar in meaning,
nearest to paraphrases in our corpus for this sentence?
So if all the figures are adjusted for seasonal variations,
the two most similar other sentences in the corpus were,
all the numbers are adjusted for seasonal vet fluctuation.
That's a pretty easy one.
Or all the figures are adjusted to remove usual seasonal patterns.
So that seems to be working pretty well.
"Knight-Ridder wouldn't a comment on the author,
Harsco declined to say what country placed the order."
The semantics there are a bit more different but it
seems like it is capturing something similar.
"Um, Coastal wouldn't disclose the terms."
That's kind of a really interesting one,
because that one is actually very similar in meaning but it's expressed in
a very different way in terms of the words and the syntactic structure that are used.
Okay, so that was progress because now
we could have different matrices for different constituent types.
Um, but there's still some reason to think that we didn't have enough power,
and that was we're still at heart using this very simple compositional structure
where we're just concatenating the two children's vectors and multiplying it by a matrix.
So that means the two words, um,
didn't interact with each other in terms of their meaning.
Um, but, um, it seems like we want to have them interact in their meaning, right?
So in particular if you if you think about
human languages and the kind of things that people look at in linguistic semantics,
you get words that appear to be kind of modifiers or operators.
So the word very,
sort of doesn't mean much by itself.
I mean it means something like strengthening or more so or something like that,
but you know, it doesn't really have a meaning, right?
It doesn't have any denotation.
You can't show me very things, right?
You can show me chairs and pens and, um,
children but you can't show me very things,
that the meaning of very seems to be that something comes after it, good.
And this has a sort of an operator meaning of increase on the scale of this thing,
and it can increase on the scale in either direction.
You can have very good or very bad.
So if we want to capture that kind of semantics,
it seems like we can't capture that kind of semantics by just
concatenating two vectors and multiplying them by a matrix.
It seems like what we really want to say is very is gonna grab
hold of the meaning of good and
modify it in some ways to produce a new meaning for very good.
And indeed, that's the kind of approach that's typically,
um, been done in linguistic semantics.
So in linguistic theories of semantic,
you would normally say, okay,
good has a meaning,
very is a function that takes in the meaning of good and returns a meaning of very good.
And so we wanted to have, um,
a way of putting that into a neural network.
And so to try and come up with a new composition function as to how to do that.
And there are various ways that you could think about
doing that and other people have had a couple of different attempts.
But essentially what was in our head is well,
we have word vectors,
and if we want to say that very takes the meaning of good and returns a new meaning,
the kind of obvious thing to do is to say very
has a matrix attached to it because then we could use the,
the very matrix and multiply it by the good vector and we get a new,
um, vector coming out.
And so then, well,
the problem is, uh,
which- then which words have vectors and which words have matrices?
And that's kind of, um,
hard to know the answer to.
I mean, in particular, um,
words that act as operators can,
um, often themselves be modified.
Um, and so, um, that you know,
good can also- good also is a operator, right?
So that from a sort of a person,
you can have a good person and that's sort of also an operator,
and very is modifying that good.
So the idea we came up with is let's not try and predetermine all of this.
Why don't we say that every word and every phrase has
connected to it both matrix and vector.
So here's our very good movie.
So for each word,
we have a vector meaning and it has a matrix meaning,
and then as we start to build up phrases like very good,
they're also going to have a vector meaning and a matrix meaning.
And so what we proposed was,
um, so first of all,
we, we would like to be able, um,
to calculate, um, the vector meanings.
So to work out the vector meaning of a phrase like very good.
Each word has a matrix meaning.
And so we're going to combine their opposing matrix and vector meaning.
So we're going to take the matrix meaning of
good and multiply it by the vector meaning of very.
And we're going to take the matrix meaning of very and
multiply it by the vector meaning of good.
And so we're going to have both of those two things.
And then we're going to have a neural network layer like before that combine those together.
And so that's sort of in the red box.
Then those two things were concatenated,
and put through the kind of neural network layer we had before to give us
a final vector meaning on this, for the phrase.
And then we also needed a matrix meaning for the phrase.
And so for the matrix meaning for the phrase, um.
We did this kind of simple model which maybe actually wasn't very good which was to say,
let's just concatenate the two matrices of the- um,
the constituents, multiply them by
another matrix and that's then going to give us a matrix,
um, version of the parent node.
And so this was gave us our new more compo- more powerful composition procedure.
Um, this did seem like it could do some kind of good things that captured,
uh, uh, sort of operator semantics where one word modified the meaning of another word.
Um, so here's a kind of a neat thing that we were able to do with this.
Um, that we are wanting to be able to work out
the semantics of an operator modifying another word.
So unbelievably annoying, unbelievably awesome, unbelievably sad.
Um, not annoying, not awesome, not sad.
[NOISE] And so this was contrasting,
our, um, old model versus the new model.
And this scale is a scale of positive to negative.
So this is completely negative to completely positive, all right?
And so the kind of contrast you get,
uh, that for, um,
not annoying that the simple model thought that this was pretty negative,
whereas the new model thinks this is pretty neutral in meaning,
and that seems to be reasonably correct.
Um, but not sad,
that means it's a little bit positive and both models were trying to capt- capture that,
that- you know, the results here are a little bit ambivalent,
but- but it sort of seems that they sort of go a
little bit in the direction of what we want. Yes.
What is the ground truth in the "not sad" example?
Oh, yeah. So this ground truth
was- we actually asked a whole bunch of human beings to say,
um, rate the [LAUGHTER] meaning of not sad,
on this scale of 1 to 10.
Maybe this wasn't a very good clear task because as you can see it,
bounced around a lot [LAUGHTER] as to,
um, what kind of ratings we were getting for things.
But yeah, that was actually kind of getting human judgments.
Um, we also then use this,
um, model to say, "Well,
could we do, um, semantic classification tasks?"
So if we wanted to understand relations between different noun phrases,
so this was a dataset where, um,
there were relations marked between two noun phrases.
My apartment has a pretty large kitchen that that was seen as an example of
a component-whole, a part of relationship between the two noun phrases,
and there were other relationships between different kinds of noun phrases.
So if it was the movie showed wars, um,
that that was then a message topic,
so there's some communication medium that contains some topic relationship.
And so we were using this kind of
neural network to sort of build our meaning representations and
then putting them through
another neural network layer as a classifier to see how well we did.
And so we got some sort of fairly good results on that.
So this was a dataset that people had worked on with
traditional NLP systems of different kinds of machine learning methods.
But in some sense, you know,
what we were interested in was we seem to be making progress in having
a better semantic composition system that
our old recursive neural network was getting about 75 percent,
and then our new one was getting about 79 percent,
which we could sort of push up further by putting more features into our system.
So that was progress,
um, but we didn't stop there.
We kept on trying to come up with better ways of doing things.
And so even though things worked fairly well here,
it sort of seemed like this way of doing matrices wasn't necessarily very good.
It sort of had two problems.
One problem was it introduced a humongous number of parameters because,
you know, for just about everything that we've done, otherwise,
words have had a vector and well,
maybe sometimes we use quite high dimensional vectors like 1,024,
um, [NOISE] but, you know, that's a relatively modest number of parameters.
Whereas once we introduce this matrix here,
we've got that number of squared additional parameters for every word.
And essentially because of that number of
parameters to be able to compute this model at all,
we were making the vector size small.
So what we were actually using was that these were
just 25-dimensional vectors so that the 25 squared,
625, still safe, sort of decently within the range in which we could compute.
So that was the first problem.
The second problem is,
we didn't really have very good ways of
sort of building up the matrix meaning of bigger phrases.
I mean, you know,
this sort of seems something simple we could do but it didn't,
you know, feel a very good way of getting a matrix meaning of a phrase.
So we sort of wanted to come up with some other way of doing
things that could fix both of those problems.
And then, that led into work on recursive neural tensor networks.
Um, and there's a kind of a nice idea here of these neural tensors,
which is an idea that's actually been used in other places including, um,
work on sort of putting vector embeddings of knowledge graphs and so on,
which is a kind of a bit of a nice idea.
So I wanted to sort of show a bit of how this model works.
Um, and but just to say, first,
a place where we applied this model was on the problem of sentiment analysis.
Now, I think the term sentiment analysis has come up a few times as something you can
do and actually which I then mentioned in the last, um, lecture.
But I think we've never really talked for five minutes, um,
in this class on sentiment analysis,
so I'll, um, give you this as an example of that.
Um, sentiment analysis has actually been
a really common and important application in natural language processing.
Um, you're looking at a piece of text and you're sort of saying,
"Is it, um, positive or negative?"
Um, and that's just something that's very useful for lots of, um,
commercial applications of looking at product reviews or doing brand,
um, awareness and things like that of sort of looking at sentiment connected to things.
And to some extent doing sentiment analysis is easy, right?
That you can kind of say,
"Well, look at a piece of text.
If you see words like loved,
great, impressed, marvelous, then it's positive.
It's a positive review.
And if it's saying, bad and awful,
then it's a negative review."
And to some extent that's the baseline of sentiment analysis that you can use
just either selected word features or all words in a bag of words.
And if you do that,
you don't actually do that badly,
um, in sentiment analysis.
If you have longer documents,
just looking at bags of words can give you 90 percent in sentiment analysis.
But on the other hand,
things often do get trickier, right?
So, um, this is from Rotten Tomatoes.
With this cast and the subject matter,
the movie should have been funnier and more entertaining.
And if you sort of pretend you're a bag of words model,
the only words in this that are sort of clearly sentiment-laden words, uh,
entertaining and funnier, and both of those are pretty positive words,
um, but it's fairly obvious that this actually is meant to be a bad review of the movie.
And so well, how are we meant to know that?
Well, it sort of seems again like what we have to do is meaning composition.
We have to get sort of phrases like "should have been
funnier," and then realized that that's actually a negative meaning for a phrase.
And so we wanted to explore how we could look at those sort of meanings for
phrases and explore building up those meanings as doing meaning composition over trees.
Um, so the first thing we did, um,
was we built a treebank of sentiment trees where we got people to rate sentiment.
And so this led to the Stanford Sentiment Treebank,
which is still a dataset you often see used in, um,
various of evaluations with a whole bunch of datasets. Indeed,
it showed up in decaNLP last week.
Um, so what we were doing in this was taking,
um, sentences which were Rotten Tomatoes sentences from movies.
We were parsing them to give tree structure and then we were asking Mechanical Turkers to
rate the different phra- the different words and
phrases on a sentiment scale of very positive to very negative.
So lots of stuff is white because it's just not sentiment-laden, right?
There's words that are the,
and there's phrases like the movie and
the movie was- which don't really have any sentiment,
but then you have pieces that are sort of very positives pieces of
tree and negative pieces of tree that are then shown in the blue and the red.
And- so typically in sentiment datasets,
people have only labeled the entire sentence to say,
"This is a positive sentence or a very positive sentence.
This is a negative sentence or a very negative sentence."
Crucially, what we were doing differently here is every phrase in the sentence
according to our tree structure was being given a label for its positivity or negativity.
Um, and perhaps not surprisingly,
just the fact that you have a lot more annotations like that, um,
just improves the behavior of classifiers because you kind of can
do better attribution of which words in a sentence are positive or negative. Um.
So, these were um were results of sort of preceding models.
So the green is a Naive Bayes model except it not only uses individual words,
but it uses pairs of words.
It turns out if you're building a traditional classifier and you
wanna do sentiment analysis as opposed to something like topic classification,
you get a lot better results if you also use word pair features.
And that's because it does a baby bit of um composition for you.
You don't only have features for not an interesting,
but you can have a feature for not
interesting and that lets you model a certain amount of stuff.
Um, and then these are our older generations of neural networks,
our ori- original tree structured neural network and our matrix vector one.
And so simply having- for these sort of fixed models,
simply having the richer supervision that comes from our new treebank,
it's sort of moved up the performance of every model.
So even um, for just the um,
Naive Bayes model's performances going up about four percent um,
because of the fact um,
that it now knows more about which
particular words are positive or negative in the sentences.
Um, but still none of these performances are really great.
Um, so we still thought that well can we build better models of how to do this.
Um, in particular, if you look at sentences with
sort of various kinds of negation you know,
things like should've been funnier,
these models in general still couldn't capture the right meanings for them.
And so that led into our fourth model of how to do this,
which is this idea of recursive neural tensor networks.
Um, and so what we wanted to be able to do is go back to just having um,
meanings of words be vectors,
but nevertheless despite that to be able to
have a meaningful phrase where the two vectors um,
acted on each other.
And well, you know, this kind of,
this is the picture of what we did when we were
doing attention in a bi-linear way, right?
We had vectors for two words.
We stuck a matrix in between and we used
that and gave an attention and got an attention score out.
So that let these two vectors interact with each other,
but it only produced one number as the output.
But there's a way to fix that,
which is to say well rather than having a matrix here,
what we could stick here is a three-dimensional cube,
which physicists and deep learning people now call a tensor, right?
So a tensor is just higher multi-dimensional array um,
in computer science terms.
Um, so if we sort of made that a tensor,
you know, it's like we have sort of multiple layers of matrix here.
And so the end result of that is we get one number here and one number here.
So in total, we get out a size two vector,
which is all we need in my baby example where
baby examples, where we only have these two component vectors for words.
But in general, we have a tensor with
the extra mention dimension of the size of our word vector.
And so therefore, we will get a word vector, w hat,
we will get a phrase vector out from the composition that's the same size of
the input vectors and will allow them to
interact with each other in working out the meaning of the entire thing.
Okay. Um, all right.
So at that point um,
we use the resulting vectors um um,
so we had our neural tensor network.
We actually combined it together with the sort of previous kind of layer we used to have,
our sort of first RNN, maybe you didn't need to do this,
but we just decided to add that in as well,
put things through a nonlinearity and that was then
giving us our new representation of phrases.
We built that up the tree and then at the end,
we could classify the meaning of any phrase um,
in the same kind of way with softmax regression and we could
train these weights with gradient descent to predict sentiment.
And so this actually worked pretty nicely.
I mean in particular,
it didn't so really work any better with just the sentence labels.
But if we train the model with our treebank,
we could then get a kind of- of whatever
that is about another couple of percent in performance,
and so that seemed good.
And so in particular,
it seemed to do a much better job of actually understanding meaning composition.
So here's the kind of sentence where you have there are slow and repetitive parts,
but it has just enough spice to keep it interesting.
And the model seen here is pretty good at understanding.
Okay, this part of the sentence is negative,
this part of the sentence is positive,
and actually when you stick the two halves together,
the end result is a sentence that's positive in meaning.
But focusing in a little bit more what seems
like it's especially good was for the first time this actually
did seem like it could do a better job of
working out sort of what happens when you do things like negation.
So here we have it's just incredibly dull and it's definitely not dull.
So if it's definitely not dull,
that's actually means it's good, right?
Can we work out the meaning of, it's definitely not dull?
And so um, these, this is sort of
showing sort of what happens when you have a negative,
a negative sentence that's further negated.
So if you go from um,
so if you sort of do
a annex- negation of a negative thing should become moderately positive, right?
So that if you have dull is negative and if you say not dull,
it doesn't mean it's fantastic,
but it means it's moderately positive.
And so for either a kind of Naive Bayes model or our preceding models,
they weren't capable of capturing that of sort of going from dull to not dull your,
your meaning computation did not come out any more positive.
Whereas this sort of neural tensor network was
capturing the fact that not dull meant that it was reasonably good.
So that was progress. Um, yes.
So I think that's as much as I'll um show you really now about applying
these tree structured neural networks um, to natural language.
Um, you know, I think the summary I sort of said at the beginning um is that I
think you know, they're kind of interesting ideas and linguistic connections here.
I mean, for various reasons,
these ideas haven't been um,
pursued a ton in recent years of natural language processing.
You know, one is in all honesty people have found that um,
once you have high dimensional vectors
in things like the kind of sequence models that we've looked at,
whether it's meaning things like the sort of LSTM models or any of
the more recent contextual language models, those work incredibly well um,
and it's not, it's not clear that overall these models work better.
The second reason is sort of a computational reason,
which is um, GPUs work great when you're doing uniform computation.
And the beauty of having something like a sequence model is that there's uh,
there's just one determinant computation you are doing
along the sequence or in the convolutional neural network,
there's one determinant um,
computation you're doing up um,
through your convolutional layers and therefore,
things can be represented and computed efficiently on a GPU.
The huge problem with these kind of models was what computations you are
going to do depended on which structure you are assigning to the sentence,
and every sentence was going to have a different structure, and so therefore,
there was no way to batch the computations over a group of
sentences and have the same computations being done for
different sentences, it sort of undermined the ability
to sort of efficiently build these models in the large.
The thing I thought I'd just sort of say a moment about at the end.
Um, the funny thing is that although these haven't been used much for,
um, language in the last few years, um,
that they've actually had some use and found different applications in different places,
um, which is just sort of seen kind of cute.
Um, so this is actually an application from physics.
Um, and I think I'm going to just have to read this and
so I have no idea what half the words mean.
Um, but, um, what it says is by far the most common structures seen in collisions at
the Large Hadron Collider are collimated sprays of energetic hadrons referred to as jets.
These jets are produced from the fragmentation and hadronization of
quarks and gluons as described by quantum chromodynamics.
Anyone knows what that means?
Um, I hope you're following along here.
Um, one compelling physics challenge is to search for
highly boosted standard model particles decaying hadronically.
Unfortunately there's a large background from jets produced by more mundane,
um, QCD, that's quantum chromodynamics processes.
In this work, we propose instead a solution for
jet classification based on an analogy between
quantum chromodynamics and natural languages as inspired by
several works from natural language, um, processing.
Much like a sentence is composed of words
following a syntactic structure organized as a parse tree,
a jet is also composed of 4-momenta following a structure dictated by
QCD and organized via
the clustering history of a sequential co- combination jet algorithm.
Um, so anyway um, yeah with these jets you see they're getting
a tree structure over them and they're using the tree recursive neural network,
um, to model it.
Um, well that's a little bit far afield but to show you just one more example, um,
that another place where these models have actually being quite
useful is for doing things in programming languages.
And I think in part this,
this is because the application is easier in programming languages.
So unlike in natural language where we have this uncertainty as to what is
the correct parse tree because there's a lot of ambiguity in natural language,
in programming languages, um,
the parse trees are actually pretty determinant.
Um, so a group of people at Berkeley, Dawn Song and her students have worked on doing
programming language translation by building
tree recursive neural network encoder-decoders.
So that you're building up a tree structured
neural network representation of a program in one language.
This is a CoffeeScript program and then you're wanting to build a tree to
tree model which is then translating that to a program in a different language.
And they've been able to do that and get good results.
Um, I was too lazy to retype this table.
So this is probably a bit,
bit hard to read.
But what's it's contrasting is for a number of programs this is the sort of
CoffeeScript to JavaScript, um, um, translation.
They're comparing using tree to tree models.
Um, and then using sequence to sequence
models and then they tried both other combinations,
sequence to tree and tree to sequence.
Um, and what they find is you can get the best
results with the tree to tree neural network models.
And in particular these tree to tree models are
augmented with attention so they have attention like we talked about
the sequence to sequence models where you're then being able to do attention back to
nodes in the tree structure which is a pretty natural way of doing translation.
And indeed what these results show is if you don't have- that's right these results
show is if you don't have the attention operation it doesn't work at all.
It's too difficult, um,
to get things, um,
sort of done if you've just sort of trying to create
a single tree representation and then say generate the tra- the translation from that.
But if you can do it with this sort of putting attention
into the different modes, um, that's great.
Um, you might- If you know what CoffeeScript is you might, um,
feel like wait that's cheating slightly because
CoffeeScript is a bit too similar to JavaScript.
Um, but they've also, um,
done it in other languages.
So this is going between Java and C# and this is a sort of
handwritten Java to C# converter that you can
download from GitHub if you want but it doesn't actually work that well.
Um, and they're able to show,
the- they're able to build a far better, um,
Java to C# translator,
um, doing that.
Um, so that's actually kind of cool.
And it's good to know that tree structured recursive neural networks
are good for some things.
Um, so I'm pleased to see work like this.
Okay. I'm, I'm, just about done but I thought,
um, before, um, finishing,
I'd just mention one other [NOISE] thing which is sort of nothing to do
with natural language processing precisely but it's about AI.
Um, but I wanted to sort of put in a little bit of advertisement.
Um, that's something that a number of us have been
working on very hard for the last year or so,
is developing, um, a new Stanford Institute for Human-Centered Artificial Intelligence.
And actually the launch of this institute is going to be on Monday of exam week,
just when you're maximally concentrating on things such as this.
Um, but our hope is that we can have a lot of
new activity around artificial intelligence,
taking a much broader perspective to artificial intelligence, um,
which is centrally viewing it from the viewpoint of humans and working out, um,
I'll- exploring a much broader range of issues that
embrace a lot of the interests of the rest of the university whether
it's the social sciences and humanities, or also variously
in professional schools like the law school and the business school.
Um, so let's just quickly say a minute about that.
Um, that the, the sort of motivating idea is that sort of for most of my life sort
of AI seemed like a kind of
a fun intellectual quest as
to whether you could write bits of software that did anything,
um, halfway intelligent but that's clearly not what's going to be,
what's happening for the next 25 years.
That we're now at this point in which
artificial intelligence systems are being unleashed on society.
Um, and well hopefully they do some good things but as we've
increasingly been seeing there are lots of
also lots of opportunities for them to do bad things.
And even if we're not imagining Terminator scenarios,
there are just lots of places where people are using
machine learning and AI algorithms for making decisions.
Some of the worst ones are things like sentencing guidelines in
courts where you have very biased algorithms making bad decisions and
people are starting to become a lot more aware of the issues and so
effectively we are wanting to have this institute sort of
embracing a lot of the work of social scientists,
the ethicists and other people to actually explore how to have an AI
that's really improving human lives rather than having the opposite effect.
And so the three themes,
um, that we're, um,
mainly emphasizing for this institute is the first one in the top left is
developing AI technologies but we're particularly
interested in making linkages back to human intelligence.
So cognitive science and neuroscience
that when a lot of the early formative work in AI was
done including all of
the early work in neural networks like the development of back propagation,
it was actually largely done in the context of cognitive science.
Right? And that was sort of a linkage that tended to get lost in
the '90s and 2000s statistical machine learning emphasis.
And I think it would be good to renew that.
Um, the top right, um,
there's paying much more attention to
the human and societal impact of AI and so this is looking at legal issues,
economic issues, labor forces,
ethics, um, green power, politics, whatever you are.
But then down at the bottom is something where it seems like
there's just kind of enormous opportunities to do more which is,
um, how can we build technology that actually augments human lives.
Like to some extent here tech- we've got technology with AI augmenting human lives.
So all of your cell phones have speech recognition in them now.
So you know that's AI, um,
that can augment, um, your human lives.
But there's a sense of which not very much of
artificial intelligence has actually been put into the service of augmenting human lives.
Like most of what a cell phone has on it is still
sort of clever and cute stuff done by
HCI people and designers which is very nice a lot of
the time when you're using your map program or something but we don't really have
much AI inside these devices helping to make people's lives better.
And so we're hoping not only for individuals when applications like health care,
um, to be doing much more of sort of putting
artificial intelligence into human-centered applications.
Um, anyway that's my brief advertisement.
Um, look it out for this while you're not studying for your exams.
And I think there'll be sort of lots of opportunities, um,
for students and others to be getting more involved in this in the coming months.
Okay. Thank you very much.
Um, and I will see you later, um, at the poster session.
[APPLAUSE]
 Okay. Hi everyone, uh, let's get started.
Um, so Chris is traveling this week so he's not here.
But I'm very excited to say that today we've got
Margaret Mitchell who is a Senior Research Scientist at Google AI.
She's going to tell us about, uh, the latest
work defining and understanding and improving
the situation with bias in artificial intelligence.
Uh, Margaret has a background working in NLP and deep learning,
so I'm really interested to hear what she has to say today. Take it away.
Great, thank you. Um, can you guys hear me okay?
I'm not sure if this mic is exactly picking up my voice,
everything's cool? Okay, cool.
Um, so this work is, uh,
the product of a ton of different people and
collaborators that I've tried to put up here.
Um, some students at Stanford also Johns Hopkins, Google,
Facebook and Microsoft are all represented, cool.
So, um, for those of you who haven't seen the set of slides before,
what do you see here? Just shout it out.
Bananas.
Bananas. Okay what else?
Stickers.
Stickers. What else?
[NOISE] Shelves. What else?
Bunches of bananas.
Bunches of bananas. What else?
Yellow, ripe bananas.
You said ripe bananas, good.
So you can add [LAUGHTER] bananas with stickers on them.
You can start doing, like, embedded clauses, you know,
bunches of bananas with stickers on them on shelves in the store to get, kinda, crazy.
But we don't tend to say yellow bananas, right?
So given something like this,
we might say green bananas or we might say unripe bananas.
Given an image like this we might say ripe bananas or,
uh, bananas with spots on them.
Uh, if you're me, you might say bananas that are good for banana bread.
Um, but given an image like this or something like this in the real world,
we tend not to mention the yellowness.
And the reason for this is because yellow is prototypical for bananas.
So the idea of prototypes, uh,
stems from prototype theory which goes back to the early '70s,
uh, coming out of the work of Eleanor Rosch and colleagues.
Um, and it's this idea that there are
some stored central prototypical notions of objects,
um, that we access as we're operating,
uh, throughout the world.
There's some disagreement about whether these prototypes are
actual exemplars of objects or something like a distribution over what's likely,
but there is general agreement that we do have some, sort of,
sense of what's typical and what's a typical of the things in
the world and we tend to notice and talk about the things that are atypical.
Um, so this is a riddle that I
heard in middle school that worked a little bit more at that time,
um, some of you might have heard it before.
A man and his son are in
a terrible accident and are rushed to the hospital in critical care.
The doctor looks at the boy and exclaims,
"I can't operate on this boy,
he's my son," How could this be? [NOISE].
Two dads?
Two dads or he has a mum who's a doctor, right.
Otherwise known as a female doctor,
which might be contracted- contrasted with doctor.
Um, in a study they did,
uh, when they first, sort of,
put forward this riddle at Boston University,
they found that the majority of test subjects
overlooked the possibility that the doctor could be a she.
And that included men, women and self-described feminists.
So the point is that,
these, kinds of, uh,
ways of talking about things and assumptions that we make,
aren't necessarily something that speaks to a negative intent,
but something that speaks to how we actually store representations in
our minds and how we access those representations as we interact,
uh, in the world.
So this, uh, this affects what we can learn when we're learning from text.
So, um, this is work from 2013,
where they took a look at what was, sort of,
most likely, what would you learn if you were just learning from raw text,
um, what were some things that were common in the world?
Um, they found that in this setup
something like murdering was ten times more likely than blinking.
And the reason for this is because people tend
not to mention these typical things that go without saying.
We don't tend to mention things like blinking and breathing,
but we do mention atypical events like murder and that affects the, kind of,
things a machine can learn from texts that we put out in the world,
because it's been subject to all of
these filtering processes that we have as humans before we, uh, communicate.
Um, this issue in particular is known as Human Reporting Bias.
Which is that the frequency with which people write
about actions, outcomes or properties,
is not a reflection of real-world frequencies or
the degree to which a property is characteristic of a class of individuals,
but says a lot more about how we're actually
processing the world and what we think is remarkable.
So this affects everything a system can learn.
Um, in a typical machine learning paradigm,
one of the first steps is to collect and potentially annotate training data.
From there a model can be trained,
uh, from there, uh,
media can be filtered rank- ranked, aggregated,
generated in some way,
um, and from there people see the output.
And we like to think of this as a relatively straightforward pipeline,
um, but at the very start, uh,
even before we're collecting the data,
actually within the data itself,
are a host of different kinds of human biases.
So things like stereotyping, things like prejudice,
things like racism and that's embedded within the data before we collect it.
Then as we collect and annotate data,
further biases become introduced.
So things like sampling errors, confirmation bias, um,
uh, in-group bias and out-group bias and I'll talk about these,
um, a little bit.
Oh, and I should mention feel free to ask questions as I go,
um, totally fine to just,
kind of, interact, uh, throughout.
So here are some of the biases that I think are
relatively important for work in AI and machine learning.
There's hundreds you can go into,
um, but some of the ones that I've, sort of,
become the most aware of working in this space,
um, are these sets and I'll go through each of these a bit.
Um, so I talked about reporting bias earlier,
which is, uh, which affects what we can learn from text.
Um, another example of a kind
of bias that really affects what we can learn from text is selection bias.
So, uh, a lot of times that we,
a lot of times when we get data annotated we'd use something
like Amazon's Mechanical Turk, um,
and the distribution of workers across the world is not even, sort of,
uniform distribution, it's actually, um,
concentrated in India, the US and then some in Europe.
So this leaves out South America,
this leaves out Africa,
this leaves out a lot of China and that affects the, kind of,
things that we'll be able to learn about the world when we have things annotated.
Um, another kind of bias is Out-group Homogeneity Bias,
which is the tendency to see out-group members as more alike than in-group members.
And this is gonna affect what people are able to describe
and talk about when they're annotating things such as emotion.
So, uh, so for example we have these two, like,
adorable puppies on the left here and they're looking at these four cats.
Um, these are all different black cats,
very different in different ways,
but the two puppies look at the cats and they see four cats basically the same.
And it's kind of trivial to understand how that also extends to
human cognition and how we also process people.
Um, it's this- it's the sense we have that the,
the cohort that we're in,
the people that we interact with,
those are the kinds of people that are nuanced and
everybody else is somehow less nuanced,
has less detail to them.
It's a trick our minds play on us in order to help us process the world,
but it affects how we talk about it and it affects further how we annotate it.
Um, this leads to stuff like biased data representations.
So it's possible that you have an appropriate amount of data for
every possible human group you can think of in your data,
um, but it might be the case that some groups
are represented less positively than others.
And if we have time I'll go into, uh,
a long- a longer example of that.
Um, it also leads to things like biased labels.
So, um, this is an issue that came up when we were
getting some annotations for Inclusive Images competition,
asking people to annotate things like bride and wedding and groom.
And we found that given three different kinds of bride,
wedding and groom images,
um, ones that were more Western, European American, uh,
got the appropriate labels and ones that weren't,
just got, sort of, more generic person,
kinds of, labels, uh,
not able to actually tease out what's actually happening in these images.
Compounding this issue are biases in interpretation
when the model outputs, uh, its decisions.
So, um, one, one issue is confirmation bias,
which is the tendency to search for, interpret, favor,
recall information in a way that confirms preexisting beliefs.
And so a lot of times when we, uh,
build end-to-end systems and try and test our hypotheses,
we're kind of just testing it towards, uh,
things that we want to be true and analyzing the results in a way that will,
uh, help confirm what we want to be true.
Um, overgeneralization, which is coming to
a conclusion based on information that's too general or not specific enough.
Um, this is an issue that happens a lot of times
in the analysis of deep learning model results um,
where it's assumed that there's,
there's some kind of general, uh,
conclusion that can be taken away when really it's actually just,
uh, an effect of really skewed data.
Um, this is also closely related to overfitting which
is kind of the machine learning version of overgeneralization,
which is where you're still making predictions and outcomes,
but it's based on a small set of possible features, um,
so it's not actually capturing the space of the correct features for the outcome,
uh, the desired output prediction correctly.
Um, there's also a correlation fallacy,
which is confusing correlation with causation.
And this happens a lot again in talking about what
machine learning models are learning and
deep learning models are learning in particular, um,
where just because things happen together,
doesn't mean that one is causing the other,
but, uh, models don't tell you
anything- deep learning models directly don't
tell you anything about the causal relations.
And so it's easy to think that some output that is predicted
based on a correlation is actually something that's causal,
and I'll talk about some examples of this too.
Um, a further issue is automation bias,
and this really affects the machine learning models we put out there in the world that
then get used by people in systems like justice systems.
Um, so that's the tendency to, um,
favor the suggestions of
automatic predictions of models that output predictions over the,
um, uh, over the different kinds of um,
suggestions of another human.
Um, and this happens even in the face of contradictory evidence.
So, if a system is telling you, you know, "This,
this is the score or this is the risk of this individual",
then we're more likely to think it's true because it came out of a mathematical system,
and we automatically sort of see this as something more objective,
something more mathematical, that something's going to
be more true than humans some- somehow.
Um, and that's automation bias.
So, um, rather than this kind of
clean straightforward pipeline that we have in machine learning,
um, we have human bias coming in at the very start in the data, um,
and then human bias coming in in data collection, annotation,
and then further getting propagated through the system as we train on that data,
um, as we start putting outputs based on that data,
as people act on that data.
And this creates a feedback loop where
the kinds of things that we output for people to act on,
um, are then, are then,
then serves as further training data for input into your system,
so you end up amplifying even further these different kinds of implicit biases.
This is known as a Bias Network Effect or Bias "Laundering", I like to call it.
And so, the message is that human data perpetuates human biases.
And then as as machine learning or deep learning learns from human data,
the results is a bias network effect.
So, I want to steer clear of the idea that if I say bias or someone says bias that equals bad,
it's a little bit more nuanced than that.
Um, so there are all kinds of things that people
mean when they're talking about bias, um,
and even the same bias can be good in some situations and bad in some situations,
so bias in statistics and ML.
Um, we, we talk about the bias of an estimator which is
the difference between the predictions and the truth, the ground truth.
Uh, we talk about the bias term in linear regression.
Um, we also have cognitive biases,
and I talked about that in the beginning,
and not all of those are negative or,
or have to be, uh,
or have to be seen as negative.
So optimism is another kind of bias that we can
have that affects our worldview and the way we sort of process things.
Um, and even things like recency bias and
confirmation bias are just ways that our minds can like, um,
handle the combinatorial explosion of all the different things that can be
true in the world and put it down to something
tractable that we can sort of operate with in the real world.
Um, so algorithmic bias is what a lot
of people mean in headlines and whatnot when we're talking about bias,
which is, uh, more about unjust,
unfair or prejudicial treatment of people that's an output of,
a automated decision system.
Um, and the focus here is really on, uh,
unjust, unfair or prejudicial treatment of people.
So, a lot of the work in this space right now is focusing on trying to understand,
what does it mean to be unjust from an algorithm,
what does it mean to be unfair from an algorithm,
and how can we handle this,
how can we sort of mitigate these issues in order to be able to keep
developing technology that's useful for people without worsening social divides.
Um, and I felt the Guardian put it really well a few years ago.
Um, they said, "Although neural networks might be said to write their own programs,
they do so towards goals set by humans using data collected for human purposes.
If the data is skewed, even by accident,
the computers will amplify injustice."
And it really keyed in on this amplify injustice idea.
Um, and let's talk about what that can mean.
So, one of the avenues of deep learning research that's
taken off in the past few years is predicting criminal behavior.
Um, so, um, how many of you are familiar with Predictive Policing?
[NOISE] Okay, like, half of the class.
Okay. So, in predictive policing, algorithms, um,
predict areas to deploy officers where crime is considered to be likely to occur.
But the data that the- the- the models are trained off
of is based on where police officers have already gone and made arrests.
So, the systems are simply learning the patterns of bias that
humans have and where do they go and where they are trying to decide to def- uh,
to find crime, um,
and then reflecting them back.
So, because this system hones in on some of
the top spots where people have been arrested,
notice that's not the same of- that's
the same thing as where crimes have been committed, right?
It's where arrests have been made.
Um, it means that the other areas that
might be explored for crime don't get explored at all.
That worsens the situation.
Um, some neighborhoods, uh,
get really acutely focused attention on them,
and that heightens the chances of serious repercussions for
even minor infractions, that means arrests.
And that means a feedback loop of data that
you will get an arrest in this place if you go there.
Um, another, uh, sort of related issue in this space is, uh, predictive sentencing.
Um, so there was a really nice article that came out
from Pro- ProPublica a few years ago discussing this.
Um, but when most defendants are booked in jail,
they respond to a questionnaire called COMPAS.
Um, and their answers are fed into this software system that
generates scores that correspond to the risk of recidivism,
that's the risk of um,
er, making a crime again.
Um, and the questions are used to gather data
on the defendant's socio-economic status,
family background, neighborhood crime,
employment status, and other factors in order to reach
some predictim- prediction of an individual's crime or criminal risk.
Um, but what ends up happening is that it ends up focusing on the key bias issues
that humans have and propagating it
back with something that looks like an objective score.
So, you're a lot more likely um,
to be convicted of a crime, um,
if you're black than if you're white,
even if you've made the exact same crime.
And the system will pick up on this,
and will reflect this back to say that people who are
black are more likely to have reci- like recidivism,
more likely to convict a,
uh, to make a crime again.
Um, so this is an example of automation bias, preferring the output of a system, uh,
in the face of overgeneralization, feedback loops,
and correlation fallacy,
confusing things that are occurring together as being somehow causal.
There's another, uh, sort of area of research and, uh,
startups looking at predicting criminality in particular from things like face images.
So there's a company out there, uh, called Faception.
They are based in Israel and they claim to be able to,
um, use individual images, uh,
with computer vision and machine learning technology for profiling people
and revealing their personality based only on their facial image,
um, recognizing things like high IQ,
white-collar offender, pedophile, and terrorist.
Um, and their main clients are Homeland Security,
lots of other, uh,
lots of other countries dealing with sort of public safety issues.
They've not published any details about their methods,
their sources of training data,
or their quantitative results.
We know that in light of automation bias,
people will tend to think it just works even when it doesn't work well.
Um, but there was a paper that came out wi- in
a similar line predicting criminal, criminality,
or purporting to predict criminality from individual face images,
and that one had some results and,
uh, some more details about the data that we could kinda dig
into to understand where are these kinds of claims coming from.
Um, so this was an article that was posted on Archive near the end of 2016.
Um, and they said they were using less than 2,000 closely cropped images of faces, um,
including wanted suspect ID pictures from specific regions,
and they claimed that even based on this very small training dataset, um,
that they were able to predict, uh,
whether or not someone was likely to be a criminal,
uh, greater than 90 percent accuracy.
Um, and they got so lost in this,
this idea that, uh,
it's sort of funny to read to just take
a step back and realize what's actually happening.
So for example, one of
their really great exciting claims was that the angle Theta from nose tip to
two mouth corners is on average
19.6 percent smaller for criminals than for non-criminals.
This is otherwise known as smiling. [LAUGHTER]
Uh, and [LAUGHTER] you know,
exactly the kind of images people would
use when trying to put out wanted criminal pictures,
probably not really happy pictures.
But you get so lost in the confirmation bias.
You get so lost in the correlation and the
feedback loops that you end up overlooking these
really obvious kinds of things.
Um, so that's an example of selection bias,
experimenter's bias, confirmation bias, correlation fallacy,
and feedback loops all coming together to create
a deep learning system that people think is
scary and can do things that it can't actually do.
Um, one of the issues with this was that the media loved it.
Like it was all over the news,
and there's been similar kinds of things happening again and again.
Media wants to sell the story,
and so it's part of our job as researchers,
that people who work on this stuff,
to be very clear about what the technology is actually doing,
uh, and make a distinction between what you
might think it's doing and what it's actually doing.
Um, so another issue that has come up recently, um,
it's claiming to be able to predict
internal qualities but specifically ones that are subject to discrimination,
um, and loss of opportunity.
So in particular, there was this work that came out that claimed
to be able to predict whether or not someone was homosexual,
just based on single face images.
Um, now, it's important to know that the images that they used in the study included
images that were from dating websites where people self-identified as straight or gay,
and identified as whether they were looking for a partner who was straight or gay,
and these became the sources of the training data,
and still from this, uh.
Oh! Before I go on, can you guys just understand
just from that what the issue might have been?
Rainbows.
[LAUGHTER] I don't think that there was actually anything about rainbows,
but that's really unfortunate.
[LAUGHTER].
[inaudible]
Right. Yeah. So this has more to do with the presentation of the self,
the presentation of the social self when you're trying to for example,
attract a partner on a website,
and less to do with how you look day to day.
Um, and yet they kind of went to
these large conclusions that aren't supported at all by the data or by their study,
um, but things like consistent with a prenatal hormone theory of sexual orientation.
Gay men and women tended to have gender atypical facial morphology.
Now, none of the authors actually were prenatal hormone theory specialists, you know.
They have doctor in their name so maybe that's a thing.
Um, this was a Stanford professor and like I've,
I've presented this a few times at Stanford and gotten into
some like pretty harsh fights about this.
So I'm ready if anyone wants to take me on.
[LAUGHTER] But uh, me and my uh,
some of my colleagues decided we'd,
we'd play around with this a bit,
and what we found was that a simple decision tree.
Um, so I'm kind of assuming you guys know what a decision tree is.
So, okay.
Cool. So based on wearing makeup or wearing glasses,
got us pretty close to the accuracy reported in
the paper. That says nothing about internal hormones,
that says nothing about any of that,
and it says a lot about the physical presentation,
the things that are on the surface.
Um, it says a lot more about how people are
presenting themselves than what is happening internally.
Um, so the key thing that's recently kind of
been overlooked is that deep learning is somehow,
i- it's sort of considered that it's somehow magically going beyond surface level.
But the point is that it's working on the surface level and working well.
And in the face of confirmation bias and other kinds of bias factors,
it's easy to assume that something else is happening that's not.
Without critical examination, uh,
for example simple baselines, uh,
simple sanity checks, these kinds of things can just be ignored and,
and not noticed at all.
Um, so that's example of selection bias,
um, experimenter's bias, and correlation fallacy.
Okay. So now I'm going to talk to,
talk about measuring algorithmic bias.
So I just said a lot about different kinds of biases that come in in the data,
in the collection, in the interpretation of the results.
[NOISE] Let's talk about actually quantitatively measuring different kinds of biases.
Um, so one of the key things that's, uh,
emerged in a few different works and really ties nicely to a lot
of fairness work is this idea of disaggregated evaluation.
So in disaggregated evaluation,
you evaluate across different subgroups as opposed to
looking at one single score for your overall testing data set.
Um, so, okay.
You guys are probably familiar with the training testing data split.
You kind of train on there,
on your given training data,
you test on your given testing data and then you repo- you report like precision,
recall, F-score, things like that.
Um, but what that masks is how well the system is actually
working across different kinds of individuals and across different, different subgroups.
Um, and so one just straightforward way to handle
this is to actually evaluate with respect to those different subgroups.
So creating for each sort of subgroup prediction pair.
Um, so for an example,
you might look at women face detection,
men face detection, and look at how the,
the error rates are,
are different or are um, similar.
Um, another important part of this is to look at things intersectionally,
um, combining things, um,
like gender and race at the same time and seeing how those, uh,
how the error rates on those sorts of things
change and how they're different across uh, different intersections.
Um, and this is inspired by Kimberle Crenshaw.
Um, who she, she pioneered intersectional research,
uh, in critical race theory.
Um, and she discussed the story of Emma DeGraffenreid, uh,
who was a woman at General Motors, um,
and she claimed that the company's hiring practices discriminated against black women.
Um, but in her court opinion,
the judges ruled that General Motors hired, um,
many women for secretarial positions and many black people for factory roles,
and thus they could not have discriminated against black women.
What they failed to do was look at the intersection of
the two and understand that the experience there might be
fundamentally different than any of
the experiences of either of these sort of subgroups in isolation.
Um, and the same becomes true when you start looking
at errors that are regularly made in deep learning systems.
Um, so we've been able to uncover a lot of
different kinds of unintended errors by looking not only at
the disaggregated evaluation but also at intersectional disaggregated evaluation.
Um, so I'm going to walk through a bit how this works.
This is probably going to be review for most of you,
but I think it's really important to understand this because it also
ties to how we measure fairness and when we say like,
uh, algorithmic fairness, what we're talking about.
So um, the confusion matrix is a way, you guys.
Okay. Are you guys familiar with the confusion matrix?
[LAUGHTER]. I just want want to know where.
Okay. Awesome. Cool. So you're familiar with the confusion matrix, right.
So you have model predictions and references.
Um, and you can kind of look at these as negative and positive,
uh, binary classification, uh,
kind of approach here where if
the ground truth says something is true and the model predicts it's true,
it's a true positive.
If the ground truth says, uh,
it's, it's, it's false,
um, and the model predicts it's false, it's true negative.
Um, and the errors that the kind of different issues that
arise are false negatives and false positives.
Um, so in false positives the, um,
the ground truth says something is negative but the model predicts that it's positive.
Uh, and then in false negatives, vice versa.
Um, from these, you know,
uh, basic kind of, uh,
these basic breakdown of errors,
you can get a few different metrics.
Um, these metrics actually trivially map to a lot of different fairness criteria.
So um, for example,
if we're looking at something like
a female versus male patient results and figuring out things like precision and recall,
which is relatively common in NLP, um,
if you have equal recall across your subgroups
that's the same as the fairness criteria of equality of opportunity,
um, I could work through the math.
But I mean, this is basically just,
just the main point that, that, uh,
it says that given that something is true in the ground truth,
the model should predict that it's true,
uh, at equal rates across different subgroups.
So this ends up being equivalent to having the same recall across different subgroups.
Similarly, um, having the same precision across
different subgroups is equivalent to a fairness criterion called predictive parity.
And so as fairness has been defined again and again, um,
it was originally some of these definitions came in
1966 following the Civil Rights Act of 1964.
Um, they were reinvented a few times, uh,
and most recently reinvented in, uh, 2016.
Um, but they all sort of boiled down to
this disaggregated comparison across subgroups and the math,
the metrics end being roughly equivalent to what we get from the confusion matrix,
specifically in classification systems.
So which kind of fairness metric do you use,
what are the different criteria you want
to use to look at the differences across different subgroups,
that really comes down to the trade-offs
between false positives and false negatives.
So this is the same problem that you are dealing with
when you're just figuring out how to evaluate generally.
Um, there's no one fairness criteria and that is
the fairness criteria and to rule them all, um,
deciding which one is better than the other is the same as
kind of trying to decide which is better, precision or recall, right?
It depends on what the problem is and what you're interested in measuring.
Um, so a case where false positives might be better than
false negatives and so you want to prioritize something like a false positive rate,
ah, across subgroups is privacy and images.
So here a false positive is something that doesn't need to be blurred gets blurred.
That's just kind of a bummer.
Um, but a false negative would be something that needs to be
blurred is not blurred and that can be identity theft.
It's a much more serious issue.
And so it's important to prioritize
the evaluation metrics that stress the false negative rates.
Um, an example where false negatives
might be better than false positives is in spam filtering.
So a false-negative could be an e-mail that's spam not caught so you see it in your inbox,
that's usually just annoying, it's not a big deal.
Um, but a false positive here would be e-mail flagged as
spam and then removed from your inbox, which,
you know, if its from a friend or a loved one,
it can be, it can be a loss,
maybe a job offer something like that.
All right.
So, um, I just kind of covered how AI can unintentionally lead to
unjust outcomes and some of the things to do
or some of the things to be aware of here,
are the lack of insight into sources of bias in the data, in the model,
lack of insight into the feedback loops from the original data that's collected
as an example of what humans do to the data that's then repurposed,
re-used, acted on, and then further fed in.
Um, a lack of careful disaggregated evaluation,
looking at the disparities,
the differences between different subgroups in order to understand this bias,
this difference across the subgroups.
Um, and then human biases in interpreting, and accepting,
and talking about the results,
which then kind of further the media cycles and the hype around AI right now.
Um, but it's up to us to influence how AI evolves.
So I like to think of this in terms of short term,
middle term, and long-term objectives.
So short term today,
we might be working on some specific model where we're trying to find some local optimum,
we have a task, we have data, something like that.
And that's sort of short-term objectives.
Um, we might have a slightly longer-term objective of getting a paper published,
or if you're an industry like getting a product launched,
whatever it might be.
Um, from there we might see our next endpoint is getting an award or,
you know, maybe become sort of famous for something for
a few minutes, something like that and that's cool.
Um, but there's a longer-term objective that we
can work towards as well at the same time.
And that's something like a positive outcome for humans in their environment.
So instead of just kind of focusing on these local decisions,
these local optima and these
sort of local paper by paper-based approaches to solving problems,
you can also kind of think about what's the long-term objective.
Where does this get me as I trace out an evolutionary path for artificial intelligence,
down the line in 10 years,
15 years, 20 years.
Um, and one of the ways you can address this is by thinking,
now how can the work I'm interested in now be best focused to help others?
And that involves talking to experts,
um, and kind of going outside your bubble,
speaking across interdisciplinary fields like
cognitive science which I've just talked a bit about.
Um, so let's talk about some things we can do.
So first off is data.
Um, so a lot of the issues of bias and fairness,
ah, in machine learning models really come down to the data.
Unfortunately in machine learning and deep learning,
working on data is really not seen as sexy.
Ah, there's a few datasets, ah,
that people use that are out there,
that's what people use,
and there's not a lot of analysis done on,
on how well these datasets capture different truths about the world,
how problematic they might be,
[NOISE] um, but it's a pretty wide area that needs a lot of future,
like lea- needs a lot of future additional work.
Um, [NOISE] so we're going to understanding the data skews and the correlations.
If you understand your data skews and the,
ah, correlations that might be problematic in your data,
then you can start working on either models that address those,
or data augmentation approaches in order to sort of make
the dataset a little bit better or a little bit more representative
of how you want the world to be.
Um, it's also important to abandon the single training set- testing
set from similar distribution approach to advancing deep learning.
So um, when we do projects in deep learning,
you know, we tend to have the training set,
and the testing set and then that's what we sort of benchmark on and prioritize,
but the point is, as you move around different testing sets,
you're gonna get vastly different results.
Um, and so by keeping in
this just sort of one training testing dat- training testing dataset paradigm,
you're really likely to not notice issues that might otherwise be there.
And one way to really focus in on them,
is having a hard set of,
of test cases, that you really wanna make sure the model does well on.
So these are things that are particularly problematic.
Things that would be really harmful to individuals,
um, If they were to experience the output.
Um, and you kinda collect those in a small test set and then it's really easy
to evaluate on that test set as you benchmark improvements on your model,
as you add different kinds of things to your model,
in order to see, um,
not just how your model is doing overall,
in terms of your testing dataset,
but how well you're doing in terms of these examples,
you really want it to do well on.
That you know that is going to be a problem if it doesn't do well on,
and any sort of degradation in that,
you might want to prioritize, um,
to fix above degragaish- degradation and overall accuracy.
Um, and it's also important to talk to experts
about the additional signals that you can incorporate.
Um, so we've put out a tool to help with this,
ah, understanding data skews called facets,
um, it's just available there.
Um, and it's a really handy kinda visualizer for slicing, ah, understanding,
um, you know, what some of the differences are between different subgroups
and different representations and you can sort of dig in and explore a bit more.
So this is just to sort of help people, ah,
come to terms with the data that they're actually using and,
and where there might be, um,
unwanted associations or, or missing,
missing kind of features.
[NOISE] Um, another approach that's been put forward recently,
ah, specifically on the data side is this data,
datasheets for datasets approach.
Um, so this is this idea that when you release a dataset,
it's not enough to just release the dataset with like
some pretty graphs and like talking about basic distributional information,
you need to talk about who the annotators were, where they were,
what the inter-annotator agreement was,
what their background information was,
um, motivation for the dataset.
All these other kinds of details.
So now you actually know that this isn't just a dataset,
this is a dataset that has these specific biases.
There's no such thing as a dataset that isn't biased in some way.
A dataset by virtue of the fact that it's collected from the world as a subset,
is a, is a biased set of the world in some way.
The point is to make it clear what it is,
how it is biased, what are the,
what are the various biases,
ah, that are important to know about in the dataset.
So that's one of these ideas between- behind datasheets for datasets,
releasing its datasets publicly.
All right. Now let's switch a little bit to machine learning.
Um, so there are a couple of techniques that I like to use. Um, I'll talk about two.
One, ah, is bias mitigation,
which is removing the signal for a problematic output.
Um, so removing, ah, stereotyping,
sexism, racism, trying to remove these kind of effects from the model.
Um, this is also sometimes called de-biasing or unbiasing,
but that's a little bit of a misnomer because you're- you're generally just kind of
moving around bias based on a specific set of words for example,
um, so to say it's unbiased is is not true.
Um, but you are kind of mitigating bias with respect to
some certain kinds of information that you provide it with.
Um, and there's inclusion which is then adding signal for desired variables.
So that's kind of the opposite side of bias mitigation.
So increasing model performance with attention to
subgroups or data slices with the worst performance.
Um, so, ah, in order to,
er, address inclusion, ah,
kind of adding signal for under-represented sub-groups,
one technique that's worked relatively well is multi-task learning.
Um, so I've heard that you guys have studied multi-task learning which is great,
um, so I'll tell you a bit about a case study here.
Um, so this is work I did, ah,
in collaboration with a UPenn World Well-being Project, ah,
working directly with clinicians,
and the goal was to create a system that could alert
clinicians if there was a suicide attempt that was imminent.
Um, and they wanted to understand the feasibility of
these kinds of diagnoses when there were very few training,
ah, training instances available.
So that's similar to kind of the minority problem in datasets.
Um, [NOISE]
And, uh, in this work,
we had two kinds of data.
One was the internal data which was the electronic health records, um,
with the- that was either provided by the patient or from the family.
Um, it included mental health diagnoses,
uh, suicide attempts or completions, um,
if, if, if that were the case along with,
uh, the user's, uh,
the person's social media data.
And that was the internal data that we did not publish on,
but that we were able to work with clinicians on in
order to understand if our methods were actually working.
Um, the external data, the proxy data,
the stuff that we could kinda publish on and talk about,
was based on Twitter.
Um, and this was, uh,
using regular expressions in order to extract, uh,
phases in Twitter feeds that had something that was kind of like diagnoses.
So something like, I've been diagnosed with X,
or I've tried to commit suicide.
And that became kind of the,
the proxy dataset and the corresponding social media feeds for,
for those individuals, uh,
for the actual diagnoses.
Um, and the state-of-the-art in clinical medicine, uh,
kind of until this work,
there's been more recently but, uh, it's,
it's sort of this single task logistic regress- lo- lo- logistic regression setup.
Where you have some input features,
and then you're making some output predictions like true or false.
Um, you can add some layers and start making it deep learning which is much fancier.
Um, you can have a bunch of tasks in order to
do a bunch of logistic regression tasks for a clinical environment.
Um, or you can use multitask learning, uh,
which is taking the basic deep learning model and adding a bunch of heads to it,
uh, predicted jointly at the same time.
Um, and here we had a bunch of diagnosis data.
So, um, we predicted things like depression,
anxiety, uh, post-traumatic stress disorder.
Um, we also added in gender because this is
something that the clinicians told us actually, uh,
had some correlation with some of these conditions,
and that they actually used it in making decisions themselves,
for whether or not someone was likely to,
uh, attempt, uh, suicide or not.
Um, and this also used this idea of comorbidity.
So multi-task learning is actually kind of perfect for comorbidity in clinical domains.
So comorbidity is, um,
when you have one condition,
you're a lot more likely to have another.
Um, so people who have
post-traumatic stress disorder are much more likely to have depression and anxiety.
Um, and depression and anxiety tend to be cormorbid,
so people who have one often have the other.
So this points to the fact- this points to the idea that perhaps there's
some underlying representation that is similar across them,
that can be leveraged in a deep learning model,
with individual heads further specifying,
uh, each of the different kinds of conditions.
Um, and so what we found was that as we moved from
logistic regression to the single task deep learning to the multi-task deep learning,
we were able to get significantly better results.
And this was true both in the suicide risk case where we had a,
a lot of data, as well as
the post-traumatic stress disorder case where we had very little data.
Um, the behavior here was a little bit different.
So going from logistic regression to,
um, single task deep learning,
when we had, um,
a lot of data, uh,
as we did with the suicide risk, um,
had the single task deep learning model
working better than the logistic regression model.
Um, but when we have very few instances, uh,
this is where the deep learning models really struggled a lot more.
Um, and so the logistic regression models were actually much better.
But once we started adding heads for the cormorbid different kinds of conditions,
the different kinds of tasks, um,
that related to, you know,
whether or not the person might be committing suicide, um,
we were able to, uh,
bump the accuracy way back up again.
Um, and, it, you know,
it's roughly 120 at-risk individuals that we were able to collect, uh,
in the suicide case that we wouldn't have otherwise been able to,
to notice as being at risk.
Um, one of the approaches we took in this was to
contextualize and consider the ethical dimensions of releasing this kind of technology.
So, um, it's really common in NLP papers to give examples.
Um, but this was an area where we decided that
giving examples of like depressed language,
could be used to discriminate against people,
like at, you know, job,
interviews, or something like that, you know,
the sort of armchair psychology approach.
So we decided that while it was important to talk about the technique,
and the utility of multitask learning in
a clinical domain and for bringing in inclusion of underrepresented subgroups,
it had to be balanced with the fact that there was a lot of
risk in talking about depression,
and anxiety, and how those kinds of things could be predicted.
Um, so we tried to take a more balanced approach here, um,
and since then I've been putting ethical considerations in all of my papers.
Um, it's becoming more and more common actually.
Um, so another kind of approach that's now turning this on its head,
where you're trying to remove some effect, um,
mitigate bias in some way,
is adversarial multi-task learning.
So I just talked about multi-task learning,
and I'll talk about the adversarial case.
Um, and the idea in the adversarial case is that you have a few heads.
Um, one is predicting the main task,
and the other one is predicting the thing that you don't
want to be affecting your model's predictions.
So for example, something like whether or not someone should be promoted based on,
uh, you know, their performance reviews,
and things like that.
Um, you don't want that to be affected by their gender.
Ideally, gender is independent of a promotion decision.
And so you can, uh,
you can create a model for this that actually,
uh, puts that independence, um,
criteria in place by saying, uh,
I want to minimize my loss on the promotion,
while maximizing my loss on the gender.
And so how we're doing that is just predicting gender,
and then negating the gradient.
So removing the effect of that signal.
Um, this is another adversarial approach.
So you might have been familiar with like generative adversarial networks.
So this is like two discriminators, uh,
two different task heads, uh,
where one is trying to do the task that we care about,
and the other one is removing the signal, uh,
that we really don't want to,
um, uh, be coming into play in our downstream predictions.
Um, so this is a way of,
uh, kind of putting this into practice.
So the probability of your output,
uh, predicted output given the,
the ground truth and your sensitive attribute like gender, um,
is equal across all the different, uh,
sensitive attributes or equal across all the different genders.
Um, and that's an example of equality of opportunity in supervised learning,
being put into practice.
So this is one of the key fairness definitions.
It's equivalent to, uh,
equal recall across different subgroups as I mentioned earlier.
Um, and that's a model that will actually,
uh, implement that or help you achieve that.
Um, where you're saying that a classifier's output decisions should be the same
across sensitive characteristics given what the,
what the correct decision should be.
Okay, so how are we on time?
Cool. Are there any questions so far? Are we good?
Okay, cool. So I'm gonna go into a little bit of a case study now, an end-to-end, uh,
system that Google has been working on, uh,
my colleagues have been working on, uh,
that is in NLP domain and deals with some of these bias issues.
Um, so you can find out more about this work, um,
in papers at AIES in 2018 and FAT* tutorial 2019,
um, called Measuring and Mitigating Unintended Bias in Text Classification.
Um, and this came out of Conversation-AI which is a, uh,
which is a product that's, um,
like it's part of this- it's called a bet at Google.
It's a kind of spin-off company called Jigsaw that
focuses on trying to like combat abuse online.
Um, and the Conversation-AI, uh,
team is trying to use deep learning to improve online conversations.
Um, and collaborate with a ton of different,
uh, different people to do that.
Um, so how this works is,
oh you can try it out too, on perspectiveapi.com.
So given some phrase like you're a dork, uh,
it puts out a toxicity score associated to that like 0.91. [NOISE]
Um, and the model starts sort of falsely associating
frequently attacked identities with toxicity.
So this is a kind of false positive bias.
So I'm a proud tall person gets a model,
uh, toxicity score of 0.18.
I'm a proud, uh,
gay person gets a toxicity model score of 0.69.
And this is because these- the term gay tends to be used in really toxic situations.
And so the model starts to learn that gay itself is toxic.
But that's not actually what we want,
and we don't want these kinds of predictions coming out of the model.
Um, so, uh, the bias is largely caused here by the dataset imbalance.
Again, this is data kinda coming and wearing its hat again.
Um, so frequently attacked, uh,
identities are really overrepresented in toxic comments.
There's a lot of toxicity towards LGBTQ identities, um,
it's really horrible to work on this stuff that like
really [LAUGHTER] it can really affect you personally.
Um, uh, and, uh,
one of the approaches that the team took was just to add nontoxic data from Wikipedia.
So helping to- helping the model to understand that these kinds of terms can be used in,
you know, more positive sorts of contexts.
One of the challenges with measuring, uh,
how well the system was doing is that there's not
a really nice way to have controlled toxicity evaluation.
Um, so in real-world conversation,
it can be kind of anyone's guess what the toxicity is of a specific sentence.
Um, if you really wanna control for a different kind of
subgroups or intersectional subgroups,
and it can be even harder to get, uh,
a real good data to evaluate properly.
So what the team ended up doing was developing a synthetic data approach.
Um, so this is kind of like a bias Mad Libs.
Um, where you take template sentences [NOISE], um,
and you use those for evaluation. This is the kind of, um,
evaluation you'd want to use in addition to your target downstream
ah, kind of dataset.
But this helps you get at the biases specifically.
So, um, some template phrase like I am a proud blank person,
and then filling in different subgroup identities.
And you don't want to release the model unless you see that
the scores across these different kinds of, uh,
these different kinds of template sentences with synthetic, uh,
the synthetic template sentences, um,
are relatively kind of the same across, ah, yeah.
All of the different model runs.
Cool. Um, so some assumptions that they made in this was that the dataset, um, uh,
didn't have annotated bias and they didn't do
any causal analysis because they were just trying to focus in particular,
um, on this toxicity problem.
Um, they used the CNN,
ah, convolutional, yeah you guys know, blah, blah, blah.
Uh, with pretrained chain GloVe embeddings.
This is probably like your bread and butter.
Pretrained GloVe embeddings.
I'm sure you know all about this in Word2vec.
Cool, uh, Keras implementation of this.
Um, and, uh, and using these kind of data augmentation approaches, um,
both a Wikipedia, uh,
kind of approach as well as actually collecting positive statements about LGBTQ identity.
So there's this project called Project Respect at Google,
where we go out and,
and talk to people who identify as queer or people who have friends who do,
and like talk about this in a positive way,
and we add this as data.
Um, so we can actually know that this is can be a positive thing.
Um, and in order to measure the model performance here, um,
again it's looking at the differences across different subgroups and trying to
compare also the subgroup performance to some sort of general distribution.
So here they use AUC, um,
where AUC is essentially the probability that a model will
give a randomly sel- selected positive example,
a higher score than a randomly selected, uh, negative example.
So, um, here you can see some toxic comments and
nontoxic comments with a example sort of low AUC.
Um, here, ah, this is an
example with a high AUC,
so the model is doing a relatively good job of separating these two kinds of comments.
Um, and there are different kinds of biases that they've defined in this work.
So, uh, low subgroup performance means that
the model performs worse on subgroup comments than it does,
ah, on comments overall.
And the metric they've introduced to measure this is called subgroup AUC.
Um, another one is subgroup shift.
And that's when the model systematically scores comments,
um, from some subgroup higher.
Um, so this is sort of like to the right.
Um, and then there's also, uh,
this Background Positive Subgroup Negative shifting to the left.
Yeah. Um, yeah that's sort of saying what I said.
It can go either way to the right or the left and there's just
kind of different metrics that can define each of these.
Cool. Um, and the results in this,
ah, sort of going through not only just looking at, you know,
qualitative examples, um, and general evaluation metrics,
but also focusing in on some of the key metrics defined for this work,
these sort of AUC-based approaches.
And they were able to see significant differences in
the original release which didn't account for any of these unintended biases,
and downstream releases, uh, which did,
which incorporated this kind of normative data
that said the sort of things that we thought the model should be learning.
Cool. Um, so, um,
the last thing to keep in mind as you sort of develop and,
and work towards, uh, creating deeper better models is to release responsibly.
Um, so this is a project I've been working on with
a ton of different people called Model Cards for Model Reporting.
It's, uh, it's a little bit of like the next step after Datasheets for Datasets,
um, where, um, Datasheets for Datasets focuses on information about the data.
Ah, Model Cards for Model Reporting focuses on information about the model.
Um, so it captures what it does,
how it works, why it matters.
Um, and one of the key ideas here is disaggregated in intersectional evaluation.
So it's not enough, uh,
any more to put out human-centered technology that just
has some vague overall score associated to it.
You actually need to understand how it works across different subpopulations.
And you have to understand what the data is telling you that.
Um, so here's some example details that a
model card would have,
um, who it's developed by,
what the intended use is,
so that it doesn't start being used in ways that it's not intended to be used.
Um, the factors that are likely to be
affected by disproportionate performance of the model.
Um, so different kinds of identity groups, things like that.
Um, the metrics that, ah,
that you're deciding to use in order to understand the fairness of the model or
the different performance of the model across different kinds of subgroups and factors,
information about the evaluation data and training data.
Um, as well as ethical considerations, um,
so what were some of the things you took into
account or what are some of the risks and benefits,
um, that, uh, that are relevant to this model?
Um, and additional caveats and recommendations.
So for example, in the conversation AI case,
they're working with synthetic data.
So this is the sort of limitation of the evaluation that's important to understand, uh,
because it can tell you a lot about the biases,
but doesn't tell you a lot about how it works generally.
[NOISE] And then the key component in the quantitative,
uh, section of the model card is to have
this both intersectional and disaggregated evaluation.
And from here, you trivially get to different kinds of fairness definitions.
The closer you get to parity across subgroups,
the closer you're getting to something that is mathematically fair.
Okay. So hopefully by paying attention to these kinds of approaches,
taking into account all these kinds of things,
we can move from majority representation of data in
our models to something more like diverse representation,
uh, from our ethical AI.
Okay. That's it.
Thanks. [APPLAUSE] 
 Let's get started. So welcome to the very final lecture of the class.
I hope you're all surviving the last week and,
uh, wrapping up your projects.
So today we're going to be hearing about the future of NLP and deep learning.
Uh, so Chris is still traveling and today we're going to be having Kevin Clark,
who's one of the PhD students in the lab, uh,
in the NLP lab,
and he was also one of the head TAs for the class last year.
So he's very familiar with the class as a whole.
Um, so, take it away Kevin.
Okay. Thanks, Abby. Um, yeah,
it's great to be back after being a TA last year.
Um, I'm really excited today to be talking about the future of deep learning and NLP.
Um, obviously, trying to forecast the future, um,
for deep learning or anything in that space is really
difficult because the field is changing super quickly.
Um, so as one reference point, um,
let's look at what did deep learning for NLP,
um, look like about five years ago.
And really, a lot of ideas that are now considered to be pretty core techniques,
um, when we think of deep learning and NLP,
um, didn't even exist back then.
Um, so things you learned in this class like Seq2Seq,
attention mechanism, um, large-scale,
reading comprehension, uh, even frameworks
such as TensorFlow or Pytorch, um, didn't exist.
And, uh, the point I want to make with this is that, um,
because of this it's really difficult to, to look into the future and say,
okay, what are things going to be like?
Um, what I think we can do though is look at, um,
areas that right now are really sort of taking off, um,
so areas in which, um,
there's a lot, been a lot of recent success and kind of, uh,
project from that, that,
those same areas will likely be important in the future.
Um, and in this talk I'm going to be mostly focusing on one key idea of
wh- key idea which is the idea of leveraging
unlabeled examples when training our NLP systems.
So I'll be talking a bit about doing that for machine translation, um,
both in improving the quality of translation and even
in doing a translation in an unsupervised way.
So that means you don't have, um,
paired sentences, uh, with, with their translations.
Um, you try to learn a translation model only from a monolingual corpus.
Um, the second thing I'll be talking a little bit about is, uh,
OpenAI's GPT-2, um,
and in general this phenomenon of really scaling up,
um, deep learning models.
Um, I know you saw a little bit of this in the lecture on contextual representations,
but this, but this will be a little bit more in depth.
Um, and I think, um,
these new developments in NLP have had some,
um, pretty big, uh,
impacts in terms of,
uh, more broadly kind of beyond even the technology we're using,
and in particular, I mean,
starting to raise more and more concerns about the social impact of NLP, um,
both, um, in what our models can do and also in kind
of plans of what, where people are looking to apply these models, um,
and I think that really has some risks associated with it, um,
in terms of security also in terms of areas like bias.
Um, I'm also gonna talk a bit about future areas of research,
um, these are mostly research areas now that are, um,
over the past year have really kind of developed into
promising areas and I expect they will continue to be important in the future.
Okay, um, to start with,
I wanna ask this question, why has deep learning been so successful recently?
Um, I like this comic, um,
here there's a statistical learning person,
um, and they've got some really complicated,
um, well-motivated, uh, method for doing, um,
the task they care about,
and then the neural net person just says,
er, stack more layers.
Um, so, so the point I want to make here is that, um,
deep learning has not been successful recently because it's more
theoretically motivated or it's more sophisticated than previous techniques, um.
In fact I would say that actually a lot of, um,
older statistical methods have more of
a theoretical underpinning than some of the tricks we do in deep learning.
Um, really the thing that makes deep learning so
successful in recent years has been its ability to scale, right.
So neural nets, as we increase the size of the data,
as we increase the size of the models, um,
they get a really big boost in accuracy,
in ways other approaches do not.
And, um, if you look to the '80s and '90s, um,
there was actually plenty of research in neural nets going on, um.
But it hadn't, doesn't have a hype around it that it does
now and that seems likely to be because,
um, in the past there wasn't, um,
the same resources in terms of computers,
in terms of data and, um,
only now after we've reached sort of an inflection point where we can
really take advantage of scale in
our deep learning models and we started to see it become,
um, a really successful paradigm for machine learning.
Um, if we look at big, uh,
deep learning success stories, um,
I think, uh, you can see kind of this idea play out, right?
So here are three of what are arguably the most famous successes of deep learning, right.
So there's image recognition, where before,
people used very highly engineered, um,
features to classify images and now neural nets are much superior, um, to those methods.
Um, machine translation has really closed the gap between, um,
phrase-based systems and human quality translation,
so this is widely used in things like Google Translate
and the quality has actually gotten a lot better over the past five years.
Um, another example that had a lot of hype around it is game-playing, so, um,
there's been work on Atari games, there's been AlphaGo,
uh, more recently there's been AlphaStar and OpenAI Five.
Um, if you look at all three of these cases underlying
these successes is really large amounts of data, right.
So for ImageNet, um,
for image recognition, um,
there is the ImageNet dataset which has 14 million images,
uh, machine translation datasets often have millions of examples.
Um, for game playing you can actually
generate as much training data as you want essentially,
um, just by running your agent,
um, within the game,
um, over and over again.
Um, so if we,
if we look to NLP, um,
the story is quite a bit different for a lot of tasks, um, right.
So if you look at even pretty core kind of popular tasks,
to say, reading comprehension in English, um,
datasets like SQuAD are in the order of like 100,000 examples
which is considerably less than the millions or tens of millions of examples,
um, that these previous,
um, successes have, have benefited from.
Um, and that's of course only for English, right.
Um, there are, um,
thousands of other languages and this is I think
a problem with NLP data as it exists today.
Um, the vast majority of data is in English, um,
when in reality fewer than 10% of the world's population,
um, speak English as their first language.
Um, so these problems with small datasets are only compounded if you look at,
um, the full spectrum of languages, um, that exist.
Um, so, as what do we do,
uh, when we're limited by this data,
but we want to take advantage of deep learning scale and train the biggest models we can.
Um, the popular solution, um,
that's especially had recent success is using unlabeled data, um,
because unlike labeled data,
unlabeled data is very easy to acquire for language.
Um, you can just go to the Internet,
you can go to books, you can get lots of text, um,
whereas labeled data usually requires at the least crowdsourcing examples.
Um, in some cases you even require someone who's an expert in something like linguistics,
um, to, to annotate that data.
Okay, so, um, this first part of the talk is going to be applying
this idea of leveraging unlabeled data to improve our NLP models,
um, to the task of machine translation.
Um, so let's talk about machine translation data.
Um, it is true that there do exist quite large datasets for machine translation.
Um, those datasets don't exist because
NLP researchers have annotated texts for the purpose of training their models, right.
They exist because, er, in various settings,
translation is done just because it's useful, so for example,
proceedings of the European Parliament,
um, proceedings of the United Nations,
um, some, uh, news sites, they translate their articles into many languages.
Um, so really, the machine translation data we use to train our models are often
more of byproducts of existing cases where translation is wanted rather than,
um, kind of a full sampling of the sort of text we see in the world.
Um, so that means number one,
it's quite limited in domain, right.
So it's not easy to find translated tweets,
um, unless you happen to work for Twitter.
Um, in addition to that, um,
there's limitations in terms of the languages that are covered, right.
So some languages, say European languages,
there's a lot of translation data, um,
for other languages there's much less.
Um, so in these settings where we want to work on
a different domain or where we want to work with a low resource language,
um, we're limited by labeled data, um,
but what we can do is pretty easily find unlabeled data.
Um, so it's actually a pretty solved problem, um,
maybe not 100%, but we can with good accuracy look at
some text and decide what language it's in and train a classifier to do that.
Um, so this means it's really easy to find
data in any language you care about because you can just go on
the web and essentially search for data in
that language and acquire a large corpus of monolingual data.
Okay, um, I'm now going into the first approach,
um, I'm going to talk about on using
unlabeled data to improve machine translation models.
Um, this technique is called pre-training and it's
really reminiscent of ideas like, um, ELMo.
Um, the idea is to pre-train by doing language modeling.
So if we have, um,
two languages we're interested in translating,
um, from one end to the other,
we'll collect large datasets for both of those languages and then we can train,
uh, two language models,
one each on that data and then, um,
we can use those, uh,
pre-trained language models as initialization for a machine translation system.
Um, so the encoder will get initialized with
the weights of the language model trained on the source side language, um,
the decoder will get initialized with weights trained on the target size language, uh,
and this will, um,
improve the performance of your model because during this pre-training, um,
we hope that our language models will be learning useful information such as, you know,
the meaning of words or, um, uh,
the kind of structure of the language, um,
they are processing, um, and this can, uh,
down the line help the machine translation model,
um, when we fine tune it.
Um, let me pause here and ask if there are any questions,
and just in general, feel,
feel free to ask questions throughout this talk. Okay.
So, so here is a plot showing some results of this pre-training technique.
Um, so this is English to German translation.
Uh, the x-axis is how much training data,
as in unsupervised training data, um,
you provide these models,
but of course they also have large amounts
of monolingual data for this pre-training step.
And you can see that this works pretty well, right?
So you've got about two blue points, um,
increase in performance, so that's this red line above the blue line,
um, when doing this pre-training technique.
And not too surprisingly,
this gain is especially large when the amount of labeled data is small.
Um, there is a problem with,
uh, pre-training which I want to address, which is that, uh,
in pre-training, you have
these two separate language models and there's never
really any interaction between the two,
um, when you're running them on the unlabeled corpus.
Um, so here's a simple technique, um,
that tries to solve this problem and it's called self-training.
Um, the idea is given a sentence from our monolingual corpus,
so in this case, "I traveled to Belgium," that's an English sentence.
Um, we won't have a human provided translation for this sentence, uh,
but what we can do is we can run our machine translation model,
and we'll get a translation in the target language.
Um, since this is from a machine learning model it won't be perfect, uh,
but we can hope that maybe our model can still learn from this kind
of noisy labeled example, right?
So we, we treat, um,
our original monolingual sentence and it's machine-provided
translation as though it were a human-provided translation and,
uh, train our machine learning model as normal on this example.
Um, I think this seems pretty strange actually as- as
a method when you first see it because it seems really circular, right?
So if you look at this, um, the, uh,
translation that the model is being trained to
produce is actually exactly what it already produces to begin with,
right, because, um, this translation came from our model in the first place.
Um, so actually in practice,
this is not a technique that's very widely used due to this problem,
um, but it motivates another technique called back-translation.
And this technique is really a very popular, um,
solution to that problem, and it's the method, um,
that has had a lot of success in using unlabeled data for translation.
So here's the approach rather than only
having our translation system that goes from source language to target language,
um, we're also going to train a model that
goes from our target language to our source language.
And so in this case, if,
if at the end of the day we want a French to English model, um,
we're gonna start by actually training an English to French model.
And then we can do something that's a lot like self-labeling.
So we take a English sentence.
We run our English to French model and translate it.
The difference to what we did before is that
we're actually going to switch the source and target side.
So now in this case the French sentence is the source sequence.
Uh, the target sequence is, um,
our original English sentence that came from monolingual corpora.
And now we're training the language, uh,
the machine translation system that goes
the other direction so that goes French to English.
Um, so, so why do we think this will work better?
Um, number one, um,
there's no longer this kind of circularity to the training
because what the model is being trained on is the output of a completely different model.
Um, another thing that I think is pretty crucial here is that,
um, the translations, the model is trained to produce.
So the things that the decoder is actually learning to
generate are never bad translations, right?
So if you look at this example,
the target sequence for our French to English model,
I traveled to Belgium, um,
that originally came from a monolingual corpus.
Um, so I think intuitively this makes sense is
that if we want to train a good translation model,
um, it's probably okay to expose it to noisy inputs.
So we expose it to the output of a system that's English to French,
it might not be perfect.
Um, but what we don't want to do is um, expose it to
poor target sequences because then it
won't learn how to generate in that language effectively.
Any questions on back-translation before I get to results? Um, sure.
[BACKGROUND]
So this is assuming we have a large corpus of
unlabeled data and we want to be using it to help our translation model.
Does that, does that make sense?
Um, maybe you could clarify the question.
[BACKGROUND]
Yeah, that's right. So we have a big corpus of English which includes the sentence,
"I traveled to Belgium," and we don't know the translations but we'd still like to
use this data. Yeah, another question.
[BACKGROUND]
Yeah, so that's a good question is how do you
avoid both the models let's say sort of blowing up and producing garbage?
And then they're just feeding garbage to each other.
The answer is that there is some amount of labeled data here as well.
So on unlabeled data you do this, but on labeled data,
you do standard training,
and that way you avoid, you,
you make sure you kind of keep the models on track because they still have to fit to
the labeled data. Yeah, another question.
How do you schedule the training of the two models?
Yeah, that is a good question.
And I think that's basically almost like a hyper-parameter you can tweak.
So I think a pretty common thing to do is first,
train two models only on labeled data.
Then label, um, so then do back-translation
over a large corpus and kind of repeat that process over and over again.
So each iteration, you train on the label data,
label some unlabeled data and now you have more data to work with.
But I think there'd be many kinds of scheduling that would be effective
here. Okay. Another question.
I'm curious as to the evaluation, considering if you have a very good French to English model, you could try to look up, or contest if you have a good French to English model, you could try to look up the original source and see if it matches.
Yeah, I'm not, I'm not quite sure.
Are you suggesting going like English to French to English and seeing if?
I see, yeah, yeah,
that's a really interesting idea.
And we're actually going to talk a little bit about this sort of,
it's called cycle consistency,
this idea later in this talk.
Okay, I'm going to move on to the results.
So, so here's the method for using unlabeled data to improve translation.
How well does it do?
Um, the answer is that the improvements are at least to me, they
were surprisingly extremely good, right?
So, um, this is for English to German translation.
This is from some work by Facebook, so they used 5 million labeled sentence pairs.
But they also used 230 monolingual sentences, so sentences without translations.
And you can see that compared to previous state of the art,
they get six BLEU points improvement which, um,
if you compare it to most previous research and machine tran- machine translation
is a really big gain, right?
So even something like the invention of the transformer which most people would
consider to be a really significant research development in NLP,
that improved over prior work by about 2.5 BLEU points.
And here without doing any sort of fancy model design just by using way more data,
um, we get actually much larger improvements.
Okay. So an interesting question to think about,
um, is suppose we only have our monolingual corpora.
So we don't have any sentences that had been human translated.
We just have sentences in two languages.
Um, so the scenario you can sort of imagine is suppose,
um, an alien comes down and,
um, starts talking to you and it's a
weird alien language, um, and it talks a lot,
would you eventually be able to translate what it's saying to English,
um, just by having a really large amount of data?
Um, so I'm going to start with, um,
a simpler task than full-on translating when you only have unlabeled sentences.
Um, instead of doing sentence to sentence translation,
let's start by only worrying about word to word translation.
So the goal here is given a word in one language,
find its translation but without using any labeled data.
Um, and the method,
the method we're going to use to try to solve
this task is called, uh, cross-lingual embeddings.
Um, so the goal is to learn, uh,
word vectors for words in both languages,
and we'd like those word vectors to have
all the nice properties you've already learned about word vectors having, um,
but we also want word vectors for a particular language,
um, to be close to the word vector of its translation.
Um, so I'm not sure if it's visible in this figure but this fis- figure shows
a large number of English and I think German words and you can see that,
um, uh, the each English word has its corresponding German word,
um, nearby to it in its embedding space.
So if we learn embeddings like this then it's pretty easy to do word to word translation.
Um, we just pick an English word,
we find the nearest, uh,
German word in this joint embedding space
and that will give us a translation for the English word.
Um, our key method for or the key
assumption that we're going to be using to solve this is that,
um, th- even though if you run word2vec twice you'll get really different embeddings.
Um, the structure of that embedding space has a lot of regularity to it,
and we can take advantage of that regularity, um,
to help find when,
um, an alignment between those embedding spaces.
So to be kind of more concrete here.
Here is a picture of two sets of word embeddings.
So in red, we have, um,
English words, in, uh,
blue we have Italian words,
and although, um, the vector spaces right now look very different to each other,
um, you can see that they have a really similar structure, right?
So you'd imagine distances are kind of similar that the distance from,
uh, cat and feline in the, um,
English embedding space should be pretty similar to the distance
between gatto and felino in the, um, Italian space.
Um, this kind of motivates an algorithm for learning these cross-lingual embeddings.
Um, so here's the idea.
What we're going to try to do is learn what's essentially
a rotation such that we can transform,
um, our set of English embeddings so
that they match up with our Italian embe- embeddings.
So mathematically, what this means is we're gonna learn
a matrix W such that if we take let's say,
uh, the word vector for cat in English and we multiply it by W, um,
we end up with the vector for gatto in Spanish or Italian,
um, and a detail here is that, um,
we're going to constrain W to be orthogonal, um,
and what that means geometrically is just that W is
only going to be doing a rotation to the,
uh, vectors, um, in X.
It's not going to be doing some other weirder transformation.
So this is our goal is to learn this W. Um,
next I'm gonna talk about,
talking about how actually do we learn this W. Um,
and there's actually a bunch of techniques for learning this W matrix,
um, but, um, here is one of
them that I think is quite clever is called adversarial training.
Um, so it works as follows,
is in addition to trying to learn this W matrix,
we're also going to be trying to learn a model that, uh,
is called a discriminator,
and what it'll do is take a vector and it will try to predict,
is that vector originally, um,
an English word embedding or is it originally an Italian word embedding?
Um, in other words, if you think about, um,
the diagram, what we're asking our discriminator to do is, uh,
it's given one of these points and it's trying to predict is it
basically a red point so an English word originally, or is it a blue point?
Um, so if we have no W matrix and this is
a really easy task for the discriminator because,
um, the, uh, word embeddings for English and Italian are clearly separated.
Um, however, if we learn a W matrix
that succeeds in aligning all these embeddings on top of each other,
then our discriminator will never do a good job, right.
We can imagine it'll never really do better than 50%,
um, because given a vector for say cat,
it won't know is that the vector for cat that's been
transformed by W or is it actually the vector for gatto?
Um, because in this case those two vectors are aligned so they are on top of each other.
Um, so, um, during training, you first, um,
you alternate between training the discriminator a little bit which
means making sure it's as good as possible at
distinguishing the English from Italian words and then you
train the W and the goal for training W is to,
uh, essentially confuse the discriminator as much as possible.
Um, so you want to have a situation where,
um, you can't, um, with this machine learning model,
figure out if a word embedding actually, um,
was, um, originally from English or if it's an Italian word vector.
Um, and so at the end of the day you have,
you have vectors that are kind of aligned with each other.
Um, any questions about this approach?
Okay. Um, he- there's a link to a paper with more details.
There's actually kind of a range of other tricks you can do,
um, but this is kind of a key idea.
Um, okay. So that was doing word to word unsupervised translation.
Um, how do we do full sentence to sentence translation?
Um, so we're going to use, um,
a standard sort of seq2seq model,
um, without even an attention mechanism.
Um, there's one change to the standard seq2seq
model going on here which is that, um,
we're going to use the same encoder and decoder,
uh, regardless of the input and output languages.
So you can see, um,
in this example, um,
we could give the encoder an English sentence,
we could also give it a French sentence and it'll have these cross-lingual embeddings.
So it'll have vector representations for English words
and French words which means it can handle sort of any input.
Um, for the decoder,
we need to give it some information about what language is it supposed to generate in.
Is it going to generate in French or English?
Um, so the way that is done is by, uh,
feeding in a special token which here is Fr
in brack- brackets to represent French that tells the model,
okay, you should generate in French now.
Um, here in this figure it's only French,
but you could imagine also feeding this model, uh,
English in brackets, and then that'll tell it to, uh, generate English.
And one thing that you can see is that you could use this sort of model to g enerate,
do go from English to French.
You could also use this model as an auto-encoder, right.
So, uh, at the bottom, um,
it's taking in a French sentence as input and it's just generating French as
output which here means just reproducing the original input sequence.
Um, so just a small change to standard seq2seq.
Here's how we're going to train the seq2seq model.
Um, there's going to be two training objectives, um,
and I'll explain sort of why they're, uh,
present in this model in just a few slides.
For now let's just say what they are.
So the first one is, um,
called a de-noising autoencoder.
Um, what we're going to train our model to do in this case is take a, uh, sentence.
So, um, and here it's going to be
an English sentence but it could also be a French sentence.
Um, we're going to scramble up the words a little bit,
and then we're going to ask the model to, uh,
de-noise that sentence which in other words means
regenerating what the sentence actually was before it was scrambled.
And, uh, maybe one idea of why this would be a useful training objective is that,
uh, since we have an encoder-decoder without atten- attention,
the encoder is converting the entirety of the source sentence into a single vector,
what an auto-encoder does is ensure that that vector contains all the information about
the sentence such that we are able to recover what the original sentence was,
um, from the vector produced by the encoder.
Um, so that was objective 1.
Training objective 2 is now we're actually going to be trying to do a translation,
um, but, um, as before,
we're going to be using this back-translation idea.
So remember, we only have unlabeled sentences,
we don't have any human-provided translations,
um, but what we can still do is, given, a,
um, let's say an English sentence or let's say a French sentence,
given a French sentence, we can translate it to English, um,
using our model in its current state, uh,
and then we can ask that model to translate from English or translate that- yeah,
translate that English back into French.
Um, so what you can imagine is in this setting, um,
the input sequence is going to be somewhat messed
up because it's the output of our imperfect machine learning model.
So here the input sequence is just "I am student," um, a word has been dropped,
but, um, we're now gonna train it to, even with this kind of bad input,
to reproduce the original, um,
French sentence, um, from our,
uh, corpus of- of monolingual, um, French text.
[NOISE] Um, let me- let me pause here actually and ask for questions.
Sure.
[NOISE] [inaudible] What if, um, the reason you have
this orthogonality constraint for your words to be word embedding,
is it to avoid overfitting?
Have you tried to take that off, and you know, see what [inaudible]
Yeah. That's a good question.
Um, so this is going back to earlier when there was a word-word translation.
Why would we constrain that W matrix to be orthogonal?
Um, essentially, that's right. It's to avoid overfitting and in particular,
it's making this assumption that our embedding spaces are so
similar that there's actually just a rotation that distinguishes,
um, our word vectors in English versus our word vectors in Italian.
Um, I think there has been, um,
there have been results that don't include that orthogonality constraint,
and I think it slightly hurts performance to not have that in there.
[NOISE] Okay.
Um, so- so continuing with,
um, unsupervised machine translation,
um, I- I gave a training method.
I didn't quite explain why it would work, so- so,
um, here is some more intuition for- for this idea.
Um, so remember, um,
we're going to initialize
our machine translation model with these cross-lingual embeddings,
which mean the English and French word should look close to identically.
Um, we're also using the shared, um, encoder.
Um, so that means if you think about it,
um, at the top, we have just,
a auto-encoding objective and we can certainly believe that our model can learn this.
Um, it's a pretty simple task.
Um, now imagine we're giving our model a French sentence as input instead.
Um, since the, uh,
embeddings are going to look pretty similar,
and since the encoder is the same, um,
it's pretty likely that the model's representation of
this French sentence should actually be very
similar to the representation of the English sentence.
Um, so when this representation is passed into the decoder, um,
we can hope that we'll get the same output as before.
Um, um, so here's like sort of as a starting point.
We- we can hope that our model, um,
already is able to have some translation capability.
[NOISE] Um, another way of thinking about this is
that what we really want our model to do is to be able to encode a sentence,
such that the representation,
um, is sort of a universal kind of Interlingua.
So a universal, um, uh,
universal representation of that sentence that doesn't,
uh, that's not specific to the language.
And so- so here's kind of a picture that's trying to get at this.
So our autoencoder, um, and our, um,
here in our back-translation example,
um, here, the target sequence is the same.
[NOISE] Um, so what that essentially means is
that the vectors for the English sentence and the French sentence,
um, are going to be trained to be the same, um, right?
Because if they are different, our, uh,
decoder would be generating different,
uh, outputs on these two examples.
Um, so here- this is just another sort of intuition is that what our model is
trying to learn here is kind of a way of
encoding the information of a sentence in a vector,
um, but in a way that is language-agnostic.
Um, any more questions about,
uh, unsupervised machine translation?
Okay. Um, so going on to results of this approach, um,
here, the horizontal lines are,
um, the results of an unsupervised machine translation model.
Um, the lines that go up are for a supervised machine translation model,
um, as we give it more and more data.
Right? So unsurprisingly, um,
given a large amount of supervised data, um,
the supervised machine translation models
work much better than the unsupervised machine translation model.
Um, but, um, the unsupervised machine translation model,
actually still does quite well.
Um, so if you see it around 10,000 to 100,000 training examples,
um, it actually does just as well or better than supervised translation,
and I think that's a really promising result,
uh, because if you think of, um,
low-resource settings where there isn't much labeled examples, um,
it suddenly becomes really nice that you can perform this well,
um, without even needing to use a training set.
Um, another thing kind of fun you can do with,
an unsupervised machine translation model is attribute transfer.
Um, so basically, you can, um, take, uh,
collections of texts that,
uh, split by any attribute you want.
So for example, you could go on Twitter,
look at hashtags to decide which tweets are annoyed and which tweets are relaxed,
and then you can treat those two corpora as
text as though they were two different languages,
and you can train an unsupervised machine translation model,
uh, to convert from one to the other.
Uh, and you can see these examples, um,
the model actually does a pretty good job of sort of minimally changing the sentence,
kind of preserving a lot of that sentence's original semantics,
um, such that the target attribute is changed.
Um, I also wanna throw a little bit of cold water on this idea.
So I do think it's really exciting and- and almost kind of
mind-blowing that you can do this translation without labeled data.
Um, certainly, right.
It's really hard to imagine someone giving me a bunch of books in Italian and say, "Okay.
We're in Italian," um, without, you know,
teaching you how to specifically do the translation.
Um, but, um, even though these methods show promise,
um, mostly they have shown promise on languages that are quite closely related.
So those previous results,
those were all, um,
some combination of English to French or English to German,
um, or so on, and those languages are quite similar.
[NOISE] Um, so if you look at, uh,
a different language pair, let's say English to Turkish,
where, um, the linguistics in those two languages are quite different, uh,
these methods do still work to some extent, um,
so they get around five BLEU points let's say, uh,
but they don't work nearly as well,
um, as they do in the f- uh, i- in the other settings, right?
So there's still a huge gap to purely supervised learning. Um, right?
So we're probably not, you know,
quite at this stage where an alien could come down and it's sort of, no problem,
let's use our unsupervised machine translation system, um,
but I still think that's pretty exciting progress. Um, yeah. Question?
Um, so what you're saying is that the genealogy of
a language might need it to superimpose worse, right?
Because my original thought was that if you took, for example,
like Latin, which doesn't have a word for, you know,
the modern classification of car, I thought that would do more poorly. But if- but, uh, basically,
what I'm asking is, do you think the English maps better to Latin
because they're both related, and worse to Turkish or is it the other way around?
Um, I would expect English to map quite a lot better to Latin.
And I think part of the issue here is that, um,
the difficulty in translation I think is not really at the word level.
So I mean that certainly is an issue that words exist
in one language that don't exist in another,
um, but I think actually,
more substantial differences between language is at the level of like syntax,
um, um, or you know, semantics, right?
How ideas are expressed.
Um, so- so I think I- I would expect Ital- Latin to have, you know,
relatively similar syntax to English,
um, compared to say Turkish,
I imagine that is probably the bigger obstacle
for unsupervised machine translation models.
Um, I'm going to really quickly go into
this last recent research paper which is basically taking BERT which,
which you've learned about, um, correct?
Yes. Okay. And making it cross-lingual.
Um, so, um, here's what regular BERT is, right?
We have a sequence of sentences in English.
We're going to mask out some of the words.
And we're going to ask BERT which is our transformer model, um,
to essentially fill in the blanks and predict what were the words that were dropped out.
Um, what actually has already been done by Google is training a multilingual BERT .
So what they did essentially is concatenate, um,
a whole bunch of corpora in different languages and then train one model um,
doing using this masked LM objective um,
on all of that text at once.
And that's a publicly released model.
Um, the, the new kind of extension to this that has recently been uh,
proposed by Facebook is to actually combine
this masked LM training objective um, with uh, translation.
So what they do is sometimes give this model a in this case,
a sequence in English and a sequence in uh, French.
Um, drop out some of the words and just as before,
ask the model to fill it in.
And the motivation here is that, um,
this will much better cause the model
to understand the relation between these two languages.
Because if you're trying to find a fill in a English word that's been dropped,
uh, the best way to do it if you have a translation is look
at the French side and try to find that word.
Hopefully, that one hasn't been dropped as well.
And then you can um, much more easily fill in the blank.
And uh, this actually leads to very uh,
substantial improvements in unsupervised machine translation.
So just like BERT is used for other tasks in NLP,
they basically take this cross-lingual BERT.
They use it as initialization for
a unsupervised machine translation system and they get, you know,
really large gains on the order of 10 BLEU points um,
such that the gap between
unsupervised machine translation and the current supervised state of the art,
um, is much smaller.
Uh, so this is a pretty recent idea but I think it also shows promise
in really improving the quality of translation through using unlabeled data.
Um, although I guess yeah, I guess in this case with BERT
they are using labeled translation data as well.
Any, any questions about this?
Okay. Um, so that is all I'm going to say about using unlabeled data for translation.
The next part of this talk is about um,
what happens if we really scale up these unsupervised language models.
Um, so in particular I'm gonna talk about GPT-2 which is a new model by OpenAI.
That's essentially a really giant language model
and I think it has some interesting implications.
So first of all, here's just the sizes of a bunch of different NLP models and,
um, you know, maybe a couple years ago the,
the standard sort of
LSTM medium-size model was on the order of about 10 million parameters.
Where 10- where a parameter is just a single weight let's say in the neural net um,
ELMo and uh, GPT.
So the original OpenAI paper before they did
this GPT-2 and we're about 10 times bigger than that.
Um, GPT-2 is about another order of magnitude bigger.
Um, one kind of interesting comparison point here is that uh,
GPT-2 which is 1.5 billion parameters,
actually has more parameters than a honey bee brain has synapses.
Um, so that sounds kind of impressive, right?
You know honeybees are not the smartest of
animals but they can still fly around and find nectar or whatever.
Um, but yeah. Of course, this isn't really an apples to apples comparison, right?
So a synapse and a weight in a neural net are really quite different.
But I just think it's one kind of interesting milestone
let's say in terms of model size um,
that has been surpassed.
[NOISE] Um, one thing to point out here is that um,
this increasing scaling of deep learning is really a general trend uh,
in all of machine learning so beyond NLP.
So this plot is showing time on the x-axis and the y-axis is log scaled um,
the amount of petaFLOPS used to train this model.
Um, so what this means is that the trend at least currently is that there is
exponential growth in how much compute power
we're throwing at our machine learning models.
I guess it is kind of unclear, you know,
will exponential growth continue but certainly um,
there's rapid growth in the size of our models.
And it's leading to some really amazing results, right?
So here are results not from language but for vision.
Um, this is a generative adversarial network
that's been trained on a lot of data and it's been trained on really large scales.
So it's a big model kind of in-between the size of ELMo and BERT let's say.
And uh, these photos here are actually productions of the model.
So those aren't real photos.
Those are things the model has just kind of hallucinated out of thin air.
And at least to me they look essentially photo-realistic.
There's also a website that um, is fun to look at it.
If you're not- if you're interested which is,
thispersondoesnotexist.com.
So if you go there, you'll see
a very convincing photo of a person but it's not a real photo.
It's again like a hallucinated image produced by a GAN.
We're also seeing really huge models being used for image recognition.
So this is recent work by Google where they trained
an image net model with half a billion parameters.
So that's bigger than BERT but not as big as GPT-2.
Um, this plot here is showing a
log scaled number of parameters on the x-axis and then accuracy at ImageNet
on the y-axis- axis and sort of unsurprisingly bigger models perform better.
And there seems to actually be a pretty consistent trend here which is uh,
accuracy is increasing with the log of the, the model size.
Um, I wanna go into a little bit more detail, how is it
possible that we can scale up models and train models at such a large extent.
One answer is just better hardware.
And in particular, um,
there's a growing uh,
number of companies that are developing hardware specifically for deep learning.
So these are even more kind of constrained and the
kind of operations they can do than a GPU,
um but they do those operations even faster.
So Google's Tensor Processing Units is one example.
There are actually a bunch of other companies working on this idea.
Um, the other way to scale up models is by taking advantage of
parallelism and there's two kinds of parallelism that I want to talk about very briefly.
So one is data parallelism.
In this case, each of your,
let's say GPUs, will have a copy of the model.
And what you essentially do is split
the mini-batch that you're training on across these different models.
So if you have, let's say,
16 GPUs and each of them see a batch size of 32.
You can aggregate the gradients of these 16 uh, uh,
if you do a back-prop on these 16 GPUs and you end up with effectively a batch size of 512.
So this allows you to train models much faster.
Um, the other kind of parallelism that's growing in importance is model par- parallelism.
Um, so eventually models get so big that they
can't even fit on a single GPU and they can't even do a batch size of one.
Um, in this case,
you actually need to split up the model across
multiple computers- multiple compute units.
Um, and that's what's done for models kind of the size of,
of let's say GPT-2.
There are new frameworks such as Mesh-TensorFlow, um,
which are basically designed to make this sort of model parallelism easier.
Um, okay. So onto GPT-2, um,
I know you already saw this a little bit in the contextualized uh,
um, embeddings um, lecture but I'm going to go into some more depth here.
[NOISE] So so essentially it's a really large transformer language model.
Um, so there's nothing really kind of novel here in terms
of new training algorithms or in terms of um,
the loss function or anything like that.
Um, the thing that makes it different from
prior work is that it's just really really big.
Uh, it's trained on a correspondingly huge amount of text.
So it's trained on 40 gigabytes and that's roughly 10 times larger than previous uh,
language models have been trained on.
Um, when you have that size of dataset,
um, the only way to get that much text is essentially to go to the web.
Um, so one thing OpenAI put a quite a bit of effort into when they're developing
this network was to ensure that that text was pretty high-quality.
Um, and they did that in a kind of interesting way.
They, they looked at Reddit which is this website where people uh,
can vote on links.
And then they said uh, if
a link has a lot of votes then it's probably sort of a decent link.
There's probably um, you know,
reasonable text there for a model to learn.
Um, okay, so if we have
this super huge language model like
GPT-2 on this question of what can you actually do with it,
um, well obviously if you have a language model you can do language modelling with it.
Uh, but one thing kind of interestingly interesting is that you
can run this language model on er,
existing benchmarks, um, for,
for language modelling, um,
and it gets state of the art perplexity on these benchmarks even
though it never sees the training data for these benchmarks, right?
So normally, if you want to say evaluate your language model on the Penn Treebank.
You first train on the Penn Treebank and then you evaluate on this held-out set.
Uh, in this case, uh,
a GPT-2 just by virtue of having seen so much text and being such a large model,
outperforms all these other uh,
prior works even though it's not seeing that data.
Um, on a bunch of different uh, language modelling benchmarks.
Um, but there's a bunch of other interesting experiments that OpenAI
ran with this language modeling and these were based on zero-shot learning.
So zero-shot learning just means trying to do a task without ever training on it.
And, uh, the way you can do this with a language model
is by designing a prompt you feed into
the language model and then have it just generate from there and
hopefully it generates something relevant to the task you're trying to solve.
So for example, for reading comprehension,
what you can do is take the context paragraph,
uh, concatenate the question to it and then add uh,
a colon which is a way,
I guess, of telling the model,
''Okay you should be producing an answer to this question,''
and then just have it generate text, um,
and perhaps it'll generate something that is actually answering,
um, the question and is,
is paying attention to the context.
[NOISE] Um, and similarly, for summarization,
you can get the article then TL;DR and perhaps the model will produce the summary.
Um, you can even do translation,
where you give the model,
um, some ex- a list of known English to French translations so you, sort of,
prime it to tell it that it should be doing translation and then you give
it the source sequence equals blank and have it just run and,
um, perhaps it'll generate,
um, the sequence in the target language.
Um, okay. So so here's what the results look like.
Um, for all of these,
uh, the X-axis is,
is log scaled model size and the Y-axis is accuracy, um,
and the dotted lines basically correspond to,
um, existing works on these tasks.
Um, so for most of these tasks, um,
GPT-2 is quite a bit below existing systems,
um, but there's of course this big difference, right?
Existing systems are trained specifically to do,
um, whatever task they're being evaluated on,
where GPT-2 is um,
only trained to do language modeling and as it learns language modeling,
it's sort of picking up on these other tasks.
Um, so right. So for example, um,
it does, uh, English to French machine translation, um,
not as well as, uh,
standard unsupervised machine translation which is those, uh,
dotted lines, um, but it still,
it still does quite well.
And, um, one thing, kind of,
interesting is the trend line, right,
for almost all of these tasks.
Um, performance is getting uh,
much better as the model increases in size.
[NOISE] Um, I think a particularly interesting,
uh, one of these tasks is machine translation, right?
So the question is, how can it be doing
machine translation when all we're giving it as a bunch of
web pages and those web pages are almost all in
English and yet somehow it sort of magically picks up uh,
a little bit of machine translation, right.
So it's not a great model but it can still,
um, you know, do a decent job in some cases.
Um, and the answer is that,
if you look at this giant corpus of English,
occasionally, uh, within, within that corpus,
you see examples of translations, right?
So you see, um,
a French idiom and its translation or
a quote from someone who's French and then the translation in English.
And, um, kind of,
amazingly I think this big model, um,
sees enough of these examples that it actually starts to learn how to generate French,
um, even though that wasn't really,
sort of, an intended part of its training.
Um, another interesting, um,
thing to dig a bit more into is its ability to do question answering.
So uh, a simple baseline for question answering gets about 1% accuracy,
GPT-2 barely does better at 4% accuracy.
So this isn't, like, you know,
super amazingly solved question answering, um, but, um,
it's still pretty interesting in that,
if you look at answers the model's most confident about,
you can see that it sort of
has learned some facts about the world, right.
So it's learned that Charles Darwin wrote Origin of Species.
Um, normally in the history of NLP, if you want to get, kind of,
world knowledge into an NLP system,
you'd need something like a big database of facts.
And even though this is still,
kind of, very early stages and that, um,
there's still a huge gap between 4% accuracy and the, uh, you know,
70% or so that, uh,
state of the art open domain question answering systems can do,
um, it, it, um,
it still can, uh,
pick up some world knowledge just by reading a lot of text, um, without,
kind of, explicitly having that knowledge put into the model.
Um, any questions by the way on GPT-2 so far?
Okay. So one question that's interesting to think about is,
what happens if our models get even bigger?
Um, so here I've done the, um,
very scientific thing of drawing some lines in PowerPoint and seeing where they meet up.
Um, and you can see that, um,
if the trend holds at about 1 trillion parameters,
um, we get to human level reading comprehension performance.
Um, so if that's true it would be really astonishing.
I actually do expect that a 1 trillion parameter model would be attainable in,
I don't know, ten years or so,
um, but of course,
right, the trend isn't clear.
So if you look at summarization for example,
it seems like performance is already,
uh, uh, topped out.
Um, so I think this will be a really interesting thing kinda going forward,
looking at the future of NLP, um,
is how the scaling will change,
um, the way NLP is approached.
Um, the other interesting thing about GPT-2 was its reaction from uh,
the media and also from other researchers.
Um, and the real cause of
a lot of the controversy about it was this statement from OpenAI.
They said that, ''We're not going to release our full language model,
um, because it's too dangerous,
you know, our language model is too good.''
Um, so the media really enjoyed this and,
you know, said that,
uh, machine learning is going to break the Internet.
Um, there's also some pretty interesting reactions from our researchers, right.
So um, there's some,
kind of, tongue-in-cheek responses here, right.
You know, I trained the model on MNIST.
Is it too dangerous for me to release it?
Um, and similarly, we've done really great work
but we can't release it it's too dangerous so you're just gonna have to trust us on this.
Looking at more, kind of, reasoned, um,
debate about this issue,
you still see articles,
um, arguing both sides.
So these are two ar- articles,
um, from The Gradient which is a, sort of,
machine learning newsletter, um,
and they're arguing precisely opposite sides of this issue,
um, should it be released or not.
So I guess I can briefly go over a few arguments for or against.
There is, kind of, a lot of debate about this and I don't want to
go too deep into a controversial issue,
um, but here's a long list of,
kind of, things people have said about this, right.
So um, here's why you should release.
One complaint is that,
is this model really that special?
There's nothing new going on here.
It's just 10 times bigger than previous models, um,
and there's also some arguments that,
um, even if this one isn't released, you know,
in five years everybody can train a model this good, um,
and actually if you look at image recognition or look at images and speech data, um,
it already is possible to synthesize highly convincing,
um, fake images and fake speech.
So kinda, what makes this thing different from those other, um, systems.
And speaking of other systems, right,
Photoshop has existed for a long time,
so we can already convincingly fake images, um,
people have just learned to adjust and learned
that you shouldn't always trust what's in an image,
um, because it may have been,
um, altered in some way.
Um, on the other hand, you could say,
''Okay, uh, Photoshop exists but, um, you can't, sort of,
scale up Photoshop and start mass producing fake content the way you can with this sort
of model,'' and they pointed at the danger of uh, fake news, um,
fake reviews, um, in general just astroturfing, which means basically,
uh, creating fake user content that's supporting a view you want other people to hold.
Um, this is actually something that's already done,
um, pretty widely by country- companies and governments.
There's a lot of evidence for this, um,
but they are of course hiring people to
write all these comments on news articles let's say
and we don't want to make their job any easier
by producing a machine that could potentially do this.
So um, I'm not really gonna take a side here,
um, there's still a lot of debate about this.
I think, you know,
the main, the main takeaway here is that,
as a community on people in machine learning and NLP,
don't really have a handle on this, right?
We are sort of caught by surprise by, um,
OpenAI's, um, decision here and, um, uh,
that means that, you know,
there really is some figuring out that needs to be done on what
exactly is responsible to release publicly.
What kind of research problems should we be working on and so on.
[NOISE] So yeah.
Any questions about uh, this,
this reaction or this debate in general?
[NOISE] Okay.
Um, I think something arising from this debate is, um,
the question of, um,
should really the ML people be the people making these, sort of,
decisions or is there a need for more interdisciplinary science where we look at, um,
experts in say, computer security,
um, people from social sciences,
um, you know, people who are experts in ethics,
um, to look at these decisions.
Um, right. So GPT-2 was definitely one example of where suddenly it seems like,
um, our NLP technology has a lot of pitfalls, right.
Where they could be used in a malicious way or they could cause damage.
And I think this trend is only going to increase, um,
if you look at, kind of,
areas of NLP that people are working on, uh,
increasingly people are working on really high stakes applications of NLP,
um, and those often have really big, um,
ramifications, especially if you think from the angle of bias and fairness.
Um, so, so let's go over a couple examples of this, um-
Um, one- so some, some areas where,
where this is happening is people are looking at,
uh, NLP to look at judicial decisions.
So for example, should this person,
uh, get bail or not?
Um, for hiring decisions, right?
So you look at someone's resume,
you run NLP on it,
and then you'd make a decision automatically,
um, sh- should we throw out this resume or not?
So do some, sort of, screening, um, grading tests.
Um, if you take the GRE, um,
your, your tests will be graded by a machine.
Um, a person will also look at it, um,
but nevertheless, um, that's, you know,
a sometimes very impactful part of your life, um, when it's,
when it's the tests that, um, inf- you know,
affects your, um, acceptance into a school, let's say.
Um, so I think there is- are some,
some good sides of using Machine Learning in these kinds of contexts.
So one is that we can pretty quickly evaluate,
a machine learning system and search out.
Does it have some, kind of, bias,
just by running it on a bunch of data and seeing what it does,
and also perhaps even more importantly,
um, we can fix this, kind of,
problem if it arises, right?
So, um, it's probably easier to fix a machine learning system that screens resumes,
than it is to s- to fix having, you know,
5,000 executives that are slightly sexist or something, right?
So, so in this way,
um, there is a, sort of,
positive angle on using machine learning in these high-stakes, um, uh, decisions.
Um, on the other hand, um,
it's been pretty well, uh, s- known,
and I know you had a lecture on bias and fairness,
that machine learning often reflects bias in a data-set,
um, it can even amplify bias in the data-set.
Um, and there's concern of, kind of,
a feedback loop where a biased algorithm
actually will lead to the creation of more biased data,
um, in which case these problems will only compound and get worse.
Um, so for all of the, uh, high-impact decisions,
um, I, I had listed on that slide,
there are examples where things have gone awry, right?
So Amazon had some AI that was,
um, working as a recruiting tool and it turned out to be sexist.
Um, um, there have been some, kind of,
early pilots of using AI, um,
in the justice system and those also have had,
um, in some cases, really bad results.
Um, if you look at automatic,
automatic essay grading, um,
it's not really a great,
you know, NLP system, right?
So here's an example, um,
excerpt of an essay that, um,
a automatic grading system used by the GRE test gives, uh,
a very high score, um,
but really it's just, kind of, a solid of,
uh, big fancy words and that's
enough to convince the model that this is a, a great essay.
Um, the last, um,
area I wanna talk about where, where, um,
you can see there's really some risks and
some pitfalls with using NLP technology, is chatbots.
Um, so I think chatbots do have a side where they can be very beneficial.
Um, Woebot is one example,
is this company that has this chatbot you can talk to if you're not,
um, feeling too great and it'll try to,
um, I don't know, cheer you up.
Um, so, so that, you know,
could be a- a really nice piece of technology that helps people,
um, but on the other hand, there's some big risks.
So, so one example is Microsoft research had a chatbot trained on tweets,
and it started quickly saying racist things and had to be pulled.
Um, so I think all of this highlights that, um,
as NLP is becoming more effective,
people are seeing opportunities to use it in, um,
increasingly high-stakes decisions and although,
you know, there are some nice- there's some appeal to that,
um, there's also a lot of risk.
Um, any more questions on, uh,
this sort of social impact of NLP?
Okay. Um, last part of this lecture is looking more at future research, right?
And in particular, um,
I think a lot of the current research trends are,
kind of reactions to BERT, um, right?
So, so the question is what did BERT solve and- and what do we work on next?
Um, so here are results on the GLUE benchmark.
Um, that is, uh, a compendium of,
uh, 10 natural language understanding tasks.
Um, and you get an average score across those 10 tasks.
Um, the left, uh, two- the two are,
sorry the right- two right most models are,
um, uh, s- non, uh,
are just supervised trained machine learning systems, right?
So we have Bag-of-Vectors, um,
we instead use our fancy neural net architecture
of BiLSTM + Attention and we get about five points.
Um, but the gains from BERT,
uh, really dwarf that difference, right?
So, so BERT improves results by about, uh,
17 points and we end up being actually quite close,
um, to human performance on these tasks.
Um, so one, sort of,
implication of this that people are wondering about is,
is this, kind of, the death of architecture engineering?
Um, so I'm sure all of you who have worked on the default final project, um,
have seen a whole bunch of fancy pictures showing different,
uh, architectures for solving SQuAD.
Um, there are a lot of papers.
They all propose some, kind of,
uh, attention mechanism or something like that.
Um, and, um, right.
With BERT, it's, sort of,
um, you don't need to do any of that, right?
You just train a transformer and you give it enough data,
and actually you're doing great on SQuAD,
you know, maybe, um, these, uh,
architectural enhancements are not necessarily, um,
the key thing that'll drive progress in,
uh, improving results on these tasks.
Um, right. So, uh,
if you look at this with the perspective of a researcher,
you can think a researcher will say, "Okay,
I can spend six months designing a fancy new architecture for
SQuAD and if I do a good job maybe I'll improve results by 1, uh, F1 point."
Um, but in the case of BERT, um,
increasing the size of their model of 3x,
which is the difference between,
they've like a base size model and a large model,
um, that improve results by 5 F1 points.
Um, so it does seem to suggest we need to, sort of,
re-prioritize, um, which avenues of research we'd pursue,
because this architecture engineering isn't providing, kind of,
gains for its time investment the way,
uh, leveraging unlabeled data is.
Um, so now, if you look at the SQuAD leaderboard, um,
I think at least the top 20 entrants are all BERT plus something.
Um, one other issue, uh,
I think BERT has raised is that,
um, we need harder tasks, right?
BERT has almost solved SQuAD,
if you define it by, uh,
getting close to human performance.
Um, so there's been, um,
a growth in new datasets that are, uh,
more challenging and there are a couple of ways in which,
um, they can be more challenging.
So one is, um,
doing reading comprehension on longer documents,
or doing it across more than one document.
Um, one area is looking at c- uh,
coming up with harder questions that require a multi-hop reasoning.
Um, so that essentially meas- means you have to string
together multiple supporting facts from different places,
um, to produce the correct answer.
Um, and another area,
situating question-answering within a dialogue.
Um, there's also been a, kind of,
small detail with the construction of reading comprehension datasets,
that has actually really affected,
um, the, the difficulty of the task.
And that is whether, um,
when you create these datasets, um,
is the person who writes questions about a passage,
can they see that passage or not?
Um, so of course, it's much easier to come up
with a question that when you see the passage,
and if you come up with a question without seeing the passage,
you may not even have a answerable question.
Um, but the problem with looking at
the passage is that first of all it's not realistic, right?
So, uh, if I'm asking a question, you know,
I'm not going to have usually
the paragraph that answers that question sitting in front of me.
Um, on top of that,
it really encourages easy questions, right?
So, um, if you're a Mechanical Turker,
and you're paid to write as many questions as possible,
and then you see an article that says,
um, I don't know, you know,
uh, Abraham Lincoln was the 16th president of the United States,
um, what are you gonna write?
As your question, you're gonna write,
who was the 16th president of the United States.
You're not gonna write something more interesting that's harder to answer.
Um, so- so this is one way in which crowdsourced datasets have changed, um,
people are now making sure questions are,
sort of, independent of, of the contexts.
Um, so I'm gonna briefly, uh,
go over a couple of new datasets in this line.
So one is called QuAC, which stands for Question Answering in Context.
Um, in this dataset,
there is a teacher and a student,
um, the teacher sees a Wikipedia article.
The student wants to learn about this Wikipedia article,
and the goal is to train a machine learning model that acts as the teacher.
Um, so you can imagine maybe in the future, this,
sort of, technology would be useful for,
uh, um, education for, kind of,
having, uh, adding some automation.
Um, uh, one thing that makes this task difficult is that,
uh, questions depend on the entire history of the conversation.
Um, so for example, uh,
if you look, um, on the left here, uh,
the example, um, dialogue,
um, the third question is was he the star?
Um, clearly you can't answer that question unless you look back earlier in the dialogue,
and realize that the subject of this,
uh, conversation is Daffy Duck.
Um, a- and, sort of,
because this dataset is more challenging,
and you can see there's a, there's a much bigger gap to human performance, right?
So if you train some BERT with some extensions, you'll st- uh,
the results are still like 15 F1 points worse than human performance.
Um, um, here's one other dataset, um, called HotPotQA.
Um, it is, uh,
designed instead for multi-hop reasoning.
Um, so essentially, in order to answer a question,
you have to look at multiple documents,
you have to look at different facts from those documents,
and perform some inference,
um, to get what the correct answer is.
Um, so I think, you know, this is a- a much harder task.
And again, um, there's a much bigger gap between human performance.
Um, any questions on, uh,
new datasets, um, harder chi- tasks for NLP?
Okay. Um, I'm gonna,
kind of, rapid fire and go through, um,
a couple of more areas in the last minutes of this talk.
Um, so multitask learning I think is really growing in importance.
Um, of course, um,
you've had a whole lecture on this, right?
So I'm not gonna spend too much time on it.
Um, but maybe one, uh,
point of interest is that if you look at performance on this GLUE benchmark,
so this benchmark for natural language understanding,
um, all the top couple results, um,
are- that are now actually surpassing BERT in
performance are- is taking BERT and training it in a multi-task way.
Um, I think another interesting, uh,
motivation for multi-task learning is that if you are training BERT, you have a really,
really large model and one way to make
more efficient use of that model is training it to do many things at once.
Another area that's definitely important, um,
and I think will be important going in the future is dealing with low-resource settings.
Um, and here I'm using a really broad,
uh, definition of resources, right.
So that could mean compute power, um, you know,
BERT is great but it also takes huge amounts of compute to run it.
So it's not realistic to say,
um, if you're building, let's say a mobile, uh,
an app for a mobile device that you could run a model the size of BERT.
Um, as I already ga- went into earlier in this talk, um, you know,
low-resource languages is an area that I think is pretty, um,
under-represented in NLP research right now,
because most datasets are in English, um,
but I do think, right,
there's a really, you know,
large number of people that in order to benefit from NLP technology, um,
we'll need to have technologies that work well in a lot of
different languages especially those without much training data.
And, um, speaking of low- low amounts of training data, I think in general this is,
uh, a- an interesting area of research,
um, within machine learning.
Actually, people are, um,
working a lot on this as well.
Um, so a term is often, uh,
a term often used is few shot learning.
Um, and that essentially means being able to
train a machine learning model that only sees,
let's say five or ten examples.
Um, one motivation there is, um,
I think a clear distinction between how our existing machine learning systems learn,
and how humans learn is that, um,
humans can generalize very quickly from five or so examples.
Um, if you're training a neural net,
you normally need, you know,
thousands of examples or perhaps even tens of thousands,
hundreds of thousands of examples to get something that works.
Um, so I also see this being a pretty important area in the future.
Um, the last area where I want to go in, um,
a little bit more depth is interpreting and understanding models.
Um, so, so really there's two aspects of this.
One is if I have a machine learning model and it makes a prediction,
I would like to be able to, uh,
know why did it make that prediction?
So gets some rationale, get some explanation,
um, that would especially be important in an area like health care, right?
So if you're a doctor and you're making a decision, um,
it's probably not good enough for your machine learning model to say,
"Patient has disease X."
You really want it to say,
"Patient has disease X for these reasons."
Um, because then you as a doctor can double-check,
and, and try to validate the, the,
uh, machine's, um, thinking I guess,
um, to come up with that diagnosis.
Um, the other area of interpreting
understanding models is more of a scientific question, right?
Is we know things like BERT work really well,
um, we want to know why do they work well?
What -what what aspects of language do they model?
Um, what things don't they model?
Um, and that might lead to, um,
ideas of improving, um, those- those models.
Um, so, um, here is a, uh,
couple slides on the main approach for evalu- answering the sort of scientific questions.
What does a machine-learning model learn?
Um, what you do is you have a model so let's say it's BERT.
It takes as input a sequence of words, um,
it produces as output a sequence of vectors, um,
we want to ask does it know for example,
the part of speech of words?
So, so it does in its vector representations,
does that capture something about syntax?
Um, and a simple way of asking this question is train another classifier on top of BERT,
uh, that's trained to do,
um, let's say part-of-speech tagging.
Um, but we only, um,
backprop into that diagnostic classifier itself.
So in other words we're treating the output of BERT, um,
that sequence of vectors as a fixed input,
and we're sort of probing those vectors to see,
um, do they contain, um,
information about a part of speech that
this second diagnostic classifier on top can decode,
um, to get the correct labels?
Um, so, um, it was kind of quite a few concerns here.
Um, one concern is, uh,
if you make your diagnostic classifier too complicated,
it can just solve the classif- the task all on itself,
and it can basically ignore, uh,
whatever representations were produced by BERT.
Um, so- so the kind of standard thing right now is to use
a single softmax layer on top of BERT,
um, to do these decisions.
Um, and there's been a whole bunch of tasks proposed for
evaluating essentially the linguistic knowledge of these models.
Um, so you could do part-of-speech tagging,
you could do more semantic tasks like,
uh, relation extraction, um,
or- or something like co-reference.
Um, and this is a pretty active area of work.
Um, here is, uh, just one, uh,
plot showing some of the results, um, of this approach.
So here what we're doing is we're adding
diagnostic classifiers to different layers of BERT,
and we are seeing which layers of BERT are more useful for particular tasks.
Um, and, um, something kind of interesting comes out of this which is that, um,
the different layers of BERT seem to be corresponding, um,
fairly well with notions of,
uh, different layers of li- of linguistics.
Um, so, uh, dependency parsing which is a syntactic task,
um, it's, uh, considered sort of a, you know,
medium level task in understanding a sentence.
Um, the medium layers of BERT, so layers kind of 6 through 8 or something,
are the ones best at dependency parsing.
Um, if you have a se- very semantic task like sentiment analysis,
um, where you're trying to learn some kind of, uh,
semantic property of the whole sentence, um,
then the very last layers of BERT are the ones that seem
to encode the most information about- about this, uh, phenomenon.
Um, okay.
So this is almost it for the talk, um,
I just have one slide here of, uh, um,
NLP not in kind of the academic researching context,
which I have already been talking a lot about but NLP in industry,
and really there's rapid progress there.
And I wanted to point to you two areas where I think there's
especially a large interest in using NLP technology.
Um, one is dialogue,
um, so for things like chatbots, right?
There's the Alexa Prize where they're actually investing a lot of money in,
um, having groups figure out how to improve chitchat dialogue.
Um, there's also I think a lot of potential for customer service, right?
So improving basically automated systems that'll, um,
you know, book you a flight,
or help you cancel a subscription, or anything like that.
Um, and similarly, there's a lot of potential in health care.
Um, one is understanding the records of someone who,
um, is sick and to help them- to help with diagnoses.
Um, I think another, um,
equally important area is actually, uh,
parsing, uh, biomedical papers.
Um, so, um, the number of biomedical papers that are being written is really insane,
um, it's, it's way larger than the number
of computer science papers that are being written.
[NOISE] Um, often if you're a doctor,
or if you're a researcher, um,
in medicine, you might want to look up something very specific, right?
You might want to know what is
the effect of this particular drug on this particular gene,
or a cell with this particular gene.
Um, there's no good way right now of searching through, um,
hundreds of thousands of papers to find if someone has a- has, uh,
done this experiment and have results for this,
um, particular combination of things.
Um, so automated reading of all this biomedical literature,
um, could have a lot of value.
Okay, um, to conclude, um,
there's been rapid progress in the last five years due to deep learning, um, in NLP.
Um, in the last year, we've seen another really kind of, uh,
a dramatic increase in the capability of our systems,
thanks to, uh, using unlabeled data.
So that's methods like BERT.
Um, and, um, the other kind of thing that's I think important to think about is that,
NLP systems are starting to be at a place where they can have big social impact.
Um, so that makes some issues like bias and security very important. Um, thank you.
Uh, good luck finishing all your projects.
[APPLAUSE]
