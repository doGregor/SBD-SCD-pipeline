 Let's get started. Welcome to CS224U.
Our topic is Natural Language Understanding.
And if you can understand the words that I am saying right now,
you are already pretty good at natural language understanding.
Uh, this might even be an easy class for you.
But you probably have a device in
your pocket that's not very good at natural language understanding.
And that's the problem that we want to tackle in this class.
Uh, but first let me introduce myself.
My name is Bill MacCartney.
I'm an Adjunct Professor in the Computer Science Department.
Adjunct Professor basically means part-time professor.
I've been teaching this class for eight years now,
but my full-time work is in industry.
Um, currently, I'm in Apple where I help to
lead part of the AI and Machine Learning organization.
I was previously a research scientist at Google,
and my focus at Google was
natural language understanding for search and for the Google Assistant.
Uh, let me also introduce my co-instructor Chris Potts. You wanna say anything?
Chelsea will probably get me later. So [OVERLAPPING].
Okay
Lots of familiar faces it's great to see.
So Chris and I are gonna share the teaching responsibilities, uh, today.
You'll mainly hear from me and on Wednesday,
Chris will take the stage.
So the first question I wanna look at is why study natural language understanding?
What are our goals in studying natural language understanding?
And a few possibilities come to mind.
One is that it may yield insights into human cognition.
Uh, human language is phenomenally complex
and understanding natural language can be surprisingly difficult.
If we can figure out how to enable machines to understand language,
it might help us to understand how humans understand language.
So this amounts to building a model of a phenomenon in order to understand it better.
Uh, another goal might be to build conversational agents that can provide assistance or,
uh, entertainment or companionship.
So for example, you might wanna build
a natural language interface to an airline reservation system.
Or you might wanna build a toy that can have a conversation with a child.
Or you might wanna build a virtual companion like in the movie Her.
Uh, another possible goal is that solving
NLU would mean solving a major subproblem of Artificial Intelligence.
Uh, after all, you can't really claim that a system has
achieved human level intelligence if it can't understand language.
So NLU is a necessary step on a path to strong AI.
So those are a few possible motivations.
I'm curious to hear if you guys have other ideas.
Are there other motivations that have brought you to this class?
Why are you interested in studying NLU?
[NOISE] Yep.
I'm mainly interested in using it to help with historical research.
So applying it to bodies of
[inaudible].
That's terrific, there's been an explosion in
the last few years of computational social science.
So using computational methods to gain
better insights into topics that are traditionally not part of computer science,
but part of social science.
So it's a scientific goal,
but it's not aimed at understanding how humans understand language.
It's using NLU as a tool to study something else.
Any other ideas? Yep.
I'm here because I feel like, uh,
NLP is not satisfying enough. [NOISE] I find like machine translation understands is like, it like, like the computer doesn't understand language. It's just like, even with Internet work. So I
was just wondering how
we could push that further and see if we use [inaudible] like make it better.
Yeah. Many of the, many of the goals that NLP has
aimed at over the last couple of decades are comparatively modest goals.
So not aimed at actually understanding language in its fullness,
but instead aimed at lower level tasks like, um, tagging tasks,
or syntactic analysis, or even translation
which is a comparatively harder and higher-level task,
but still falls short of full understanding [NOISE]. Any other thoughts before I go on? Yeah.
Helping older people with technology through natural language understanding.
Yeah. Can you say a little more about that?
Uh, actually as people get older their memory fades,
they start to hit other issues. [NOISE] Talking becomes harder, so having some form of companion.
Someone who can understand how to communicate with you,
to help better your condition, something like that.
Yeah. Um, so so maybe a combination of assistance and- and companionship.
I was just going to say you might need to repeat the questions, so it can pick it up.
Okay.
[inaudible].
Yeah. So the so that comment was about, uh,
providing assistance and companionship, companionship to older people,
uh, to help them with their lives and and,
uh, help them not feel so alone.
Great. Okay. Let me keep going.
Um, think I skipped a slide.
Oh, uh, I was gonna make an observation first which is that, um,
some of the goals we've talked about are primarily scientific goals,
and others are primarily technological goals.
So maybe Number 1 is a scientific goal.
Number 2, I would call it a technological goal, trying to create a new piece of technology.
Number 3, may be ambiguous between the two.
But this split between, uh,
scientific goals and technological goals is
one that's been a part of NLU for a long time.
And in fact you see it in this quote from James Allen,
who's the author of one of the earliest texts on natural language understanding.
He says, "There can be two underlying motivations for building a computational theory.
The technological goal is simply to build better computers
and any solution that works would be acceptable.
The cognitive goal is to build
a computational analog of the human-language-processing system.
Such a theory will be acceptable only after it had been verified by experiment."
So the question is when we build a natural language understanding system,
do we care whether it understands language in the same way as a human,
that is using similar techniques?
If we're cognitive scientists,
then of course we do care.
Uh, if we're technologists then we probably don't.
We asked you to read this short paper by Hector Levesque for today,
and he makes the same distinction between scientific goals and technological goals.
He points out that in some cases,
you can achieve the technological goals of
Artificial Intelligence through what he calls cheap tricks.
And that pejorative term makes it clear that what he
cares about are the scientific goals, not the technological goals.
Uh, in this course,
we're gonna approach the topic of natural language understanding
primarily from the technologist's perspective.
But I think it's useful to keep this other perspective in mind.
Another important question for us is,
what counts as understanding?
If we're building a natural language understanding system,
how do we know whether we've succeeded?
How do we know whether our system truly understands?
Um, over the years,
many different criteria have been proposed.
One possibility is to say that I understand
a statement if I can specify its truth conditions.
So if I'm gonna claim to understand a statement like, uh,
every amino acid contains nitrogen,
then I need to be able to state what has to be true in order for that to be so.
So this seems like a plausible criterion for understanding,
any reservations about that?
Any- anything wrong with that as a criterion for understanding? Yes.
[inaudible].
Yeah there's opinions.
There's lots of other kinds of statements that don't really have truth conditions too.
Like if I say a great example of an opinion,
if I say like, hey did you see Saturday Night Live this weekend?
It was the funniest thing I'd ever seen.
Like how would I- that's meaningful.
You understand it.
You understand my meaning but how would I
specify the truth conditions for- that's the funniest thing I ever saw.
It's hard to imagine. Any other thoughts about this as a criterion of understanding? Yes?
This might be going off the course but like you may argue that, um, an
AI would never understand
certain emotional or human condition statements such as nurtured.
Yeah. If I say,
ta- I take a statement like,
um, love is the most important thing in life.
What are the truth conditions for that?
I wouldn't have the foggiest idea how to specify the truth conditions for that.
And yet, it's perfectly meaningful.
And we can talk about understanding what that means.
Another possible criterion is to say
that we understand a statement if we can calculate its entailments.
This is closely related to the first one,
but we're shifting our attention from the conditions
of the statement to its consequences.
Later in the quarter, we're gonna look at the task of
natural language inference which is one way of operationalizing that criterion.
Another possibility is to say that,
to understand a statement is to respond appropriately.
To take appropriate action based on the statement. So this might be relevant.
For example, if you're playing, um,
if you're playing a text-based adventure game like, uh,
the kinda game where you wanna be able to type commands like go North,
or take gold, or stab dwarf repeatedly.
Or you know, if you're talking to your iPhone and you just wanna
be able to say set an alarm for 9:00 PM.
Does it do the right thing?
Does it take the right action?
Another possibility- another possible criterion for understanding is, uh,
to understand it, to be able to translate into another language.
This seems like a high bar for understanding.
Maybe not, um, maybe not a necessary condition,
but perhaps a sufficient condition.
What do you guys think?
If you're building a natural language understanding system,
how will you evaluate whether it understands?
How will you- how will you judge whether it actually works?
This question of evaluation is gonna be one that we're gonna
look at a lot more later this quarter.
Other thoughts here? Yeah?
This isn't necessarily a task for like a
computational system but from a human perspective,
I think an important way to understand this statement is to
empathize with the person making the statement based on what the statement means.
Yes. So your suggestion is that, uh,
an important test of- of understanding is
the ability to emp- empathize with the statement.
Um, I don't know how we would operationalize that
as a- as a measure of computer understanding of a system.
But I think it's a good insight. Yeah?
I want to follow on that. I agree.
Because I think that's, uh,
one of the biggest problems for like mental health chatbots.
Yeah.
Where is the lack of, uh,
be able to empathize or respond appropriately,
um, based on like nuanced responses, depending who's there.
Yeah.
And that's like a big- that's a big problem in those.
Yeah. So it seems closely related to take
appropriate action in light of it or respond appropriately.
Um, if you had a chat bot that's trying to, um, uh,
maybe almost play the role of therapist,
you wanna make sure that your chat bot is,
um, responding in a way that shows real understanding and real empathy.
Uh, and right now, that's a- that's a pretty high bar.
A challenge that- that seems pretty- pretty,
um, far out of reach at this point. Yes?
Um, maybe a way to think of understanding is to be able to create
a graph that has like concepts as- as nodes and then draws and relations in between them.
To see like how, uh,
to sort of like not just like- like climb the tree
or anything, but just understand I guess like what it means.
Um, how these different objects interact with one another,
uh, just like as a concept,
I don't know if that was a fair, all of it.
Yeah. So to- to- I just- I'm just gonna try to repeat it back.
Build a graph of concepts and relationships between them,
based on that understanding.
Based on the statement that was given?
Based on the statement that was given.
Yeah.
And in fact, one of the topics that we're gonna look at
this quarter is the topic of relation extraction,
which is precisely about taking a piece of natural language text like a web page,
and identifying the entities that are
mentioned in the text and the relationships between them.
And one of the applications for this is to automatically construct
knowledge graphs from text on the web at scale.
So that we can learn about,
you know, um, uh,
the fact that a certain album came from a certain band,
and the band originated in a certain city,
and it got started in a certain year.
And we can learn all this from some, you know,
fan page that somebody wrote or a review or something like that.
Okay. So a bunch of good ideas about,
uh, criteria for understanding.
This question of understanding has been at the heart of philosophical debates,
uh, about artificial intelligence for many years.
Uh, quick show of hands, how many people in the room are familiar with the Turing test?
Okay. That is reassuring.
I would've been alarmed if it were not a high proportion.
How many people are familiar with, uh,
Searle's Chinese Room argument?
Okay. So maybe half.
[NOISE] Um, so Turing's position is essentially a behaviorist position.
He says, that a system is intelligent if
its behavior is indistinguishable from that of an intelligent human being.
For our purposes, we might say that a system truly
understands if it behaves as if it understands.
In other words, behavior is,
uh, behavior itself is enough to demonstrate understanding.
Searle rebutted by saying,
that even if a system behaves as if it understands,
it might not truly understand.
And he asks you to imagine that you're locked in a room,
and you're passed questions in Chinese through a little slot in the door.
And you have to respond in Chinese and pass the answers back out through this slot.
And the problem is, that you don't speak any Chinese,
but you have in the room with you thousands and thousands of books that
specify in exhaustive detail an algorithm that you can follow to generate the answers.
So you're essentially playing the role of
a human emulator of a Chinese natural language understanding system.
Now, Searle's argument is that by executing this algorithm,
you might succeed in fooling people outside the room that you understand Chinese.
But in fact, you don't understand at all.
He says, "Behavior is not proof of understanding."
So what do you think about this argument? Do you buy that?
Is it plausible? Does it stand up?
Thoughts on this? Yeah. Um, it sets a pretty high bar if it's
correct because if we continue with like
our architectures you always have some tiny little Turing machine.
And that's never going to have a state that fully
encapsulates like the Chinese language or any other concave system.
It's just going to be traversing a big algorithm.
Um, so I would go with Turing.
Yep. Did you say- did use the phrase tangled Turing system?
No, I said I would go with the Turing [inaudible].
Oh, Um, I misunderstood you.
Okay. Yes?
I also feel like that assumes that like we know what it means to understand as humans.
Like what if we get an algorithm in our head that we're just kind of like following.
So like, it's just hard to separate like the algorithm from- that-
that- we only know what understanding- he hasn't really
redefined what understanding is specifically, he sort of left that vague.
That's right. Uh, Turing gives,
um, Turing gives this very behaviorist definition of what understanding is.
He's basically saying, understanding is just behaving as if you understand.
Yeah.
And Searle rebuts that, but he doesn't replace it with
any other definition of what constitutes understanding. Yes.
I feel like as humans,
we also extrapolate like going off of that.
Uh, if an invariable, uh,
algorithm's not that standard of extrapolation.
Yeah, as humans, we extrapolate a lot.
Actually, can you say a little bit more about that?
Yeah. So there's like concepts that are related to something that we don't know.
We might guess, or come up with, uh,
a definition that might be close to the right definition, and an
algorithm's doing the same thing.
It's pretty [inaudible].
Yeah. Okay. Yes.
So my question is, uh,
how do you define human understanding are we trying to supersede human understanding?
Like surpass human understanding?
Because I don't even know that we understand how humans think at the moment.
So [inaudible].
Yeah, uh, so I think what you're really getting at is- is
human understanding is- is the way that human understands necessarily the goal.
Or maybe there can be other kinds of understanding which are not the
same as human understanding but are still,
um, important and valuable.
Um, and that's a nice segue to,
uh, this quote from the eminent linguist Noam Chomsky.
He says, "The question of whether a computer is playing chess or doing long division or
translating Chinese is like the question of whether
robots can murder or airplanes can fly.
Blah blah blah blah blah.
Uh, these are questions of decision, not fact; decision as to
whether to adopt a certain metaphoric extension of common usage."
So he basically seems to be saying that, in fact,
the- there's no fact of the matter to debate here.
It's just a question of how we choose to talk about it.
It's just a question of how we choose to use the word understand.
And I think more and more people feel comfortable using
the word understand to talk about computers.
It may not be the same as human understanding.
But we're still okay calling it understanding and it's still something which
can be valuable and useful and can help us achieve, uh, applications.
Okay, I think that's the end of my philosophy for today.
You have to indulge me for a little bit of philosophy at the beginning of this class.
As an undergrad I was actually a philosophy major.
So I like to wax a little philosophical now and then.
We're going to turn now from philosophy to history.
Uh, and here's a nutshell history of natural language understanding.
So in the early days of computing,
uh, NLU was front and center.
Complete, precise understanding of human language
was seen as a central goal of artificial intelligence.
And there was actually quite a bit of optimism,
that it would soon be achieved.
And there were some impressive early successes which- which led to this optimism.
For example, there was a system called SHRDLU,
uh, which was created by Terry Winograd who some of you may know.
He's now a professor, he wasn't then,
but he is now a professor here at Stanford.
Uh, and SHRDLU allowed you to have a conversation about a simplified blocks world.
But the systems that were developed at that time tended to be
quite narrow in scope and their understanding was very brittle.
So if you said something one way, it might understand.
But if you said the same thing using slightly different words,
it might not understand at all,
uh, and would completely fall down.
In the 1990s came the so-called statistical revolution in
NLP and this brought some important new tools like data and machine learning.
Uh, but it also brought a general retreat from
the higher level ambitions of natural language understanding and instead
the focus shifted to lower level tasks, like part
of speech tagging, and syntactic parsing, and things like that.
And then about 10 years ago,
we saw this big resurgence of interest in natural language understanding.
Um, now leveraging new machine learning
techniques and vastly greater computational resources.
And starting about five years ago,
um, deep learning methods have completely transformed natural language understanding.
So starting with, um,
neural embedding methods like Word2Vec, which is
not actually deep learning but kinda gets bucketed together with them.
And then moving on to things like LSTMs, and
sequence to sequence models, and many other flavors.
And now trained on vastly larger datasets than had ever been available before.
Uh, and so that means that this is
an incredibly exciting time to be working in natural language understanding.
So in academia, there's been this resurgence of
interest after many years in the wilderness.
And in the commercial world there's a growing sense that
natural language understanding is on the verge
of breaking through as a mainstream technology.
And over the last few years there's been an explosion of,
uh, businesses and services that are based on natural language understanding.
So things like Siri and the Google Assistant, and
Amazon Alexa, and Microsoft Cortana, and Samsung's Bixby, and many more.
Um, and many people see conversational agents as
being a key battleground between the tech giants over the coming years.
Uh, and by the way that means that Stanford grads with
expertise in NLU are in very high demand.
So where is the state of the art today?
Well, today's best NLU systems are sometimes
impressive but they have severe limitations which quickly become apparent.
And I'll show you a bunch of examples of those limitations in a few minutes.
Um, and I think what this shows is that NLU is far from a solved problem.
There's still a lot of discoveries waiting to be made.
And for me as a scientist and a technologist, uh,
that means that NLU offers this irresistible combination.
On the one hand, there's a lot of interest in NLU,
uh, there's great demand for it.
It's ready to make a huge impact.
But on the other hand,
even though we're making rapid progress,
like it still doesn't really work yet.
And there's still so- there's still a lot of discoveries that are waiting to be made.
There's still a lot of gold in the ground.
And that's really exciting.
Uh, one of the most visible NLU applications today is Siri,
the personal assistant on your iPhone.
Um, despite growing competition,
it is still by far the most used virtual assistant with half-a-billion active users.
Um, when it first appeared in 2011,
some even described it as representing a breakthrough in artificial intelligence,
and as defining the next generation of interaction design.
Even if you think that, uh,
those claims are just a little bit inflated,
there's no question that Siri has been tremendously impactful.
And over the last few years,
we've seen Google, and Amazon,
and Microsoft, and others develop similar products,
and add some degree of natural language understanding not only to phones,
but to watches, and TVs,
and cars, and now, smart speakers.
So how do conversational agents like Siri work?
Well, if the input modality is speech,
then we start with automatic speech recognition.
And as you probably know, uh,
the accuracy of speech recognition systems
has increased dramatically over the last few years.
Thanks to the application of neural networks and deep learning methods.
Um, you might think based on the name,
that natural language understanding would include speech recognition.
But historically, the two fields have been quite separate,
and the term NLU is usually restricted to understanding of written input,
that is understanding of text.
Uh, of course, in some use cases,
the input is already written or typed.
So we can skip the automatic speech recognition.
On the output side, uh,
we often have a text-to-speech module or TTS module.
So that's not NLU either.
The NLU component sits in the middle,
and it's often surprisingly simplistic.
We typically start with pre-processing by
some standard NLP tools such as part-of-speech tagging,
and named entity recognition and resolution.
And then the heart of it is an interpreter that generates
some kind of meaning representation as output.
So, that's a machine readable representation of the semantics of the request.
And then that meaning representation is passed to a service manager which can make
calls to internal and external APIs to execute the request.
Now, there's a lot of room for variation here.
Um, the meaning representation can be discrete or it can be continuous.
Uh, the interpreter can be manually
engineered or it can be learned from data or most commonly,
it's some combination of those two,
and in this course we'll take a look at a lot of those design choices.
Now, there are a lot of interactions that you'd really like to be able to have
which are just beyond the capabilities of today's conversational agents.
So here's an example dialogue.
Uh, you say, "Where's Black Panther playing in Mountain View?"
and the agent says, "Black Panther is playing at the Century
16 Theatre." "When is it playing there?"
"It's playing at 2:00 PM, 5:00 PM. and 8:00 PM."
"Okay. I'd like one adult and two children for the first show.
How much would that cost?"
This would be incredibly useful,
but it's also incredibly hard.
Let's take a minute to dissect exactly what makes this hard.
And I'm gonna focus just on one aspect of understanding here.
I'm gonna focus just on reference resolution.
So this is the problem of understanding what the various expressions refer to.
In order to do that,
we're going to need a bunch of knowledge.
We're going to need, first of all,
domain knowledge because we need to figure out that Black Panther refers to a movie,
and Mountain View refers to a town.
We also need discourse knowledge because we need to figure out
that it refers to Black Panther,
and there, refers to the Century 16 Theater,
and that, refers to the tickets.
And we're gonna need world knowledge because we need to
understand that people don't usually buy adults and children.
So the user is talking about buying adult and child tickets.
And also, we need to figure out that the first show refers to 2:00 PM,
because 2:00 PM is before 5:00 PM and 8:00 PM.
So there's a lot going on here in what- at first glance,
appears to be a relatively simple dialogue.
There's a lot of different kinds of knowledge that we need to bring to
bear in order to execute this conversation successfully.
So just for fun, let's see how Siri actually does with this dialogue in real life.
So these are some screenshots that I captured last year when
Black Panther came out, and, uh,
I'm gonna try to recreate the interaction that we had on the previous screen,
and we'll see how Siri does with this.
So I start off by saying,
"Where's Black Panther playing in Mountain View?"
And Siri says, "Here's Black Panther playing in 9 theaters in Mountain View."
Okay. So Siri definitely understood what I wanted.
That's great. We're off to a good start.
You know, you might quibble that the first result is actually in Saratoga,
and the second result is actually in Cupertino,
[LAUGHTER] and the third result,
I'm not sure where that is but we have to go all the way down to like
number 5 or 6 before we get to one that I'm pretty sure is in Mountain View.
So, you know, we might quibble about that.
But I- all in all, I think it's a pretty promising start that we're off to.
Now, I'm gonna try to continue the dialogue from the previous slide.
So I say, "When is it playing there?"
You might think my question is kind of stupid
because Siri already gave me the show times,
and I guess I failed to read them or something.
So okay, I'm a bad user.
I'm an idiot. [LAUGHTER] But
my claim is that Siri ought to be able to do something useful
with this. Even if I'm an idiot, I think Siri ought to be able to make some sense of this.
A human would be able to make sense of this, right?
If you had a human, the human be like "Okay he's a dumb user,
but I'm gonna give him the answer anyway." When is it playing there?
I think you ought to be- you ought to be able to interpret that as
referring to maybe the first result that you gave me.
And so maybe give me the show times for the AMC Saratoga.
Let's see if that's what Siri has done.
And Siri says, "Here are some movies playing at theaters near you."
I don't have the feeling that Siri really understood what I was asking for.
It didn't give me show times for the first result,
didn't give me show- well,
I guess it did give me some so- shom- some show times.
But I don't feel like Siri really understood me at all.
I feel like this conversation is kind of going off the rails.
Let's forge ahead though and see what happens [LAUGHTER] with the hardest part.
"Okay. I'd like one adult and two children for the first show.
How much would that cost?"
Now, again, I'm just following the script from the previous slide,
and maybe I'm an idiot,
but I think Siri ought to be able to interpret this as referring to- I mean,
I said the first show.
So I guess maybe that's this one or maybe it's this one or something.
I wanna buy some tickets,
and I want Siri to understand that I wanna buy some tickets.
Siri says, "Here's what I found on the Web for 'Okay,
I'd like one adult and two children for the first show. How much would that cost.'
Have a look:" And it gives me some results about how much I should spend on groceries,
and when should a child be taken from his parents.
[LAUGHTER] Seems a little odd.
Um, it's interesting though Siri's fallback strategy when it doesn't understand.
I mean, it- it has to do something, right?
It can't just, like,
raise an exception and print a stack trace.
[LAUGHTER] It- it has to do something.
So, its fallback strategy is, it takes the last thing
it thinks you said, and it searches the web for it.
And, you know, as a fallback strategy,
that doesn't seem like the worst idea.
Um, I'm really not trying to make fun of Siri,
my point is to illustrate just how hard NLU is.
Uh, by the way, I tried the very same interaction with the Google Assistant,
and it didn't do any better.
It was pretty much the same kind of outcome.
Uh, besides, I don't need to make fun of Siri because
people much funnier than me have already done the job.
For example, Stephen Colbert.
So this is from when Siri first came out in 2011.
I'm not gonna try to play the clip but I put the URL, uh,
and I'm gonna post these slides so that you can go check it out later, if you want to.
Um, Colbert explains that he's been playing with his new iPhone 4 all day,
and so he didn't get around to writing the show.
So he wants Siri to write the show for him because,
like, Siri is magical, right?
And it'll just work. So he says to Siri, "Write the show."
And Siri says, "What would you like to search for?"
And he says, "I don't wanna search for anything.
I wanna write the show."
And Siri says, "Searching the Web for 'search for anything.
I want to write the shuffle."
[LAUGHTER] So you see this fallback strategy again of searching the web.
And again, you get the impression that Siri
didn't really understand what he was asking for.
Now, in this case,
maybe it's forgivable because,
you know, his meaning was complex.
He wasn't just asking to set an alarm.
Um, but it's kind of interesting to see the same- the
same fallback strategy. Uh, and then I love this.
A few minutes later, he says,
um, "For the love of God,
give me something- no.
For the love of God, the cameras are on, give me something."
And Siri says, "What kind of place are you looking for?
Camera stores or churches?"
[LAUGHTER] So this is very revealing.
I- I think what's happening here is that Siri has like latched on to cameras and God,
and then it's like an eager puppy that just wants to go search the Web for you.
[LAUGHTER] And then a little bit later, he says, "FU."
And Siri says, "I'd blush if I could."
[LAUGHTER]
So this, this fallback strategy is very reminiscent of Eliza.
Do you guys know about Eliza?
So Eliza is this very famous program from the 1960s.
It was one of the first programs that, at least,
appeared to be doing natural language understanding.
Eliza is basically a chat bot that assumes the role of a psychiatrist, uh,
and at first the dialogue seems to be
surprisingly natural and life-like, um, and in fact,
some of the early users of- apparently found it so
convincing that they asked to be alone in the room with a therapist.
But if you're a slightly more discriminating observer,
you soon realize that it's just reacting to triggers in what you say.
It's just matching patterns and then transforming the patterns in some way.
So if I say, x,
then Eliza says, why do you think x?
Or does it please you to believe that x?
Now, it does demonstrate,
uh, at least a little bit of linguistic capability.
So for example, it knows how to swap the first and second person when reformulating x.
But this is only the shallowest imitation of understanding.
It's not the real deal.
It's what Hector Levesque would call a cheap trick.
Now as you probably know,
NLU has become a major priority at Google.
It was my focus when I was at Google.
And one reason for that is that,
more and more search activity is shifting to mobile devices,
and more and more queries are spoken rather than typed.
And it turns out that when people talk to their phones,
they're much more likely to use
natural language than when they're typing into a search box.
So Google has made a big push on what's known as conversational search,
and a big part of making that work is understanding the context of the conversation.
So instead of a bunch of isolated queries,
you can have a coherent conversation and you
can refer back to stuff that was mentioned earlier.
So you can ask about Chicago and then you can say, who's the mayor?
And Google knows that you're talking about the mayor of Chicago,
and then you can say, how old is he?
Who is he married to?
And Google knows what you're talking about or you can ask about the San Diego Zoo,
and you can say, is it open?
How far is it? Call them.
Uh, of course, it doesn't always work perfectly.
Getting this stuff right is remarkably hard.
But more and more it actually works.
Uh, actually semantic interpretation is not just for natural language queries.
It's also increasingly important for queries that
aren't natural language but are more like what you might call Google-ese,
whether spoken or typed into a search box.
That's because a growing proportion of the query stream is not well
served by traditional keyword-based information retrieval.
More and more queries are seeking answers that are not found on any web-page.
So consider something like,
how to bike to my office.
There's not a web-page that I- there's not a, like,
static ordinary web-page that I can give back,
that's gonna give you the answer to that.
Um, and more and more queries are seeking action rather than information.
So something like text my wife, I'm on my way.
Just imagine if Google responded to queries like these,
by giving you 10 blue links to documents containing those terms.
I mean, that might work okay for this one,
I don't know, I haven't tried it.
It might work okay for this one.
Uh, it's definitely not gonna give you a satisfactory result for this one, right?
That's gonna be a really bad user experience.
Satisfying queries like these requires semantic parsing,
which means mapping the query into a structured,
machine readable, representational meaning that can be passed to
some downstream com- some back-end component to take action on.
Uh, and I've shown some examples of
what semantic representations might look like here but I'm not gonna dwell on this now.
This is a topic that we'll return to in a couple of weeks.
Another big application for NLU is sentiment analysis,
and growth in this area has been driven by
the explosion of user-generated content on the internet.
So this basically means looking at product reviews and social media,
and trying to understand how people feel about companies and brands,
and products, and which specific features of products they like and dislike.
There are zillions of startups that are,
that are operating in this space, uh,
and they are pursuing a variety of different business models.
So, for example, there are so-called market analytics firms that
do sentiment analysis on social media and product reviews,
and then sell the results to marketers,
to help them understand how consumers feel about their products.
And apparently if you're an airline,
the answer is, they hate you with every fiber of their being.
Another intriguing application for sentiment analysis has been in quantitative finance.
Uh, as an example there was a paper that came out a few years ago,
that claimed to use analysis of sentiment on Twitter,
to predict moves in the Dow Jones Industrial Average up to six days in advance,
with 87 percent accuracy.
That sounded pretty awesome, pretty exciting.
So a couple of guys started a hedge fund to trade this idea.
The fund was massively ober- oversubscribed.
They quickly raised $100 million to try to exploit this idea.
Unfortunately, the methodology was completely bogus,
and the hedge fund soon had to shut down.
[LAUGHTER].
Uh, but just because this idea didn't work,
doesn't mean there's nothing there,
and in fact many asset managers are now using
natural language understanding techniques in automated trading.
Of course, the shops that are using these techniques are,
um, extremely cagey about,
a, a, ab, about how they're doing what they're doing.
So it's hard to know for sure,
how prevalent this is.
But if you look closely,
you can see lots of signs of automated trading being driven by text analysis.
For example, a few years ago a blogger named Dan Mirvish,
noticed that every time Anne Hathaway was in the news,
their was a jump in the stock price of Berkshire Hathaway.
The holding company run by Warren Buffett.
Nobody knows- knows for sure,
but it seems plausible that some automated trading system
was confusing the actress with the holding company.
So some NLU systems demonstrate very limited understanding.
Nevertheless, this is a fast-growing area.
These days, most trading is automated and most trading strategies rely in
part on automated analysis of unstructured feed, unstructured data feeds.
So that means, natural language text.
That means, things like news stories,
and broker reports, and, uh,
SEC filings, and transcripts of conference calls,
and social media, and so on.
It turns out that you can make enormous trading profits
if you can discover and act on market-making news,
just a little bit faster and more accurately than your rivals.
So, um, you know,
things like Disney's acquisition of 21st Century Fox or,
uh, Trump's summit with Kim falling apart.
Essentially, they're using natural language understanding to predict the markets.
[NOISE] Actually it's when things go
wrong that the use of NLU in automated trading systems becomes most apparent.
And their was a very interesting example in 2008,
involving United Airlines' stock.
So what happened is Google News crawled the Florida Sun-Sentinel's website,
and they found a story about United Airlines filing for bankruptcy.
So they began serving it up on Google News.
Automated trading systems began to react within seconds,
and this triggered an avalanche of stock sales.
Within 12 minutes more than a billion dollars in stock market value had evaporated,
uh, and that's this huge cliff here in this, uh, price chart.
The problem was, the story was six years old.
It was from 2002.
Um, for some reason,
the newspaper had posted this old story in their popular news section,
and the only date on the article was September 7th, 2008.
So it was the day before this thing happened.
So finger-pointing all around,
the newspaper should have dated their article properly.
Google should have recognized that the story was
a duplicate of a story from six years old,
you know, from six years ago and above all,
automated trading systems shouldn't believe everything they read.
There was an even more dramatic example in 2013.
Uh, you might remember this one.
Somebody hacked the Associated Press Twitter feed,
to report explosions at the White House and Obama injured.
So instant pandemonium.
The Dow immediately plunged more than
140 points and then recovered within just six minutes.
[LAUGHTER].
Because and look, look at the size of this spike right here. This is incredible.
Because this affected the entire market and not just a single stock,
the impact of it was much bigger.
The S&P 500 temporarily lost $136 billion in market capitalization.
Now you might say, oh, but it came right back up.
So no big deal, right?
But the people who bought on the way up were
not necessarily the same as the people who sold on the way down.
[LAUGHTER].
Chances are, there were some huge winners and some huge losers from this event.
And this was again, very likely driven by automated trading systems.
Uh, here's a quote from some dude who says,
"That just goes to show you how algorithms read headlines
and create these automatic orders.
You don't even have time to react as a human being."
So there are tremendous economic incentives to improve
the accuracy and the reliability of these NLU systems,
and banks and hedge funds are making big investments in this area.
Okay. So that was, kind of,
a whirlwind tour of a bunch of real-world commercial applications of NLU,
uh, with particular emphasis on limitations of NLU.
There's a lot of stuff that's still not really possible or still goes wrong with NLU,
and for us that represents a big opportunity
to make new discoveries and find better ways of doing things.
[NOISE] Enough about applications.
How are we going to approach this topic?
Uh, NLU is a big field,
it covers many different subtopics.
So how can we organize the content of the field?
One way to organize the subtopics is into three levels of meaning.
So at the bottom level we have words,
and the meanings of words,
and that's the province of lexical semantics.
Then at the next level,
we can look at how the meanings of words combine to form the meanings of phrases,
and clauses, and sentences.
And that's the province of compositional semantics.
And then at the highest level, we can look at the meaning of language in context,
and the meaning of dialogues,
and discourses, and this brings in topics like reference resolution,
and pragmatics, and so on.
A different way to cover up the field of NLU is to try to categorize tasks based on the,
kinds of, output representation that we're trying to produce.
In most NLU tasks,
we take a sample of text as input,
and we're trying to map it into some,
kind of representation of meaning, either as an output,
or as an intermediate step toward a practical goal.
Those semantic representations can be continuous or they can be discrete,
and they can be simple or they can be complex.
So for example, in sentiment analysis,
we're most commonly trying to generate a
scalar value indicating positive or negative sentiment.
It might be just -1 or +1,
or it might be a five-star rating system, or something like that.
With vector space models and meaning,
that we'll start looking at on Wednesday, uh,
we represent the meaning of a word or a phrase as a point in some high dimensional space.
Or a different perspective, is that it's some, kind of,
distribution for example, a probability distribution over abstract topics.
So those are continuous representations of meaning.
We can also have discrete representations of meaning.
So for example in relation extraction,
we're trying to produce, uh,
relation instances or database triples,
things like Larry Page, Founder,
Google, or Google located in Mountain View.
And in semantic parsing we're trying to produce
semantic representations that can have arbitrarily complex logical forms.
So here's a logical form that's intended to denote the largest state.
Uh, and we'll look at all four
kinds of problems in this class.
[NOISE] Two themes that will appear again and again in this class.
One theme is semantic composition.
And the question here is, how are the meanings of bigger pieces,
like phrases and sentences,
built up from the meanings of smaller pieces like words?
Within li- within linguistics,
this question has been a focal point of compositional semantics for
decades and it will be a big theme when we look at semantic parsing in a few weeks.
But in recent years as vector space models of meaning have grown in popularity,
there's also been a lot of interesting work on how to
combine vector representations of meaning,
and we'll look at some of that work.
Uh, the other big theme is learning.
And here the question is,
how can we build models of semantic,
uh, for semantic interpretation automatically from data?
And there are two reasons for wanting to do this.
One is a matter of principle, and the other of pragmatics.
The principled reason is that we want to be empiricists,
we want to be data-driven,
and we want to build models that can interpret the language that people actually use,
not the language that we imagine that they use.
The pragmatic reason is that building large complex models by hand is slow,
and tedious, and expensive,
and it doesn't scale.
So we want to automate that work so that we can build these models quickly,
and cheaply, and by the way,
also build them in 40 different languages.
[NOISE] Now we have some goals for this course that go beyond just covering the material.
One is that we want to give you the skills and tools,
uh, that you need to be- to quickly apply, uh,
that you can quickly apply to NLU problems beyond
this class whether in academic research,
or in internships, or jobs in industry.
I mentioned earlier that there's- there's
a voracious demand for smart people with knowledge of
NLU and we want to enable you to jump in and be effective,
uh, wherever you go.
[NOISE] Another goal is to support you in
completing a project that is worthy of presentation at a top NLP conference.
The biggest deliverable of the class will be an independent research project and
a final paper in the style of NLP conference papers.
In previous years, many CS224U papers have actually been turned into conference papers.
And whether you are an undergrad,
or a Master student,
or a PhD student,
presenting a paper at a top research conference is one of
the best ways to enhance your credentials and get to the next level.
So I'm gonna stop here,
and I'm going to turn things over to Chris to talk about course logistics.
All right. Great. [NOISE] So I'm just going to take
the remaining 20 or so minutes that we've got to talk about some course logistics.
Uh, I want to make sure that you leave here with
a good sense for how the course will work,
what kinds of work you'll be doing, and so forth.
Um, and I thought a good way to start with this is to just show
you how the regular assignments are going to unfold.
As Bill said, essentially everything about this course
is geared toward giving you hands-on experience with the,
kinds of, models that we're exploring.
What we would like for you to do is take like baseline code that we've implemented,
and take it much further.
And we think that a key aspect to doing
that is that hands-on experience, not just listening to us
talk about these models and showing off
the best results in the field but rather down nitty-gritty,
actually exploring the code,
trying different simulations out and seeing what happens.
And so a lot of what we do is oriented toward giving you that
kind of hands-on experience.
Um, and the one way you can see that is the way we've structured these assignments.
So there are four assignments,
um, and they unfold over the first half of the course.
The first one you have two weeks,
because the start of the quarter is always, kind of,
chaotic but after that they go on a one-week cadence Monday to Monday.
Um, the first one is due on April 15th as I said,
and then weekly after that.
Um, each one is hands-on experience.
What- what you'll do is work inside a Jupyter Notebook,
upload it to Canvas, and we'll evaluate it.
But the primary thing beyond the evaluation is
that we want you to explore different kinds of
models and the assignment questions are oriented
essentially toward having you set up some baselines.
Um, and then a crucial part of the assignment here is
that each one culminates in what we call a Bake-off,
and this is a term from the field.
Um, the idea is that you enter
an original system and it's
evaluated on some test data that we're gonna provide during the bake-off,
and you enter your score.
And we're gonna give a little bit of extra credit to the teams that have the top system.
Um, and the way the assignments work is,
you set up those baselines then you implement your original system,
and you get nine points for doing that,
and then you get an additional point if you just enter the bake-off.
So you'd- all you have to do is enter your system,
and that happens Monday to Wednesday after the assignment is due.
And the idea, you know,
best practices in the field is you do
all your development until you submit the assignment,
and then you don't do anymore tuning or anything like that,
you just enter your system into the bake-off.
And then winning systems get a little bit of extra credit.
Um, what we're trying to do here with these assignments and the bake-offs is kind of
give you a sense for how we think projects should unfold because
what we're going to push for you in the second half of
the course is that your project should kind of
implement some baseline systems,
and then offer an original system that tests
a hypothesis about how you think the world works, a scientific hypothesis.
And we're trying to push you in that direction and get you to build
those baselines and then doing- do something original.
And the bake-off aspect of this is meant to be just, kind of, fun.
And what we'll do is reflect back to you
which systems one- which system does really poorly,
and try to offer insights as to why that happened.
Um, and that's always been really interesting because this is like
crowd-sourcing lots of different ways that you might approach these problems,
some good, some bad and I think we can all learn from the results.
What we're trying to do as I said,
is kind of exemplify best practices for NLU,
the code that we offer is meant to do that,
the assignments are meant to push you in that direction.
You should let us know if you think we're not living up to this.
Um, we would love to have an exchange with you about how to do
better when it comes to best practices for solving these problems.
So that's kind of the first half of the course,
and you'll see that when we look at the web page,
you'll see that what we're doing as part of that,
is introducing lots of topics and they are
the topics that we think of as the kind of essence of NLU,
the ones that if you really mastered
them then you can do pretty much anything in the field.
Uh, and that's just, we have limited time but we're hoping that that's, kind of,
a generator for you for lots of different approaches in problems.
This is a practical note here.
For the assignments, and for the projects,
and the bake-offs, you can work in teams.
The way you'll do this is,
you should form a team on Canvas,
and I've given instructions
here. You go to the People tab,
and then the Groups tab,
and then you have to pick a pre-selected team from
the large number of teams that I created for each one of these assignments.
That's a little bit tedious but once you've formed
your team that means one of you can submit the work,
and it will be- you'll all get credit for it.
And you should make sure that if you formed a group,
then your assignment is associated with your group.
I think the interface is pretty clear there.
So the only tedious part is actually
forming a group because you have to pick from a pre-selected one,
that was the best I could figure out.
But in terms of actually your work being uploaded to Canvas,
it's just that you'll upload a
Jupyter Notebook and then the team will take it and evaluate it. Did you have a question?
So are there different teams for each different bake-off?
There can be. Yeah, that's the way I set it up.
You'll see that they're called like Assign 1 Bake-off 1,
Assign 2 Bake-off 2,
that means you can switch.
There's also one for- one for the final projects.
If you want to have the same team through all of them,
you'll just have to reconstitute your team for each one.
Um, it's not the best system,
but I think if we get used to it it'll work okay.
Questions about the assignments and the bake-offs before I move on? Yeah.
Is grading based off the number of people that are assigned on a team?
Not for the assignments and bake-offs. I'll show you when we look at the website in a little bit.
We have slightly different criteria for large teams working on the projects,
but it's a very soft touch thing, yeah.
But for these, I don't know.
We're hoping that you have a fun experience of
collaborating and you yourself can try lots more models if you work in a team,
and see what works best.
Then the second half of the course,
you'll see this when we look at the schedule.
Like the first half is topics and then the second half is less about topics,
and more about us trying to help you successfully execute on a high-quality NLU project.
And so what we do is shift toward lectures that are much more about metrics,
and methods, and tick tricks,
and tips, and other best practices.
Things that will help push your project ah, in better directions.
Um, so it becomes less about introducing
new content and more about that kind of methodological stuff.
And we're hoping that in parallel with that,
of course, you're working on your project as a team.
And the way we're going to encourage that to happen is that it kind
of- the project unfolds over a series of assignments.
So the first one is a lit review.
This is just kind of you and your team, if you form one.
Getting oriented around what problem you want to solve.
Maybe less familiar, what- what we've done is set up
an experimental protocol as the second um, ah, assignment.
And I'll show you what it- what its contents are when we go to the web page.
But the idea here is just that we're trying to push you to be sure
that your project is testing some scientific hypotheses.
Then you do a video presentation.
Um, we haven't typically done
poster sessions because it seems like an awful lot of work to create a poster.
When this class was much smaller,
back when NLU was much less popular,
we would have people give three minute talks and that was always
lots of fun because we saw all these wonderful little presentations.
The class has gotten too big for that.
So as a compromise,
what we've done in recent years is have teams do
little short videos and then you could put them on YouTube or upload them to Canvas,
and it can be public or private, whatever you want to do.
Um, but the point is that gives us a glimpse of what your project is
like and gives you a first chance to think about how you want to present it.
And then of course the final thing is the final paper.
Uh, and if you're interested to get started here,
this is a link, we'll post the slides in a bit,
a link to some past projects that are really
exceptional from a bunch of different years and from recent years,
I have some video links available.
Um, so you can also see what good video presentations were like.
Questions about that?
I'm going to circle back to this in a second. But yeah, go ahead.
[NOISE]
Can we make our final project in open source like public on GitHub?
That would be wonderful. Yeah, of course.
Yeah, I don't want to make it a formal requirement but one of the best things
about NLP in general and this is sort of true of all of AI at this point,
is how much the community has formed norms
around making sure that you open source your code and data.
It's almost like you don't have results.
If you don't have results that other people can pretty
easily reproduce and this is wonderful because,
when I started out in the field,
you would spend months or years trying to build
up baselines that other people had established,
and now the pace is picking up just because it's so easy to share these results,
and that kind of comes back to people's willingness to distribute data,
right away, and also open source their code.
So not a requirement but it would be great to be
getting like GitHub links for all these projects.
Other questions? Course logistics.
So I'm going to go follow these links in a second here,
but so I'm going to go to the website.
The teaching team is me and Bill,
and we have 10 TAs.
I was wondering, this is going to be hard because of
the microphones but could the TAs who are here stand up.
I would love it if you introduce yourself and just
said kind of what your interests are in NLU,
and also if you're an alum of the course,
it would be cool to mention that. We're going to start.
I am a master's student in the CS department.
I am an alum of the course and my interest in NLU
is nothing in particular, I don't have a good answer.
Not everything.
[LAUGHTER]
I am also a masters student.
I took this last spring.
My interests in NLU are generally building systems
to understand like language of the internet in English and also other languages,
and everything else attached to it.
A masters student.
I did course last year and my interest is in modelling dialogue and conversational systems.
I'm also a master student and my main interest is
natural language imprints and I did take this course last year.
I am a PhD student.
My interest are in deep reinforcement learning [inaudible].
Hi, I am a masters student.
I am an alum of this course and I am interested in learning about [inaudible] social data sets.
[NOISE]
[inaudible] I am actually manager at the implication of genomics but I'm
interested in [inaudible] , and also [inaudible]
Let's see here. I'm not sure who's missing.
We'll try to introduce lots of people next class and after that.
I do want to mention that [inaudible] who can't be here in person.
He's one of the TAs, he's going to be more or less a virtual TA.
So he graduated from Stanford a number of years ago.
He did his master's thesis and his honors work in Natural Language Understanding,
kind of applied to social problems, and he's returning.
This is a program that was created by SCPD.
So it should be fun to interact with him because he's been
in industry doing NLU type things now,
for a number of years,
and I think you'll be able to reach him on video.
Uh, and I wanted to say also,
thinking about these alums and I remember some of their projects,
one of the wonderful things about this course
is that we get a really wide range of projects.
They kind of go from, yes,
like hard-core deep learning things,
all the way through to digital humanities and kind of social science problems,
and I think that's a wonderful aspect of this course for me
because like for me, as a researcher.
One of the exciting things I think I can do is
identify results that come out of NLP and show people in neighboring fields,
especially in my own field of linguistics,
that they have real value,
and it's just great to see people doing projects that kind of
exemplify that in sociology and in history,
and in English, and in psychology,
and linguistics of course, and so,
we're very encouraging about getting really creative,
about the kind of problem that you take on for your final project.
The lectures for the course will be streamed and stored on Canvas.
You should all have access to Canvas.
If you want to reach the staff,
this is the address to do that.
But you can also just post on Piazza.
We- I think the whole teaching team will be quite active on Piazza.
So you can use that to get in touch with us and then here's a link,
which I'll show you in a second just to the kind of things that
make up the core components for the grade.
Before I switch out of these slides, special announcement.
We have a special session on Friday.
It's going to be run by ,
that is about- it's kind of a refresher or to help you get up to speed on Python,
and on working in Jupyter Notebooks.
We're kind of assuming that you're a pretty good Python programmer already.
The Jupyter Notebook thing might be more
unfamiliar to you but it's going to be a kind of norm in the course,
that we do a lot of our coding and exploration,
and assignments, and bake-offs and stuff in notebooks.
So if that's new to you,
I highly encourage you to go to this Friday session and here are the details.
We will give you a bunch of other reminders for this.
It's great that are doing this.
And then for next time,
you should get your computing environment set up.
I'll show you in a second what I mean by that.
But here's a kind of run down.
We have a course GitHub repository that has lots of code.
It has the notebooks;
it has the assignments.
Um, you should get in the habit of kind of syncing with
that repository, making sure you have the latest updates and so forth.
Um, a lot of the tools are there,
and this is a kind of run down.
We're, we're gonna encourage you to use Anaconda for your Python environment.
Officially, the course is using Python 3.7,
I have tested the code all the way down to 3.5,
and it actually basically works under Python 2.0.
But please, do not use Python 2.0,
um [LAUGHTER] it's now moribund.
Um, but if you're stuck on 3.5.
for some reason and you don't want to create a virtual environment,
you're probably going to be okay.
Uh, and then there's a bunch of other stuff that you want to install.
A lot of this is taken care of if you're using Anaconda,
but if not, you're going to have to kind of do all the requirements.
Uh, and then there's a huge data distribution folder.
Um, I just updated the link to one where I think the archive is going to work.
I was really proud of myself for getting all of
the data for the whole quarter together into one archive,
and of course, it was like too big to be a ZIP archive.
[LAUGHTER] Um, I'll post some corrected links,
and with luck we'll get this sorted out.
Uh, but you only have to get it downloaded once.
Uh, and then what you should do for the next time,
and kinda over the next couple of weeks,
is watch the screencast that we have available for this unit.
Uh, to the core reading is the one that's by Turney and Pantel 2010,
and then you should just start exploring the notebooks.
Uh, and you could do that kind of on your own speed.
Uh, I'm gonna be here with you giving lectures about it,
and the lectures are also going to be trying to get you to work on
your laptop more or less as I talk to explore things and figure out what's going on.
Um, let me just show you a bit about the website in the remaining time that we have.
Okay. So this is the main page.
It has pretty much everything you want.
So, there's a link to the Piazza site, uh,
which you should join if you're not already enrolled there,
and the Canvas site and our GitHub,
and there's the, uh, staff address.
And then down the left column here,
you have the teaching team,
and we've listed all of our office hours.
So that's kinda organizational stuff,
and then here's where all the action is going to be.
So this is the main schedule.
Uh, it goes kind of topic by topic.
So, as Bill said, we're gonna do vector space models,
that is, vector representations of meaning for the first two weeks.
It's kind of a slow start as people get oriented, and we've,
you know, kind of get people who are newer to the field up to speed.
And that covers homework one and bake-off one,
and then we do some supervised sentiment analysis using the Stanford Sentiment Treebank.
Then, relation extraction.
Then, natural language inference,
grounded language understanding and semantic parsing,
and I believe that's the kinda,
kinda the point where we shift.
The assignments become more project oriented,
and the topics are more around,
like, metrics and methods,
um, contextual word representations as a tool to make your models
even better and some other stuff like writing up and presenting your work.
Uh, and now I'm,
I'm, I'm not sure of the scheduling yet,
but I'm hoping that on May 22 we have
a kinda panel discussion with people who had been doing NLU in industry for
awhile to kind of give you guys a sense for what it's like
to take this research and apply it out there in the world.
Um, and then and more kind of projects stuff.
Um, the last, the last part of the week of the course because of NAACL
when a lot of people have to be away is kind
of just given over to you for doing project work,
and then the final project is due on June 9.
Another thing about the schedule,
so there are a lot of links already in place.
Those are the notebooks for the assignments,
and the kind of core notebooks that will allow
you to do the hands-on work that we're envisioning,
that will help you do the assignments,
and get really accustomed to what these tools are like.
So, those are already there,
and you can start to work with them,
and you might want to skip around especially if you're,
um, familiar with NLU problems, like,
you took the deep learning course or something like that,
you might want to skip around because, for example,
you could do a really good job on the bake-off if you can figure out
those contextual word representations, that kind of thing.
So browse around, see what's there, um,
we'll be posting slide shows pretty regularly
that kind of try to integrate all this material together.
Along the resources column,
we have links to recommended readings,
and in some cases those screencasts which are short
YouTube videos that I made that are kind of encapsulations
of the really technical parts of the material that we'll be covering,
the stuff that's kinda hard, uh,
to just get from a lecture that you hear once.
Um, I've tried to distill it down to just the essence.
We have especially good coverage for those,
for these early units,
which is good because in this first unit for vector space models,
we're really trying to build the foundation for thinking
about how to build up more complex NLU systems.
I think that's it for now.
Any questions? Oh yeah,
I could show you just one more thing.
So, the other two pages are,
we have a detailed description of how the projects unfold,
other requirements are here for,
like, what we expect from a lit review,
um, what we expect from that experimental protocol.
Um, some guidance about video presentations and the final paper,
and here are those links to, um,
past projects that are really exceptional,
in case you want some examples,
the range of projects people have
explored and also just what really good projects look like.
Um, and then finally,
still in this last minute, um,
there's also a kind of policies and requirements page that reviews, like,
how much homeworks are due- worth,
the lit review, the protocol,
the presentations and the final project.
Uh, and there's some details here about kind of policies and other stuff like that.
I guess, I'll leave it to you to read that on your own.
It's not so thrilling for me to review it now.
[LAUGHTER] Um, before we wrap up,
are there any questions or comments,
or concerns that I can field? Yes.
Is the winner of the bake-off, is it just determined by
the multi-criteria of C or other criteria?
For each one, we've specified the criteria.
There's a specific score typically,
and what I've said is that the systems that have the top score will get the extra credit.
Um, we're going to have to see what the range of scores is like,
but it's very tightly regimented the way these bake-offs tend to be. Yeah.
Will the projects get sponsored with
compute time on AWS or Azure, or GitHub?
I'm working on it. Yeah, I've been trying to get Azure.
Uh, uh, I have to be a little persistent I guess,
but I am hoping that I can make resources available to you guys.
 Welcome. Today we're gonna dive into the material in earnest.
Start building a foundation for thinking about natural language understanding models.
Before we do that though,
I wanna do just a few logistical things,
um, because, well frankly,
because they're not so intuitive.
So let me start with the least intuitive one of all.
Uh, as I said last time,
we're gonna be using Canvas to do all the submission of work,
which basically means that you're gonna use it to upload notebooks.
Um, and I think that part is straightforward, um,
because then we're gonna take those notebooks and,
uh, e- evaluate them separately outside of the system.
So you don't have to deal with very much data entry.
The one thing you do have to confront if you wanna
work in groups is actually joining a group,
and that's the part on Canvas that I find quite strange.
So let me just walk you through what life is like
from my perspective in understanding Canvas.
Here, I'm logged in as an instructor and I can get
a feel for what you all see when I choose the student view.
So here I am in the student view and this is what I hope happens for you if you want to,
say, form a group for assignment one and bake-off one.
You go to People.
I'm not sure why it's called People,
I guess groups are made of [NOISE] people, okay?
And then choose the Groups tab,
and what it's gonna do is load a ton of pre-created groups.
That's the least intuitive part of all for me that you guys
can't just create your own groups under the rubric of assignment one,
but rather that I have to create hundreds of empty groups for you,
and then you can join them on your own.
But what- and I see that people have done it, so I'm reassured.
Like, for example, this is a group that has
currently one person in it and this one has two.
Um, now I can't verify that this works because it says group is not available for me,
and I did in a panic Google around and I discovered that I think that's
because I'm merely a test student who can only aspire to be a real student,
[LAUGHTER] and a test student can't actually join a group.
But I am reassured that a bunch of people have,
and so my assumption is, like,
if a few of you wanted to start your own group,
what you would do is scroll
this endless list until you found one that had the right name,
like, Assign 1 and Bake-off 1,
or Assign 3 and Bake-off 3 in the future.
Claim it for yourself so that you can all join and then you've formed a group.
So I hope that works out for everyone,
and I'm just recognizing that it's a kinda strange system.
Let us know if you see any unusual behavior for this.
But once you've done that, then again,
I'm reassured that if I had joined a group when I went to the Assignments tab,
it would prompt me if I wanted to
submit Assignment 1 and it would give this warning, like,
"Hey, just so you know,
the work you're about to submit is being submitted for your full group."
It's taking a long time to load which leaves me a little bit nervous that
lots of you are downloading the data distribution file right now,
in which case the Inter- Internet might slow to a crawl.
But in any case, that's what would happen.
Uh, and I'm hoping it works out.
And the other thing that I wanted to mention is the reason that we created that
we want you to get different groups for the different assignments,
and then the final project is just that I couldn't tell from the system whether or
not if you formed a group at the start you could
change it without having things go crazy.
And so I thought, the worst case scenario,
people just reconstitute the same group for each assignment and then the final project.
Um, maximum flexibility in it just means that at
some point you have to do a whole lot of scrolling.
Okay. I'm gonna give up on that,
but I- I think it's going okay.
Let's get oriented a little bit.
So here's the- the main page for the course
which you can find by Googling CS224U Stanford.
Nice addition, we have a calendar now that shows all the office hours,
that's a more intuitive interface.
And it also is a reminder here.
You can see that we have a special session on Friday 3:15,
uh, a kind of refresher or an intro to Python and Jupyter Notebooks.
Uh, and Lucy and Ashcan,
two of our teaching team are doing that.
And I took the liberty of posting
Lucy's Jupyter notebook tutorial here which is nice and interactive.
I think you could use it with them and- on Friday and then going forward.
It has lots of great tips about how to use notebooks.
And you can also see everyone's office hours
including- and it's gonna have office hours on Saturday.
So if you're working over the weekend he's
available in the- in the Huang auditorium basement.
Another quick update. So I did scroll through this last time.
You can kind of see our units,
we're gonna start building a foundation for
NLU today and then we kind of branch out and explore
some tasks largely in the service of giving you
a sense for what different models you might use for your own projects,
and to start building up some best practices.
The one scheduling changes, um,
I know a lot of you are probably thinking about
going off into industry and maybe doing NLU in industry.
And so I had scheduled a day on May 22, where I was gonna have, like,
a panel of people come in,
who are people who are doing NLU in industry now.
And just kind of interview them and get questions from you all about what
their experiences are like to give you
a sense for what's happening out there in the world.
I should have predicted this but it's very hard to schedule people.
Everyone is very busy.
And so I think what we're actually gonna do is
have two of those panelists come on April 24,
and that'll be Marta Recasens,
who was a post-doc here and did beautiful work on co-ref.
Very linguistically informed, but also successful on the engineering side, great work.
And she's been at Google for a long time, having gone kind of in and out of academia.
So I think she has a really interesting story.
So she'll be here on April 24,
and she'll be joined by Guillaume Genthial,
who was a master student here and is now working in
industry doing things like NLU for healthcare.
I've worked quite, quite closely with Guillaume.
And he's also got an interesting story to tell about why he
decided to stick in industry as opposed to pursuing a PhD.
And another thing I like about Guillaume's profile is
that he's been very helpful to me in producing
lots of notebooks as you can see here
that kind of show you how some of these interfaces work.
And so for example,
if you look at the TensorFlow code that I distributed for this course,
you can see that I have, like,
shamelessly ripped off all of his best practices and tips and tricks for doing,
uh, TensorFlow with these new estimator classes.
So I feel indebted to Guillaume.
So they'll be here on April 24,
and then I think we will have a similar panel discussion in late May on the 22nd.
And I think I have successfully guilted
Bill into being one of those panelists because Bill has
an incredibly interesting story where he started
out as a philosopher and did all this stuff in finance,
and then did a PhD.
And now he's played essentially every role you can
play doing things related to NLU in industry.
So it'll be Bill and maybe somebody else.
Uh, I'll tell you more about those things in the future
and I think I'll be soliciting questions from you all that we could,
uh, could ask these panelists.
Let's see, I have a few other assignments,
uh, oh, sorry, other announcements.
Um, I hope you've gotten set up with the course materials.
There's a notebook here for doing that,
and also I'm thankful to [inaudible] for posting
a really detailed post on Piazza about how to get set up with things.
And we're going to try to continue to do that kind of FAQ for the assignments and
other important aspects of the course because Piazza can be hard to browse.
There's like lots of times in my life where I've said, "Oh,
this student is the 12th student asking the same question," and then I
can't find the 11 other posts about exactly that topic on Piazza,
and I feel sympathy for people just doing fresh posts.
We're gonna try to gather together all of that information so that it's available to you.
Um, another announcement Ignacio wanted me to make.
We're gonna update the setup instructions for people who
might have access to GPUs on their laptops,
like if you have a Linux box.
But I think the primary advice is,
if you want help getting set up so that you could take advantage of your local GPU,
come to Ignacio's office hours.
There he is. He's happy to help.
He says it's inevitably gonna be a mess,
but in the end,
you can probably get it working together.
And just a quick note about that,
I am working very hard,
shamelessly asking as many people as I can about
getting you cla- cloud credits for doing projects.
Um, more, more details to come on that.
Okay. I think that's it for announcements.
Any questions or comments before I dive in? Yeah.
What were the differences between Assignment 1 and Bake-off, are they the same thing?
I wanna leave some time at the end to talk about this in
a little more detail but the gist of it is,
the assignments are nine points and they involve, er,
each one of them involves setting up some baseline systems for a task,
and then developing your own original system.
Then you enter that system into the bake-off
and you get an extra point for just entering,
and you get some extra credit if you're the top-performing system in the bake-off.
Um, so it's a kind of little project.
The assignment plus the bake-off,
is a kind of little project and we're trying to exemplify best practices as we do
that with the chance of having a little win. Yeah.
I don't understand the screen-cast sound?
The screen-casts are videos that are on YouTube that I think of as like,
the essence of what I wanted to say.
Like just the really important details.
So I'm gonna talk for example a lot about vector comparison and re-weighting today.
If you want the fast version,
you could go to those screen-casts.
It's like I've tried to distill it down and do some example calculations.
And I'm glad you brought this up.
The way I think about this unit here right now is,
we're on April 3.
We're doing distributed word representations.
I'm gonna be working with these slides
today and next time and probably a little bit next week.
Simultaneous with that, you could explore
these notebooks which also reinforce the material.
But the nice thing about them is they mix everything with code.
So you could be hands on all the way through in a way that you just can't with a lecture.
Um, and then you could reinforce that with these screen-casts if you wanted to.
Um, we have a lot of coverage for thi- this first
unit because I think that the backgrounds for you all are very different.
I think some of you are in like
your fourth AI course and have done lots of projects and some of you are basically new
to the field and this is a way for us to kind of
get everybody well onto a solid foundation.
But, like kind of officially none of this is required.
You should just do what you can.
We're making a lot of materials available for you to get,
to build up that foundation.
Anything else before I begin?
Then the slides for the,
for today are posted here,
and they should carry us through all the way to next week as I said.
The reason you might want to download them is,
that I am going to encourage you at various points to do a little of,
to do a little coding on your own.
So if you do have the data distribution and you do have all the code set up,
then I would encourage you to like open a terminal or a notebook if you
want and paste in code, and then maybe play around with the code.
Okay? Just because you know,
it might lead to new questions or give you a better feel for what I'm talking about.
And I hope you do a lot of that as part of solving the homework problem one.
And then just generally thinking about these issues.
Alright, so but let's begin with this content.
So this is the unit on distributed word representations.
This is a big slideshow,
but you can navigate it by clicking around up here,
if you have it locally and it's pretty intuitive. This is our plot.
We're gonna talk about matrix designs, comparison, re-weighting,
do a little subword modeling and then move on to
dimensionality reduction which here is going to encompass LSA,
GloVe and Word2Vec.
To give you a sense for kind of the things people are doing in that space.
And then the final unit is on retrofitting where you might go beyond
just co-occurrence and bring in
some more structured information into these representations.
So I hope that makes it easier to navigate with what is a very large slideshow.
Yeah, that's the kind of the plan here and you can see
it reflected in that plot on the top.
Here's the big idea that you need to get used to if you haven't already.
And I think this idea is like,
incredible and subtle and interesting.
So you can always just continue to meditate on how and why it works.
But the core idea is that if I gathered together into a matrix like this,
a whole bunch of co-occurrence information about words from large collections of text,
then the resulting representations,
we're going to think of them as rows here in these matrices.
Those representations latently contain
lots of important information about linguistic meaning.
[NOISE] Some of you might ha- already be familiar with that idea,
for some of you that might sound really strange.
I can still remember it sounding very strange to
me and I'm gonna try to make it seem less strange to you.
But yeah, this is a co-occurrence matrix.
This is like our starting point for the whole lecture, right?
It's a word by word matrix in the sense that it's just counting the cell values here,
or the number of times that those words occur with each other.
And the- my pitch to you is that latent in there
is a whole bunch of really interesting semantic information.
And just one other way to plant this idea,
here's a kinda little thought experiment for you.
I've called this a hopeless learning experiment.
On the left, I have a little labeled data set of
words and you can kind of see that 0 for
that class value means a negative word in terms
of sentiment and a 1 means positive sentiment.
And that seems perfectly sensible.
[NOISE] If you were to try to generalize that information
to this data set over here where I haven't told you what the words are,
of course, this is completely hopeless.
No matter what model you have,
there's just no way that you're gonna be able to make
predictions about these anonymous new cases.
Right? Compare that with this much more promising learning scenario.
So over on the left, I have that same labeled data set.
Except what I've done is represent each word just with some numbers
that represent the association of that word with two other words, excellent and terrible.
And the kind of plot for this course is,
I might have started with the co-occurrence counts and had done a bunch of
massaging and stuff and gotten down to more interesting values.
And having done that,
it doesn't really matter what your learning model is.
You can just intuit that like hey,
if a word has 0 class,
it likely has a high, terrible value, right?
Those are all these values here,
and a highly negative excellent value.
And then when I switch to the positive ones,
it's the reverse, right.
They tend to be strong positive and negative associati- association with terrible.
And once you see that and you can easily imagine that
a machine-learning model would see this more or less instantly,
then when I give you this generalization problem,
it's trivial to solve, right,
and I think that's one of the ways in which you can start
to see that latent in that co-occurrence information.
Especially if I do some work,
there might be a lot of important information about meaning.
Yeah, so these are high level goals.
Let's start, let's start thinking in a deep way about how
these vectors actually encode the semantic information.
We're also going to build up some foundational concepts
that will be with us throughout this course.
And that's gonna be a useful foundation for the deep learning models that we develop.
And also for you,
if you're maybe just thinking about word meanings,
I think this is a great tool kit for that.
And then down the line what,
what you might end up doing with the stuff that I introduce now is,
kind of thinking about how these representations are interesting for linguistic problems,
or social problems, or whatever you decide to
tackle, or just feeding them into other models.
Because they are a great starting point for the reason that I just showed you.
They end up encoding a whole lot of
semantic information and your model starts in a really interesting place.
[NOISE]
Here are the associated materials which have kind of accumulated.
There are three notebooks,
and they're kind of backed by this code
module that I'm gonna encourage you to play around with,
uh, as I talk.
I'm gonna try to leave some time today to talk about homework 1 and bake-off
1 to kind of reinforce the connections and give you a feel for how that will work.
And as I mentioned before, there are a bunch of screencasts for this.
And I would think these are the core readings.
We've posted some other ones, but Turney and Pantel is a great kind of
compendium of ideas and insights about these vector space models.
Um, Smith is a newer paper that's kind of
an informal introduction along the lines that I just gave you,
like why does any of this work?
Uh, and then Pennington et al is the GloVe paper, we'll probably talk about GloVe next week.
And then Faruqui et al is a paper on
retrofitting which I think is a really inspiring idea.
So I don't know,
experience all of this through this week and next week.
Some guiding hypotheses, I guess I just feel obligated
as somebody giving this lecture to give this Firth quote at the top.
How many people have heard that before?
It's a kind of canard by now.
You shall know a word by the company it keeps.
Uh, Firth has lots of quotes like this.
The complete meaning of a word is always contextual,
and no study of meaning apart from context can be taken seriously.
I also have a quote from Wittgenstein,
the meaning of a word is its use in the language,
who knows really what Wittgenstein meant but it sounds really interesting [LAUGHTER].
I also have a quote from Zellig Harris.
He was an American structural linguist, uh,
strongly believed in this distributional hypothesis.
Distributional statements can cover all of the material of
a language without requiring support from other types of information.
That quotation and actually also these Firth ones, just as an aside.
These are kind of interesting because these linguists, um,
philosophically were associated with this tradition of nominalism which was kind of
the view that you could only trust things that were more or less in the physical record.
And so the one thing they felt they could trust was the stuff that they would see in
corpora which really comes down to just the distributional things that they can observe.
So I have no idea what they would make of the models that I'm presenting you today.
Uh, I have a feeling that they might be quite at odds
with their philosophical position but at least superficially,
it sounds like they're on our side.
[LAUGHTER] Uh, and then Turney and Pantel really
argue- articulate the modern version of this hypothesis.
If units of text have similar vectors in a text frequency matrix,
they tend to have similar meanings.
That's kind of our guiding idea for now.
Also by way of sted- setting the stage and gi- giving you
a sense for why this lecture has the thought that it does.
I just wanted to walk you through this overarching set of ideas here [NOISE].
So if you're gonna build one of these models,
the first choice that you might make is what your matrix design is gonna be like.
And I'm gonna show you a few but think like,
word by document, that would be words along the rows,
the columns would be documents,
and the cell counts would be the number of times
that those words appeared in those documents.
So that would be like not too tall,
but incredibly wide, if you had a really big corpus.
Word by word is the one that I just showed you.
Those are the two most familiar designs but you could also
have a whole bunch of other notions of matrix,
because you can have different notions of context along those columns.
And those will give you very different results, right?
Like this is a really fundamental issue,
uh, for building these representations.
But that's not really the first choice that you make
because to build one of these matrices,
you make lots and lots of decisions about how you'll tokenize, whether you'll annotate,
how you'll chunk up units of text,
whether you'll do some feature selection for your vocabulary,
and on and on, right?
So these are all design choices that you have to make.
Most feed into the matrix design.
And then you might wanna do some re-weighting,
because as you'll see today,
the raw counts are not so good as representations for meaning.
You have to do some massaging as I said.
And so the first way that you might massage is by doing a
re-weighting to kind of amplify the important things,
and diminish the things that aren't so important.
And you have lots of options there.
Oh and I wanted to say [NOISE] if you're experienced in this space,
a kind of interesting thing that emerges from this whole unit
is that the star of our show is pointwise mutual information, or PMI.
It's incredible how often the insight behind
that re-weighting scheme emerges in the models that I'll present.
It's like the hero of the story.
After re-weighting, you might wanna do
some dimensionality reduction and again you have lots of options.
Latent semantic analysis is one that I'll show you,
but you might have heard of some of these others like latent Dirichlet allocation,
or principal components analysis, and on and on.
I'm not so concerned about these acronyms,
I'm just planting in your heads the idea that if you're building one of these things,
you have lots of options.
And then finally, you have to decide what vector comparison method you're going to use.
That's gonna be like your fundamental notion of what it means to be similar.
And if you believe that Turney and Pantel quote,
that's pretty central here.
And different notions of vector comparison are
gonna give you very different notions of what it means to be similar.
One problem in this space is that basically,
you can choose from any one of these columns and get some kind of model.
And there's not a whole lot of guidance in
the literature about what's sensible and what isn't.
Um, so this is like you're kind of looking at a methodological disaster zone, right?
Because I'm not giving you much guidance about what to do,
and there's untold number of things that you could try.
I will say that in the time that Bill and I have been teaching this course,
there have been so- there has been some real progress on this question,
I've kind of noted that here.
Models like GloVe and Word2Vec which I'll show you later,
they kind of attempt to unite or like
take care of re-weighting and dimensionality reduction.
And some of them even dictate what your matrix design should be like.
And that's a kind of like a way of saying that it's one stop shopping, right?
If I choose GloVe,
then I don't have so many design choices.
Or if I choose Word2Vec,
I don't have so many different options to explore.
And that's interesting you- and actually some of
them might even diminish the importance of
which comparison metric that you use because of the scaling that they do on the values.
So there has been some progress but still you'll find actually
the bake-off and Homework 1 are trying to get you to confront this.
There's lots of stuff that you can try.
[LAUGHTER] Uh, and you might feel like you're doing it kind of blindly. Questions so far?
That's kind of my way of introducing this stuff.
That's my framework for these lectures.
Okay. So let me just show you a few of these matrix designs,
um, because already this is important as I said.
So here's that word by word one.
And I would say that its most salient property is that it's very dense.
Um, if you have a large enough corpus,
most words will tend to co-occur with most other words,
and so you get this relatively few zeros.
I mean there aren't a whole lot of zeros here,
in a re- in a real matrix,
there'll be a substantial number of them.
All of these things are pretty sparse,
but this will be like the densest of all.
Word by document, that's another common thing.
Words along the rows,
documents across the columns,
that will be much sparser of course.
And if your documents are short,
it will have an incredible number of zeros in it.
Very different from the word by word one.
Sorry, someone, yeah.
Yeah, just a small question.
Like in the last word to word matrix,
So when we say, there's a column against and row
against that would mean the count up against in all documents that are here?
Yeah or some multiplying version thereof depending on how you built the matrix.
But yeah, you could think of these- the diagonal as giving just the token. [NOISE]
Okay.
Yeah. [NOISE]
Right. This one is kind of nice because [NOISE] it's not
gonna grow very rapidly even as you introduce more data.
The worst-case scenario here is like large vocab by large vocab.
So like 100,000 by 100,000,
but you might have underlying that a billion documents, right?
Whereas this one is gonna get big very fast.
On the other hand, this one will be sparse.
So you might have some interesting ways that you could represent it in a way
that actually gives you some gains. Yeah.
[inaudible] they're like neighbors or would it be,
based on like [NOISE] neighbors based on like arbitrary values?
It's a, uh, that's a great question.
I wanna return to that [OVERLAPPING] because it's another one of these design choices.
What you mean when you say co-occur?
Yeah.
Uh, let me just show you
a few more matrices [OVERLAPPING] and then we'll, we'll get to that.
Uh, here, I just wanted to give you a sense for
creative thinking around this, this issue.
So this is a word by discourse context matrix.
In the Switchboard Dialog Act Corpus,
you have acts of dialogue and they've been labeled with things like,
this is a question, or this is an introduct- interjection,
or this is a back channel.
And there are, those are all these funny symbols here.
And so I could create a matrix of words by
dialog acts and that would give me
a very interesting perspective on the usage patterns for words,
very different from what we saw before because the notion of context is so different.
And here's another one that's kind of unusual and this one is less distributional.
So this one is, uh,
the linguists will like this one, I hope.
This is phonological segments along the rows.
Those are symbols from the international phonetic alphabet.
And then they have their feature representations along the columns.
Like, linguists do things like saying,
this segment is plus voicing or minus voicing.
And so that would be a column there and you can,
and they have lots of these.
I guess there's ah, uh, 28 different dimensions that this fo- font,
uh, this linguist has measured.
So it's not distributional,
it's more like, uh, a feature representation.
And here's a picture of it visualized and you can see that it's really good.
Even if you don't know this alphabet,
you can kind of see like all these nasal N type things are up here.
[NOISE] and [NOISE] differ only by their voicing and they've been clustered together,
but otherwise, they're very similar.
Here are all these rhotic sounds and, and so forth.
And that's a nice transitional idea,
what I'm showing you here because what I showed you
before is all about co-occurrence in large corpora.
This is more like a careful analysis of these phonological segments.
And that's a nice reminder that when you think about these vector space models,
you're actually doing something that's pervasive in all of science,
which is taking natural objects and
representing them with a handful of features that you could measure.
I've given some other examples here.
So I might just as, um,
somebody doing more hand-crafted work say,
the movie was horrible.
Is this vector representation?
Which is just counting some abstract properties of that phrase.
Um, or this is even clear, like,
you might model a human being like a subject in your experiment as a vector 24,
140, 5, and 12.
And that's just a reminder that you might know that
the first is an age and the second is a weight,
but it really has that meaning only because you're embedding it in
a larger dataset where the first column is the age and the second one is the weight.
And that's the sense in which those numbers have any meaning at all.
And it's exactly the same when we think about these co-occurrence matrices.
The vector representations acquire their meaning
because they're embedded in a larger matrix,
and it's essentially about those comparisons along
the column that you get meaning coming out of the dimensions.
So the, the really special thing about
the models that we're exploring is not that they're vectors,
but rather that the vectors are coming from co-occurrence counts.
Maybe that's the idea that you really need to get used to,
which is that somehow just from seeing all those associations and enough text,
you can extract what we think of as a semantic meaning.
And here, I've listed just a bunch of other designs by no mean- means exhaustive,
but some of them are kind of unusual.
Uh, and again, I'm just trying to hammer home the point that this is a big deal.
Like, again, if you're thinking of building a social sciences project
on top of these representations or you're
thinking about developing machine learning models,
this is your first choice and it's going to have
really large impacts on the subsequent results.
Uh, and there's lots of options that you can pick from.
[NOISE] Let's get to that.
The final thing I wanna address just for this unit
is the question that you raised before which is what we think of,
what we mean when we say co-occurrence.
[NOISE] And I've kind of broken it down into two things, window and scaling.
So just by way of making this concrete, here,
I have part of the first sentence of the novel Finnegans Wake,
which I chose because the first sentence is also the last sentence.
And so who knows where that novel begins and ends.
That's kind of cool for co-occurrence.
[NOISE] But just some abstract text.
And what I've done is picked "to" as our focal word.
[NOISE] In fact, we wanna process the whole text.
So they're all going to be focal words at some point,
but it, let's just say we've,
we've homed in on "to" and I've numbered going out from there.
[NOISE] Let's suppose that our window is 3.
We could impose a hard window,
[NOISE] and what we would be saying at that point is that to
co-occur is to just be in that window.
[NOISE] And obviously I could pick different window sizes from 1 to,
it could be as high a number as you want it, right?
So that's a design choice.
[NOISE] And then another choice
that you might make is how to scale those co-occurrence counts.
So you know that you have co-occurrence within the window.
[NOISE] If I did [NOISE] what I call flat scaling here,
then I would be saying that each one of these is a 1,
standing for one co-occurrence instance.
Maybe that's the default idea.
[NOISE] But I could also do something more interesting,
which is to scale those values.
[NOISE] So I might say that to be closer is to be
more meaningful in terms of co-occurrence,
and as I get farther out,
those numbers should diminish because they're less strongly associated.
And this is a standard thing.
Like, 1 over n, where n is the position from the focal element,
that would give you this like really rapid drop-off in what it meant to co-occur.
[NOISE] And you can imagine mixing and matching these ideas, right?
If you set a really large window but you have
a scaling property like this, at a certain point,
it's gonna be like you're not co-occurring even if you're in the window,
whereas with flat scaling,
everything in the window counts equally.
[NOISE] Yeah.
Do you think what matters is what, what,
like right now shore and bend have the same weight regardless.
Do you think that's positional as well?
Like if it's after 4?
It's a great idea, I hope everyone heard that.
The question was, could it be asymmetric?
That is, could I scale it differently right and left, right?
And I think the answer is yes and it might be reflecting
a linguistic intu- intuition that you have
that maybe things that come after have a different,
different weight, different significance, yeah.
And the proof would be downstream in how well your representations performed.
Yeah. And you could have it scaled at different, different, um, strengths.
[NOISE] Yeah, I have a couple of generalizations to offer here.
Larger flatter windows will capture
more semantic information because
it's kind of like what you're doing there is just saying,
hey, you're in the same topical space as me.
Whereas if you pick small more scaled windows,
it's going to be much more syntactic and
collocational information because you're saying like,
what I really care about is that you're in this local linguistic environment.
And so those matrices will en- encode a lot of
idiomatic information and a lot of stuff that's just like,
hey, this adjective tends to modify this noun,
which will be a very different picture than the first one where you'll
get almost none of that information because you were saying,
hey, just being in this document with me is enough to count as a co-occurrence.
The other thing I want to say is,
even if you've settled these questions,
you probably still have some design choices.
So you're gonna have to impose some textual boundaries.
Like, you might say that sentences,
or paragraphs, or documents,
or collections of documents are
your basic unit, and that will interact with what you choose for the window, right?
So for example, if I was focused on swerve here,
then as I go left,
I just get a very narrow window.
[NOISE] But if I decided that my textual boundary wasn't that from but rather like,
stretched back into the other part of the novel,
um, then I would get different values for co-occurrence.
[NOISE] And I'm going to try to show you that all this matters.
And, and one way that you could start to get a feel for how it
matters is to do a little bit of coding as I talk from now on.
So I included these code snippets.
Actually, I think I wanna run them.
You should be able to paste them out,
[NOISE] copy and paste them out
[NOISE].
There we go.
And it'll take a minute to load.
While it loads, let me just say that he- here's the way I'm- I'm
trying to give you exposure to a bunch of different designs.
So what I have is two fundamental datasets;
IMDB movie reviews and Gigaword.
So IMDB, that's like user-supplied reviews of movies and Gigaword is newswire text.
So already, these are gonna be very different in terms of their semantic content,
and you expect that to be reflected in the- the representations that we develop.
What I've also done is try to pick two extremes.
So for each one,
you have one version where the window size is
5, and the scaling is 1 over n. By what I said before,
that should be a lot of like collocational syntactic information.
And you also have one where the window size is 20.
So that's really big,
and it's- the scaling is flat.
And if I'm right about these generalizations,
then you would expect that that's going to be a lot of kind of topical information.
Uh, you have that for Gigaword and IMDB.
Uh, I should say that all of these objects are Pandas DataFrames.
Um, in my experience, Pandas is great, it's fast,
it's really flexible, but it does have a steep learning curve.
I've tried in the notebooks to show you kinda how to interact with these dataframes.
If you panic for an object,
just two dot values and you'll be- have a NumPy array,
and maybe it will calm you down and you'll be able to interact with it more fruitfully.
But like for example, I could do this,
um, and that shows me a series.
What that's doing is showing me in series form the entire row for the word happy,
uh, and that's a bit weird.
So you might say, .values,
and then you get, you know,
the- the actual representation in a more digestible format.
Um, all of these matrices have- they're all word by
word and they're all just 5,000 by 5,000.
So that's a pretty small vocabulary.
I want you to be able to work with them pretty quickly,
and we can't have you loading massive objects into memory.
Um, but I think it's a pretty interesting vocabulary as you'll see.
So they- they are meaningful spaces to explore. All right.
Any questions so far?
Okay. So another major unit here is vector comparison.
You've got your design sorted out.
You- you've got a bunch of counts.
Let's say for now that it's word by word,
and you want to think about that core hypothesis,
that similarity in the space is gonna correspond to similarity of meaning.
What does it mean to be similar?
That will turn on what vector comparison method that you choose.
I'm gonna use this as a running example.
This is also the one that- that appears in the screencasts.
Um, there it is as a tiny little word by document matrix over there.
It could be word by word as well.
Uh, and I've also plotted it.
So here's A down at (2, 4).
B and C kind of keyed into document x and document y,
and the conceit of this example is that two things are happening with those counts.
So first, A and B are similar in the sense that if you mentally kind of rescale them,
then you can see that there's more y than x, right?
The- the numbers are larger for y than for x.
So that's the- that's the sense in which A and B are similar.
B and C are similar in the sense that their overall magnitudes intuitively are similar.
They both have pretty large counts,
whereas A is pretty small.
And you can see that here in the way they've been plotted,
like A is kind of lonely down in this corner,
and B and C are up here pretty close together. Make sense?
First distance measure, Euclidean distance.
This is the standard metric that you might think of if you looked at that example.
It's measuring just the shortest point
between A and B in the plane here for two dimensions.
So you get this dimension here and this dimension here.
And it's the sum of the squares of all these differences,
and then you take the square root of it to rescale it again.
And it's just reflecting what you might think from the picture,
which is that in Euclidean distance,
A and B are very far apart,
and B and C are very close together.
So it's already if- you- I mean,
the heart of this is that Euclidean distance is favoring the magnitude over
that more abstract notion that I pitched to you before
about A and B kinda being proportionally similar.
And that might make you kind of unhappy as a linguist because I contend that,
abstractly speaking, A and B are kind of like,
for example, superb and good.
Both are positive, but superb is vastly less frequent than good,
whereas B and C are kind of like good and bad.
They're very different in terms of their polarity,
but in terms of their raw token counts, they're very similar.
And I think this is borne out if you look at these matrices,
that just kind of having the same frequency is
going to give you very similar Euclidean distance,
and that might be at odds with what you want from semantics.
I think frequency is correlated with meaning.
I think it's not an accident that superb, and terrible,
are infrequent, and good and bad are frequent.
But I probably primarily care about that polarity,
that sentiment information, and so
Euclidean distance looks like it's not my choice if that's my objective.
But let's work on that a little bit.
So once- one way you could work on that is
by doing length normalization of these vectors.
And the way that works is you first calculate
this quantity for the vector that I'm gonna call the L2-length.
There's lots of names for it.
It's like Euclidean, um, norm,
and if you look on Wikipedia,
it's a- a whole bunch of names.
But there's the calculation there.
It's the sum of the squares of all those values and then you take the square root.
And what you do to length normalize the vector is defied- divide
each element in the vector by that normalization quantity,
and it gives you a new vector down here.
Here's a kind of look at how that happens.
So if I start with that original matrix,
I put in this column, the L2-lengths,
and then to get the new matrix,
you just divide every element by its corresponding row length- L2-length.
Here's what happens to the space.
So that's the original one on the left.
And when I length normalize,
they kind of all go up.
They're kind of all sitting here in this unit sphere now.
And now A and B are really close together,
and B and C are far apart.
Because what I've done with this L2 norm here,
is kind of abstract away from a lot of the information about the overall magnitude,
and that has pulled A and B together.
And what we're seeing is much more like their proportional comparison.
Cosine distance, this is like the workhorse of this, um,
this is the distance measure that everybody
uses if they don't mention which distance measure they used.
And I think there's a good argument for that because what cosine distance is doing is
simultaneously a Euclidean-like comparison but in that length-normalized [NOISE] space.
And here's the calculation here.
The part after the 1 - is cosine similarity,
but I've kind of regularized everything about this framework into distances.
So I do 1 - that value.
The numerator there is the dot product of
the two vectors and the denominator is the normalization factor,
and it's the product of the two L2 lengths.
So this is kind of like I'm doing
this dot product comparison but I'm also controlling for the length of the vectors.
This is a look at what the calculations are like.
A, B, and C are sitting there in their original positions,
but when you walk through these calculations,
you'll find that, um,
A and B are very close together,
0.008, and B and C are pretty far apart, 0.065.
So cosine in one step is kind of incorporating that length normalization,
you can see that there in the calculation and as a result,
it's favoring that proportional similarity,
that sentiment similarity that I was pitching to you.
Another way to think about this is after you first do
the L2 norm and then you do cosine distance,
you get exactly the same result.
I know it's small but I've walked through
the calculations and the point is just that you get exactly the same values,
whether or not you first do the normalization or not,
and that's because of the denominator there.
[NOISE] A couple of hands. Yeah. Go ahead.
Sorry, you said, you do the, the L2 norm and then you do the Euclidean distance,
is it the same as calculating [NOISE] the cosine distance?
I do [NOISE] cosine here.
Oh, I should have checked this, I meant to.
I think that up to ranking,
if you first length normalize,
then Euclidean distance will give you the same rank.
[NOISE] Yeah.
So tha- that's the sense in which, like,
if you start to massage the space,
your distance measure might not matter.
Because if I first normalized all of these vectors,
Euclidean and cosine are probably gonna be approximately the same,
although I should check on the exact relationship. Yeah.
Having normalized, now, we're sort
of highlighting the relationship between the value of the,
the x axis and the y axis,
[NOISE] and that I guess to some degree is trying to
give us the relationship between the two documents.
Is that the part that highlights the actual value of the sentiments?
Or where is this [NOISE] sentiment data actually coming from as well.
Well, I was sort of making up the sentiment part.
That's my just so story about these things and I do think it's realistic.
I've focused on the,
the rows not the documents.
So I'm sort of about how A and B are similar,
and how B and C are similar,
um, not so much the documents.
Although I think you could tell a similar story about the documents. Yeah. Go ahead.
[inaudible] that ratio like,
[NOISE] actually y would be half and then it would be about 2-3.
And then, C is the only case where x
has the higher value in the ratio than y. Is that what we're highlighting?
Yeah. That's kind of what I meant.
And um, I-, we can substantiate this in lots of ways,
but you just like, uh,
articulated the core insight.
You had like, B is the only one where the left value is bigger than
the right once I adjust for their overall values. Yes.
Thank you.
[LAUGHTER] And then what I'm showing you here
is kind of a bunch of ways that you could cache that out.
So one would be the L2 norm,
uh, and that does reflect that, right?
So um, well, I didn't make it so transparent,
but once I've normalized in this way,
you can see [NOISE] this one is bigger than this one and
those two are kind of very similar in their proportional values.
[NOISE] Other questions?
[NOISE] That's kind of the heart of it.
As I said, cosine is a good default.
I wanna show you a few more just because you might see them in the literature.
And again, there are some interesting things to say about them.
So another popular family [NOISE] of these comparison methods,
I've called matching-based methods.
So the matching [NOISE] coefficient is the one that I've given at the top there.
Um, and you're just summing up all of the smaller of the two values,
doing a point-wise comparison across the vectors that you wanna compare.
[NOISE] That's the matching coefficient.
And then, Jaccard, Dice,
and Overlap [NOISE] are all defined in terms of that matching thing.
So like, matching, matching,
matching in the numerators.
And then, you can see that what they're doing is just
normalizing the vectors in different ways.
[NOISE] But that's it. Yeah.
Is there a theory behind why cosine works so well or we just,
we just observed is what.
I guess we should be careful [NOISE] about what it means to work well
because if your goal was to really capture a lot of the,
um, raw frequency values,
then it would be a poor choice.
Um, I think that it works well for NLu problems because
primarily we care about what I'm calling the sentiment association.
We don't so much care that superb is less frequent than good,
we care about how they're alike.
And so in doing cosine,
you're kind of homing right in on that.
[NOISE] But it's worth mentioning that there's
a danger to abstracting away from frequency information.
And I'm gonna, you'll see that actually as we go,
especially through these weighting schemes,
that frequency is important in language because it's important,
and also because a lot of our datasets have mistakes in them.
[LAUGHTER] Uh, so infrequent events,
you don't want to amplify them too much.
Um, so I don't know,
I think it depends on the problem.
[NOISE] Yeah.
[NOISE] Can you, um,
explain a little bit more about the next slide,
what the actual matching method case,
[NOISE] the very first, um, algorithm.
It's the sum of all the smaller of the values.
[NOISE] That's sort of the beginning and end of it.
What, wha- what's behind your question?
I guess, why does that mean anything?
Ah, how could I substantiate that?
[NOISE] One way to think about this is that,
sometimes you see all of these metrics just defined for binary vectors.
And then for example,
what Dice reduces to is the intersection over the union.
And then it's giving you some abstract measure of overlap.
And then, you can see that in that context,
matching is like intersection.
Uh, but when you move to continuous values,
you can't just do a straightforward set theoretic intersection,
and the closest you can come kind of in continuous space is the smaller of the two.
Because that's like, if I have a 0 and a 1,
I'm gonna get the 0, uh,
and that corresponds to the intersection idea.
If I have two 1s, I'll get a 1.
And again, just like intersection.
That's the best I can do.
Is there any other, can anybody else
help to substantiate this? What's going on with matching?[LAUGHTER] [NOISE].
Maybe a concise way
to say it is that any dimension that has a 0,
it's gonna contribute nothing to the matching score.
And intuitively, there's no match because there's a 0 there.
And more broadly,
any small number that's close to 0 contributes little to the matching score.
That's right. [NOISE] Yeah.
Sorry, sorry.
Oh, yeah.
Um, so in the previous slide, you showed,
um, the co- the cosine distance between length normalized vectors.
Why did you, uh,
why do we try to normalize the length?
[NOISE] So why does length,
[NOISE] this one shouldn't be, right?
Affecting our notion of distance here.
[NOISE]
Let's see. Let me, let me see if I can unpack that a little bit.
So in showing you cosine with and without a first length normalization step,
I was just trying to get you to see that cosine is doing that length normalization.
That's what its denominator is doing.
And that's why it doesn't matter whether I first length
normalize or just skip that step because fundamentally,
it's building in that re-scaling.
[NOISE] That's all I meant there.
The broader question of why you're normalizing,
which you can see reflected here because I could pick matching and just use that,
or I could pick one that has some kind of normalization.
That's kind of like picking Euclidean or picking cosine.
And this gets back to this deeper issue of,
what am I doing when I abstract away from some of
the information about the overall frequency in the vector?
[NOISE].
Uh, oh, yeah. I saw your hand up and then we could go back to-
[inaudible]
It's okay. Yeah, go ahead.
[inaudible]
I'm not sure. I probably wouldn't use any of these.
You see, these are allowed for- [LAUGHTER]
We, we'll, we'll talk about this later.
I mean, you see them all over.
So presumably people have arguments for why they've chosen them.
Um, you do see Dice in the context of binary vectors.
And I can kind of see why that is because it's equivalent to the F1 score, um,
which is a very deep intuition about how we do kind of,
um, basic metrics in NLU.
Because it's capturing kind of your,
the ability of your model to reproduce maybe some human behavior in an exact way,
capturing both, like, all kinds of mistakes essentially.
And so the, it doesn't surprise me that you see Dice sometimes,
but for the others, I'm just not sure.
It might not matter too much.
Maybe that's why people just pick one.
[NOISE] Let me show you a few more.
And then I do have some generalizations that I can offer
about how these relate to each other and be a little more precise about that.
But just one more family that you,
that you'll encounter that are interesting to think about are
kind of probability-based comparison methods.
One of them is the KL divergence.
Um, and this isn't strictly speaking a distance metric because you're comparing
kind of the deviation from the reference distribution p to q,
and this is the value that you get here.
Oh, and I wanted to say this can be a little bit fiddly.
If qi there for example is 0,
then this whole value becomes undefined,
and so you have to do a step for any real-world application of kind of adding
some small value to all of these things, so that your calculations don't explode.
And that's one way in which KL is kind of fiddly.
And the other is just the fact that it's not symmetric.
So it matters whether you pick p as your reference distribution or q.
Here's an example of it happening.
And I have to remind you that because this is a probabilistic notion,
you have to first normalize all the vectors, that is,
sum up their values and then divide each value by
that sum so that you get a proper distribution,
which imposes lots of other design considerations.
Like for example, this has to be a bunch of,
uh, positive values so that the normalization step makes sense.
Uh, basically, you have to be thinking I'm gonna use this only if it makes sense for
me to think of these values as a probability distribution.
Uh, but if you're in that kind of environment,
then this might be a good choice.
I've given some calculations here, I know they're small.
I think the fundamental thing to observe is that
like cosine and like length normalization,
A and B come out very close together,
and B and C far apart,
and that's because of the probability step that moves them kind of,
abstracts away from their overall magnitude differences.
[NOISE] And then, here are the calculations,
just to demystify all this notation and so forth,
if you want to check that out later.
[NOISE] There are a bunch of
variants that might be interesting and are a little bit less fiddly.
So one of them would be symmetric KL,
the sum of the two values.
And then it doesn't matter which you pick as
your reference distribution, that's kind of nice.
KL divergence with skew gives you a parameter that lets you
control how much you trust the reference distribution versus the other one.
[NOISE] Uh, and you can kind of,
like, pull them together.
And then Jensen-Shannon distance is
a proper distance metric that is defined by this calculation,
which uses KL twice.
And a nice thing about this is,
you don't have to worry about that smoothing or anything.
Uh, and it's symmetric, it's a proper distance metric, right?
So this will be like less fiddly than the other options, especially KL.
And I, I guess my answer to why you might pick one of these is just
that maybe you're already dealing with values that you think of as probability values,
in which case, these are very natural metrics to choose.
[NOISE] Let me sum this up with a few relationships and generalizations.
Euclidean, Jaccard, and Dice with raw counts
will favor raw frequency over the distributional patterns.
That's my generalization there.
[NOISE] I showed you to number 2 before.
Euclidean with L2 norm vectors is equivalent to cosine with regard to ranking.
[NOISE] Oh, that's a firmer answer to your question.
I, I thought I knew that from somewhere.
I guess I learned it from Manning and Schutze's textbook.
[NOISE] Uh, so that's kind of nice,
because then once you've normalized your space,
it doesn't matter whether you pick Euclidean or cosine.
Jaccard and Dice are equivalent with regard to ranking.
You can probably see that if you look at their two definitions,
they just have slightly different denominators.
Both L2-norms and probability distributions can
obscure differences in the amount or strength of evidence,
which can in turn have an effect on the reliability of cosine,
normed-Euclidean, and KL divergence.
And then skew, which I showed you briefly is KL but with a step that gives you,
lets you give special credence to the reference distribution.
Any other questions or comments? [NOISE] Yeah.
If you have a sparse dataset that, in,
in which like one incorrect co-occurrence
might mess with the frequency of the overall dataset,
is there a default, uh,
[NOISE] like comparison method you should go to use?
[NOISE]
Again, you're asking a deep question, which is like,
what to do with the fact that we know these matrices have some problems.
[LAUGHTER] Um, I, I think,
let, I have some, a bunch of examples that key right into this issue that you're raising.
[NOISE] And I think it's not purely about the, um,
comparison method but also about what you're doing to the raw counts.
[NOISE] So let me show you some examples and then you
could reraise this question if it doesn't resolve them.
Um, finally, here, here are some code snippets.
Um, it's kinda small I guess,
but I hope you can read it locally.
So I've loaded this VSM,
that's from the course repository,
and it contains a bunch of functions that are
useful for all the stuff that I'm talking about now.
Um, and actually, this example is from the notebook here.
This is the, this is the space we've been working with A, B,
C. And I'm just showing you here is for example,
vsm.euclidean with A, B,
C. Looking at the A vector and the B vector will
give you their Euclidean distance as the return value.
[NOISE] Um, you could also do vector length.
You can do length-norm, cosine, matching, Jaccard.
That might be all of the ones that I defined in there.
[NOISE].
Yeah. And you could add [NOISE] your own if you want to.
I think I didn't do KL because it's too fiddly.
Um, but if you're interested in KL,
it's straightforward to implement.
Uh, [NOISE] good.
[NOISE] And he- here,
oh, this is, this is kinda more substantive.
So [NOISE] this is another set of code snippets.
At the top there, I just reloaded for good measure that imdb5 matrix.
And then I took the cosine distance of good and excellent and it's 0.96 or so.
And then I did the cosine distance of good and bad and it's 0.94.
That's actually not what I wanted, right?
Because I've been telling you that we want to
capture the sentiment similarity in these spaces,
and I think we can.
But so far, those two very similar words even with
cosine are farther apart than good and bad.
And another way you can see that the counts aren't so great,
you have this function vsm.neighbors and you can give it a word,
and one of these vector spaces as the second argument.
And it will give you a series that gives you
the ranking of that word and all its closest neighbors.
[NOISE] And the closest neighbors in imdb5 to bad are guys,
a period, taste, and guy.
I think that validates something I've been saying before.
Does any- anyone want to say, like,
do you agree? [NOISE] Yeah.
Should the punctuation be treated as words?
Punctuation shouldn't be treated as words.
Shouldn't, the punctuation should be treated as what?
I-I want to defend the idea that punctuation could be important.
So that's why I've left them in.
I guess we're both being provocative about this.
I would wanna say two things.
First, punctuation could be super meaningful like
exclamation mark, versus period, versus question mark.
That'll be important for lots of models but also if you're right and
the period shouldn't be important then I should. What's that?
I didn't say it wasn't important, sorry.
Oh you didn't say, okay. But if somebody said [LAUGHTER] and you see lots of people
with these stop-word lists where they're filtering off lots
of stuff that they regard as unimportant and a priori.
I'd say that's a failure, that your model should be
able to pick up on whether or not those things are important.
And that's why I deliberately left in a bunch of stuff that you might think is junk.
And I say if it's junk your model should treat it as junk.
If I think punctuation can be good,
what I just meant is that,
I told you that narrow window with scaling would capture a lot of collocational
information and it doesn't surprise me that bad guys is
a very common bigram in IMDB movie reviews.
Um, but it's also not such a happy picture here [NOISE] and I did
bad with Jaccard and it's a totally different picture maybe less understandable.
Uh, but in any case,
this is kinda of a cliffhanger for you, if things aren't looking so good, right?
With these raw counts.
Ah, but we'll improve on it [NOISE].
And let's see, I'm trying to,
yeah, let me show you one thing from this.
And then I want to talk a little bit about
the homework because then we'll have lots of tools under our belt.
So the next big unit here is, basic re-weighting schemes.
And again, there're goals of re-weighting as I said before we wanna amplify the things
that are important and trustworthy, this is relevant for a question I got before.
Oh yeah, for your question about idiosyncratic stuff we want the
trustworthy stuffs that's important and unusual but trustworthy.
And we wanna deemphasize the mundane and also the quirky, and problematic, and so forth.
Now, absent a defined objective function.
This is gonna remain pretty fuzzy.
But I think it's a pretty clear idea intuitively.
Um, the idea is that raw counts have important information in them but there are
kind of poor proxy for the things that we actually wanna care
about but we can find the relevant information if we look.
So I'm gonna show you a bunch of
these basic re-weighting schemes and give you a feel for the different properties and
what I suggest is a framework for thinking about what they're
doing is that you ask these questions as we go.
So first, how does the re-weighted set of values compare to the raw counts?
For example, if it was identical to
the raw counts it would be not such a very interesting weighting scheme, right?
Or if it was just a proportional re-scaling of
the counts then you might think why bother.
But if it gives very different values
from the raw counts then you might be onto something.
You might also want to ask how does it compare to the word frequency,
which is kind of getting back to this theme I was articulating before which is,
how much of these methods are just capturing the fact that
words have different rates of usage?
And then finally, what is the overall distribution of values?
You might know from other linguistics classes that,
the distribution of the raw counts is gonna be like this horrible Zipfian
drop-off where a few words have
very high frequency and lots of words are used hardly at all.
And you might also have an intuition that that's kind of
a terrible distribution for lots of different modelling that you might want to do.
So you might ask of your re-weighting scheme that it give you something better than
that crazy log-log Zipfian drop-off. Yeah?
[inaudible] what's the difference between this kind of re-weighting in like the scheme that we did earlier [inaudible].
Very similar.
Or something?
Yeah very similar. That's a good question.
Um, when we did L2 norm and when we did
probability distributions I think we were doing re-weighting. Yeah, they're kinda coupled.
Oh, and I articulated this as a value before.
I don't wanna do feature selection based on counts,
I don't wanna get rid of the most frequent words or you know use stop-word dictionaries.
Rather, I want the models to know that for this dataset it's not so
interesting to have used the word "the" but
maybe in another dataset that's an important property.
Oh, this is a good way to connect with the question you just asked.
So the most basic kind of re-weighting that we could do would be normalization.
And you've already seen two ways of doing that before.
L2 norming you know,
dividing all the values in the row by its L2 length and
probability distribution which just divides
all the values in the row by the sum of those values.
That makes sense, right? But it is a re-weighting scheme and
you can already think about how it's gonna impact, how it's gonna be relevant here.
Like you're going to lose a lot of frequency information and gain a lot
of, a really rich picture of how things proportionately differ from each other.
Observed over expected.
This isn't involves a lot of notation but I think it's a pretty intuitive idea.
I just wanted to be precise.
So let me just use row sum X i to mean the sum of all the values in row
i and column sum X j to be the sum of all the values going down that column.
And then some of the X is the sum of all the values in the entire matrix.
The observed over expected calculation just takes each value in
your matrix and divides it by this value the expected value.
I think stepping back from the calculations intuitively the way that,
what this, what this is saying is,
for a given cell value,
if I think about the probability of the column and the probability of
the row, that's giving me a default expectation for what that count should be.
And then I can compare that kind of default
null hypothesis expectation with the actual value.
And if the actual value is larger than I would
expect given those row and column probabilities,
I wanna amplify it.
And if it's smaller, I wanna make it much smaller.
And this is a kind of like that phrase I gave you before like amplify the important.
This is like amplifying the things that are departing
from my default expectation given the row and the column.
And you're gonna see that again and again through all of
these weighting schemes and on through GloVe, and Word2Vec, and all of
them is kind of like, null hypothesis based on the row and
the column and then a comparison with what you actually have. Yeah.
I think multiplying a numerator you're supposed to say something.
That's the joint probability.
An- and actually, like I haven't done this here but if you
think about the sums that you are doing,
like I have this count sum here and these two sums here,
that's actually once I've done, gone all through
all the calculations, it's gonna give me a probability value and then
the product is the joint probability and down there is
the full probability for the whole table which will be 1.
So here's an example of the calculations.
And I've done some color highlighting here just to
show you what quantities are involved like first,
compute the row sum and the column sum.
And then appear is the actual count.
And then this gives you that expected value down here.
One more thing, and I'm going to return to
this next time because this is all really important.
Pointwise mutual information, which I said is the hero of this whole unit.
This is just observed over expected in log-space. That's all it is.
And so PMI is also drawing on this insight that I have
a null hypothesis based on the row and the column,
and then I can compare that to the actual value.
It just puts them into log-space so that you see many more differences than you saw in
that original space. Yeah.
So previously, you mentioned that KL divergence is very finicky. Why does PMI work then?
Because PMI is very related to KL divergence.
Yeah and it's going to be finicky.
This is also related to your question.
You guys have the same intuition and I've given an example calculation here,
and what I've done is just highlight this value. So this is a 1.
It's maybe, maybe that's just a mistake in your corpus.
That's what I'm asking you to imagine.
But you can see that,
this value if I compare it to its row and column values
is gonna look huge compared to these other ones.
And so I formed this joint probability table with the row and the column probabilities.
This is kind of working off this version and not this one but these are equivalent.
And then when I applied PMI look how big that one has
gotten compared to all these other values and that's the,
like a standard property which I think you're keying into that.
Often, these log and log odds comparison values
really amplify very small values that you cannot trust.
And this is a problem with PMI is that it
does too much amplification of these very small values.
And you'll definitely see that if you use it, yeah.
[inaudible] values like that.
I think I'm going to opt for the latter.
If you look at the Turney and Pantel,
they consider the Laplacian smoothing which you mentioned and they also consider
a method of contextual discounting which is
like adjusting the rows and the columns a little bit.
Um, but I think that you know playing this forward
a little bit you'll see that GloVe is basically,
regularized reduced dimensional PMI.
And you might say "oh, well,
they just discovered PMI then."
But that's not quite true because what GloVe is
doing is all that stuff that you just mentioned.
And then you might have a hypothesis which I think is borne
out that it suffers less from this problem here.
I'm gonna stop here because I want to
take the last few minutes to just show you what the homework is like.
I think we'll do that again next time in a little more detail.
And we're gonna continue working through
this slideshow and call back on previous themes.
But this is a good moment to kind of plant in your ideas that I think you now
have all the tools that you need to make progress on this homework.
So I will just describe the task
basically and then we'll wrap up and we'll return to this later.
So the homework and the bake-off are based around
a classic task in NLU for these vector space models which is word similarity.
I've given you for your development phase, five datasets.
These are all kind of famous datasets from the literature,
um, they are human curated.
So like human annotation measures of similarity for pairs of words.
So they're all just these lists of two words and then a score.
And the overall task for you is to develop
a vector space model where when you do comparison between vectors,
the values that you get are proportional to,
directly related to the ranking that the humans have provided.
That is that they reflect those similarity scores from
these human curated datasets. That's the heart of it.
Um, I will say that there are kind of two notions at work here.
Sometimes the first three,
people call those relatedness tasks and the second two,
they call similarity tasks.
I'm not gonna make such a big deal about this,
but you can imagine that the authors of these datasets do make a big deal out of it.
Um, and you'll have to think for yourself about whether you want
your model to key into one or another of these notions.
So you'll do all your development on these datasets.
Um, and I've given you some information about exactly the metrics involved.
And then for the bake-off,
I'm gonna give you two new datasets that you haven't seen before.
Um, and you will have developed your system and you'll simply run that system on
those two new datasets and we're all kind of on the honor system here
that you don't do subsequent tuning to these new datasets, right?
The way the field works is we trust you to run it once and report that score, ah,
and that should give you a good measure for how well
your model generalizes to new notions of relatedness and/or similarity.
The notebook embeds all the readers that you need.
And if you've already downloaded the data distribution,
then you basically won't to have to fiddle with any of this.
You can just read in all of these datasets.
I do recommend looking at this code just so that
you know exactly what it's doing but basically,
it's just reading in these pairs of words with the scores.
There are all the readers there.
And then in the middle of this notebook I did a bunch of stuff again,
trying to illustrate best practices like,
if this were really your project,
you would wanna understand these datasets
and how they're related to each other and you would want
that exploration to inform how you talked about your model and your results.
And so I've just done a bunch of analysis unlike how the datasets relate to each
other at the level of vocabulary and at the level of the scores they provide.
So that's the middle of this notebook.
And you might want to fiddle around with this to get a feel
for where you can tweak your model to get improvements.
And then finally, at the bottom we get to the evaluation.
And this is the, the main thing that you'll want to use.
So you have word similarity evaluation which you know basically,
a reader is a dataset here and a df dataframe.
That's the vector space model that you created.
And then you can optionally change the distance function if you want.
And so that gives you a correlation coefficient
and the evaluation which has a bunch of information in it.
There's some stuff in here already for doing error analysis which I recommend.
And then finally, here's the full evaluation.
Full word similarity evaluation if you
just feed in the vector space model that you've built,
it will run it on all the datasets,
and it gives you scores like this for each one.
You can think of this as my baseline and it's truly horrible.
Um, so like for example,
all I did was read in giga5, so raw counts,
narrow window, heavy scaling,
you actually have a negative correlation with SimVerb dev and test.
And the other ones are pretty dismal.
So there's lots of room for improvement over that baseline.
What the homework questions ask you to do is build up some interesting models.
So first of all, just do positive pointwise mutual information which I almost got to.
I showed you PMI. So you would do that.
I'm gonna do LSA with you next time.
Explore different, a bunch of different values of LSA. We'll do
GloVe maybe next week, I'm asking you to do a little bit of exploration of GloVe.
All of this is just kind of coaxing you in directions that might be interesting.
So you produce a little bit of code.
T-test reweighting uh, that's not what I'm going to talk about in
the lecture but that turns out to be a really powerful reweighting scheme.
And if you stare at it you'll see that it builds on
this row column intuition just the way PMI does. So you'll implement that.
And then finally, you just develop an original system.
Drawing on all those ideas that you had
before, maybe you pipeline a whole bunch of these models.
Maybe you do something that I never even
mentioned, some crazy thing that you think will work.
And you can hammer away at those readers.
You can do as much evaluation as you want.
Don't overfit to them because I'm gonna give you different datasets to test on
but you know, do as much development as you want and you submit that.
And that's how you get 9 of the 10 homework points.
On the day that the submission is due which is April 15,
we'll distribute those two new datasets.
All you have to do at that point is run the model that you developed on those datasets,
and submit the resulting notebook.
There's actually what we want you to do is fill out
these cells down here but pretty much just submit the completed thing.
And then you've entered the bake-off.
So there's no real work beyond just, you know, you have to
like, paste in some code and then run your system.
And then we're gonna see what works best.
And once all the scores are in,
we'll announce the winner,
and then one of the TA's, or a pair of the TA's is gonna reflect back to you
everything that we learned from looking at your systems
and thinking about the results that we got in. With luck,
having done that, not only did we have a fun competition,
but we also learned something about what works and what doesn't
for these problems. All right.
Let's wrap up there and we're off to a great start.
Next time a lot more re-weighting [NOISE].
 Welcome, everyone.
I propose that we get started. There's lots to do.
Um, we're gonna continue on vector space models,
um, focused especially today on dimensionality reduction.
We'll look at a whole family of models for dimensionality reduction, and
kind of try to understand what they're doing
and how they're related to each other and so forth.
Stepping back for a second,
I was thinking that the ideal timing for
everything this week would be that we actually finish this slide show, like,
midway through class on Wednesday,
because what I'd love to do is just create some space in
here for people to do hands-on work with the notebooks, uh,
or especially with the homework,
because it would be wonderful if you all left here on Wednesday feeling like, you know,
your system was set up,
you knew what to do with the data,
you kinda knew the rules of the game,
you could very quickly run through
the homework problems to get thinking about your original system.
That's the ideal.
Um, I do want to balance that though against the fact that I don't want to rush,
uh, and in particular the questions from people last time were really wonderful.
They were very perceptive and kind of pushed us in just the right direction.
So I certainly don't want you to feel like you
can't raise your hands because we're not, in fact, in a rush.
This is just an ideal and we have to balance these pressures.
But just so you know,
I would like to create some space for
hands-on problem solving because I think that's really rewarding,
and we have a lot of the teaching team available to help you
out with exactly the problem that you have at exactly the moment you have it,
and I think that's a really great environment for learning.
So that's where we're headed.
Um, a few other announcements.
So we have a very accomplished teaching team.
Um, they have lots of different kinds of expertise,
and so a bunch of them have volunteered to do kind of short little lectures or
units that we sprinkled throughout
the course that I think are going to add various things.
So the- the first one- yeah,
the first one comes on May 1 at the end of our unit on natural language inference,
which is our first look at like really big supervised labeled data sets.
[inaudible] is gonna talk to us a bit about Lifelong Learning,
and I'll let him unpack that term for you,
but he's done a lot of work under that rubric.
That's kind of going to be giving you
some interesting machine-learning insights and
also maybe connecting with things from cognitive science.
So that's nicely timed,
and then a little bit later in the term,
when we talk about metrics and methods,
has volunteered to think about,
kind of, a next phase of that,
where we move beyond standard precision, recall, accuracy, time kinds of
things, and really think in a serious way about what it means
for a system to generalize in a useful way on a real-world problem,
and kind of introspect on what the systems are actually learning.
Uh, so that'll be exciting,
and that's nicely timed.
When we talk about contextual word representations in this class,
I am gonna talk about the models,
but we're gonna be, kind of,
focused on how those,
um, models and, you know,
pre-trained representations could be used as tools that would improve,
uh, whatever you're doing for your project.
That's going to be my pitch.
And as a kind of counterpart to that,
just gonna talk about another very familiar thing that might improve your systems,
which is just I give you a pretty long blob of text,
like a whole article or some kind of
piece of context for the problem you wanna talk about.
What's an effective way to turn that big blob of text into
a fixed-dimensional dense representation that you could use for another problem?
On May 22, again,
in the spirit of just helping you get moving in interesting directions on your project,
he's going to talk about data augmentation,
which is I guess like making maximal use of the data you have available to you.
And then, when you guys are thinking about actually writing up and presenting your work,
he's going to talk about new techniques that you might use to
kind of probe what might be a black-box model to figure out what it's learning,
and that could be illuminating just for you
as someone who wants to understand your own system.
But also, of course, that's wonderful stuff for
a discussion or error analysis section where we
kind of get a higher-level insight beyond the numbers into what your system is doing.
So I'm really excited by the sequence of
things because I think this is gonna really nicely
complement the core strand of work that we plan to talk about.
So thanks to the teaching team and lots more to come.
Oh and- and relatedly, um,
the Friday session on Python and Jupyter Notebooks, that was great.
Uh, thanks to everyone who participated.
The video and corresponding materials,
um, linked to them from Piazza.
So if you miss that session,
but you're fumbling around with your notebooks or with your Python,
do check out that stuff.
And we're also thinking of doing one, kind of,
a bit later in the term, uh,
that would be focused on NumPy for scientific computing,
that is kind of like vectorizing your code and also PyTorch,
which this year we're kinda pitching as the default choice you might make for,
uh, doing deep learning, and we've got a lot of support for that.
All right. That's some stuff to come.
A few other things that I just wanted to mention.
First, a bunch of people have asked about the role of the notebooks.
In my view, for this year,
what the notebooks do is complement the main lecture.
So what we are trying to do in here with you all,
is give you a guided tour of the content and highlight some stuff.
But I think the real work will start when you open up one of those notebooks,
follow that narrative and whenever you don't understand
something or you have an open question or something else you want to pursue,
you use the code embedded in that notebook to answer that question for yourself.
That's the kind of hands-on learning that I think can really push
you in new directions and that's what the notebooks are for.
They're also useful if you open up the homework notebook and feel,
kind of, at sea about how things work.
You can bet that the actual notebooks are meant to get you oriented and so forth.
So, but, they're meant to play that kind of informal role. Yeah. Go ahead.
At least for the notebook, we use, we read it, if we have questions,
but we don't need to read every line of code to try to understand what's going on.
Yeah that's fair.
I mean you must have questions,
because this is complicated stuff and you could spend
a whole lifetime meditating on any one point of it.
So my pitch to you would be when you have questions,
the notebook is a good way for you to begin to answer them for yourself,
and that's the primary thing that I want.
It's not so much about evaluating you or making sure that you do a particular,
kind of, reading or anything but rather just empowering you to try some new stuff.
And all of this is meant to give you a head start on a project or something like that.
And also as I said on the first day, to, kind of,
start to try to convey to you what we understand to be
best practices for setting up projects,
running experiments, dealing with data and so forth.
Um, if you are struggling with your setup,
you can post on Piazza and we might be able to help,
but there is some stuff that you could do on your own that might help with debugging.
And I think one powerful way that you could, kind of,
check the health of your class environment,
is to actually use the tests that are embedded in the GitHub repository.
There's a sub-directory called test.
You can see from all these files test_NLI and so forth.
We've got pretty good coverage across
all the core code and if you use PyTest, which is installed,
if you use Anaconda and you type PY.test and I like to do -vv because it gives you a,
kind of, health dashboard,
and you run it on one of these files,
it will run a bunch of tests,
and all the tests in these directories should pass and if they don't,
you might get an error saying like,
"Hey you haven't installed this library," or something and
that would be really helpful for people trying to debug with you.
And also I'm just kinda pitching
this, PyTest is a very easy way to write a bunch of tests.
You basically just name your method.
The first word should be test and then
PyTest will treat it as a test and if you look at my code,
you see you can get really good coverage really fast.
And one thing that might be of utility to you,
is that I have in test NP model gradients.
That's a kind of gradient checking infrastructure.
So if you write your own model,
like in pure NumPy,
then you could use that to check its health,
essentially the health of the back-prop.
That's it by way of,
kind of, semi-random announcements.
For the homework one, I have a few things that I wanted to clarify.
Um, let's start with the easiest stuff.
So I hope it's emerging by now,
and it's certainly going to emerge today that PMI,
pointwise mutual information is a very powerful insight,
um, related to all this stuff,
not only re-weighting but also dimensionality reduction.
And as a result, it's a very natural baseline.
If you were actually doing a project in this space,
then it would be nice if the main results table had a row that was just what
happens with positive PMI, or PMI, because you can bet that
the fancy thing that you're actually advocating for is in some sense related
to, or complements, or is in opposition to the PMI insight.
And so the reason for having that as the first homework is just to
push you to essentially fill in the first row of that results table.
From there, you know,
you're, kind of, exploring more advanced things.
We're gonna talk about LSA today,
and I'll talk about- I'll, sort of,
explain to you why you might wanna check a bunch of
these different k-values, and why that's gonna be an issue for you in all likelihood.
And then this final one is just the toe in the water on using the GloVe model,
which we're gonna talk about today,
to possibly develop some powerful vectors.
And I'm hoping that having done
those first three questions and thought about the rest of this material,
you've got some ideas for your original system,
which we'll enter into the bake-off.
Let's see overall rationale.
Any questions about that that have come up? Makes sense? Yeah.
Uh, I've two questions. One is like,
[NOISE] it's just hard.
[inaudible] kind of distance.
If you can interpret what that is or time distance.
It's a little confusing getting through this series when like
the columns or the words and the rows are like, you know,
either the window and like the scaling,
like what is that distance?
Oh, you didn't quite go where I thought you were going
because what I thought you were gonna say is,
if you give me a word by word matrix,
it's kind of interpretable what's happening because if I
look across the row with the different column values I can see like,
oh, this is the strength of association in
some sense with the word amazing or the word terrible.
And that's pretty interpretable.
I mean, we're talking about huge high-dimensional spaces,
so it's not like any of us has analytic command but that's pretty good.
And, but then once you start to do things like GloVe or LSA which we'll talk about today,
the columns become much less interpretable.
But you were actually at the first point feeling like you
didn't have the intuitions, is that right?
Yeah. [NOISE] It's like the column has like the window, right, of like how
[inaudible] of the distance vector between two columns.
How is that, what is that?
I don't know. I mean, the scaling thing I would separate out,
separate out and I would say scaling is you modeling your intuition
about how proximity to your focal word counts, for co-occurrence.
It could be that you count them all the same or that you feel like
being farther away is less of a co-occurrence.
Um, but once you've built the matrix,
you know you have this high dimensional space and for there,
I always do this but I kind of want to quote this famous, um,
quotation from John Von Neumann,
"In mathematics you don't understand things,
you just get used to them".
Uh, and I feel like a lot of this stuff is just about getting used to it.
And I've already pitched to you that
VSM.neighbors function as a way to kind of see what's happening,
um, and comparing across different matrices you've built will give you intuitions.
And I'm, we're gonna do some visualization stuff today that might also help.
But beyond that, you might just have to reflect.
Two points of clarification,
the first one is easy.
I threw into VSM data
this adjective adverb matrix which is
collected from Gigaword as adverb-adjective dependency pairs,
which I think is a really interesting notion of context,
very different from just raw,
like being in that, um, linear window.
Um, but I didn't mean for you to have to make
sure that your bake-off entry worked for that matrix,
it just won't, it's a completely different vocabulary and so forth.
So I added in just a,
a clarification that all I'm talking about is
the two IMDB ones and the two Gigaword ones.
Don't do the heroic thing it would take to
get the experiment code to work with that other one,
that's certainly not something I wanted you to construct.
Um, then more subtle,
maybe more worth discussion here is,
I added a note just clarifying being,
trying to be really explicit that for the bake-off,
you should not bring in external vectors.
Like you shouldn't just download the pre-trained GloVe or the pre-trained
BERT or ELMo or other representations that
you've heard about and enter them into the bake-off.
What I'm trying to do the spirit of this is to say,
"Start with the count matrices that I gave you.
So we all begin from like a level playing field, same spot",
and then do cool things to those matrices,
that's the spirit of this.
Um, it gets a little complicated because I want to encourage you to do retrofitting.
So I can't say something as clean as don't introduce any outside information,
because I think retrofitting to WordNet is a worthwhile thing to
do and I'm gonna talk about how you might do that in a little bit.
So I've done this thing of saying
no external vectors but I don't know that this is completely the right answer.
I mean part of me wants to just leave you completely unfettered to do
whatever you want beyond like figuring out what the test set is, right.
You know in the real-world if you'd entered a bake-off,
you could do pretty much whatever you wanted to do.
And here I don't know. This is an attempt to keep us all on the same trajectory.
But, I don't know [NOISE] I did download BERT and ELMo,
at least some small models and GloVe and see how well they would do in the bake-off,
uh they don't, they do okay, um,
but then again, I never win these bake-offs, so what do I know?
Yeah. I don't know, do people have thoughts about this?
You know, I don't know what the right answer is. Yeah.
Just a few questions not necessarily particularly on this one, but one of them is certainly related. In VSM 3,
the retrofitting one, it does actually download the pre-trained GloVe model and uses that.
Would that be something that we work- is that what you're [inaudible] be okay,
to use in retrofitting? [NOISE]
Um, so that's I would want to exclude is that you would use the GloVe ones but
that you could use the vectors from like Gigaword or ones that you had developed,
you could retrofit them.
That's the spirit of this.
But in doing that, you'd bri- be bringing in WordNet structure and
that's where I fall apart on this idea of not introducing outside information.
And then, but the real battle I'm having here is like,
maybe it's the right thing to do,
to download BERT and do some cool reweighting and
then learn a function that will do real- really well at this task, right.
That could be a, a scientific discovery or combining
BERT and ELMo and representations for
my matrices that you, you know, massaged in various ways,
maybe that's the right answer.
And sometimes, I don't want you to feel
constrained artificially because what's the sense in that,
on the other hand it would be kind of sad if off the shelf BERT ,
everybody did it and everybody got the same score.
Um, so I don't know, um,
I guess I'm the authority,
and so I've set the rules now and we'll see what happens.
For, for future bake-offs,
I don't think we have to play this game because I think I can more or less just say,
uh, do what you think will be interesting scientifically and successful empirically.
Yep.
The same question was you mentioned earlier not having to
manipulate the bake-off matrix for a T-test.
Uh, I don't think we have that matrix now currently, is that correct?
So, uh, I didn't follow,
you could- once you've implemented this T-test thing here,
you can run that on one of these matrices.
I think that's pretty successful actually.
Is that- yeah, that'll work.
So what were you saying with heroic effort that we didn't need to do? [NOISE]
What- when, I did say heroic effort but what was the context?
[inaudible]
Oh, for that, one other lonely duckling in the VSM directory,
it's called- I clarified down here.
If you have the latest notebook,
you can just read through this;
Gigaword NYT adds mod matrix.
That one is just there for fun.
Um, if you're a linguist who has studied
scalar adjectives then this is a particularly interesting space.
But for most of you,
it's probably just an aside here.
But it has the wrong vocabulary to enter the bake-off.
You'll get a whole bunch of errors,
if you try to make sure your code works with that matrix and that's what I was
worried about you all like battling against this when really I just meant to exclude it.
Thank you.
Sure [NOISE] One more thing about this notebook.
So this is a small change that I
pushed and this is so strange how the mind works I guess,
I woke up this morning just sure that I had made this omission and sure enough I had.
I don't know why it occurred to me that the point that it did,
but this full word similarity evaluation,
the change that I made is just exposing distfunc.
Before, it wasn't exposed and that meant that when you called
word similarity evaluation down here it was going to use cosine,
because cosine is the defa ult for that function.
And the reason that I'm shocked that I did this
is just that I had been quietly thinking that
a really interesting source of innovation for
this bake-off would be that people actually thought a lot about that distfunc there,
because you can use Cosine or Euclidean or Jaccard but all you really need to have in
that position for this code to work is
some function that will take two vectors and produce a real value.
And if you think about it at that level,
there's lots of stuff that you could do that would kind of go beyond the methods I'm
introducing but some of you have the background that could make that very exciting.
It could be a learn function of the data for example,
that you wrote yourself.
Uh, and so I was sad that I had not exposed this,
nothing will go wrong,
if you don't use my updated version but I kind of encourage it
because that's a point, something to think about.
That makes sense?
Good. There are just a couple more announcements that I wanted to make.
Um, again just by way of you getting you familiar with this- this is the- the repository.
I hope you all are working productively in this repository.
I just wanted to point out that kind of on the backend modeling side,
there are three groups of models.
There are these NP prefixed ones, that's for NumPy.
Those are simple versions of the models like autoencoder,
and classifier, and so forth.
Um, but they expose everything.
So if you really want to understand what's happening
under the hood when these systems learn things from data,
that would be a good place to look.
But if you just want to get some interesting work done,
then I would use either the Torch versions or the TensorFlow versions.
Um, I had kind of planned to just switch to Torch.
Ah, because I think the code is really nice and transparent.
And if you want to do modifications and so forth,
it's typically pretty easy and I'm going to ask you to do some of that later.
So I recommend the Torch,
but the TensorFlow is faster.
Um, and so if you need to get a big job done or you're just accustomed to TensorFlow,
then those are there for you as well.
And just conspicuously missing is the tree version.
Um, because at least as far as I can tell,
it's very difficult to do, ah,
neural network models with kind of arbitrary graphical structure in TensorFlow.
But in PyTorch, that's wonderfully transparent and easy.
So that's kind of what pushed me over the edge.
Relatedly, I thought I would do a shout out.
I just pushed some nice improvements to the PyTorch code, um,
that are thanks to who's a student in this class. So thanks .
I also wanted to thank the people who are helping me get credits.
So we have Google Cloud Credits that's posted on Piazza,
and I'm working with some of these other vendors to try to get credits for you,
ah, via them as well.
All right. Those are- that's it for announcements and prefatory material.
Any questions? Ask me anything before we dive in.
Oh, one more thing.
Uh, I made some updates to the slideshow.
So you might want to download the version,
download it from the web again.
Uh, mostly it's just additions,
but there were a couple of corrections.
Um, I guess this will be on the video.
I'm not sure if you're taking notes and you started taking notes on the first PDF,
this will help you merge the two having this list here.
Um, but otherwise, you can just download a fresh copy.
One of the imp- improvements is this slide 26,
and that's kind of where I want to start.
Um, but let me build up to that.
So last time we talked about matrix designs,
we talked about vector comparison,
and we started in on basic reweighting.
For vector comparison, the- there were kind of three big ideas,
like three big comparison methods.
So first was Euclidean and Cosine.
And tho- you could call those kind of like the-
the classical geometric vector comparison methods,
both stated as distance, um, measures.
And then, I introduced all these matching coefficients, all- sorry,
all these methods that are based on matching coefficient like Jaccard,
and Dice, and Overlap.
And then, I introduced a family of like probabilistic vector comparison methods,
that seem like good choices if you are
dealing with things that you would call probability distributions.
So that was the kind of- kind of the plot of that section.
And I stated some relationships and
generalizations which you can use the code to check if you like,
and this was by way of trying to make sense of this.
Now, the other shoutout that I wanted to do is to [inaudible].
So ah, here now we're chatting after class,
we walked back to the department, and,
um, he was very supportive of course, a very gentle guy.
And I- I- but I could tell that something was bugging him a little bit.
And so we kept talking and I kept trying to figure out what was bugging.
And the one thing that I managed to extract from him that was clearly bugging him
is that I had called a lot of these things distance metrics, especially cosine.
And the way I stated cosine distance,
that is not a proper distance metric.
Uh, and I- so this is- this is true.
I kind of knew this. I had just not made much of it,
but I actually like this as a way of additionally like making sense of the options here.
So this slide kind of summarizes the argument.
To qualify as a distance metric,
a vector comparison method D has to be symmetric.
It has to assign 0 to identical vectors,
and it has to satisfy crucially this thing called the triangle inequality,
which is the distance between x and z has to be less than or equal to xy to yz.
And it's called the triangle inequality.
You can kind of see here like if this is z at the bottom, x and y,
it's just saying that these two things,
the sum of them have to be larger than the shortcut here. All right.
Very intuitive. Cosine distance as I defined it,
is not a metric because it fails that triangle inequality.
And you know it's very easy to find counter-examples,
and here's just one random one that I've found,
uh, that would show that as I defined it,
this is not a distance metric.
There is a way to correct that.
Here's the definition here.
I actually don't know myself why this isn't for example the version that's in NumPy.
Uh, NumPy uses the one that I introduced,
which I thi- which seems to be perv- pervasive.
But this is the correct Euclidean,
this is the correct metric version.
And if you want to think in these terms,
then this could help you make decisions,
because the only things that are distance metrics
of the stuff I introduced are Euclidean,
Jensen-Shannon, and cosine as it's defined in this new way here.
The things that aren't distance metrics are all those matching methods.
KL divergence because it's not even symmetric,
but also symmetric KL is not a proper distance metric.
And of course KL with skew which is just a kind of
weighting of the reference versus the target distribution.
That's just like KL, it's not a- a distance metric.
So if you find this- this
elegant as a classification and you want to work only with proper metrics,
then some of your choices become clear.
If you're in a probabilistic space, use Jensen-Shannon.
It's got all these nice properties.
If you want a more classically geometric notion,
use Euclidean or the corrected cosine.
And the nice thing about that that I think will emerge especially today is,
if your vector space is kinda properly normalized like for
example if you did the L2 norm of all the vectors,
then Euclidean versus cosine doesn't matter at all.
That's a point that I made before that actually I just wanted to verify this,
that you can check with the code that if I do, um,
cosine distance here, I get the exact same result
if I do the L2 norm and then use Euclidean as the comparison method.
That was one of those generalizations.
The numbers are different,
but the ranks are all the same.
And that's kind of like saying for what we care about,
there's no difference at that point.
And so that's actually kind of clarifying from this whole mess of things that I offered
kind of like by way of just helping you with
keywords you might encounter in the literature,
I think we can distill it down to some pretty clear guidance.
So thank you for being particular about this.
And then there were some code snippets here.
Questions before we move on.
That was kind of a recap with a little bit of bonus content to help clarify. All good.
We also started talking about reweighting.
Er, and I gave the goals here which is kind of
amplify the important and de-emphasize the mundane and the quirky.
We covered L2 norming and probability distributions as way to- ways to do that.
And then I kind of paused here and I said, observed over expected.
This is a really important idea because you see
this throughout all of these methods that I'm gonna introduce today.
Uh, and there's a whole mess of math up here.
What it's essentially doing this metric here is measuring
like calculating our expectation for the count based on the row and the column,
and then comparing that to the actual count.
And if our expectation is smaller than the actual count,
we should amplify that, that kind of thing.
There's a lot of math here. And so what I've tried to do is,
just I- I added this slide.
This is a kind of intuitive example of why these,
why these numbers are interesting.
So I created this fake,
this toy vector space here,
and the conceit of this is that the word keep is incredibly frequent.
It co-occurs with lots of other words.
So 20-20-20 and enjoy is pretty frequent.
And then if you think down the column,
this word tabs here,
my idea is that it's idiomatically related to keep, right?
Keep tabs as an idiom and tabs itself.
It does occur outside of that idiom, but relatively infrequently.
And surely because this is a kind of collocational idiomatic expression,
keep tabs should have a really high count there.
That's what we would expect because of that idiomaticity. That's the count matrix.
And when you count, when you go through the expected values here,
this is the full calculation and then you get these expected values,
you get the result that you were hoping for in the sense that
our expected count for keeping tabs,
just given the probability of the row keep and
the probability of the column tabs is like 13,
but the actual count is 20.
So it's o- it's occurring more than we would expect given this null hypothesis.
And correspondingly, tabs and enjoy occur less often than you would expect.
And that's reflected in the fact that the actual count is 1,
but our expectation is 8.5.
And then all observed over expected is doing- doing is dividing the actual count,
the observed count by
the expected count to kind of bring these things into a ratio space.
That makes sense? That's what we hope to see.
Yeah.
[inaudible] the larger number tends to like,
incur more expectation,
so why is it that we're okay with keep tabs being less than say keep reading?
Or is the metric preferring lower values?
Well, I think but that- that's kinda the intuition that keep tabs
and keep and reading co-occur a lot because they're both kinda frequent,
and it's no surprise that they occur together.
And so actually, having their expected value be
just a little bit bigger than the actual value,
you know, that's kinda good.
But keep and tabs, uh,
co-occur way more frequently than we would
expect given the row and the column and that's why we're amplifying it.
My question formerly, the 12.48 versus 23.76,
why is it that it's not higher than 12?
Oh well, maybe realistically it is.
It's just a toy example.
So if I had changed the numbers I guess I could have made it higher.
But this seemed pretty good to me in the sense that 20 divided by
13 is much different than 20 divided by 23,
just in kinda an absolute sense and that's the sense in which this is occurring
way more often than we would expect. Yeah.
So expected is basically what we would expect if everything was independent?
Yes.
And then our OE is a frequency independent,
um, frequency independent like,
evaluation of how often something happens.
Definitely right for the first part.
If you think about these in probabilistic terms,
then this is a kind of- that's the- the probability,
the joint probability that you would expect given that
the row and the column were independent in the probabilistic sense.
For the second part, I wasn't sure about
what you meant because it is dependent on the actual frequency,
that's the observed count.
So I guess you could take like one class of documents and another class of
documents, do the OE for them separately.
But then still take the OE that you get from the class one
and class two and compare them without doing any scaling.
Sure.
Yeah, yeah, yeah,
um, I just paused a little bit just because I do wanna show you
some stuff about how these OE values scale.
It's maybe not ideal but I think,
I think I understand your intuition. I'm inclined to agree.
I belabor this point in part because I told you that PMI was the hero of this story.
And PMI is just log-space observed over expected.
Um, where like in- when you normally see this calculation,
people have done what I've done here, which is,
first create a joint probability table and then do the calculations in terms of
those probabilities but everything
comes out the same as if you do it with the raw counts.
Um, but this is a kinda basis for
the calculation and then here's the PMI reweighted matrix.
Um, it's doing the same fundamental thing as OE but in log-space,
and I highlighted these orange values because you can
see that this is prone to exaggerating very small counts.
The one over here,
because it's kinda lonely in this row and this column,
ends up really big in the final matrix.
Even though, if you've worked in NLP for a while,
you might think that 1 is probably not so important as an event.
[LAUGHTER] Uh, maybe in more precise spaces this 1 would be very exciting.
But for us it's like a mis-tokenization or some kind of artifact in the document.
I gave a few plots here just to kinda give you a sense for PMI values and how they scale.
You could, uh, check that out on your own.
This is an important, um,
extension of PMI, positive PMI.
[NOISE] Uh, this is actually the default for VSM
because it's kinda better behaved in certain respects.
Um, but let me articulate the argument for why you might
choose this positive PMI over regular PMI.
PMI is actually undefined when the count is 0,
because I have to at some point take the log of something that would be 0.
So the question is what to do in those situations
because we can't just leave them as infinite values or something.
So the usual response is the one that I gave above here kinda implicitly, right,
with log 0 = 0,
which is to map it to 0.
But so Levy and Goldberg have this nice paper about this general class of methods,
and one thing they point out is just that this is kind of incoherent.
And the argument is, look,
for PMI larger than expected counts get
a really large PMI and smaller than expected counts get a really small PMI.
That's perfectly intuitive.
But if we do this log 0 = 0 thing,
then the 0th count stuff gets placed in the middle of that ranking.
But we don't have evidence for it being in the middle of the ranking,
like, actually, we just don't know that's the point.
Um, and so there's kinda something amiss here.
And a response that kinda cleans this up is just to
say that for anything that would be below 0,
I map it to 0,
and that's the sense in which it's positive PMI.
Make sense? Yeah.
Could we just lose all information about elements
that are smaller than expected counts if we're mapping to 0?
Yes. Just to repeat that because I can't, I can't disagree.
You're losing a lot of information about all of those negative values.
[inaudible]
Just guessing based on not knowing
whether we have smaller or larger than expected?
It's a great question I guess,
I have to do this a lot especially in this unit.
It's an empirical question [LAUGHTER],
um, which behaves better.
I am gonna give you- try to give you some guidance around this
because obviously you're doing something pretty serious to
the underlying distribution of values and that could give you
some analytic guidance about whether this is the right choice or not.
Um, but fundamentally, I have to agree. Yeah.
How could the expected value be 0?
If it was just exactly the same.
So for O over E,
if it was just exactly the same as the actual count,
then it would be 0 in log-space.
Oh wait, it's like [inaudible].
And that would be kinda like, saying this is exactly what we expected.
Let me introduce a couple more and then we'll step back.
Another famous reweighting scheme that draws on
slightly different notions of row and context is TF-IDF.
This is like famous from information retrieval.
Uh, it's defined in terms of two things.
So first, term frequency,
which you would get in our, um,
vector spaces by dividing everything in the- I don't wanna get this backwards,
so you would normalize column-wise.
Uh, if you think row-wise, right?
So you would be getting column-wise term frequency
for each one of these columns or documents.
And then the inverse document frequency is essentially counting
up how many columns that term appears in.
Uh, and so what the- what the calculation is essentially doing
is amplifying the values for things that occur in very few documents.
And then the TF-IDF is the product of these two.
So the intuition is that you're gonna get
really high TF-IDF values for cells that have very high probability for your document.
That is, they're like over-represented along that
column and occur in very few documents, that is,
traveling along the row because that's like saying that that term
is really specially associated with that document.
Over-represented in it and unusual in appearing in it at all.
Let's say it's those two intuitions. All right.
Here's an example of the calculations just to make it really concrete.
You can do the IDF values and the TF values
and then you just take their product to get the full reweighted matrix.
And again, I have some plots that kinda show you how these values tend to scale.
So for example, setting aside the 0 case,
which is always a problem in these log, uh, models.
As I get larger and as I occur in more documents,
that's along this X-axis,
the IDF values go down and like the epitome of
a high IDF value is to occur in just one document because then it's like,
really specially associated with that one document.
And if it has high frequency in that document,
then its TF-IDF value is gonna be really, really large. Yep.
So for our homework,
I don't think we'll be able to use the TF-IDF
because our matrices are word-by-word, is that correct?
Well, technically you can,
nothing would stop you. It's perfectly defined.
However, um, it can be really problematic to
use these very dense matrices with, uh, TF-IDF.
They are really kind of expecting a lot of sparsity.
And in fact, if you have a word that appears in every document, uh,
which can- that is that it co-occurs with every other word,
like a stop word, then these TF-IDF values can get completely ridiculous,
or even be undefined.
Yeah. Um, well, I was kind of hoping you discover that for yourselves,
but now I have given that away.
Well, it'll save you some time.
Um, but for a very sparse word by document, for example,
this has proven time and again to be a really powerful reweighting scheme.
And a powerful way to find documents for
a given query that are really like highly associated with that query.
And here are a few others.
T-test is when you're implementing,
and I like that one just because if you stare at this long enough,
you can see that this is a kind of PMI intuition as well.
There are lots of TF-IDF variants.
And then another one that I could just throw out there
is I could create a pairwise distance matrix,
and that might be an interesting reweighting scheme,
where like all of these values in here are
the cosine distances between the various elements.
Let's step back for a second.
I gave you last time a little bit of a framework for
thinking about how- what these re-weighting schemes are doing.
Uh, you know, things you might ask yourself as you're making these choices.
And one of them was just that you should look at
the distribution of cell values that result from doing the reweighting scheme,
and think about what that might mean for your problem.
And here I've kind of mapped that out for the ones that we've looked at.
So up here, these are the raw counts.
And they're actually- like you can see that they're shaped like this.
Uh, it's actually much worse than this plot appears,
because I did the log scale for the y-axis.
Otherwise, it looked like everything was just at 1,
uh, and then, you know,
a long tail of things.
So that's a really strange, you know,
Zipfian or log-log distribution.
It's kind of hard to deal with for lots of methods.
And then we can look at what happens when we take that that incoming distribution,
and change it in various ways.
So for example, the L2 norm has more of a U-shape,
probabilities are kind of similar.
Although I confess that I don't have a firm grip on
why it has this kind of hump before dropping off here.
Observed over expected values have a similar distribution to the raw counts.
So that's maybe not so good,
because even though we're amplifying some stuff,
it's kind of like the underlying statistical problem remains.
[NOISE].
PMI, this is a kind of friendly picture because
it looks quite normally distributed. That's pretty good.
Uh, PPMI does the thing of just chopping off the left half and amplifying all the 0s.
Um, but it's still not as scary looking as for example the raw counts.
And then TF-IDF looks a lot like PPMI.
One more thing before I take questions is just that
beyond just staring at these distributions.
You can also, I know it's hard to read,
but you can look along the x-axis at the actual cell values.
So of course for the counts,
it goes from 0 to 100,000 or whatever,
um, for these word-word matrices,
whereas the L2 norm of course is 0 to 1.
Um, the probabilities are 0 to 1.
Observed over expected, that's kind of constrained in a space,
like, so even though it looks like a scary distribution,
the x-axis is much tighter.
For PMI and PPMI it's like here, -10 to about 20.
So not as normalized as probabilities or L2 norms,
but at least constrained.
And then the TF-IDF values have a similar kind of 0 to,
you know, small value, um,
to 3 or something like that.
And the reason I emphasize that is just that if you
think about taking distance measurements in these spaces,
we've seen how sensitive some methods are to the magnitude of these values.
And this is giving you a picture of how much that choice
about distance is gonna matter for these various spaces.
Sorry. There's a question, did you have a?
[NOISE] Um, how did you obtain the probability distribution?
Well, for the probabilities,
I just mean that I row normalized using that scheme,
and then I just looked at all the values in that entire matrix.
That's what I did for all of these in fact.
This is just a histogram of all the cell values.
Think of them as just one long vector.
[NOISE] Uh, another pitch I made to you is
that you might think about how the co-occurrence counts relate to the reweighted counts.
So obviously, for example,
if your reweighting scheme,
just makes a proportional shift of the underlying counts.
It's probably not so meaningful.
What we really want presumably is to see something really
different when we look at the new cell values,
and how they correlate with the old ones.
And so here are some plots that do that.
These plots are a little bit harder to understand.
But like, up here, L2 norming,
so along the x-axis,
these are the co-occurrence counts.
That is the raw cell values,
although I did the log scale to make it easier to see what's happening.
And these are the reweighted values.
And you can see it's a very different distribution of values.
And the correlation here which I've given up at the top is pretty low.
So it's- it's not that I can make sense of the space per se,
but rather at least I know it's different from the underlying counts.
So I did something meaningful.
Uh, same thing with the L2 observed over expected,
the correlation is kind of strange.
I actually don't know what these artifacts are with these kinds of
shocks of correlated values.
But it looks like PMI, that's really nice.
Right? There's essentially no correlation between
the underlying counts and the reweighted values.
So we did something.
Uh, PPMI actually has a pretty high correlation of all of these,
it's the one that's by this measure the least different from the underlying counts.
Make sense? I'll let you stare at these and try to make sense of them on your own.
I actually don't understand completely why they have the particular shapes that they do.
But I'm reassured that something is happening when I reweight.
Are we looking for one, uh,
one of the- like is it the smaller correlation, the better?
No, I wouldn't say that.
I can't go so far as to say better,
I can just say that at least I know I did something.
[LAUGHTER] But it really depends on
how much you think the underlying counts were important to retain.
Uh, which kind of relates to whether or not you think
frequency is information that you wanna preserve in your model.
Here are a bunch of generalizations,
just by way of wrapping up this this unit here.
So a theme of all of this,
as I've said before,
is that we want to weight a cell value relative to the value we would expect,
given its row and its column.
I think that runs through all these schemes.
Many weighting schemes end up favoring rare events that may not be trustworthy,
so you have to kind of watch out.
And you might want to smooth that out.
The magnitude of counts can be important.
Right? So 1 in 10 and 1,000 in
10,000 might be very different situations in terms of language data.
But for example, if you take a probability distribution,
they will look identical.
PMI and its variants will amplify the values of counts that are tiny,
relative to the rows and the co- and columns,
and that can be a problem,
something to watch out for.
And then relate it to the question from the back there,
TF-IDF severely punishes words that appear in many documents.
So it will behave oddly for our dense matrices,
which are word by word.
And then I offered a few code snippets here.
Again, all of this is in the notebooks,
all of this is relevant for the homeworks,
but I just thought it might be nice to consolidate it here,
and show for example you can do observed over expected,
you can length normalize PMI with and without positive and TF-IDF.
And just to show you that we're making some progress,
at the end of class last time I showed you the neighbors of the word bad,
in the raw imdb5 matrix.
So tight window, highly scaled.
And it looks terrible.
It's highly collocational, and then there's a period,
and then taste, and then guy again.
When I do PPMI on that same space,
then the neighbors of bad are good,
awful, terrible, and horrible.
And that looks like progress.
Right? We're in a coherent semantic space,
terrible and horrible seem like they should be near bad,
even though they have different frequencies.
And then you might just worry that good is in there.
Maybe that's the problem that you kind of wanna qualitatively solve,
because you have what looks like a sentiment confusion,
in those being so close.
I have a short section in here on s- bringing in subword information.
This is a newer idea and I just wanted to make you aware of
it and show you some code that will allow you to do it.
Um, I think, this idea in my mind at least traces to a,
a seminal paper by Henrik Schutze, um,
who showed that if you did some subword modeling you could, kind of,
overcome some sparsity problems and also find
different and more abstract connections between words that might
turn on their morphological analysis, for example.
Uh, yeah, I've said that below here,
subword modeling will pull morphological variants closer together.
Right. You might find yourself seeing
abstract connections between all the adjectives that end in able,
uh, in a way that you wouldn't if you just looked at the words as,
kind of, atomic units.
Uh, it can facilitate the modeling of out-of-vocabulary items,
because if you don't have a representation for a word you can
retreat to using its subparts which are probably highly represented.
Uh, and it can also reduce
the importance of any particular tokenization scheme which is nice because
tokenization in NLP is
this hard a priori thing that you do before you start solving your problem,
that often has huge downstream consequences for your- for the models that you've built.
So it's nice that we can find ways to make that less of a major choice.
And what I did here is just sketch
a technique for doing that using the data that we have.
I'll just walk through the method quickly and if there are questions feel free to ask.
Given a word-level VSM like the ones we've been working with,
you can say that the vector for a character level n-gram
x is the sum of all the vectors of words containing x.
So, it's like you've gone down into
the word and treated all of those things as [NOISE] co-occurring.
Uh, and then you can represent each word as
the sum of its character [NOISE] level n-grams,
so that's the sense in which you could gather strength from all the parts.
And then, I think this is a nice touch.
If you have a representation for the actual word bring that in as well.
And then maybe just, like,
sum up all these vectors and now you've done
some subword modeling and brought in a whole lot of
information from different parts of your corpus that would have been
completely neglected if you hadn't gone down to the subwords.
And here, I just gave an example of how you might decompose the word
superbly into its 4-grams and the only thing that's worth pointing out there is,
I do think it's valuable to have this notion of
start and end contexts which I've signaled with a w here.
And then, it's just a code snippet,
so I just wrote a few utility functions that will allow you to
create an n-gram space for any n that you choose.
And then this l- like character_level_rep here,
it's just a small function that I wrote that preven- presents
one way of mapping a word into its subword model.
And I just showed at the bottom here that even though
superbly as a word is not in the vector space,
I can now get a representation of it because
all its subparts or at least most of them are in the corpus.
And this is kind of interesting.
If you look at how that vector superbly relates to other words,
you see some stuff that's good and some stuff that might worry you,
like, now it's very close to super.
Um, and other, like, add,
like add adverbs that end in ly,
and, and that might- it might be over-emphasizing the subword information.
Um, but in general,
I think- if I think back to the bake-offs last year,
subword modeling was often an important step for the top systems.
Make sense? before I, yeah, go ahead.
How do you use subword modeling in the same VSM as like irregular verbs? [NOISE].
You can't quite do the same VSM.
That's why you have to create this imdb5
n-grams and it has different dimensionality, right.
All ours have dimension 5k by 5k,
but this one because there are more n-grams, it has almost 10k by 5k.
So why [inaudible] 5k why [inaudible]? [NOISE].
That's just a design decision so that we can use the original count matrices and,
kind of, find all this subword structure in them.
Because we, we keep the notion of co-occurrence context the
same on the approach that I've sketched. Yeah.
How's this compare to, like, just stemming your documents beforehand?
I think stemming would be really different, right?
So stemming would be, like,
opposed because there you would be saying that,
I'm gonna get rid al- of a lot of this morphological stuff at the end, whereas,
here you might be saying,
I'm gonna find the way in which superbly is related to
all these other adverbs by the fact that they end in ly.
Another quick unit here,
I'm not gonna spend too much time on this,
but I think this can be really valuable for doing exploration.
Just some fast visualization techniques.
So let me state the goals. Our goal is to visualize
very high-dimensional spaces in two or three dimensions so that we can understand them.
And you just have to accept upfront,
that this is going to involve a bunch of compromises.
There's not gonna be a system,
a mapping from this high dimensional space into 2D that preserves all that structure,
unless in fact, your original matrix had very little structure to begin with, right?
So we have to have some compromises and but,
uh, my pitch to you would be that,
visualization can still be great because it can give you a feel for what's in your VSM.
And it can be especially nice to pair this with some, kind of,
qualitative investigations using the neighbors function.
I feel like with a holistic picture that's approximate,
and some sampling with neighbors,
you can get a feel for whether your matrix contains what you want it to contain,
and wheth- whether it has promise.
Even despite all these compromises that I'm flagging for you.
There are lots of techniques for doing this and, uh,
in fact in recent years scikit-learn has added a bunch of them,
and they have a wonderful user guide.
So I would think if you want to go beyond the one that I'm going to show you, uh,
check out scikit-learn [NOISE] and see what,
what they can do and they talk a lot about the trade-offs for various methods.
But what I've included in VSM is
a little wrapper around scikit's implementation of t-SNE.
I'm not actually sure whether I'm saying that right but I've been saying it for years.
It stands for t-Distributed Stochastic Neighbor Embedding.
And I would say intuitively,
what this algorithm is trying to do is,
go from a very high-dimensional space,
to a dimensionality reduction in a way that you can lay things out
in a few dimensions while preserving a lot of local structure.
This is a t-SNE plot of giga5,
that matrix after I did pp,
sorry giga20 after I did positive pointwise mutual information on it.
And I would just say that,
this looks pretty good to me.
Uh, yeah actually, at this point I think this is quite beautiful.
You might come to also find these things quite
beautiful because it looks like a giant blob,
but actually there's a lot of local structure.
And it's that local structure that I think t-SNE can capture pretty well,
and that is telling me that there's, kind of,
neighborhoods of coherence which is what I expect from this very high-dimensional space,
that I would find some dense neighborhoods that are interesting.
So that's where you can see and I actually,
so here are two examples.
This is the cooking one,
it's probably too small to read but it's lots of stuff about cooking,
and over here this is lots of stuff about conflict.
Terrorism, war, politics, and so forth.
But it's really semantically coherent which is nice.
And then I did the same thing, this is imdb20 with PPMI,
again pretty good local structure in here.
Here's a positive section,
this is lots of positive words and then negative ones.
And I would say this is nice,
because not only is there pretty good preservation of sentiment in
this space but also you- if you look beyond
the adjectives you can see that when people talk about
positive and negative things they talk about
somewhat different a- aspects of these movies.
So for example, they complain a lot about the dialogue, um,
but they talk- they say lots of nice things
about actors and their appearances and so forth.
Um, so kind of,
one layer beyond just raw sentiment.
And then here's a little bit of code for using the wrapper that I wrote.
It's really straightforward and the only nice
add-on that I want to mention is that the function which is called, um,
tsne_viz, if you give it a vector of colors which you can set up however you
want as long as it's aligned with your vocabulary
then it will display the words in those colors.
And for this example here,
I just downloaded a sentiment lexicon and then
displayed all the positive words in blue and negative in red.
And that can be, kind of, nice to see for something that you
expect to align with your semant- the semantics of your space,
how well is it actually aligning, right?
Surely, I hope that I'll see lots of blue clusters
and lots of red clusters amongst all the gray.
Yeah. [NOISE].
Just a bit more like global structure of a 2D visualization,
like, this, like, meaningful ever.
Can you interpret, like,
where the clusters are in relation to one another?
I think not. I think that's where this starts to break down.
Certainly, you can't trust anything about position on
the plane in an absolute sense because as you re-run t-SNE,
you'll see rotations of approximately the same space.
But I think, correct me if I'm wrong,
but I think it is not meaningful that,
like, whatever blob this is,
is kind of, close to whatever blob this is.
I think that kind of
influence is getting dispersed pretty fast throughout this visual- visualization.
Does that accord with your understanding?
That's why I tend to zoom in on local structure.
Excellent. Final phase that we wanna do here is dimensionality reduction.
I'm gonna present to you a few intuitive methods for doing this,
um, and try to give you a sense for why you might pick one over the other.
We're gonna start with the classic which is
a matrix factorization method called Latent Semantic Analysis.
It's also called truncated SVD for singular value decomposition,
that's really the underlying algorithm.
This is one of the oldest and most widely used reduction techniques,
and I would say that it is a standard baseline,
and often very tough to beat.
This is a powerful method.
And if you think about building up a table of results,
then this may be the second row beyond just raw PMI.
I am not gonna walk through the full algorithm with you because in my experience,
this is kind of the culmination of a class in linear algebra,
takes a long time to build up the relevant concepts here.
But I think, even if we don't do that,
I can give you a sense for why this works and why you might want to apply it.
So let's start with those guiding intuitions.
Thinking just in 2D,
I've got these points A, B, D,
C, and you're probably accustomed to doing
something like fitting a linear regression to those points.
So that's that orange line there.
And by definition, that linear regression is gonna
find this- the source of largest variation among those dots,
and fit a line through them.
And then one thing you can do is think of that model as saying that B,
its fitted value is on this line right here.
And C, its fitted value is on this line right here.
So you lose one dimension of variation for the sake of this linear model.
And notice what happens if you do that.
If you do that mental projection of pulling B down and pulling C up,
now they're very close together.
That's the sense in which I have- I've abstracted away from one dimension of variation,
and found an abstract connection between these points,
they were modeled in a similar way.
I actually find that this is even easier to think about if you go 3D.
So think about the Stanford campus.
It's very flat, its main source of variation is kind of XY let's say on the plane,
but it does have some tall buildings.
So if you are standing at the base of the Hoover Tower,
let's say I'm at the base and Bill is at the top,
we're far apart because the Hoover Tower is pretty tall.
But if I decide to abstract away from this vertical dimension,
I decide I'm just gonna look at the [NOISE] at the plane,
then Bill will be pulled down and he'll be right next to me.
That's the sense in which we're doing dimensionality reduction.
We're taking something in 3D,
smooshing it down into 2D.
And when we do that,
we find a connection that was kind of missing from the original space. Does that make sense?
The method is singular value decomposition,
and kind of the fundamental theorem here is that for any matrix m by n,
I can do a decomposition into three matrices,
T which you could think of as like the row rotation or the row matrix,
some singular values, that's the diagonal,
and a kind of column rotation or a column matrix that's over here.
And the idea is that this- the combination of these three matrices equals this one.
So at the level of the full decomposition,
I haven't done anything, right?
I've just shown you a decomposition of the original matrix.
Where this gets interesting is when we start to think
about doing reduced dimensional versions of it,
where we're gonna approximate the full matrix by some subparts of the decomposition.
Let me try to motivate that.
So here's my example that I like.
This is a little vector space of adjectives.
The conceit of my example [NOISE] which I think is still pretty true,
is that gnarly as an adjective is a West Coast thing.
Whereas, wicked is kind of like [NOISE] its counterpart on the East Coast.
If you're from Boston, you say wicked
and if you're from Los Angeles, you say gnarly.
If this is not true anymore,
that's a little embarrassing because I'm the linguist here but set that aside,
I think it's ki- it kinda makes sense.
The issue there is that gnarly and wicked are both positive.
But because of this dialect split,
they're unlikely to co-occur in any document.
Um, rather, what will happen is that they'll have neighbors in common.
So gnarly and wicked will both tend to over-occur with awesome,
because that's like across the world and across the country anyway.
And not tend to occur with lame and terrible because they're sentiment opposed.
But if I just use the methods that I've showed you so
far and I calculate distance in this little matrix here,
gnarly and wicked are really far apart.
And the reason they're really far apart is,
again, think back to my [NOISE] campus metaphor.
There, uh, they're not abstractly anywhere near each other because they never co-occur.
What we want is a kind of second-order notion of
co- co-occurrence that's taking into account the other things that they vary with.
And to get at that notion,
we have to do some work.
Here's the Singular Value Decomposition,
term singular values, and document.
But what I've done here, this is the truncated part.
I'm gonna look at just the top two dimensions.
The algorithm ensures that the singular values are organized by their size, uh,
and that corresponding these- these columns are organized by a kind of the,
uh, amount of information [NOISE] that they contain.
So when I do the approximate reconstruction using just the two columns here,
I get this new vector space, reduce dimensional.
And in that new vector space gnarly and wicked are really close to each other because
the method has found this abstract notion of co-occurrence,
second-order co-occurrence you might call it.
Make sense? Yeah.
How is this different from PCA?
It's very similar.
Yeah. In fact I- I do- I do wanna point out that the-
there's a whole family of these methods that you could think about,
and they're all drawing on essentially the same intuition,
uh, and they just kind of play out in slightly different ways.
I picked LSA because it's like the classic for this space.
Cell-value comparisons, right?
I'm pitching this as a way of understanding what happen to the space at a high level.
Here, the raw counts.
If I just apply LSA directly,
I get kind of this funny distribution here.
And I think that's because LSA as fundamentally a least-squares method is
not so well matched to the Zipfian distribution of counts coming in over here.
So if I first reweighted by PMI
which I showed you has this kind of nice normal distribution,
then the resulting values for LSA also have a similar distribution over here.
They're slightly differently scaled,
and they're different values,
um, but it's kinda more tractable looking.
One more note about this.
So when you read about truncated SVD or LSA, er,
the- the question comes up of how you would pick the dimensionality K,
in the sense of like I picked two here.
But you're working with matrices that have 5,000 dimensions.
How will you pick the number of singular values that you wanna pay attention to?
The dream scenario that you read about is that when you
plot the singular values by their rank,
you see a bunch of high values and then a really rapid drop-off.
And if you saw that,
you'd say, "Oh good,
I'm gonna pick 20 as my dimensionality because I have
such a clear fall off in information value from there."
In my experience, this never happens,
my distribution of singular [LAUGHTER] values tend to look like this.
This is actually from one of the matrices.
And I just feel like I have no idea which value of k to [LAUGHTER] pick, um,
because there's this kind of quick thing and
then a smooth drop-off all the way to the end.
And that's why, as you can see from the homework,
this emerges as a kind of
heuristic empirical issue of maybe trying a bunch of different values of k,
and seeing which one gives you the gain you were looking
for on the tasks that you're actually trying to solve.
If you ever see one of these dream scenarios,
do post it on Piazza,
we could like have a wall of fame for them.
But probably, it'll be mostly this wall of confusion.
And as I said, here are a bunch of other methods and a lot of them are implemented
in scikit-learn, in its decomposition and manifold packages.
Questions or comments before I move on to another method?
[BACKGROUND]
There's some LSA code snippets.
I think this is really easy.
They're just there for reference.
Let's move on to autoencoders.
This is our first deep learning model I guess.
So autoencoders are flexible deep learning architectures
for learning reduced dimensional representations, right.
That's the name of the game here.
It's a nice reference.
Let me give you the intuition.
For an autoencoder, the incoming examples could be for example,
the 5,000 dimensional rows from one of the count matrices that you guys are working with.
The idea is that I'll take that really high-dimensional thing
and pass it through this very narrow layer in orange,
maybe that has dimension 100 or 50 or 10 or whichever one you pick.
The job of the autoencoder is to do its best to reconstruct the input.
So you pass it through this narrow pipe and then try to reproduce it.
Of course, you expect given
a complex enough incoming matrix to have some information loss,
but we also had information loss with LSA.
The hope of the autoencoder is that it will learn to lose the information
that's not so important and preserve the sources of variation that really matter.
And that's the sense in which you'll get a powerful intermediate representation.
So when we run an autoencoder,
we train it against this reconstruction objective.
But we're at, what we're actually interested in is, you know,
this hidden value here or one of
the hidden values if you've decided to put a lot in the middle there.
I will say that it can be hard to make it work with
that raw 5,000 dimensional vector coming in.
So people do things that are outside the autoencoder,
uh, to reduce the dimensionality.
So for example, you might first run LSA and kinda do a hard extern,
model external dimensionality reduction and then feed that through
your network and try to reconstruct it to do a further layer of dimensionality reduction.
For this second panel here,
I thought it would be nice to just show you everything you need.
If you wanted to implement this autoencoder from scratch,
the forward pass would go up and the notation is all
here and then this shows you how to calculate the gradients,
and I used exactly the notation from the NumPy implementation that's included
with the course repo in case you wanna
really go down and see what the computations are like.
But it's a pretty straightforward model,
compress the information and hope that it works out for the best. Yeah.
So the hidden layer here would be what we use,
uh, let's say in the homework to predict word pairs.
Yes, and my autoencoder as the fit method returns that matrix of
hidden representations so that it kinda acts like LSA. Yeah.
Uh, on note of the task that's being used,
is it more like reversed LSA
because if you process LSA beforehand you've sort of done dimensionality reduction,
and if you're trying to reinvent the input,
do you mean the original like giga5 matrix that was going to fit to LSA,
or you're trying to re-engineer your matrix after LSA.
Uh, well, let's just say there's lots of possibilities.
It will depend on what you do but the sharp answer is if you feed in raw counts,
the model will try to reproduce the raw counts.
If you feed in the LSA representation,
it will try to reproduce that.
Other mixtures would be different than not, they would be, not standard autoencoders. Yes.
[inaudible]
Yeah. It would and if you had the hidden dimensions,
the same as the input and output,
you would hope it would learn to just copy.
But since you're forcing it to pass it through this narrow pipe,
it has to lose some information or at least represent it very differently.
Thank you.
Sure. And here's some code for doing this, the interfaces are
maybe a little bit non-standard and I tried a few different things here in particular.
I guess here I tried to length normalize
the inputs so that it didn't get raw counts coming in because I
think that's a very difficult learning problem
for a deep learning architecture to have raw counts coming in.
Uh, and it works pretty well. I think you need to run it for
longer than I did but it's definitely on
the right track to doing something interesting. Oh yeah.
Here's a little bit of proof of that finance in giga5 raw counts that does not look good.
Uh, the autoencoder looks much better, right.
So that's the raw autoencoder,
and here I got a little fancy, I ran PPMI and then
LSA with dimension 100 and fed that into the autoencoder,
uh, and it's at least different.
I don't know how to evaluate the two.
I guess that's what the homework is all about.
Any questions about that?
This is great timing then.
Let me introduce one more model just quickly and then we'll circle back to it.
Uh, I think this is my favorite of
all of these dimensionality reduction techniques for a few reasons.
So this is GloVe.
Global Vectors, you might have heard about it.
It was developed here at Stanford by
Jeffrey Pennington and Richard Socher and Chris Manning.
This is the paper, and I guess I want to say right at the outset as a kinda meta comment.
This is a lovely paper.
I highly recommend that you read it not only for the content but also just
for to experience a really well-written well-motivated paper
in our space, not all of them are.
Uh, but this is a kind of exemplar.
They really nicely articulate
the high level goals that kind of place you in the context that you're in right now,
which is that like, we have these matrix factorization methods,
we have things like PMI,
we have Word2Vec.
Let's think about their strengths and weaknesses.
And then let's really think about what we are trying to do when we build
word vectors and from there over
like section two they build up this model very carefully.
Uh, and then of course, it had great experiments and they
released code and they released these lots of pre-trained word vectors,
which the community has benefited from in lots of ways.
So it's like the complete package of what you'd be looking for in a contribution.
Uh, roughly speaking, the objective for GloVe is to learn vectors for
words such that their dot-product
is proportional to the probability of their co-occurrence.
Keep that in mind because that might already sound to you like PMI.
In terms of practical details,
we're using the implementation in the mittens package,
which I wrote with Nick Dingwall.
It's called mittens because we have a variant of GloVe, uh,
it's GloVe with a warm start and we take Mittens to be warmer than GloVe's.
And it should also be fast because we found a way to vectorize GloVe.
So I would use that for modest jobs and for really big things,
you should use the GloVe team's C implementation,
which is also an impressive engineering feat and works extremely well.
Let me do this one slide for you.
Maybe two and then we'll wrap up for
the day but I would like to just plant the seed for this.
So at the top here,
this is the GloVe objective.
You can see that you have two words,
I and K. Those are the vectors that we're learning.
We're gonna take their dot product.
That's an idea that you've seen before because if you think back to cosine distance,
the similarity part is the dot product over the normalization constant there.
So this is kinda like un-normalized cosine similarity.
There are two bias terms,
and the idea is that that's equal to the log of
the co-occurrence count for I and K. They kind of unpack that a little bit so they
define this dot product here as the log of the probability of
co-occurrence and they define that as
the log of the co-occurrence minus the row probability.
The reason that they subtract only the row log probability,
is because they're assuming at this stage what they call the exchange symmetry,
which is that in the matrix the rows and the columns are the same,
um, because they assume they're dealing with a word-by-word matrix.
It certainly needs to be a square matrix for GloVe to work.
If you allow that the rows and the columns might be different,
and in fact they construct matrices where the rows and
the columns are different and you might do that as well.
Then you would wanna factor in
the column probability and that's what I've done down here.
So now you have the log of the co-occurrence
minus the log of the product of the row and the column probabilities.
And that is something that you've seen before, that is PMI,
right, just by the equivalence of the given down here.
That's quite, quite interesting, right?
They give this lovely argument that sounds like it's in a different place
from PMI and they end up back at the PMI place.
Now, it's not literally PMI,
because you have to remember that what GloVe is doing is
learning reduced dimensional representations,
right, it's doing reweighting and dimensionality reduction all at once.
But it is certainly and you can see this objective drawing on the same insight as PMI.
And that's the sense in which this is the kind of the hero of our story.
There's a few other things that are really
meaningful so I don't wanna minimize them because this is kind of
the ideal objective and then the actual objective is
a weighted version of it and we should talk
about why they chose the weighting scheme that they did.
Um, but just in the interest of this having been a lot of content,
I propose that we wrap up here,
maybe read the GloVe paper,
think about that slide and then we'll pick up
there and move through on into retrofitting next time.
 Just a few things that I want to do today.
Uh, my plan is to get through those things,
which is really just a bit more on GloVe,
then Word2Vec and retrofitting,
and that kind of rounds out, uh,
this first big unit on distributed, uh, representations.
Next week, we're going to turn to the topic of
supervised sentiment analysis and explore a different set of topics.
But my idea for today is that we get through this stuff,
we're not in a rush, so feel free to ask questions.
But when we get through it,
we're just going to break, um,
and I'm going to encourage you to do some hacking.
Uh, to explore the notebooks if you haven't already to get
a more hands-on look at the material or also you could just dive into homework one.
Um, you should certainly make sure that your system is set up.
Please don't leave here today without feeling
like you're ready to just work productively on the problems.
Um, so a bunch of the teaching team are here in
the room and then we're going to have a setup,
I'll give you the details on that later,
where you could video-in to talk to
some of the TAs who are going to be available in a kind of queue.
So if you're not in the room right now but you need some help, uh,
feel free to contact us,
and I just want to emphasize that this is the moment,
the best moment to get your system working,
do not wait until Monday afternoon to discover that you don't have crucial components.
That's what this is. This is the perfect moment to make sure that's all straightened out.
Before I dive in, any questions or comments? Yes?
There's two office hours tomorrow, uh,
one of them says it has Zoom,
the other one doesn't because both-
Are they both also online?
For tomorrow.
So there's another one in the evening?
That's right. He's kinda always by video.
Oh.
You can reach him, and I think the questions was
featured [inaudible] [LAUGHTER]
So you could choose either one.
Can, can that one also be online or is it only.
No, that one you have to go see the [inaudible].
Yeah, what if you want to just call in,
we can arrange that?
Oh, thank, thank you.
Yeah, you could uh, I don't know.
Yeah, you could- I don't want you to email me directly,
but maybe you could post on Piazza.
Actually, that would be tough too.
Maybe I'll just create a link,
so if people wanna call in from the line, they can as well.
Just so I'm inclusive of everyone whether they are in person or not.
Okay, do what you can,
but otherwise, you might have to make your way [inaudible]
Do we use QueueStatus?
We do, that's what we're using today.
No, I don't think there's Zoom today,
because we have the [inaudible]
Okay. Yes so we use Zoom sometimes.
Can you give details on it when we- we're in the middle
[OVERLAPPING] on how it works? Yeah.
It's very easy.
You can just basically call me,
and then [inaudible] and that might
mean that I'm not going to be able to see it like you, but we see it.
Okay, great.
Where we left off is that I had planted the seed for GloVe.
Basically, I said some nice words about this paper.
I'm a big fan of this paper and I gave you the intuition and
also here are some other practical details about implementations that you might use.
So we're using mittens by and large,
there's also C implementation,
and I think in a minute,
I'm going to show you kind of a reference implementation that is very, very slow,
but it's kind of what you would do to just
naively implement the algorithm and I think that
there's a real pedagogical value to see exactly how those computations would go.
So we're going to glance at that but not-
I'm not saying that you should use it for anything beyond study.
So here's how I introduced GloVe,
I gave at the top there what they present as kind of their ideal objective which is
that for row representation or a row embedding w sub i,
it's dot product with a column embedding w sub k. That's the primary term,
and then you have two bias terms,
and the idea is that that should be equal to the log of the co-occurrence probability,
or at least, I guess proportional to it.
They kind of dive down further so their ideal reconstruction of what that dot product is,
is that it should be the log of the co-occurrence probability and they define
that co-occurrence probability as the log of
the co-occurrence count minus the log of the row probability,
um, and if you do allow that the rows and columns in this matrix might be different.
So you'd want to not just concentrate for normalizing on that row but also on the column,
I think you get this expression here which isn't in the paper but I think
it's certainly in the spirit of the paper because even for their matrix,
it sees the rows and columns could be different.
And so then you've reconstructed the co-occurrence probability
as the log of the co-occurrence minus the product of the two,
the row and the column and what I just pointed out to you is that,
you know, that's PMI.
That is certain like in some sense,
that's the literal calculation of PMI and you can certainly see that what we're,
what we're getting from this picture is that GloVe is built on that PMI intuition.
It's not the same as PMI,
of course because we're learning,
we're learning regularized reduced dimensional representations,
whereas PMI is just a re-weighting of the entire count matrix.
But I really like this as an entry point to GloVe.
When you think of GloVe,
you could think of it first and foremost as capitalizing on that core insight.
That's where we left off.
There are a few more details for the paper though that I think are
really important for why GloVe works so well.
So again, I just repeated the original GloVe objective up here at the top.
What they actually argued for is a weighted version of
it which you see down here and the,
the new pieces like at its core you have
that dot-product again of the row and the column.
You have two bias terms for the row and the column and then
you subtract out the log co-occurrence.
It's additionally weighted by this function f. So f is a function of the counts,
and what it's doing essentially is kind of
flattening out and re-scaling those count values.
So f of x,
that's the general function defined here.
You set x_max, the default and the spirit of the paper is to set it at 100.
So a count of 100.
So if x is less than that max value you set it as a proportion of
that max with an exponential scaling here at Alpha,
and if it's equal or greater to the max,
you just set it to 1.
Yeah as I've said down here,
Alpha is typically 0.75 and x_max is typically 100.
I think that's fine, reasonable defaults.
I am gonna suggest that you think a little bit about that though.
But is that clear, so far?
That's kind of the whole model, right?
When you think of GloVe you should think of that dot product of row and column with
some bias terms and then subtracting out the log co-occurrence. Make sense?
Plus a re-weighting of that.
The GloVe hyper-parameters.
So there are really three.
The first is just the dimensionality of the learned representation.
You get to set that.
You can set it at any number and you can imagine that setting
it at larger values is going to give you more capacity to learn,
but also take more time and everything else.
So that's kind of something that you play with for all of these models.
x_max is going to flatten out all the high counts and the idea I
want to plant there is that you might
reflect on that value and the way you might think about it is,
for my count matrix what percentage of
the values are actually at or above that threshold,
if it's 100% of them, for some reason,
then picking x_max to 100 is going to turn your entire matrix into a bunch of 1s.
That's certainly not going to be good, right?
That's unlikely, that you would have a count matrix that was that dense but,
what percentage is it,
and what are you doing to your matrix when you flatten out?
Maybe that's a large percentage of the, of the values.
So you can think about that,
and then you could also think about Alpha which is going to control
how much scaling you do for counts that are below your max.
And here, I've given an example so the incoming row vector for this f,
I've defined with the defaults is 100, 99, 75, 10,
and 1, and once I've applied this re-weighting, it looks like this.
So obviously, the magnitudes are much different.
1, 0.99, 0.81, 0.18, and 0.03.
So you're doing something very important to this space when you do
this free-weighting. Makes sense?
This is a new slide that I added.
I recognize that it's very dense.
It's kind of there for a reference,
but let me give you a guided tour a bit.
Let me step back and just say,
"I was thinking about other ways that we might,
at an analytic level,
get a deeper understanding of GloVe.
We have the PMI insight."
I've shown you the GloVe objective,
um, but what else could we do?
And what I thought is,
when I motivated LSA before,
I gave you this example of gnarly and wicked.
And my motivation, my motivation for showing you that was,
these are two words that are similar and if semantically,
they're both kind of positive slang terms maybe.
Uh, but they're unlikely to co-occur with each
other because they're kinda from different dialects, that was the idea,
and my pitch to you was that LSA has the capacity
to recognize that they are similar despite never co-occurring
because they have other neighbors in common because they
over-occur together with awesome and less than you would expect with terrible.
And as a result LSA is able to capture that insight.
Right, that was one of the motivations there.
And I thought, how does GloVe do this?
Because GloVe, by its nature,
because of this log count thing here it doesn't even visit 0-valued cells.
Yeah, this is like my just very quick reference implementation,
where you set up the word and the context matrix and the bias terms.
You can take the log of the entire matrix all at once.
So do that as a pre-computing thing and you can
re-weight the entire matrix in the way that I just showed you with f,
could do that outside of the loop.
And then once you start iterating and working on specific examples,
you don't visit the values that are 0,
and the reason is because of that log thing right,
you could either have a problem there or you'd want to be able
to say that this is just the value of 0.
There's no update for this example.
So that's a kind of definitio- definitional thing about GloVe.
But then, how is it going to do what I want it,
in the case where,
here's my count matrix,
I've said that wicked and gnarly never co-occur.
This is a version of that LSA example but I made it word by word because of course,
GloVe requires a word by word matrix or a square matrix.
So how is it going to do this?
I kind of hope that it does have the capacity to do it.
Actually, does anybody have any guesses?
There's a lot of detail here but like what's your hunch?
Anyone have a hunch about how it might be able to do this,
or maybe it's going to be unable to? Yeah.
Basing it on the relation to the word terrible which is a negative, sure,
so how, how the positive nature of it is connected to the matrix.
Yeah, beautifully put.
That's in fact what happens.
I think experimentally is that what GloVe [NOISE] do is,
push both gnarly and wicked away from terrible and toward excellent,
because it does have co-occurrence values for them.
And you're kind of relying on its ability to do that,
in order to achieve this abstract connection.
Um, and I think that does happen.
So what I did for this little problem is,
I just got rid of the bias term so that I could display the calculations.
And I also started
the GloVe optimization process in a way
that I was sure that gnarly and wicked were really far apart.
So iteration one I basically made them like to
be complete in- completely separate parts of the space.
And then I thought what's going to happen as I do these updates?
And building on exactly your intuition,
what I've decided to do is look at what's
happening for both of them as it relates to excellent.
And I could have done it for terrible where you'd expect them to push away.
And I thought that might happen,
that they- that they both gravitate toward excellent, and it does.
So here's it's kind of small but this is wicked and terrible and awesome and gnarly.
And that's what I meant that gnarly and wicked are at like
opposite ends of this little two-dimensional space here.
And after one iteration [NOISE] it really has been reorganized.
Here's awesome, here's wicked,
and here's gnarly and terrible.
It's been thrust up in- into the corner here.
And I think if I keep running this,
[NOISE] then- then gnarly and wicked are just
gonna gravitate closer to awesome and farther away from terrible.
And that's the sense in which we'll be capturing their abstract similarity.
And then on the right here,
what I've done with kind of color-coding is just make that really concrete.
So there are the counts up in the upper left.
This is the reweighting.
This is w_0 and the context vector zero.
So that's the starting point.
And I am watching for gnarly and wicked and how they're gonna get closer to awesome.
And when you do that calculation,
this is just the derivative of the GloVe objective here,
which I stated up in the corner.
You get these error terms and then,
that updates the weights.
And then after another iteration,
the error terms have gotten smaller.
That is, just as you've seen in the picture over here.
These terms are gravitating toward excellent and you can imagine that as I go,
these values are gonna get closer and closer to 0.
So I've- I don't know, I've found- I found that reassuring.
Does that make sense? Any questions I can answer about it? Yes.
Do you mean awesome or excellent?
[inaudible].
Awesome. That's the one that I picked.
Oh did- I wrote excellent up here. I did mean awesome.
Now this is my space;
gnarly, wicked, awesome, terrible,
and I set it up so that awesome is really
frequent and co-occurs a lot with both gnarly and wicked,
and hardly ever with terrible.
That's the idea. Then you could see it pulling toward the blue.
Those are the comparisons I keep making.
And just for the comment before,
if I had been done comparable comparisons with terrible,
I think I would have seen them pull apart.
Well, why would gnarly and awesome have like a lot of co-occurences?
They have none. Gnarly and awesome.
Yeah.
Oh, because they're both positive slang terms.
But if a co-occurence like right next to each other, I'm trying to think sentences.
Oh, this would be like if I was reviewing your paper and I liked it,
I would say it was both gnarly and awesome. I don't know.
Probably that would be for more like your skateboarding moves
[LAUGHTER] than your paper.
[LAUGHTER] There is one more thing
just to kinda fill out my little analytic framework here.
So when I showed you all those reweighting schemes before,
I showed you before and after on the distribution of values,
and here for GloVe, this is really nice looking, right?
So the raw counts come in and they have this really difficult scaling.
And then, when you reweight with GloVe,
that's just raw counts coming in,
you re-weight and you get this like completely normal distribution here
of cell values and look how nicely they're scaled between -2 and 2.
Ah, and we'll see this.
I'm gonna return to this point again and again.
Like a lot of the deep learning models that we look at are gonna be
very sensitive to the scaling of incoming values.
And you could just say like,
this is a- a really nice starting point for a lot of
those networks because everything is kinda scaled in a rational way.
So this is also partly why I think GloVe does well as an input to another system.
And then I had some GloVe snippets here just showing you how the, um,
oh, first one I did is,
just loaded the, um, data frames two of them;
IMDB5 and IMDB20 and I just wrote a little function that would look at
the number of non-zero values that were below some threshold that I set,
so that would be x_max.
And for IMDB5, it's about 2% of the values and for IMDB20,
it's 15% of the values.
Uh, corresponding to the fact that IMDB20 has a much larger window and therefore,
it's much denser because more things co-occur with other things.
But you can see that that is interacting in an important way with the chosen value for
x_max because I've done relatively little to IMDB5 by thresholding,
but I did a whole lot [NOISE] to IMDB20.
I really made a lot of those values the
same even though they might have been very different.
So something to keep in mind as you use GloVe that you might, I mean,
I think I should have been more reflective on this in the past when I've used GloVe to
think about the impact of that hyperparameter.
And then, for this code snippet here,
what I did is just test this idea,
the GloVE objective is to learn
word vectors whose dot-product is proportional to their co-occurence count.
And so you might think that if I can somehow perfectly achieve this objective,
then I'll get a perfect correlation between the dot product for
two vectors and their co-occurrence in the original count matrix.
So I just wrote a little function correlation test that does
that and I ran GloVe for a little while.
And you can see that the correlation is about
0.38 for IMDB5 and about 0.48 for the IMDB20,
for the amount that I ran in GloVe.
I would expect that value to get larger the longer I ran GloVe for.
But this is a kind of measure of how much we're living up to
the GloVe objective and also kind of how
much we are compromising on the original count matrix.
It's not that we want to reconstruct the original count matrix.
So maybe some loss is useful actually. Yeah.
This slide changed from the prior classes.
I made just one change.
I meant to do a shout out for that.
Somebody noted that I had taken the log twice.
It doesn't matter whether you take it in the line row_prob or row_log_prob.
And so I just did it on both,
um, which was not good.
[LAUGHTER] It was affecting the scaling and since
the correlation test that I chose is sensitive to the scaling of the values,
the original correlations were much higher.
And I should have been reflected.
They were so high but I was like,
can that really be right that it's doing so well?
This is much more realistic.
I thought we would do one more model here.
word2vec. I'm just gonna do this one quickly, uh,
and I can justify that in part by the fact that the TensorFlow tutorial on
word representations or vector representations
of words is basically an introduction to word2vec.
And it's really, really good.
So that's my first practical tip and the other is that if you wanna train
your own word2vec representations,
then Gensim is a nice Python package,
kind of arbitrarily scalable that you could use to do that.
Well, lemme just give you the gist of this so that you have it in your toolkit.
So the fundamental idea that I take it for word2vec is,
that I can take a corpus and think of it as a labeled data set.
And the way that I can do that is by setting a window size,
just the way we did for all our count matrices,
and think of pairs of words that co-occur
within that window as labeled training instances.
This is a positive label that because it was,
is there in that sequence.
Then I say it is labeled with was.
And I also say it is labeled with the and so forth for all of these co-occurrence pairs.
So it's a funny kind of supervision, right?
It's not, um, but it's like a positive example of this word co-occurring with the other one.
And still it's sensing in which this is what they call.
Where does it and best co-occur?
Here.
It's window two, right?
Yep. One, two.
Yeah. So it and best do not co-occur because they're outside that window.
But that's a parameter that you would set in the larger.
You've said it, the more training data you have in some sense.
Of course it's not all equal.
So this is the basic model and you can probably see at the top
here that this is a kind of log linear or softmax classifier.
You might have seen this equation before.
This is the explicit objective but I
think we could reduce it all the way down to this kind of
blurred model here where I have X is my embedding matrix for the vocabulary,
W is some weights,
maybe we also have a bias term.
And then the kind of hard thing to get used to about this model is that
C is a label vector for an individual example.
And it's one-hot encoded but it has the dimensionality of your entire vocabulary.
If you have 20,000 words in your vocabulary,
then it's a 20,000 dimensional vector with a single word having-
a single dimension having a 1 corresponding to
the word for that training instance that co-occurs with your input word.
So this is obviously really difficult to
train and it's quite slow but that's the intuition, right?
Is that I created this labeled data set
and then I just do a very standard thing in machine learning,
which is have a classifier that learns to predict those labels.
It's just that it happens in this very high dimensional space.
And I take it that that's kind of the core insight
behind what's called the skip-gram variant of word2vec.
And then this one,
other thing you might think about is I kind of- I can do
this for individual examples as they come in.
But if I do it for my entire vocabulary all at once,
that's fine because they're kind of all independent.
But when you phrase it that way,
you start to see that this is kind of like another one
of these matrix factorization methods, where I have, like,
my word vector, where my word embedding space and I have my context embedding space W,
just like GloVe did,
and I am taking all those dot products.
And the objective of the model is to push those dot products in a particular direction.
And in this case, it's in the direction of favoring the things that co-occur a lot,
and in turn disfavor the things that don't co-occur a lot.
In practical terms I think people don't use
this model because of the very high dimensionality of C
causing lots of problems.
So the more popular version which is implemented for you in TensorFlow,
you'll see that in the tutorial,
is called skip-gram with noise contrastive estimation.
And this is the kind of approximation of that model that I just showed you,
where instead of doing that very high-dimensional thing,
I have the sum of two separate objectives here,
each one of them binary.
And it's just saying, for the left here those are the things that actually co-occur,
you know, push in that direction.
And then I sample some negative instances to be the negative ones over here,
and they give some advice about how to sample negative instances.
But those are presumably things that you didn't see co-occurring in your training data.
It's not that they never co-occur.
It's kind of a miracle that this works at all because of course they might be
perfectly well formed pairs but the idea is that on average with a large corpus,
when I sample these negative instances they will be kind of sequences that are
unlikely to appear together and therefore it's right to down-weight them. Yeah.
Is there is some intuition that you would want to choose negative pairs that kind of
could appear now or should you just pick a totally absurd pair?
That's an interesting idea.
It sounds like something like a linguist would suggest,
which is let's think very carefully about which words are plausible.
I think they don't do that.
They have some schemes for sampling from the frequency distribution,
which might kind of align with your intuition.
But I've not seen work that tried to get down to
like can this adjective appear next to this verb or not.
Anyone seen anything like that?
It's a potential place for innovation just
because this negative sampling part really does
matter for how efficiently this model is
trained and what kinds of embeddings you could have.
Okay, final section here.
Retrofitting. The spirit of this
is I think it's very exciting to study distributional representations,
and I think they've been very successful.
And it's- it's kind of amazing how well it works given that they
operate only on co-occurrence statistics in large data sets and large corpora.
So obviously there are
important aspects of meaning that are latent in that co-occurrence data.
But I do not myself believe the claim that that's all there is to meaning.
If I think about meaning as it's studied by
linguists or by cognitive and developmental psychologists,
they emphasize other things beyond co-occurrence,
like grounding in a social situation,
interaction with other people in language.
All of that stuff, it's not merely co-occurrence but
rather about kind of our entire cognitive lives.
And that leads me to doubt that
these distributional representations are going to tell the whole story of meaning.
And a response that I really like about that is that we might look to ways that we could
take these distributional vectors and improve them with some supervision,
that is imbue them with information that wasn't merely co-occurrence information.
That's the pitch of retrofitting,
and it's really just one of many methods that you could adopt for doing that.
Yeah, here I've given a kind of summary.
I'd like that these things are high-dimensional.
I like that they're dense and capture lots of linguistic relationships.
But I'm doubtful that we can capture meaning solely from
co-occurrences but I don't like that there's no grounding.
You can leave the symbolic questioning for later.
Not sure I have an opinion on that.
But these two things are things that I would really like to address.
There have been in the past a bunch of different ways to do this,
a bunch of models proposed.
This is some work that I did with Andrew Maas when he was a PhD student here,
and this was really just taking distributional vectors that were learned from
a kind of auto-encoder and infusing them with sentiment information,
like what's the gradient information from a sentiment classifier.
And what we're pitching here is that you could take this space that's kind of not
well differentiated but when it comes to sentiment,
as you can see by the mixture of colors,
and kind of really spread it out so that the negative and
the positive are in their own regions.
And that's coming from the supervision.
And there are lots of ways you could do that. Here's another example.
This is actually some stuff that I did in
an exploratory way with , one of our TAs.
Like here I have a kind of PMI space,
and if I take that space and train it against a sentiment objective with a deep classifier,
then I look at the hidden representations from that classifier,
they of course have shifted the space around in response to their supervision.
And you can see a really deep sentiment split here with
all the positive and negative words really
gone into their own part of the embedding space.
These methods can be kind of hard to tune,
so I was really excited when I saw this retrofitting model appear.
This is from Faruqui et al.
It was a NAACL paper.
I think one of the best paper works at NAACL.
Some kind of pioneering work,
and it's implemented in the notebook in case you want to explore it.
I've showed you how to experiment with WordNet and so forth.
The gist of this is here's the objective,
and what I've said here is that this is an objective that's balancing two pressures.
First, I want to be like the original vectors that I've got.
That's the pressure in orange,
but I would also like to look more like my neighbors in a knowledge graph.
Let me unpack that a little bit.
Let's suppose that you trained a bunch of representations by whatever method.
So you have some vectors for a vocabulary of English.
The sensor which you're retrofitting is that you could find all of those words,
say in the WordNet graph. Just look them up.
And now you can imagine that those nodes in
that graph are actually your vector representations.
What the retrofitting model does is say try to remain like
those original vectors but also update
them so that they're more like their neighbors in the knowledge graph,
and that's the sensor which you're retrofitting.
And it's kind of nice and modular because you can take any embedding space,
and as long as you can ground it in some kind of graph you can
run this objective and move those vectors around.
So here's a quick example.
In this tiny little graph here,
this node zero is related to 1 and to 2 in the unidi- unidirectional fashion.
When I run retrofitting,
you can see that 0 moves closer to both of them,
and they stay put, 1 and 2,
because they're not related back to 0.
It doesn't matter which direction they go in.
Here's a case where they're all related to each other,
and what you can see with retrofitting is no matter where they start,
they're gonna kind of pull together because everything is by
this graph similar to everything else because of its connectedness.
And then here's one more example.
I meant there's a,
a hyper-parameter alpha and beta,
and if you set alpha to 1 and beta to 1,
then they're evenly balanced.
Suppose I set alpha to 0.
That's a kind of interesting edge case where I
say "I don't care what the original embedding was like.
All I care about is being like my neighbors," and that's degenerate, right,
because very clearly they just become identical to each other with alpha at 0.
And conversely, if you set alpha really high,
then you'll really remain like
your original representations and the graph will have almost no influence.
And between there, you might find something that's kind of usefully making-
using the original embedding and also making use of the knowledge graph structure.
Yeah.
Where is this knowledge graph structure?
Like where does it come from?
You need to have it out there in the world.
So I gave the example of WordNet, ah,
ah [NOISE] as a case where you have really easy alignments
between whatever representations you've trained for words in English
and this preexisting structure.
But you can think really creatively about this.
Lots of data, can be represented in a graph
and if you can find that structure,
then you can perform the retrofitting step.
And I've given some extensions here,
one thing that you might find unsatisfying about the original retrofitting model,
is that it's baking in very deeply the idea that if I'm connected in the graph,
it means that I'm like my neighbors.
It's like one semantics for these things.
And this is a bunch of attempts.
So, uh, I was involved with some of this work,
to relax that assumption.
So for example, in WordNet there is an edge of relation for an antonym.
And I think it's probably a mistake to say that,
just because I'm connected via th- that edge that I want to be similar to that word.
Antonym has very different semantics from for example synonym,
which is the do- a dominant kind of edge, or
a hypernym like entailed or entailed by.
So these are nice extensions and we released the code for this middle one,
this is kind of the most general version of this that subsume would this other
work that was more specialized around different, ah, graphical relationships.
Then here are a few code snippets.
Uh, I'll let you look at the notebooks for the full story,
because there's some data processing involved in,
for example using WordNet for retrofitting.
But here, I just gave a simple example that kind of documents the interfaces,
um, so that you can think creatively about applying in- in your domain.
That's it. Hope that's an inspiring idea to end on. Yeah.
Could you in order to construct the graphical representation,
just take like any set of embeddings and then just sort of you look at- choose k
closest neighbors and then artificially form
a graph that way and then run retrofittingly?
Oh interesting, so you'd say from the original space,
connect things that are close by some standard and then retrofit to them.
I mean you can certainly do it. It's well-defined.
Ah, like I said don't know what would happen I guess,
what you'd be saying is that,
for whatever threshold you set,
you really want those things to be pulled together.
Um, and then everything else should kind of drift into the background.
That's unusual because I- I think of retrofitting as,
you know, I've my embedding space,
and I have some other resource,
some other knowledge graph and you're kind of using them together.
I don't know but I- you could try it out and see what happens. Yeah.
Um, what does like retrofitting, uh,
with WordNet do to the like analogical relationships?
Does it preserve those things,
or make them better, or destroy them?
Oh that's a good question.
Well, oh, so in the original paper,
I believe they test on- on analogies and see some improvement.
I think that's one of the tasks that they evaluate on.
Um, I should double-check that.
They retrofit to WordNet and FrameNet,
maybe one other gra- oh,
and also a dataset of, um, paraphrases.
But I forget precisely the experiments,
that it's an empirical question,
I would check the paper. Yeah.
Ah, like model on solving, uh,
does the idea of- of the word embedding and solving,
ah, can move quicker, like some- something they were to work on GloVe.
Like some kind of weighted or something on both of
these can be improved than their original input.
Does this idea of wording, uh, give strength.
Are you trying for the bake-off?
No, I'm just, ah.
Yeah, the question was basically whether or not you could combine forces.
That is, bring a bunch of different embedding spaces together and see some gains.
I think the general answer is yes,
and my- thing you might think about is,
what's the overall scaling of the representations that you're creating?
Are they kind of harmonized or is it obviously,
just two things that you smash together?
Because that's going to affect how easy
some like later learning problems like for example.
But I think the general answer is,
yeah, pull these things together.
If you look at recent papers that use
these newer contextual word representations like BERT and ELMo,
you might notice that I'm not- in a number of places,
they have actually included the GloVe vectors as well,
to have even more dimensions and
even more information brought to the problem and you see gains in that case.
And for the bake-off,
it's certainly within bounds to smash together all four
of those count matrices or things that you've done to them [NOISE] and,
uh, see whether that leads to improvements.
And that's a nice transition point.
So what we wanna do now is break,
no more lecturing, get hands-on.
Make sure you can get through the homework because I think those are
nice preliminary steps to building your own system.
Build that system and then you have a bake
off entry ready for when we do the announcement
on Monday with the new test sets. Um, a few tips.
Um, I was [inaudible] I thank you again.
In the back somewhere,
we added to utils a function called fix_random_seeds,
then you have a bunch of options for setting random seeds for
various parts of the scientific computing environment that you're in.
You might wanna do that for your bake-off entry,
as a way of ensuring that we can exactly reproduce whatever you got.
And then you could have- we can have a separate discussion about whether,
if you- if your win depends on seed 42 and you don't win with seed one,
what that means for us.
So let's not dwell on that now.
You might make use of this function in utils
to- to make it easier for us to reproduce exactly what you've solved.
And that is allowed. The bake-off is of course
oriented towards the best-performing system.
That's the standard thing for bake-offs.
But in talking with you, I have come- become interested in two other questions.
Definitely, what's the worst-performing, but well-motivated system?
We were talking about this yesterday.
So not just the worst-performing system in general,
that's not so interesting because you could just do a bunch of incoherent stuff.
But like what's this system that really ought to have worked if you
think about the theory that was a disaster for the problem?
I really would be interested in seeing those systems.
And this is a kind of grim joke but which systems consumed the most resources?
[NOISE] So you can see I've gotten that somebody did train GloVe for 50,000 iterations,
19 hours of CPU time,
and it didn't perform so well.
But maybe that gets the award for most resources consumed.
So I don't know.
If you think you're not going to win then maybe try something crazy,
you know, shoot the moon as they say.
You'll get full credit for entering,
and we would be interested in these systems.
I think you're on your own now.
So now excuse me, is there anything we should say about how people
can connect with TAs, if they're remote.
I think they can just call.
Okay, just call in and there's some details on Piazza, right?
Yeah. [OVERLAPPING]
Okay. We're available.
Great, get to work I guess.
 All right. Hey, everyone. I propose we get started.
The first bake-off has begun.
You should have gotten an email from Piazza.
Uh, I think I'll just go through my posting really quickly, um,
in case any questions arise,
I think it's straightforward,
but let's just make sure everyone is clear on this,
because the time window is tight.
You just have until Wednesday at 4:30.
So here's the procedure.
The test data are at this link,
uh, so download that, and unzip it,
and then move the whole directory into your WORDSIM data directory,
whatever you have set for WORDSIM_HOME in the, um, homework notebook.
It's probably just data WORDSIM.
So move the data in there so your system can find it. That's step 1.
And note this: move the directory,
not just its contents,
unless you wanna have to fiddle with this reader code.
Then open your completed homework 1 notebook.
You should have already submitted that,
but now you're gonna add to it.
Um, find the cell near the bottom that begins with,
"enter your bake-off assessment code into this cell."
Let's call that the bake-off cell.
And then you just paste in this blob of code here.
It's just two new readers.
You can tell that these datasets are the mturk287 dataset,
which I think in this literature is thought of as a relatedness data set.
And SimLex-999, which I think is a similarity dataset.
So if your own model favored one or the other kind of problem, well,
it's evenly matched here because we're gonna
macro-average the score from these two test datasets.
And you can see here, I've gathered the two readers into this tuple called bake-off.
So that's step one.
Now, let's suppose that the custom model you developed for part,
you know, for part four,
let's suppose you call that data frame custom_df.
Then all you have to do is,
in the bake-off cell at the, at the very bottom,
so it prints out "enter this code",
where custom_df is the model that you're evaluating.
Crucially, you have to set the reader's argument to bake-off so that you don't use the,
um, development data for this.
And then optionally, if you feel your model needs to be
evaluated with a specific distance function,
you should specify that as well,
otherwise, it's going to default to cosine,
which might be fine for you but maybe this is an important design choice for your model.
So then you just run that bake-off cell,
um, and this is important, right?
So we're all on the honor code here.
Running the cell more than once with
different choices for custom_df is against the rules.
That is dishonest.
The idea is that you have completed development and now on completely fresh data,
you are seeing how that system works.
So if you go back and make an adjustment to custom_df and try again,
as soon as you do that,
you've broken the rules, okay?
Uh, we don't have a way to really check this,
but that's the way the field works and it, you know,
progress kind of depends on people being responsible about this.
So run that code once.
And then, to help us see whether we can reproduce the results that you got,
in the cell right below that, this is step five,
just enter the macro-average score that you got so that we can,
if we rerun your system,
compare against that and maybe contact you if there's a big discrepancy or something.
Having done that, you just upload the notebook to Canvas [NOISE],
um, and if your system, as in the homework 1,
if it depends on external code or data, you know,
things that won't be in our course environments,
then put it in a zip archive and upload that as well, and then you're done.
Um, you have to do that by Wednesday at 4:30.
We are going to look at a bunch of the systems and we
hope to announce the results next Monday.
Certainly, next Wednesday.
I think it should be straightforward.
I think it should be really rewarding to see what happened with your systems.
Any questions or concerns?
All right. We have officially begun Bake-off 1.
Now we want to switch gears.
The topic for this week is supervised sentiment analysis.
Um, I'm gonna try to do a bunch of things with this unit.
So as you can tell from this overview here,
and by the way, the slides are posted,
so feel free to download them.
And as before, they include a bunch of
code snippets that I think will be useful for you in terms of
taking the material and turning it into code that you
can use for your homework and for your bake-off submission.
And I'm- I'm gonna leave some time at the end of the day
today to introduce the next bake-off and the homework,
um, so that you guys feel like you can get started right away on these problems.
Well, I'm gonna be trying to do for these two lectures is first of all,
introduce sentiment to you in a kind of general way,
because I think it can be a wonderful source of projects.
There's lots of great data.
And I'm gonna argue for you that I think the sentiment challenge is a deep NLU challenge.
Some people regard it as kind of superficial,
but I think with some serious thought about this,
we can see that this is a kind of microcosm for everything in NLU.
Then I want to give you some general practical tips that are kind of
outside of the code base but illuminating,
I think, about how to design
effective systems for sentiment but actually for lots of problems.
Then we get down to the nitty-gritty.
I'm gonna introduce the Stanford Sentiment Treebank which
is- which is kind of our core data set for this unit.
That should be pretty fast.
The main thing that I wanna do with you is make
sure you feel comfortable working with this code in 4 here,
sst.py, which is a module that is included in the course repo.
And it plays the role of vsm.py in the previous unit.
It's got like a lot of functions that I'm gonna ask you
to use for the homework and for the bake-off.
And frankly, also, my claim is that it represents a lot
of good practices around designing experiments,
especially in situations where you might wanna try lots of
systems and lots of variants of systems which is,
you know, par for the course in lots of types of machine learning.
Having done that, I think we won't get through that,
all of that today, but I hope to at least get to 4.
For Wednesday, we'll talk about methods,
and then we're gonna kind of dive into different model choices you might make.
So feature functions you could write if you're dealing with,
kind of linear classifiers,
and then we'll look at RNN classifiers and tree-structured networks,
which are more modern and maybe more successful approaches to sentiment.
And this is a nice topic because sentiment is great,
but it's also a chance for us to kind of get some common ground,
build up a foundation around best practices for supervised NLP problems.
That is something I'm trying to do in parallel with you.
So questions you have about methods,
metrics, models, all that stuff should be on the table.
We're kind of using sentiment as a chance to introduce all of that,
okay? The associated material.
This is a small typo, that should be 2,
um, but just to review.
So this is the core code,
and then there are three notebooks.
The first one is an overview to the SST.
I'm gonna kind of give a, a look at that today.
The second one is more traditional approaches to
supervised sentiment or supervised learning in general.
I've called that hand-built features because I think that's their characteristic.
And then the third one, oh,
that's also a typo, I'm embarrassed,
that should be SST.
The third one is, um, neural approaches.
So the RNNs, and the trees,
but also kind of intermediate models that use,
um, GloVe type representations,
distributed representations as the input to traditional linear classifiers.
It's a kind of nice progression of ideas.
Uh, then the homework 2 and bake-off 2 are paired in that notebook.
The core reading is this paper by Socher et al.
that introduced the SST,
and also introduced a lot of very powerful tree-structured networks for the problem.
And then I would suggest this auxiliary read- reading.
So if you want to just learn about sentiment,
this Pang & Lee thing is great.
Uh, it's a kind of compendium of ideas for sentiment analysis.
And then Goldberg 2015, that's different.
That is a really nice primer for doing deep learning in NLP.
Uh, it, it kind of unifies in notation and,
and in concepts a lot of the models that you'll encounter.
Uh, if you need a further review of kind of more basic supervised learning stuff,
then check the website.
I provided a few links to different online tutorials and stuff that
would be like one level back on linear classifiers and things,
which I'm kind of taking for granted in here.
Let's start with the conceptual challenge.
As I said, that sentiment was a deep problem.
Sometimes, people think of it as kind of superficial.
I want to claim different.
So just as an exercise,
let's ask ourselves the question,
"Which of the following sentences express sentiment?
And what is the sentiment polarity,
positive or negative, if any?
There was an earthquake in California."
It sounds like bad news,
maybe unless you're a seismologist who was safely out of the state.
Uh, does it have negative sentiment?
You know, this is a kind of edge case in the sense that you might-
depending on how you phrase the problem to the people doing annotation,
they might say yes, they might say no.
It's worth reflecting on, right?
But to give an answer of, "Yes,
this is sentiment and it's negative," you need to bring in a lot of other assumptions.
"The team failed to complete the physical challenge."
Negative or positive sentiment? What do you think?
First of all, is it a sentiment relevant statement or is it neutral?
I guess you could ask that primary question.
And the secondary question, is it positive or negative?
Well, that would probably depend on [NOISE]
what the team referred to and what your perspective was.
And I feel like if we know that it's another team,
this is clearly bad news.
Maybe we give that negative sentiment,
maybe not, maybe we exempt the whole thing.
Again, this could be a point of unclarity.
"They said it would be great."
This seems clearly to involve some kind of evaluative sentiment, right?
But it is arguably not the sentiment
necessarily of the person or the- the author or the speaker.
So is it positive or negative?
It feels like it's positive with respect to some other agent.
But as you can see from these continuations,
they said it would be great and they were right.
That seems clearly positive, right?
By- by normal definitions.
But they said it would be great and they were wrong,
that seems more like negative sentiment, right?
And the nuance there is that in this case,
for the first sentence,
we need to know not only that the adjective great was used,
but that it was used in the context of this verb of saying,
and verbs of saying do not necessarily commit the author to their content, right?
They're just reporting other facts.
They might create a certain bias,
but it's only until we get to the second sentence that we kind
of know how to resolve the sentiment here.
What about this one, the party fat caps-
the party fat-cats are sipping their expensive imported wines.
Negative or positive affect?
It sounds kinda negative to me.
It sounds like it's, you know,
playing a role, doing a little bit of mimicry.
The challenge here would be that there's a lot of positive words in that sentence.
So your system is apt to mis-apprehend the sentiment here as positive,
when in fact, the vibe that I get is pretty negative.
So again, like perspective is creeping in.
What about, oh, you're terrible.
I think that would really depend on the context.
On the face of it, it sounds quite negative.
It sounds like an accusation.
But if it's a couple of friends,
kind of teasing each other about a joke or something,
then it could be endearing, right?
And it would be hard to classify,
maybe the whole point of this little bit of
negativity is to create a positive social bond.
In which case, calling it negative is really to miss the point.
There's a lot of this stuff,
here's to ya, ya bastard.
That sounds really negative,
but that could be very affectionate in the right kind of context, right?
This is a real snippet from a review of the movie,
2001, I'm sure we have some 2001 fans in here.
This is a classic movie that is also very long,
and let's face it, at times, very boring.
[LAUGHTER].
Many consider the masterpiece bewildering,
boring, slow-moving or annoying.
That has a lot of negative stuff in it.
But, you can probably intuit that the author of this sentence likes this movie,
like this is probably embedded in a positive review. Not for sure.
But that kind of seems to be- it seems like they're building up
some kind of expectation that they're going to then kind of thwart, right?
So again, very complicated perspective fact.
And let's also just round this out by thinking about
all the complicated ways that we as human beings can be related to products,
and people, and events and so forth.
Think about long-suffering fans who like the first album,
but feel like all the other ones were sellouts.
Their negative affect is gonna have a whole lot of
complexity to it that isn't just like I dislike this new album.
Or what about bittersweet memories,
or hilariously embarrassing moments, right?
It seems off to just say that these things are positive or negative,
there's just a whole lot of other dimensions here.
And so my- my- my summary of that would
be positive-negative is one dimension that's important,
but there's lots of other aspects to sentiment analysis
if you think more generally about kind of affective computing.
Makes sense? I guess part of the lesson here is be careful how you define the problem,
and the other lesson here is that you could make this problem kind of endlessly deep.
And here's another perspective on this.
This is actually work that I did with Moritz
Sudhof, one of the teaching team awhile ago.
This is actually from a network, sorry,
a dataset of people transitioning from one mood to another.
They would give kinda mood updates on a social network platform.
And there are two interesting things about this.
First of all, the number of kind of moods that people are in, is astoundingly high.
And also, the kind of ways in which they relate to each
other and are differentiated, it's also fascinating.
And you start to see really quickly that standard things, like positive-negative,
or maybe like, emphatic or attenuating,
those are just two of many dimensions.
And then the other part about this since it's a transition diagram,
is that you can see that people are likely to transition in and
around certain subparts of this emotional space,
and less likely to transfer across to certain other parts.
And it kinda gives a picture of the contours of our emotional lives.
But it also shows you just how high dimensional this problem could be.
Unfortunately, we're going to look at just positive-negative sentiment, but,
there are data sets out there and ways of thinking about
the problem that would embrace much more of this nuance.
Final thing about this, because one thing you might do
and actually build set- a set for this on the first day,
is think about applying your sentiment analysis system in the real world.
And my- my pitch to you there would be,
you'll encounter many people in the business world who
think they want pie charts like this,
this is my imagination about what it's like to be a business leader.
In Q1, [LAUGHTER] the negative effect was 30,
and in Q2, it's 35.
Maybe that's an interesting lead, right?
To see that this breakdown in your reviews or
whatever your social media updates are trending negative.
But the thing of this is,
is it's very unlikely that that answers the question that they have.
Probably, even if this was estimated correctly,
what they really want to know is why.
And, one exciting thing to think about is how you could take
an NLU system that could do this basic thing and offer much more of that why question.
Because after all, the why answer is latent in these texts that you're analyzing.
And your capacity, or your system's capacity
to just label them as positive or negative is just
like the thin- thinnest slice of what you could actually be doing with that text.
And so, immediately, you could think about branching out and going deeper.
And I think we're now at a state in NLU where- where we could
think seriously about answering those why questions.
One more thing about this by way of general setup.
So we're going to have to be pretty narrow,
talking about the Sent- Stanford Sentiment Treebank.
But there are lots of other kind of tasks or areas within
NNLU that are adjacent to sentiment that I think are really interesting to address.
We won't have time for all of them,
but what I did here is, list out a whole bunch that I could think of.
And then for each one,
I listed a paper that I think would be a great starting point.
Obviously, each one of these things has a huge literature associated with it.
So I had to be kind of subjective and selective,
but what I tried to do is pick papers that either are just
great about giving you a picture of the landscape of ideas,
or even better, that have
associated public data that you could get started on right away.
And I've really actually tried to favor papers that have a dataset associated with them.
And it's not that I'm saying that these problems are the same as sentiment,
I think they're actually very different.
Um, even the ones that might superficially look like sentiment, like hate speech.
But rather, people tend to think about them together.
And I do think a lot of the methods that we
discussed transfer pretty nicely into these new domains.
Especially if you're willing to do the work of really
thinking deeply about what these problems are like,
in the way that I'm encouraging us to do for sentiment here.
And this is not an exhaustive list,
I just thought these are really exciting problems to work on.
Questions or comments about that?
kind of general setup, or about sentiment analysis in general. Yeah.
What would be a possible architecture for doing
some things similar to what you described with like,
say, given a review trying to figure out why the review is positive or negative?
Let's say expensive is a category,
or like, good food is another category like restaurants?
I think you're already on the right track.
Right. Take this review that has four stars associated with it,
and see if we can get down to the level of which aspects are being highlighted.
Maybe some of them are negative.
There are some data sets that give you that kind of multi aspect sentiment,
but it might be a kind of latent variable
that you want to try to induce with some uncertainty.
Yeah. It's a great question.
How about some general practical tips?
I think these are interesting.
They don't necessarily apply to the SST because of its unique characteristics,
but this is good stuff to know about in general. I claim.
First, some selected sentiment data sets that we won't get to look at.
Here's a whole mess of them.
Um, what I've tried to do actually is favor either data sets that are massive,
like the, uh, two Amazon ones.
One released by Amazon and one by,
uh, McAuley, who was a post-doc here.
Those are both truly enormous,
and offer breakdowns by product and by user,
and some other interesting metadata,
so that you could use them to do
some sentiment analysis that also broaden other kind of contextual predictors,
and you could certainly work at scale.
Like if you wanted to build your own, word representations,
the way we did in the first unit,
as the foundation for your project,
these data sets would support that.
Which I think is very exciting.
And RateBeer is also like that.
Also really large.
Bing Liu has released tons of data sets about sentiment,
and they range from just like, you know,
30 million reviews up from Amazon that he got somehow,
on down to kind of what you are working into ,
which is like annotations at the level of aspects of cameras.
So you could do much more fine grained work.
With Andrew Maas, I released a data set up here that gets a lot of use.
This might be a good kind of development dataset for you.
It's pretty big. It's got a mess of reviews
that are unlabeled for developing unsupervised representations.
And I think it's a pretty easy problem as they go.
Um, oh, also, this one is different, so I was involved with this work as well,
sentiment and social networks together.
I think this is a really exciting extension.
And if you've done work on kind of like social network graphs or knowledge graphs,
it's really cool to think about combining them with textual sentiment analysis.
We did that in that paper there, West et al.
And we released a data set of people who are running for office on Wikipedia.
And all those discussions are public,
and a lot of them are evaluative.
And so we have this network of people,
and we also have all these evaluative texts,
and we brought them together.
And then finally, the SST,
is one we're going to look at.
You have other greatest hits in mind?
Data sets you've worked with for sentiment.
This isn't exhaustive, I just thought these were cool ones basically.
There are also a lot of sentiment lexica,
and these can be useful resources for you.
And actually, this can be used for problems related to the SST.
So I thought, I would just highlight them here.
Uh, Bing Liu's Opinion Lexicon is just two wordless: positive, negative.
You're gonna work with it on Homework 1 because it's built into NLTK.
And it's really nice in the sense that it does
a good job of classifying words in a pretty context independent way,
and it's pretty well attuned to the kind of data that you might see on the web.
Uh, SentiWordNet is a project of adding sentiment information to WordNet.
And so if you're already doing work in WordNet,
it's a kind of nice counterpart.
Uh, and NLTK has a SentiWordNet reader, which actually I wrote.
Um, the MPQA subjectivity lexicon is a kind of classic.
And if you follow that link,
you'll see that that group released a bunch of datasets related to sentiment.
Uh, the Harvard General Inquirer is just a huge spreadsheet of words along the rows,
and then lots of different affective
and other social dimensions measured along the columns.
It's almost like a vector space that's been hand curated.
Um, and it's used a lot in the field.
LIWC is similar, Linguistic Inquiry and Word Counts.
Although, you kind of have to pay for LIWC.
But, uh, so if you don't want to fork over whatever they're charging,
you could just use the Harvard Inquirer.
And then, I listed two others: Hamilton et al.
Will Hamilton was a student in the NLP group here,
and he released these data sets called SocialSent,
and that's really cool because that's not only some lexica,
but also some methods for developing context-sensitive sentiment lexicon,
so that you could learn like the sentiment words that are associated with music,
and with cooking, and so forth,
and there's lots of nuance there.
So I find that very exciting.
And then, finally, this is just a big human developed, so human annotated.
It's kind of like an experiment,
a huge lexicon of scores for different words along a few different affective dimensions,
and I've worked very successfully with this final one in the past.
I think, I won't spend too much time on it,
but I do have this slide here that shows that for those classic lexica,
um, what kind of what their relationships are like?
So that you could figure out whether or not it's worth
bringing two of them in or you could concentrate on just one.
And I've done that by quantifying not only their overlap,
but also the number of places where they disagree on sentiment.
I think, I won't dive into this but it's there for you as a resource if
you decide that you're gonna bring these things into your systems.
[NOISE] Let's do a few nuts and bolts things too.
The first one I wanted to start with is just
tokenization because I think people tend to default,
to default tokenizers and sentiment is an area where you can see that,
that might be a mistake.
So here's my argument for that.
I've started with this imaginary tweet.
"NLUers: can't wait for the June 9 projects!
Yaaaaaay!" Uh, and then,
it has an emoticon that's gotten garbled and then a link to our class site.
So the first thing you might wanna do as a pre-processing step is
fix up the HTML entities that are in there,
so that it looks better.
And so that you can recover this emoticon,
but this step here,
it's very common to encounter text like this.
And you might wanna check to see whether it's worth fixing those up.
So if I've done that,
then I have this text here.
If I apply just a whitespace tokenizer,
it does okay, um,
in the sense that it has preserved the emoticon because it was written
in a kind of cooperative way over here with the whitespace around it.
Um, it didn't quite identify the username because it has a colon stuck to it.
It did well on these tokens.
It didn't do well on the date,
um, and it didn't do well on the URL because it left that period attached.
So, this is maybe not the best.
The Treebank tokenizer, this is the one that you
will likely encounter other systems using.
It's by far the most pervasive tokenizer,
just about any large NLP project has this step under the hood.
And this is a really meaningful choice because it comes from a different era,
and you can see that it has really made a hash of this text.
So, it took the username and split off the @.
It divided up the contraction,
so that the negation is separate.
That might be good for you. It might not be.
It broke apart the hashtag, which again,
it could- the hashtags are different on Twitter and certainly different for sentiment.
So you might wanna preserve them as distinct tokens.
Um, it broke apart all this stuff,
maybe that's okay and it also destroyed the emoticon.
It turned that into just a list of punctuation and then it also destroyed the URL.
Um, so if you think about using your text, you know,
text that you're gonna find on Twitter is probably a pretty dangerous movement.
It's certainly not gonna allow you to find the emoticons,
or travel around in the links that people have provided,
or do any aggregation by hashtag or username.
So this is maybe not the best and that kind of queues up a bunch of
stuff that we might want from a sentiment-aware tokenizer.
Right, it should preserve emoticons.
It should preserve kind of social media markup.
You might also want it to preserve some of the underlying markup.
It's meaningful for example,
that people have wrapped some words in the strong tag,
for example, to indicate that they were in bold.
Um, might wanna do something with people
hiding curse words because those are certainly socially meaningful.
You could think about preserving capitalization where it's meaningful, right.
It's one thing to write great and another thing to do it in all caps.
More advanced thing would be like regularizing the lengthening,
so that when people do Y,
and then just hold down the A key and then Y to indicate real emotional involvement,
where the longer they held down the key,
the more involved they are.
You know, all those things are just gonna be very sparse tokens.
And if you could normalize them to like three repeats,
then you might get a meaningful signal from them.
And then, you could think even further down the line about capturing
some multi-word expressions that house sentiment like out of this world,
where none of the component parts are gonna look sentiment laden,
but that n-gram there is certainly gonna carry a lot of information.
And NLTK also has this tokenizer here, which again,
I wrote or at least I wrote the core of it, uh,
that does pretty well on these criteria here.
So, if you're working with social media data,
I'd certainly argue for this one over the Treebank one, for example.
And here's what you might hope that the sentiment-aware one would do.
It would preserve the username and the hashtag.
This one also does the date which can be nice.
It gets the URL and the emoticon,
and it regularizes this.
And I can quantify that a little bit.
So I'm gonna report on some experiments and what I've done for these experiments,
is just take a whole mess of OpenTable reviews and fit classifier,
simple like Softmax classifier to them with different amounts of training data.
I'm always testing on 6,000 reviews.
But here, it goes from 250 training texts to 6,000.
The gray line is whitespace tokenization.
The green line is Treebank and the orange is sentiment-aware.
And this picture just shows that you get
a consistent boost on this sentiment problem from choosing the sentiment-aware tokenizer.
And it's aspecially large where you have
relatively little training data and that makes sense
because those are the situations where you want to kind of impose
as much of your own bias on the data as you can,
assuming it's a good bias because your system doesn't have much to work with.
Whereas, by the time you get to 6,000 reviews,
these differences have been minimized.
And just to round it out,
what I did here to test for robustness is train on OpenTable reviews,
again from 250-6,000 [NOISE] but I tested out of domain.
So I tested on IMDB reviews and it's the same kind of picture.
The, the performance is more chaotic because of the out of domain testing.
But I think pretty consistently,
it's worthwhile to do the sentiment-aware tokenizing.
The orange line basically strictly dominates the others.
Make sense? Questions or comments about the tokenization stuff? Maybe I've convinced you.
How about stemming?
People ask me all the time about whether they should,
they should be stemming their data.
So stemming is heuristically collapsing words together by trimming off their ends
typically and the idea is that this
is helping you kind of collapse morphological variance.
I think that's what people are always imagining that it will take like thinks,
thinking and smush them together into think and you'll
get like less sparsity in your data and therefore more clarity.
There are three common algorithms for doing this all in NLTK, the Porter stemmer,
the Lancaster stemmer, and WordNet.
And my argument for you here is that Porter and Lancaster
destroy too many sentiment distinctions for you to wanna use them.
Actually I think that applies outside of sentiment.
The WordNet stemmer on the other hand,
it doesn't have these problems because it's much more precise but
then again you might think it's not worthwhile when you see exactly what it's doing.
Porter stemmer, what I've done here is, you know,
the Harvard Inquirer lexicon that I mentioned
before, it has categories for positive and negative,
it writes them without E's for some reason.
And what I've done here is just give you a sample of words that
are different according to their Harvard Inquirer sentiment.
But that become the same token if you run the Porter stemmer.
So defense and defensive become defens,
or extravagance and extravagant become extravag,
or affection and affectation both become affect.
You can see what's happening here.
Real sentiment distinctions are being destroyed by this stemmer.
Yeah, tolerant and tolerable, toler.
Temperance and temper become, both become temper.
But here's the argument for the Lancaster stemmer.
I actually think this is even worse.
So again positive and negative and when you Lancaster stem
them this is to the point where, like,
fill and filth both become the same token,
or call and callous,
truth and truant for some reason both become tru.
I think you're doing real violence to your data quite generally frankly by
running the stemmers but for sentiment you're
obviously gonna be losing a lot of important information.
The WordNet stemmer is different,
so what the WordNet stemmer is doing is you need to give it
a string and a part of speech and then it will
use its very high precision lexicon to
collapse them down into what you might think of as a base form.
So it does do the dream thing for sentiment which is that,
like exclaims, exclaimed and exclaiming all become the same word.
It doesn't do it for noun forms.
And then on the flip side,
it does collapse all comparative and superlative variants
of adjectives down into their base form.
So it's doing it with very high precision.
If it's not in the lexicon it's not gonna do anything to your word.
Um, I think this is fine to do.
It's just quite costly to do it at scale.
Um, it's probably not worth it in
general for collapsing these if you have sufficient data,
and then for sentiment,
you might regret actually having combined together
happy and happiest because they're different in terms of sentiment.
And just to round this out, I did the same kind of experiments.
So this is OpenTable n-domain testing.
250 to 6000 reviews.
The sentiment-aware tokenizer beats both Porter and Lancaster.
Basically, that the idea there is that if you take
the sentiment tokenizer as your kind of default baseline,
stemming is only hurting you.
Maybe that's convincing and out of domain testing or did I not include that?
It's the same kind of picture when you do out of domain testing.
Makes sense? Any defenders of stemming wanna chime in?
Part-of-speech tagging. Oh, yes.
It's a question actually. Whenever do you prefer to use
these kinds of approaches for preprocessing versus character level models?
You know that's a great question.
Um, one thing I'll say and that you'll see emerge later in the course,
is that the move to first of all sequence models
because they process all of these things in
the context of what came before and maybe after them.
And then also analyzing done to the character level has
made these decisions in some cases less important, right?
So a lot of these newer models can recover from
a bad tokenization scheme because of all the contextual modeling that they're doing.
Overall I say this is a great development
because whatever you think about my sentiment aware tokenizer,
even it is probably not getting a perfect read on exactly what the unit should be.
Um, but I would still say that it's
probably worth your while to start all these systems in a reasonably good place.
Um, even if these differences are becoming minimized. Yeah.
Er, how would still bottom,
stemming in bottoms work on misspelled words?.
[NOISE] How does the stemming relate to misspelling?
How do they work on misspelled words because, like,
when it's sentimental very word
understand all the misspelled word would leads to [NOISE].
Like when it just make it go they, um. [NOISE].
The stemmers are gonna do what they do.
When you look at them they're just basically huge regular expression mappings.
So they don't care about misspellings because they
don't actually even care about word identity that much.
Um, for newer methods,
misspellings, this kind of a related point.
If you have distributed representations and you have a common misspelling,
it's likely to have a very similar representation to the one that's spelled correctly
in which case those systems one of
their selling points is that they gracefully recover from that stuff.
And therefore, reduce the need to, like,
run a spell checker as a preprocessing step.
A lot said, and the other side of this is and I think you can see that in this,
in these plots, like,
this makes it especially clear, I think.
The more data you have the less
these choices are gonna matter because the more your system
is gonna be able to uncover or recover from this bad starting point.
It's when your data are sparse that these choices really matter.
Part-of-speech tagging could help with sentiment.
That's my first pitch here because there are a number of cases in the English lexicon
where two words with different sentiment are distinguished only by their part of speech.
So arrest as an adjective, like, as in,
it's arresting I guess that's positive but arrest
as a verb according to the Harvard Inquirer is,a negative.
Uh, fine that so-that's a clear case.
So it's a fine idea.
That's a positive thing but to incur a fine.
That's the noun version.
That's typically a negative.
So it might help you to create distinctions by part-of-speech tagging
your data and essentially considering
every unigram to be a-its word form and its part of speech.
That's the first pass here and that's some evidence for it.
But even that sentiment distinctions transcend part-of-speech.
Here I've got a bunch of cases from SentiWordNet where one in the same word,
with the same part of speech has different sentiment.
So mean could be that your mean as in, you know,
you're not nice to people but a mean apple pie is a good apple pie.
Um, or something could smart.
That might be bad and that means it hurts.
But somebody being smart that's a positive thing.
Serious can obviously mean different things depending on the context.
That's something I experienced a lot in my life.
A serious problem might be a good one or
a bad one depending on your perspective and so forth.
So I don't have an answer here except to say that even adding as much preprocessing
as you can think of is not gonna fully
disambiguate the words in a way that aligns with sentiment.
That makes sense? I don't know,
that's a kind of ambiguous thing here though, like, is part of,
it is part of speech tagging worth it as a preprocessing step for sentiment?
It's not so clear to me.
That's a kind of empirical question.
This is really powerful though.
And again this is a heuristic thing and an intuition that we'll return to
in the context of the SST and this is just simple negation marking.
So the linguistic thing is that you have all this these ways of expressing negation,
and they obviously kind of flip intuitively what the sentiment is.
So I didn't enjoy it is probably negative,
whereas I enjoyed, it is positive.
I never enjoy it, negative.
No one enjoys it, probably negative.
This is an implicit negation here I have yet to enjoy it.
That's like, it might as well be I don't enjoy it.
And I don't think I will enjoy it.
That's a case where it's probably negative sentiment and the negation word is really
far from the associated thing that you want to treat as negated which is the word enjoy.
Early on in the sentiment analysis literature,
a few people have proposed a simple heuristic method which
is just basically as you travel along,
if you encounter an,
word that is negative according to
some lexicon that you've developed which would include,
like, n't, and not, and never, and no one,
and so forth and you have a whole lexicon of them,
then you just start marking all the tokens after that negative word with _NEG.
Up until maybe you hit, like,
a comma or a period or some kind of a punctuation mark that tells you
heuristically again that you've reached the end of the scope of the negation.
So what you're doing when you do that upending of
_NEG is essentially creating a separate token.
You're saying that this word when it's under negation is
a different word than when it's in a positive context and that will help
your system distinguish enjoy NEG from
enjoy positive and give your system a big boost presumably.
So that's a really easy preprocessing step.
That is basically just giving your statistical
model a chance to learn that negation is important.
So here's kind of what happens.
No one enjoys it would mark everything after no with NEG.
I don't think I will enjoy it but I might if you've got
your algorithm set to stop at a comma,
then it will stop that NEG marking at the end
of what's intuitively a clause boundary there.
And that's probably good.
And here's a little bit of evidence that this can really help.
So got a full comparison.
Gray is just whitespace,
green is Tree bank,
orange is the plain sentiment-aware tokenizer,
and red is the sentiment-aware tokenizer with NEG marking.
And that gives you a consistent boost all
across these levels of training data from 250 texts to 6,000.
And yeah, even out of domain,
this is a useful bias to have imposed.
It's really giving your system a chance to see that negation is powerful for sentiment.
That make sense? .
Er, is after the post-processing, what models consume the data?
I just used a simple softmax classifier and noted it down here,
which is just a pretty good standard linear model.
Uh, and that's really it.
It's just a bag of words classifier.
It's just that when I've done the NEG marking,
this bag of words has been kind of annotated in this clever way.
Excellent. That was it by way of general stuff.
Now, we're gonna dive deep on the Stanford Sentiment Treebank.
And this is perfect timing,
because this will give us a chance to talk about the code itself under SST.py.
And then you guys can leave here,
actually ready to do the homework if you want to,
and think about the bake-off.
So the SST project,
the associated paper is Socher et al 2013.
I was involved with this project.
This is a- it was tremendously exciting at the time for me.
It was the largest crowd-sourcing effort that I had ever been involved with.
Um, I remember feeling somewhat nervous that it would even work,
because we had people annotating hundreds of thousands of phrases.
Um, in retrospect, it looks kind of small.
Now, Stanford has produced annotated data-sets that are vastly larger than this.
But still, uh, it's an impressive effort.
Um, full code and data release credit.
There goes to Richard Socher.
I think, um, you can still- if you,
if you visit that link,
you can play around with the system.
It does wonderful visualizations.
You can even, um,
offer to give it new examples that it will then learn from.
Um, and then of course, you can use the code in lots of ways.
So a kind of model for being open about your data and your methods and your results.
Uh, it's a sentence-level corpus.
It has about 11,000 sentences,
and those sentences are derived from
a classic sentiment data-set that was released by Pang and Lee,
who are, were really pioneers in doing sentiment analysis.
And I actually, this paper,
at last year's NAACL won
the Test-of-Time award.
Uh, I think the argument there is,
that they really did set us off in
the direction of thinking seriously about sentiment in its own right,
but also as a great test bed for NLU models.
So it starts from those Rotten Tomatoes, uh,
sentences which were labeled naturalistically by their authors,
because it's a review data-set.
But what the SST project did, is crowd-source labeling,
not only those sentences,
but every single phrase in all of the trees that are contained in those sentences.
Uh, it's a five way label thing, actually,
reviewers were given a slider bar,
and then the labels were extracted.
And that was justified on the grounds that by and large people picked
points on this slider bar that were kind
of consistent with the labels that had been provided.
So the result is a Treebank of sentences that look like this.
Here, NLU is enlightening.
These are real predictions from the model.
All new, new test cases.
I was very impressed at how well this did.
Uh, it labeled NLU UNK,
but it still got this right.
So you have labels for all of the sub-constituents.
So, is is neutral,
NLU is neutral, enlightening is positive,
and as a result of enlightening being,
being positive, that projects up the tree essentially.
This is an example that I use to motivate, right?
So they said it would be great.
This is really cool.
It knows that be great is positive,
down here, and it knows that these things kind of aren't contributing sentiment.
And the sentiment signal goes pretty strongly up until the top here,
but then it's diminished somehow,
and the overall sentence is neutral.
And that's the prediction that I wanted from this sentence,
because I feel like this sentence alone does not tell us about the author's bias.
It just reports somebody else's perspective.
So, you know, who knows what was happening up here to cause this to emerge, but it did.
And then really cool,
they said it would be great, they were wrong.
It got that that was negative.
And it got that that was negative because it end,
it must know to project things from the right more strongly.
And so, that they were wrong over here with a one projected up to the top.
Even though this remains kind of ambiguously neutral or positive.
It didn't quite nail- they said it would be great, they were right.
I think right must not have a strong enough sentiment signal on its own.
Um, but at least it did figure out that this is kind of just neutral here.
Even though there's positive- positivity kind of weakly expressed on both sides.
Anyway, so I was impressed by the system,
but I introduced these examples more to show
you the latent power of this particular data resource.
This is kind of unprecedented that you would have this many labels,
and this kind of degree of supervision for the individual examples. Yeah.
The example that they said it would be bad, they were wrong.
That would be negative, because they said it would be bad,
it probably be neutral.
And then, like they were wrong,
and be negative if that's where a negative sentiment is coming from.
That's my intuition. Well, my intuition as a human is that you're right.
They said it would be bad and they were wrong, should be positive.
It seems like a very interesting stress test
for the system to see whether it got that right.
Because then it needs to know not only how to balance these sentiment signals,
but kind of how they've come together.
[NOISE] There are a few different problems that you can define on the SST.
There's a five way problem that's just using basically the raw labels.
They go from very negative,
negative neutral, positive and very positive.
And this is a breakdown for train and dev.
There's a test set which I'll return to and I've not shown
the statistics because I'm kind of trying to keep it out of our view for now.
Uh, but it's comparable to dev.
And I think this is fine,
but there are two things you might keep in mind about this version of the problem.
So first, intuitively, if you say,
think about sentiment strength separating out polarity,
then 4 of course is greater than 3,
but 0 is greater than 1.
I think it's much more natural to think about this as a kind of,
two scales with neutral kind of unranked,
with respect to the other two.
Maybe that doesn't bother you too much.
What should bother you is that,
if you fit a standard classifier to this kind of labeled dataset,
which has a ranking on its labels,
then your classifier will be conservative with respect to how well you're actually doing.
Because, it will regard a mistake between 0 and
1 as just as severe as a mistake between 0 and 4.
Whereas, really you might think about how you could get
partial credit for being close to the true answer.
And there are models that will let you do that,
even in the classification context,
like involving ordinal regression or ordinal classification.
But your model is probably not doing that, and so,
it's just worth keeping in mind for this problem that it's a little bit strange.
There are two versions of the problem that make more sense to me.
Uh, the first, is this ternary one,
which we're gonna make a lot of use of.
Here, you just group together 0 and 1 as negative,
and 3 and 4 as positive,
and 2 in the middle.
So you've lost some sentiment distinctions,
but at least with regard to the way the labels were given,
it seems quite justified to me.
And this one is nice, because you keep all of the data that you have available.
There is another version of the problem that is discussed
a lot in the paper alongside the five way one.
And this is a binary problem,
where we simply drop out the neutral category,
and then cluster together,
0 and 1, and 3 and 4.
Perfectly respectable, uh, the only shame here is that, first of all,
in the world, there's a lot of neutral sentiment,
not everything is sentiment-laden.
And the other is that we had to drop out a lot of our data.
These i- these statistics here are just for the root level labels.
So they kind of ignore all the labels that are down inside the trees.
This is the all nodes task,
where you actually try to predict all of the labels on all the sub constituents.
You get many more training instances of course,
because some of these trees are really big.
Uh, but you can define the same three problems in this way.
To kind of give an equal footing to all different kinds of models,
we're mostly not going to look at this problem.
I'd have, my only regard about that is just that,
this is one of the more interesting aspects of the SST,
that it has all the supervision.
And so, I do encourage you for projects and things to
think about how you might model the fullness of this data-set.
The labels would it ever be like a good idea to have two labels.
One for like the valence,
or one for this intensity and one for the valence?
Oh, absolutely, yes.
I mean, that's kind of,
I mean the SST doesn't have that kind of multi-way label.
But, my linguistic intuition,
my kind of understanding of the psychology of affection,
of affective states is that they have many dimensions.
And you can see that in one of those lexicons that was released by Victor Kuperman,
the last one that I listed which has a few different dimensions that
they take as kind of the true substrate for emotional expression.
Yeah, I wish we could do it.
Um, here, they've all been collapsed into a single scale though.
all of the ones like creating, yeah,
I guess like the very's, having that as a label,
whether it's very or another thing or two.
So 0 and 4 would be plus, um, emphatic.
And the rest would be minus emphatic,
as a binary problem.
And then you also have polarity,
which is positive, negative or neutral.
I think, oh, yeah, no, I take it back.
That's a great idea. That's worth thinking about.
Yeah. And certainly as a way of kind of seeing what your model is learning,
it's interesting to impose that distinction.
Other questions or comments?
Great.
Okay. Let's dive into the actual code.
This is perfect because I can cue you up to start working productively.
So sst.py, you'll have to get used to my interfaces a little bit.
But my claim is that having done that,
you'll be able to work really productively because I brought together- I've created
a little framework for you that should make you quite
nimble in terms of running experiments.
First thing, you just have all these readers.
Um, so you have train reader and dev reader.
And then each one of them has this argument class func.
And there are basically three choices.
If you leave that argument off,
it will give you the five-way problem.
If you set class func to ternary class problem,
it will be positive neutral negative.
And also binary class func will give you just the binary problem.
So these are nice prepackaged ways for different- getting different views on the data.
The notebooks explore all these different problems and for the homework and the bake-off,
we're going to do the ternary one.
Because I- I think it makes a lot of sense.
But this is easy and this is just kind of set up.
You could paste this in and follow along if you wanted,
SST is the library.
The trees are already there for you in your data distribution,
and then these are the readers.
And maybe the only other thing to mention is that each reader,
you can think of as yielding tree score pairs.
So I'll show you what the trees are like in this- in a second and the scores are
just strings as you would expect given the way the labels work.
I have a separate slide here on these Tree objects.
So they are NLTK Tree objects.
Up here, you can see I created one from a string.
This is just an illustration of how to do that,
and if your notebook is set up properly,
it will do this nice thing of displaying them as pretty intuitive trees.
So this is NLU, is amazing.
Down at the bottom here for subtree and tree.subtrees, that's a method.
That will cycle through all the subtrees for you.
Just a really useful method to know about.
And then up here this is basic tree, um, components.
So tree.label will give you the root level label for that tree.
And then tree zero and tree one will give you the left and right
children of that tree respectively, assuming they exist.
You can see that for this tree over here, the left tree,
the left child node is that subtree with- rooted at two and NLU,
and the right subtree is the verb phrase, "Is amazing."
And then obviously, that's recursive.
So tree 1_0 would take you down to two is and so forth.
There's a bunch of other stuff you can do with these trees,
but in my experience writing feature functions,
these are the important components that I needed.
If you think about writing a function that's going to crawl around in them,
this is kind of the nuts and bolts.
Okay. We'll build up this framework in three steps here.
The first is this notion of a feature function.
And this is a very simple example here.
This is a kind of bag-of-words feature function.
All the functions that you write-
all the feature function should opt- should take a tree as input.
And so if you think back to the readers, right?
Readers are yielding tree score pairs.
It's going to operate on the left value of those pairs, the first one.
And it needs to return a dictionary,
where the values in that dictionary are counts or booleans.
Ah, and they can be real-valued as well.
So like Integers, counts- integers, floats, and booleans.
And that's kind of the contract, right?
You can write any feature function.
It will be any function of individual trees as long as it returns a dictionary.
And what I've done here is do tree.leaves to get all the,
um, just the lexical items.
So I'm not really making use of the tree structure.
And then counter here just turns
that list into a dictionary where the elements have been counted.
So that's a nice really quick way to do a bag-of-words feature function.
And I've shown you, I created that same tree,
NLU is amazing down here.
Unigrams phi of that tree yields this dictionary. Does that make sense?
This is something that you'll want to make a lot of use of.
I'm- I'm assuming that part of your bake-off and part
of your homework will be writing interesting feature functions.
And they all- they can do- can do whatever you want
inside here as long as it's tree to dictionary.
Any questions about that? All right. Good.
Next step; model wrappers.
So this might look a little redundant at first, but bear with me.
You'll wanna write functions that take as their input xy pairs,
where x is your feature matrix,
and y is your vector of labels.
And what those functions should do is fit a model on that data,
so a little supervised training data,
and then return the fitted model. And that's all.
So here what I've done is logistic regression from scikit,
it's a good baseline model.
I said I wanted the intercept feature,
like the bias feature.
I specified solver and multi-class equals auto just so it wouldn't issue its warnings.
So it seems to be in the habit now of issuing warnings about those changing or something.
Ah, and then I fit the model crucially.
You have to remember to fit the model in here and then it gets returned.
And again, this might look like just a tedious way of calling the fit method on a model,
but as you'll see,
by putting wrappers around these things,
you can do a lot of other stuff as part of this process without changing your interface.
And for more sophisticated models,
you might want to do a bunch of
sophisticated things before you fit or part- as part of fitting.
And then, finally, this brings it all together.
SST.experiment is like a Swiss army knife for running experiments.
Um, I've given it here with all of
its default values so that you can see at a quick glance,
all the different things that you can do.
But the point is you just point it to your SST,
you give it a feature function and a model wrapper,
and that's really all that's required.
If you give SST.experiment those three things,
then it will train a model and run
an assessment on a data that's separate from its training data.
And it will give you a report like this.
Not only will it give you a bunch of information about your experiment encoded here,
but it will also print out a classification report.
And that's it, right?
So you have to write very little code in order to
test your feature function and/or your model wrapper.
And then if you want to try out different conditions like, you can go in here,
you could specify that your assess_reader was SST.dev reader and assess on the development data.
You can change the training size if you
haven't specified the assess_reader if it's doing a random split.
You can change the class function.
You can even- you can change the metric.
Um, you could turn off printing and I'll return to this vectorized thing a bit later.
But that's all kind of if you wanna do more nuanced experiments.
The thing to keep in mind is that you can
just very quickly test your feature function and your model wrapper.
And the other thing I want to say is that you'll see this
throughout for this entire unit and in fact,
for many units in this course.
When we see these classification reports,
we are gonna care mainly about the macro average F1 score.
Um, the reason for that is we have slight class imbalances.
Of course, for other problems,
the class imbalances can be really large.
And the idea behind macro averaging is that we care equally about all those classes.
Now, I think that's justified in NLU in-
sometimes the smallest classes are the ones we care about the most.
Um, micro averaging would favor
the really large classes as would accuracy and weighted average.
But macro is a kind of good clean picture at how you're
performing across all the different classes despite their size.
So keep that in mind as you look at these reports.
You'll kind of wanna hill climb on that value.
SST experiment returns this thing here,
unigrams softmax experiment and that contains a lot of information about your experiment.
Again, I think this is part of best practices.
It's like when you run an evaluation,
you keep as much information as you possibly can about what you did.
Every single time in my life that I've decided to leave something out,
I've regretted it because I wanted it later
and then I had to like retrain the whole system or something.
So what I've tried to do is package together all the information that you
would need to study your model's performance and to test it on new data.
And you can see that listed here.
You've got the model, the feature function,
the data that it was trained on, and the assessment data.
And that's relevant if you did a random split.
Because this'll be the only way that you could recover exactly what data you were using;
the predictions, the metric, and the score.
And then each one of these train and assess data-sets has the feature matrix,
the labels, the vectorizer,
which I'll return to, and the raw examples before a featurization.
I think that's really important for kind of error analysis because,
if you try to do error analysis on x and y,
you're just staring at these high dimensional feature representations.
It's hard to know what's going on.
You as a human,
unlike a machine learning model,
would prefer to read raw examples.
And so they're there for you as well.
So this kind of brings it all together.
This is on one slide here,
a complete experiment testing bag-of-words on a logistic regression model, right?
So set up SST_HOME,
phi returns the dictionary counting the leaves.
This is that simple fit_model thing,
and then experiment SST.experiment just runs that.
I didn't show it, but it would print
that report and give you all your experiment information.
Does that make sense? It's kind of like,
let's do experiments without a lot of copy and paste.
Without a lot of like just repeated cells that do basically the same thing.
I'm hoping that in a notebook you could use this to kind of reconstruct or, you know,
to conduct a series of experiments and study them
without having a huge mess. That's the core of it.
Let me say one more thing,
and then, well, actually we have time for a few more.
[OVERLAPPING] there's one more glimpse- I
want to give you a glimpse about what's happening under the hood,
um, because again, if you move outside of this framework,
this is a nice thing to know in terms of best practices.
So under the hood,
when combining your data with your feature function,
I have used the scikit-learn feature extraction DictVectorizer.
And I want to walk you through why I'm doing that because I think this is
a really convenient interface for doing all kinds of problems.
And I've done it by way of illustration.
So imagine my train features up here are two dictionaries a, b,
b, c and they each have their accounts associated with them.
That's like, you know,
if you write a feature function,
mapping a tree to a dictionary,
what the code internally is doing is applying that feature function to
all your examples and creating exactly a list like this.
So that happens under the hood.
But what a machine-learning model needs or at least all the scikit wants,
what they need is a matrix.
They don't operate on dictionaries.
They operate on matrices that look just
like the vector space models you were building before, right?
Strictly numerical data, where the columns are the meaningful units.
DictVectorizer maps, these lists of dictionaries into such matrices.
So when I call,
yeah, I setup the vectorizer here,
vec.fit_transform is the idiom in scikit on that list of dictionaries.
X_train here, that's a matrix.
I put it inside a DataFrame just so you could see what had happened,
but under the hood what it's really doing is operating on an NP.
I'm giving you an NP array.
But the Pan- Pandas DataFrame is nice because you can see that
the keys in these dictionaries have been mapped to columns.
And then each example: 0 and 1,
we have the counts of those features for that example.
That's really good because, for example,
a failure mode for me,
before scikit came along,
was that I would go by hand from dictionaries to NP arrays,
and I would get confused or have a bug
about how the columns aligned with my intuitive features,
and then everything would get screwed up.
Uh, now I just trust the DictVectorizer to do that.
So I as a human,
I'm very happy to have these dictionaries,
I think it's an intuitive interface.
But my machine-learning models want this and DictVectorizer is the bridge.
The third thing, that's really important about these DictVectorizers,
the third problem they solve,
suppose I have some test examples, so here I have,
the first one is a and it gets a count of 2,
a, b, d with their counts.
But notice that d is a feature that I never saw in training.
If I call my vectorizer up here,
and I just say transform on those test features,
then I get X test,
and notice that it's perfectly aligned with the original space a,
b, c, and d has been dropped out.
It's not part of my feature representations from my training data.
My model will be unable to consume d,
and so by calling transform,
I have just gracefully solved the problem that d would have imposed.
And also, it's doing the thing of aligning the columns and so forth. Yeah.
If you had fit transform,
it would have just made a new column for the entry?
Exactly. Yeah, and it would have four,
and I would have lost this correspondence with my original problem.
Yeah. Um, yeah, that's a great point of contrast.
This is so important because this is the way that you can
swiftly go from a train model and a vectorizer,
you know, process new data in the way that you expect for your model,
and get the desired results.
And again, this was an area where my code would contain mistakes before,
and now it just doesn't because of this DictVectorizer.
But also it's really good to know about,
and also it's good for you to know that under the hood this is what happened.
This is what's happening as we take
your feature functions and turn them
into something that a machine-learning model can consume.
[NOISE] Excellent.
This has been a lot of material.
The next phase of this is I'm gonna show you some methods,
hyper parameter exploration and classifier comparison.
But I propose to save that for next time because this has been
a bunch of material and I think in reviewing this section,
you're now pretty well set up to start doing the homework if you want.
So let me just review that quickly and then we'll wrap up.
It's a similar prop plot to the first homework and bake-off.
Except now focused on the sentiment, Stanford Sentiment Treebank.
So what I've done in this notebook is set you
up and give you a couple of baselines, mainly there.
I'm trying to document the interfaces for you.
And I'll do more of that next class but I've shown you
how to fit a softmax baseline and an
RNNClassifier and do some error analysis
that might help you bootstrap to a better system.
And then the homework questions and there are just three.
The first two involve a bit more programming than the first one.
So they're, they're, you know, they're worth a bit more.
But then you have this familiar pattern of you develop your original system.
And your original system will be one that you enter into the second bake-off.
So, that's the overall plot.
For the bake-off itself,
we're gonna focus on the ternary task, as I said.
And basically, you're just going to try to do as well as you possibly can on that task.
And in this case,
I don't have to impose very many rules, you know,
before I didn't want you to download external vectors and so forth.
But in this case, I feel like we can just say,
do whatever you think is best in terms of developing a good solution for this problem.
If you want to download vectors from the web, that's fine.
If you want to download other people's code, that's also good.
The one note that I have added is that it needs to be an original system that you enter.
So, you can't just download someone's code and retrain it and enter that.
You have to make some kind of meaningful addition or modification to that code.
But beyond that, anything goes, um,
except for the fact, of course,
that again here now we are really on the honor system.
For the first one, I could withhold the test data.
But in this situation,
the test data are just the test distribution
which you all have in your data folder already.
So we're completely on the honor system just like we would be if we were publishing,
to do all our development on the dev set
and only use the test set for that very final phase,
when you actually submit your system.
Okay? Yeah, I think that's it.
You know, the- the softmax baseline is the one that I just showed you.
And I run the experiment.
And here I'm using the dev reader for development.
So this is the kind of setup that you'll be defaulting to.
Next time I'll show you,
there's a few little tweaks that you need to make to
do deep learning models in this framework but it's really easy.
So I'll show that to you next time.
Giving you some tools for error analysis and then finally the homework problems.
The first one is like, write an original, um, feature function.
The second one is a kind of transition into the world of deep learning.
And then the third one is your original system. That makes sense?
All right, it's a bit early but I propose that we stop here.
Next time, I'm gonna finish this lecture and then I hope to
leave time for you to do some of your own hacking,
so that I'm sure that you'll leave here on Wednesday,
feeling like you can complete the homework.
The time window here is tighter,
so that's really important.
 I want to do a couple of announcements and then
uh have a quick guest lecture from Moritz Sudhof.
Just because it was irresistible in terms of Moritz
just having done a project that kind of bridges us
from the VSM unit into the big themes that
I tried to introduce last time around sentiment analysis
and why you would want to do it and tasks that are adjacent to it.
But first, just a couple of quick announcements.
First, uh, it's really exciting to see the bake-off submissions coming in,
I've been checking them periodically throughout the day.
Uh, I'm not going to reveal anything about
the scores because we want to reproduce everything,
but I will say that I myself I was pretty proud of my system,
I thought I was doing pretty well,
and it's pretty clear actually that I would have gotten about 30th place.
Uh, some people have really done better than I did.
Uh, but anyway we will have a fuller report after we've done some reproductions next week.
Uh, the other announcements I wanted to make is like
my bias for this course is definitely that you all come up with your own projects,
I think that's more exciting.
But when people write in with project ideas that you might, you know,
for ongoing things that you might want to join,
I can't say no because some of those things are really exciting as well.
And I wanted to just mention two today.
So this first one,
this is from the people at Gridspace which is
an area startup that does a lot of um, speech-to-text work,
they have lots of Stanford connections as well,
some of you might have encountered them.
They're developing some new datasets that are gonna
be kind of quasi-public not protected data,
so it's pretty free use and they're around um, dialogues.
Um, and at least part of those are people interacting with artificial agents.
So if you're interested in that project they
have this Google form that you could fill out.
And then I think they'll get in touch with you about sharing the data and so forth.
Um, and actually this is exciting because they're still in
the process of getting feedback from people,
potentially you, about what kind of labels
they should get and how how they should design the data-set,
so this is a chance to kinda get in early.
The other thing I would say is that I
attached a bunch of sample files which they sent me.
And the automated_dialog
MP4 is hilarious as advertised here,
so check that out.
And then the other one is from Vinay Chaudhri who has done this in the past.
He's got a project ongoing that some students worked with
successfully in the winter for the CS224N.
Uh, and this is nicely timed because
the centerpiece of this is relation extraction using distant supervision,
which is our topic for next week.
So you potentially get a nice confluence of the work
we'll be doing and a project that you might develop with Vinay.
So, uh, again I've provided
his contact info and I think you should feel free to write to him.
Okay. And then before we dive back into the SST,
let me invite Moritz to come up.
Um, as I said this was just a great opportunity because, you know,
Moritz is doing work kind of in industry right now,
and the topics are nicely aligned with both things that we've done so far.
Thank you. Right. So as we're diving into the next unit,
many- many of you are already starting on your projects or at least thinking about it.
I wanna take five minutes to just do a digression into
politics and also make
make my case that you should not
forget about anything that we've learned in the last two weeks,
because it's extraordinarily useful for whatever you're going to be doing moving forward.
And to prove that point,
um, let's talk about politics.
So five-minute digression, politics specifically, um, political polling.
Um, so I do some work with a Democratic political polling company.
And uh what I learned about the space,
which is kind of interesting,
um, is they, political polling is actually really bad right now.
Most of it still happens on the phone.
And you get these like really kind of structured questions like,
do you care more about balancing the budget or income tax?
Choose A or B.
And so the, the,
the actual like information that politicians
politicians get back is very sterile and it's like highly issues-based.
Um, but what they really wanna know is they really wanna know like as a voter,
who are you voting for and why?
What's driving your decision?
How are you thinking about candidates?
How are you thinking about the election?
Like what messages are resonating with you.
Um, what's top of mind for you.
They wanna get into this a lot more about attitudes, perceptions, opinions.
And so, the parallels of sentiment are clear where if we know that
Beto O'Rourke is you know has 32% of the vote and Elizabeth Warren has 18,
is the same as knowing that one product has a five-point,
you know, a five-star rating, the other has a 4.5.
The question is why?
What's the difference between those two?
Why- why do some experiences resonate more than others?
Why are some, why do some things receive more support than others?
Um, and so imagine
that you are building an NLU system and it needs to answer these questions.
What are voters looking for in a candidate?
Why are they- why are they supporting their stated choice for the election.
Um, and for this dataset, we're going to be looking
at three quick questions from a survey.
They were asked what qualities do you look for in a president,
in a candidate for US Congress or Senate or for somebody for a state or local government.
And we also have for each respondent the party identification.
So baseline you have a few thousand responses of people basically talking about what is,
how are they framing this election?
Who are they supporting and why?
And we want to start to understand, okay,
how can we give politicians real kind of
richer feedback on what's driving these decisions?
Um, and so I wanted to show you a notebook but I couldn't get my own computer connected.
But the only reason I wanted to show you a notebook is
because this is literally the only code in
that notebook because using our wonderful VSM code,
I took a, I built my own like word-by-word matrix
but that's everybody knows how to do that.
You call these two lines and suddenly all the rest of
the exploration basically is just based off of these two lines of code,
so I think it does a nice job of showing that how
quickly you can get started with a totally new and unknown dataset.
Um, so what I wanted to just do very quickly is look at some of these TCGA plots.
So let's say that we're interested in understanding,
okay do Democrats and Republicans make decisions differently based off of,
um, uh, for, for the election?
Um, so this is obviously you can kinda just see there are some colors here.
So I've we- I've colored the terms based off
of whether they are more associated with Democrats or Republicans.
So, um, green is independent,
red is Republican that's kind of up here.
How do I- I'm not Chris, I'm Moritz.
Red is up here,
blue is more down here.
And if you actually zoom into it,
there's a- there's a couple of nice things that happen,
I've already annotated it just for clarity here.
You can see that on the Republican side,
people are talking a lot about issues.
So um the words that you can't really read up there are words like defense,
fiscal, um, you know, words like Constitution.
Um, these are all about conservative values,
immigration, policies and values.
But if you look at to where the clusters are more dominated by blue,
here you get things like collaborative,
co-operative, composed, articulate, charismatic, well-spoken.
These are all descriptions of personal characteristics.
These are descriptions of somebody that
probably doesn't match how these people are thinking about Trump.
Um, and so you can see already kind of a clear split between how,
uh, the Democrats are thinking about candidates in the election and how Republicans are.
We can also look at this again taking advantage of that structure we had in the dataset,
which was, we also know what office they're talking about.
We can look at the same thing,
this time we're coloring the colors, the words,
based on whether it's more likely to be in a review or in a comment about presidential,
local, or congressional elections.
And as you would expect,
there's a ton here,
of the personal qualities this blue,
this dark blue, that's all uh, presidential.
So personal qualities like temperament, being quick,
being unifying, being a negotiator,
being classy, those are all super important for president.
But if you look more at the area over
here where it's the other colors for local elections,
whoops, sorry Chris, um,
you can see that it's a lot more about issues.
We have advocates accessible,
there's health care, affordable housing,
these are much more the bread and butter of like I have local policies that I care about,
uh, that need to get done.
I'm gonna comment here, great work Moritz.
Oh, wow, thanks Chris.
Um, and then the final thing I wanna show, so okay,
so this has been- this was like how long did it take, uh, me to do this?
All I needed was to take the original data and then
call two lines of this code and another visualization line.
And I feel like we're already starting
to get a sense for how we would tackle this problem.
So we're building an NLU system to answer the question; what are candidates?
What are voters looking for and why are they supporting candidate X over Y?
So I think one thing we know is we would have to be sensitive
to whether they're talking about issues or personal characteristics,
that seems to be kind of a cleavage in our data.
And then the other thing that we might want to understand is like, okay,
uh we wanna understand personal qualities more and how those,
um, which qualities are resonating more with other people.
So maybe we dig deeper and try to find sub-clusters of, you know,
this is about decency and humanity and fairness but this is about being well spoken.
Um, and so this is- this would be a first step.
You're diving into a new unknown dataset.
You wanna know what is in it,
how do I actually solve those higher-level NLU problems I wanna solve?
The code that we've all explored so far in the first unit is your best first stop.
Um, and even for- I got paid to do this by the way.
Um, even for like an industry,
this sort of code is it's not just starter code.
It can give you a great um handle on- uh,
on a dataset on a problem.
Um, and so as you embark on this next unit and your projects
uh I encourage you never to forget the humble vector space models, um,
and know also that domains like politics, like customer, um,
understanding and experience, like employee and their experience,
all of these domains need NLU people.
Um, they do not have it figured out.
Um, so it's up to all of us to help them and do exciting new things for our projects.
Thanks. [APPLAUSE]
Oh yeah any questions?
You guys are convinced, oh there we go.
Yeah. Why are you assuming that [inaudible] [BACKGROUND].
Yes, so the question here is somebody answered a survey,
do we even care about the data that comes back?
Do we- do we trust it? Does it- do we think it means anything to us?
So the deeper question there is we actually have, you know,
if- in the actual application of this analysis we have data on
turnout and previous supportive candidates and so
there are things that we can verify about like, yeah, these people vote.
But I think the broader question is,
when people are telling you about their experience,
should you care? Does it matter?
And I'm gonna say- I'm gonna make the claim that whenever people are being emotional,
uh, and expressing opinions,
you should care because they have taken,
like they did not need to fill out these surveys or offer these reviews.
Uh, and so there's- my first bias is always there's something of value here.
They had some reason for telling us this and
an aggregate if a million people tell you something you can learn something from it.
Um, so I'm always optimistic I guess to answer that question.
Another question.
[NOISE]
[inaudible].
Absolutely. I think uh- I mean I think one thing that is true overall is in politics,
if for presidential elections in America, style matters.
So there's always going to be some of that entering,
but I think definitely like it also speaks to kind of in the age
of Trump things are things are different and like the word,
orange showed a lot,
showed up a lot in this dataset.
So you would not expect that to ever be
a relevant thing to say in a presidential survey otherwise.
Um, so I think like, but that also speaks to how,
like when I applied GloVe to the same data it failed,
like it didn't because the- this- the length- the words people were using in
this context just means something a lot more
specific than in all of the English language,
um, so there's also kind of PMI coming back to the rescue when GloVe failed.
Okay, um, let's dive back into the sentiment stuff.
And just to recap, what we did last time was, I set the stage.
I talked about sentiment as
a general problem in NLU and I tried to make the case that it's
a very interesting problem even if at first blush it might look kind of simple.
And then I gave you some general tips,
showed you some evidence that it's worth thinking about preprocessing your data,
how you do that thoughtfully and so forth.
I introduced the Stanford Sentiment Treebank that's up here and
highlighted some of its unique properties
as a dataset in general and certainly for sentiment.
Then we spent a bunch of time walking through the basics of sst.py.
And I'm gonna show you one screen from that- that kind of summarizes that whole unit.
The reason that's important is that's what you want to work with
productively as you're doing your Homework 2 and Bake-off 2.
And in fact, I'm going to try to get through this material at a leisurely pace,
but we'll keep uh,
keep moving forward so that we can have
some class time today to make sure that you guys are all set up and working
productively with it and a bunch of the team is available in
the classroom and then did a post on Piazza in case you're
remote and wanted to just log in and chat with someone about how things are working or
maybe take a step back from where we have and think about regular supervised learning,
the kind of stuff that I'm sort of taking for granted in this course,
but I think the team is happy to fill in any gaps that you might have.
But the point is, since the time window is tight,
we want you to get good with the code by
the end of the day today and we're available for that.
This slide here kind of summarizes the entire framework and in fact,
what I'm showing here is kind of for
simple linear methods which we're going to explore first,
but exactly this same framework works for
all the deep learning models that we'll discuss and it's meant to be flexible and
modular in a way that will let you run a lot of experiments
without making a lot of mistakes and the long and short of it is,
you know, set yourself up to point to the data distribution.
Your feature functions here I've called it phi should
always operate on trees and return dictionaries.
Let's assume they're all count dictionaries.
You should also have these model wrappers which take in a supervised dataset,
an X, y pair fitted model and return that fitted model.
And then all you have to do to run an experiment is call
sst_experiment pointing at
your data distribution with that feature function and that model wrapper.
And that's it and already from here,
you can see that there's lots of space to explore.
If I wanted to try Naive Bayes or
a support vector machine I would just write a different model wrapper.
If I wanted to explore different feature functions,
I would just write new functions of
trees and that would be very quick in terms of evaluating and by default,
this is gonna evaluate on random train test splits that are drawn from the training data,
um, and you could periodically test against
the devset to see how well you're doing in reality.
That's the rhythm that I'm imagining,
and I give you a quick peek under the hood at how this code is designed.
I would encourage you to look
yourselves and find out even more about how this is working,
but it's kind of all based around these dict-vectorizors,
which I think help us avoid a lot of
common coding mistakes when building feature representations of data.
That's the quick recap.
Are there any questions or comments about it before I dive into this new stuff?
Any concerns that might have emerged over the last couple of days?
All right, let's dive in.
We have budgeted some time later in the term when you guys are in the thick
of your projects to talk a lot about methods and metrics.
And we're going to return to the two themes that
I'm going to introduce now in that context and
kind of talk about them even with a little bit of philosophy behind them.
But I want to introduce them now because I think that both of these can make
you a better experimenter right from the start.
Those two things are kind of hyper-parameter exploration and classifier comparison.
So let's start first hyperparameter,
here I've called it hyperparameter search and I want to give you the rationale for it.
So I'll just walk through this argument. First, some terminology.
The parameters of a model are those
whose values are learned as part of optimizing the model itself,
those are sometimes called weights, right?
And that's mainly what we think of when we think of machine learning is that like from data
our model has a capacity to set
all these parameters in a way that's effective for new data.
I'd say equally important are what are called the hyperparameters of the model.
These are any settings of the model that are outside of the core optimization process.
And some examples are like in GloVe or LSA,
you have the dimensionality of the representations that you're going to learn.
Um, for- for GloVe you have the learning rate,
for GloVe you have Xmax as part of the weighting function and alpha,
which is also a preprocessing step essentially that does something to your count matrix.
Neither of those values is learned as part of GloVe,
but they're super important in terms of what you learned.
Also, regularization terms, hidden dimensionalities,
learning rates, activation functions.
You can even go so far as to say like the optimization method itself,
the algorithm that you use is the hyperparameter to your model.
There's no end of these things essentially,
if you go on to scikit-learn and just look like in the linear model package,
that thing's like logistic regression or if you look at
the support vector machines in there or the Naive Bayes models,
they have dozens of things that are
hyperparameters and you can see them codified in code because it says like,
here are the keyword arguments.
They have some defaults,
but that's just one of the many values that
you could explore for each one of those things.
This is crucial because to build a persuasive argument,
you need to be thoughtful about your hyperparameters.
And the rationale there is what I've said here,
every model that you evaluate really needs to be put in the best possible light.
The- to round that out otherwise, right?
In a kind of antagonistic situation,
you could appear to have evidence that your model,
your favorite model is better than some other one just by
strategically picking hyperparameters that favored yours over another one, right?
Pick really good settings for your model and ones that you
know are kind of degenerate for the other one and then you say look,
"My model is better." That's a problem.
Hyperparameter search is all about kind of making sure that
you don't have the opportunity to do that opportunistic selection.
And the other way to think about this is that,
in science, there is a kind of antagonistic dynamic that happens,
and I think it happens primarily in the service of making sure we make progress,
which is that you submit your work somewhere and some referee is now evaluating it.
And the default mode for that referee is to think,
"Does this person really have results, right?
Can I trust what this person is saying?"
And from their perspective, they might think, "Well,
they've only shown me two settings,
how do I know that they didn't pick those settings
in a way that would rig the game in their favor?"
And what that referee is really looking for is evidence from
you that you have done what I have described in three here,
which is put every model that you evaluate
in the best possible light given the data that you have.
And what that really implies then is that when you evaluate these two models,
you do quite extensive hyperparameter search and you describe
not only the- the space that you explored and that- but
then also report some statistics about how those models performed.
Certainly, the best-performing model,
but maybe other information about average performance and so forth.
Once you've done that,
if your model wins,
we have a lot more confidence and your referee is gonna feel less
antagonistic because they can feel like really both of these models were given a chance.
And it doesn't need to be antagonistic,
you yourself could be that critic, right?
You're evaluating a bunch of models,
you want to enter the bake-off.
You have every incentive to really and truly pick the best model.
In that case, you should explore wide ranges of
hyper-parameters so that you can give the best entry given the things you're exploring.
So, I'm encouraging you to do this.
Another nice thing I'll say about this is if you use the code to do this search,
then you can set up a grid of hyperparameters that you wanna explore,
Click Run and then go to the movies or take
a hike and know that that entire time you are working hard exploring,
finding the best possible model.
That's the reason for automating this stuff.
Um, what I want to talk with you about later
especially in the context of your projects is bringing some perspective to this.
Really, we're talking about infinite spaces of hyper-parameters and
you could spend infinite dollars doing these explorations.
Someone has to impose a budget on you and I think we can have
a discussion about how best to think about those budgetary constraints for,
you know, a priori constraining
your search space and convincing reviewers that you haven't done anything harmful.
But for now, I think it's okay.
I think what we just wanna enable you to do is some exploration of
pretty reasonable spaces of parameters for your models and that's what this is all about.
So again, this is a complete experimental setup.
What I've done is leave phi alone,
it's just a bag of words classifier
representation but fit soft-max with cross-validation.
Here you start to see the value of having put
wrappers around all these models as opposed to just calling fit,
because what I do in this case is first setup a logistic regression model.
I decide that I wanna cross-validate over five-folds,
that's cv = 5,
and then the really interesting thing is this what in scikit-learn is called this param grid.
So it's saying for the intercept bias,
I'm gonna try the model with and without for this value c which
is the inverse regularization strength for the- oh yeah,
the inverse regularization strength.
I'm gonna explore this range of values from
0.4-3 and I'm also gonna explore which kind of regularization I do.
If I pick L1,
I'm probably gonna get a model that favors really sparse feature representations.
A lot of the values for the weights will go to zero whereas
L2 will give me more non-zero weights kinda evened out.
So I'll explore that as well and then I've
included in the software for the course in utils,
fit classifier with cross-validation.
You give it your data-set XY,
your base mod and your param grid and the CV
value and it will find for you via cross-validation.
So it should be pretty responsible,
the best model and then you return that and that's so by default,
here it's exploring the full grid of values.
So these spaces grow very quickly,
that's why you might wanna pick a long movie or
a long hike if you've picked a large param grid.
But the point is that your model will churn away at this and it will
return what in a data-driven way it has decided is the best model,
and then you could enter that into the bake-off or into a subsequent evaluation. Yeah?
[inaudible].
It checks every setting,
every combination so it does true 0.4 L1,
true 0.6 L1 like that for all of the values.
That's what it's doing, it's called grid search.
I've hardwired grid search in,
it's perfectly reasonable to think about things like random sampling from the space.
I think that typically performs basically as well and then there are
more sophisticated packages like Scikit optimize that will try to do more data-driven,
model-based exploration of the parameter space.
But all these things come down to the same intuition which is
explore wide under some budgetary constraints.
And this function here best_mod,
I think it actually prints out,
sorry yeah, when you call that it prints out the best parameters.
You'll get some feedback but you also get
the model itself which you can use subsequently.
In terms of SST experiment,
nothing changes and the reason nothing changes is because all of
that cross-validation stuff was packed into this model wrapper.
Makes sense? All right.
So go forth and explore widely.
Let your computer run overnight or something like that.
Second methodological thing that I thought I would introduce now is
classifier comparison and let me
set this up again thinking about interacting in the community.
So suppose you've assessed a baseline model B and your favorite model
M and your chosen assessment metric favors M. So good news, right?
Looks like you won, but is your model M really better?
That's a deep question.
So first of all, if the difference between B and M is clearly of
practical significance like your model is
gonna save lives and you can show it's gonna save 1000 lives.
Then maybe you don't need to do a subsequent round
of like statistical comparison or anything.
Right? Because maybe it's just clear that we should pick your model.
Um, but still especially in this age of deep learning,
you have to ask the subsequent question of whether or not there's
variation in how either of those two models performs.
Because maybe on one run,
you saved 1000 lives and then another actually your baseline model was was saving more.
Um, so even in the case of practical significance,
you wanna do- might wanna do something further.
Now I've offered two methods for doing something further.
The first, is to run a Wilcoxon signed-rank test.
This is just wisdom that I got from the literature.
That's a kind of non-parametric version of
the t-test that's not assuming anything about the distribution of
your scores and is allowing you to assess based
on repeated runs of a model which one is likely to be better,
and we can talk later about the rationale precisely for choosing that test
but I guess the bottom line for here is that it's gonna be a pretty conservative,
statistical test that will probably especially if you can afford to run the model
a lot give you a pretty robust picture of
whether or not your model is truly better than another one.
That's really good. The only downside there is you have to
be in a situation in which you can repeatedly assess
your two models B and M. So your dataset has to support that like
a lot of random training test splits and also your budget has to support that, right?
If it takes you, uh,
a month to optimize B and M,
then you're probably not gonna be able to do this kind of testing.
Because you maybe can only afford to do one or two runs
and really what you wanna have done is like 10 to 20 runs.
In those situations, my,
uh, offer to you is McNemar's test.
So this is applicable in a situation in which you can just run the models once,
you get a single confusion matrix from
the models and it's essentially operating just on that fixed set of values,
and the null hypothesis that you're testing there is
basically do the two have the same error rate?
So this is noisier and you might have less confidence in it but I
think it's better than nothing when it comes to comparing classifiers.
It's certainly better than just looking at the raw
numerical values and deciding that the larger one is clearly superior.
That makes sense. And again if you think about reviewers,
reviewers when they see just two numbers,
they want context, they want to know,
well, how big is that difference really, right?
And, you know, practical significance is the best answer to give,
but then these things will further substantiate.
And in situations in which the values look small,
you might still have a good argument in favor of
your model if the two models are very consistent in their behavior,
and then you could use these tests to substantiate that
even in this face of what might look like a small difference.
And I hope I've made that easy.
So just to set that up,
here what I've done is,
you know, fixed bag-of-words feature representation phi like before.
But now I have two model wrappers.
One I've called fit_softmax,
you might call it fit logistic regression,
and the other is fit_naivebayes.
This is a classic kind of comparison especially for sentiment,
like let's see which is better,
logistic regression or naivebayes.
So you set up those two model wrappers,
and then the two ways you can do this are first again,
this is a kind of Swiss army knife, sst.compare_models.
You give it at least one feature function,
and if you give only one,
it will assume that it should use both for
both experiments and at least one train function.
Here you can see fit- filled that out with fit.softmax and fit.naivebayes.
The rest of the things are just default values that you can probably leave alone,
and it will run all the comparisons that it needs to to run the signed rank test,
which is given here as stats_test.
And what you get out is the two means for the models,
this is just printed, and the p-value.
And up here you can see I'm also
returning the full vector of scores in case you want to like
plot them to get further information
beyond just the means about how these two systems compare.
One might, you know, have chaotic performance that you see
when you look at the scores and another might be more rational.
[NOISE] And then the other test kind of simpler to run,
here I've just run two fixed experiments,
softmax and naivebayes, and then you can run the McNemar's test.
And here you just have to make sure that you've run them on the same data.
And this is the interface here.
I just get the actual values from one,
one or the other of these because they're meant to be
the same and then look at the predictions.
And so again, this will take a while to run,
because you have to run a lot of experiments,
and this requires running just one experiment.
Questions about that? All right.
Those are the two methods things that I wanted to do.
Let's dive into this feature representations stuff then.
I'm just doing this by way of example,
and the idea is that you see having seen
a few examples the ways that you can
start to get creative about how you represent features.
So so far what we've done is bag of words.
It's kind of small but what I've done here is just do bag of bigrams,
and you could do trigrams and so forth.
That's pretty standard.
It's worth trying out.
An interesting thing that you can do because we have
tree structures is actually use that structure.
And so what I've called that down here is five phrases,
and what it does is represent the data as
all the words that occur in isolation and all the pairs of words that
occur as siblings of the same parent on
the assumption that it's interesting that is amazing, form a phrase linguistically,
but NLU and is do not.
And you can see the differences here.
So NLU and is for example is part of the bigrams,
but it is not part of this phrasal representation because there's
no single parent that dominates NLU and is.
And if you play that out over the very rich tree structures that are in the SST,
you find that you're getting very different representations than you
get from a simple linear pass like bigrams is doing.
And you can see down here I've just said the height is
less than 3 to get just these local trees,
but you could set it higher, right?
And that would be a version of like getting higher level n
grams because you'd be finding larger and larger chunks of these trees.
The other class that I want to highlight here is negation.
So last class, I highlighted a heuristic method that I
think is very powerful of just finding negation words that are in a lexicon,
and then marking them although following tokens with _neg to kind of
indicate that they are in the scope in some sense of that negation word.
The problem with that is that that can be pretty imprecise.
You end up depending on things like punctuation,
and if the punctuation isn't there,
then this _neg marking just runs on and on way past
what you would think of as any kind of semantic scope for the negation.
Because we have tree structures,
we can be much more precise about this.
So like in this example,
the dialogue wasn't very good but the acting was amazing.
It seems clear that the negation is meant to target only very good.
That's hard for us to keep track of in the linear method,
but it's easy with the tree structure,
because basically what you want to do is write
a feature function that when it finds a negation,
goes to the parent,
and then marks everything that's,
that's below that parent.
And that's very close to what linguists think of as the scope of the operator.
And that's nice because then all this clearly unnegated stuff is left alone.
And that's a very general idea.
There are lots of things in language that take scope in this way that have
semantic influences over the things that are next to them in the tree.
So we actually, we touched on this last time.
They said it was great.
It's not a speaker commitment to it being great,
whatever it is, because of this verb of saying.
And so you can imagine writing feature functions that are also marking
things that are in the scope of verbs like say and claim,
and maybe doubt, and deny marking them in
some special way so that they can get treated differently by your model.
Here I did it another one. It might be successful, right?
That's a kind of hedge,
very different from it is successful,
and you could capture that by doing some of this scope
marking from might down into the thing that's next to it.
And this is something that's really possible only because we have these trees.
[NOISE] And here are a few other ideas.
So obviously, lexicon derived features,
you're gonna do some of those for Homework 1.
You could also think about modal adverbs marking their scope like it-
it is quite possibly a masterpiece or it is totally amazing.
Those adverbs are doing something special to commitment.
Another idea is what in the literature is called thwarted expectations.
This is the case where I build up what seems to be
one kind of evaluation only to offer really another one, right?
Many consider the movie bewildering,
boring, slow-moving, or annoying.
That speaker might be building up to
a positive endorsement because of this shift in perspective they're performing.
And one signal of that is that they're kind of laying it
on thick with all of this negative language.
It could happen in both directions.
It was hailed as a brilliant unprecedented artistic achievement
worthy of multiple Oscars,
but it was terrible, right?
That kind of review is one that your model is going to struggle with because of
all the indicators of positivity balanced by that one word that was negative.
But the imbalance actually might be a signal to
your model that somebody is keying into this thwarted expectations thing.
So you could just count all these words essentially and look at their ratios.
And then even harder of course kind of like the Holy Grail for work in
sentiment analysis is getting a really good grip on non-literal language use,
like if someone says it's not exactly a masterpiece, it's probably terrible.
If they say it's 50 hours long for a movie, that's hyperbole.
They probably don't literally mean it,
and they're doing something special socially or
emotionally when they pick that hyper- hyperbolic expression,
right, or the best movie in the history of the universe.
That one is hyperbole and you feel like it could go either
way in terms of whether it's positive or sarcastic.
And there are lots of other ideas.
Uh, this is just a sample of them.
And that lexicon idea I showed you some lexicons before,
that's a pretty rich set of resources to mine for doing sentiment analysis.
Oh, I had one other methodological note that I thought I would.
I had one other methodological note that I thought I would insert here.
Because, it might be that for the bake-off for example,
you end up writing a lot of feature functions.
A bet that you could make early on.
For example in this bake-off,
is that you should be using linear models not deep learning models because you
only have till Monday and it can take a long time for deep learning models to work.
And so that might set you on the path of writing a lot of
interesting feature functions of the sort I've just been describing to you.
And I thought I would insert here just a methodological note
about assessing those feature functions.
So suppose you've written a lot of them,
and you might want to be combining them into a single model,
which I'd also encourage.
Scikit-learn offers in its feature selection package,
a bunch of methods for doing,
for assessing feature functions individually.
And you can sort of what you're doing with those tools
is assessing essentially in isolation,
how much information they tell you about the class label.
And that can give you a picture of how much predictive value they have.
What I've done here with this little dataset which I constructed artificially,
is just try to send you a warning that assessing these feature functions in isolation,
might be a kind of good first pass heuristic,
but it can be very dangerous because what you end up doing in the presence of
correlated features is often overstating the value of those features.
Because what you're really doing when you run your model,
is assessing them all in the context of that unified model.
And that's doing something very different than you're doing with this individual testing.
And I'll let you think about this example here.
But the point is, if I use this chi-squared test in isolation,
it looks like at least these two features,
X_1 and X_2 are really good to include.
And maybe you decide to drop X_3.
But the truth of the matter is,
if you fit an integrated model,
then X_1 alone is the best model.
And including X_2 actually degrades the performance of the model.
And so that argues I think for something more like a holistic assessment,
where you are maybe dropping or adding individual features,
but always in the context of the full system that you're evaluating and not so much
in isolation the way I've done here. Uh, yeah?
For like, related features. Is there something like simple you can do like
maybe some PCA thing to understand how your features um,
maybe are like [inaudible] before you like actually like choose which ones to use there?
I think it's a reasonable response.
It's certainly a way, uh,
of removing those correlations.
Um, it can cause some problems at test time.
If you pick one of those matrix factorization methods and do that to your trained features,
if I give you new examples,
you have to be able to do that reliably on those new examples as well.
Um, and that can be problematic because
the characteristics of those new test examples might
be very different from the training examples.
So it's just something to think about in terms of a real deployment.
You mean the data distributions are different, or?
Yeah, because if you do that reduction on your training set let's say, you've done that.
And I give you a single test example,
it might be not so clear what you're supposed to do with
that test example in terms of reweighting it,
reweighting its feature values.
There are some like Scikit actually tries to manage this.
So for example, I'm not sure what it does for PCA.
But if you run TFIDF,
then if you call fit.transform,
it will do this to your feature matrix or reweight it.
And then if you run that with just transform,
I think it uses its IDF values
that it has stored from training and it applies those to the new test cases.
Oh wait, I meant, like sorry, um,
I don't know if I'm communicating what I meant, but like,
for example construct like a correlation matrix of agreements between
like your features and how you label the class,
to like, use that as a way to select which ones stay?
Oh, so not as a way to change the representations [OVERLAPPING]
Yeah.
But rather just to do the selection.
Um, I think it could be a good heuristic.
Yeah. It's certainly telling you something about
the entire matrix which was what I'm pushing here.
Yeah.
Yeah.
Pretty much the same question.
I was going to ask about PCU. That's pretty much answered.
I think you'll get finer grain inflow- information
if you can afford it by just using, actually,
Scikit has these functions like ablation things that will
repeatedly run your model having added or removed some of the features.
And then you can kind of see
their practical significance in the context of the full model,
with exactly the scaling that you're going to be using for your task.
Final thing about feature representation,
that's a kind of transition into the world of deep learning.
What I'm pitching here is,
using distributed representations as features.
So imagine my simple example from the SST is The Rock rules.
Uh, you know, The Rock is an actor.
What I can do in this case,
is just look up all of these words in an embedding space.
Maybe GloVe, maybe it's one you built yourself,
right from the first unit.
So look them all up. What I need to do in this space is,
find a way to combine them into a single fixed dimensional representation,
because all these methods presuppose that
all the examples are coming from the same embedding space essentially.
So in this simple mode,
what you would do is combine them.
You could use for example,
the sum of the values component-wise, or the mean,
or the product, or whatever some function that will
take them together into a single representation.
And then that function X here,
is just the input to your classifier.
Whereas, before if we built a bunch of feature functions like the bag of words one,
your feature representations will have you know,
like 20,000 dimensions and will be very sparse.
These distributed representations of your data will have
50 to 300 dimensions depending on what vectors you downloaded or built.
And they will be very dense, of course, right?
All the vectors will be active.
So it's a very different perspective on your data.
But it's amazing how much- how well these classifiers can perform,
given how little information you seem to be feeding them.
And also how hard this combination step typically is,
because of course, this is a hyperparameter tier model.
It's not something that your model is learning. Yeah?
If you had um, multiple sentences,
so let's say paragraphs, uh,
what would be your limit for how many uh,
pre-trained vectors you would add together before uh,
you couldn't really get a signal out of that data?
I'd say not pre-rated limit.
I think it's actually striking how you could do this with fairly long documents.
Just sum them up and get a pretty good representation of what's in there.
In that, in that situation,
my intuition is that you would not want the mean.
Um, but rather, something the sum that was
encoding a lot of information about the length of
the text which you'd essentially get from
the magnitude of the dimensions as you added them together.
But it's kind of amazing how well that can work,
as a kind of all-purpose rough look at the full text.
In terms of running experiments for this again,
I claim it's really easy.
Oh, a question in the back here. Go ahead.
[inaudible].
Um, I think that this is a nice baseline for a sequential model.
If I wanted to argue for an RNN that used GloVe inputs,
I might use this as my baseline because what I would be doing there is saying,
keeping constant the amount of lexical information that I'm introducing,
what precisely is the value of modeling the full sequence?
And then you'd be assessing your RNN about how well it did
over and above this simple combination function,
because it's kind of like the simplest version is sum, uh,
to combine these vectors and the complicated version is your fancy LSTM.
Um, the framework can accommodate these representations.
I've given you the full recipe here,
and most of this is just building up the embedding space,
GloVe lookup, have a general purpose feature function
that has too many arguments for the framework.
So then I define the special purpose one that builds my GloVe representations.
Softmax is as before, no change there.
And then you call the experiment,
and the only thing you have to remember,
if you have this kind of data coming in,
that is vectors and not dictionaries,
is to say vectorize equals false.
Otherwise, it's going to assume that those vectors
coming in are actually dictionaries and it will do,
if it does anything at all,
it will be something quite crazy.
What you're essentially doing here is saying,
"Hey sst.experiment, don't featurize my data form using a dict vectorizer,
just take it as it comes."
And then it all works out.
Other questions about that approach?
Great, let's do two more things.
And this is a foray into deep learning.
So the first one will be, uh, RNN classifiers.
This is implemented in a few ways in the package.
If you want all the details,
go to np_rnn_classifier, it gives you
the forward algorithm and also the back prop for learning.
But I'd actually recommend that you use what's called Torch, a classifier RNN,
or the TensorFlow version,
because they're faster and more robust.
But all of them are drawing on the same basic structure which I've depicted here.
So The Rock rules is my example.
I look them up in some embedding space which could
be pre-trained when you built yourself for GloVe,
or it could be a random embedding.
And then the crucial thing is the action of this RNN.
So it has this hidden layer here,
and you do combinations to get hidden states at each time step.
And then in the simplest version,
the final time step is the basis for your classification decision.
So imagine that by the time I've gotten to h_3,
I'm looking at some fixed dimensional dense representation,
and that's the input to a classifier.
This little part here is essentially this,
where x is the final hidden representation from the RNN,
and that's why I think these are kind of a natural pair if you think about assessment.
And so this is just the label for the SST sequence, and these are the leaves.
People might have seen these models before.
It's worth noting, uh, one thing about how you do preprocessing,
that makes these a little bit fiddly.
So your examples here are presumably lists of strings.
Here I've got this tiny vocabulary,
so imagine these are all just different words, two examples.
In the standard preprocessing flow,
you map those into indices,
and those indices are into an embedding space that you've built.
By default, the code will randomly initialize one of these embedding spaces.
But if you feed in your own space,
it will use that as the embedding.
And so from these indices,
you do all the look-ups,
and what the model actually processes of course,
is essentially a list of vectors,
and so the final version of these examples is this list of vectors down here.
That's the way, this is a kind of standard setup for these problems,
it's not obligatory of course.
And when we look later in the course at contextual word representations,
we will skip all these things and just directly look up entire sequences,
because the point there is that words could have different initial representations,
depending on the context they're in,
which is something that this embedding look-up thing of course can capture.
But the RNN itself is an exciting development to my mind,
because even if I have fixed representations down here,
because I'm modeling the full sequence like this,
I'm modeling all of these words in the context that they occur in.
So for example, one thing I really like about this as a linguist is,
I have an intuition that negation is important for sentiment.
I showed you a messy heuristic method based on the tokens,
and then I showed you a very precise version that depended on trees.
But, you know, the trees might be wrong or my,
I might just be confused about how scope works in my dataset.
So both of those have drawbacks because neither of them is really
responding to information from my labeled dataset.
This RNN here of course,
this sequence of hidden representations, you know,
one in the same token at point h_2 could be different
depending on whether or not h_1 is the origin for a negation,
and that's the sense in which these models just out of the box are allowing you to
have the power to see those semantic influences play out at the level of the sequence.
And the same thing for might, and for say,
and for claim, and for no one, and for everyone,
all these things are affecting the semantic context,
and therefore affecting the hidden representations
associated with the words in the sequence.
So it's a lovely holistic analysis of your example,
with clearly the potential to capture lots of interesting semantic influences.
And I think that's born out, these are,
are very successful and powerful models for lots of different tasks.
I have a note here on LSTMs,
I'm not going to dive into this too much.
Suffice it to say that,
regular RNNs like that NP one down here,
they struggle with long sequences,
and it's not a surprise that they do because
the signal they get from the training label up
here becomes very diffuse and problematic as these sequences get longer.
So LSTMs and other mechanisms like them,
are ways of managing that flow of information in a way that leads to better results.
And so the Torch and TensorFlow RNNs both have LSTM cells by default.
And I'm not going to give you an explanation of them here,
we don't have that much time,
and also frankly, I feel like I just can't improve on these two blog posts,
especially this first one.
If you want an intro to LSTMs,
I highly recommend it.
It uses a lovely kind of visual language for
helping you think about what's happening inside these cells,
and why they work so well,
um, yeah, very helpful.
And then finally, here's a code snippet to round this out again.
I'm, here I'm building a GloVe space for my embedding.
For the feature functions of course,
they get kind of trivialized,
because what I wanna do is just return
the sequence of words that are the leaves, so I just do that.
No featurization at that point because the real action is inside fit_rnn.
I decide on the train vocabulary,
and I've decided that I'm gonna keep just the top 10,000 words by frequency,
and all other words will be mapped to UNK,
and you can set that parameter however you want up here.
And then I create a little embedding space and feed that in here,
and there's lots of other parameters that you can fiddle around with.
Fit that model to the x, y pair that come in.
And from there, it's just SST experiment as usual, and again,
remember to say vectorize equals false for these models,
so that it doesn't assume its a bunch of count dictionaries.
I'm not sure that would even work in this context.
And in terms of hyperparameter exploration,
you're seeing a glimpse of it here,
there's a lot of values that you could explore,
and maybe the dark side of deep learning is
that these things will matter a lot to the final performance of your model.
Let me do one more thing,
and then why don't we just do some coding here in the room?
And that's these tree structured networks.
So it's, it's a very similar idea to the RNN,
except instead of having simple sequences,
we're going to process the SST trees as they come.
So I have that same example, The Rock rules,
and you have some hidden parameters in here that are
forming hidden representations here in orange.
It's just that instead of processing them input to hidden and then hidden to hidden,
you take the child nodes and process them together to form the hidden representation,
and you do that part recursively until you reach the root of the tree,
which has a hidden representation,
and that's the basis for your classifier.
And from there, it's kind of,
in my code Softmax classifier as usual.
And then the back-prop is a process of taking the errors and feeding them down,
and splitting the gradients apart,
and then feeding them down like that,
and you can see tree- np_tree_nn exactly how that process works.
Combinations is like how to mark products summations,
how do you get two and then put them together?
There's lots of options here.
[LAUGHTER] So my code,
is a great question, my code is just concatenating the two, uh,
child nodes and feeding them through this kind of simple classifier here,
where you have learned weights and a bias.
I think that's a good baseline for the space.
It's the first one that was explored,
and the first one that everyone kind of sets up,
and I've mapped out three alternatives.
All of these were explored by Richard Socher, Matrix-vector,
where you represent each word as both a vector and a matrix.
And when you do that combination to form the parent,
you kind of cross the matrix and the vector so that
you get lots of multiplicative interactions, so very powerful.
I think it was too powerful in the sense that it
had too many parameters relative to the data,
and that's how Socher et al.
motivated this tensor combination here,
which takes this combination function and extends it
with essentially a method for combining in every conceivable way,
the concatenation of the two child nodes with a higher order
tensor kind of sandwiched in the middle allowing for all those interactions,
and then you essentially add that to a function that looks like this.
So that's very powerful and successful,
and like when I was showing you examples from the SST project before,
that was a model that was using this combination function,
and then just to round it out as a more recent development in Tai et al,
they have the feature that so the nodes that are
doing the combination functions at the nodes are LSTMs,
and the innovation there is that they're kind of gathering state and other kind
of gaining information from the child-
the two child nodes separately so that they can be gated separately,
and then combining them into a single representation.
And I highly recommend all these papers if you're interested in this space.
This is a nice progression of ideas.
And then the final thing that you could say that unfortunately we don't get to
explore too much in here is that recall of course that in the SST,
you actually get supervision not just at the tree node the way this model is implying,
but also at all of the sub nodes,
and I have included model implementation in our class repo that will do that,
and I think the guiding intuition here is that I have all these points of
supervision on shared parameters for this classifier.
So I can gather them all together and get lots information for all of them,
and then pass that down at each one of the sub trees.
So in terms of implementation,
once you get your head around how all this information needs to be
passed around in the tree, it's pretty straightforward,
and then of course you can get
really powerful models coming from the fact that you're getting
so much more information about what all the subconstituents mean in terms of sentiment.
And then just to round this out,
this is a full picture of how you use SST experiment with these tree models.
Uh, and I think the only special thing about this is that the tree models are
unusual in assuming that their label is part of the training instance itself.
So in the default, the regular tree_nn,
it looks to the root node to find its label as opposed to having it in a separate vector.
And so that's a little bit funny down here that mod.fit takes
only X and not Y. Y is just there for assessment.
It's operating on X, and the reason for that is that
then in the context of sub-tree supervision,
I can just gather all of those points of supervision from my training instance,
and not worry about how they align with some other vector.
But I think that's the only gotcha.
Other than that, you can use this framework for
the full set of models that I've introduced here.
Are there questions? This is good,
it gives us some time.
What we're imagining for the homework of course,
let me just run through it quickly to get you oriented,
and then you can start coding on your own.
So sentiment words alone is just asking you to use
a lexicon to create a filtered bag-of-words feature function,
just to see how much information about
the class label is encoded in this off-the-shelf lexicon.
And so what I'm asking you to do is design that feature function,
and then use the McNemar test to compare them,
just to expose you to that interface and get you thinking about classifier comparison.
So I think that's pretty straightforward.
A more powerful vector summing baseline is asking you
to explore in a little bit more detail that
distributed representation approach that I showed you as
the final phase of thinking about like linear classifiers,
and you're just sort of seeing whether with
a more powerful model than just a linear classifier one
that might uncover lots of interactions among
the feature dimensions seem whether you could do better at the problem.
And then the final one is your custom system,
and custom here just means basically anything goes.
It's okay to download other people's code.
Of course, I have a huge bias for you taking code that we've
provided and doing cool modifications to it to see whether they pay off.
Uh, but we don't have to place too many rules here for designing your original system,
and that's what you'll enter into the bake-off.
We're evaluating on the ternary class problem for all of this stuff.
So you should kind of hill climb on that.
What you should do is probably do a lot of testing just within your training data and
periodically test against your dev set to get
a more honest look at how you're actually generalizing to new data,
uh, and never look at the test set of course.
The test set although it's distributed is
completely out of bounds until the bake-off starts,
and then we'll actually run the evaluation and I'll give you
some code to guide you on exactly how to do that,
but have in mind that you want a system that does
well on the ternary class problem on the test set.
And the only other thing that I wanted to note is that I added,
um, maybe it hasn't reloaded here.
I just added a note at the bottom here
asking you to give an informal description of your system,
just to help the teaching team kind of get oriented
on what your code actually does and what your solution is like, uh,
so that we can classify the solutions and come to some kind of understanding of
what worked and what didn't. Yeah?
For these bake-offs will we go over in
class like the best one, and why we think that's the best one, or are we going to dissect them?
That's the plan. I mean,
we hope to give you really rich reports on this.
Um, we're aiming to have the first one on Monday,
and I can tell from the submissions that have come in that there will be
interesting lessons that we can reflect back to you about what worked and what didn't,
and I'm hoping that continues.
Yeah. Okay. I'm gonna stop talking and let
you all just get coding, and we're here to help.
 Hi everyone. Hope you had a good weekend.
Uh, we're gonna start off today's class talking about the
bake-off that happened last week.
So if you don't remember what you did,
you evaluate distributed- distributed representations using word similarity datasets,
and so all the similarity datasets had these word pairs with associated, um,
human annotated similarity scores,
and your evaluation measured the distance between the word pairs and your chosen VSM.
So here's the histogram of the scores.
Um, as you can see, there's kind of two bumps: one on the lower end,
one on the higher end, most people are on the higher end.
Um, so what Chris did was, um,
calculate the observed over expected scores after
separating all the bake-off entries into
the top scoring teams and to the bottom scoring teams.
So on the left here, we have,
um, that- the words that are more likely to appear for top-scoring teams.
And then over here, we have the ones that are more likely to
appear for the ones in bottom scoring teams.
That helps you get an idea of like what distinguishes
the really good models from the meh models.
So, uh, so it seems like retrofitting on WordNet and using
LSA are common ways to build better models from this rough analysis.
Okay. So our first place team is Group 80.
Is Group 80 here? No, okay.
But yeah, Group 80 basically, uh,
stacked p- pp- PMI,
t-tests, retrofitting on WordNet,
and in autoEncoder, and they got the highest score.
So good job for Group 80.
[LAUGHTER] [APPLAUSE] Yay.
Um, now, second place is- is this person here?
No? Okay. So what this person did was it looks like a lot,
but it's basically PMI plus retrofitting with a sub-word model, and they did pretty well.
Third place, are you here?
No. Um, so they also piled on a bunch of things, PMI,
t-test, LSA, retrofitting on WordNet,
and they did pretty well as well.
Okay. Now, [LAUGHTER] a model that- that did not work as well.
We're not gonna say who this model belongs to.
But it is kind of interesting how they did like t-tests, LSA, Jaccard.
So like, so they did t-tests and LSA together,
but I think the thing that made them kind of
suffer a bit more is they used Jaccard as their distfunc,
while the winning teams all had the default distfunc,
which I think is a cosine, so that's like an interesting detail there.
So it could be cool just to see like if we swapped out Jaccard with cosine,
maybe they wouldn't have scored so low.
Um, but it's still an interesting result,
so like if you're not doing too well on your models for these bake-off,
it's still interesting to see like how something that could
be very well-motivated still fail.
Um, and yeah, I think that's all I have.
Any questions? Yeah.
There was like a- another category for like took the most effort for real-time, right?
Who is- who is the winner for that?
Oh, I wasn't able to find who was that because it's so hard to like,
do you like, do you quantify it by lines of code, you know?
Because I saw a lot of teams who they ran a lot of tests,
like they had like 10 cells and like each cell has like a different experiment,
and they did like a lot of hyper parameter tuning, and stuff like that.
So but it- it could be there.
Yeah. There are some low scoring teams that put in effort.
[inaudible] their teams.
Yeah.
For some special consideration,
if you could quantify it yourself in terms of compute time or [OVERLAPPING].
Great.
I'm gonna hand this off to the real professors.
Thank you miss.
[APPLAUSE]
Okay. Uh, I have a couple of announcements
before we get to our main event. So here they are.
Bake-off 2 has begun,
so there was a- there was a post on Piazza within
the last hour or so giving you details on how to do that.
So make sure you take a look at that post.
Uh, Bake-off 2 closes on Wednesday,
so you definitely wanna,
uh, look at that today or tomorrow.
Um, on Friday, we are gonna have
a special session to give you an intro to NumPy and PyTorch.
Uh, so the details are there and this session is gonna be recorded.
On Wednesday, we're gonna have some guests.
So Guillaume Genthial, don't know how to pronounce that,
and Marta, uh, uh,
who's from Roam Analytics and Marta Recasens who is from Google Research,
are gonna be visiting for a panel discussion on NLU in industry.
So we wanna encourage you to attend in person if you can to, um, show them a good,
good, uh, show of support from,
from Stanford and to have lots of good interaction.
I think the value of that session will be greatly enhanced if there's,
um, you know, lots of in-person interaction.
And if you can come to the session with
specific questions that you'd like to get some insight into,
what is it really like to use NLU techniques, um,
in the industry and, and apply the,
the stuff that we've been talking about in real-world applications.
Um, if you have specific questions or topics in mind,
there's a Piazza post, um,
en- encouraging you to think about this and you can add your,
uh, proposed question as a comment on that Piazza post.
And it's pinned so it's visible at the top of the list.
Um, that NLU panel discussion probably won't take the,
the full hour and 15 minutes next time,
so we'll use the extra time for coding in class, uh,
focusing on Homework 3 and Bake-off 3,
which is about relation extraction,
that's what we're gonna look at today.
But having some time in class for that on Wednesday will be particularly valuable,
because you'll have a little bit less time, uh, to- to,
uh, to prepare before that homework is due next Monday.
So any questions before I go on? Yes.
Bake-off 2, uh, do you have to be trained one more time?
Um, if you have a model that you think ha- has like good links or something,
could you just like, uh,
not do that, and get points.
Not sure I got that.
Yeah. I think you won't be able to use that as the experiment.
But I gave some guidelines about this in the close.
Just make sure that you're using
exactly the right training data that you did that before.
You test on the test data and then you report Macro at form.
And if you do those three things, it's totally fine.
[NOISE] Okay.
Today, I wanna turn to a new topic,
which is relation extraction.
Um, and there's kind of two big ideas that I wanna introduce you to.
One is the task of relation extraction itself,
and the other is this idea of distant supervision.
Um, relation extraction is an interesting topic
for a bunch of reasons that I'll say more about in a moment.
But one of the reasons that it's an interesting turn for us in kind
of the- the narrative that we're developing is that it's the first time
that we're looking at an NLU task where the output of
the task is a discreet object rather than a numeric value.
So we've just been looking at, uh, sentiment analysis.
And sentiment analysis, the output is a scalar value,
it might be like one through five or
in the ternary task that we're doing for the homework and the bake-off,
it's just, uh, minus one,
zero, or plus one.
Some versions of sentiment analysis,
it's just zero or one,
it's basically binary classification.
But in any case, it's a scalar value.
Uh, before that we looked at vector space, models,
and meaning where the output is a vector of real values.
With relation extraction, for the first time,
we're gonna be looking at a task where the output is an object.
In this case, it's a relational triple.
So it's like, uh, a row in a database essentially.
Um, so that's, uh, uh, something new.
And a couple of weeks from now,
we're gonna look at the task of semantic parsing
where we go even further in that direction,
and we're looking at outputs which are complete logical forms,
so that have real complex structure to them.
So relation extraction is one big idea.
The other big idea is distant supervision,
and this is interesting because it's a strategy
for breaking the data bottleneck that has been,
uh, one of the biggest, um,
sort of obstacles to progress in applying machine learning methods to- to NLP and NLU.
Um, the idea of distant supervision enabled people working on
relation extraction suddenly to- to be able to use
100 times as much data as had ever been used before for relation extraction.
Um, and that enabled, um,
vastly greater statistical power in the- in the models that were trained.
Um, and I'll explain how- how that works in a moment.
But that was a big innovation that's applicable not only to relation extraction,
but to other kinds of problems as well.
Um, what's interesting is that the innovation here is not so much a new kind of model.
It's not a new neural architecture or
a new loss function or a new optimization algorithm.
Instead, the innovation is in the kind of supervision that's used to provide,
uh, uh, uh, a signal on which to hill climb.
Um, it's also interesting because
the ideas of distant supervision were developed right here at Stanford.
So the idea first came about in this paper by Rion Snow and colleagues.
Uh, in 2005, Rion Snow was a PhD student who was advised by Dan Jurafsky,
whom some of you may know.
Um, so that idea, um,
incubated here in 2005.
It was then extended by,
um, Mintz et al.
I think I have a reference to Mintz et al right here.
In this paper, distant supervision for relation extraction without labeled data.
So that was also in Dan Jurafsky's research group and part of the,
part of the Stanford NLP group.
Um, and then this same strand of work led
eventually to a generalization of the idea of distant supervision,
which was the idea of data programming developed in Chris Re's group.
Some of you might know Chris Re who's a-
also a professor in the Computer Science Department here.
And, um, a package called Snorkel,
which implements some of the ideas of data programming, um,
which you can see as kind of a generalization of,
of this basic idea.
These are all strategies to break through
the limit imposed by needing supervised, manually annotated,
uh, sup- uh, fully supervised data for training.
So let me say a little bit about the task of
relation extraction is it- is this big enough for people to read? This okay?
Legible? Okay. So the task of
relation extraction is to extract relational triples from natural language text.
And by relational triple,
I mean things like these examples here, so Founders, SpaceX,
Elon_Musk, has_spouse, Elon_Musk, Talulah_Riley, worked_at, Elon_Musk, Tesla_Motors.
So each of these expresses a fact about the real world.
Each of them has three components.
There is a relation and then two entities.
Um, the entities have these underscores in here.
So that suggests that they're some kind of
unique identifier of these entities and I'll say more about that in a moment.
And the relation is one of
a small number of pre-defined relations that we're interested in.
So why is this interesting? Why do we care about this?
Well, if we can accumulate a large collection of these relational triples,
that amounts to a database.
Uh, a database of facts about the real world.
Um, building a knowledge base manually is slow and expensive and
time-consuming but the web or
other large do- collections of documents are filled with these kinds of facts already.
So if we can figure out a way to extract
these facts automatically from natural language text,
we can greatly accelerate the creation of knowledge bases.
So why would you care about that?
Why do we want to have these large knowledge- knowledge bases?
It turns out that,
um, they're incredibly useful.
First, because a huge proportion of human knowledge can be expressed in this form.
Um, and second, because there are
abundant commercial applications for knowledge bases like this.
Um, for example, my current boss, John Giannandrea, uh,
earlier in his career,
founded a company called Metaweb and was the co-creator of Freebase.
Freebase is a very large- it doesn't actually exist anymore which is a terrible shame.
But Freebase was a large community-authored knowledge
base which was essentially a vast collection of relational triples like this.
Um, it was acquired by Google in 2010.
And it became the foundation of
Google's knowledge graph which was used to
power a variety of different knowledge applications
including question answering and also the knowledge panels that you sometimes
see on the right- on the right-hand side of Google search results.
Um, it was a really powerful idea.
Uh, a couple of years later in I think 2014,
Microsoft introduced its own version of a knowledge graph,
theirs was called Satori but was fundamentally the same thing.
Um, it also matters for Apple.
Uh, in 2017, Apple acquired a startup called
Lattice which was co-founded by Chris Re
from the Stanford Computer Science Department and Lattice's,
uh, business was also to do relation extraction from,
uh, documents from the web, um,
in order to build and extend, uh,
knowledge bases that in this case are helped to- are used to help theory better answer,
uh, questions about the world.
Um, so there's, uh,
abundant commercial interest in building knowledge bases like these.
There are also other applications that might be a little bit less obvious.
Um, one is extending WordNet.
Um, WordNet is basically a knowledge base of lexical semantic relations.
So in WordNet, the role of entities is played by words or actually synsets.
And the relations between them are things like
the hypernym relation or the synonym relation or the antonym relation.
So, um, you can formulate the task
of adding new stuff to WordNet as a task that has this form.
There's, um, there's lots of stuff that should be in WordNet but isn't.
So, you know, WordNet doesn't know,
um, about, um, things like,
um, like there's, um,
categories and subcategories and sub-subcategories of video games, right?
And, uh, WordNet doesn't know about all those different categories of video games.
And the fact that this one is a subcategory of that one.
Um, but it would be great to be able to automatically
extend WordNet to have that kind of knowledge.
And so I mentioned that this technique of
distant supervision was introduced by Rion Snow in 2005.
That was actually the application that he applied it to,
was automatically inferring relationships between
word senses by extracting them from natural language text.
Um, there are other applications for example in biology.
Every year there are thousands and thousands of biology research papers published,
some of them have to do with, uh,
gene expression and gene regulation networks.
If we can make these,
if we can apply these techniques to biology research papers maybe we can run, uh,
relation extraction, uh, focused on
genes and gene activations over these thousands of research papers.
And extract from it lots of relationships between different genes
and whether they have activating or suppressing effects on other genes.
And by doing that, build a big graph of relationships between
all these different genes and then somehow that's- I'm not a biologist,
so I don't know what you do with that from that point.
But- but that can be really useful, uh,
to biologists who are trying to understand
these complex networks of relationships between different genes.
So that gives you a sense of some of
the applications and why you might be interested in this task.
Okay. So how do we actually do- let's return,
let's come back to these examples here.
We want to learn some facts about
Elon Musk and we want to extract it from natural language.
How can we actually do that?
One of the techniques that might- might come to mind first is
to just write down some patterns that express the relations that we're interested in.
So for example, um,
a pattern like X is the founder of Y that seems to express the founder relation.
So maybe we can just search a large corpus for occurrences of that pattern
and extract the XY pairs and add those to our knowledge base.
So we might find, um,
in a large corpus, a sentence that says Elon Musk is
the founder of SpaceX and we're like, "Okay, great."
I can add a new triple to my knowledge base,
that seems very straightforward.
Um, and in fact,
this approach was the dominant approach that's sort of the dominant paradigm for
relation extraction for many years like during the 60s and 70- 60s and 70s.
So in the- in the very early days of natural language understanding.
People tried to be slightly more sophisticated by instead of defining, uh,
an exact string and looking for exact matches on that string,
defining patterns that could be like regular expressions to
accommodate some minor variation of things like verb tense and so on.
Um, but nevertheless it was the basic approach.
But this doesn't get us very far.
And the fundamental reason it doesn't get us very far is
that language is incredibly varied.
There's just a million different ways to say the same thing.
So look at examples like these, um,
you may also be thinking of Elon Musk,
parentheses, founder of SpaceX who started PayPal.
Interesting fact, Elon Musk,
co-founder of PayPal went on to establish SpaceX.
Or if SpaceX exploration parentheses SpaceX
founded by PayPal pioneer Elon Musk succeeds blah blah blah.
So each of these sentences expresses the fact that Elon Musk is the founder of SpaceX.
But none of these match the pattern that we proposed X is the founder of
Y nor any other pattern that would be obvious at the outset, right?
You wouldn't have anticipated any of these exact patterns.
And even if you ima- imagine that you like, um,
create a little bit of flexibility by making
it a regular expression instead of an exact match,
there's no way you would've anticipated any of these patterns.
Now, once you see these examples,
you could say, okay,
well I'm going to create new patterns.
I'm going to add to my pattern list by having a pattern that captures, you know,
X parentheses founder of
Y and maybe that will match some more examples somewhere else in my corpus.
And then I can add another pattern which is X,
co-founder of PayPal went on to establish Y or maybe I
should actually replace the PayPal with a star so that I get some more generalization.
But like these things aren't really going to help at all,
these patterns are still really specific.
They're not going to match that many if any,
other examples in your corpus.
They're not going to generalize well to new examples.
And so the result of adopting this approach is even if you manually engineer,
even if you write down lots of patterns and you try
to make them general by using regular expressions,
you're going to wind up with a system which might have
high precision but it's inescapably gonna have very low recall.
There's gonna be lots and lots of sentences which
express a relation which
don't match any of your patterns and you're just gonna miss those.
You're not gonna have the opportunity to do those extractions.
So that strongly suggests embracing machine learning,
and so this is what happened in the- in the 90s and the 2000s.
There was the so-called statistical revolution that came to NLP,
and people said, "Hey,
let's use data in machine learning,
ah, to learn the things that we wanna learn. "
Um, and in addition, um,
let's use a feature representation that will allow
much better generalization from the examples that we have.
Um, so to do this,
we're going to have, um,
we're going to need lots and lots of supervised data to train our model.
So what we would do is,
go through a corpus and every time there's a sentence that has two entities in it,
we would manually annotate it with the relation, if any,
that holds between the two entities
or if there is no particular relation then we just say, "No relation".
So, we go through and we'd find this sentence and we'd say,
"Elon Musk" and "SpaceX" and we'd annotate that with something that says,
"Founder," and we'd annotate
the second sentence and the third sentence with something that says.
"Founder". Um, there might also be sentences in our corpus like this one,
billionaire entrepreneur- entrepreneur Elon Musk announced to blah blah blah.
I'll let you read it. It's got both Elon Musk and SpaceX,
but it doesn't actually express the founder relation.
It's not saying that Elon is the founder,
and so we'd manually annotate this example with a label that says "No relation".
So now, we've got a big corpus where all pairs of
entity mentions are annotated with a relation that holds between them, or no relation.
And we can train a class,
if we can define a feature representation,
ah, for each of those pairs of entity mentions.
Maybe it's just the bag of words in the sentence,
and then we can train a classifier using tec- techniques very
much like what we've been exploring for sentiment analysis, and this works.
It totally works and it was a huge step forward for relation extraction.
It allowed us to build relation extraction systems that got
much better performance than the handbill patterns that came before,
and particularly, much better recall than what came before.
But it has a big shortcoming as well which is that it re- relies on
having this manually annotated data from which to learn.
You have to go through a corpus and- and manually label everything,
and that manual annotation, again,
is slow and expensive and it doesn't scale.
And in fact, when this was the dominant paradigm
in relation extraction during the 90s and 2000s,
the largest data sets that were constructed for
relation extraction had on the order of thousands of examples.
And today, a data set with only thousands of examples seems kinda small.
And it's certainly not enough to do-,
um, to apply sophisticated,
ah, modern methods to.
So the big innovation that I- I wanna, um,
focus your attention on and gets you excited about is this idea of distant supervision.
So here's the big idea.
This is the big conceptual breakthrough.
We're gonna sidestep the need to manually annotate entity mentions in
a sentence by using an external resource as a source of truth.
So in particular, we're gonna use relational triples from an existing knowledge base,
ah, to automatically identify extraction examples in a big corpus.
Let's say for example, we already have a knowledge base that contains
this relational triple, founders, SpaceX, Elon Musk.
What we do, is we take that pair of entities "SpaceX" and "Elon Musk",
we search through a large corpus for any sentence that contains those two entities,
and then we make the unreliable assumption
that that sentence expresses the founder relation,
and we effectively label each, ah,
entity pair, each example in a sentence with that relation label.
So it's a way of using this external data source, the knowledge base,
to generate labels in the corpus rather
than having to manually label all of the examples in the corpus. Yeah.
Is it possible that we have situations where we have both Founder,
[NOISE] SpaceX, Elon Musk,
and like- likes SpaceX, Elon Musk.
So when we're looking through a corpus we are going
to assign more than one label to a sentence?
Absolutely, and we'll come back to that question in a moment because it- it goes to,
how do you formulate the classification problem?
Is it a multi-class classification problem
where you have to choose just one relation out of a bunch of relations,
or it is- is a multi-label classification problem where you
can assign as many labels as you want. Yes.
So can you only do this on something that you've
already built up a knowledge base [inaudible].
Yeah. One of the limitations of this approach is that because it
starts with examples of the relation that you care about,
um, you can't use it to build a knowledge base for a completely new relation,
or add a new relation to a knowledge base.
If I wanna learn to do extractions for the founders relation,
I need to start with some examples of the founders relation.
Um, if I don't have any examples yet,
I'll need to use a different strategy. Yep.
Bootstrapping?
It is, ah, um,
say a little bit more, bootstrapping is an overloaded term.
What are the concerns when you do that
you have like seed pairs, and then you check over dataset seed to see different relationships that uses those pairs, and then use that to try to find the relation.
It is basically the same idea.
If you read, you might have- you might be referring to the reading from
Jurafsky and Martin where they talk about bootstrapping?
Yes.
Yeah so it's basically a way of- of
formalizing that idea and making it a little bit more rigorous.
The way the bootstrapping idea is presented in Jurafsky and Martin,
the seed examples, the seed tuples are just chosen by hand.
You just kind of think about it.
What are some examples of the founder relation?
And you come up with a very small number like maybe a dozen,
and start the process from there.
The other difference is that, um,
the way bootstrapping is presented by Jurafsky and Martin,
it can be, um,
a repeated cycle where you use the seed tuples to get some extractions from your,
ah, or to identify some examples in your corpus which may help you identify new patterns.
Those patterns in turn can help you identify new tuples,
and the process can repeat.
Um, with distant supervision we're not repeating that process.
We start with a much let- much larger set of seed tuples.
Hopefully, if our, you know,
if our knowledge base is already kind of
large and our goal is to extend the knowledge base,
and we're starting with lots of seed tuples and where,
ah, we're not sort of going around that cycle repeatedly.
We're just doing it once.
Other questions before I go on?
Somehow related to a earlier question,
but say for example like that sentence with PayPal, could it
extract that Elon Musk is both the founder of PayPal and the founder,
or co-founder of, sorry,
founder of SpaceX, co-founder of PayPal.
Absolutely. So the way the problem is usually set up,
every pair of entities is a candidate for relation extraction,
and so if you have three entities in a sentence,
there are three pairs and you can consider each one of those three pairs.
Um, and, you know, if there are more entities there you have
even more potential pairs and each one can be considered. Yeah.
So using this external knowledge base as a source of supervision,
that is the central idea of distant supervision and it's a really powerful idea.
Um, and one of the reasons it was
such a game changer for relation extraction about 10 years ago,
is that it allowed us to suddenly use
100 times as much data as it had ever been used before.
I mentioned that the largest datasets that were
fully supervised had thousands of examples.
But at one stroke,
this idea allowed us to, uh,
allowed Mintz et al.,
the authors of the 2009 paper to use 100 times as much data.
By using 100 times as much data,
they were able to define a feature representation that had
much more precise sort of
specific pinpoint features and they actually had enough data to,
um, overcome the problem of sparsity for those very specific precise features,
and that enabled them to get much better power from the,
from the model that they were building.
And in fact, like, at that time,
they were able to achieve a 100X gain.
I mean, there's no, there's nothing in principle that limits even larger scaling up.
Um, they were constrained by the computational resources available at that time.
Today, we could probably easily go to 1,000 times or 10,000
times as much data as people were using for relation extraction,
uh, in the 2000s.
So it's been a very successful idea.
It has two limitations or two,
um, [NOISE] yeah two limitations.
One is that it relies on making this assumption that every sentence where so,
um, I'm starting from this triple in my knowledge base, founders, SpaceX, Elon Musk.
I go to the corpus and I find every sentence where those two entities co-occur.
I'm making the assumption that every sentence where those two entities co-occur,
actually expresses the founder relation.
But we know that's not true.
We know that's an unreliable assumption, assumption and in fact,
I just showed an example of a sentence that doesn't express the founder relation.
The distant supervision approach is gonna label that as an example that
does express the founder relation, that's wrong.
So by doing this,
I'm essentially injecting noise into my dataset and that makes,
um, that creates a,
a challenge for the learning algorithm.
It's harder to learn when you have noisy labels.
That disadvantage is more than outweighed by the benefit of having vastly more data.
And so even though your learner has this additional challenge,
it's still able to learn much better models.
Um, the other limitation of this idea is the one that was already mentioned,
it can only be used to extend an existing knowledge base or an existing relation.
Uh, but because it relies on existing triples from the database,
from the knowledge base, it can't be used to create a new one from whole cloth.
Okay. So that's the biggest idea.
Um, if you only got that from today's session,
um, I think you would have already gotten something valuable.
What I wanna do with, uh,
the rest of the time is to walk you through the rest of this code book, um,
to help prepare you for looking at Homework 3 and Bake-Off 3,
which are gonna be due next week.
So we have a little bit of setup stuff.
We're gonna import some useful classes and libraries,
um, and some data.
So the data directory for the class includes a sub-directory called rel_ext_data.
And there's two things in there;
there's a corpus and there's a KB and I'm gonna say a little bit more,
uh, about each of those.
Um, if you have sort of the default setup,
um, then this code should work for you out of the box.
If you put the data directory somewhere else,
you may need to fiddle with the line that tells it where to look for the data.
Okay. So two data assets to talk about;
the corpus and the knowledge base.
Um, as usual when- whenever we're doing NLU we start with a corpus,
a large collection of examples of language and in this case,
we, there are some specific things that we need from our corpus.
We need lots of examples that contain two entities.
That's the whole idea.
Um, and it's gonna be really useful if those entities
have or already have what are called entity resolutions.
So an entity resolution basically takes an entity mention like
a string in a sentence that refers to an entity and it maps it into,
um, a unique unambiguous identifier.
Um, and that's gonna be really useful because we're gonna wanna
connect the corpus to the knowledge base,
which is gonna use the same unique and unambiguous identifiers.
Um, so, um, making the identifiers unique and unambiguous kinda solves two problems.
One, is the problem of polysemy,
which means polysemy is when one word or one phrase can mean two different things.
So the example here is New York.
New York can mean New York State or New York City.
If I've done entity resolution correctly,
those will map into two different unique identifiers.
New York City will have one identifier,
New York State will have a different identifier,
and hopefully something in the context around,
uh, will tell me which one of those it should map to.
So that's polysemy.
The other problem that helps to solve is synonymy,
when you can have two different expressions that refer to the same entity.
So New York City and the Big Apple both refer to New York City.
Hopefully, those two different expressions will both get mapped to
the same unique identifier when we do entity resolution.
Um, entity resolution is a whole topic unto itself.
There's people, you know,
work on developing models for doing entity resolution properly.
That's a topic that we could unpack in this class but we're not gonna do that.
We're gonna kinda treat it as
a black box and assume that it's already been done correctly.
And for this project,
what we're gonna, what,
use for a corpus is, uh,
a bunch of examples that are derived from this Wikilinks corpus.
Wikilinks was, uh, the result of a collaboration in 2013
between Google and some rese- researchers at UMass and um,
it's drawn from a
really large snapshot of the Internet,
um, 10 million web pages altogether.
Uh, what Google did is they,
Google has a terrific entity resolution module in-house and they just ran
this entity resolution system over
millions of web pages and basically recorded the results.
So the entity resolutions that appear in the Wikilinks, um,
corpus are not the result of human annotation.
They're the result of, uh,
a model that's just running over webpages.
And if you wanted to,
you could build an entity resolution component from
scratch and then you don't need any annotations on your data at all.
You just start from, uh,
arbitrary text from the web.
Uh, but to make our lives a little bit easier for ourselves,
we're gonna leverage the output of
this entity resolution system from Google through, through, uh, Wikilinks.
[NOISE] Um, there's something else we want,
which is that we wanna have the context around the two entity mentions.
So we'd like to have the text before the first mention,
the text between the first mention and
the second mention and the text after the second mention
because all of that context is potentially predictive of what the right relation is.
If you look back at, um,
some of the examples that I gave, um,
like, in the first example,
Elon Musk, founder of SpaceX.
Well, there's something in the middle here, the word founder,
that's strongly predictive of the right relation.
Um, [NOISE] the, in the second example,
you've got this word established that's in the middle,
which is strongly predictive.
Um, [NOISE] oh, I thought I had an example where at least intuitively
the most predictive thing is not in
the middle but it's outside maybe that example comes later.
Anyway, we wanna have that context around the,
around the two entity mentions and, um,
the UMass version of the Wikilinks dataset actually has that context.
So to help set things up to be convenient for the purposes of this class, we, um,
uh, massage the data,
we filter the data to make it small enough to easily load into memory and,
and to work with.
Uh, but what we wind up with is
a compact corpus that's gonna work really well for, for our purposes.
Um, there's, uh, a corpus class that provides some easy access methods to some of this,
to, to, to the contents of the corpus.
So I wanna give you a, a peek of some of that,
and at the same time do some, some data exploration.
So to load the corpus into memory,
um, we just use, um,
this call here and,
um, we can start to,
to poke around and see what we're working with.
Um, so the corpus has examples.
Every example is two entity mentions in the context of a sentence.
There are 332,000 examples.
So that's great. That's plenty of examples,
but still small enough to easily work with in memory.
Um, and let's take a look at what an example looks like.
So we'll just look at the first example here.
Um, this is horizontally scrolling so it's a little hard to read.
But, um, each example is basically a tuple and it has if I remember correctly 12 fields.
So the first two fields are called entity_1 and entity_2.
So in this example,
entity_1 is New_Mexico and entity_2 is Arizona.
Um, these, these are the unique identifiers for entities that I mentioned.
Um, the fact that New_Mexico has an underscore
in it kinda suggests that this is not just an English expression,
this is actually a unique identifier in some kind of entity space.
And in particular, these are so-called Wiki IDs.
So this is basically the last part of the,
the URL for the Wikipedia page corresponding to this thing.
Um, and serves as a unique identifier for New Mexico and for Arizona in this case.
Um, then the next five fields are fragments of the text from this example.
We have a field called left,
which is all the text to the left of the first entity mention.
Then we have mention_1,
which is the English text.
So here it's New Mexico without the underscore
because this is the actual fragment of text.
Then we have the middle,
which is everything between the two mentions.
In this case, it's just and.
Then we have mention_2, Arizona,
and then we have right,
which is everything to the right of the second mention.
And so if you paste those together,
you can kinda get the whole sentence.
Um, the procedure is still used in parts of New Mexico
and Arizona is to build a fire inside the Horno and so forth.
And then, the last five fields are similar to the previous five fields,
except that, um, they have part of speech annotations.
So every word has a slash after it,
and following the slash is something in all caps TO, DT,
JJ, NNS, DT, NN,
and so forth, that indicates the part of speech of that particular word.
And again, the parts of speech have been generated by an automated system, by a model.
Um, you don't need to worry too much about the meaning of the different parts of speech.
They come from the Penn Tree-bank annotation scheme.
There's about 40 or 50 different categories.
You can sort of figure out what they mean,
NN means noun, NNS means plural noun, things like that.
But you probably don't need to worry too much about what the specific things mean.
The reason they're here is that, uh,
they could be useful in building feature representations for your model.
And you may find- in fact,
I think one of the homework questions asks you to try to
leverage the- the parts of speech information to get,
uh, better- better generalization capability.
Oh, and here's a code fragment which actually pastes.
Um, the left mention one, middle, mention two,
and right all together to sort of reconstruct the original sentence.
Let's do a little bit more data exploration,
just to get a better sense of what's in this corpus.
And by the way, this is good metholo- methodological practice.
Whenever you start working with a new data resource,
do some poking around,
and figure out what's in there.
Try to- try to look at some summary statistics of what the data looks like.
Uh, we wrote a little bit of code to extract,
um, all of the entity mentions,
and then count them up.
And so, here's a list of all the entities by frequency.
Um, there's a lot of geography things.
In fact, looking at this list,
you might think that it was only geography.
That's not actually the case,
but it certainly looks that way right here.
Um, one of the benefits of having a corpus class,
is that it makes it- we can do some indexing and make it
easy to retrieve examples having specific characteristics.
So in particular, there's a method called show_examples_for_pair that makes it
really easy to look up all the examples that contain a particular pair of entities.
So here, we looked up, uh,
the examples that contain Elon Musk and Tesla Motors.
And the output tells us there's five examples like that,
and it prints out, uh,
the first of those five examples.
But actually, direction matters here.
So this isn't necessarily all of the examples that contain Elon Musk and Tesla,
it's just the examples that have Elon Musk as entity one and Tesla as entity two.
It could be the other way around.
So let's also look at,
show examples for pair Tesla Motors and Elon Musk.
And it turns out that there's two more examples that have it the other way around.
So we need to be conscious of direction.
If we're looking for examples that have Elon Musk and Tesla Motors,
we need to check in both directions,
um, when we're making those calls.
This corpus does have some flaws.
One flaw that you'll discover as you get
further into things and start poking around more,
is that there are lots of examples that are near duplicates of each other.
So not exactly the same,
but with minor variations,
like only a few words different, or something like that.
I think this is a consequence- so this com- this is- um,
this comes from the Wikilinks data-set.
And I think this is a consequence of the sampling strategy that was
used to choose the 10 million web pages from which the data-set was constructed.
I don't know all the de- I don't know exactly how they did that,
but there's lots of near duplicate documents on the web,
pages that differ from each other only by a little bit.
And I think they didn't try to weed those out.
And so, you will see lots of those near duplicates,
and they will cause some funny-looking results that we'll run into later.
Um, it's a flaw, but it's not a showstopper for this data-set.
Um, the other observation to make here,
is that this corpus doesn't contain any information about relations.
Unlike the supervised paradigm,
there's no explicit indication in
this corpus about what relations hold between the entities.
Uh, there's nothing in this corpus that tells us- nothing except
the English words in this corpus that tells us that Elon Musk is the founder of Tesla.
So to get that,
we're going to have to connect the corpus to the knowledge base.
So we'll turn our attention to the KB now.
Before I go on, any- any more questions about the corpus? Yeah.
[inaudible] like a record for every single [inaudible] factory and space and things like that.
Say if we did like space in two.
Would we get a list of words that have both space and two
or are there only certain entities that it's marked out?
Only entities. And in this case,
it was entities that were identified as entities in the Wikilinks corpus,
and therefore entities that were recognized by
the entity resolution system that Google used to generate this data.
Um, but that's a possible variation.
And in fact, er- earlier I mentioned the work by Snow that was used to extend WordNet,
and they had a different notion of like what were the units of extraction.
Um, and instead of entities, they were- they were looking much more
generally at nouns and trying- or noun phrases,
and trying to learn relationships between noun phrases.
Okay. Let's turn to the knowledge base.
Now, the knowledge base that we're gonna use is a fragment of Freebase.
I mentioned Freebase earlier.
It's this, uh, large, uh,
just sort of community constructed knowledge base of,
uh, relationships between entities.
It was very sadly shut down in 2016,
but you can see- if you poke around on the Internet,
you can still find data dumps from Freebase.
And the- the knowledge base that we're gonna include
here was derived from a Freebase data dump.
So in Freebase, you have these relational triples that we've already looked at.
Each one consists of a relation,
a subject, and an object.
Uh, subject and object that's just kind of, um,
the- there's no connection to- to subject and object in a syntactic sense,
in the sense of English syntax.
It's just kind of an arbitrary designation for the first argument to the relation,
and the second argument to the relation.
The relation, the thing in
the first position is one of a small number of pre-defined relations,
like place of birth or has spouse.
And the entities that appear in the second and third position,
again, are Wiki IDs.
So it's like the last segment of a Wikipedia URL,
and it constitutes, uh,
a unique identifier that we can connect to the identifiers used in the corpus.
Um, just like we did for the corpus, um,
we should do some data exploration and see what this data actually looks like.
So here's a line of code to load the KB into memory,
and then we can start poking around.
So there's 46,000 KB triples here.
That's great. That seems like a fair amount to work with.
Um, there's 16 relations in this knowledge base.
Wikipe- uh, Freebase the original free,
like the full Freebase has way more than this.
So Freebase has thousands of relations,
millions of entities and literally billions of relational triples.
What we're working with here is a- is a fairly small slice of that whole knowledge base.
Um, then you might want to know how big is each relation.
How many triples does each relation have?
So we've counted them up,
and you see the results here.
It looks like the contains relation is the biggest one.
It's got 19,000 triples.
Um, and some of them are quite small.
So the capital relation has only 500 triples.
So they vary quite a bit in size.
Um, here's some code that prints one example from each relation,
just so you get a sense of what's in here.
So adjoins, France, Spain, pretty intuitive.
Author, Uncle_Silas, Sheridan_Le_Fanu,
Capital, Panama, Panama_City.
I think these are pretty intuitive for the most part.
Um, one thing to point out is that,
some of these relations are intuitively symmetric relation.
So I presume the adjoins relation is symmetric.
And if adjoins, France, Spain is in the knowledge base, then probably,
hopefully, adjoins, Spain, France is also in the knowledge base.
However, there are no guarantees.
Um, in theory, it's symmetric,
but nothing actually guarantees that
the inverse relation is actually in the knowledge base.
Um, other relations are intuitively asymmetric.
So Author, Uncle_Silas,
Sheridan_Le_Fanu, that's fun to say.
Um, it's a bit arbitrary that the relation was defined in that direction.
There is no reason why it couldn't have been the other way around.
So it could have been instead,
author of Sheridan_Le_Fanu, Uncle_Silas.
That would have been fine.
The representational choice that was made here was to put the-
the author first and the work second.
Um, and by the way, that also points out that relations
frequently or typically have types.
So, uh, a relation will have
a type for its first argument and a type for its second argument.
Um, just like in the corpus,
there's a convenience method that makes it easy to look up the KB triples that,
uh, contain a particular pair of entities.
So we can look up all the,
um, triples that contain France and Germany.
Uh, it looks like there's just one.
There is a relation adjoins, France, Germany.
Um, and we can also look up,
uh, Germany and France.
So it turns out, yep,
the inverse relation is in there. That's great.
Um.
Uh, we can look up the relation- relations between Tesla Motors and Elon Musk who- uh,
we get the result that the founders relation holds between Tesla and Elon Musk.
That's great. I kind of said some of this stuff already.
Oh, and then we can look up things the other way around.
We can look at the relation between Elon Musk and Tesla Motors and
there is a triple, worked at, Elon_Musk, Tesla_Motors.
So that kinda makes sense.
Um, and then this point addresses a question that came up earlier.
There can be more than one relation that holds between a given pair of entities.
So, uh, Cleopatra and Ptolemy blah, blah, blah,
have both the- has sibling relation and has
spouse relation which seems you know unusual but,
um, I guess things were different back then.
Um, and then we can look at the distribution of entities in the KB.
So here we go through all the triples,
count up all the entities and list them in order of frequency.
Um, and again you see that it's dominated by entities that are related to geography.
Um, and in fact this list look like- list look like- looks like it's only geography,
but that's not actually the case.
One last observation, um,
there's no guarantee that the KB is complete.
So this knowledge base has founders, SpaceX, Elon_Musk.
It has founders, Tesla, Elon_Musk.
It has worked_at, Elon_Musk, Tesla_Motors.
But it does not have worked_at, Elon_Musk, SpaceX even though he did.
And given the other three you would think that it
should but it just so happens it doesn't.
And in fact, that's the whole reason that we're doing this.
The whole reason that we're doing this is that we want to extend
an existing knowledge base with facts that should be in there but aren't.
If the knowledge base were already complete we wouldn't have any work to do.
So the fact that it's incomplete is no surprise and in fact,
it's the whole motivation for doing this thing.
Okay, so the next question to look at is, um,
how are we going to formulate the prediction problem that we're going to undertake?
And there's really two questions here.
One is, what's the input to the prediction problem?
And the other is, what's the output?
What's the input means what is the- um,
what is the unit that we're going to try to make a prediction about?
The classic formulation of the relation extraction problem was to say,
the input is a pair of entity mentions in a specific context in
a specific sentence and for a pair of entity mentions in context,
we're going to try to predict the relation that holds between them.
There's an alternative which is to say,
the unit of prediction,
the input to the prediction problem is instead just a pair of entities.
Not a pair of entity mentions but just a pair of entities
abstracted away from any specific context.
So that's one choice we need to make.
The other choice is what's the output of
the prediction and this ties back to the question that came up earlier.
Are we trying to predict a single relation that holds between the two entities?
Or can we predict multiple relations to hold between the two entities?
Like the example of a half sibling and half
spouse both holding between one- one pair of people.
Um, it's worth noting that the classical approach to
relation extraction chose the first answer to both of these questions.
But today, the path that we're going to pursue chooses
the second answer to both of
these questions and I'll spell that out a little bit more as we go.
So we're first going to talk about the first one.
How do we- how do we formulate the input to this prediction problem?
And that really comes to the question of how we're going to
connect the corpus and the KB?
Um, there's kind of two possibilities.
The way I described it earlier it kind of sounded like we were going to use
the knowledge base as a way to generate labels on entity mentions in context,
and that's similar to the classical approach to relation extraction.
Um, and we could do that.
But actually, we're going to do something else.
Instead we're gonna, um,
de- define our problem as the problem of
classifying not a pair of entity mentions but a pair of entities.
And we're going to use all of the examples from the corpus where
those two entities co-occur to generate
the feature representation for that classification problem.
So instead of trying to classify a mention of Elon and Tesla in a specific sentence,
we're gonna take all of the sentences where Elon and
Tesla co-occur and use those to generate
a feature representation which will help us make a prediction
about whether Elon and Tesla in abstract,
apart from any specific context,
are related to each other.
That's a really important step.
So let me pause there and let that sink in and make sure that it makes sense.
We're gonna- the input to the prediction problem will be a pair of
entities considered apart from any specific context.
But when we generate a feature representation,
we're going to use all of the sentences in the corpus where
those two entities co-occur as the source for those features.
Okay. Um, so that means that we need to, um,
use the, um, the entity IDs,
the- the Wiki IDs as the way to join the corpus and the KB.
And continuing the idea of data exploration,
we can look at how many examples we have for each triple.
So we can go through the triples in our knowledge base.
For each one we look up all the examples in the corpus that contain those two entities.
And we can look at in aggregate or on average over
the- over the knowledge base how many examples we have to support each triple.
So this table summarizes that.
Like for adjoins there are 60,000 examples altogether,
um, but 1700 triples,
and it means there's a lot of examples for every triple.
We have about on average about 34 examples for every triple. That's terrific.
We have lots of evidence,
lots of examples to draw features from to
help us learn an extraction model for this relation.
There are other relations where the number of examples is much smaller.
So some of these have an average number of examples per triple of like one-and-a-half,
and that means we're going to have a lot less evidence in the corpus to draw on.
Um, there's one more thing we need, um,
to train these models and that is negative examples.
The, um, the- the connecting the- the KB triples to
examples in the corpus is gonna give us positive examples for training our models,
but to- but you can't train a model from positive examples alone.
You need negative examples as well.
So to get negative examples for each relation, we're basically gonna,
um, use pairs that appear together in the corpus.
So we have some,
um, evidence for them.
We have features for them.
But, uh, pairs of entities that are unrelated according to the knowledge base.
That is they don't appear together in any KB triple in the knowledge base.
So these are, uh,
pairs of entities that appear to be unrelated,
but where we have sentences that they co-occur.
Um, and we've written some code that,
um, walks through and finds all of those unrelated pairs.
And here's some examples: William_Randolph_Hearst, The_Cat's_Meow,
Les_McCann, Bobby_Timmons, Shatoy, Ibn_al-Khattab.
I don't even know what most of these things are.
But they appeared together in a sentence somewhere and yet
according to the knowledge base they're not related.
So we're going to use these as the negative examples for our classifier.
Okay so that was all related to the input side of the prediction problem.
On the output side, we have this choice to make about whether we're doing
multi-class classification or multi-label classification.
Um, and the direction we're gonna go is
multi-label classification and part of the motivation for that is
that it's actually quite common in the knowledge base
for a particular pair of entities to belong to more than one relation.
We already saw one example,
the one with Cleopatra and has,
has_spouse and has_sibling.
But there's a lot of them. Uh, and we can write some code to count them up.
It turns out that there are some overlaps that
are really common and they're kind of intuitive.
So, um, it's really common for a pair of
entities to belong both to is_a and profession, that makes sense.
So it's like Einstein is a physicist,
um, and his profession is Physicist.
Um, capital and contains,
that makes sense as well.
Place_of_birth and place_of_death,
there's no necessary relationship there.
But in practice, it's quite common for people to die in the same place they were born,
so that's not too surprising.
When we go down further, there are some that are surprising, uh,
has_spouse and parents,
um, co-workers at least once.
Oh, this is interesting, has sibling and has spouse,
it turns out Cleopatra's not alone.
There is at least seven other cases where those two overlap.
Uh, there's one case where we have three relations that overlap;
nationality, place_of_birth, and place_of_death.
I think that one makes sense.
Um, but like this one looks like noise,
parents and worked_at,
there's two examples of those overlapping,
and I think that's just some kind of data error.
I don't know what happened there, but I think that's noise.
So we're gonna formulate our problem as multi-label classification.
This means that we can assign multiple relations to a given entity pair.
There's many different ways to do multi-label classification,
but the simplest way to do it is,
uh, what's called the binary relevance method.
And the binary relevance method basically means factor
your prediction problem into unrelated binary classification problems.
So if you have, in this case,
we have 16 relations, right?
So what we're gonna do is just factor it into 16 separate binary classification problems.
For each relation, we're gonna try to predict,
does this entity pair belong to the relation or not?
It's just a binary classification for each of the 16 relations.
Um, there are smarter ways to do multi-label classification because this way of doing it
doesn't take account of the fact that the labels or in this case,
the relations can be correlated with each other.
Um, it just assumes everything is independent,
but it's a very simple way to set things up and it
will work well enough for our purposes.
So one way to think about our prediction problem is for a candidate KB triple,
for a specific relation and entity one and entity two,
does it, is it or is it not a valid KB triple?
Should we add it to our KB if it's not already there?
That's the binary classification problem that we're reducing this problem to.
Okay, then there is some code to create datasets.
I'm not gonna dwell on this, um,
but a dataset basically is gonna combine um, sorry, no.
Uh, a dataset is gonna be a bunch of KB triples along with labels that say,
uh, what the correct,
what the correct binary classification for that KB triple is.
Um, the thing to be aware of is that because we're
factoring our prediction problem by relation,
all of these data structures are gonna be organized into maps,
fro- or dictionaries from a relation to some other stuff.
So we're gonna have um,
these things where it says kbts_by_rel.
That basically means a collection of KB triples that have been grouped by relation.
So the data structure is a dictionary where the key is
a relation name and the value is a list of KBTs.
There will be a list of KBTs for this relation and a list
of KBTs for that relation and so on.
Um, that's this first thing here.
The second thing here is an exactly parallel data structure that has the labels.
For each KBT, it's gonna say whether that thing should have,
uh, a true or a false prediction made for it.
And um, this method here build dataset is,
um, gonna combine positive and negative data.
It's gonna ri- derive the positive data from the KB,
and it's gonna derive the negative data in the way I described
by looking for pairs of entities that co-occur in the corpus,
but don't co-occur in the knowledge base.
It will include both positive and negative data.
There's a parameter that lets you control the sampling rate.
The default sampling rate is 0.1 because we have
about 10 times as much negative data as we do positive data.
So using the sampling ra- sampling rate will give you a roughly balanced dataset.
Um, and it also lets you specify
a random seed so that you can have reproducible results if you want. Yeah?
Um, what do we do with
overloaded words like I'm playing the guitar, someone's playing soccer.
Or like anything of the sort that could be I guess confused.
[NOISE].
Um, I'm not sure if I understand the question.
Are, are you talking about words that you might
use in the feature representation as predictors?
Um, I guess so.
Like I'm just thinking like,
I guess you can tell me whether or not this is an issue.
When I think of sorting these triplets by relation, um,
I can see the word play as being used for either playing
an instrument or playing a sport which are very different things.
Yeah.
And so A, is a problem,
I guess and then B,
if that is a problem how do you,
I guess, counter that?
Um, so I think you're asking about,
um, the, the words that you would use to tr- the,
the way you'd construct a feature representation um,
to try to make a prediction and do you need to account for ambiguity?
Do you need to resolve ambiguities like that
one in constructing the feature representation?
Um, that's something interesting you could play with.
I think the typical answer is no.
We don't typically try to do word sense
disambiguation before constructing a feature representation.
Um, but it's possible that that could yield some,
that it could yield some benefits.
Um, I don't know, do have a-
[inaudible] in the knowledge base if a,
we would hope that the knowledge base distinguish those two senses of play.
It might or it might not.
But we hope that it'll work with
some different symbols and then it will go to two different relations.
Now, the data that you've learned on,
the thing that you perform the feature representations from,
it's like complete chaos.
But we would hope that the relationship [inaudible] that.
Are, are you imagining that play is a relation that appears in your, in your KB?
Yeah.
Okay, I missed that. I missed that.
Yeah, the, the knowledge base should if it's,
if it's a well-constructed knowledge base,
it should resolve those ambiguities.
Um, part of what we're aiming for with
the knowledge base is that it's completely unambiguous.
That, um, I kind of spoke about this a little bit
earlier in connection with the entities that when we have an,
uh, um, an entity ID that shows up in the knowledge base,
we definitely want that to be unambiguous.
We wanna have for New York City,
we wanna have an unambiguous entity ID in our knowledge base.
So there's no question about which New York we're talking about.
The sa- the same is true for the relations that we don't
want the relations to be ambiguous in any way.
Okay. Let me keep going.
Um, we wanna talk a little bit about the approach to evaluation.
And it's a really good idea to define the way you're gonna
evaluate your systems before you start building systems.
It's kind of the same idea as test-driven development in software engineering.
So first, you set up a test harness that allows you to measure performance,
and then you can start building your system and iterating on performance.
Having that test harness setup in advance means that you
can sort of measure progress as you go.
And actually, a great first step is to build
a random classifier that only takes five minutes to write but gives you sort of a,
a, a baseline, uh,
on which you can begin to measure progress.
A good approach to evaluation typically
starts with splitting the data into different pieces.
And in this case, I'm advocating a three-way split into a tiny split,
a train split, and a dev split.
So the tiny split is only 1% of your data.
It's gonna be really small,
but it's also going to be really fast to train and test on the tiny split.
And the merit of having a tiny split is
that you can do your early development on this tiny split,
and it can help you like just sort of flush out bugs in your code,
like just make sure your code is running properly before you scale
up to training on the big test training split and testing on the dev split,
which can take a lot longer to run.
So I think that's a great methodological practice,uh,
that I encourage you to use in other places.
And the train split is gonna be about three-quarters of our data.
The dev split is gonna be about a quarter of our data.
That's a pretty typical setup.
And during development, you'll typically train on
the train split and test on the dev split.
There's also, we've also held out a test split that we're gonna use for the bake-off.
So you don't have access to that yet but it exists.
And we're, you know, keeping it in a,
in a vault deep underground for now.
Um, I'm gonna skip this part.
There's some discussion of how to split,
but we want to split both the knowledge base and the corpus.
And there's some discussion of how to try to do that split
so it does align well with each other as,
as much as possible,
but I'm gonna skip that for now.
Um, then we wanna talk about evaluation metrics a little bit.
So this is a, we're factoring our problem into 16 binary classification tasks,
one for each relation.
Because it's binary classification,
evaluation metrics are pretty straightforward.
This is, uh, an arena where it's sort of
well understood what the best evaluation metrics are.
Typically, precision and recall are what you look at,
particularly when there's an unbalanced label distribution as there is for this problem.
So we're gonna look at precision and recall.
But it can be really helpful when developing a system
to have a single summary statistic on which to hill climb.
So to combine precision and recall into one statistic,
the most common solution is to use F measure,
which is just the harmonic mean of precision and recall.
Usually, people use F1,
which gives equal weight to precision and recall.
For this application, though,
I think you can make an argument that precision matters more than recall.
The goal here is to identify new KB triples that we can add to our knowledge base.
And we don't wanna be putting garbage into our knowledge base.
It's better to miss something than to put garbage into our knowledge base.
So we wanna give more weight to precision than we do to recall.
And the way to do that is,
um, using the F_ 0.5,
score which gives, it's a little bit counter-intuitive,
but gives twice as much weight to precision than it does to recall.
Um, and SKLearn makes it easy to, to calculate that.
We also want to have a summary statistic that averages across labels.
So we'll compute precision, recall,
and F_0.5 for each relation separately,
but we wanna have a way to combine it all into one summary statistic,
an average, and the only question is whether to do a micro-average or a macro-average.
We're gonna do a macro-average which gives equal weight to each relation,
because it's not really significant.
It doesn't really matter how many instances
happen in this dataset to fall into each relation.
So we're gonna treat each relation as equally important.
Um, so there's some code that helps you actually run these evaluations.
Um, we run it here on a random classifier.
And what you see when running on a random classifier is that
precision is bad almost everywhere,
which kind of makes sense.
Uh, but recall is always right around 50%.
This random classifier is just flipping a coin.
It's predicting true half the time.
So it kind of makes sense that if
there actually is a true lab- when the label is actually true,
we have about a 50-50 chance of getting that,
and that's what recall is measuring.
But we're seeing true all the time and most of the time,
that's not the right label which is why precision is quite low.
So the macro average precision is 8%,
the macro-average recall is 51%,
the macro average F-score is 10%. It's really low.
It's much closer to the precision than it is to the recall,
which is a characteristic of F-measure.
It tends to be pessimistic, uh,
much closer to the lower of precision or recall.
Um, and then the next thing I do is create a simple baseline model,
which, uh, I'll let you, uh,
look at it on your own, but the basic idea with a simple baseline model.
It's not even a learned model.
It's not, uh, it's not a machine-learned model.
Instead, all we do is look through the examples of, uh,
tied to each relation and we find
the most common phrase that occurs between the two entity mentions.
We count up what are the three most common phrases for each relation,
and then we just use exact matches to one of those three phrases as the predictor.
Um, so we find,
for example, that for the,
um, there was a- there was a particularly good one,
was it parents, yeah.
So look at the parents relation.
For the parents relation,
the most common phrases are comma son of,
and, and comma in the forward direction.
And in the reverse direction,
and comma, and and his son.
Um, so you can,
like those sort of make sense.
You're certainly seeing the word son in there, makes sense.
You also see that,
um, punctuation and stopwords are really common.
So it's probably a bad idea to drop those in some other NLP tasks.
People advocate dropping stopwords and punctuation.
You probably don't wanna do that here.
Um, you also see the comma by itself,
a bare comma shows up almost everywhere.
Almost every one of these relations has comma as one of its most frequent middles.
So comma is significant,
but it's extremely ambiguous.
It could indicate almost any relation.
Um, we can evaluate how this does and it does significantly better.
The summary statistic is 11.5,
so that's the macro averaged F-score,
significantly better than the random guesser.
But on the other hand, still really terrible, right?
The random guesser was like 9%,
now we're up to 11.5%.
It definitely helped, but it's still really terrible.
Um, and actually, you should have modest expectations for how we're
gonna be able to do on this task on this dataset.
We're gonna have trouble getting great precision because the KB is incomplete.
Um, there could be some relational triples that actually are
true in real life and there are sentences in the corpus that tell us that they're true,
but we can't measure that they're true because they're not in the KB.
Um, that's the KB being incomplete.
And we're also going to have trouble getting great recall because
there could be some relations that are true and are in the KB,
but there's no evidence of it in the corpus.
Both the KB and the corpus are kind of small,
and that's gonna put limits on how good the precision and recall we can achieve will be.
Nevertheless, the next step in this journey
is to apply a real machine learning model to this problem.
And in the second notebook, uh,
which is linked from the,
from the course webpage you'll see an example of doing that.
So using a bag of words feature representation,
logistic regression classifier, the most vanilla setup possible,
and even that very vanilla setup gets to a summary statistic of,
if I remember 55%,
which is vastly better than the simple baseline model.
So that will start you down the road.
That vanilla setup, um,
leaves lots of room for improvement.
And one of the things you'll be looking at in homework 3 and in
a bake-off is how to do much better on that. All right. Thank you.
 Hi everyone, happy Monday.
Um, we're going to go ahead and
start off today by reviewing the bake-off results from last week.
Um, so just as a refresher,
last week's bake-off was on the task of sentiment analysis. Um, yeah.
So it was the three class task,
positive, neutral and negative.
Um, we're evaluating on the Stanford Sentiment Treebank Test Set, um,
that had a total of 2200 sentences, um,
and we used the Macro F1 Score is the evaluation metric.
Um, yeah. So, uh,
basically like taking the accuracy on every class,
um, and then averaging all those. Um, yeah.
So we have a pretty in-balance test set, um,
with like 900 of the positive and negative,
uh, examples, and then 400 neutral.
Um, and in general, um,
the worst performance is seen on the neutral class,
um, which is kind of expected,
given this class imbalance.
So yeah, um, here is a histogram of the scores that you guys submitted.
Um, so yeah, the highest score was almost 0.7,
um, and, uh, the mode was around,
uh, 0.5 or 0.55.
Um, and you can kinda see like where you fall in comparison
to our like just basic unigrams plus softmax baseline,
um, which is actually kind of like above this peak here.
So that was pretty interesting, um, that, you know,
basic unigrams can get you so far. Um, yeah.
And so, we have, uh,
the winners of the bake-off, oh, oh,
before that, um, what's gonna distinguish the high scores and low scores.
Um, so we've kind of looked at, uh,
the like, tokens that appeared, um,
in the submissions, um,
for the top scores.
Which, uh, we can define as like scores that are about above 0.58,
um, and as you can see,
uh, it's like, you know, mostly,
kind of a lot of deep learning approaches,
and people who leverage BERT representations.
Um, yeah and then in the low scores,
um, it's kinda of a mixed bag.
I, I see some of the like, uh,
baseline of feature functions, um,
that were in the assignment originally,
um, as well as like some glove tokens.
Um, so it's interesting to see like how far BERT has surpassed,
you know, like, uh,
approaches that are not even really outdated.
But, um, like, you know,
things that we were using just last year that were like winning the bake off,
are now, you know, in this low scores section,
so that's really interesting.
Um, yeah. Now onto,uh,
our first place groups.
So congratulations to group 13.
Um, achieving the score of 0.692.
Um, using their balanced data-set and end-to-end BERT approach.
Um, yes so, uh, group 13, um,
used some smart, like, data process- preprocessing,
um, first like balancing the dataset by oversampling.
Which was very smart, because of the dataset imbalance,
um, that I mentioned before.
Um, and then actually like rejoining,
um, the contractions in the dataset.
Um, and this is kind of like an artifact of how,
uh, BERT was trained.
Um, so it's interesting that like,you know, this,
uh, like, you know,
undoing the preprocessing essentially,
um, actually helped a little bit in terms of performance.
Um, and then they trained,
um, the BERT model end to end.
Uh, basically, like fine-tuning BERT for the sentiment classification task.
Um, yeah so, uh,
good job to that team.
Um, now for our second place team,
group 51, um,
used a similar approach, um,
of using BERT to create the, uh,
feature representations, and then, uh,
using a neural net classifier on top of that.
So congratulations to group 51 as well.
Um, they achieved a score of 0.651.
Um, you know, doing a similar thing of fine tuning BERT on SST, um,
and then running inference to, uh,
generate the features for each sentence,
and then feeding that into our TorchShallowNeuralClassifier.
Um, and they also used, uh,
the strategy of kind of like up-sampling
on examples from the neutral class to do better on that class.
Um, and then we also wanted to point out,
um, some other interesting approaches in addition to our top two teams.
Um, so we wanted to shout out group 9,
which did super well, actually getting a- uh,
like the same score as ou- our top team,
but unfortunately can't qualify to win the bake-off,
um, because they actually use the sub-tree labels.
But it's still a very interesting approach,
we wanted to point it out.
Um, this group actually, uh,
formulated the task as a sequence to sequence task, um, using,
like the strings containing the sentence annotations and tree, tree structure, um,
as the input sequence,
and then the sentiment label,
um, as an output sequence of length 1.
Um, so that was pretty cool.
Um, basically the only group,
um, to think of formulating the problem that way.
Um, and actually one of the only groups,
uh, among the top teams to not use BERT or ELMo.
Um, and then just using a-, uh,
two-layer bidirectional LSTM, um, for a classification.
Uh, and then we also wanted to shout out anyone who tried feature engineering.
Um, unfortunately, uh, feature engineering didn't make it into,
uh, I mean, like the top scoring systems,
but that was interesting to us on the teaching team, because, uh,
last year's top two systems actually both used hand-built,
like bag of words based features with a simple logistic- logistic regression classifier.
Although this year, um,
a lot of the top teams relied on deep learning, um,
kind of shows like a shift in the field. Um, yeah.
So these are the top two teams from last year,
um, and these scores aren't comparable to this year,
because they refer to the binary task.
While we were doing the ternary task this year.
But yeah, our teaching team is well represented here [LAUGHTER] um,
but on first place, um,
uh, last year, Jada's team used, um,
preprocessing, uh, with just like removing punctuation, and then, um,
character n-gram features of various lengths,
as well as tf-idf, um,
and then just a logistic regression classifier.
And that managed to get a really,
really good score, um, on this task.
Um, so yeah, it's interesting to see like,
you know, how, what we were doing last year with like,
manual on feature engineering, um,
has like kind of been totally replaced this year by,
um, massive compute, created pre-trained BERT representation.
And second place, um, Lucy's team, shout out to Lucy.
Um, also used similar like kinda feature engineering approach, um,
with like very, very rich features, uni-grams,
bi-grams, and negation words, leveraging sentiment lexicon.
Um, yeah. So that's like also a direction
that liking you could definitely go um, if you were to,
you do the sentiment task maybe like in your class project or something like that.
Um, yeah. And I think that's about it.
Um, anyone have any questions about,
uh, last week's bake-off?
I wonder if they're representatives from the top two teams here,
I wonder whether those are kind of theoretically the same system.
Because it sounds like the first team used BERT and
used a PyTorch layer on top to fit a classifier, maybe a deep one.
And the second placed team just used our classifier as opposed to the,
um, one that the BERT team released.
So I wonder, if it's just a difference of like how they were optimized,
or actually maybe there's a,
there's a deeper difference that I'm overlooking,
but they look very similar.
Yeah. Anyone from group 13 or 51 want to weigh in?
Because the- it must be that team 1 just didn't use TorchShallowNeuralClassifier,
but rather, something that's part of the BERT,
the BERT, PyTorch code release.
Yeah. I believe this group, um,
just kinda like tweaked the final layers of the BERT model, um,
like the preexisting, like PyTorch implementation.
Yeah, this is cool to, this is common to
see on deep learning code on the internet.
You find that in some file somewhere in the code base,
someone has done some very careful preprocessing.
Presumably, this is to,
to sort of get properly into the BERT vocabulary or something.
Yeah. I was curious about this,
they actually say on the BERT page,
you shouldn't like unprocess your token so it looks more like raw text. Okay.
Well, thank you. [APPLAUSE] Yeah.
And I've got my slides and everything,
maybe this will be my new solution.
Um, so okay.
So thank you [inaudible] Um, bake-off stuff.
First, I just wanted to say that Amazon gave us enough codes so that we can give $50,
um, codes for AWS to people who win the bake-off.
So the first team got their codes,
and that winning team there will get codes, and we'll keep that up.
It has to be just the top scoring system numerically,
because we don't have that many extras.
Um, some, some more teams might get extra credit than that,
but it is nice that we can reward the very top teams.
Um, other bake-off stuff.
So I've posted bake-off 3 on Piazza,
I think it's straightforward.
I- it looks a lot like the procedure for bake-off 1,
in the sense that you need to download some data, um,
and then you're pretty much just,
just running code that you've written.
Um, but of course, post on Piazza if there are any issues there.
Uh, I also wanted to mention that there was a PyTorch and numpy tutorial on Friday,
and the team produced great notebooks for that.
I think it's especially [inaudible].
And so, I posted those at the homepage,
um, at the top, on the first day.
It's a bit revisionist,
but it's kind of like introduction to Jupyter Notebooks,
and now you have the stuff for numpy and PyTorch.
And I think the timing of that is really nice,
because you guys might be thinking about projects you wanna do,
and for your projects, you might wanna,
um, dive into the PyTorch a little bit.
[NOISE] And then the final thing I wanna do here,
before I dive into NLI,
is just kind of introduce homework 4 and the bake-off.
And the reason I wanted to do that first is,
work- it's a kind of reversal.
And we're gonna talk- we're gonna talk about
natural language inference all week this week.
Um, and the bake-off is about natural language inference,
but because the really good NLI datasets are enormous,
I didn't want you to have to do that for a bake-off,
because it would consume a lot of time and a lot of resources.
So instead, for homework 4 and bake-off 4,
we're gonna do just a word-level entailment task.
[NOISE] Uh, and this is kind of nice,
because it's like a microcosm of the whole NLI problem,
it just removes a lot of the overhead from processing large datasets,
and it's kind of nicely constrained in some interesting ways.
So let me just start by introducing it,
because they're kind of two or three intellectual things
that I think are worth calling out.
[NOISE] So the- the first, the framing, uh,
throughout this problem set,
the training instances are gonna be pairs like,
hippo and mammal, just words.
Um, [NOISE] people have actually developed
labeled datasets that just tell you for these at the word level,
whether the first entails the second,
and that's basically the task.
So it's a binary task,
um, with vocabulary like this.
Basically, because it's constrained to just words,
you have to make just a few decisions here.
So first, kind of like in bake-off 1,
you need to decide how you're gonna represent the pair of those words.
So hippo and mammal are just items in your vocabulary,
but you can look them up in some embedding space,
first of all, and you can decide what embedding space,
and it can be any embedding space.
I guess the smart move based on past bake-offs is to use something like BERT or ELMo.
Sorry, my phone is hardwired to stop,
to shut off very quickly.
I'll try to avoid that.
[NOISE] So you'll wanna look up hippo and mammal in an embedding space.
You can also decide how you're gonna represent them, right?
So I think like, what I've suggested here is concatenation,
but you could do something else if you wanted, uh,
difference, or multiplication, or sum, whatever you want it to do.
So, so embedding and then representation.
And then from there,
you're gonna wanna fit some kind of model, right?
So the simplest thing would be to fit, uh,
like a basic classifier,
but you could have also a very deep network in there, right?
Um, lots of hidden representations.
And I'm gonna leave it entirely up to you what you wanna do there.
I've fit a baseline that's kind of a shallow deep learning model,
but you can do really whatever you want.
Uh, and that's the bake-off.
That makes sense?
And you can see that this is a microcosm of the whole NLI problem, because,
after all for NLI, it's just that hippo and mammal
could both be complete sentences or very complicated noun phrases.
But in the end also we have three labels,
as you'll see for our output space.
But fundamentally, this is the same kind of problem,
it just means that you can iterate much more quickly
because the datasets are smaller and there's less to process.
There's one other subtlety that I wanna point out here.
So you have a dataset that comes with the notebook,
it's already in your data folder.
And there are two conditions for it.
I've called the first one edge_disjoint,
and that just means that the pairs,
uh, on training and testing,
you're trained in dev, but also test,
which I'll release as part of the bake-off.
At the edge level, things are disjoint.
What that means is,
that you could see a lot of the same vocabulary at train and test time, right?
So like hippo and mammal,
if you saw them in training,
you won't see exactly that pair,
but you might have seen hippo paired with other words,
and mammal paired with other words.
And that's a kind of standard formulation of this problem.
But, it's kind of easy to game,
in the sense that, you know,
the- the systems that result from this often look very good in terms of performance,
but you might doubt whether they have actually learned to generalize.
Because it could be that they've just kind of learned
to triangulate within the space that they're in,
to do pretty well on the test set.
And you can kind of see that because some baselines,
which I'm gonna tell you about later,
tend to be very high for this problem.
Like, if I give you only the second word,
what we'll call only the hypothesis,
maybe you get like 80%.
So I- we're not gonna focus on that task, instead,
we're gonna focus on this condition that I've called word_disjoint.
And what that means, is that, for train and dev,
and also for test, the vocabularies are disjoint.
So that means, if you saw a hippo and mammal in training,
you will not see either of those words at test time.
Um, and it, it entails therefore that you
won't see that- you'd haven't seen those edges before, right?
So this is a strictly harder problem that is really testing your network's ability,
not only to do well at words it's seen,
but also to generalize from that into other parts of the lexicon. Yeah.
I think you mentioned slightly above that one of
the possibilities for the embedding is just a random embedding?
[NOISE] Does that work with word_disjoint if you had a random embedding?
It certainly does not [LAUGHTER].
Yes, um, your intuition is exactly right.
If you have a random initial embedding,
then you will produce random results on the- not on the edge_disjoint,
you could do really well on edge_disjoint,
but on word_disjoint, you will flounder.
And what that means is,
that you need to initialize with a rich space,
and you'll see in the baselines that I've put down here that, uh, I did GloVe.
Um, But kind of the striking and interesting thing about this, is that,
initializing with GloVe or I assume with BERT and ELMo,
really gives you a lot of traction on this problem,
even on the word_disjoint condition.
So I've walked through just to show you in a bit more detail in this notebook, um,
what these two datasets are like,
and I've kind of built up this argument here for why
we might wanna use the word_disjoint condition.
I also set a baseline, um,
which uses these GloVe vectors here as the kind of, um,
fundamental representation of the words,
you just do a GloVe look up in a way that you guys have done on earlier problems.
And for my baseline co- combination function for the two words,
I just use concatenation.
And then for the classifier, as I said before,
I use this TorchShallowNeuralClassifier from the course repo,
50 dimensional hidden representations,
a modest number of iterations.
And, as you guys are probably accustomed to at this point,
we have this kind of little experiment wrapper,
that should make it pretty easy for you to run experiments.
So, you know, model is your,
um, the model that you wanna test, it could be anything.
A glove_vec and vec_concatenate,
those are really the three points that you'll want variation on,
and you'll probably just wanna use train and dev there for your, um, development phase.
So for my baseline,
I got macro average 0.69,
uh, which is pretty good.
But obviously, the goal for you in doing the homework in the bake-off,
is to see how much you can build off of that baseline,
with a richer network or more interesting representations,
or more in different ways to combine
the two-word vectors to form your input representation.
Is that making sense so far? Any questions?
Comments? Concerns? [NOISE] It should be a fun one,
because you can really develop models fast,
so you should be able to try a lot of different things.
This is sort of anticipating stuff that I wanna discuss for you,
but let me just walk through the rationale for the homework.
So the first one is called hypothesis-only baseline.
As you'll see as we go through the material,
people discovered a few years ago that you could actually do
really well on many NLI datasets if you threw out the first part.
So in my hippo-mammal pair,
just throw out hippo,
and make your classification purely on the basis of the word mammal.
And I wanna discuss that with you,
because I think it's kind of an interesting thing that mixes,
maybe some artifacts in these datasets,
but also a pretty deep fact about how the lexicon is organized,
and what it means to be in an entailment relationship.
I mean, if you think about that, it's not surprising that mammal would be
more likely to occur on the right and an entailment pair than the left,
because of its generality.
[NOISE] Oh, and actually,
this fact about hypothesis-only baselines,
was discovered by a student in this class, which I think is really cool.
For his- as far as I know,
the first person to observe this for the SNLI dataset,
which is a big entailment dataset,
was a project in this course in 2015.
And then, subsequently a number of,
um, other groups discovered it and quantified it.
[NOISE] So what we're asking you to do here is to just see how
strong the hypothesis-only baseline is for this dataset.
And what we're trying to signal there is that,
given these- for this prior work,
if you tackle an NLI problem,
then basically, you should fit a hypothesis-only baseline,
because random is not sure baseline at all,
it's actually this one.
So it asks you to do that, that's pretty straightforward.
And then it just asks you in the spirit of thinking freely about
this to explore an alternative representation to concatenation,
so that you might think about some interesting kind of
apriori ways that you might combine the two representations,
maybe difference is interesting for
an entailment space or multiplication or something like that.
And then, this is where that PyTorch tutorial might come in.
I'm gonna ask you to do a little bit of work,
kind of sub-classing one of our PyTorch classes and adding essentially another layer,
and a dropout layer.
Dropout is a form of regularization.
That's very easy to implement in PyTorch,
I've kind of presented the mathematical details here,
but I think you'll find that the important thing is to just use
the PyTorch documentation to figure out how to add into your sequential model,
a dropout layer, and then you'll be off and running.
And again, the spirit of this is to get you thinking about,
how regularization for your networks might be
a powerful technique to avoid over-fitting and stuff like that.
And I've kind of started you,
and given you some instructions,
and we're happy to talk with you about
the best strategy for this kind of coding on Piazza.
And then, finally, as usual,
you do your original system.
We'll see what happens.
I don't think I have to impose any rules on this.
You can download vectors from the internet.
You can download code as long you do- as you do something original with it.
And the reason I can say that is because the bake-off will be
conducted on a held out data set that I have not given you.
So you can do all the development you want on the data we've provided.
Uh, just don't overfit too much,
because then you'll get hit at test time, in the bake-off.
Makes sense? Any questions about that before we dive into th- the regular material?
So that was a small glimpse of the problem that you'll be exploring.
Um, but what we're gonna focus on this week is the problem in its fullness,
this problem of Natural Language Inference.
Um, I think this is a wonderful problem,
because I think it gets at something really deep about what we're trying to do in NLU.
And also, there is an abundance of really interesting data sets at this point.
Uh, because the field has kind of, um,
triangulated on an interesting methodology [NOISE] for collecting these data sets,
and people have found that it's pretty easy to crowd source good ones.
I mean, there are some issues that we'll discuss.
But by and large, this has kind of been productionized.
And so, you have multiple datasets that are in
the same format making it very easy to experiment.
So I don't encourage having a default project for this course,
because I think it's interesting for you guys to get creative.
But if there were a default project,
I would certainly say it should be something with NLI.
Because it tests lots of the concepts that we've introduced,
there's a really wide space of models that you can explore,
and these datasets are so good.
I also want to say, that there are many special connections to this course for NLI.
So it used to be called RTE until Bill came along.
For his thesis, he started calling it Natural Language Inference,
and Bill did lots of seminal work in this space,
kind of exploring how you might apply natural logic,
which has a very rich algebra of inferential relationships to NLP tasks,
uh, in the space.
And so, we'd like Bill is part of the resurgence of interest in this problem.
And then, Sam Bowman was a student in this course.
Sam was also my student.
And I think the full history of this is that,
Richard Socher came and gave a lecture on,
you know, tree structured networks for sentiment analysis.
It was about the time that we were developing the Stanford Sentiment Treebank,
and Sam was captivated.
Uh, I think Sam was not focused on NLU at the time for his research,
but that was a kind of important moment for him.
Uh, and he wanted to work on NLI,
because I think he admired Bill's work and he liked thinking like a semanticist.
But there weren't really good data sets.
He felt kind of stuck by the ones that were available,
and he just did the really admirable and ambitious thing of saying,
let's collect a massive data set for NLI,
so that we can test these new deep learning models.
Because one, one sticking point at that point was,
that the available data sets were kind of too small
to really test these deep learning approaches.
So Sam developed SNLI and the rest is history,
the, the task has been renamed.
Nobody says RTE anymore.
Um, and we have all these interesting data sets,
and Sam has been a real powerhouse in terms of developing new ones.
So he's responsible for both SNLI and MultiNLI,
which is the two data sets that we're gonna concentrate on.
And then, just while I'm in this vein,
I want to mention also that both,
and so, both working hard on NLI problems right now,
kind of using it to stress
test neural systems to see what they're actually learning about semantics,
is using them as a test bed for
interesting new neural models that he's developing with some researchers at IBM.
So there are lots of us on
the teaching team who are interested in talking with you about NLI.
[NOISE] Here's my overview.
Um, I'm gonna give you a bit of background on the problem,
and I'm gonna introduce these two datasets, SNLI and MultiNLI.
And then, we're gonna do the kind of standard narrative for this course.
I'm gonna show you some hand-built feature functions,
which I think are really good and interesting baselines for the problem.
And having done that, then I'm gonna show you that we have in the course repo,
a nice framework for doing experiments on all these kind of,
um, SNLI style data sets.
From there, we're gonna start to look at deep learning models.
And NLI is interesting as compared to sentiment,
because basically, you have two texts to work with,
the premise and the hypothesis,
and that opens up lots of interesting avenues for doing modeling.
Because, you can think about how to model the premise and hypothesis separately,
and how to relate them,
and then finally how to fit a classifier.
So we'll look at sentence-encoding models, chained models.
Those are kind of the main classes.
And then, as an orthogonal development,
I'm gonna show you some stuff on attention,
which has really risen to prominence recently.
I think an NLI and also machine translation
were kind of motivating problems for attention in NLU.
So we'll talk about those kind of things and how you might
add them to your systems to do, uh, even better.
And then, finally, we'll talk about error analysis.
These things will probably come on Wednesday.
We'll see how things go.
But I think again,
because of NLI's very rich grounding in linguistics,
you have some very interesting avenues for thinking about error analysis,
and how it might relate to linguistic patterns,
and logical patterns, and stuff like that.
So that's our full plot.
Uh, as usual, we have a lot of material.
So nli.py is your main module,
both for the homework,
and if you want to work with these larger data sets.
And then, there are two notebooks.
So the task in data one introduces SNLI and MultiNLI,
and shows you the code that we have for working with them.
And then, the second notebook is a whole lot of modeling code,
to kind of get you started if you want to pursue this task.
We've gone over Homework 4 and bake-off 4.
And then, I'll just mention that the core readings in my mind,
we posted a few more,
but I would say the core readings are Bowman et al.
2015, that introduced SNLI and set up some initial baselines.
And then, this Rocktäschel et al.,
that's a seminal paper in terms of introducing attention,
word by word attention into the NLI problem,
and showing that it could lead to big gains.
And then, I also recommend some auxiliary readings.
So the Goldberg 2015 is that primer on deep learning models,
that I also recommended in the last unit.
[NOISE] So that's kind of separate.
Then, for NLI in particular, Dagan et al.
is a very important foundational paper that more or less introduced the task of RTE.
And then, MacCartney and Manning,
that's a nice presentation of this natural logical approach that Bill developed.
And then, Williams et al.
is the paper that introduced MultiNLI,
and that's another good resource for you in terms of
baselines and in terms of understanding what's in that data set.
These are typical NLI examples.
I picked them a little bit strategically just to give
you a sense for the kind of variation that we see.
So the top one there,
you would say that turtle and linguist contradict each other.
That would be the label for that pair.
In our homework, we just have yes-no in entailment,
but in the data sets that we'll focus on today and on Wednesday,
you have three-way labels,
contradiction, entailment, and neutral.
And so, we would say that,
turtle and linguist contradict each other.
There's already some complexity there,
because you can easily imagine a possible world in which
some turtles have- are qualified as linguists, right?
Um, you can kind of already see that
we're not gonna be dealing with a notion of logical contradiction,
but rather something closer to common sense inference based on world knowledge.
A turtle danced and a turtle moved,
that actually might be closer to just being a true entailment fact,
because of the lexical relationship between dance and move.
The only, a- assumption you need to [NOISE] bring in
there is that these two turtles are the same turtle.
Uh, and that's actually
a touchy point when you get to naturalistic data and I'm gonna return to it.
But kind of what we're doing now is making some assumptions about
event or entity co-reference across the premise and hypothesis.
And in that case, dance entails move and so we get entailment. Yeah.
[inaudible] I guess like so you're assuming that turtles can dance.
And I guess the question is like,
where do you get like this basic like common sense knowledge?
That's interesting. I guess to,
to, to be careful about it,
I'm not assuming that a turtle danced,
but rather if a turtle danced, then the turtle moved.
I think the flavor is much more like that.
Um, but again, with some qualifications,
because this is naturalistic data,
as you can see, it's actually based on image captions.
And so there is some commitment to the premises being realistic.
I guess I just like turtles, and that's why I picked it.
So for the first example it's like, it's more like, a turtle can't be a linguist kind of thing, right?
Because you're interpreting that within like the real world.
Yeah. [OVERLAPPING].
It's hard for me to see like the difference between the two.
Because you can imagine a world in which a turtle is a linguist?
Yeah.
Yeah, fair enough. Then, you would be the- probably the
outlier annotator if you were asked to do the annotation,
but you have a defensible point.
The logician in me wants to say,
you know, that's certainly not a contradiction, the first one.
Um, and that- the second one is not necessarily an entailment unless I have
proper existential closure of the two variables across the premise and hypothesis.
Yeah. This is the kind of thing you have to get used to.
How do you check for like common sense,
um, knowledge, like for the first example?
I would frame it differently.
I would say it's not so much checking for common sense knowledge
as acknowledging that this is pervasive in this task.
And that you might- the way you might even think about the NLI problem is as, you know,
one that requires an understanding of common sense reasoning and kind of world knowledge,
and that that's fueling the system doing well at these problems.
But it's, it's really difficult.
[inaudible] Why-
I love it. Yes, you live in a magical world.
[LAUGHTER]
Well the turtle danced, the turtle moved.
I was a little curious why you're saying it has to be the same turtle.
Is it- Is it because a turtle danced then that implies that
at least some turtles moved and so a turtle like doesn't have to be the same?
Well that's a fair point actually for this one.
Yeah, we just follow logically that if the- if
any old turtle danced than there is some turtle that moved. Yeah, that's fair.
Yeah. Every reptile danced,
a turtle ate, those would be neutral.
Again, like that- that's sort of a logical fact, I think.
Some turtles walk, no turtles move,
those contradict each other and this is a classic example from Bill's thesis.
James Byron Dean refused to move without
blue jeans entails James Dean didn't dance without pants.
I guess, Bill is highlighting not only the kind of linguistic complexity of
this reasoning task because it might take you
a minute to see that those are an entailment relationship.
But also that as part of this you might have to solve some stuff like
[NOISE] figuring out that James Byron Dean and James Dean are the same person.
And for that, you might need a lot of world knowledge of a particular kind.
But I think that those follow more or less logically,
we don't need much common sense reasoning there.
That will be very different for the next one.
Mitsubishi Motor Corp's new vehicle sales in the US
fell 46% in June and Mitsubishi sales rose 46%.
Oh no, that one's straightforward.
That's just a contradiction, fall and rise.
I guess, we have to make some assumptions about the domain of
these claims and I'll return to
that but by and large those seem like they contradict each other.
This next one definitely involves common sense reasoning.
Acme Corporation reported that its CEO resigned,
entails Acme's CEO resigned.
It certainly doesn't because companies can report anything they
want and sometimes they report false things and so strictly speaking,
if we were semanticists,
we would say that this report has no entailments with respect to,
um, the complement, Acme's CEO resigned.
But again, it's a kind of common sense thing given that we know that
this corporation is likely a reliable source about this kind of information,
then we might infer entailment or annotators might be encouraged to infer entailment.
[OVERLAPPING] James Byron Dean and James Dean one,
in my mind I feel like we're walking a fine line here because to assume they're
the same person because take the example of like
George W Bush and George HW or something.
If you call one George Bush and the other you had to
specified by quantifying their middle name or something.
That's a great point. Yeah. Um, maybe in the- so,
in the back of your mind,
you should be thinking that there's a practical side of this,
which is that we want systems that could just read through newspaper texts for
example and figure out what the entailments are or what the logical relationships are.
And in that case, this isn't a problem we can ignore
rather we need systems that are sensitive to that level of
distinction and that know how many George Bushes there are at
some implicit level and that can also resolve these to the same entity.
But the uncertainty that you guys are feeling,
this is wonderful you sound like linguists.
This is the- the linguist's reaction to this problem is
usually none of these things have any of these logical relationships.
It's all neutral because I can think of edge cases where it falls
apart and I have to reti- retreat to the idea
that we're kind of trying to get a hold of what you as
a reader would infer from these texts or infer from these pairs of sentences.
[OVERLAPPING] Is there a way to rephrase the task,
so that it can detect the ambiguity and sort of report it or relabel it? And if so,
how would you go about doing it?
Sort of make finer distinctions between logical entailment
and common sense reasoning and
abduction and all these other notions that are in play here.
I think, yeah, here.
So, this kind of summarizes this and I would refer you to this trio of articles.
This is a kind of back and forth.
These are all Stanford people or at one time they were all Stanford people,
kind of having a debate about
how much non-logical reasoning we should allow to creep into this problem and so,
I think I won't resolve it but it's an interesting discussion.
I do think though that the top of this kind of
summarizes the view that I'm trying to convey which is,
does the premise justify an inference to the hypothesis given
some common sense assumptions about the world
and the speaker and our intentions and so forth?
Um, yeah, that's the heart of it and then
the other aspects of this problem is that there's been
a focus on kind of local inference.
So just premise and hypothesis, um,
and there's a lot of emphasis on capturing variability in
these expressions like different ways that you might refer to people,
um, different ways that the same concept might be articulated and so forth and so on.
I do think that what happens if you tr- try to be strict
about this and say that you're gonna look only at logical entailment,
is that the problem gets kind of small and limited.
It's basically, just like
really reliable lexical relationships and then even then the linguists come in and say,
wait a second, word sense ambiguities are causing
me even to doubt this thing that you thought was a solid entailment,
then it all kinda falls apart.
And that's why- that's the story I tell myself about
why it actually makes sense to think about this
as a common sense reasoning task as much as I might like logical inference.
This is kind of nice. This is from that important paper by
Ido Dagan and colleagues that introduced RTE,
what we now call NLI. I'll just read this.
It seems that major inferences,
as needed by multiple applications,
can indeed be cast in terms of textual entailment.
Consequently, we hypothesized that textual entailment recognition is
a suitable generic task for evaluating and comparing applied semantic inference models.
Eventually, such efforts can promote the development of entailment recognition engines,
which may provide useful generic modules across applications.
I think it's a nice articulation of a dream for NLI as a task.
The idea would be, if we can get our systems to be really
good at this kind of basic but pervasive logical reasoning,
then the capabilities they have would apply in lots of domains.
And that certainly resonates with me as a semanticist because I kind of think of entailment and
contradiction as being really central to how we think about language.
How we reason with it?
How we use it? And so forth. Yeah.
This could be used to like,
[NOISE] for example, say someone made a query but they like
sort of misunderstand what they're like trying to get.
But it entails like the same like generic things
that it like goes to the other thing that entails the same thing.
Does that even make sense? Sorry.
I mean, you're speaking Ido Dagan's lang- language, right?
You're keying in into his framing,
which is to say like, you know,
challenging problems and NLI maybe they turn on misunderstanding or reformulation.
We can actually reduce them into an NLI task and that would be wonderful because if
we could then the models I'm showing you today would have a lot more applicability.
So, I'm not sure precisely what you mean but I'm really
open-minded about this and I've actually tried to do some of this here.
This is actually taken from Ido Dagan's article,
um, you know, Paraphrase detection is a well-known task.
You could think of that as just the task of saying,
is the text and its purported paraphrase equal in some extended sense of,
um, common sense reasoning,
or summarization would say,
does the input text entail the summary which would be like saying we want.
At least as a constraint on summarization that
the result is somewhat more general than the input.
Um, information retrieval could go in the reverse direction.
You could be- you could formulate that as saying that
you would like essentially to return
documents that entailed the query in the extended sense of NLI.
And then question answering is one that you can basically
reduce to saying that you want the answer to entail the question.
If you are willing to massage the question into a declarative form a little bit.
So for example, if you were willing to replace in English
all the WH words with indefinites so that who left became someone left.
Then you have a straightforward NLI task and
you're really saying you would like an answer like,
Sandy left to entail someone
left and you would call that your- your notion of answer-hood.
And I think there are other tasks out there that you could try to
reduce to the same form and that's the inspiring idea that we've
really gone kinda down to the metal here in terms of
understanding reasoning in language which is applicable to lots of different areas.
Again, by way of background this is sort of interesting.
This is a landscape of different approaches that people have taken to the NLI problem.
So up here, I have logic and theorem proving,
those are kind of the earliest approaches inspired by logic and linguistics.
And I've put them very high on the depth of representations
axis because of course they give you very high fidelity pictures of the data.
But along the X axis,
I have effectiveness which is kind of like,
if you give me a whole lot of data,
does this model do good and interesting things with it in a comprehensive way?
And, ah, the main limitation of logic in theorem
proving systems is because they tend to be handcrafted.
They're high on the Y axis but low on the X axis.
Um, Bill developed this natural logic approach [NOISE] which is kind of intermediate.
So it's very rich logically,
but because the logic is defined over natural language utterances,
you have a lot of power to apply it to
larger data-sets and so it kinda moved us to the right along the X axis,
um, with some sacrifice in terms of the depth of representations but still very rich.
Um, below that I put semantic graphs,
which was another kind of handcrafted approach that depended a lot
on very high fidelity alignments between the premise and hypothesis.
Again, getting more effective with some sacrifices in terms of interpretability and so
forth and then over here at the top I have clever hand-built features,
ah, kind of as exemplified in this production system called the Excitement Open Platform,
which is a good kind of classical system for solving the NLI problem.
And then down just below it,
I put n-gram variations.
We're gonna look at some of those today.
So, by that point you're very effective,
those models can be very powerful but very low in
terms of them offering a deep understanding of the NLI problem.
Basically, you just have a lot of
superficial associations and then something interesting has happened,
so I put deep learning 2015 there.
Because in 2015, when we really introduced this task, um,
into the course proper,
the deep learning models we're not really a slam dunk.
This was kinda before or just at the time SLI was being
introduced and by and large those systems were being beaten by hand-crafted ones.
I do think it's safe to say that by now in 2019,
deep learning models have overtaken the hand built ones to the point
where even for the little experiments that you see in my notebooks and so forth.
It's very easy for you to beat a very strong,
um, handcrafted features baseline using some deep learning techniques.
And then I mentioned that there are a lot of datasets.
Here's a whole bunch of them.
Um, so the FraCaS textual inference test suite
is interesting historically [NOISE] because that is an attempt to have
a data set [NOISE] where basically you have been confined at least for
the parts that we're talking about to logical inference problems.
So it's small but I think still really interesting as a kind of test
set if you wanted to stress test your system for example.
And then pre SNLI,
there were a few data sets like SemEval 2013 to SemEval 2014.
That was the sixth dataset which I actually distributed as part of the dataset, uh,
for the course because Sam Bowman put it into the SNLI format.
So it's there if you wanna play around with it.
But again, it's kind of small and had some idiosyncrasies and that was before SNLI.
And then after SNLI,
you have a flourishing of datasets that are in the same mode.
So MedNLI is for medical stuff.
Very challenging problem, very rich vocabulary.
XNLI is multilingual NLI datasets that had been
derived from MultiNLI by a group at Facebook.
Um, diverse natural language inference collection is again a really large-scale,
um, very diverse set of problems from a group at Hopkins.
SciTail is SNLI like,
um, but it's derived from
science questions and multiple choice questions and also web texts.
So that's also really challenging.
And then I put two just to kind of get you thinking about this in the mode of Ido Dagan.
So like the factoid question answer corpus can be
thought of as a kind of NLI problem if you wanna put it into
that mold or the Penn Paraphrase Database is
just a simple version of NLI where you're trying to jus- get just one relationship.
And then really interesting,
this is a newer development,
the GLUE benchmark was released by Sam Bowman and his group,
and what they did is,
it's a whole bunch of NLI tasks,
I think five of them, and then a whole bunch of other tasks that aren't strictly
NLI problems but are meant to kind of test
the Dagan assumption that if you have a good NLI system,
then those representations will transfer to other tasks.
Um, and a lot of people have been working with that.
The- there's a lot of progress right now on the GLUE benchmark.
So for a very ambitious final project,
you could tackle that in whole or in part.
I think I'll skip over this. These are
some different label sets that have been explored over the years.
For better or worse,
we've kind of consolidated on this three-way one because of SNLI.
Um, but before that there were four-way versions
and simpler two-way versions like you'll explore in the homework.
Uh, I mentioned this before because this is relevant for the homework.
I have your testing this hypothesis, yes.
So Leonid Keselman in this pro,
in this course for his final project observed the strength of
hypothesis-only baselines and it's been
substantiated by a bunch of other groups including one by Sam Bowman.
It's interesting to ask why this holds.
So why would it be,
why could you get any traction at all if you were
just shown the hypothesis in a premise hypothesis pair?
Initially, that sounds very worrying, right?
Because it sounds like there's something deeply
problematic about the data that wou- would
allow you to leave off so much of your example and still do well.
I think it is something to worry about but I'm not myself so worried because I think
this has a principled explanation that is more or less grounded in the problem itself.
And I've kind of summarized that in these,
um, three statements here.
So specific claims are likely to be premises and entailments cases.
That's really easy for me to imagine if I think of the WordNet hierarchy.
So WordNet is basically a huge hierarchy of entailment relations for nouns.
The things that are at the bottom of those,
that hierarchy are very specific things like puppy and kitten,
um, and they entail almost everything else,
else in the hierarchy on up to object.
So if I ha-,
if you show me just the word puppy or kitten,
[NOISE] I have a very good guess about whether it's in the premise or the hypothesis.
Conversely, if you show me something very general like mammal or object or dog,
then it's very likely to be on the right in that pair just
because if I drew another word from the vocabulary,
it's unlikely to be above it because so few words are.
And then, this extends also to contradiction.
So I think that specific things are more likely to lead to
contradiction by the kind of rules of the SNLI game because,
we make some assumptions about event co-reference and so forth
and if you've described something very specific in the,
in the- these terms,
it probably excludes most other claims.
And so again, very specific things likely to lead you to contradiction on either side.
So I think it's the result of these three observations
that is driving a lot of the strength of hypothesis-only baselines.
And if you believe that argument,
it means that this isn't really something we are gonna be able
to naturally factor out because
this fact about the entailment hierarchy
is something that we want our systems [NOISE] to learn.
It's a, it's an important semantic fact,
and if you decided for example that you would get rid of it by balancing
your dataset so that every word had
equal likelihood of appearing as a premise or hypothesis,
you could weaken the hypothesis-only baseline but it's not so clear to me that
the resulting systems would be solving the problem as we encounter it in the world.
And for that reason, my response to this is just to
say let's always have in the top of our results table,
a hypothesis-only baseline so that we know the starting point for our task.
Does that make sense? Anyone who wants to push back on that? Yeah.
Just out of curiosity, does a premise-only baseline do anything?
It's under Explorer.
If you believe my generalizations,
it ought to be better than random because I think there is information,
er, in just that,
that part but I haven't explored it too much.
My coach should make it really easy to explore.
Okay, this is good timing.
Let's dive into what these datasets are like.
So we're gonna look at SNLI and MultiNLI
which are kind of common mold for datasets like this at this point.
So here's a quick summary.
It was released by Bowman et al, 2015.
Um, all of the premises are image captions from the Flickr 30K data set,
and that's really important to keep in mind because image captions are kind of
unusual bits of language and it also means that all
of these things int- at some level for people who produce them were grounded
in a specific image and I think that has shaped the data.
The hypotheses were all written by crowdworkers.
So this was a very large crowd-sourcing task because this has over 500,000 examples and
all of those hypotheses were written by people and
then parts of the dataset were evaluated by other humans.
Right? Um, I should note that some of the sentences reflect stereotypes.
This is a nice paper from a Hopkins group again.
Just pointing out that as you might expect,
people who are doing lots of crowdwork kind of relied on some shortcuts to producing
contradiction and attainment pairs and a byproduct of
that is that they end up relying on problematic stereotypes.
So this is something we would like to eliminate from
these datasets because we don't wanna perpetuate these problems.
Um, but for now I can just say that you should be aware of this.
Yeah, so about 550 training examples,
10K dev and 10K test.
I've given some information of like the mean length of the examples,
um, the types of clauses,
so they're not all sentences.
Some of them are noun phrases.
Er, it's got a fairly large vocabulary,
almost 40,000 words, er,
and then it has a smaller subset of about 60,000 examples that were validated
by four additional annotators if you want increased confidence that the label is correct.
Er, and that went rather well,
there was a very high level of agreement for the, um, labels.
And then finally down here, there's a leaderboard which has
a truly overwhelming number of papers on it now.
It's kind of hard to digest and I think we're past the day where you
would just like hope that you were the highest row.
Um, I would think about it some more flexibly.
So Sam already divided into
a few different model classes and you can think about where you want to slot in.
And then another distinction that has emerged is
just whether or not your model is an ensemble.
I would say that for SNLI in particular,
we've entered a phase where ensemble models are just doing best.
Um, and that's an interesting game if you want to just pursue
raw performance but it's a kind of different thing you're doing on, in,
in the modeling sense than picking, uh,
a single model that you think reflects an intuition about
the data or the problem itself and pursuing it as far as you can.
So think in a nuanced way about how you're
doing with your system with respect to the leaderboard.
This is a screenshot of the crowdsourcing methods that were used.
It's kind of small.
It's here just as documentation.
Um, obviously for crowdworkers,
we couldn't just tell them, "Hey,
construct a sentence that entails the premise or contradicts the premise",
because that's a kind of specialized logical notion.
So I think we found a way of framing it so that
it was pretty clear what we were after but more naturalistic.
But if you wanna do a deep dive on what the data actually mean,
this is important information because this is
the starting point for how the examples were constructed.
And here are some actual examples.
These are, these are the ones that are given in the paper.
Um, a man inspects the uniform of a figure in some East Asian country.
So remember that was the ca- the caption of an image, uh,
and then the contradiction pair that the worker came up with was,
um, the man is sleeping.
And in a subsequent validation phase,
all five people agreed that that was a contradiction label.
Um, I think these are pretty straightforward.
You can kind of see that con- it looks,
anyway like contradiction is very consistent, er,
and there's more nuance especially around what the neutral categories mean.
I think that's borne out in the data.
Here are cases that I picked again just to reintroduce
this idea that it's not so obvious how we're defining this.
So these aren't actual, er,
examples from the corpus but I think they abide by the standards that were adopted.
So here's the idea.
A boat sank in the Pacific Ocean and a boat
sank in the Atlantic Ocean would be labeled as contradiction.
Of course those two things could happen together.
So there's no logical sense in which these are contradictory.
But if we assume that what we're
doing is talking about a single image, like an, you know,
that from the image Flickr 30K data set,
then it's likely that these contradict each other,
that only one of them could possibly be true and
that's the sense in which this is a contradiction relation.
And here's a more extreme case that Sam came up with.
So Ruth Bader Ginsburg was appointed to
the Supreme Court and I had a sandwich for lunch today.
By this logic, you would say that those were contradictory.
Again, not because they couldn't both be true.
They're, maybe they're both true of you or of me, um,
but rather that they couldn't be describing the same event in some sense.
Does that make sense? This is tricky stuff,
I mean we're more more or less at mercy,
at the mercy of the data that we have but all this is worth keeping in mind.
Okay and then the other data set I wanted to introduce is MultiNLI,
um, released by Williams at al.
In 2018.
It's very similar in terms of its design with some added nuance.
So first, the training set is drawn from a bunch of different genres,
fiction, government reports, the Slate website,
the Switchboard corpus and some travel guides.
And then with- for the dev set,
you have one condition where you test on those same genres.
And another where you,
where you test on some completely different genres and those are the 9/11 report,
um, face-to-face which is
some narratives, fundraising letters, nonfiction, and articles about linguistics.
And that's a really cool idea because you
might think that that's a way again of stress testing
your system of forcing it to go outside of its training experiences.
Again it's very large.
Um, it has the same kind of an, um,
additional annotations for validation.
And then the one twist here is that I think to
avoid kind of a lot of hill climbing on the test set.
Uh, the test set is released only as
a Kaggle competition to kind of
reduce the number of times that people can evaluate on it.
And you can check out the project page.
I think it links to a leaderboard and some other information.
Is like this data set, um,
[NOISE] I was curious if the captions for the images happening maximally informative.
Say you have like a picture of a soccer game happening and
then the premise is like a man kicks the ball.
And then the hypothesis is just like a man is watching the game or something.
So it's bo- they're both happening in the same photo but the photo is multiple events?
That's a great question. So basically like how
comprehensive can we count on the captions being?
I think you can't count on anything.
I think image captions are strange things that reflect what humans regard as salient.
Um, and so they have their own interesting biases.
And there have been some attempts like here at
Stanford with a Visual Genome Project to do
image captioning of a sort that's not so naturalistic to
really comprehensibly label what's in the images.
Um, but that's not what was done for image Flickr 30k.
It's much more naturalistic.
And so you're starting in
an interesting social and linguistic space
and then building on top of it in an unusual direction.
That's the long and short of it.
I've always thought though that, um,
there's an opportunity here which is that you might leverage the actual images.
This has been done infrequently or not at all.
But you can imagine that you solve the problem by reasoning not only about the premise,
the image caption but also about an- a representation of the image itself.
Sorry about the phone doing that.
Here's another really interesting thing about the multiNLI dataset.
So Sam and the group they released with the dataset a bunch of
annotations for properties of the individual examples.
So for example they just labeled some as
being active passive pairs in the premise and hypothesis.
And that's really interesting because you expect that to be
kind of meaning preserving but it's
an open question whether your system can be sensitive to that kind of transformation,
um, or also categories like whether there's a belief
predicate like believe or claim or say.
Whether there's conditionals, whether co-reference is crucial.
Um, because many systems just ignore kind of like the pronouns
but entailment can often hinge on the nature of those co-referential relationships.
Uh, is it a long sentence?
Does it contain modals and negation?
Um, could these two things be called paraphrases?
What's in- are their quantifiers in the sentence?
These are kind of quantifying the, um,
semantic difficulty of the examples.
And this is really nice. You'll see this later when we do error analysis.
You can use these categories for a kind of out,
out of the box error analysis.
You can ask for a bunch of different systems,
which of these categories are they doing well on and where are they failing?
And that's often very illuminating.
Okay. And just to round this out.
Just so you know about it,
this is also in those notebooks.
Here is some code for dealing with these datasets.
Um, you have basically snli_train_reader,
snli_dev_reader, MultiNLITrainReader,
and then you have the two dev readers matched and mismatched.
And so those should be good like easy ways for you to
interact with the data and feed it into machine learning systems.
And I think the only thing you have to get used to is,
they yield what are called example objects.
You can see that happening I guess here.
And that's because the underlying data has not only the strings and
the relationship but also if it was multiply annotated all the
decisions the annotators made and also binary and non-binary
parsers of the examples which are
automatic parses that come from Stanford's CoreNLP package.
Um, but you know they,
they're good parsers by and large and worked well.
And that gives you access to very rich tree structures if you want to make use of them.
And so that's a bit of a glimpse of how you deal with those example objects.
And here's another example.
I know it's small but it's there for you to download and check out if you'd like.
And then here's a bit of additional code that just shows you
the interface that we've provided for dealing with the annotated subsets,
so that there are no obstacles to making use of it.
Because I think it's really exciting to stress test
your system by looking at how it does on different classes of problem,
and they've made that very easy for us.
Excellent. Any questions about those datasets at
any level before we started diving into the modeling parts of this?
We'll kind of embark on our familiar plot then.
So the first thing that we'll do is look at some hand built feature functions and
some kind of classical linear models and then we'll move into deep learning.
What I've done here is defined just two very simple baselines. Well that's nice.
I made that bigger. Um, that you see a lot in the literature.
The first is what I've called word_overlap_phi.
And it's just creating a feature for every pair,
every word that occurs both in the premise and the hypothesis.
So that gives you pretty sparse models.
And they're pretty small because you're really looking at only the overlapping words.
You can contrast that with word_cross_product.
And what that does,
is create a feature for every pair
of words that occurs in the premise and the hypothesis.
So you know the,
the full cross product in the sense if you think about them as two sets of
words you look at every pair that you can draw from that pair of sets.
And that leads to really enormous feature spaces.
Uh, I think that if you fit a model with this feature function on all of SLNI,
the training data, you end up with,
uh, well over a million and a half features.
So very high-dimensional very sparse feature space.
Uh, yeah here's a way of looking at that.
So like if I have Tobi is a dog,
and Tobi is a big dog,
then the word overlap thing just keeps track of Tobi,
dog, is, and a, and it drops out big.
Whereas the word, oh, um,
word cross-product thing creates
this enormous list of all the pairs that I can form from those two,
um, sets of words, with their counts.
So much, much bigger [LAUGHTER].
Those are kinda baseline things.
Here's something else, a whole class of features
that you might think about that could come from WordNet.
So as I said before WordNet is a kind of very high-fidelity handcrafted resource full of
information about word-level relationships, in
particular entailment relationships and
some contradiction information that is in the form of antonyms.
Um, and what I've done here is just give you
a little bit of code that shows you how you might
create feature functions relying on the WordNet hierarchy.
So as a quick example,
if I just look up puppy, uh,
I get all its synsets and they're like dog,
pup and young person.
So you can see that the sense of the word puppy is
kind of ambiguous and WordNet is capturing that.
If you go for just the first of those synsets then you get a kind of
more conservative perspective because the first sense I think is the most frequent,
uh, according to their data.
And so from that,
and actually there's lots of methods you could use,
hypernym, hyponym and so forth.
Here's a little example of how to create a WordNet feature.
And I've done it for hypernyms and hyponyms and I'll just show you an example.
So again if I have the puppy moved and the dog danced,
then hypernym features would give me a feature for puppy and dog.
And hyponym features would give me moved and danced.
Um, and you could do- with this code here,
you could do this for lots of other WordNet relationships.
And I think carve out pretty interesting subspaces,
uh, of like lexical entailment and contradiction and so forth.
And here's some other ideas.
So you can do lots more stuff with WordNet.
You could also think about some edit distance features
between the premise and hypothesis maybe at the word level.
Um, I showed you word overlap but maybe
it's even more natural to think about word differences.
Maybe the much more of the story is told by
what is in the hypothesis but not in the premise or the reverse.
Um, alignment based features that would key into
an older idea from NLI that it's kind of important to figure out how the two,
the premise and hypothesis relate to each other.
Um, negation of course I think it's
worthwhile having separate features that would key into negation as well as
other lexical classes because all of this stuff is
really deeply affecting entailment you know
things like verbs of saying and attitudes and modals and hedges and all that stuff.
A lot of stuff that, that I think is relevant for sentiment
carries over here because of the way it modulates claims that we make.
Quantify our relationships.
Of course you'd want to capture those.
They are not in WordNet but definitely worth maybe even the effort of hand coding them.
And then you could also have named entity features.
That's kind of like the James Dean,
Jimmy Dean, James Byron Dean situation.
And this is not meant to be exhaustive rather just to get you thinking
about different ways that you could key into lexical information,
constructional information and also world knowledge.
All that stuff should be brought in.
Given the way we frame the NLI problem.
Questions about that or comments or ideas?
All right. NLI experiment.
Again, this is the- the pattern that's repeating for all of this code stuff.
It's kind of standard language for thinking
about running experiments in a productionized way.
And I've provided a bunch of code for that.
Let me just walk through it at a high level,
just to show you how easy this could be.
So as usual, you point your repository to the,
um, data, here that's SNLI_Home.
Here, just to illustrate,
I've defined word_ overlap_ phi as my feature function.
As usual, we have these wrappers,
here it's fit_softmax, um,
so that you can as part of the experiment pipeline,
do cross-validation and fiddle with the parameters and do
other things that involves- that are part of model setup.
Slightly different from the Stanford Sentiment Tree-bank,
we have these reader objects.
Uh, so here I've set up train_reader_10,
and the reason for that is they have this extra argument
called sample percentage, samp percentage.
Ah, and here I've set it at 0.1,
I think that's really important for development because if
you leave that off or say samp percentage equals none,
then your experiments are gonna be run on the full 550K examples,
and your progress will slow to a crawl because every experiment might take hours or days.
So what I would encourage as at least one way that you would develop on this dataset,
is to do it on small subsets which are randomly selected,
and then once you feel like your system has stabilized,
expand out to a larger,
um, chunk of the problem.
So you set up that train reader and then it's gonna look a lot like the SST, right?
So you have the train reader argument,
your feature function word_overlap_phi and fit_softmax,
it's like a Swiss Army knife you have a lot of other options,
but those can be left off.
And that's pretty much all it takes to run an experiment.
And as before, basic experiment,
there is a dictionary that more or less stores
all the information about your experiment for
reproducibility and error analysis, and so forth.
Does that make sense? That's the basic framework,
very familiar, I think.
Let me show you two other things that you- that I would encourage.
So first, hyperparameter selection on train subsets.
So again, so unlike SST where it's small enough
that you can kind of do a lot of experiments in a pretty manageable way,
when you move to the level of SNLI,
every experiment is a major investment, and again,
you just won't make that much progress if you're not thinking about
how to do smaller stuff that will inform the full picture.
So I'm gonna show you or give you- suggest to you two ways
that you might do that when it comes to hyperparameter selection.
So first, let me just set this out.
This will be on train subset,
so point your data,
you know, SNLI_Home, as usual,
word_overlap_phi will be my feature function, fit_softmax_with_crossvalidation,
that's gonna look at a lot of different values of the regularization strength C,
and both penalty types l1 and l2.
So that's a pretty big grid of hyperparameters,
you have to fit a lot of models,
especially with cv at 3 there.
So that's gonna have to fit a lot of models.
And if again, if you do it on the full dataset,
it will just take too long.
So one approach you might take is to set
those hyperparameters based on a small subset of the data,
here I've done it at 10%,
and you can see they get printed there.
So this model chose C of 1 and penalty of l2.
And then having made that selection,
you could play it forward, right?
So here- now, same feature function,
but now it's fit_softmax classifier with pre-selected parameters,
where I just hard-code in the values that I selected from my earlier experiment.
And then you go forward.
And in that way, you got some of the benefits of
exploring the hyperparameter space without all the costs.
[NOISE] Makes sense?
Any questions or comments or concerns you wanna bring out?
The random sampling method,
[NOISE] is it like- just like assuming there's subclasses of questions like say
you need like, entailment, contradiction neutral, [inaudible] [NOISE] full number
each of the matching distributions based on the datasets or it's truly random?
[NOISE] It's truly random but because the dataset is almost exactly balanced,
you get balanced subsets out.
But I think you're raising an interesting weakness of the approach I just sketched,
which is, that 10%,
how do I know how representative it is?
It- is it of the true full dataset?
And in particular, the feature space that you form on
that tiny subset might be very
different from the one that you get from the full training set.
And for example, that might deeply affect how much regularization you wanna do,
um, and I think that's a weakness of the current approach. That makes sense?
So that's- that's just- just to be- like that's a compromise of- of that one.
Here's another method that you might use that has not that weakness but maybe
others and this would be hyperparameter selection with a few iterations.
So all of these machine-learning models,
they go through a certain number of epochs of training.
Uh, and, you know,
a single epoch might be manageable,
but if it takes 1000 for your model to converge,
that's where you're incurring the real costs.
So another approach you might take would be to,
as I've done here, fit_softmax_with_crossvalidation_small_iter,
where I just find the parameter for the model I'm fitting,
there it's max_iter there for logistic regression,
and set it to a very small value,
like 3 or 5.
And then I do the full grid search that I wanted to do before,
it's just that that takes much less time even if I do it over the whole dataset,
as I'm doing down here because the-the iteration number is so low.
And again, you get a report down here.
Here it shows l1 as the regularization parameter, not l2,
and the performance is slightly better,
and then you apply that forward in the way that I showed you before.
So the implicit assumption you're making there is that
the future for training epochs is gonna be like the past,
that a setting that was really bad early on
is not gonna somehow emerge as the hero of this story later.
Once bad, always bad.
And conversely, the ones that are good right out of the gate,
are probably the ones that will be good even if you run the model to convergence.
I'm not offering you a guarantee because I think no such guarantee could be offered,
but rather that this might be
a good compromise given a budget that you have on training time,
or on compute resources, and so forth.
And you could mix and match these because both of these things,
in the grand scheme of things,
are very, very quick compared to fitting a model on the entire dataset.
One more thing here and then we might wrap up.
I just wanted to show you how easy it is in the framework tests to,
um, uh, set up and run a hypothesis-only experiment.
Um, and I think this is pretty obvious.
There's the feature function,
hypothesis_only_unigrams_phi takes in two trees,
t1 and t2, but then it operates only on t2.
It just throws away t1.
Uh, I used fits_softmax_classifier_with_preselected_params,
I ran the experiment, and lo and behold,
65.3 for the macro f1, right?
That is very different from a random baseline which would be like 33, 34%.
That shows you how powerful this can be.
So if your model is at 68 and you feel like,
oh that's wonderful compared to random,
well you should contextualize it in this way [LAUGHTER].
Um, I hope that's not dispiriting [LAUGHTER].
And I- i- i- it's worth checking to see how much information is in the premise, actually.
Uh, I'm gonna do that as a follow-up.
Well, yeah, there it is compared to random.
And actually, here I just wanted to show you,
just in case you didn't know this,
that scikit has all these great dummy classifier models,
that allow you to fit models that just make use of
the like label distribution or just guess at random.
Ah, and so here I just did the diligent thing of actually using one of those,
and sure enough, yeah, 33.4%.
So the hypothesis-only baseline is a very strong baseline. Questions about that?
[inaudible] baseline, should we, um, uh,
should we make the baseline the- doing
all the exact same operations as our model but only on the second tree,
or should it be just like the counter?
Well, that's a great question. So like what is
the- given the model that you're developing,
what is the true nature of the hypothesis-only baseline?
Um, that's a judgment call.
Um, because, for example,
if I picked the word cross-product features as my basic feature function.
That's undefined If I'm looking only at the hypothesis,
and so I'd have to say like,
well, the unigrams are pretty close to being like my true feature function.
Um, but it's - it's a tough thing to say if you've- especially if you do- if
you define a lot of features that make use of both the premise and the hypothesis.
But if it's handled features,
I might just default to unigrams.
It seems pretty plausible.
And then, for the deep learning models,
I think these questions are typically more straightforward.
Good. I think rather than diving into those,
um, let me leave this for next time.
So next time, we're- we're gonna look at sentencing coding models,
where you get a separate representation for premise and hypothesis,
and then we'll look at chain ones which more or less just put the two
together into a single, typically an RNN of some kind.
And then we'll look at attention mechanisms and do some error analysis.
And if we get through that, as usual,
leaves- we'll leave some time for you to just code on
Homework 4 with the teaching team available for you. Great.
 All right. I propose we get started.
Plan for today is to finish up NLI.
I was just talking with the group that was here before,
and I think the consensus is that if we finish that early,
I'm gonna give you a glimpse of the topics for next week, um,
because they're basically my favorite topics,
and I wanna, like, get people to do projects on them.
And I know that we're introducing that content kind of late, um,
in terms of people settling on project topics,
but maybe this is my last chance to sneak some things in.
I've got lots of good links to datasets and papers and so forth.
That would be about- that'd get us pretty close to the true dream of NLU,
I think, which is like, you know,
language use in grounded situations in the real world.
Um, and it just comes late in the course because we have to build
up a lot of material to get to the relevant models.
Um, but now, certainly by the end of the day today,
you're well-prepared to get your hands dirty
with the kinds of things that fall under grounded language understanding.
So I might say a few things about that if there's time.
And just by the way, it's gonna get a little bit of a rush next week.
We have just one day for grounded language understanding,
and one day for semantic parsing.
The saving grace there is that we have a huge amount of material for semantic parsing.
It's a challenging topic, it's very interdisciplinary,
you basically have to know a bunch of
linguistic semantics and a bunch of machine learning.
Um, but if that sounds exciting to you,
then it's a wonderful area to work in,
uh, and we have this whole Codebase sippy cup,
and Bill has created lots of notebooks for it.
And then we have two supporting screencasts that kind
of get to the essence of the technical ideas,
and then a bunch of wonderful papers.
So if you're still looking around for a project idea,
and especially if you took semantics with me before,
then this is like the- the dream- the marriage of Linguist 130A and this course,
and so I encourage you to check it out.
And Bill will give us an introduction to it,
but there's no- only so much you can do in an hour.
So it will really pay to do that kind of self-study.
Um, just while I'm talking about the schedule,
so you're- you're doing the last homework and the last bake-off.
Uh, and then after that,
we kind of settle into a rhythm of project-oriented assignments.
You do a lit review, and at that point,
you'll be paired with a mentor from
the project team who will be with you through the whole process,
so like somebody you can count on for
advice and who will be giving you a lot of feedback.
So we do the lit review first,
and then we have this thing called an experimental protocol.
And I'll tell you much more about what that's supposed to contain closer to the time,
but kind of hovering in the background here is that we're gonna push you to
make sure that your project has some- some kind of
quantitative evaluation associated with it.
Not because we think that that has to be the way all work in NLU works,
but rather just because we think it's healthy to kind of push you in that direction.
And that's why you'll find that when doi- when you interact
with people from the teaching team about your project,
they wanna know what data you're gonna use.
Because I think that we all on the teaching team
think if we can figure out what data you're gonna use,
then we can kind of figure out what your metrics will be,
and get you to the point of a quantitative evaluation.
So have that in mind as well as you do project planning and know
that the project assignments are gonna be pushing you in that direction.
And then after that, it's just a short YouTube video,
those are typically fun,
uh, and then the final paper itself.
And, uh, correspondingly, most of the material that we do in class in
the second half of the course is kind of oriented toward you doing your project.
So it's kind of, it gets a little bit meta.
We talk about methods and metrics,
and presenting your work, and so forth.
So it should be relevant to you no matter what
kind of project you're working on, that's the rationale.
There is one thing that I wanted to just clarify about Bake-off 4,
just so that you know what the points of freedom are.
So remember Homework 4 and Bake-off 4 are all about word-level entailment.
It's a kind of small version of the NLI problem,
and I've supplied you with datasets that you can work with.
And anything goes for this in terms of you bringing in outside vectors,
for example, you could do it with ELMo or BERT,
um, or vectors you build yourself,
GloVe, right, you name it.
You can go onto the web and in fact the- the task is pushing
you in the direction of having really rich inputs to these models,
otherwise, you're just not gonna do very well.
So from the perspective of modeling and
outside data that will initialize your- your vectors,
we wanna be, you know, casting the net wide.
There is one restriction that I feel like I didn't emphasize enough last time,
though, which is that when you get down to it,
when you develop your original system,
and when you enter the bake-off,
you have to be working only with the word disjoint problem, right?
It says that here: Keep in mind that for the bake-off evaluation,
the edge disjoint portions of the data are off limits.
You can train on any combination of word disjoint train in dev,
you just can't look at that edge-disjoint part,
and the reason for that is that this problem,
your performance just goes way up if you're allowed to look at edge-disjoint,
and the reason for that is that that means that in training,
you get a lot of words that you see at test time,
which is kind of contrary to the direction that I'm trying to push us,
which is, these are all new words at test time.
And so what we're doing is kind of stress-testing your system on its ability to
generalize from what it knows about its training words into a new part of the vocabulary.
Um, so yeah, this is really important,
and you will definitely not be allowed to win the
bake-off if we see that you've brought in the edge-disjoint stuff.
[LAUGHTER]
[inaudible] .
That'll be okay. You know, I'm actually,
so I'm assuming that whatever embedding space you have when it
gets to test-time those words are in that embedding space, right?
Because that's kind of the point,
Because it's like your embedding space has interesting structure,
and then your model helps you generalize into the entailment problem,
so it'll be- your system is gonna do really poorly,
in fact, if none of the test words are in your embedding.
Um, I've tried to disguise where I got the entailment data,
and the spirit of that is just that,
yo- you know, you guys can search widely and stuff as long as you
don't make a deliberate effort to figure out where my data come from.
That would be contrary to the spirit of all of this.
Um, but definitely go out and see, you know,
whether there are new word embeddings that people have
released that are especially good for this task.
Okay. Any questions about that before we dive back into the content?
Okay. Just as a kind of recap,
so last time I introduced the problem of NLI,
and I think the summary of that discussion is kind of like,
NLI at this point is a common sense reasoning task of some kind.
Um, you know, it's kind of hard to define exactly what
the criteria are for entailment and contradiction and neutral.
Uh, but we have these naturalistic datasets that reflect
some aspects of how humans actually reason about language,
and that's what we hope our systems do as well,
because the whole idea of NLI is that
it's kind of keyed into fundamental information about how we use language
to build arguments and to reason and to find out new information,
and if you think in an extended sense,
even how to answer a question and so forth.
Um, I introduced SNLI and MultiNLI, which are exciting,
large datasets, um, that can- that will test disparate- different aspects of your system.
So SNLI is just image captions,
and MultiNLI is drawn from a bunch of different genres.
And a nice twist about MultiNLI is that,
in addition to the diversity of the training data,
there are two dev sets,
one which is matched,
which means that you evaluate on the same genres that you trained on, and one mismatched,
which is kind of seeing- it gives you a chance to see how well
your system can generalize out of its comfort zone, so to speak.
Um, and another exciting thing about MultiNLI is that the team released it
with a bunch of annotations that you can use for kind of error analysis,
and I'm gonna give you a bunch of illustrations of that later,
I think it's a nice opportunity.
You know, writing a good error analysis section for your paper
is often very difficult and requires a lot of creativity,
and I feel like they've given you this gift of the structure
for a very rich error analysis by providing these annotations,
so I'll show you a bit about how that works.
And then we got as far as talking about hand-built features,
um, which are kind of, like,
word overlap, word cross-product.
And what you're seeing here is a bunch of stuff that you can derive from WordNet.
The one- and then here, a bunch of other ideas.
I added one slide here,
which is, um, we have this narrative of, like,
talking about hand-built features and linear models,
and then talking about deep learning models,
and you could fall into a trap of thinking these ideas are in direct competition.
Um, I think that's not so,
I think it could be very interesting,
and maybe some of you have done this in your work so far,
to think about combining,
like, a word embedding,
something short and dense, with a
feature representation that comes from hand-built features,
and that will have the characteristic that we- it will have
very high dimensionality and be very sparse.
And that really contrasts with what you would get from BERT or ELMo or GloVe or whatever,
which tend to be very sh- short,
low dimensional and very dense,
I've kind of represented that here,
short and dense, long and sparse.
And the question arises then, like,
what's an effective way to combine these two representations in a unified model, right?
Because it's not like I have to choose just long and sparse or short and dense,
maybe I can have the advantages of both.
And the naive thing to do would be just- to just concatenate these two representations,
um, so that if, like,
the green has 20,000 dimensions,
which is pretty realistic,
and the short and dense one has 50,
then you have 20,050 dimensions,
but even that way of posing it suggests that there might be an issue here, right?
Which is that there's a kind of wild imbalance,
and it's two kinds of imbalance.
On the one hand, you have all this sparsity dominating the representation.
On the other hand, it's kind of in two phases,
like, with very different kinds of information.
So I think it might not be effective to simply concatenate the two representations.
One thing that I've found effective in various work that I've done recently is to do
a little bit of preprocessing on
the long and sparse side to just make it short and dense.
Uh, and there are kind of two classes of approach you could take there.
So you could just apply something that is
outside of the model you're developing, like, you know,
apply LSA or PSA or something like that,
to shrink it down into a dense representation.
Uh, that's one step.
That's what I've called this, Model External Transformation.
Another approach would be to actually have
your model learn some weights that perform that transformation,
that embed this really long thing into a smaller space.
That will reintroduce some difficulties of optimization,
but I think if you give it separate parameters,
then it will become kind of manageable.
Uh, and then at that point,
you would concatenate short and dense and short and dense here, and you'd have,
kind of two comparable objects and then move forth.
And I- I think that there's good evidence that that representation
would have a lot of the advantages of both sides of the incoming information.
Yeah.
[inaudible] combine the two?
I don't know about that.
Attention would normally be defined over entire representations,
not over the individual dimensions,
so you'd have to think about it somewhat create- creatively.
Does that ring any bells for you over on the teaching team?
[inaudible] of success.
Okay, and we'll sa-
If you perform this transformation,
then this thing, once concatenated,
if you have subsequent weights up here,
then you are giving your model a chance to learn lots
of interactions between those two sides,
and I think that's where this becomes potentially so powerful.
Okay. And then I introduced nli.experiment, um,
which is not required for the homework but if you want to do
a project in NLI, I think this could give you a real head-start,
and it kinda makes all these ideas concrete.
And I introduced these two ways of doing hyperparameter selection in the context of
a really large dataset where you can't afford to sit
around through 50 iterations because,
you know, that could consume all your time or resources.
And then toward the end here,
I introduced this idea of a hypothesis only baseline.
Um, and the relevance of that is just that,
this can be a surprisingly good baseline for NLI.
Uh, and I gave you a rationale that was a kind of linguistic scientific rationale for
why this works related to the fact that there are some intrinsic biases,
um, when it comes to entailment problems about
what's likely to appear on the left or the right,
uh, and that wasn't my kind of justification
for why hypothesis only might always be pretty strong.
And you can see here at the bottom that it is indeed quite a strong baseline, 65%,
uh, and compare it to random 33%, you know,
it looks like you really want to think about
the strength of the hypothesis only baseline.
And then asked, what about premise only experiments?
And I had not thought enough about that.
And so in the moment I think I said something like, well,
based on what I said they ought to be pretty good as well.
Because the same kind of biases that are telling you,
for example, that if I give you a general word it's likely to be a hypothesis.
Conversely, if I give you
a very specific word it's likely to be a premise in an entailment pair,
and also very specific things are likely to be participating in
contradictions in the sense of this problem
because they're likely to exclude other event descriptions.
And all of that leads and push me in this direction.
All of that leads to think that a premise only baseline ought to be pretty good.
So I ran this experiment on SNLI and I was like aghast,
so 33.3 which is the very definition of random for this problem.
And at first I thought there must be some kind of bug in the code
or something because there's just no way it could be this random.
And then, then I and I couldn't find
any bugs until I started to do some real soul-searching,
like did I give you guys a misleading assessment of this problem?
Um, and so I was in a kind of a bad state for awhile I guess.
And then I figured it out,
does anybody actually see already what I was missing in running this on SNLI,
it's a subtle fact.
It took me awhile to realize it.
[inaudible].
[inaudible] That's true and I am going to bring in
the local task but I don't think that's the answer.
Why is it random for SNLI?
It's because of the way the dataset was constructed.
So every premise was given to- to crowd workers and
exactly one of entailment contradiction and
neutral was constructed on the basis of that premise.
So those constructed sentences differ wildly of
course and they do contain biasing information about the label.
But if I give you only the premise,
this was like what the crowd workers got before they started their work.
And so what you see is exactly what you get from
the data collection which is perfect randomness,
because all the system has is this unbiased information.
So phew, and then I did just to reassure myself check what the- what it is for
the word entailment model because your homework asks you to do
hypothesis only and that's about 0.5.
Premise only is about 0.47.
So those are pretty strong baselines.
Again, it's the same lesson.
But here, I think my- my insights are at least consistent with those results.
So all is well with the world.
Like I said, it's the bottom line here [LAUGHTER] as far as I know.
Questions about that or additional concerns?
It was- it was because kind of by construction in any of the premises could have
led to any kind of results like
you could have constructed something that was entailed or a contradiction or neutral.
In fact, even more by design.
It's like, uh, literally
the crowd worker was given the premise and constructed three sentences;
one entailment, one contradiction, one neutral.
[LAUGHTER] So it's not just by chance,
this was really by design and I should have known this.
Um, I did eventually oh, phew.
Okay. So that's a kind of recap with a little bit of new information.
And then the, the final things I wanted to do wa- was move into the neural realm,
because I think there's exciting models that are kind of specific to NLI,
um, that we can discuss of it.
Yeah. Just, just a quick question previously.
So what about the logistic regression model would
differentiate it from this model and let it have a higher baseline than randomness?
Oh, well, I just picked logistic regression for the,
the experiment down here just to have
some model to implement premise and hypothesis only baselines.
And the observation is just that both of them have a lot
of information that makes them much better than random.
It's a different dataset. It's not- it's not a model. It's the dataset.
Oh, it's a different dataset. Okay.
Yeah, because, so this one was constructed to have
no bias when it comes to the premise but the word entailment one was not.
It's just a bunch of words drawn from some vocabulary.
Okay. So there are kinda two classes of models here
that and th-this is a by-product of us having a premise and a hypothesis.
The first-class or what I'm going to call sentence encoding models,
and in these models you get
a separate summary representation of
the premise and the hypothesis and then you do something with it.
Um, I think this is the kind of model that
you're pushed toward if you have in mind that Ido Dagan
dream that you're learning representations from
NLI data that are going to transfer to lots of other tasks,
because for many other tasks,
you need an individual sentence representation.
Uh, and this offers exactly that.
And I've started here with a natural baseline for this kind of model,
which is just that kind of sum of
word vectors approach that you've explored in various places before.
So just to summarize this,
I've got every dog danced as my premise,
every poodle moved as the hypothesis.
I do an embedding lookup to get this layer, right?
That would come from some pretrained space presumably.
And then to get a fixed dimensional representation for each side here,
because the word- the number of words could differ,
you just do something that's a kind of hard summary of all those vectors,
like the sum, or the average, or the difference,
or whatever, right, but sum operation that's gonna take you from
this collection of vectors into a single fixed dimensional one,
and that would give you xp and xh.
And then after that,
again, this is familiar,
you might just concatenate them or do something else with
them to get to the input to your classifier.
And then from there,
this could be any classifier you want.
So a simple linear one or a whole deep learning model, whatever you choose.
And this is a baseline in the sense that,
this is a pretty primitive way to combine
all those words into a fixed dimensional representation,
um, but actually these models turn out often to be pretty good.
A lot of information from the words ends up encoded in those sums that
you get for the premise and the hypothesis. Make sense?
And here you can see that like, you know,
one simple approach you could take to the bake-off would be, you know,
I just have a one word premise and
a one-word hypothesis so there's nothing to sum or average.
I get xp and xh directly,
concatenate them, and then fit a model.
Make sense? Questions about it?
Here's some code for doing that, um,
and it's familiar code in the sense that like go- glove_leaves_phi,
glove_leaves_phi, some phi.
These are functions that you've actually worked with before.
And the only twist here is just that as you process examples,
you need to process two parts,
the premise and the hypothesis,
whereas before when you've done this,
you are processing like just a
Stanford Sentiment Treebank Representation.
But everything else is the same.
Um, and it does okay.
Uh, on SNLI here training- trained dev 51.6%,
so it's actually below the hypothesis only
baseline which might make this a kind of nonstarter,
but it could still give you intuitively some information about like, uh,
a model that you build on top of this,
how much you're adding if you do something more sophisticated
than the sum or the average the way this model does.
Here's what I take to be the rationale.
I kind of alluded to this before.
So first, encoding the premise and hypothesis separately
might give the model a chance to find rich abstract relationships between them.
You might think that that's a kind of very human way of doing this.
Like I have an understanding of the premise,
an understanding of the hypothesis,
and then there's a subsequent step of figuring out what their relationship is,
and that's more or less directly encoded in this class of model.
Um, and in addition,
this is the Ido Dagan dream.
Sentence-level encoding could facilitate transfer to other tasks. So yeah.
And so if you wanted to do like the- the GLUE benchmark, for example,
that it's really natural to choose a sentence-encoding model,
so that you could use the premise or hypothesis in one of
the sentiment tasks or in one of the grammaticality judgment tasks and stuff.
Here's a model that's received a lot of attention in the literature.
That is a kind of step up in complexity from
that simple averaging or summing of vectors,
and this is where I kind of have two RNNs.
So one for the premise,
and one for the hypothesis.
So I've used the same example,
every dog danced, every poodle moved.
I looked them up in what I presumed is a single embedding space,
although you could have
different embedding spaces for premise and hypothesis if you wanted to,
you know, on the assumption that
word sentences are different depending on where they appear.
And then what I am assuming here is that you would have
different parameters for this recurrent neural network.
So I put that in green and in purple here just to signal that at that level you
might want to learn something very different
depending on what the environment for the content is.
But that's not forced either.
You could have tied parameters.
It could be a single RNN.
I think the point here is that the way this model subsequently works is that I get
this final representation h_3 and this one over here h_P,
and those get combined somehow.
Maybe concatenation, that's a likely choice,
and those get fed into the classifier.
And so what you're really depending on is that like the final state in this RNN,
is a good summary representation of that example,
and similarly for the hypothesis.
And then they get concatenated,
and you make a classification decision.
So from the top here,
once you get to combo,
this is exactly like that first baseline that I showed you.
You should really think of this as like instead of having something
primitive like sum or average to combine my word vectors,
I have this really complicated function,
an RNN, and it has parameters that I learn as part of the task.
Instead of imposing this ideal from on high,
that sum is the right choice so that mean is the right choice.
For all you know,
your network is going to discover that the sum was exactly the right function to learn.
It's unlikely but conceptually,
I think it's useful to think about that possibility.
And of course, this has many variants.
So like even our code base will let you explore a variant,
where this is a bidirectional RNN.
And in that case, you might be using a summary representation derived
like sort of at the h_1 point combined with the one from h_3.
Those might get concatenated,
and that's your representation of the premise and same for the hypothesis.
But you would feed all of that into this combo func here and fit
a classifier. Make sense?
Questions? It would be
wonderful to see you guys exploring different variants of these.
I've only provided one.
Um, and here's a kind of strategy for- actually,
I did implement this full one in the notebook.
You can see that linked up here.
But I thought for here, instead of just looking at the code,
which you can do on your own,
it would be more useful to talk about
the conceptual strategy that I took for implementing this.
So sentencing-encoding RNN.
How would I implement this model here using PyTorch?
So here's the way I thought about it.
First, it's- it's nice and clean,
gives you good code if you define a dataset class.
The code that's in the repository by and large for these models,
it deals with just a single example.
So a single sequence of words,
a single length, and a single label.
That's like what you would do if you just use the RNN class.
It would force you into that mold of having a single list. We need two.
So for the dataset,
you could subclass the existing one and just have it
yield batches of objects that are pairs,
like every dog danced as tokens,
every poodle moved as tokens,
their individual lengths and then finally the label.
So that's kind of like bookkeeping,
and that's just imposed on you by the structure of the model,
that your examples need to look like that.
Then for the classifier model itself,
the subclass of NN module in PyTorch.
Um, you need it to conceptually be a premise RNN and a hypothesis RNN.
So what you do is implement its forward method to
use those two RNNs, and they get, you know,
the premise and hypothesis get processed in
exactly the same way as the current RNN code is processing regular examples.
And it gives you the two final state representations.
And then you just need to by hand as part of
the forward method concatenate them and return them,
and those will get fed into the classifier.
And then finally, the- the way the code is written,
you don't really need to do anything to the, uh,
base class for RNNs when you define this- this special one.
Uh, it just needs a slightly different predict problem method,
and the reason it needs a different predict method is that it
needs to deal with examples like this, right?
That's- that's kind of just back to the fact that your data are a certain way.
Uh, but everything else about the logic of optimizing the model is the same.
Really the changes are localized to datal- data handling and
to the structure of this core like the- what you think of as the computation graph.
And I do love PyTorch because it makes all this so easy and with- actually,
if you look at the notebook, you'll see that in relatively little code,
you get one of these sentence-encoding models.
And then because you're inheriting from a simpler class,
all the options like whether it's bidirectional,
and what the hidden activation function is,
and the dimensionalities and so forth,
that's kind of all just taken care of.
Make sense? Now I encourage you to- to look at the code, and you'll see.
You know, it's like 50 lines or something.
But most of it is just bookkeeping.
Final entrant into this class of sentence-encoding models would be,
again just a small variant of the one that we just saw.
Instead of having to RNNs,
you have two TreeRNNs, right?
Recursive tre- you know,
tree structured neural networks.
Um, you know and so that if you look at just the premise or just the hypothesis,
this is exactly the model that we used or that I showed you for the SST.
Uh, and all the same variance holds.
So if you wanted like more com-
more complex combination functions than the one that I've signaled here,
um, you could introduce those.
It would all be kind of nicely modularized
because you would just be dealing with a TreeRNN,
a tree- a recursive, uh, neural network.
You would define it for the premise and the hypothesis,
get their final states,
concatenate them, and fit the classifier.
So from this point on,
it looks like exactly all those others- other models.
And again I think this would be pretty straightforward to implement given
the TreeRNN PyTorch implementation that comes with the course repo.
[NOISE]
All right. And that's it for the sentence-encoding models.
I hope that gives you a sense for the ways that you
could impose new innovations and so forth.
The second class that I wanted to introduce I've called chained models.
And the reason I've called them chained is that basically you just
run together the premise and hypothesis.
The simplest version of that,
is the one that I've given here where I have every dog dance to every poodle moved,
you could insert your own boundary symbol if you wanted,
if you thought it was important for the model to learn that transition point.
It can be like a unique token that you learn a unique embedding for.
Um, but in principle you can just run them together like this,
and then have, you know,
the same embedding space.
That's all these gray cells,
and then the same RNN parameters process
the entire thing premise and hypothesis together,
and then finally on the basis of the final state here make a classifier prediction.
And it could be bidirectional in which case it would be
using h_6 and h_1 as the basis for the classifier,
you know all the same variants apply here.
I mean, this is literally just using the RNN that you might have used
already where I just feed in the blurred together premise and hypothesis and process it.
All right. This is the- I guess the rationale for this would be like
the premise really and truly establishes the context for the hypothesis,
because it has- it says,
"Though a human reader has gone through and just read them,
and then made a decision about the blurred together example pair."
And maybe that yeah, as I say here,
maybe that corresponds to some real processing metaphor or something.
The- then this is the throwaway here like,
you don't have to really do anything except def- define a new feature function,
which just concatenates the leaves of the two trees,
and then everything else is just code that's already in the course repo,
and you can run your experiments in the usual way.
And here's a simple variant where I
decide that I'm going to run together the premise and hypothesis.
But now I'm gonna give them sle- separate parameters.
So I have like an RNN for the premise here,
and an RNN for the hypothesis, green and purple.
And the linking point is just that the initial hidden state
of the hypothesis RNN is the final state of the premise RNN.
Bu- but, you know,
but different parameters for the two.
And this is kind of intermediate between
the sentence encoding thing which was keeping them completely separate,
and the blurred together one that I just showed you.
Because they do get blurred together but with
different parameters reflecting different parts of the example.
Yeah, and then obviously this has lots of variants as well, ah,
where I could fiddle around with different views on each of
these RNNs bidirectional different,
um, hidden activation functions,
different cell structure and so forth.
To state, I'm just curious based on what I've presented,
do people have intuitions about which model is likely to be better?
I think there is a kind of clear winner if I've read the leaderboards
correctly in terms of just raw performance of the NLI problem.
Do people have intuitions about which one to bet on? Yeah.
I've bet on this learner.
Oh, you would why is that?
Um, well it seems to capture
the entire premise and hypothesis, or pre- premise and conclusion,
and also have a delineation between both of them.
I would imagine that maybe,
it's more expressive than having just this concatenation.
So maybe I forgot,
but I think we presented three different options.
Oh, so your bet is this one, the first,
what about this one?
Yeah, this I think will be weaker because you don't have a separation.
Weaker than the sentence encoding ones like,
and this would be a minimal pair?
Um, I think it would- I don't know.
I'm not really sure how this combination function works,
like with the concatenation functions.
It would likely just be concatenation?
Yes, so-
Because the- probably better than not ones.
Because- but all those models have that in common, right?
Because the idea is, I get these two representations for premise and hypothesis,
and I want both of them to contribute to the classifier decision.
So concatenation is a good choice because then it will
give the- the weights that I learned above that for the classifier,
it will have access to all the different dimensions from both premise and hypothesis.
So I think that part is shared, combo, right?
Like this certainly has it.
I guess this doesn't have it,
but if it was bidirectional,
then you'd probably concatenate h_6 and h_1 having run in the reverse direction.
So but what was your ranking?
It was one?
Well, you start from the first one, I'll help you guys. [LAUGHTER]
Ah, you know. Anybo- anybody else have bets? Yeah.
Ah, I'm gonna bet on this, the tree RNN.
Oh, this one here? Yes. So if we stuck to RNNs,
this would be the best one, wh- why was that?
Ah, RNNs tend to lose some information over time.
Yes.
So unless we're using an upgraded RNN, like an LSTM or a GRU,
I think that having two separate ones and
then combining their outputs is likely to perform better.
Oh, that's interesting.
Just in keeping it shorter here,
and getting this representation here kind of saving it for the classifier.
I might be doing better, and I- I'd say like [NOISE] that might
hold even if you've got a sophisticated cell like an LSTM,
just because these things inevitably get kind of attenuated.
That's an interesting insight.
Yeah, and actually- maybe I'll have to really look at these leaderboards here because,
the way you framed that,
that insight attention which I'm gonna show you next is highly relevant to this.
Because to the extent that I have mechanisms that help me remember distant information,
the calculus might change.
Um, but still I like that, that intuition. Yeah.
I can use intuition where a simple RNN is best.
Because if you don't have a lot of data,
then maybe you're not gonna have enough data to be able to
differentiate the difference between your hypothesis and everything,
so to just have a good language model frankly,
seems like you'd wanna take advantage of both,
versus if you had a ton of data,
I can see how having two different ones would give a little bit more readiness.
Oh, that's another great insight.
Yeah. Yeah, just if you think about data versus the number of parameters that you need to learn,
[NOISE] then small data might do best with this one
which is the kind of most minimal model that I presented.
Yeah. Correct me if I'm wrong.
But I think that these models are the best-performing models.
Um, especially if you allow that you can have attention,
and that sentence encoding models are chosen by and large,
by people who are doing multitask learning,
so they want a representation of the individual sentences so that they can transfer to
other tasks or representations from other tasks can easily transfer into this one.
But it's like, if you just wanna solve NLI,
you pick a model that's like this,
and you add attention mechanisms which I'll show you next and that's
like a very powerful approach. Yeah.
When you say this comprise the best,
do you mean on the bake-off data or on the original dataset?
I meant like SNLI.
Actually let me, um-
Because I guess the way they created the hypothesis was by reading the premise,
and then saying, "Mmh, I'm going to now write an
entailment," that model might model exactly that process.
I had in mind in particular this leaderboard. So let's look.
So this is for SNLI, feature-based models,
that's like handcrafted stuff. Those are up here.
Sentence vector-based models that's like the sentence encoding ones.
The top score there is currently 87.4 on the test set,
and then down here I think these- before we get to the ensembles,
I think these are by and large, so that's a tree one.
But I think a lot of these are the chained,
but separate premise and hypothesis LSTMs,
and these numbers tend to be a bit higher, right?
Even setting aside the ensembles.
Well, it's less obvious just at a glance exactly what these models are doing.
But I'm pretty sure I'm right.
[NOISE] I'm pretty sure I'm right for
these big well-used tasks chained separate RNNs are the best-performing with attention.
So let me show you the attention.
That's the final stage here.
So here are the guiding ideas for attention mechanisms.
We need more connections between the premise and hypothesis.
Both classes of model that I showed you are kind of limited in what they allow.
So the sentence encoding ones just use
the final two representations and concatenate them,
and all that other information is kinda,
it has to be summarized by the two that I used for the classification decision.
Otherwise, it's lost.
Here's a more informal statement of this,
in processing the hypothesis,
your model needs some reminders of what's in the premise.
And that final hidden representation isn't enough to get those kind of reminders.
And then another guiding insight here might be that it's
useful to have a kind of soft alignment between the premise and hypothesis.
Alignment is an early insight from approaches to NLI,
where people would use algorithms to construct kind of
high-fidelity alignments between premise and
hypothesis and use that as the basis for making,
uh, decisions about entailment.
And you can imagine that attention mechanisms
especially the word-by-word ones that I show you are
kind of a soft way to explore that kind of alignment by
establishing some connections and learning weights on those connections.
But again, this is all in the spirit of kind of making
more dense interconnections between the two parts of your examples.
I'm going to start with what I think is the simplest version of attention.
You'll see that it's already kind of involved.
But I think I can convey to you the intuitions about how this works.
So again, we have our familiar example,
every dog dance, some poodle danced.
And this'll be the target state here, h_c.
So I use h_c to get a score vector that combines h_c with h_1,
h_2, and h_3, you can see them here.
And this is the simplest variant in the sense that all I'm doing is forming the dot-product.
It's kind of like un-normalized cosine similarity.
And that gives me a vector of scores, alpha here.
Those, uh, scores get normalized by
a softmax function into what are properly called the attention weights.
And so that's another vector of length 3 in our example here,
giving a weight, a kind of weight on the, in this case,
the association or the similarity between h_c and each one of these elements.
Then you get what's called a context vector.
And a simple version of that would be just that I take each one of these,
and multiply it by its attention weight down here,
and then take their average.
So this is a vector that has the same dimensionality as each one of these hidden states.
It's just been weighted by attention.
The attention weight is measuring similarity with this final state,
which is so important for these problems.
Then you do some kind of attention combination function.
So here I've concatenated K,
this representation with the final representation here and fed it
through a kind of dense layer and apply to an activation function.
And that gives you what I've called the attention combination representation.
Here's another variant where I have separate parameters for the context vector,
and for this vector here and just sum them,
very similar I think.
And then finally, you fit a classifier on top of
this attention combination in the usual way and make your classification decision.
So it looks like a lot,
um, but, you know, intuitively,
what's happening is in this basic form here,
I'm getting a measure of similarity with the final vector.
That gets averaged into this context vector,
and then that context vector,
and the usual thing that we use for prediction is
fed through into this classifier to make a final decision.
So it's like the usual thing and then K would be what I called the
reminders, an attention weighted view of what happened previously in the premise,
that's been like fast-tracked up as an input to the classifier.
Here's a specific example.
Just in case you're the sort of person who likes to see numbers here.
So imagine these are all just two-dimensional vectors.
I get those scores.
And here, the intuition which you could probably read off of the vector values is
that the first word every is really very similar to danced.
And It's got high magnitude.
So you get a really large un-normalized,
um, cosine similarity value.
And then they sort of drop off.
And then they get normalized by the softmax function,
which sort of flattens things out.
Then I do the mean to get that context vector,
and that context vector is dimension 2, right?
It's just an average of all these weighted vectors from down over here.
And that gets concatenated with this representation.
Hope you can see that in the purple.
So that gives me a 4-dimensional vector,
which I feed through this dense layer to get the attention combination.
And then finally, that gets fed into the classifier as before. Yeah?
Seems to me like the larger your output vector,
the more attention you get pretty much no matter what.
[inaudible] It's got the highest weights because it had larger numbers?
It did, yeah. On this scoring function.
And that's because, well,
it's not only larger numbers but also
larger numbers that are very similar to this final representation.
That's sort of the way I cook things up so that you would see what the vectors are.
But you're right in the sense that the more similar
I am to this final representation and the larger my magnitude,
the greater my attention weight will be.
[inaudible] you're going to get pretty close to the original vector.
Yeah. I made up the boundaries.
So I think you have exactly the right intuition. Yeah?
So to what extent does, do you
benefit from the attention versus just seeing all of the hidden states from previously?
Like how much does,
where does the attention like add to that?
It's an interesting question, so how would I see all the other states?
[inaudible] them before, right?
Right.
Um, so if you just had a classifier on all of those as an addition to the final one,
like would that give you the same like results status using this attention layer?
I see what you mean. So how would that model deal
with varying length for the premise or the hypothesis,
especially the premise in this case.
If it could be 3, 4, 5, 6.
If I didn't know the length,
how would I summarize them into something that was fixed
dimensional so that I could feed it in the way it happens up here?
I think that would be the blocker to actually just literally reusing them.
And so the attention mechanism is giving us a kind of blurry look back in a way that for
this mechanism is just hardcoded to be
weighting things that are similar to this final representation or higher.
But we are going for
the same intuition which is that we want to kind of bring them back in.
The idea s that like all of these models,
they are kind of like as they move through data,
they have a kind of hazy view of their own past.
Uh, and then we want to give them these reminders
in the sense in the reminders come through this very thin pipe on this model.
But this is a good transition point.
So, um, there are other scoring functions that people have explored.
I just showed you the dot-product.
But you could have learned parameters in here,
like this one that's called general by Luong et al.
That's like general because it's giving us this like way of combining h_c.
The final representation with each one of
these vectors and learned parameters for that kind of bilinear combination.
And so that's giving us a chance to go way beyond
the hard-coded ideal of the dot-product into learning just in general what,
what relationships are valuable for the problem that I'm
trying to solve and concat is very similar here.
I think the crucial innovation from both of these is
that you're learning weights as part of the attention mechanism.
Would there be any benefit, especially when the premise and hypothesis get along to have
attention for the previous parts of the hypothesis?
That's a wonderful question.
I mean, yes and if you look at the literature,
especially the early literature on attention which was focused on machine translation,
that's, in fact, the way they're defined, right?
So I have attention for this state but I also have it for
this one and this one all the way through the hypothesis or,
you know, the translation.
Um, the reason that's meaningful is because,
in translation, each one of these has an output.
So you're like predicting a word for example.
Whereas in NLI you have an output only in this last state.
So for this presentation,
if you did compute them for all these other steps the information would just disappear.
Um, and that's a limitation of the,
the places where we're getting supervision.
So for this global attention,
the answer is kind of no,
it's not worth doing for every step in
the hypothesis but that's a nice transition to word-by-word attention.
So word-by-word attention is gonna try to make good on exactly your intuition.
I would like to get information at all of these steps
about what my premise was like but it gets more involved.
So here let me see how well I can do this.
So every dog danced,
some poodle moved, focus your attention on this B state.
And keep in mind that this has a kind of recursive property.
So you'll feel a little bit at c at the start.
The first representation to build this is kind of feeding,
it's looking at B here.
So B gets copied over and it gets weighted by
the previous attention weights K which came from this A representation.
I didn't show you those but just imagine you had them.
So I re-weight B by those attention things and then I add
in all of the vectors from everything in the premise, right?
So this gets copied. These are all the same.
But the reason they're repeated is so that I can add in
all the previous states and so this would be
for whatever dimensionality you had for the premise.
You can see already that in this case I'm gonna
have to make the simplifying assumption that I
have the same length for the premise and hypothesis so that this parameters,
these parameters W are well-defined.
But I hope you can see that the fundamental thing
is we're bringing in all of the premise.
We're kind of infusing the current state
with everything from the premise and then feeding it
through this neural layer to get M. Now,
M doesn't have the right dimensionality for us going forward.
So the true weights for this are fed through a softmax combination of
M with some other learned weights W which are performing this transformation.
Then I get what is kind of like the context vector that we saw before.
And here, it's all those premise states again weighted by alpha B.
And then you bring in also the previous attention weights here.
That's KA and you get some parameters for that as well.
And then finally, it's the same as before.
This classifier layer is just KC and HC.
So down here I would use this one.
And the final calculation that I got if I was doing the final state and
feed that through a classifier. Makes sense?
And you do this, so I do it for A and for B and when I get to C,
I create this classifier.
And since it's dependent on KC,
which is dependent on KB and B,
which is dependent on KA and A,
I've also got lots of influences from all these attention weights that I
kept word by word through the entire processing of the hypothesis.
So if you squint, I think it makes good on the intuitions that both of you had. Yeah.
The question about the intuition behind attention.
So it seems like sort of the underlying hypothesis is that
if your representation states have similar values, right?
So their dot product or their matrix multiplication will be similar.
But in this case, right?
Maybe it's really useful if you're trying to say that one thing
is not related to the other that your hypothesis is not related to your premise.
Maybe the states will have very different values and
your attention would almost make you not pay attention to that at all.
Like, I guess at what point does it make sense that you're always assuming
that similar values for your features is what you want to pay attention to?
Does that make sense as a question?
I think that this is a failing only of the dot-product one, right?
Where I've kind of hard-coded the ideal because I tried to highlight them in orange here.
I have three chances to learn parameters
and those parameters could reflect all of these different associations.
So not only strong things but also weak things, right?
And you see this in visualizations of
the learned weights that you kind of learn to amplify
important pairings and de-emphasize ones that are not informative for the final task.
So this is kind of like again the data are speaking very
loudly in this case in a way that they weren't for the dot-product view.
Yeah and let me just mention a few other variants.
I mean, what I've done there with those two views.
I mean, I just picked two that seemed kind of representative of strategies people use.
My personal opinion is that the literature on attention is
kind of hard to read because there are so many variants.
And when you open up a new paper and see that
people have tried a different view of attention it's hard to
know where the meaningful differences are
and where they're just kind of incidental implementation details.
But I hope you're now armed to think like there's kind of
these two classes global with like
pretty hard-coded values about what attention weight should be like on
up to this one which is like very freely
learning word by word what the attention weight should be.
Other variants so Luong et al also applied, explored local attention
which is like instead of looking at
the entire premise when forming whatever vector of attention weights I'm gonna form,
I look only at a controlled window.
Um, I think that's most applicable in situations like machine translation,
where you have lots of output states and so you're kind of moving through
the hypothesis attending to different local windows in the premise.
Um, or I should say the source and target languages for machine translation
or for like language modeling or dialogue which could have a lot of output states.
Um, for NLI it kind of seems like it's
that final state that really matters and then you might as well
attend to the entire premise. Yeah.
[inaudible] and that works?
Yeah, I, I've sort of mentioned that down here.
And we're gonna talk about those later.
This is what I mean by, um,
attention has become kind of pervasive and maybe even the primary source of connection.
Let me just go through these now before getting to that.
So word-by-word attention.
This thing that I showed you here can be set up in
many ways and you could have many more learned parameters.
And as an extreme case of that,
this Rocktaschel paper which was really
foundational in showing that attention is powerful for NLI,
they basically seem to have taken the view
that for every one of the matrices that you have here,
you have associated embedding weights or transformation weights.
So they have a lot more, um,
W's that would be in orange if you depicted their full model.
But I think that with this summary here,
it's kinda the same intuition which is as you troop through the hypothesis infuse
every state that you look at with an attention weighted version
of the entire premise and learn parameters along the way.
You could, you could, um,
as you troop through the hypothesis,
append attention weights from the premise at each time-step.
That's another variant that Luong et al explore,
and then relevant to things like the transformer.
So a recent development which I think we'll talk about
in a couple of weeks is that in the
usual framing like the one I have taken for
this lecture uh, the primary connections are the RNN ones.
And then you do this like additional layer of attention connections.
And in the paper,
I think this is the one that's called attention is all you need.
Uh, the idea there is that drop
the basic recurrent network connections and just have lots of attention mechanisms.
And in my view what that's doing is kind of just
saying for every point that I'm at in an example,
just freely explore connections to my neighborhood maybe through that,
throughout the whole example and just see if there are
any influences anywhere around me that are relevant for solving my problem.
So it's a very open-ended way of exploring the data.
It doesn't even encode kind of the linear structure that you get from an RNN.
But those are, those are proven to be really powerful.
And then as a final kind of more indirect connection,
but still interesting the memory networks that were developed a few years ago,
they're kind of similar to attention mechanisms
in the sense that they're trying with additional,
augmented representations to address the fact that
your network might be sort of forgetting things as it processes examples.
But the mechanism there is quite different.
So here you have this kind of
memory store representation as opposed to additional attention connections.
But of course, those ideas could be combined as well.
Okay, that's very technical stuff.
Um, but I hope you feel,
if you weren't already kind of prepared to dive into the literature,
it will be rewarding.
Ah, I think it's a very clear result at this point that for lots of problems in NLU,
retention mechanisms are valuable.
Excellent, so final phase here,
I wanted to show you some error analyses.
An and some- suggest some new possibilities for doing this.
So I mentioned the MultiNLI annotations,
here I've given a few examples,
the annotations are on the left,
I think there are 16 categories for them,
and all they're doing is just giving you know
a human's judgment about certain important semantic properties of the examples.
So this one is MODAL and COREF because it has can in here, ah,
and it's sort of got an anaphoric connection between students of
human misery and those who study human misery.
NEGATION, TENSE DIFFERENCE, CONDITIONAL,
QUANTIFIER ACTIVE, PASSIVE, this active passive thing is nice.
They consolidated programs to increase efficiency and
programs to increase efficiency were consolidated.
You know, like it's not surprising that
that's a common strategy for people who were constructing examples.
And it's really interesting to ask of your system whether it has learned the kind of
human linguistic thing that active passive variants
preserve some aspects of meaning or many aspects of meaning.
So just as a kind of example of something you might do,
I used some of my own Amazon credits that I got along with you
all to just run a bunch of experiments on the cloud just using the course code.
So I did Logistic Regression with cross-product features,
um, and I explored like the regularization strength and the penalty type L1 L2.
And then for LSTMs,
I did a chained one.
So that's the most naive version where you have shared parameters across
premise and hypothesis as well as a sentence encoding one,
which I just pulled out of that notebook that I showed you.
And I explored the embedding dimension,
the hidden dimensionality, and the learning rate and the activation function.
Um, up here, I've given the performance on the 1,000 or so annotated examples.
So the logistic regression was best,
chained, and then sentence encoding.
Just as an aside here,
I noted I wanted to store
all the model parameters so that I could test them on new cases.
The model file for the logistic regression has
over 600 megabytes because it has 16 million features.
The cross-product feature spaces are really large,
contrast that with the LSTMs,
where the model files are each about a megabyte.
So you know this performance difference if I wanted to run these models on a phone,
um, then I just can't use that logistic regression right,
but these, these models down are nice and compact.
An important thing to keep in mind.
[BACKGROUND]
Oh for the annotations?
I don't know. That's a good question.
I should have checked. Um, It
would be quick to check though because those models are quick to train.
I'm less concerned though with the absolute performance.
Although it would be nice to bring it into the mix for an error analysis,
but what I was really interested in is just that you have this view
of absolute performance where maybe logistic regression is best.
But how are they differing according to these different annotation categories?
So there are like 16 of them,
so I grouped them in a few ways.
These are the categories where all the models are more often correct than incorrect.
Um, that's a kind of high level summary and that's ACTIVE/PASSIVE,
antonyms, BELIEF reports, conditionals,
long sentences, MODALS and NEGATION.
It's pretty good. Okay, and it's good that there are a lot of them where there doing,
they're more often correct than incorrect.
Here's the one case where all the models were more incorrect than correct.
So this is presumably,
no matter what your model the hardest and this is QUANTITY/TIME_REASONING.
And this doesn't surprise me,
because I think a lot of the models and also the feature spaces are just
not sensitive to this low-level kind of stuff, it's very subtle.
This is a classic case of a very small set of
semantic morphemes having very large consequences for what the overall sentences mean.
And so I'm not surprised that this is a hard one.
Here's a case where only the logistic regression was more often correct than incorrect,
the other models were the reverse and that's COREF and tense differences.
And that's kind of interesting.
Um, the TENSE_DIFFERENCE one makes sense to me in the sense that
the word product model first of all is keeping track of all of these pairs.
But since I didn't do any stemming or other normalization,
it also knows about differences in tense.
Um, and so it has this enormous space of features and it's just kind of
pretty well attuned to the fact that you might have like
was to is because it has that directly represented,
whereas the, um, LSTMs are probably struggling because they
probably end up with pretty similar representations for
the tense variants because they occur in a lot of the same environments.
And you might tell a similar story for the COREF case.
Here's one where only the chained LSTM was
more correct than incorrect and this is WORD_OVERLAP.
That's kind of surprising to me,
I don't have an explanation for that.
This would make perfect sense if I had added some attention mechanisms,
um, but I didn't.
And I would have thought that the logistic regression would be really good at this,
but it's just about you know 50-50.
And here the finally, here the cases where only
the sentence-encoding LSTM was more incorrect than correct.
So the other two models were better, right?
This is our worst model remember and this is PARAPHRASE and QUANTIFIER.
I don't have a deep explanation for this I guess.
Except that it's the worst, worst of the three.Yeah.
Ah so did you [inaudible] ask where the initial input embeddings weren't random,
instead of like GloVe or something?
I considered it, but my,
I took the philosophical view that I would introduce
no outside information in this competition and since
the logistic regression model can't benefit from GloVe vectors,
at least not in the way that I implemented it,
I would also not give the other models that advantage.
My hunch is that if I brought in rich initialization
than the LSTMs would just jump up in performance.
But they would have a lot more information at their disposal.
I guess my assumptions that the logistic regression
has a lot more information [inaudible] cross products such a large feature space.
Yeah fair is fair,
it's like I gave these three players the data and nothing more.
But another dimension to this problem would be,
if I compare an LSTM with and without BERT representations for its words,
where do the differences arise?
And I think this could again illuminate like you probably even have some expectations
that it's going to do better at things like that involve general lexical information,
maybe word overlap,
antonyms, maybe belief, negation.
Um, but long sentence might be unaffected by this outside information. Maybe not though.
A couple more things.
So another thing that you can do in NLI,
ah, is resort to some linguistic expectations that you might have.
So you could ask, for example,
does your model know that negation is downward monotone?
That is, can it get right consistently that Fido ran entails Fido moved?
And Fido didn't move entails Fido didn't run?
So the up arrow means I have entailment in that direction,
down entailment in that direction.
And what this would mean for your model of course is that,
any example that you plugged in,
maybe where you would say where it got this right,
if you just add a negation it should get this one right.
In my experience, these models disappoint you and that they have not learned this.
And you will certainly be disappointed if you
try something a little bit more sophisticated like,
does your model know that every is
downward monotone on its first argument and upward on its second?
This is a semantic fact that I think is very clear.
Um, but your model probably will not have the systematicity
that leads you to think that yes it has fully encoded this logical fact.
You will find some examples where it seems to be behaving in
accord with this generalization and others where it does something quite crazy,
even if it is performing well on a large-scale assessment like SNLI.
And that's something to think about.
Here's a related thing just by way of wrapping up.
So I've mentioned before that,
that Bill as part of his thesis did lots of rich work on
natural logic, applying it to NLI tasks.
And natural logic has the characteristic that it has
very sophisticated little algebras of semantic relationships.
And I've summarized one here.
This is a kind of theory of negation, um,
where you know if I have p and q are disjoint like dog and couch,
then the negation of those two are neutral.
And I filled out this whole table here and you can construct, um,
artificial data sets where you just insist that the lexical items, so to speak,
have certain semantic relationships like entailment, consistency, overlap contradiction.
And then you can define the algebra on top of them and
construct as many examples as you want in a kind of algorithmic fashion.
In past years of this class,
we've done this as a homework problem just to have
people confront the semantic subtleties.
And so then you could ask,
if you train your model on a doubly-negated dataset,
an artificial one that you create like you know,
not not p not not q.
Does it generalize to the EEE negated cases?
You might think it should, right?
It should have all the information that you need at
that point because it's seen the base cases,
It's seen single negation and it should now
know that double negation just kind of flips you back,
or you know flips you to another systematic space in this little algebra.
Um, but again, I think it's unlikely that your model has fully learned
this algebra and you will see that as you try to
ask it to make predictions about longer and longer sequences.
Um, and this is a nice way to stress test your model and I've given
a few examples of papers that I think do this quite
well including one by who's on the teaching team.
[BACKGROUND]
Trained on like not, not-p,
not, not-q, not-p, not-q, p,
not-q, not-p, q or just,
on not, not-p, not, not-q?
I will do it on- yeah,
that's a good point like the unnegated cases,
the singly, and the doubly.
I would think that that's giving your model
a fair chance to have seen all the relevant kinds of patterns,
and it should have everything it needs now to generalize to triple, quadruple,
and so forth. Yeah.
When were cleaning, um, these types of models,
they're completely not transmissible from
one language to another even as we're part of the same language family because say,
for example, double negation is a thing in
other languages that acts differently as in English,
I would expect them not to work.
But say is we traded on a huge dataset of English words,
English sentences, and that has included, I don't know,
some other sentences in
a different language but related to
English and then we test them on that different language,
we expected to pick up things that are- uh-
Yeah, it's a good question. For- for this probing artificial data case,
this is meant to be sep- separate from any intuitions you might have about negation.
This is more like testing the sense in which the model is doing something systematic.
In the sense of cognitive science like has it really learned to abstract from
your data a kind of systematic pattern as embodied in this little algebra up here?
And this is just nice because since you control all
of the- all aspects of the model and the data that come in,
you can have a pretty good sense for whether you've posed this in a fair way,
uh, and that can give you insight into what your model is actually capable of learning.
For the more general and interesting linguistic question of
whether or not parameters you've learned on one language would transfer to others,
there are experiments of people trying to do that and in fact,
you can try that for NLI because that Facebook group
released a version of MultiNLI, that is multilingual.
Um, pursuing the hypothesis that there might be some abstract layers of
representation that are good for multiple languages. Yeah.
Yes, are you ultimately limited to
feeding your model certain things and seeing what it spits out and you can't,
sort of, look inside and like
look- obviously it's not easy to say have you learned the concept?
And like look at the activations,
but is this still, sort of,
the main way of asking this question?
It's just throwing things in and seeing what comes out.
I'm just smiling because I,
sort of, feel like the answer is yes,
and this is just amusing because it's like a sense in
which deep learning has become like a psychology experiment,
where instead of having humans come into the lab
where you probe them behaviorally to see what they know,
you now have this black-box model that you probe behaviorally to see what it knows.
[LAUGHTER] Um, there are great papers that are trying to push us past this.
Um, where you do, kind of,
you have another model that does some local expe- inspection
of the parameters or maybe looks at how,
um, different examples as they pass through the network activate
different parts of it to give you insight into what they actually have encoded.
But I think a lot of it still comes down to this kind of behavioral thing.
Which is not necessarily bad it just means that you might also wanna get good at running
human subject experiments so that your artificial agents are treated fairly,
and the protocols. [NOISE] Yeah.
And sort of related to this- that I have seen some, kind of,
optimism that attention is not only, kind of,
a performance enhancer but maybe also some kind of
explanatory metric that you can look at or what does my model you're paying attention to,
and maybe get some insights there but I was just talking to someone recently who was
saying that there's been a paper published recently and it's saying that attention,
actually, is not any good for this.
It doesn't actually help us explain things.
Wondering if you have any insight on that.
Yeah, again, I think there are differing opinions.
My view would be- let me give you two views.
So first for NLI,
when you look at the heat maps of
the different attention weights that it has learned on word-by-word attention,
those seem often to be quite illuminating to me and do, kind of,
reflect what you would expect that certain pairs of words across premise
and hypothesis are really informative about the classification label.
So I think that all looks good to me.
Conversely like I was really excited when I saw people adding attention mechanisms into
these problems because it kind of looked like it was a soft parse of the example,
like a dependency parse,
and I thought that would be really cool because maybe
you'd be able to read off of the attention weights
what the true parse was or maybe what the top most likely parses for this utterance were,
and I think that that dream is not quite realized,
it's less clear that that's happening.
But for the NLI case,
I think this is like one as- one way of making good on your- on what your goal was.
Do you have different views?
[OVERLAPPING] You think a lot about these things.
[LAUGHTER] We'll talk more about attention when we talk about the transformer,
and BERT, and ELMo,
because attention is a big part of what makes those models work.
I think that was a lot of material,
and so I think I won't try in this- in these last few minutes to do
the ambitious thing of showing you too much about these grounding slides,
um, they are up.
Let me just show you just one thing.
So this is about grounding, and what I did is,
I gave you a bunch of linguistic insights about
why grounded language understanding is important,
and then I frame this as speakers, listeners,
chat bots, and reasoning about other minds.
And so I'll just say that speakers- that would be like what I've done for a lot of
these slides is focusing on colors, people describing colors,
because it's a simple version of a very large class of interesting problems
and colors are great because they have like
cognitive complexity and linguistic complexity.
So for speakers here,
you could think of models that are learning to describe colors.
And there's great data for that,
and this is a nice manageable problem that would allow you to confront,
um, like color to sequence, right?
So color comes in and then you need to produce a description.
Then the listener view is the converse, right?
So somebody gives you a color description and you
need to make a guess about what color they are describing.
That's interestingly different, the inputs are different
and the output space is a little bit, uh, harder to deal with.
And then grounded chat bots.
You probably heard about chat bots?
I think that they are often kind- kind of at C because they're just using language and
producing language and like they're not like actually
connected to any problem or any social situation.
This group- this Facebook Paper, uh, on negotiation,
is a data set where you have chat bots that are grounded in a goal oriented task,
and I think that's very exciting.
So I've just tried to present their model in its basic form,
and you should know that it's a really cool dataset.
And then for other minds,
I've sort of made a connection between pragmatics and machine learning,
and another- and an interesting thing about that is that again,
you have a bunch of really interesting quite manageable datasets for doing projects on.
And you guys are really well-prepared to think about all of these models,
you'll find, if you look through,
you'll see that because basically,
all of them are like NLI models where instead of having just one label,
you have a label for every stage in your hypothesis or in your output space,
and what's called the decoder part of the problem.
Really easy to implement based on what you guys have already done,
and then you have all these cool data sets to think about.
So I'm gonna give you much more detail about that on Monday,
but if you're still casting about for projects,
you might page through this and just, maybe,
follow the links to various data sets and so forth,
because it is such a rich space.
But let's stop for now,
that's been enough content. Thanks everyone.
 Welcome to the brave souls who are here in person.
Get to see this thrilling report, first-hand.
We are here to talk about, uh, Bake-off 3.
Um, if you've already forgotten,
uh, what we were doing was,
developing the best relation extraction systems we possibly could.
Um, uh, highly multi-class problem and we are
looking at F1 for- to determine the best systems.
Histogram of the results from about 100 different, um,
models here compared to where our simple bag of words
featurizer baseline was from the homework.
You can see this is very exciting.
Um, most of us managed to beat that baseline. So that's great.
Um, we've been doing this thing where we use our, uh,
VSM code to analyze the code variables views to see if we could find any trends.
Um, in this case, it's really noisy.
So we're gonna skip this, and, uh,
instead, look at, um,
something that's a little bit more anecdotal.
But we did- we went through the top 15 and the bottom 15, um,
systems and tried to draw out patterns of wha- what were some of the commonalities.
And, um, the top-performing systems,
I think the biggest- the biggest trend is, uh,
caring about your features,
seems to be a lot more productive than caring
about your model or your modeling architecture.
So, um, the top systems,
um, actually we- we can see here.
Uh, 66% of the top systems used just a simple logistic regression.
The other 33% used the random forest.
Um, and really it was in just the- the- the wealth,
the voluminous- voluminousness of, uh, featurizers.
Where, uh, that really distinguished the top systems.
Um, so it's kind of fun scanning through this code
because it's like playing Mad Libs with the words left,
right, middle directional, POS, bigram, and trigram.
So you can kind of combine- like pick three,
combine them, and do that 10 times.
And you'll have like a really good system.
And definitely combining different kinds of features was,
uh, was the biggest kind of- brought us the biggest lift.
So on average, the top 15 systems had five featurizers,
compared to the bottom ones which generally just had one.
And so I think that- that shows that you can kind of
get along get pretty far by thinking about,
um, what are the inputs to the model.
So one example, the very top system,
congratulations to this group.
Um, so this- you can see here, uh, we have,
I think this is like ten different,
um, ten different featurizers.
This group did include some GloVe,
uh, features as well.
And this kind of on repeated runs,
um, brought them in just above,
um, our second place.
Um, who also had a, um,
nice long list of featurizers but,
um, uh, the main difference here being,
did not include GloVe.
And these- these are good.
These are good numbers here, we're- I'm delighted by them.
And then- yeah, the less magnificant- magnifi- magnificent systems,
um, you know, generally,
there was more exploration of different modeling, uh, model type.
So some people tried different, um, you know,
neural classifiers or different like an SVC or something.
And generally, just a very short list of featurizers.
Um, and so in- it was- it was
fun to see in some of the top systems how there is like tons and tons of
code trying all these different models that was just ultimately
abandoned for the default mode- model factory from the homework.
Um, all in all, uh, loved looking through the systems,
a lot of creativity and I think we can all go to sleep proud that we as a group have,
um, managed to beat our baseline so- so resolutely.
So thanks for a really fun bake-off and,
you know, one more to go.
Um, all right. Thank you.
Any questions?
Yeah, any questions? Yes.
The median?
I'm sorry.
The median?
Whoa, I don't have that offhand.
It looks like, it's probably about 0.65.
That was the score that almost everybody got, that was the most commonly occurring one.
That's below, oh, I see that's below 0.7.
Yeah, that's about 0.65.
So the- the winning teams- that's really far above that.
Yes.
That's very impressive.
Yeah, really. That's true.
Anything else? All right.
Did you get that baseline number?
Mine is a lot higher.
Your baseline's a lot higher?
Ooh, maybe I'm being way too generous here.
Um, that was, uh,
just the number when I ran it.
So I guess, maybe I should have done it more.
Maybe I'm too optimistic.
Um, okay. We can check that again.
So maybe- maybe the big spike here is right around the baseline.
Okay. That's median, yeah. All right.
Good point. I'll run it again a couple of times.
We'll see where it shakes out. All right.
Time for the real star
of the show
[NOISE].
Alright. Thank you Moritz, that's great.
And I guess it's worth pausing and just saying,
so Bake-off 4 has been launched,
the poster's up on Piazza.
Um, and I know a lot of you have really
invested in the systems that you produce for these four bake-offs.
And for people who did that,
you should feel proud.
I mean, you've worked out in a deep way on four very different problems in NLU,
which is maybe four more deep,
uh, analyses than other courses offer.
And I think that's a real accomplishment, right.
You've got real expertise now in in those four different problems.
um, and some of you have done
incredibly impressive systems for multiple of these bake-offs which is again,
just a very impressive accomplishment.
Um, yes, so Bake-off 4 is up, uh, due Wednesday.
That's the final one for us,
and then we'll move into a more project oriented mode.
And I guess we'll talk, um,
a lot more about that in the coming weeks.
The- the lit review after
the bake-off is the next thing that's due- that's due on May 13.
Um, and there are guidelines, uh,
at the website about what that should contain.
Um, we could look at them now just in case there are any questions,
actually that might be healthy, let's see.
Just so you know where they are and everything.
So it's a short paper,
um, you can do in a group.
You should pick one of the final project groups on Piazza to do that.
So that credit gets assigned in the way that you expect.
Um, groups of one should do 5 papers.
Groups of two should do 7.
Groups of three should do 9.
So that- those numbers there are kind of encouraging you to work in a large group.
It's by no means required but you have to do overall
less work in terms of reviewing papers if you do that.
And the other thing I want to say here,
is just that, this is more than just a lit review.
So it is also attempting to push you to
begin thinking about what problem you're going to tackle.
And I will say that,
given the time allotted,
the ideal thing is that over the next week or so,
you and your group just settle on a problem,
um, and then go forth and do it.
Changing in midstream, there just- isn't a lot of time to do that.
And I feel like that leaves you behind.
So you should try to figure things out now and then just go forward with that plan.
Um, and this is meant to sort of encourage you to
start that problem solving because we want you to state,
for the papers that you chose,
what their general problem is.
That's a good creative act,
a way of kind of summarizing what you've studied.
You should offer some concise summaries.
Um, but what you should really be doing there is trying to tease out maybe
the major contributions and also getting to three here like,
where these papers are alike where they're different.
Maybe where they're starting to fall down or where they've
left gaps or where there's obvious room for improvement.
Maybe like, it's an older paper and you know about some newer models,
that would be worth trying on the task,
in which case, that's an obvious thing to point out because that might start
to point out the outlines of the projects that you ultimately want to do.
Uh, and as we say here,
this is the most valuable.
This could be the basis for the lit review section.
Because in the paper that you write for the lit review section,
the goal there is not to just rehearse what's been said,
but rather to contextualize
your work and sort of justify the modeling choices that you made.
And this compare and contrast is going to lay the groundwork for doing that, I would say.
So that's the part that you'll want to give real thought.
And then obviously future work suggestions for how these papers could be extended.
If you do that in a thoughtful way,
then it's an obvious discussion place for you and
your teaching team mentor because your mentor can go in and say like,
actually, I think the first one is resolved,
but the second one is absolutely worth pursuing.
The third one is maybe too hard.
So why don't you think about two as kind of
a guiding thesis for your work on a project, right.
That's an incredibly productive thing that could happen if you
think really seriously about kind of next steps for this.
And then we also want you to have a proper references section.
We're gonna be nitpicky about that.
You will lose some credit if you don't have a proper bibliography,
so just follow through on that.
And again like, that's going to carry through all these assignments.
I think it's just part of scholarship.
We're not picky about the format for this thing,
but we are picky that you have a proper bibliography.
And that's the spirit of this point 5 here.
So think about this with your team,
get to work on this as a document.
Uh, and do try to use it as a chance to really
lay the groundwork for your project because then you'll be off and running.
Um, any questions about that?
Good. So after that,
the next document for
the final project is the experimental protocol, as we're calling it.
There are also, um, requirements for that.
I think I won't review them right now.
Except to say that, this is part of us kind of
pushing you in the direction of having a quantitative analysis that uses data.
That's a value that we've encoded in this whole project structure.
And the protocol is gonna be a kind of
check-in that your project will succeed in that way,
and that you're also doing the right things in terms of setting up
baselines and that you have the proper metrics, and things like that.
Uh, so once you've done that,
I think you really have like,
the foundation for your paper in terms of its experiments and everything.
And then the final things after that are more presentational.
So a video and then the final paper itself.
Good.
Okay. So I propose that we dive into the material.
Today we're gonna talk about grounded language understanding.
This is like maybe my favorite topic for NLU,
and this is kind of where we finally start to
realize what you might think of as the true dream for NLU,
which is like that you'd have a robot friend or something.
Um, and the reason I emphasize grounding here is just because I do
think that if we're gonna achieve that goal of having robot friends, uh,
they're gonna have to exist in kind of social physical environments in some sense,
or at least be attuned to the fact that humans live in such environments.
I think they will not succeed if all they have done is
experience text flying through them and maybe producing text.
Right? Uh, and I'm gonna try to substantiate that claim a little bit.
But I think it's pretty intuitive that a big part of the human experience with language,
but just in general is being grounded in physical and social situations.
And so we leverage a lot of that information in language use, production, and understanding.
And so all of that brings me back to the fact that if our systems are going to succeed,
then they'll need to be grounded in these ways.
And so what I've tried to do is just assemble kind of
the core building blocks that I think you can
productively use to start to design systems that are grounded.
And at the end of the lecture,
I'm going to suggest some ways in which even if you don't
think of your problem as a grounded NLU problem,
there might be ways that you could bring in some of that grounding,
and thereby enrich your system in terms of its performance,
but maybe also just in terms of the rich ways that you could conceptualize your problem.
And I've framed this as a linguist would.
So I'm gonna give you some history,
and then I'm gonna talk about speakers.
Those are gonna be agents that observe some non-linguistic thing,
and produce a linguistic expression.
And then a listener is an agent that consumes language,
and makes an inference of some kind about the world.
And then they come together in chatbots,
but the thing that I want to emphasize there is that I think for a chatbot to succeed,
again they need to be doing something that's grounded in something language external.
If they're just talking back and forth at each other with no purpose or goal,
um, they're not gonna succeed at doing anything like what we want them to do.
So I'm gonna emphasize grounding there.
And then finally it gets most pragmatic when we talk about reasoning about other minds.
This would be how I could be a more effective user of language.
If I think not just about production or comprehension,
but also about what it's like when I'm a speaker,
to be a listener, and when I'm a listener,
what it's like to be a speaker.
And that can enrich the system that you're using,
it can fill in lots of the gaps in terms
of the actual sentences we exchange with each other,
and I think it keys into something deep about cognition,
but also about kind of efficiently learning from data.
That's another angle that I'm going to try to
convince you of when we talk about those models.
And then finally, just a smattering of things that I didn't really have time for.
But let's start with some of this history.
This is a slide idea that I stole from Andrew McCallum,
um, and he just encourages us to think, maybe in 2019,
now, uh, about the movie 2001,
because you could think of that movie as in part
making predictions about three things in terms of technology.
First, a prediction about what graphics th -
the computer graphics will be like in the year 2001.
About computer chess playing in 2001,
and finally about NLU in 2001.
Because the on-board robot HAL,
that is kind of the ship,
it conducts very natural open-ended conversations with humans.
And we could think about what life is like for us.
So let's do those comparisons.
For graphics, these are the graphics that they show for HAL.
And in 1993, so even well before 2001,
we had things like Jurassic Park.
So this would be a case where the creators of this movie just
dramatically failed to imagine how rich the future would be, right?
This is- it's kind of a failure of imagination that,
this is what computer graphics would be like,
and this was the reality in 1993. Big gap.
We made a lot more progress than expected I guess.
For chess playing, you could argue that they got it exactly right.
So HAL the on-board computer plays chess like an expert, uh,
and just before that,
in terms of actual human history Deep Blue beat Garry Kasparov in 1997.
So that's pretty impressive that they more or less nailed the kind of
timeline in which computers would get good at a game like chess.
The third one, dialogue.
So if you've seen the movie, you can see that HAL has
these incredible human-like conversations with humans.
And it's not just that it has these conversations,
but that they are obviously like deeply grounded
in this row- this in HAL's plans and goals.
He's in fact conspiring against them for his own purposes and so forth.
But these are incredibly rich interactions, right?
By 2014, Bill showed us what this was like.
So this is the dream for Siri,
which is already not quite approaching what HAL can achieve,
but maybe at least I can get my tacos ordered or something.
Uh, but then Bill showed us this funny skit from Stephen Colbert,
which just showed the,
the gap between what Stanley Kubrick imagined and what we actually realized about,
I mean, even, uh,
more than a decade after 2001.
And that's sort of funny, because like if you compare, you know,
chess right in the middle,
but graphics, this failure to imagine.
And then when it comes to NLU,
this kind of, uh, overstating what we would actually achieve.
And I- I think that comes back to the fact
that humans using language, it feels effortless.
It feels like something that we ought to be able to master.
And in fact like, all throughout the history of AI,
you have people stating that they're just on the verge of solving all these problems,
that they kind of see now that it's the perceptron or,
you know, computer programming in Scheme,
or now Deep Learning.
These things are just about to solve all these problems,
and they always turn out to be elusive.
They always turn out to be harder than we imagined.
And I guess I'm going to try to eliminate part of why that is.
I like this quote from Terry Winograd, uh,
that kind of keys into this grounding idea language is action.
All language use can be thought of as a way of activating procedures within the hearer.
We can think of an utterance as a program,
one that indirectly causes a set of operations to be carried
out within the hearer's cognitive system. It's kind of nice.
It keys into the goal-oriented nature of this,
and also the social component.
Also by way of conveying to you why
this problem is persistently more difficult than people think,
I thought I would present this analogy that I
use when I teach semantics and pragmatics as well.
This comes from the linguist Steven Levinson.
And he asks us to imagine,
you know, this- there's a Rembrandt sketch here,
and just observe for a second that you can,
even if you've never seen it before,
kind of make out that there's a figure in the center.
There seemed to be some people, uh,
kneeling at the feet of this- this figure in the center.
There's an archway or something,
some kind of buildings is in the background.
And if you have additional context for Rembrandt sketches,
or for sketches like this,
you could probably even make the inference that this is a Christ figure in the middle,
with disciples around, right?
You probably have other images in mind that kind of resemble it,
that are more high fidelity.
Levinson's point is that this is incredible.
If you really look at this sketch,
you start to realize that it is
just the barest outline of all of that rich stuff that you were able to infer.
I mean, it is incredibly sketchy.
It just- at some level of detail,
that's like a random collection of lines.
But we are geared as humans to be able to take
those random squiggles and infer
all that richness from it and you do it at kind of multiple layers.
Right? So like, even if you don't have much context,
you can infer that there's a figure.
If you have additional social and cultural context,
you can figure out that it's a Christ figure,
and if you know something about Rembrandt, right,
this could get arbitrarily deep because you- when you interpret that picture,
you're drawing on much more than just the raw visual image there.
You are able to imbue it with lots of other information.
And that's a kind of miracle about
human vision and the cognitive stuff that goes along with it.
Levinson's point is that utterance interpretation is very much like this visual act.
When people speak to you,
you are able to extract much more information from
their utterances than seems to be encoded in those utterances directly.
We do an incredible amount of kind of what you might call reading between
the lines or figuring out from somebody's very sketchy utterance,
and the context you're in,
and the goals you take them to have,
and what you know about them as people.
You're able to infer much more than what's just directly encoded there.
And in fact, it might be that most of
the information that you extract is not coming directly from what's encoded,
but rather from all of that other stuff working in
conjunction with what people actually said to you.
Just like here, the lines tell
only this very small part of the full story about what you extract from the image.
And the naturalness of what you do with the image is akin to the naturalness of
what you do when you interpret people's utterances or
when you expect others to interpret your utterances.
You don't reflect on all of the gaps that people are filling
in and you're apt then to underestimate just how hard that is.
But when you confront NLU problems you
start to become aware of very quickly of just how much of this is
implicit and coming from context and
other kind of like things that aren't encoded in the language that we use.
And I like this analogy a lot.
I mean, this is sort of like vision would be really easy,
you know, for robots if everything had a barcode on it.
And language understanding would be really easy for
artificial systems if everything was
limited to what's encoded in the language that we actually utter with each other.
The language itself would be incredibly
cumbersome but at least it would be easy for machines just like those barcodes.
But the reality is that so much of it is coming from elsewhere.
And that's why I think that in the end,
for- to have a rich NLU agent,
you're going to have an agent that has lots of other human-like capabilities.
Let me highlight a few of these things just to kind of random collection of
things that I like from pragmatics that
sort of point at how hard this problem is going to be.
So I'm starting with what linguists call indexicality.
Like, I am speaking.
The indexical here is the obvious one,
is the first person pronoun, I.
So you might think that's a pretty easy problem to solve if you have some kind of grounding.
I mean, obviously if you've just got text coming in and out,
then figuring out what I means is a kind of bizarre question even to ask.
But if you think about today's devices like your phone,
this is something that you might have
a pretty sophisticated view of already like the phone could
just at some level know that the referent of I is the current user,
or the owner of the phone,
or something like that and that will be correct a lot of the time.
Already, I think that's pretty sophisticated,
but that's just the easy case.
So what about, we won?
The meaning of we is constrained probably to include the speaker but
the actual collection of people that it refers to is incredibly context-dependent.
It could be a team I'm on,
it could be a team I support,
it could be the people in my community,
it could be the people in my country, right?
You just this- every way that you could possibly
imagine this going could be a meaning for we.
So already when we move from first to third person,
we're confronted with essentially all the problems of pragmatics.
I am here maybe the I is easy but what about the here?
Could be the classroom, could be Stanford.
It's maybe not gonna be interpreted as planet Earth because
that's not such an informative thing to say in 2019.
But maybe someday, I am here will be a meaningful thing,
where here refers to planet Earth.
What about we are here pointing at a map?
So that's an incredible kind of abstraction that with a visual depiction,
somebody can point and have that transfer to the physical location,
and that it doesn't mean that you're at the point on the map.
I'm not here now.
This is a wonderful example from linguistics and philosophy of language.
But I'm not sure that people would use this anymore because there are
no landline phones and therefore, no answering machines.
But in the old days your answering machine might say I am not here now,
which is a wonderful little puzzle because how could it
be that the current speaker is not in the place that they are speaking?
We went to a local bar after work.
This has the challenges of we.
And I was just highlighting here that adjectives like local,
their interpretation is again like here.
It's heavily context dependent and highly
variable in terms of exactly what kind of thing it refers to.
And there are lots of expressions like this.
Three days ago is indexical on the time of utterance in terms
of how many days back you go tomorrow and now work in a similar way.
And now, of course,
has all the uncertainty of here.
It's just in the temporal domain.
So those are some small words.
I think the only way that you could get a grip on them in
any sense is with some grounding and then of course,
you see that they have like- they're all the problems of pragmatics writ small.
Here are a few others context dependence, where are you from?
This is a frustrating question for me.
I think about this all the time because when people ask me where I'm from,
I never know what sense they mean.
Do they mean Connecticut, where I was born,
or the U.S., where I'm a citizen, or Stanford,
which is my affiliation or planet Earth, again,
nobody asks that but maybe we'll live in an exciting future where they do.
This is coming from where and the reason I highlight this example is it's
a kinda mundane question but already you can see
that it keys into the goals that the speaker has.
They're trying to figure out something about you and
the sense of the question is determined by those goals.
Now, those goals might be kind of under-determined.
I feel like when people ask this,
they often don't know what kind of information they're hoping to get back and you
have this awkward little dance about whether you resolved their issue.
That's an added layer of complexity on top of the fact that,
to know what the question means,
I have to know what your goal is and that implies a sophisticated kind of grounding.
Here are some more. I didn't see any,
this is kind of keying into ellipsis but also what linguists call quantifier domains.
So are there typos in my slides?
I didn't see any, you might but.
Are there bookstores downtown? I didn't see any.
Are there cookies in the cupboard? I didn't see any.
Right. There's no way I'm going to get a grip on what this utterance
means unless I know at least what the preceding discourse context is.
But then you also have to do some really sophisticated stuff about figuring
out how essentially to expand the meaning of this elliptical phrase here, any.
And that again has two layers of complexity.
First, I have to figure out what thing you were talking about but then I might
also have to do some domain restriction in the sense of like,
when I say that everyone entered the bake-off,
I don't mean everyone in the universe.
You guys effortlessly figure out that when I say,
everyone in this context,
I mean the students of CS224U,
but in a different context,
if I use it in my house or in my department,
everyone takes on a completely different sense.
The light is on. Chris must be in his office.
The Dean passed a new rule.
Chris must be in his office.
Here showing that modals have a kind of sense ambiguity.
This one is epistemic and this one is deontic.
So this one is keying into knowledge and here
you're offering your evidence that the light is on,
that's evidence that he's in his office.
This one is deontic in the sense that it keys into laws of the campus in this case.
And in this case, this is the relevant rule.
They have very different senses.
This is pervasive in language.
Modals exist not just as these auxiliaries like must and may, and
can and should, but also in adverbs and things like that.
And a big part of figuring out what the statements mean is figuring out
the sense epistemic, deontic, circumstantial.
There's lots of different flavors.
And again, we do this without much reflection.
Classic example, if kangaroos had no tails they would fall over.
It probably seems true but suppose they had jet packs.
If I am able to insert that into the kind of
implicit counterfactual reasoning that goes into this conditional here,
then it feels false, right?
They wouldn't fall over in the worlds while they have jet packs,
they can just fly around.
Somehow, you know not to consider that possibility until I introduce it, in which case,
maybe your judgment about the truth of
the statement changes and
that highlights the flexibility of your reasoning in these counterfactual situations.
And also again, the fact that you didn't really
reflect on it when I gave you the initial example.
But it's certainly important to be able to do that to
figure out what these claims that are even saying.
There's another example that I love.
These is two books, I think they're real books.
What they teach you at Harvard Business School,
what they don't teach you at Harvard Business School,
and then there's a masterful tweet,
"These two books contain the sum total of all human knowledge."
[LAUGHTER]
That person is thinking like a logician.
The actual sense of both of these titles,
I guess is that there is implicit domain restriction on these whats here and so it's not
this knowledge and its complement in the full space of all the things that can be
knowable but rather they both have taken on a much more refined sense.
Perspectival expressions.
This is another kind of grounding left and right, front and back,
former and latter, um,
and I- this was a case where somebody's confused about perspective.
This is from the website,
You had One Job.
Please when using the stairs,
stay to the right when going up,
stay to the left when going down.
This will keep people from running into each other.
It will not. And so here just to kind of summarize this.
I have this fairly mundane statement.
Many students met with me yesterday and in a kind of informal way,
I just highlighted all of the ways in which to understand that expression.
You need some grounding in the physical or the social context.
So like how big is the set of students?
For many students that's relevant.
Uh, what's the time of utterance for understanding these temporal expressions.
Uh, who's the speaker for me,
what's the additional contextual restriction on students which
might affect the domain of these students here, what's the time.
And then you do other kind of things that we call Gricean pragmatics like,
if you said many students met with me,
does that mean it's false that all or most did for
the constrict, the relevant set of students and so forth, and so on.
Uh, this is- as I said, just completely pervasive.
Any questions about that before I switch gears a little bit?
That's a quick run through my greatest hits of phenomena in semantics and pragmatics.
I could do that all day.
If you want much more of that you could take 130A with me next winter.
If you haven't already. I know some familiar faces here.
One thing I love about older work in natural language understanding,
is that it seems like they had fully
internalized this message that I just sent about the importance of grounding.
Because from the beginning,
the systems were grounded in,
in particular physical spaces and like
a really influential paradigm case of this is Terry Winograd's SHRDLU system.
Um, there's a YouTube video that kind
of depicts what this was like but basically it was a,
a blocks world in which things could be moved around.
The blocks have colors and so it's a very finite domain.
Uh, and you could issue instructions into
this world and then the blocks would be manipulated.
And the, the beauty of that first of all,
it's just that from the start,
these dialogues with the system were fully grounded in that phys- in that world.
And then when you look at actual examples of things that the system could do,
you can see just how much, at some level,
the system was leveraging the fact that it was grounded in that space.
So like what does the box contain?
The box is an expression that you can
only make sense of if you know what environment you're in.
That is if you, you and your listener can figure
out what the speaker's intended referent for the box is.
Um, the blue pyramid is the same way.
What is the pyramid supported by?
That's kind of anaphoric to utterance 2 there.
The boxes- the same box as 1.
Um, how many blocks are not in the box?
Again, that's an impor- that may- that's
a very difficult question to make sense of unless you're grounded in an environment.
Four of them is referring back to blocks.
System could do that.
Um, the red cube, yeah,
all these things, kind of- I mean,
the only way any system I think could make sense of a dialogue like this and
perform these actions and answer these questions is if it was grounded in a world,
like the one that Terry Winograd had constructed.
And then there's another contribution that Terry Winograd made early on that I
think still influences how people think about NLU.
And these are these sentences called Winograd sentences or schema sometimes.
Um, here's one. The trophy doesn't fit into the brown suitcase because it's too small.
What is too small?
And you're able to figure out that that means
the suitcase and the reason you're able to do that is because you have
an ability to kind of simulate mentally
what these objects are like and how they're related to each other,
which is coming from the fact that you live in
a physical world and you have a lot of these experiences.
So you're able to figure out that there, it's the suitcase,
whereas the trophy doesn't fit into the brown suitcase because it's too large.
What's too large? There, you resolve to the trophy.
Again, via the same kind of reasoning about physical objects.
The council refused the demonstrators a permit because they feared violence.
Who feared violence?
You can figure out that it's the council and you can do that
because of your understanding about social roles and their relationships.
If I change it a little bit,
the council refused the demonstrators a permit because they advocated violence.
Who advocated violence?
Then your interpretation flips to the demonstrators.
That first one is very physical in its grounding and
the second one is grounded in more like social constructs.
Uh, these are very difficult problems.
I should've checked on this but at least in- as of recently,
for the corpus of Winograd sentences,
even very sophisticated deep learning models were more or less performing at chance.
And people worked really hard and continue to work hard to kind
of construct systems that can solve these problems.
I suspect that the real answer rec- will come from systems that have a whole lot of
really sophisticated and robust kind of commonsense understanding about
the world and that it won't come from just pattern matching on examples like this.
And then finally another thing we could do is just take our cue from neighboring fields.
So for example, developmental psych is a field that is
all about how humans learn language,
at least in part it's about how humans learn language.
They have lots of lessons about this and after all,
humans are our best language learners by far in the universe, as far as we know.
Uh, kids can learn word meanings for example,
with incredible speed despite relatively little input,
certainly nothing like the amount of input that you would get for
a model that- even models that you've trained in this class.
How did they do it? The evidence suggests that they use information
that's inherent in the kind of ways that forms
contrast with each other but also social cues.
Assumptions about the speaker's goal and regularities in the physical environment.
Only this first one is the sort of information that you would get from looking just at,
for example co-occurrence information.
These other ones depend on grounding.
So some consequences for NLU.
As I said, human children are by far
the best agents at learning language and they depend heavily on grounding.
That's an important clue for us.
Problems that are intractable without
grounding are solvable with the right kinds of grounding.
You know, the SHRDLU cases are a dramatic example of that.
I think you won't get traction on dialogues like that.
Reproducing them or understanding them or anything,
unless you know about the blocks world that they're describing.
Deep lear- here's a bright spot for this.
It's a real change over recent years.
So deep learning systems,
they're really flexible toolkits for reasoning about
lots of different kinds of information in a single model.
And this has I think,
led to some real breakthroughs when it comes to grounding because after all,
one way you could think about grounding is,
I'm going to have some vectors for
my words but maybe I'll also have vectors for the people in
my environment or the objects in my environment
or like a vector representation of this image or this video.
And these deep learning models are happy to say I'll take
your word representations and your video representations,
smush them together and do something interesting.
[NOISE] Uh, and that's a very productive way to think about how you
might gather a lot of evidence and use it to make a decision.
Which is more or less the,
the lesson of this lecture here.
So we should seek out data sets and develop data sets that
include the right kind of grounding and to the extent that you can do that,
I think your system will get better.
Because at one level or another,
all the language that we use with each other is grounded if only indirectly.
That's my pitch, high-level description. Yeah.
Should also kind of edit like the [inaudible] web edit to sort of evaluations we're doing like,
even the Winograd schemas which seemed like
they're getting at something various and rich about language.
They don't have a, a video associated with that more like, you know, uh,
here's the still of the picture that I'm describing.
Um, do you think that would be something necessary in order to,
to kind of go along with these data sets?
I think that's in the spirit of what I'm saying, right?
Which you- so you could say,
I think that no system is going to get traction on your suitcase example with the trophy,
uh, unless I also show it lots of images of suitcases and trophies,
and maybe their physical relationships to each other.
And maybe that's indirect about exactly what we care about,
but maybe that's enough to push the system in the right direction,
and that would reinforce my narrative which is,
it wasn't there in the language but it
might be there in this other kind of representation.
Um, I'm also reminded of like,
um, it's related to things I've worked on that kind of key into your intuition.
So it would be very unusual for somebody to describe the fact
that the computer in a room is on the desk because that's the most mundane thing ever.
It's the default assumption.
If I'd describe the room to you,
I might not bother to mention it and I might just say there's
a computer and you'll infer that it's on a desk or something.
And it's only if it's on the floor that I would bother to describe that.
So given that bias that I have for describing things,
only things that are unusual,
how will your system ever learned the default stuff
and why might one answer might be that it can consume
visual representations and that fills in all those gaps and
things that we never describe in language and showing that would be wonderful.
You somewhat answered my question but the later part of the
the explanation to answer but I was thinking to try and take away
sort of one level of indirection and also maybe trying to
build a logic network. [NOISE] So if you could recognize that
like a suitcase is an objects and the objects have
size and then try to sort of work out that simulated model.
Uh, is it another approach of trying to do the same thing?
Great example. And so they're,
they're the grounding in my terms,
might be a knowledge graph that has all this, all this information.
Yeah, and that would get you over maybe the hurdle of default things not being described.
Yeah. Great idea.
I'm gonna move on to these speakers.
This is gonna get more low-level now.
Because partly what I wanted to do is empower you to be able to do some experiments.
And I also think scientifically it's useful to start small.
I mean, those SHRDLU dialogues are really ambitious.
We're probably gonna get more traction
if we start with things that are kind of more controlled.
So that's the spirit of what I'm gonna talk about.
You'll see that a lot of this is gonna be about color reference.
Uh, I forecasted this before.
I chose color reference bec- for a few reasons.
So we have wonderful datasets for this.
Colors are great because it's a-an interesting kind of grounding.
You don't have to deal with all the complexity of visual scenes.
But colors themselves are cognitively complex and also pretty important to us.
And then the language that you get for describing colors is also quite rich,
especially if you don't constrain people to a color vocabulary.
So as a starting point, um, Randall Munroe,
the guy behind XKCD, years ago,
he did a color survey where he just asked people to describe colors.
It's an- it's a massive dataset with millions of
instances of people describing color patches.
Here are some examples.
And they go from the very simple things like green and purple up top,
grape which is a little more poetical, turquoise, rarer.
Then they get more descriptive, moss green,
light blue-gray, robin's egg blue.
Already some domain knowledge creeping in.
British racing green is not a phrase I'd ever heard before.
Baby puke green is a classic example from this dataset.
There's lots of puke colors in the dataset.
That's the cutest. Um, so thi- this is kind of what I mean,
like we have a,
a wonderful playground of reference,
these color patches and then the language people
produce in response is also very colorful.
Very first model to think about here is my most basic speaker.
And we're gonna do this in these kinds of encoder
decoder neural frameworks that I'm sure you guys have heard about.
This is a simple version of models you might have seen.
So down here, I'm gonna assume that what comes in is
like an RGB representation of the color.
So just three floats.
All right, this one happens to be HSV,
I think but some kind of color representation comes in.
You might do something to embed that color,
like some kind of transformation of it to make it higher
dimensional or maybe more cognitively interesting. So that'll happen.
And then you might map that into
a hidden representation which I'll call the color representation there in orange.
That's the encoder part.
This is a very simple encoder which might be just kind of a,
a feed-forward model that produces this hidden representation.
The decoder part for our speakers is more interesting.
So the way the decoding works is,
we just assume they generate a start symbol.
That's where they begin to speak.
That gets an embedding representation just like any other word would.
And then you produce a hidden representation which is
a function of the color representation and your word rep.
So this is like where we've chained them together with this red line.
These two have to have the same dimensionality,
because this color rep will be the initial hidden state of the decoder.
This is kind of the input.
And then the model makes a prediction,
and what it does here,
this is a kind of subtlety with what I've written as dark.
It's making a prediction that is a probability distribution over the entire vocabulary.
So this is a very high dimensional probability distribution.
Uh, that's an idea that can take some getting used to it.
It's a kind of miracle that this works because it is a classifier,
but it's a classifier into
this incredibly high dimensional space but things work out all right.
That's where you get your error signal.
So the truth in the data might be that the first word is light,
uh, for this color that we're describing down here.
And that- that's a one-hot encoded vector and you get a standard kind
of error signal there. Here's a kind of summary.
So embedding derived representation coming from these guys,
predicts a distribution maybe like with a Softmax classifier,
and that gives you your error signal here.
I've called this teacher training at the top
because even though we produce the word dark,
the actual data is light.
And so in teacher training here,
we just make light the next word.
So that is the input.
We look up its embedding.
We grid hidden representation from H1 and x37,
they come in and then we make another prediction blue.
And in this case, the error signal we did a good job there and we continue.
So blue, look up its embedding, hidden representation.
We predicted green, but the actual thing to do was to stop to produce,
produce the stop signals.
So that's another error signal for the model.
And in teacher forcing,
the model will stop even though it didn't produce the end token. That make sense?
This is a generalization of our model for NLI, right?
Where you ca- what, what we did for NLI was just look at the final output state.
That's where we get our label.
Here, it's like we get a label at every turn.
And then the other twist which might take some getting used to is
this teacher forcing idea so that even we- though we made a wrong prediction here.
We insert the correct prediction down here.
In actual practice, when you train these models,
I guess best practice is kind of to mix some teacher
forcing with some more exploratory work where the model,
even though we're like predicted the wrong thing here,
you actually insert dark as the next word and let it explore a wider part of the space.
So that's how training works.
And then at prediction time,
again, a color comes in.
That's what's given to us. Look up its embedding.
Get a representation, and then we have to just go on our own here, right?
So produce these predictions and stop making predictions when we hit the end symbol.
And in that case, here we produced this presc- description dark blue.
Does that make sense?
I don't know how many of you have seen these models before.
Um, on the one hand, it looks just like the NLI model.
On the other hand, when you think through how
training has to work and how prediction have to work,
this is a bit of a twist here.
The way you use the predictions as inputs to the next step but it is straightforward.
If you wanted for example to implement this straightforward to extend the PyTorch,
NLI model to do this kind of thing.
In a paper that I did with Will Munroe and Noah Goodman a few years ago,
we did exactly this model using the XKCD dataset.
We did a Fourier transform to get our initial representation here.
And then, another trick that we did is,
so that the model wouldn't forget what color it observed.
We appended the color representation to all of the inputs.
So that was kind of like at each token.
Remember that you're trying to describe this particular color and I think that helped it,
kind of, be guided in the expected way
by this little bit of grounding that we're giving it.
So [inaudible] to generate the weights and then use
the weights or was it actually build a model that is good to describe in colors?
We wanted the model that would describe colors.
Um, and then- so this is another nice side effect of working in a simple domain.
Uh, even though this is a big crazy neural language model,
we could actually get a lot of analytic insights about what it was
doing by looking at its representations in color space.
Uh, because like we can give it as many colors
as we want and see what things it produces,
and then we can look at the resulting representations and you could see like for example,
that it had learned that greenish typically excludes green in a kind of interesting way.
Um, stuff like that. But fundamentally,
we just wanted a model that would do
multi-word color expressions of the sort that are in the dataset.
Just kind of thinking again, with what mentioned earlier
It'll be really interesting to see if like you took
the NLI images along with the read captions and fed it through something like this,
if you get a contextualized.
weight vector, it's kind of [inaudible]
I like- I like it.
It's there for you with SLNI you've got images, you've got captions.
Yeah. I'm surprised more hasn't been done with it.
Here's a small twist that brings and what it might be a more interesting corpus.
So again with Will and some people here at Stanford, Robert Hawkins and Noah and,
um, and me, we collected this colors in contexts corpus.
So this one is different in the sense that instead of
describing color patches in isolation,
people were given three color patches and
told which one was their target to describe to a listener.
And the idea was that they succeeded to the extent that
the listener could figure out from the same context which one was their target.
So here you've got some, you know,
a lot of simple color expressions in these very easy
to distinguish contexts, blue, purple, green.
The per- person just said blue.
But in this case here where there are two blues,
they said the darker blue one. And I love that, right?
Because that's grounding, that comparative the darker blue one is
making implicit reference to the fact that there are two very similar color patches,
and it's picking the darker one.
And this is even more explicit.
Teal not the two that are more green.
So these are all greenish colors and this person
was like very explicit about picking out that one.
Dull pink not the super bright one.
Not any of the regular greens very rich context reference.
And then I love these two. So purple and blue.
This is exactly the same color patch,
and you can just see that depending on
the context they were in people described that color differently.
It's a classic thing about colors that even your perception of them
can be somewhat influenced by the competitors in the space,
and here you see that reflected linguistically.
And you can have a simple speaker that does this.
I'm gonna return to this model but even this baseline is interesting,
where you just have it process three color descriptions,
embed them in the same space.
You, you just designate that like maybe the one closest to the end is the target,
and then have it described.
And what you get out of this model is color descriptions that
are implicitly context sensitive in the way that I just showed you.
We did that same trick of reminding the, um,
thing what the target referent was through its training procedure.
And here's some related ideas.
So you could think of what I just showed you with
color describing as a special case of image captioning,
which is a task I'm guessing you guys have heard about.
It was revolutionized a few years ago in part by researchers here at Stanford,
um, by, you know,
having these captions be generated by a neural encoder decoder.
Um, for those models,
the encoder part is typically much more involved than the one I just showed you, right?
All we had to do is embed the color in some interesting space.
But here we might have to embed an arbitrarily complicated image.
But the basic structure is the same, you know,
do some processing to your input object,
and then produce some language.
And then anticipating the colors in contexts thing.
There were a couple of papers from these groups at Google that
explored image captioning as a kind of contextual task.
So the intuition there is,
if you give me three pictures of buses,
it's okay if I say bus,
bus, bus for my captions.
But what you really might want there is captions that distinguish the three buses.
So what you want is a model that has some incentive to say like,
school bus, tourist bus, double decker bus,
or something that is like that because of the context and the confusability,
this pressure to be informative in that context will lead to more interesting captions.
And then finally, just kind of more open-ended here visual question answering.
That's a task where you're given an image and a question text,
and together you have to produce what I
would call a grounded answer to that question in the image.
Those are my speakers. Questions.
[inaudible] for when you have the colors up?
Um, is there any literature on like given an utterance and three contexts say,
uh, to choose the context in which that
utterance would have been needed to distinguish between colors?
So not just the target but the entire context.
Right. Here I was thinking of
this because I read the teal not the two that are more green,
and then I was thinking okay,
which one of these would uniquely be the one that would,
that would be necessary.
I don't know if stuff that's work on that
but it's a very interesting thing to think about.
It's another perspective like you as a human,
if I say the darker blue one,
you learn that there are two blueish things, right?
And you're saying that for this one you learned a whole lot
about what the whole context needed to be like.
Just because of presuppositions that you have
about the things speakers will do in those contexts.
I love that. Yeah. And potentially
you could use this corpus to get some traction on that problem.
Would there ever have been any work done on image generation from a caption?
Yes, I have a few reference, that would fall under my listeners.
So I have a few references for that too, right?
Image shimmery- yes.
Yeah, that would be like a listener.
So for the listeners again,
you could have the same dataset.
People have done this, same task formulation we'll have this colors and contexts thing.
Um, except now we're given the description,
like light-blue, and this could be a standard sort of RNN here.
And this will be the crucial hidden state.
You might do some transformations on
that hidden state to kinda move yourself into color space.
And then what you're trying to do is you're given this context and it says,
one, two and target.
So imagine this one is the target here.
That's what you want your agent to pick.
So you could do some kind of transformation in work that I've done.
We've done a Fourier transformation to put these into a more interesting space.
And then use this hidden representation or some statistics derived
from it to create a score for each of these elements of your context.
And then those scores could be fed into a softmax classifier that finally decides
based on all of this stuff which of these things
was actually the target of the description.
So it's a little bit more involved because of
the structured nature of the space on the right,
how to be a listener here.
But it's entirely doable and again colors are cool
because like you could just make some simple assumptions about what these,
uh, representations should be like to construct all the scoring stuff.
And here are a few other things- other ideas.
Look into your question.
So first, just for a framework here.
Even a simple classifier in my terms as a kind of listener, right?
You consume some language like a document or
NLI entailment pair or a sentence about a movie.
And you make an inference into a structured space.
And that's the kind of simplest version of this.
And then the other extreme I would say are the models that Bill is going to talk
about on Wednesday like semantic parsers where you consume some language,
construct maybe a very sophisticated latent representation of that language,
and then make a prediction into a highly structured output space.
And then more keyed into what I just showed you for example is like scene generations.
So the task of mapping from language to structured representation of visual scenes.
And here are some papers that do this.
A common theme that runs through them is,
what you actually do is map from language into like,
uh, an attribute value structure of an image which then some other process produces.
Because you kind of need some of
these aspects of the highly structured output space to get traction.
But even that could be really interesting like in
this work with Angel Chang and colleagues we, um,
went from language to attribute value descriptions that could be fed into Pat Hanrahan's,
um, 3D visual scene generator.
And then it could produce these really rich visual scenes.
And then, this is another important paper.
This is the image Flickr30k paper from Julia Hockenmaier and her group.
And their idea was to learn visual denotations for linguistic expressions.
So one version of this, uh,
of this problem. Question? Yeah?
So it seems like maybe the way that you've,
um, mentioned this for the color, uh,
task is that the speakers and
the listeners are sort of disconnected that we have, you know,
dataset of like [inaudible] says we're training a speaker to kind of match those.
And we also have dataset of utterance colors and we're training listeners to go from that to,
uh, from the language to the colors.
But I feel like there's,
there's also some research right?
Um, just like end to end like a speaker is generating, um,
a message that is directly going to
a listener that then needs to try and recover the initial thing.
Yes.
Does that falls somewhere like-
I think that's next, right?
That would be like a chatbot?
I think if I understand correctly,
if I don't- if the model you have in mind is different from the one I showed you just,
just ping me again.
Um, but I think you're describing a chatbot,
and my pitch here will be that our chatbots should be grounded.
So here's a standard kind of neural chatbot setup,
where you have some agents say, "What's up?"
And it produces like a hidden representation.
Then you have this bridge into another RNN, the decoder.
And this thing is tasked with producing utterances,
and it does it in exactly the way that my basic speakers did in the color space, right?
With the- the teacher training and so forth.
So what's up, not much.
Uh, there's lots of work on this.
A lot of it came out of Stanford.
I think a lot of progress has been made on this problem.
But I do think that you're kind of fundamentally stuck,
if what your model is doing is just reading in lots of dialogue text,
and trying to mimic what is in that text.
I think the lack of grounding is always going to limit
the ability of these chatbots to do interesting things.
Um, especially did make a lot of progress here like,
getting them to be consistent in their utterances,
and kind of getting them to produce
interesting dialogue so that they don't degenerate into just saying,
"I don't know," or "I hate you," which is
a common strategy for these things to fall into.
And actually one of the ways that he overcame some of
these limitations were with what I would call grounding.
So for example, to enforce
consistency so that they wouldn't in one dialogue turn and say that they had children,
and then in the next say they had no children.
Um, he actually created like user embeddings for the different agents.
And that was enough of an anchor to enforce a lot of the consistency that he was after.
But what I'm gonna describe to you here is an even richer form of grounding.
This will be like task oriented chatbot.
And this is grounded in a
really wonderful couple of papers that came from the Facebook AI group,
uh, the first paper is called Deal or No Deal,
it has something that comes after that, um,
they released a dataset of about 6,000 dialogues that
are grounded in about 2,000 unique scenes,
uh, this is the interface.
The gist of this is that you have
two agents that are trying to negotiate on a trade of some objects.
These are the objects in this scene.
And for this agent,
they've been valued differently.
So the book has a value of 8,
the hats have a value of 1,
and the balls have a value of 0. So that's the first thing.
And then each agent is given a subset of them.
So this agent has one book,
one hat and zero balls.
And the agent that they're playing with has a similar,
uh, or may- you know,
a different value function and a different set of the objects.
And the idea is that they want to maximize the reward by via a negotiation.
And it was nicely set up because in every single scenario,
there's a little bit of tension.
So it's not the case that both of them can walk away completely happy.
There has to be some compromise,
and that's what leads to the kind of interesting negotiation setup.
Here's what the data actually look like.
It's kind of confusingly encoded for a human,
but may be easy for your neural model.
So this is the representation for- there,
there are two perspectives; you and them.
This is the you perspective.
And the string of digits up here is saying that you have 1 book worth 0,
4 hats worth 2 and 1 ball worth 2.
And then you get this little dialogue, and then the outcome.
So in the end after this trade,
you have 0 of item 1,
or item 0, sorry.
4 of item 1, and 0 of item 2.
And you get a little boundary symbol.
And this encodes your reward for this negotiation,
and whether or not you agree to it,
and then finally down here that's
a representation of the goal function for the other agent.
These dialogues in the data set that they released are all paired like this.
So this was you perspective and here's them.
It's the same dialogue, but different outcomes,
different goal function, and of course this was the
you one that I showed you before in orange.
It's just been flipped.
So you get both perspectives.
And the reason for that is, because of these different goal functions,
the rewards are different,
and even the agreement can be different.
Does that make sense? And I think I can clarify about the task.
I think it's kinda masterful and it's,
it's got some subtle complexity that
generated really interesting social dynamics I would say.
Here's their model. Oh go ahead.
[inaudible] the other side?
Because I think they started off with a book and they got a book?
You are much faster at this than I am.
I'll double-check. [LAUGHTER].
[inaudible] Got it. Okay.
It's all I can do to remember the ordering of these things up here.
That's why I put these reminders to myself.
[LAUGHTER] Here's what their model looks like.
There are- and obviously there's a huge amount of information,
but I think at a high level,
you can kind of get the gist of what they're doing.
So it's a bunch of RNNs,
they happen to use these GRUs.
So Gated Recurrent Units,
it could be LSTMs.
Um, they have a goal encoder that just
encodes in its own embedding space and for its own hidden representations,
the goal here like, 1,
0, 4, 2, 1, you know,
that's the goal sequence I showed you before like,
object value, object value.
And then this is HG.
This'll be an important state,
because you'll see it in other places in the model.
But just imagine that the agent has consumed that goal,
um, representation and produced HG.
Then you have a dialogue encoder.
And what that thing is doing is just being a kind of chatbot,
so producing language, so consuming and producing language,
uh, and it does that,
HG gets appended to all of these tokens here.
Again, I think that's just like the color thing.
It's a kind of a reminder about what the goal was in this diffuse neural sense.
So it works like a chatbot there.
And then finally for the output encoder,
this is a kind of nested thorn of- a thorny nest of these attention mechanisms.
I think I've captured the gist of it.
So for the output encoder here,
you notice that you have h_1, x_1, h_2, x_2.
That's the conjunction of these representations down here.
They produce their own bidirectional RNN,
which is fed through these attention mechanisms.
HG appears again through the summary repres-
concatenated with the summary representation from all this attention business.
And then that representation is the basis for a bunch of different classifiers,
item 0, 1, and 2,
predicting the number of objects that were obtained after this negotiation happened.
So there's a lot going on here,
but the high level summary would be like,
a neural encoding of your goal,
be a dialogue agent,
and then a neural classifier for the different output states,
for the different items. Makes sense?
They do an interesting thing, which I'll return to.
But just at the high level.
So the dataset is kind of small.
So what they do is they train two agents and then fixed one, fix one.
And then another agent can interact with that first
fixed one and start updating its parameters. Here I've walked through that.
So agent A, that's the one we're training,
it reads in the goal,
it writes out some dialog, and then agent B,
this fixed one responds,
because it hit this end token here.
So the agent B produces some stuff.
And that could continue,
these agents could go back and forth.
And then finally agent A does this classification thing and gets a reward.
And now you could do a reinforcement learning thing of
feeding that reward back through the parameters for A.
And in that way,
even without any more human data potentially,
agent A could get updated.
And maybe get better at this negotiation task.
And here I mean, uh, in principle,
agent B could be a human,
uh, helping to train the system.
But I think they just use their pre-trained one.
Another really nice insight from the paper is that
even this agent here is not especially well-grounded.
Certainly this one is just kind of
operating in the world of language and representations.
But it's not making use of the reward.
Here you see the reward come in.
And then what they did is this thing that they call dialogue rollouts,
so in this case,
these agents make predictions about how the negotiation is gonna go.
Get a reward signal,
and then that's fed back through,
and it helps them make local decisions.
In the way of like,
uh, Markov decision processes.
So like here I am in my state,
make predictions about the future, get a reward,
do that for a bunch of different times and pick the, the state transition,
that is like the dialogue move,
that will give me the highest expected reward from this rollout.
And that's really nice, because then you're making very rich use of
the rewards for these local negotiation decisions.
That's my fast high level summary.
I do encourage you to look at the paper for more details.
There's just one more thing that's fun that I wanted to say about this.
So there was an amusing media narrative.
You guys might have heard about this paper without even
realizing so far that you've heard about it.
So this is the relevant passage.
"During reinforcement learning, an agent A attempts to
improve its parameters from conversations with another agent B.
While the other agent B could be a human,
in our experiments we used our fixed supervised model that was trained to imitate humans.
The second model is fixed as we found that updating the parameters of
both agents led to divergence from human language."
Do you guys remember this now?
Okay. So then it had a blog post,
which is also pretty,
you know, pretty modest about this.
The second model is fixed,
because the researchers found that updating the parameters of
both agents led to divergence from human language,
as the agents developed their own language for negotiating.
Okay. So then Newsweek.
"The bots ran afoul of their Facebook overlords
when they started to make up their own language to do things faster.
Not unlike the way football players have shorthand names for
certain plays instead of taking the time in
the huddle to describe where everyone should run.
I don't know about the football analogy,
but at least the first site part of this is pretty clear sighted.
It's not unusual for bots to make up a lingo that humans can't comprehend.
Though it does stir worries about these things,
that these things might gossip about us behind our back.
Facebook altered the code to make the bot stick to plain English.
I think that's still pretty good as a report about what they did."
This next one is different. Tech Times.
"Facebook was forced to shut down one of
its artificial intelligence systems after researchers
discovered that it had not star- that it had started
communicating in a language they could not understand".
And it gives examples.
And the examples are like,
the bot saying, "I'm me me me me me me me me Ball."
That's the language that they had discovered on their own.
[LAUGHTER] Which, I mean,
it is kind of interesting to think that for
these agents that might be a meaningful negotiation move,
and maybe you're creeped out by that.
But I think this is kind of overblown.
It doesn't stop there.
The incident evokes images of the rise of Skynet in the iconic Terminator series.
Perhaps Tesla CEO Elon Musk is right about AI being the biggest risk we face.
I mean, you've seen the model.
I think it's a little bit overblown.
[LAUGHTER] Back down to Earth.
If you're interested in these problems,
here's a bunch of other datasets.
Uh, some of these are pretty hard, um,
but there are also instances of doing
task-oriented dialogue and I think you can have a lot of fun exploring with them.
Okay. Last phase of what I wanted to do.
I can at least give you a flavor for this.
This is what I've called reasoning about other minds.
Here's my little motivating example,
which will remind you of Grice,
if you've read Grice.
So they have these two agents here,
and just imagine that they're referring to these two cylinders.
So this agent is going to say,
"The blue one please," and it has in mind this one on the left, this agent.
The idea is that this agent is thinking,
my listener knows that I'm cooperative in the Gricean sense.
And what that really means is just that, um,
I'm trying to communicate in an open and honest way,
and my listener knows that.
And we kind of collectively know that both of us can leverage information
about the context and what we know about being human and so forth,
in order to figure out what we mean.
So they'll be able to work out,
that I mean the unmarked blue one.
The idea there is that since you just said blue and not something like baby blue,
that it must be the unmarked form of blue.
And then if you're the listener you think, "Wait a second.
The blue one, that's ambiguous."
But I was assuming that the speaker was cooperative.
That's this kind of collective social thing.
Uh, but if I assume that they have- would have picked a marked form like baby blue,
if it were true, then it must be that they mean the unmarked form.
Like, if they could have been more informative in that way with their utterance,
they would've been and they weren't,
so I'll default to assuming they mean the unmarked one.
And in that way, they get this kind of mind meld around this one.
And the idea is that this would be completely different in this context, the blue one.
This is the same color blue that I had over on the right here.
But now the blue one would just obviously refer to this one. No confusion.
So that's the kind of dynamic that we're trying to capture.
All these thought bubbles,
the idea is that if our systems could do this at some level,
then they would be more efficient in communication.
And here's a simple model for doing that.
It's called the Rational Speech Acts Model.
It was developed by people here at Stanford.
And it has a kind of hierarchy of three probabilistic agents.
So the literal listener here, observes a message.
This is like the listeners I showed you.
And we're assuming some language that they speak, Lex.
And on the basis of that message in that language,
makes a guess about what the world is like.
And they do that based on a prior over the world, P(w).
And also just this core lexical information,
which you could think of as kind of just you-
your knowledge of the language that you speak.
So that's the literal listener.
The pragmatic part about this is that we have a pragmatic speaker.
Now, speakers in this model observe the world and the language,
and then they produce a message,
just like the speakers that I showed you before.
And the idea is that this speaker does this based on the literal listener,
that's the purple here,
and some information about how costly different messages are.
How marked they are or something.
But the important thing here is that the speaker
is not just reasoning directly about the language,
this Lex, but rather about a listener- literal listener who reasons about the language.
So that's that one layer of indirection.
And already there, you get
a richer communication system than you would've
had if you were just reasoning about the language.
And then finally, this pragmatic listener,
here you have two layers of indirection.
So it's a listener,
it observes a message and makes a guess about the world,
but it does it by reasoning about the prior over worlds,
and the speaker who's reasoning about the literal, literal listener.
This back and forth is kind of mirroring those thought bubbles,
where it was like, "I'm a listener,
and I imagine I'm a speaker.
Oh, and then I can work it out on the speaker side as well."
You can reorient this model however you like.
But the important thing here is that,
if you're a pragmatic listener or pragmatic speaker,
you don't reason just about the conventions of your language,
you reason about agents who are using those conventions.
And from that, you get additional meaning.
Here's a simple example.
So I have just a very simple case here.
We have two messages beard and glasses.
This is David Lewis with his magnificent beard.
He was a philosopher.
And this is Paul Grice,
he doesn't have a magnificent beard at this time in his life,
but they both have glasses.
So true, true, true, false.
At this level, if you, uh,
were a listener and you heard beard or glasses,
beard would be unambiguous for you right away,
but glasses is true of both these entities.
So if you heard it, you would have to make just a random guess
about which of these two entities the speaker was referring to,
if you were a kind of mechanical semantic, uh, listener.
If I perform that speaker calculation,
that speaker agent that I just showed you,
already now, surprisingly I have an unambiguous system.
If I want to refer to Lewis,
I'll probably say beard.
And that follows from the fact that if I want to refer to Paul Grice down here,
my only option in this constrained signaling system is glasses.
And that creates this kind of separation.
And then when we go one layer further, now beard,
that unambiguously refers to Lewis,
because Grice doesn't have a beard.
But notice that that 50-50 is now 25-75.
A lot of bias in favor of hearing glasses and thinking it's Grice.
And that is just like that, uh,
dynamic I showed you before where you kind of were literally just
mathematizing the idea that if I wanted to refer to Lewis,
I could've done it with beard,
and since I didn't, you must mean glasses.
You must mean Grice. And the model has more or less
just turned that into a little probability Bayesian model of that reasoning.
That's a wonderful model. It's been incredibly successful in linguistics and psychology.
Lots of good experimental evidence that people are doing something like this calculation.
I think a lot of problems in pragmatics were solved quite neatly by this model.
But if you wanna think like a computer scientist, it's pretty limited.
Because in all that work,
you have a hand specified lexicon,
and then when you perform that speaker calculation,
if you take the model literally,
you have to reason about every possible alternative message they could have produced,
which for a language like English,
is not even a finite number of things, presumably.
And then it's a very high bias model,
because I hand specify this, and there's no learning.
I don't have many chances to learn that.
For example, people love to say beard for David Lewis,
because his beard is so visually salient.
The colors in context
corpus is a setting in which we can think about this as a machine learning problem.
It's too large for a hand specified lexicon,
but maybe we can think about the speakers and listeners I showed you before,
and whether they could solve this problem.
So here's how this might go.
The literal neural speaker is the one I showed you before.
Let's call that S_0.
It just produces the context,
sorry, consumes the context,
and produces a description.
Presumably, of the target here.
The neural literal listener is the one that I showed you before.
It consumes language, and together with the context,
makes a guess about what the referent was in that context.
Those are the literal agents,
the ones that we would have hand specified in the previous mold.
Now we just have these kind of black box neural models doing this work for us.
And then in the simplest version of this model,
you could just apply that RSA calculation on top of those two agents.
So here's the speaker.
It's gonna depend on that literal listener,
which is the neural listener I showed you.
And this is kind of cool.
Instead of having to worry about all of
the conceivable utterances that are alternatives to one,
the one that was chosen,
you could just sample from that speaker,
um, and use that as an approximation of this denominator here.
Once you've done that, your problem is solved,
the pragmatic listener is just an application of
the RSA calculation on top of this S1 here.
So in that way,
you've got these pre-trained models,
they're done, you apply the RSA calculation.
And presumably, those agents can do better at this task,
because they're effectively reasoning about each other.
And I'm running out of time here,
but I wanted to mention also that another way of solving
this problem here is to just change the space of atomic messages,
and that's what my student did.
So he did what we call pragmatic image captioning,
which is like that work I mentioned from the Google groups before.
Um, and here he applied RSA to a standard image captioning system,
and he actually instead of sampling from S_0,
reasons that- applies the RSA reasoning step at the level of individual characters,
and shows that that leads to better and more informative captions
in these contexts than alternative simpler models.
And that's like a wonderful instantiation of this idea,
this intuition that for informative image captions,
I should be thinking about the context I'm in,
and trying to produce a caption that is distinguishing my target from the rest.
And here's a bunch of other work.
Some of it with different kinds of corpora associated with it,
some of it from robotics,
some of it from semantic parsing,
lots of datasets, some newer developments down here that you might wanna pursue.
I think this is a very exciting space.
And then here are a few other corpora.
TUNA is a classic one,
that's like those philosophers I showed you.
I- don't ask me why it's called TUNA,
I can never remember.
Uh, SCONE is a newer one from Stanford.
And then Robert Hawkins actually released a crowdsourcing engine
that would allow you to collect your own reference games if you wanted to.
For this final step here,
I just wanted to gesture at a few ideas.
I know we're out of time, so I won't go through them in detail.
But like here's just,
again the simplest case of this sarcasm detection.
There's a wonderful book Corpus for this called SARC.
Sarcasm is a classic case of a problem
that you will not solve with looking at text alone.
It is not a fact about, uh,
a statement like, "Great idea,
that it is sarcastic or not."
It is sarcastic when used by certain people in certain contexts.
And this SARC corpus is derived from Reddit.
It's an opportunity there because you could learn user embeddings, form embeddings,
immediate dialogue context embeddings,
and show that all of those things are influencing the decision
about whether an utterance like great idea is sarcastic or not.
And in this deep learning mold, that's easy.
Just add more representations into your model and see if you can learn them effectively.
And then I pointed out a few other cases.
This is an example where you could do something similar,
but in the context of social graphs,
which would be another kind of grounding,
kind of keying into your intuition earlier about knowledge graphs.
PLOW, this is wonderful work from James Allen
where the context that you're in is a webpage,
which is very highly structured.
Uh, it's just another way to think about how you can get beyond
the limitations of language by actually having a rich representation of the context.
Here it's more symbolic.
But there are lots of webpages out there.
So there's no shortage of these structured contexts.
And then here's some work that I did with Adam Vogel,
that is more in the mold of like, um,
old-style decision process models.
And here I just wanted to point out this is- this is just wonderful.
So just give me one minute.
So we had these agents play this collaborative game,
where the- their goal, it's kind of hard to see,
but their goal was to just be co-located with this card here.
They succeeded if they both found it in some sense.
And they could communicate,
and we didn't color code anything about their incentives to actually communicate.
We just said, "You have this ability to,
uh, understand language if it comes in,
and to produce it if you wish," where speaking was an action,
just like moving around in the board was essentially.
And these agents get trained over a long period,
after just a few hours of policy exploration,
which is like training for these models.
They would do funny things like,
this agent found the card,
but what it said was top,
and that sent the other agent off toward the top.
You can see it has a high probability for thinking that the card is at the top.
It's a very bad state for them to be in.
They learn pretty quickly from this kind of grounding,
together with their reward function that they should not use misleading language.
So after a week of policy exploration,
now they produce true utterances.
And about that same time,
they realize that it- there's a reason for them to produce utterances.
Because if they do,
they can quickly influence the actions of
the other agent and they solve the problem quickly.
And since these agents are set up where every moment for them is misery,
all they wanna do is find this card so that they're freed from this misery.
Um, they pretty quickly learn that they should produce utterances,
and actually true utterances.
And that's a kind of Gricean thing that we claim was emerging from their grounding,
in this particular goal,
in this particular environment.
And I thought that was very exciting.
The problem with these models is that they're really intractable.
And I think they're just waiting for, um,
you know, the day when they get replaced by neural approximations,
which might be less crisp in terms of what they learn for policies,
but more able to deal with changing environments,
and complicated environments, and so forth.
Uh, here I think that kind of keys me up for these next frontiers here.
So what I expect is much deeper integration with devices and the environment,
more sophisticated reasoning about goals,
better tracking of dialogue history,
and as I said just now,
approximate states- state representations to address these scalability issues.
I think progress here is gonna lead us to agents that are truly grounded,
and that will lead us to agents that are truly
able to use language in a sophisticated way.
Okay, thanks.
 Today, we're gonna look at the topic of semantic parsing.
And this is my favorite topic of the class,
and it's one of my favorite topics in NLP altogether.
Um, when I was at Google,
my team was responsible for doing
semantic query parsing for Google Search and for the Google Assistant.
So we applied the semantic parsing techniques that we're gonna look at today
to billions of queries every day and in 40 major languages.
Um, now, I'm at Apple,
and we use very similar techniques for query understanding for Siri.
So semantic parsing is highly strategic for Google,
and Apple, and Amazon, and Microsoft.
Uh, and it's also been a very hot topic in
academic research over the last five or ten years.
Now, semantic parsing is a big topic,
it's a complex topic, uh,
I think more than any of the other topics that we've covered,
it draws on concepts from linguistics and logic.
Um, so I can't possibly do the topic justice in,
uh, just the next hour that we have.
But fortunately, if this topic catches your interest,
we have an abundance of material on the website that can help you go deeper.
Um, we have a series of four codebooks that
introduce a simple semantic parsing system called SippyCup.
Um, Chris has made a few screencasts that
explained the main ideas in very simple and approachable terms.
And there's also this terrific paper that Chris co-authored with Percy Liang in
2015 that I think conveys
the main ideas of semantic parsing very clearly and very concisely.
For today's lecture, I think the best that I can do is, uh, first,
to give you some high level motivation for why
this is a problem that you might care about,
why this is an interesting and impactful problem.
And second, to- to describe
the standard approach to semantic parsing in- in very high level terms.
So let me begin by talking a little bit about the motivation for semantic parsing.
Um, at this point in the course,
you might be asking yourself, "Wait,
I thought we were supposed to be doing natural language understanding,
but I still don't know how to build C3PO.
I still don't know how to build a robot that can really understand me."
Uh, the stuff that we've looked at so far, um,
vector space models of meaning,
sentiment analysis, relation extraction.
They seem to capture aspects of meaning or fragments of meaning.
But there's still a lot that's missing.
We still don't know how to generate
complete precise representations of the meanings of full sentences.
And there are still many aspects of meaning that we don't know how to capture.
So for example, things like higher erroty relations,
or events with multiple participants,
or temporal aspects, like this example, uh,
Barack and Michelle Obama got married in 1992 in Chicago.
Or logical relationships as in,
no one may enter the building except policemen or firefighters.
As an example, try to imagine building
a natural language understanding system that could answer logic puzzles like this one.
This was actually the very first project that I worked on when I was a young,
eager PhD student became- before I became disillusioned.
Uh, this is a type of logic puzzle that appears on the LSAT exam,
and it used to appear on the GRE exam as well.
Uh, this is a typical example,
and this is basically a constraint satisfaction problem, a CSP.
So you have six sculptures which you need to assign to
three rooms while respecting some constraints that are given there.
The interesting thing about this type of puzzle is,
it's difficult for humans and it's difficult for computers.
But what makes it difficult is completely different for humans and for computers.
For humans, understanding the language is easy.
And in fact, the authors,
I guess ETS, uh,
has taken great pains to ensure that the language is clear and unambiguous.
They really wanna avoid any misunderstandings,
where like, the human readers just didn't get it.
So they work hard to make the language very clear and unambiguous.
For humans, the language is easy, but solving CSPs,
doing the logic part of it is hard,
and that's what this is supposed to be testing.
For computers, it's the other way around.
For computers, solving CSPs is easy.
In fact, it's trivial.
For computers, understanding the language is hard.
If we could build a model that would automatically translate
this text into formulas of formal logic,
then a theorem prover can do the rest.
It'll be completely, uh,
trivial for a theorem prover.
But that translation, translating this into formal logic
is far more difficult than it might at first appear.
Uh, and to illustrate this,
I wanna take a close look at just a few specific challenges that might arise.
So our challenge is, build a system that can
translate these words into formulas of first-order logic.
Um, one challenge is that many words have semantic idiosyncrasies.
So an example is the word, same.
So consider this sentence,
"Sculptures D and G must be exhibited in the same room."
If you translate this into formal logic,
you're probably gonna get something like this.
So I've glossed over some details here like I didn't
worry about existentially quantifying variables X and Y.
But I think it's enough to convey the- the basic idea.
What this says is,
if D is in X,
so D is a sculpture and X is a room.
If D is in room X and G is in room Y,
then X and Y must be equal to each other.
Okay. So that seems straightforward.
But the point I wanna make is that this adjective,
same is a funny kind of adjective.
And to make that clear, let me compare it to some other adjectives.
Let's instead look at this sentence,
"Sculptures D and G must be exhibited in the red room."
Think about what that logical form for that would look like.
It might look something like this,
"If it starts off the same way,
if D is in X and G is in Y,
then X is red and Y is red."
By the way, I'm using sort of a list be- list- like,
list- like syntax here rather than standard first-order logic syntax,
but hopefully, it's still under it doesn't really
matter what the syntax is and hopefully it's still understandable.
Uh, you might quibble with my logical form here because, well,
I didn't properly account for the semantics of
the- the definite determiner which is a very rich topic in itself,
but I think that's, uh,
an issue that we can safely put to the side,
it's not really the main point here.
The real point is, when I say the red room or a red room,
maybe it will be better, um,
I'm just, uh, asserting something about,
uh, the rooms that D and G are in.
I'm saying whatever room D is in,
it's gotta be a red room and whatever room,
uh, G is in,
it's gotta be a red room as well.
And if I look at other adjectives like if I change it to large room or smelly room,
it's gonna have the same kind of semantics.
It's gonna be the same mapping into a logical form. Does everybody buy that?
Do these logical forms look more or less right putting
aside issues like the- the definite determiner?
Red, and large, and smelly are ordinary adjectives,
they're what's called intersective adjectives.
They have semantics which are intersective.
Same is a funny kind of adjective,
it's called an anaphoric adjective.
And it- it- that refers to the- to the idea that the semantics of same,
um, uh, refer back to something earlier in the sentence.
The same room, it's not a specific room or even a room with a specific quality,
it's just whatever room D and G are in.
If you build- let's say you've successfully built
machinery that can do semantic interpretation of this sentence,
and this sentence, and this sentence,
it can successfully handle ordinary adjectives.
That same machinery is gonna break down when it gets to an adjective like same,
which syntactically looks identical,
but semantically behaves very differently.
And there are other examples too.
If you're an eager young PhD student,
you might think, "Oh, okay.
I ran into a problem. No worries. I'll just fix it.
I'll just make my model know about same."
But then you realize,
there are more problems like that.
There's other anaphoric adjectives like different,
which have similar properties and you might say, "Okay, well,
I'll just add another epicycle to my model and I can handle that one as well."
But then you start to encounter
more and more and more quirky semantic phenomena that are not anaphoric adjectives,
but have other kinds of idiosyncrasies that you haven't accounted for in, ah, in advance.
Let me give you a different, ah, kind of challenge.
The challenge of scope ambiguity.
And to introduce this, I'm gonna use,
ah, a joke from Groucho Marx.
It's not a very good joke, but it is a good example of scope ambiguity.
It starts off, "In this country,
a woman gives birth every 15 minutes."
Okay. I didn't know that, but all right.
And then he continues,
"Our job is to find that woman and stop her."
So this joke hinges on a semantic ambiguity.
When you read the first part, you're like, "Okay.
Every 15 minutes, there is some woman who gives birth."
But when you get to the second part,
he's giving that first sentence a different reading instead,
there is a specific woman who gives birth every 15 minutes.
That's the semantic ambiguity.
It's a question of whether the universal quantifier, every,
or the existential quantifier,
a, takes wider scope.
Um, because I'm a language nerd,
I collect examples of scope ambiguities.
That's a perfectly normal thing to do, right?
Uh, and I have another nice one which has exactly the same structure.
"Children, there is a time and place for everything."
You can see again it has the- the- the existential quantifier and universal quantifier.
So the, you know,
the- the standard reading is,
"For everything, there exists some time and place for that thing."
But the alternate reading is, ah,
"There is one specific time and place which is the time for everything."
Right? and the joke continues,
"And it is called college."
This is from South Park. I thought that
might resonate with you guys because you're at the university.
Um, so this is the- this is- this is the idea of scope ambiguity.
How does it relate to the LSAT puzzle that I just showed you?
Well, um, one part of that LSAT puzzle was this,
"No more than three sculptures may be exhibited in any room."
So this is interesting because
there's not just one possible reading here or even two possible readings,
there is at least three possible readings here,
which if you render them as first-order logic,
are quite different in terms of their logical form and their logical consequences.
But all are plausible readings of this sentence.
And from a computer's perspective,
there is no way to know in advance which one is the correct reading.
I think the most obvious one,
the one that the authors intended you to get was this one.
There cannot be a room that contains more than three sculptures.
But there's some other possible readings here.
Can anybody think of another way of reading this sentence?
No more than three sculptures may be exhibited in any room,
that has different- that would have a different logical form,
that has different logical consequences? Yeah.
Do you consider, uh, no room should have more than three [NOISE] sculptures across?
You know, like, across a specified- like you can't
have one and then take it out and then bring another one,
take it out and then bring another one.
So in the end it has three but [OVERLAPPING].
Oh, that's interesting.
I think that that says-
That's how- that's not one of the ones I was thinking of but,
ah, you're- you're touching on, ah,
something about whether we're talking about a single point in time or over time,
um, whether, ah, whether the assignment of sculptures to rooms could change over time.
Yeah.
Yeah.
Ah, is it that there cannot be any- no more than three sculptures,
as long as they present it again?
As long as they what?
They pres- they are in a room.
So a sculpture is in a room?
Yeah. You could- you could read this as,
"No more th- no more than three sculptures may be exhibited in any room."
Like, it doesn't matter what room it is.
No more than three sculptures maybe exhibited at all, in any room.
That reading of this sentence is maybe a
little bit less obvious and a little bit less accessible than the first one.
But I think there are ways of contextualizing this that make that reading plausible.
Are there any differences between the first and the second in this case?
Yeah.
Yes, there are.
Because in the first one,
a possible world- something- a possible configuration that's consistent
with the first sentence is you have
two sculptures in this room and two sculptures in that room.
But that is not consistent with the second reading,
which says, "No more than three may be- may be exhibited at all."
Totally at all.
Yeah. Exactly. So they have
different logical consequences depending which one you're reading.
Um, there's actually a third reading which I think is even harder to get
to but I- I claim is still a plausible reading of the first sentence.
And that one is, "At most three sculptures have
the property that they may be be- may be exhibited in any room.
For other sculptures, there are restrictions on allowable rooms."
So go back to that first sentence now,
"No more than three sculptures may-
no more than three sculptures may be exhibited in any room."
The other ones have restrictions.
Right? That's a plausible reading.
It's not the most obvious reading,
but it's a plausible reading.
And from the perspective of a computer algorithm,
like computers don't have intuitions about
which one of these things is more plausible or less plausible.
Um, so when you're building a system to- to map the sentences into logical forms,
your senten- your model probably needs to account for all of those possibilities.
Because in different contexts,
any one of them might be the correct interpretation.
Of course, you also hope that your model will be able to make
good predictions about which one is more likely and less likely,
uh, and a little bit later we'll look at ways of,
um, building a scoring model which will help you, uh,
choose among multiple possible parses,
multiple interpretations of a given input.
Um, I'm not gonna say anything more about the LSAT puzzle but I could go on.
There are lots more challenges.
I'll just mention one- one other off the top my head.
Um, if you, um,
there's nothing in the problem itself.
Let me go back to that problem.
There's nothing in the problem that says,
"A sculpture can only be in one place at one time."
There's nothing that says a sculpture can't be in two rooms at the same time.
But if you just map this into first order logic and then give it to a theorem prover,
the theorem prover will happily put one sculpture in two different rooms.
Because that's common sense knowledge that a- that
a sculpture can't be in two different places at the same time.
A human just fills that in automatically,
without even thinking about it.
But a computer doesn't know to do that.
So you ha- have to, ah, if you wanna make this work,
you have to also supply all kinds of
background knowledge which is kind of unstated in this problem.
So um, we started this problem, you know, ah,
as I mentioned at the beginning of my PhD and we worked on
it for about like three months and the further we got into it,
the more we realized we're not gonna solve this problem.
And that was like 15 years ago.
And as far as I know, this problem remains unsolved today.
This is a really hard challenge.
But it looks, at first glance,
like it should be achievable.
Okay. Here's a very different kind of example.
Um, this, this is taken from a consulting project. Yeah.
It's like graduate students kind of perspective on it.
Has anyone tried to do this with a more limited scope
and using something like a knowledge base for grounding,
kind of similar to what we talked about on this lecture?
I'm not sure.
Yeah. It's a good intuition though,
uh, that you could,
it can give you a lot of constraints that I was talking
about might come from the domain that you're in.
Uh, and that can be an interesting thing scientifically, uh,
over trying to prove it. Thank you.
Um, so here's a very different kind of example.
This is taken from a consulting project that I worked on when I was in grad school,
and I was working with a startup that wanted to build
a natural language interface to a travel reservation system.
So they wanted to be able to understand and respond
to natural language descriptions of travel needs like this.
The idea is you'd be able to, uh,
send an email to this service and,
um, they would automatically figure out some travel plans for you.
So what are some of the challenges to semantic interpretation that you see here?
There's lots of them. Yeah, what's,
what's an example of ambiguity here?
Ambiguity in Oakland or SFO,
you could use Sunday evening or Monday morning.
It's like we have to extract all of those and realize that they're on
the same sort of fill in the blank blank that they made.
Yeah. One kind of ambiguity here,
I mean, it refers to Oakland,
but it doesn't actually make it clear whether Oakland is
an alternative to SFO or an alternative to Boston.
Now as a human, you know that it's an alternative to SFO,
it wouldn't make sense to book a flight from SFO to Oakland.
Uh, might be fun.
Um, but, uh, but an automated system,
of course, you know,
it, it, that relies on world knowledge
which might not be available to an automated system.
What else could make this hard? Yeah.
I also just realized that the he later on,
the first back to the husband about the different flight,
has to be, the return flight has to be both of them.
That's right, you have to, uh,
resolve some anaphora here and figure out what that pronoun refers back to.
Um, and there's other,
there's other, um, [NOISE] uh,
kinds of reference resolution here.
Like, for example, Sunday evening or Monday morning,
we need to figure out that that refers to the Sunday that,
presumably that refers to the Sunday that follows that Friday,
not the Sunday that precedes that Friday, right?
So I guess we're talking about Sunday the 14th or Monday the 15th.
There's something else weird here which is that,
if Friday is the 12th,
then Wednesday can't be the 18th.
Like, mathematically that doesn't work.
The human that, that Friday and Wednesday are five days apart,
but the 12th and the 18th are six days apart.
The human made a mistake and the system has to deal with it somehow.
Like, a reasonable thing will be for the system to say,
"Did you mean Wednesday the 17th?"
Um, but unless you engineer that into the system,
it's not gonna be able to do it.
What other, what other challenges do you see here?
It seems like there's also some extraneous information like it,
uh, it's important that they don't want to fly on United,
but it, this model probably doesn't need to care about why.
That's right.
Or why has the husband is staying later.
Yeah. So how to focus on what matters and what doesn't matter.
Hating their guts is an idiom.
Um, so this, this gives you a,
a flavor of some of the
kinds of things that you run into when you try to do
complete and precise semantic interpretation on real-world problems.
This goal of seman- of
complete and precise natural language understanding goes way back to the beginning.
Uh, last time Chris mentioned the SHRDLU system,
which was developed almost 50 years ago by Terry Winograd when he was,
uh, a grad student, a PhD.
Uh, he's now a professor here and he
doesn't work on natural language understanding anymore.
But this was arguably the birthplace of natural language understanding.
So SHRDLU is a blocks.
It's, it's an NLU system that's grounded in a blocks world.
It parses the user's input.
Uh, maps it to a logical form and then tries to interpret that logical form in its world,
and answer a question or take an appropriate action.
Um, I give a you- link to a YouTube video here and it's worth checking out later.
The- these slides by the way are linked from the website,
and from there you can find your way to this YouTube video.
And it's kind of a fun video to watch.
Um, and so you can say,
even quite complicated things,
like find a block which is taller than the one you are holding and put it into the box.
And it understands and it just does the right thing.
And at the time, people were totally wowed by this.
I mean, I think even half a century later,
this kind of elicits a wow.
But certainly at that time,
it really had people thinking that human level NLU was just around the corner.
But then, that excessive optimism crumbled when later systems,
later systems tried to deal with
more realistic situations and with real-world ambiguity and complexity.
Part of the reason this was achievable was that it was
this very constrained blocks world that,
um, that we were operating within.
Another milestone for semantic parsing was the CHAT-80 system.
Uh, this was developed around 1980 by Fernando Pereira,
who's now a research director at Google.
So this was basically a natural language interface to a database of geographical facts.
So you could, uh, it could answer geographic queries like what's the capital of France,
or how many countries are in Europe or things like that.
Uh, it was implemented in Prolog and it was,
it used a hand built grammar,
uh, and a hand built lexicon.
So there was no machine learning in sight, it was just lots and lots of rules,
uh, that drove this, this semantic interpretation.
Now, here's something astonishing.
It is still possible to run CHAT-80.
Uh, even though this code is older than most of you,
you can still run it on the Stanford rice machines.
Uh, and if you want to try it yourself,
here's a recipe for getting started.
And it's actually really fun to, um,
kick the tires and,
and try it out and see what works and what doesn't work.
Um, in previous years, I've actually taken time in class to do this live in class,
which is kind of fun,
but it's a little bit cumbersome and I feel like maybe
the educational value of it isn't worth the, the cost.
So I'm not gonna do that this time.
But, um, I want to show you some examples of,
of some queries that, that it can handle.
So you can ask it, like,
is there more than one country in each continent?
By the way, what's the right, let's see how good you guys are at geography. No, why not.
In Antarctica and Australia.
Yeah. Funny thing is, uh,
this system doesn't say,
doesn't think that Australia is a continent.
It thinks that Australasia is a continent.
So the reason it says,
it does say no but the reason it says no
is because of Antarctica and not because of Australia.
Uh, what countries border Denmark?
It's a pretty easy question.
Yeah. Except, remember, it's 1980 [LAUGHTER].
Oh, the Soviet Union was there.
No. East Germany?
West Germany. West Germany, yeah.
It's- again, yeah, kinda, kinda,
kinda funny to step into the time machine,
back to the- back to the, um,
[NOISE] uh, Cold War and all that.
Uh, I'll skip the next two,
but this- this one is really impressive.
Which country bordering a Mediterranean- bordering
the Mediterranean borders a country that is bordered by
a country whose population exceeds the population of India?
CHAT-80 knows how to get this question right even
though it's incredibly syntactically complex.
Can anybody get this one?
[NOISE]
All right. I'll let you guys think about that one.
Its the country whose population makes [inaudible] the population [inaudible] is that Russia or?
China.
China.
So China is the only one.
Is that the one?
In 1980, I believe. [NOISE].
[inaudible].
I'll let you think about it.
[LAUGHTER].
No, I'll let you go try CHAT-80 on the rice machines,
you can ask CHAT-80 [NOISE] and CHAT-80 will tell you the answer.
Um, [NOISE] by the way, uh,
one of the things that I- that I- when I showed this slide,
one of the things that I get out of it is trying out small variations.
So if, if instead of saying,
what countries border Denmark,
if I said what countries [NOISE] touch Denmark or what countries adjoin Denmark,
it frequently just says, "I don't understand."
You change one word and it just says I don't
understand because that other word or that other formulation,
that other way of asking the question just wasn't in the hand-built grammar.
Uh, the last example kinda falls in the same bucket.
How far is London from Paris?
It feels like a question that has very similar flavor to all the other questions,
just another geographical question,
but it turns out that no matter how you ask this question,
CHAT-80 just says, "I don't understand."
It doesn't know about distances between- it knows about like areas,
it'll tell you how many square miles, uh,
you know, the total area of countries south of the Equator and not in Australasia,
but it can't tell you about distances between places.
So it has two pretty profound limitations.
One is, um, that it is
it only knows about certain things, it's certainly restricted to geography,
even within geography, it only knows about certain kinds of geographic facts.
That's one restriction.
The other restriction is on the ph- the phrasing the allowable phrasing of questions.
Even questions that it knows the answer to,
you have to express your question in a certain specific way.
Maybe if- maybe there are a few different ways that it can handle,
but there are lots of other ways that to a human are
perfectly reasonable ways to ask that the system just can't handle.
[NOISE] So on the one hand,
systems like SHRDLU and CHAT-80 are pretty astonishing because, um,
[NOISE] long before most of you guys were even born,
these systems demonstrated precise and complete understanding
of even quite complex sentences.
And, by the way, they did it while running on
hardware that's less powerful than you know my rice cooker.
Um, admittedly, I have a really fancy rice cooker, [LAUGHTER] but still,
uh, it's, um, [NOISE] it's kind of impressive what they were able to do.
But their coverage was extremely narrow,
it was limited to one specific domain,
and even within that domain,
they were very brittle.
If you ask a question this way, you get an answer;
if you use just slightly different words, nothing.
[NOISE] Today, by contrast,
we have systems that exhibit vastly greater robustness and with very broad coverage,
but with very fuzzy and partial understanding.
So you can ask Google about just about anything and it never says "I don't understand."
But it might not exactly ask your, uh,
it might not exactly answer your question if you ask Google this question about
what country- which country bordering Mediterranean borders a country that
is bordered by a country whose population exceeds the population of India.
Google is not gonna tell you the answer or actually what it
probably will do is it will link you to my slides from last year,
um, which maybe had the answer.
Um, but it's- it doesn't have
that complete precise understanding that's needed to actually answer the question.
Uh, where we wanna be is up here with C-3PO and the pot of gold.
We want to build systems that can understand precisely,
and completely, and robustly over broad domains.
[NOISE] Now, you might not feel super inspired by examples like that- that one,
what's the population of the largest continent, that- that one.
Um, because let's be honest, who cares?
But the ability to answer these highly structured kind of
database-y type of queries has lots of applications that are more compelling.
[NOISE] So, for example,
if you're a policy analyst working on global warming,
you wanna be able to answer questions like these.
And it's easy to imagine that you have a database that
contains all kinds of statistics about carbon emissions.
But unless you are a programmer,
you probably don't know how to write
SQL queries to pull out the information that you need.
So it will be great to have a system where you could just
type in questions like this and get the answer.
[NOISE] Here's another example.
If you grew up in America,
there's a good chance that you have an Uncle Barney who
is a total freak about baseball trivia.
Uncle Barney would love to be able to ask questions like these,
but again he doesn't know how to write database queries.
Um, but if you can invent a semantic parsing system,
um, maybe you can make a website and sell
subscriptions and Uncle Barney will pay you 10 bucks a month.
[NOISE] Ah, and here's an application that has become very important at Google,
and Apple, and Amazon,
uh, voice commands for getting things done.
[NOISE] Um, one thing to notice here is that
these queries are not well-served by conventional keyword-based search.
Just imagine if you ask Google,
"How do I get to the Ferry building by bike?"
And Google responded by showing web pages containing those terms.
That would be a disappointment because you don't want web pages containing those terms,
you want a map with a blue line on it.
And the only way we can draw that map is if we know where you are and we compute a route,
a route from where you are to the Ferry building,
and doing that requires understanding your intent in a complete and precise way.
So satisfying queries like these requires mapping
them into structured machine-readable representations,
with meaning, that we can pass to a downstream component,
uh, to take- to take action on.
It requires semantic parsing.
[NOISE] So the goal of semantic parsing is to do precise complete, uh,
interpretation of linguistic inputs and to map those inputs
into structured machine-readable representations of meaning.
[NOISE] So really important question is,
what kind of semantic representation are we aiming at?
What's our target output?
Um, and there are a lot of different possibilities kind of depending
on what the problem is and what the domain is.
Um, [NOISE] if the goal is to facilitate data exploration and analysis,
then the right semantic representation might be a database query language like SQL.
[NOISE] Uh, for a robot control application,
you might want a custom-designed procedural language.
[NOISE] For interpreting voice commands,
you can often get away with a relatively simple meaning representation,
which is based around a fixed set of
high-level intents that are parameterized by specific arguments.
So, for example, if the query is directions to SF by train,
the semantic representation might be
something that first indicates that this is a travel query,
so that's kind of indicating the intent here,
the type of intent.
And then, with two parameters.
One is a destination parameter and the value there is a Freebase ID,
which is the Freebase ID for San Francisco.
And then the second parameter describes the transportation mode.
And here the value is,
uh, I guess any num value,
there's maybe like five different transportation modes and transit is one of them.
And so this little machine-readable expression conveys everything that you need to
pass to a back-end system which can actually
like figure out the directions and draw a map and give it to the user,
uh, and similarly for other kinds of,
uh, other kinds of queries here.
[NOISE]
Okay. So to illustrate the central ideas of semantic parsing,
we've created a simple semantic parser called SippyCup.
Although it's designed for simplicity and readability,
it uses methods that are commonly used at Google and
Apple for semantic interpretation of user queries.
Ah, and we produced a series of four codebooks that introduce SippyCup.
So Unit 0 is kind of a high level overview of semantic parsing,
and then the remaining units demonstrate
the application of SippyCup to three different problem domains.
So Unit 1 focuses on the domain of natural language arithmetic.
This is queries like 2 times 3 plus 4 as English words rather than mathematical symbols.
Um, Unit 2 focuses on the domain of travel queries.
So this is queries like driving directions to Williamsburg Virginia, ah,
which has pretty obvious applications for assistant products like,
ah, Google Assistant and Alexa and Siri.
Ah, and then finally,
Unit 3 focuses on geographical queries.
So things like how many states border the largest state,
and this has a very similar flavor to the kinds of,
ah, queries that CHAT-80 takes on.
So Unit 3 kind of illustrates
a modern machine learning based approach to
the same problem that the CHAT-80 system took on.
By the way, this approach,
that, that SippyCup, uh,
illustrates was pioneered first by Luke Zettlemoyer and his group at Udub,
and later by Percy Liang,
and his group here at Stanford.
Um, so the key elements of this approach are, first,
a context-free grammar which defines the possible syntactic structures for the queries.
Ah, second, semantic attachments to the CFG rules,
which enable bottom-up semantic interpretation.
Third, a log-linear scoring model for,
which is learned from training data.
Fourth, phrase annotators for recognizing names of people,
locations, dates, times, and so on.
And finally, grammar induction, inducing grammars from training data
in order to quickly scale into new domains and new languages.
Um, and over the following slides I'll,
I'll talk about each of these five elements in turn.
So let's start with the grammar.
Ah, the grammar is the core of the semantic parsing system,
and it has two parts.
It has a syntactic,
a syntactic part and a semantic part.
Um, the syntactic part is a fairly conventional context-free grammar.
So we have terminals like Google,
and NY, and me,
and bike, and car.
Um, and then we have non-terminals which we indicate with the dollar sign.
So like $look, for example, is a non-terminal.
Um, and we also use, oh,
we have a designated start symbol which,
ah, here we're by convention,
we're calling $ROOT.
So every derivation has to,
has to bottom out with $ROOT.
We're also using a notational convenience here.
This question mark indicates that this element is optional,
and so that's just a notational convenience.
If I wanted to avoid using this,
I could instead have two rules here.
One that has $ROOT,
and all this stuff without the parentheses and the question mark,
and another rule which has $ROOT,
and all this stuff except that.
So this is just saying, "This could be here,
or it could not be here, it's optional."
So this particular grammar fragment could be used to parse just a handful of queries.
It could be used to parse route
me to New York by car.
That's one possible thing that can match this grammar,
or it could match,
um, route, I'll skip this,
to, let me do a compound location,
Google in New York by bike or a handful of other queries.
Ah, there's not very many queries which match this very limited fragment,
but it gives you a flavor of the,
the kinds of things that are,
that are, that are achievable.
Um, as is typical for grammars of natural language,
these grammars are usually non-deterministic.
So if you've looked at CFGs in the context of programming languages like if you took CS143,
you're probably, there you probably have seen
deterministic grammars where there's only one possible, ah, parse.
One possible interpretation for any given input.
In linguistics, we typically deal with nondeterministic grammars,
where there are multiple possible parses for a given input.
And that's important because natural language utterances are very often,
I dare say usually ambiguous.
So here's an example of a parse for the input route me to Google in New York by car.
Ah, we recognize me as an optional word.
We recognize Google, as a $loc, a location.
Recognized New York as a $loc,
we then recognize $loc and $loc as a compound $loc.
Ah, we recognize bike as a $mode,
and then because we've got a destination and a mode,
we can use that last rule to put the whole thing
together into a $ROOT which is the the final,
ah, syntactic production parsing.
So this is basically the syntactic parse,
one possible syntactic parse for that input. So far so good.
So this is the syntactic part of the grammar.
Um, okay.
So given a grammar and an input query,
we can generate all possible parses of the query using dynamic programming,
and specifically we use an adaptation of the well-known CYK chart parsing algorithm.
So if you've ever looked at syntactic parsing before,
ah, you may have seen,
ah, you may have seen this.
Um, here's how it works.
The first thing we do is, we rewrite the grammar so that all of
the rules in the grammar are binary or maybe unary.
Binary means they only have two things on the right-hand side.
What we don't want to have,
is three or four or five things on the right-hand side.
We want to have at most two things on the right-hand side.
Then what we do, is we consider every span of tokens,
every subspan of the whole query,
and we do that bottom-up.
So we work our way up from very small spans,
like spans of one up to larger and larger spans.
As we go, for every span,
we consider all way,
all ways of splitting that span into two parts.
And then we consider every grammar rule whose right-hand side can match those two parts,
and that's why it's important to make the grammar binary,
so that we only have to consider splitting things in two,
and not splitting things into three or four or five.
Every time we have a grammar rule that match,
that can potentially match that right-hand side,
that tells us that we have a way of making
the category which is on the left-hand side of the rule.
And so we can record that as a possible, ah,
interpretation, a possible parse for that span,
and that can be helpful as we work our way up to larger and larger spans,
because it can help us build bigger things above that.
We can use those categories in trying to interpret larger spans.
Okay so that's the syntax.
What about the semantic part of the grammar?
Every rule in the CFG can come with what's known as a semantic attachment,
and I've shown the semantic attachments here in the square brackets in green.
You can think of these semantic attachments as little programs that are
run when the parser applies the corresponding syntax rule,
and their outputs are fragments of our meaning representation.
Um, they basically, the semantic attachments basically specify how to construct
the semantics for the thing on the left-hand side
from the semantics for the things on the right-hand side.
So, um, for Google,
this one is very straightforward.
The semantics for Google is just gonna be a freebase ID which means Google.
It's the entity Google.
And similarly for New York.
For $loc, in $loc.
Um, this thing says,
"The way to construct the semantics for this $loc,
is to build it up out of the semantics for this $loc, and this $loc,
if you've already got semantics for this $loc, and this $loc,
I'm going to call the first one $1,
and the second one $2,
and this thing says,
"Just take the semantics for this $loc,
and semantics for that $loc,
and stuff it into one of these S expressions with an in, in the front".
It's a fragment of our- our meaning representation,
and this thing tells us how to build up these pieces into larger and larger pieces.
These ones are straightforward.
The semantics of, of a $mode is just one of these enum values,
and then the last thing that's interesting,
is that this one tells us how to build the semantics for the entire request.
It says, "It's a Get Directions Request."
I guess that comes from the fact that it's a route.
Um, the destination is gonna be whatever the semantics is for this non-terminal,
and the mode is gonna be whatever the semantics is for this non-terminal.
So these semantic attachments tell me exactly how to construct
my semantic representation from smaller pieces building up to larger, and larger pieces.
So here's our example parts again,
but this time I've added in green
the semantic yield associated with each node of the parse tree.
By the way, if you're a linguist,
this is basically Montague semantics.
It's bottom-up syntax driven semantic construction.
So first, we construct the semantics for Google and for New York as Freebase IDs.
Um, and for bike as an enum value, ah,
then we combine these two to get the semantics for the compound location up there,
and finally we combine everything to get
the semantics for the Get Directions Request at the top.
Okay, let me pause there because that was a lot to absorb.
Any questions so far?
Okay. Let me keep going.
Um, so the next question is, how do we recognize
na-names and dates and numbers and things like that?
Um, we could do it all in the grammar and that's just what I did,
uh, with Google and New York here.
I had directly in my grammar, uh,
something that associates the string Google with the semantics which is a Freebase ID.
And I could do that for long long lists of entities and I could also
do it for dates and numbers and things like that.
But, um, if I do that,
it's gonna mean, um,
adding lots and lots like millions of rules to my grammar,
um, which is gonna be really messy and cumbersome and brittle and difficult to maintain.
And it's gonna have limited coverage anyways.
So I mean, I can't possibly put
every possible date or every possible number into my grammar.
So instead, what we're gonna do is leverage special-purpose annotators for,
uh, phrases that describe entities,
locations, names, numbers, dates,
times and things like that.
So here's an example. The, the example query is reserved Gary Danko with Tom next Friday.
And so here we're imagining that we have
three different annotators that are gonna help us interpret this query and they
basically function like black boxes which run first before
the syntactic machinery gets to work
and can hand their results to the syntactic machinery.
So first I imagine that I have a Freebase annotator, and this thing, its job
is to look for entity mentions that are things that Freebase knows about.
And it recognizes this phrase,
this string Gary Danko and it says,
oh I know about that.
That's Freebase entity this thing.
And it tells me this is a $restaurant, um,
which I think it figured out from, uh,
it, it, it, is also able to generate lots of
metadata because it has all of Freebase at its disposal right?
It knows about Freebase.
And so it's able to look this thing up and figure out it's a restaurant and it
knows to generate that syntactic category.
And then it also, uh, generates this helpful metadata that can be passed downstream.
So for example, it has a confidence score that says how,
how sure it is that it's made
the right interpretation here and that confidence score, uh,
can be passed through, um,
and, and uh be an input to scoring the interpretation later on.
Um, I also mentioned that we have
a contact annotator which recognizes this, this string Tom.
And it says, oh Tom, I know who that is.
It's this user ID and he has this email.
So obviously the only way that can work is if
the contact annotator knows who issued the request.
It knows that this request is from a specific user and that user
has a friend named Tom and this is his user ID.
So it needs access to some personal information in
order to ground these semantics with a specific user ID.
And finally, a date annotator.
It recognizes this phrase next Friday.
And it says oh, I know how to generate semantics for that.
Next Friday means May 10th, 2019.
And it was able to do that because it knows when the query was
issued and therefore is able to interpret next Friday appropriately.
So next Friday is an indexical expression but to ground it correctly,
I need to know something about where the,
whe- when the query came from.
So we have uh these annotators that can run essentially as black-box modules,
run over the input query,
generate hypotheses about how to interpret small spans and record
those hypotheses as inputs to the syntactic machinery that will will,
uh, will get rolling afterwards.
Okay. Um, a pervasive problem. Yes.
Just going off back to that approach.
Yeah.
What exactly is the issue it's tackling?
Like when you first pitched sort of the problem,
I thought a solution could be to affect the annotation itself.
So rather than saying location goes to New York,
you could save location goes to span, and then the location would
be invoked lookup on span, rather than running the entity recognition first,
you could go to the semantic parser
and it would be able to identify the span and then pass it to the annotator.
Is there a reason why we put on the annotator first?
Um, I think maybe what you're suggesting
is that the parsing could happen top-down instead of bottom-up.
Does that sound right?
That could be yeah.
Um, and that's, uh,
conceptually that's definitely possible.
Um, and like when you for example if you've taken CS 143,
you study lots of different approaches to
parsing and some of them are bottom-up and some of them are
top-down and some of them are a mix of both and all of them are conceptually possible.
But there can be big differences as far as the efficiency of parsing.
And particularly for natural language which is so highly ambiguous.
Um, top-down parsing turns out to be prohibitively expensive.
And bottom-up parsing is, um,
way more efficient because you can eliminate,
uh, unlikely possibilities early in the process.
Um, when I was working on this problem at Google,
the grammars that we were working with, uh,
it was commonplace for
even very pedestrian inputs to have literally thousands of possible parses.
In fact that's the next point I'm gonna turn to is ambiguity.
Um, and when there are thousands of possible parses,
you need to do pruning fairly early in the, in the process.
We, we would use, uh,
what's called a beam search to basically maintain
as, as the, the process of parsing is happening to maintain
a finite length list of
the most promising possibilities and aggressively prune beyond that list.
Um, and that was the only way to to keep the search for a plausible parse manageable.
Just following up on that. When we do the annotations,
it does help us to limit the number of possible parses and
so that's one of the big advantages to running the annotator first?
Yeah. Yeah.
Thank you.
Yeah. So this problem of ambiguity is, uh,
this is a problem that's that's pervasive, uh,
throughout language and certainly, um,
a big challenge for semantic parsing and ambiguity
can include both syntactic ambiguity and semantic ambiguity.
So here's an example, uh,
the input is mission bicycle directions.
So imagine the the user issues this query to Siri, or to the Google Assistant.
Uh, one possible interpretation for that is that we wanna get to the mission by bike.
I want mission bicycle direct,
I wanna ride my bike to the mission.
That's certainly a possible interpretation.
It turns out that there's a bike shop called Mission Bicycle.
So another possible interpretation is I want directions to Mission Bicycle.
And I'm not specifying transportation mode.
I just want direct maybe, it's maybe I want driving
directions but where I wanna go with mission bicycle.
Um, so here I show these two different parses for this query.
In this one mission is the location,
bicycle is the mode.
In this one, Mission Bicycle is the location and I'm not specifying a mode.
In this case, in this example,
there's only two possible parses but as I mentioned a moment ago, in complex domains,
it's with rich grammars,
it's commonplace that there are tens or hundreds or even thousands of possible parses.
So if our grammar supports multiple interpretations,
how do we know which one to choose?
The answer is with a scoring function.
We need a way to score
the different candidates so that we can figure out which one is the most plausible.
And one approach to doing that is with
a log-linear model to score alternative derivations.
So a little bit of terminology here.
I'm gonna call the input that is the the query,
the national language query X. Um,
I'm gonna call the derivation or parse.
So that's the syntactic parse tree with all of its semantic attachments.
I'm gonna call that Z.
And I'm gonna call, uh,
I'm gonna use Y to to designate the semantic yield.
So that means the final semantics.
It's basically the semantics that you get at the root node in the parse tree.
Uh, by the way, Z completely determines Y.
Uh, but for any given X,
you may have lots of candidates Zs and correspondingly lots of candidate Ys.
So to to build this scoring function,
what we're gonna do is first,
define a feature representation which captures the key characteristics of the input X,
the candidate parse Z and the candidate semantics Y.
Uh, there's a lot of room for variation
in the feature representation but just to give you
a flavor of like some common commonly used features.
Uh, your feature representation might include, um,
an indicator function which is one,
just in case the input contains the word to.
And the candidates semantics contains a destination parameter.
Presumably like intuitively makes sense those two things are likely to go together.
And so seeing those together,
might count as weak evidence.
It's hardly proof but it might count as weak evidence that this is actually a good parse.
Um, or you might have features which capture again,
indicator features so Boolean features which capture the occurrence of
specific CFG rules or specific categories the dollar the dollar things,
um, in the candidate parse.
The intuition here is some rules in your grammar might be much more likely or
unlikely than others to participate in good parses, in valid parses.
You might not be able to anticipate that in advance but if you have features like these,
then you have the opportunity to learn that from data.
You can learn from your data that some rules work much better than others.
Uh, another kind of feature you might include is features which
pass through the confidence score that you got from an annotator.
Presumably, if the annotator's really confident that makes it more
plausible that a parse that includes that annotation is a good parse.
There's lots of room for variation in
the feature representation but once you have a feature representation,
the score for a, a derivation,
the score for parse Z will just be the dot product
of the feature vector and a weight vector theta which is learned from training data.
And once you have a score,
you can turn the score into a probability
in the usual way by using the soft-max function.
[NOISE]
So the next question is,
where do we get this thing from?
Where do we get the weight vector theta?
This is basically the parameters of our scoring model.
Um, well, we're going to get it from data,
we're gonna estimate those model parameters from training data,
and we can do it using a variant of the EM algorithm.
The reason for using the EM algorithm is that our training data consists of
pairs of inputs x and target semantics y,
but the training data doesn't include the correct parse trees, z.
So there could be multiple parse trees that yield the same semantics.
So we have to treat those Z's as latent variables,
and that's what the EM algorithm is good at. Yeah.
[inaudible]?
Yeah. I'll, um, I'll try to,
um, explain it in application to the,
to- to- to this specific application.
The EM algorithm, uh, alternates between E-step and M-step.
Um, EM stands for Expectation Maximization.
And in the E step,
what- what we do here,
is use the current model parameters theta to parse the inputs in our training data,
and for each input x,
generate an end best list of possible parses.
So we're just parsing all the inputs using our current model.
Then, in the M-step,
we're gonna change the weights of the model.
And specifically we change the weights to put more probability mass
on the elements of the n-best list that actually generated the correct semantics.
Um, in the n-best list,
some of them are gonna have good semantics which match the target,
some of them will not, and we want to shift
the probability mass towards the ones that match the target semantics.
Um, and then we go back to the E-step and do it again.
We reparse everything using
our updated weights that could cause the n-best list to shift around,
um, and we keep doing that back and forth between the E-step, and the M-step.
So these updates to the models are basically SGD updates.
This is kind of like a stochastic gradient descent algorithm,
uh, and the weights will adjust gradually over time,
uh, toward weights that are more and more
successful in generating the target parses for our training data.
Okay. So we've got a grammar,
it's got syntax rules,
it's got semantic attachments, there's ambiguity,
we have a scoring function,
we've learned the scoring function from data.
The next question is,
where did the grammar rules come from?
If- and the answer really depends on what domain you're working in.
If it's a small simple domain,
then grammars with a few dozen or a few hundred rules,
are commonly enough, and at that's sca- at that scale,
it's practical to write grammars manually.
You just like, think about the domain,
and look at lots of example queries,
and just manually generate some rules that will capture,
uh, that will cover the- the- the- the- the- the set of input,
uh, input the- the set of inputs, uh, properly.
But that's not the common situation.
The common situation is,
you're in a large complex domain,
um, and you, uh,
need thousands of rules to model the domain well,
and at that scale, it's just not feasible to write down all the rules manually.
Uh, so instead we want to learn the rules automatically from- from training data.
And this is the challenge of grammar induction,
and it's where some of the most interesting research work lies.
Um, one strategy for grammar reduction which- it's
a very simplistic strategy but it's illustrated in Unit 1 of the SippyCup codebooks.
The idea is simply to generate all possible rules and add them to your grammar, uh,
and then to use standard parameter learning to learn weights for each rule,
to learn which rules are most likely to contribute to a successful parse.
So all- what is- first,
what is all possible rules mean?
It basically means, um,
all ways of combining the symbols of your semantic representation.
Your sort of semantic,
like your language of formal semantics
always combining those symbols into syntax rules,
every possible right-hand side,
every possible left-hand side,
the cross-product of all those things.
And if we add all those rules to our grammar,
then we can generate a huge variety of different parses.
Most of them are bad,
but we can use the- the weight learning that I talked
about on the previous slide to learn from data which ones are good,
and which ones are bad.
This works but it comes at a heavy cost because
generating all possible rules leads to
an exponential blowup in the number of possible parses,
and consequently in the time that's required for parsing,
uh, and if you work your way through SippyCup Unit
1 you'll see this illustrated very vividly.
So more sophisticated approaches to
grammar induction look for ways to be far more selective about,
uh, introducing new grammar rules,
and also ways to prune them
aggressively when they're not contributing to successful parses,
to keep the size of the grammar manageable so
that training can still run in a feasible amount of time.
So we've talked about two different ways of using training data in this process.
One way of using training data is to induce rules of the grammar,
to figure out which syntactic productions should be part of the grammar.
The other way of using data is to estimate the parameters of our linear scoring model,
um, and these two- two way, uh,
two ways of using data are both really important and kinda work hand in hand.
Um, so that's, this kind of
underscores the importance of data for making this whole thing work.
You can't do grammar reduction and you can't
learn the parameters of your scoring model without training data,
and not just a little bit of data,
to really make this work, you want massive amounts of data.
So organizations that are doing this at scale,
like Google and Apple invest a lot of money in data annotation,
uh, using proprietary Crowdsourcing platforms similar to Mechanical Turk.
So paying graders, paying human annotators to look at example
input queries and write down the-
the target semantics for those queries so that we can then,
um, train- train machine-learning systems to- to- to,
um, to capture that.
However, that's really expensive.
Labeling examples with targeted, er,
target semantics, um, is slow and laborious,
and costs a lot of money.
So another really productive direction has been to
enable learning from indirect supervision.
Um, an idea pi- pioneered by Percy Liang is the idea of learning from denotations.
So denotation basically means,
if your semantic representation is something that can
be executed or evaluated in some way,
that execution or evaluation could result in something that's a lot,
uh, simpler and more intuitive for a human annotator to produce.
So an example here, I should have put an example on the slide,
an example might be like,
you know, what's the capital of the largest state in the United States?
The semantics for that might be some kind of complex, um,
logical formula that literally says,
except in logical language instead of English,
what is the capital of the largest state?
The denotation of that semantics would be the answer to the question like, Austin.
Juneau, yes.
I guess, if we're talking about large in terms of area, right?
What is the capital of the largest state?
Juneau, Alaska.
Um, so that's the denotation.
If your training data consists of inputs, like,
what is the capital of the largest state,
paired with logical forms,
we can learn very effectively from that,
but it's hard to produce that training data because it's hard
to get human annotators to produce those logical forms.
Um, and in Google and Apple we figured out all kinds of tricks to make that a little bit
easier for ordinary people to produce logical forums,
but it's an intrinsically hard problem.
It's much easier to get ordinary human annotators to produce the answers, like, Juneau.
And so if we can have training data that says,
What's the capital of the largest state?
Juneau. If we can figure out a way to learn from that kind of data, uh,
we'll be able to get lots more data, um,
and, um, and the benefit of- of learning from that data.
So that's the idea of learning from denotations.
Um, this is a really powerful idea,
and it's illustrated again in Unit 1 of SippyCup,
where the domain is natural language arithmetic.
So there the donations are just the answer to a simple arithmetical computation.
Okay. So to recap,
the key ingredients of this approach to
semantic parsing are CFGs with semantic attachments,
log-linear scoring models, annotators,
grammar induction, and above all,
lots and lots of training data.
Um, I think- I hope that that's enough of a high level overview,
that if this topic interests you,
you'll be able to dive in to the SippyCup codebooks,
um, start reading some papers,
and get a much more concrete sense of how all this stuff fits together,
and how all this works.
I think I'll stop there for today.
 Hi everyone. So we're gonna start the, um,
the Bake-off 4 Report.
So the task.
So, um, for this Bake-off,
we had to do word-level natural language inference with binary classification.
So, basically we wanna be predicting,
um, the word entailment given two words.
So, um, we had a disjoint train- train test split which reflects
our expectation that we wanna be able to generalize on unseen words and vocabs.
So our evaluation dataset consisted of 1,767 negative labels and 446 positive labels,
which is a pretty, um, unbalanced dataset.
And thus our evaluation metric was Macro F1 Score.
So, um, some people actually reported their Micro F1 or the weighted F1,
but we only looked at the, um, the Macro F1.
The Micro F1 and the weighted F1 were a bit higher than the- than the, um, Macro F1.
And Macro F1 is a de- desirable matrix because,
um, because of data imbalance.
So this is a histogram of the submission scores.
As you can see, um, the baseline is right around 0.67.
And, um, as you can see
all people were able to surpass the baseline, which is really good.
As usual, we're gonna look at the, um,
the VSM results for top per- performing models than the,
um, the less significant ones.
So, um, for the top submissions, so, um,
as you can see the GloVe embedding was being used very frequently.
And as you can see from the variable name such as torch.tensor,
you can see that they probably played around with neural network frameworks.
[NOISE] And this probably doesn't have to do a lot with the design but, um,
the top submissions they had the, um,
the variable name custom_experiment a lot,
which we thought was, um, pretty interesting.
And let's take a look at the,
um, the bottom performing models.
As you can see, there are a lot of numbers which probably
means that people handpicked hyperparameters.
And again, um, as you can see the-
the variable name word_disjoint_experiment appeared a lot more in,
um, the bottom submission than in top submissions. All right.
And- So, um, the first place goes to group 26.
Congratulations, um.
They got the score of 0.7852.
Is group 26 here?
And, um, what they did was they used
a combination of BERT Sequence Classification Model with oversampling.
So, um, they used transfer learning.
They used the pre-trained BERT, um,
end-to-end sequence classification model,
and then they trained on our training dataset and,
um, tested on the validation set.
And in order to account for the data imbalance,
they used oversampling, [NOISE] specifically,
um, the Random Oversampler.
They randomly sampled with replacement, um,
the current available samples,
to make- to balance out the,
um, the label distribution.
And the second place goes to group 9.
Is group 9 here? Congratulations, um.
You got the score of 0.7541.
So what they did was they used Facebook's InferSent Model.
And they pre-trained the- the InferSent model on the SNLI,
Stanford NLI corpus dataset.
And they transferred the weights and then they added
an extra layer at the end for binary classification.
And for the, um,
data imbalanced problem, they used weighted loss.
And they set, they set the weights to 1 and 5.3,
meaning that they gave 5 times more emphasis to class 1 than class 0.
So, um, the both, these top submissions,
the top first and the top second.
They used transferred learning and some type of weight to balance out the data imbalance.
Which was really impressive, um.
Okay, moving on.
We also looked at the models that,
um, that performed the worst.
And we figured out some things that didn't really quite work.
So first of all, um,
element wise multiplication is not a good function to combine vectors.
And for this task,
shallow networks linear- linear regressions or
SVMs did not work as well as the deep neural classifiers.
This is kind of understandable because, um,
for this task it's really hard to, um,
handcraft, um the feature representations.
Okay, um, that's all I have.
Any questions? Cool.
All right, thank you Min.
Ah, that was cool.
I guess a few comments about that.
So first, the same team that won this time won Bake-Off 2.
As far as I know that's unprecedented that the same group won two bake-offs.
Although I do remember that on
separate teams last year finished in the top two or three on two of the Bake-offs,
which is also very impressive.
But it's striking that,
uh, the same team won.
And I believe that they won with broadly the same approach,
which is BERT with some task specific fine-tuning,
and then a few other tricks that are specific to the dataset.
Um, and that's pretty cool.
Um, next week we're gonna be talking about BERT and ELMo and the Transformer,
and that might give us some insight into why those techniques are working so well.
So stay tuned on that.
The other thing I wanted to say is just,
I mentioned this last week,
it's really cool now that a bunch of you have done
really serious work on four separate NLU tasks.
You develop custom models for them.
Some of the things that you did are incredibly ambitious,
and I just think that's cool in terms of you having experiences to
report and now experiences to draw on when you go forth and do your own projects.
So congratulations to you all.
The plan for this week is to try to do things that will
be like directly relevant to your projects whatever they are.
Because it's- it's kind of awkward to be introducing new topical material now when we
know that you all are probably focused on
whatever special thing that you're doing for your own projects.
But this is a nice moment in which to start talking
openly about methods and metrics and things like that
because the next major assignment that you have to do is
this new document that we're calling an Experimental Protocol,
which is replacing the kind of milestone document that we have used in years past.
So in years past you did the lit review,
and then the milestone was a kind of generic check-in.
And it was kind of an attempt to make sure that
you were working steadily on the projects.
What we- what we're trying to do with this new approach to this interim report,
is kind of save you from yourselves to make
sure that you're incrementally working on these projects.
But also to kind of codify our belief that it's
valuable to be doing a particular kind of quantitative evaluation,
and also encourage you to start talking in a detailed way with
your teaching team mentor about exactly how the project should be structured,
what kinds of experiments you could be running,
what kind of additional data you could bring in, and so forth.
So let me just walk through this to make sure it's clear kind of what we're expecting.
It's due on the 27th.
It's a highly structured report in the sense that you could
just copy out these prompts if you wanted to and fill them in.
Um, you don't need to invest in writing a nicely structured paper.
I think it would actually be valuable for the team if you kind of stuck to this framework
because in turn this is the framework that
we're gonna use as a kind of rubric to evaluate the work.
So right off the bat,
we want you to state a hypothesis or hypotheses.
Um, I think that this is often not done or not done enough in NLU and NLP.
And one reason that I can make that inference is that, in recent years,
if you review for ACL,
they have started codifying exactly this requirement in their referee form.
So as a reviewer,
you're meant to state back to the authors and to
the area editors exactly what the hypotheses are.
To kind of get everyone thinking about whether or not the paper is doing something beyond
just throwing a bunch of parts together and running a quantitative evaluation.
So we want you to state your hypothesis.
These hypotheses can take many forms,
and this will depend a lot on what kind of project you're doing.
It could be something as simple as,
"My hypothesis is that for my dataset,
the transformer is a better model than an RNN at making the relevant predictions," right.
That would be kind of functional kind of engineering oriented,
but it is nonetheless a hypothesis.
And if you reflect on why you believe in your hypothesis,
that might you lead- lead you to even deeper insights like,
for my dataset which is structured in a particular way,
the transformer has the right kind of structure to reflect the data, something like that.
But it could be quite functional and just describe what kind of experiments you wanna do.
But it could be something as interesting as,
"I believe that in processing Project Gutenberg files I can tell whether the author
is a man or a woman based on their portrayor- portrayal of female characters."
Which is a really interesting hypothesis from a past year that a project team did.
I think it's one of the ones we picked as the- as a top project, right.
And that would be more social sciences oriented and more
removed from the particular modeling tools that they're using.
Because they're just saying like, "This is a signal that I can detect using NLP tools."
And there's lots of stuff in between those two extremes.
The point is state it,
state it as clearly as you can and start talking about it with your mentor. Makes sense?
Then as you can imagine,
coming in at second here, data,
we want a description of the datasets.
It could be again, it's something as simple as,
"I'm using MultiNLI," or something as complicated as,
"Here is my procedure for crowd-sourcing a web
scraped custom dataset that I developed for this project."
For the first one it's probably okay to just mention it and give the relevant citation.
For the second one, I think it would be helpful
if you gave us some details on the nature of the dataset.
Because, for example, our suggestions
for you are going to vary depending on whether or not you have 5,000 instances,
500 or 5 million.
And also in turn like the internal structure of the dataset will matter.
So give us a clear picture of that.
Then the metrics, you'll state the metrics.
That's gonna be one of our topics for this week.
These are gonna be the things that form the basis for your quantitative evaluation.
If it's familiar, that's good, right.
If you're doing a classification problem with imbalanced data
easy to justify that you- you're using Macro F1.
May be harder to justify Micro F1,
but maybe you wanna do it.
May be harder still to justify something like
an imbalanced version of the F1 score because you wanna favor precision or recall.
Or if you're working on something like dialogue agents,
then as you'll see today or next time,
this is a tricky question exactly what metrics to use.
Because it seems to me that all of the available ones have their own faults.
And so you might want to say,
"I'm gonna use perplexity,
it has these known downsides,
but then again for my project it's the right thing
because it fits intellectually," or you could just be up front and say,
"It's expedient because I don't have
any other kind of extrinsic evaluation I can run," right.
Just be open and upfront about what choices you're making and why.
That's the spirit of this.
And then we'll give you a response like, "Hey, you know,
I really feel you could go beyond your chosen metric.
Here's a slightly better perspective on the problem or something like that."
Then models, we want a description of the models that you'll be using.
And I think the thing to do here is remember that we
don't just want a description of your favorite model,
the one that you're kind of advocating for,
but also a statement of which models you'll be using as baselines.
Because as we're gonna talk about a little bit later today,
none of these evaluation metrics really have any meaning in isolation.
If you repoint- report a 0.76 for your model,
just as a lonely number there,
nobody has any idea what that means.
If it's a familiar dataset,
maybe we can fill in some background.
But then again, it should fall to you to fill that in.
Um, and if it's an unfamiliar dataset, we just won't know.
So one thing you can do to provide that context is set up some baselines.
And I'm gonna go over a bunch of ways that you can do this
where the baselines maybe get more and more ambitious.
Um, but all of them will help us in understanding what you achieved.
[inaudible] dat- uh datasets and someone else already produced a model,
is that a case that,
that number is our baseline number or do we need to run the actual
[NOISE] dataset ourselves to generate that number? [NOISE].
Great question. Yeah, I think- certainly at this stage,
the more published numbers you can provide the more context we'll have.
Um, we might encourage you also to fit some of your own baselines,
um, especially if the published numbers are astronomically high.
Because then you might want to say like,
okay somehow teams at Google have achieved 0.90 because of their vast resources,
but the competition I'm running is between
two more modest models and mine still wins within that space,
and in which case your own baseline is serving as a kind
of given your budget and time constraints and goals,
what the baseline is.
But I think those upper bound numbers like top numbers and literature are great.
Human performance, if you know it, uh,
information about inter-annotator agreement also valuable context for,
for what we can expect for your data. Yeah, great question.
Others about the models?
This is kind of the heart of it here.
Um, and what we'll be looking for this kind of leads
into this general reasoning part is a statement or
brief statement of how the data and the models come together to inform your hypothesis.
And we have this as a separate item because we don't want you to take it for granted.
We would like to hear your version of why your data and
models and baselines are gonna come together to achieve this.
This is not idle work because if you do this well,
it's probably gonna be the basis for the abstract for your paper.
Because this is more or less what you want from an abstract minus maybe,
maybe the abstract has some additional context.
And then we would like a summary of your progress so far.
So what have you done?
If you've run experiments and they're all in and you have confidence, that's great.
Preliminary numbers are great.
What you still need to do, obstacles, that would be helpful to raise at this point.
Um, things that we should know that might be blockers for you in getting all
that you want to achieve done by the time it's all due.
And then don't forget this because we'll ding you for it,
references section, proper bibliography. Yeah.
So all questions should be a good foundation for our abstract,
so what's the expected size of the experimental pro- protocol?
I don't know.
Um, because I don't really think of the length as having
any kind of inherent virtues or, um, vices here.
Um, how long would it take me to answer these questions?
It could be as short as a couple of pages, right?
For some projects where it's like,
my data is SNLI,
my hypothesis is that the Potts model is better than the some other model.
Um, my metric is going to be macro F1.
My models I can sketch out.
The general reasoning is clear like the Potts model is keyed into NLI,
the way no other is and here's why colon, you know,
here's the- it wouldn't take me very long.
Um, but if you're doing something quite custom
or you still have open questions like I've talked with a few people who,
maybe even it's unclear what metrics they'll be able to use,
I think you might devote more space to it.
Any other questions? All right.
So I have- what we're gonna do for this week is,
I have two notebooks that I'm gonna show you.
They're both posted at the website,
they both have code and I've kind of alighted
of- alighted the code for a lot of this presentation,
but it's there for you if you wanna get more hands-on with the material.
But the first one is going to be on methods,
and the second one on metrics.
Um, the metrics one I might not do too much lecturing about,
it's not the stuff of really compelling lectures,
but I would like to just broadcast to you my framework for thinking
about these metrics and how you might choose your own and think about your own,
um, and then you can kind of use that notebook as a reference.
And then more exciting,
on Wednesday, he's gonna give, uh,
a short presentation about what you might think of as like the
2.0 version of the traditional metrics and methods for NLP.
Where we go beyond the stuff that I'm gonna show you today and start thinking in
a more serious way about what it means to truly generalize and maybe to truly have,
like what we could call understanding for a phenomenon or,
or a domain or a task.
Um, and I think that's gonna be really fruitful because you all might want to push
beyond like reporting macro F1 scores
and think about what your systems have actually learned.
And this is an opportunity for you to do that.
That kind of brings me to this overview here.
So our primary goal is to talk about projects.
I can't do methods or metrics in a comprehensive way,
but I hope I can give you a kind of foundation for thinking about all these issues.
And partly, what I wanna do is just bring some of this out int- into the open,
because I have a feeling that a lot of it is
stuff that you're supposed to just kind of pick up on the streets.
Um, everyone kinda presupposes it and you get
a feel for what the issues are and how to navigate this.
And I was just hoping we'd have an open discussion about it basically.
And as I said, that brings me to the project.
So I wanna make a few things clear about how we think about projects.
Um, first, we will never evaluate a project based on how good the results are.
Let me just make this font a little bigger.
Okay. Uh, only those arrows are getting bigger.
[LAUGHTER] I hope you can- if not- if it's hard to read,
you can follow along at the website.
All of the stuff is drawn from that main notebook, and I'll read it aloud.
So just to emphasize again,
we will never evaluate a project based on how good the results are.
I grant you that publication venues,
have- they do this.
Whether they have to is secondary,
but they do it and they do it because they have constraints on
space that lead them to be only able to publish a few things,
and then all of science has a bias for
positive evidence for things over negative results.
That's why you might get your paper rejected because you don't have
state of the art results even if you have wonderful insights and analyses.
It's a sad thing about the state of publishing.
And I think it's true all over science.
But in CS224u, we are not subject to that first constraint.
Um, which means that we can do the right and good thing of valuing positive results,
negative results, and everything in between, right?
So just because you got what looks like a state of
the art number on your dataset does not mean that you will get an A triple plus.
And correspondingly, if your hypothesis totally falls flat but you
give a really stellar evaluation of it and
like maybe some insights into why everything fell apart,
that could be an A plus paper, right?
So we're going to evaluate you based on the appropriateness of your metrics,
the strength of your methods,
and the extent to which you are open, and honest,
and clear-eyed about the limits of your findings and other things like that.
And I've been trying to push this on people in office hours like,
because of that first dynamic about publishing,
this is a rare opportunity for you to try something really wild and crazy.
Uh, and, you know,
something that would just be too risky as a conference submission because you never know.
And surely, you know,
the really earth-shaking ideas began as things that sounded too wild and crazy.
In fact, my read on the whole history of deep learning is that it
was too wild and crazy for 30 years before it wasn't.
Um, so you never know, right. Yeah, yeah.
So let's start walking through these issues.
I think some of them seem obvious but all of them are worth reflection.
I would love to have your questions and comments I
would love for this to be a bit of a dialog here.
Start with stuff that's basic.
So data organization, you guys have heard about this.
We have like the idea of a train dev test split.
Ah, and this is a common thing to see in large publicly available data sets.
Even before diving into the details of how this works as a community you should
reflect on the fact that even this is a non-obvious choice, right?
Because in the background here is an assumption that you will develop
a system using the training and dev data and then evaluate it on test data.
And the reason that we draw those from the same body of
examples is we feel that that's in some sense a fair evaluation that um,
if my system has learned its parameters on the training data it ought to be able to da-
generalize to data that is in some sense from
the same empirical distribution as the training data.
And we think of that as a kind of fair evaluation.
It's not the only conceivable approach that you could have to evaluation.
And in fact you might think it's kind of stacking the deck
in favor of all of the systems and overstating all of
our results because you're given even before you look at
the test data a kind of guarantee that you'll be looking at similar examples.
And you can imagine that the whole field took
an alternative route of saying our evaluations,
the ones that we'll allow into our prestigious publishing venues are all gonna be
human subjects evaluations where humans have to deal with
a system and give their ratings about whether the systems are good or not.
And, and we wouldn't value the numbers that we value in this course so much.
So even this is a non-obvious choice and
the other part that might be kind of non-obvious about
this is that the test data does come from the same body of examples as the training data.
And that's one where I can feel the community kind of starting to push back,
starting to acknowledge more and more that
this is overstating what our systems can actually do.
Um, and for example in this course the MultiNLI dataset is one that has
the mismatched test condition where you train on
some genres and test on genres that you've never seen before.
That's starting to push back on
the core assumption that you're seeing on this slide about train dev test.
And I think people aspire to do even more radical things to give up on this the,
the way people tend to state it is training and testing on the same distribution.
Um, it's hard to know what it would mean to test on a different one
because there's an infinitude of different data sets that you could
pick and surely trying to generalize a system trained on SLI and
testing it on evaluating chairs and tables in the real-world, right?
Completely different data that's gonna fail.
What's in between there that would be a more ambitious test.
Multi NLI is one gesture in that direction.
But back down to earth many data sets that you study will have this kind of
split and as we've emphasized before
you are- you're on the honor system to not use your test sets.
Like the paradigm for you if you're publishing is that you are
on the honor system to do all your work on the train and dev sets,
never looking at test even if you possess it.
And then just before publication with all the systems
done you run a single test evaluation,
enter the numbers into your document and submit
and you're just at the mercy of what happened on test.
I don't know how often people really adhere to that.
It's sort of scary to think about people compromising that position.
And I think that's one reason why you're seeing an increased use
of test sets that are truly held out in the sense that they are on
Kaggle or held by a system and we may even keep track of
how many times different teams are evaluating on that test set. Yeah.
Does that sort of mean that you,
you wouldn't be able to analyze,
like how you did on the test set ultimately?
Like if you're just running it and immediately
submitting or you can't actually run it while
you're still writing your paper is the idea that when you're
discussing your results you're discussing your dev results?
That would be my preference.
Maybe people differ on this but my preference would be that when you discuss,
when you do error analysis you do it on
Dev because of this last point here, this test set.
People start hill climbing on it.
So if you publish a wonderful paper with detel- detailed error analysis on your test set,
then I get to learn a lot about what works and what doesn't.
Even if technically I have never peeked at the data set.
And in that way we would just end up with a lot like community hill climbing on the data.
So my preference would be that these are just kind of stark numbers that you see.
Um, even that's going to allow some hill-climbing because
we can get a community-wide sense of what's working and what isn't.
But yeah, that's my preference. Go ahead.
In the case where we have like totally different sets, right?
Like we're, we're testing on a different distribution entirely.
Is it, there even not useful to kind of talk
about like say you know you just do abysmally worse on
this new data set like that seems kind of notable that you would
want to talk about like gosh there is something in my model that,
that sort of failed to capture this difference.
Yeah, that's a fair point.
If you think there's some value you might,
like if you- I mean if you're doing the thing of just saying like,
"Hey, this system that was trained on Twitter,
I'm going to apply it to the Wall Street Journal."
And it falls down because the styles are very
different for that Wall Street Journal data set.
I'm not even sure whether you're thinking of that as
your final test set and it might be okay because it's more qualitative.
But I think it will depend on the circumstances.
But if you're in this hypothetical situation,
if you're releasing that Wall Street Journal test set and you want others to use it,
it probably benefits you to be kind of tight-lipped about
it and maybe have a dev portion that is keyed into that.
[NOISE] Other questions about this? Yes.
It's seems that if you're [inaudible] your data set you then want to ensure that your sort of
class distribution is the same across the train test dev
because sometimes if you're just arbitrary chunking up your data,
this might go sideways.
You're raising a very important issue, yeah.
I agree with the core of it.
We want similar distributions here.
But this can get quite intricate as you can imagine.
If we think about problems that have different structures where I always think that
the really scary version of this is what if your data set depends on a knowledge graph,
then how are you gonna do this in a way that's fair.
Um, and these issues are arising in a more subtle form and
I'll show you some tools that help you manage some of
these problems but you're absolutely right about the kind of core default insight.
The other point I wanted to make here is that even doing this
presupposes a pretty large data set, like um,
if you do work in health care or something where every example is kind of hard won,
you might be reluctant to hold out a test set because it really means that you're
giving up on a whole bunch of data that could be used and studied in an interesting way.
So that brings me to this next thing here which should be like no fixed splits.
And some public data sets do have this property.
And I think this can be good um,
but it does pose a challenge for assessment because for really robust comparisons,
you have to run all the models using whatever assessment regime you've picked.
Right? So if it does a bunch of
experiments on one of these data sets and he splits in his way.
And I don't maybe know the details of how he did those splits.
And I do different splits.
Strictly speaking if I want to compare our numbers now,
I can't quite do it right um,
because for a really fair comparison we should be looking at
exactly the same splits because who knows what
happened when we did these random divisions.
Let me just say that because I always want to balance this, that's the ideal.
And so the ideal is that somehow I get from exactly his splits or his model and I rerun
everything in a super careful way and then
my results table can be reported with no caveats.
That's the ideal. But of course we don't live maybe in the ideal world.
We all wanna have room to say unfortunately,
I couldn't get exactly the splits that it's used.
So my comparisons are informal in that sense but nonetheless reliable and I think
the important thing is that you report them and talk
about their value but with that caveat attached.
And that's very different like if you think about reviewers being your antagonists.
If you just report these numbers without giving context,
the reviewer might say thumbs down,
those comparisons weren't fair.
It wasn't the same splits.
If you give the caveat and acknowledge it,
it's kind of harder for them because you've already
contextualized the results that you're offering. Yeah, you had a question?
Is there a point where you almost have enough data that over-fitting would be the goal?
Like if you had all text ever written.
At that point would it be almost not be good to just
over fit to that in the sense that you have so much data
that if you fit it well then you just kind of fit text well.
Like it's so large that it might as well be as large
as all utterances that will ever be produced by humans.
[LAUGHTER]
I guess is like is there- is there
sort of an implicit assumption that no dataset is too large,
that like every dataset; it doesn't matter how big it is,
you still need some- something that has been held out in order to evaluate it properly?
I think I do want to stand by that.
I think language is so complicated that you never see even
but the tiniest portion of the full distribution of utterances.
We're incredibly creative with language.
We produce, um, entirely new utterances all the time,
and that means there's always room for testing generalization.
Yeah, that's one of the exciting things about the domain I would say. Another question?
[BACKGROUND] Something about that,
I feel like maybe if you're in a setting where you're just trying to that,
create like word vectors that encoded like a hybrid in your relation with WordNet,
you could think that like the entirety of
WordNet taints all the relations you might need to use.
So maybe in that setting,
it's appropriate to train and test on the same data.
Because really, you're just trying to encode the relational information and it doesn't
really matter like what you do in a general way as long as it can be accessed.
Oh, I see. So you don't mean literally
the same data but rather just the same kind of dataset that gets split.
Yeah. In which case, I think your,
your observation kicks in for sure.
If we're looking at the dictionary of English,
we might think that this is the entirety of the domain.
Yeah. For large datasets,
if they don't come with a predefined split as part of your project,
you could impose splits and just say like here's
what I'm going to use and I don't look at the test data.
Um, this can be nice.
If you have the luxury of doing this,
I would do it because it simplifies your experimental setup in many ways as you'll see.
Um, and in particular,
the second is important.
You'll see that for very careful hyper-
hyperparameter optimization you end up doing a lot of evaluations and having
fixed splits as opposed to doing something like cross-validation
could dramatically reduce the number of things you have to do here,
and also just kind of yes simplifier li- like
remove an inner loop from your experiment code as you'll see.
So I'd encourage you to do that,
and then the qualifier there is just for small datasets.
You might feel too constrained to leave out some data for testing.
And I'll talk about strateg- strategies for that next.
Oh, yeah. That kind of brings me to this other mode; so cross-validation.
So instead of having fixed splits in cross validation,
we take a set of examples x and partition them into two or more
train/test splits and average over the results in some way.
You probably have heard about this before.
There are two kind of classes of ways you could do this.
The first one, I'll call random splits.
So in this case, shuffle your data,
hold out 10% of the data for training.
Probably the rest for testing,
and run an experiment;
and then make another split,
randomly, run an experiment, and so forth.
And you can do that as many times as you want,
and you might want to do it a lot to get some insight
into how much variance there is in your system's performance.
Uh, relevant to that question in the back.
I'm sorry, I don't know your name,
but this was a great insight that when I do this,
I probably want to make sure that I have
the same distribution of labels across the splits.
So Scikit makes this very easy.
The, uh, relevant code has a keyword called stratified,
and you can just make sure that you've told the code to
honor the distribution of the train and test,
have them equal across all these splits,
and I think that will lead to more consistent evaluations.
The good and the bad of random splits.
Okay. So the good is that you can create as many as you
want without having this impact the ratio of training to test examples.
That'll become clear when we look at true cross-validation.
But the idea is that you can just run as many of these experiments as you want.
Because probably, with a large dataset,
there might as well be an infinite number of ways that you could do these divisions.
The bad is kind of related to that.
There's actually no guarantee that
every example will be used the same number of times for training and testing.
So depending on the nature of the splits that you do,
you might be looking at kind of
distorted evaluations as you combine them and average them.
But this has really nice flexibility to it.
And in situations where your model is pretty fast to train and test,
the fact that you can do lots of splits this way without
impacting the ratio of train to test examples is very powerful.
And will open the door to doing some testing of
the statistical differences of your systems.
Let's contrast that with, um, oh,
and here, here are some, you know,
some code that you can use.
Scikit is as usual wonderful for helping you do this kind of thing.
Let's compare this with K-fold's cross-validation.
This is different from the random train test split things.
So in K-folds, you divide your data into what are called K-folds there.
So different sets here like fold 1, 2,
3 and you conduct K experiments.
In each fold i- in each fold- sorry.
Sorry, at each of the experiments,
fold i is used for assessment and the other folds are merged together for training.
So if I have three folds here,
these are subsets of the examples.
For experiment 1, I hold out fold 1,
train on 2 and 3 and test, and I get a number.
In the second experiment,
I hold out 2, train on 1 and 3 and I get a number.
So forth for the third one,
here I test on fold 3 and train on 1 and 2.
So now, I've seen every combination of these three folds, and I get three numbers,
and I can average them or something,
and maybe report if I do more folds,
um, kind of confidence intervals around those means.
Let's do the good and the bad here.
So the good and the bad kind of complementing the last few.
Every example appears in a train set exactly k
minus 1 times and in- in the test set exactly once.
So that's nice. You have some guarantees about how you've gone through the data.
I'd say the bad of this is that the size of K
determines the size of the train test splits.
So with three fold cross validation,
you train on 67%of the data and test on 33.
But with 10-fold, you train on 90 and test on 10.
Those are very likely to be different experiment scenarios,
and I feel like two things have gotten muddled together when you're doing this.
On the one hand, you wanted a lot of runs because you wanted
a real sense for a system performance across different settings.
On the other hand, you end up changing
the size of the training and test data when you do that,
uh, and that's just a consequence of the good here.
So it's not like we can blame the method.
But if- that might steer you back toward random splits,
where you don't have this relationship,
and all you're giving up on is this absolute guarantee up here.
Make sense? And as usual, as I said,
Scikit is great for this K-fold and stratified K-fold,
and then it has these helper methods here;
cross validate and cross val score,
which will allow you to,
um, setup models and datasets and kind of under the hood,
do all of these evaluations so that you don't write
nested for-loop code which is hard to maintain.
And the only two, um,
tips I want to make there is first,
when you use these guys,
you might be explicit about what your CV argument is so that you have
absolute certainty that you are doing
stratified if you want to or just K-fold if you don't.
If you're kind of nervous like me.
And then maybe more important is that when you do this,
you set the scoring value.
If you don't set a scoring value for these functions,
it's going to default to whatever the model's inbuilt score is.
And for example for a classifier that will be accuracy,
and we've seen I think time and again that that's probably
not the metric that we want to be using for our evaluations.
So you have to take the step of explicitly changing that to
macro F1 or whatever your choice is.
And just to mention a few variants K-fold has some special cases.
Again, Scikit is good about this.
So in LeaveOneOut cro- LeaveOneOut cross-validation,
you actually- this is an edge case of K-folds
where I hold out just a single example for testing,
and train on all the r- rest and I do that, k- you know,
for every- the number of times that there are
examples in my dataset and average the results.
You would do that if you have a very small dataset in the relevant sense.
And there might be two ways that could be small.
First, you might just have a tiny number of examples,
um, in which case,
you'll be compelled to do this.
The other situation would be that,
maybe you have a very large dataset,
but you would like to conduct a valuation- an
evaluation that is like at the level of users.
And so, maybe every user is associated with 100,000 text,
and you have 12 users,
so you actually have a quite large dataset.
But if I wanna do this user-level evaluation,
suddenly, the data seem quite sparse.
And in that case, you might do the LeaveOneOut thing.
And then finally, Scikit has a function called LeavePGroupsOut,
that's actually quite sophisticated about helping
you to do splits that are not just the standard ones.
So for example, it has keyword things that you can do to make sure that you
split along a temporal dimension which would be
important for anything with a time series aspect to it.
Um, I think you could also manage things that needed to be divided by,
like, demographic group, um,
or other things like structure in your data that
aren't immediately evident from your labels,
uh, in which case, you know,
if it's just the labels,
then the stratified option will probably take care of that.
Good. So that's it for splits.
[NOISE] Did I leave anything out in talking about all this stuff,
k-fold, cross-validation, stuff like that. Yeah.
So if you are putting forth like a trained model,
would you ultimately train it on all the data,
and then just use this cross validation as like an approximation of the [NOISE] accuracy?
Probably not.
If, if you have a fixed test set,
then you certainly won't do what you just described.
I think- I think you would do a lot of cross-validation as part of development,
uh, but then finally,
evaluate on the test set.
So for example, if your dataset has just train test and no dev,
then in place of dev,
you might do a lot of cross-validation runs inside the training data,
to get a sense for exactly how your system
generalizes in all sorts of ways that you can throw at it,
and then evaluate on test.
I think you would have these as
your final evaluation numbers only for datasets that don't have a test set.
Because they're smaller, they weren't released with one.
Uh, in which case, you know,
you get the benefits of not having just one number,
just this test set number because maybe the test set was chosen very strangely,
and it's distorting progress on the dataset.
It's kind of nice to know that we're reporting the average of a bunch of different runs,
gives us a sense of more robustness.
On the other hand, you don't get that pristine kind of held out nature of a test set.
Other? Oh, yeah.
So when you talk about test- on test set,
[NOISE] is there any intuition for how- think about the ratio between training and
test that should be [NOISE] [inaudible]
That's a good question. I don't know it.
I feel like you see 80-20,
75-25, 67-33, but I don't think there's much behind this.
It's probably people just stepping back and thinking like,
what's a goo- a large enough number that the testing isn't gonna be kind of wild.
Um, but also, isn't gonna done- deny me too much training data.
That's the best answer I can give.
Um, it certainly gets more
interesting if you think about holding out much more test data,
how good can these systems get if I showed them only half the data, for example?
But as a dataset creator,
you might be shooting yourself in the [NOISE] foot by doing that
because people want lots of data to train on.
And maybe it's that game theoretic back and forth that gets us to 80-20.
[NOISE] Yeah.
[NOISE] Systems and industry do they ultimately use a similar metric like,
you know, so almost the same thing as an academic setting
when you have your training set of training data,
and your test set, and all of that jazz?
That's a great question.
Um, and contribute I see nodding.
My first pass at this would be- the answer is yes.
But then one fascinating thing that might happen to you,
um, is that your incentives are switched.
So in academia, we would kind of like all our systems to look good,
and maybe that creates a bias for having them look better than they
are because that's the way you get published.
In industry though, when you think about deploying a system,
if your- if your neck is on the line for that deployed system,
[NOISE] you might take a much harder look at the kind of evaluations you're doing
to essentially get a conservative number about how it's gonna perform.
And you might do much more kind of like truly te- held out testing on completely or,
you know, partially unrelated distributions.
So that when you do deploy, you're not fired.
[NOISE] [LAUGHTER] Do other people who worked in industry wanna comment on that?
It's an interesting dynamic. Yeah.
What you said sounds exactly right to me.
I mean, um, your, um, the,
the question of incentives is,
is [NOISE] complicated because like on the one hand,
um, do you wanna get your project out the door?
[NOISE] On the other hand,
do you also wanna keep your job?
[LAUGHTER] So you kind of have, uh, conflicting incentives.
But I think, um, uh, ultimately,
you want evaluations which reflect the true strength of the system.
Um, and doing a train te- test split and,
and using this- the same kind of, um,
methodology that we're using in academia is pretty, pretty similar.
[NOISE] Let's change topics a little bit.
Oh, another question. Yeah.
Kind of [NOISE] piggybacking on what he was asking.
Um, is there any point in say doing like
a meta study of like all the major datasets in a given domain,
and seeing what the best ratio of train test splits are?
[NOISE]
How would you define best?
I mean, it would get interesting very fast because,
yeah, what do you mean by best?
Well, I guess like which one offers the best validation performance?
Um, yeah.
Let me- my answer is kind of yes like if you
could find a way to do this that offered, er,
intel to the community so that we make
better decisions going forward, we would all benefit.
It's kind of hard for me to see how we get the ideals right.
I think a part of it would be you stating what your ideals were.
But you do remind me that there's another interesting thing you could
do which is related to the split sizes which is to say,
okay, I have my fixed test set,
and my default is to use all the training data.
But maybe you're advocating for a system that really shines with very little data.
In which case, I think it would be smart for you to
do evaluations where you reduce the amount of training data,
and maybe what you're doing is showing that.
Okay. For all the training data, your competitor wins.
But you can get by on 10% of the data,
and your system is still doing much better at that point than the other competitor.
That could be incredibly valuable because [NOISE] we never have enough data.
And so we should in some sense favor systems that can thrive in that sparse area.
[NOISE]
Yes.
Is that something that people actually do in papers?
Like what you have just described.
Yes.
Yeah. Okay.
Yeah. And I think you could tie into like
the general feeling that we should make efficient use of all the data that we have.
Yeah. Let's talk a bit about baselines.
Again, kind of maybe stuff we've been taking for granted
but it's nonetheless we're thinking about these things.
Um, first of all, as I said before,
we can never understand evaluation numbers in NLP in isolation.
And I feel like you get- you get it from both sides here because on the one hand,
if you just try to publish a number like my system gets
0.95 or 0.99 even F1 on a dataset,
you might think that you had- you just declare victory by default,
right, because you've solved the task.
But what will actually happen is that your reviewers will say,
Well, obviously, this is too easy.
Um, and in that ca -and then this is also like related to
that deb- debugging phenomenon that might be depressing about NLP,
that if you get perfect performance,
you probably don't cheer but rather think something is broken about my code.
[LAUGHTER] I can't be that good.
Um, so even those really good numbers require maybe
human level performance and then at the other side of baseline.
On the other hand,
if your system gets 0.60, um,
you might despair but maybe it turns out that humans aren't a whole lot better,
and maybe the baseline is way below that.
In which case, that 0.60 might mark real progress on the problem.
Uh, and that's a clear case where we need context ,
where you do not want people defaulting to what they think
0.60 means in the context of classification or something.
So these baselines are really important for building good arguments.
Um, they shouldn't be an afterthought but rather
pretty central to how you set up your hypotheses,
and you can hear us encouraging that with this protocol document.
Um, that baselines are really important to building a good case.
And the other thing that you can do is use them to illuminate
specific aspects of the problem that you're
studying or some virtue of your proposed system.
And I think we've seen versions of this before, we didn't dwell on it.
But think like, if I have, uh,
a model that's just summing GloVe vectors,
it's completely insensitive to word order.
If I compare that to a model like, uh,
an RNN, which has a lot of word order dependence,
and I show that the RNN is better,
then I've got a kind of
intuitive indirect argument that word order matters for my dataset.
Um, and you can think of lots of other examples that will have that quality.
Maybe you added some attention mechanisms to your NLI model.
And then the difference, you know,
the gain that you get from that is kind of
illuminating the fact that there are important kind of
dependencies between premise and hypothesis
that a standard model was just missing out on.
Maybe forgotten but important to mention here random baselines,
you might want to include them.
And this is, uh, I mean, often,
the random baseline will be easy to construct like you could just say I'm gonna guess
proportional to the class label or I'm gonna just pick the majority class,
those are two common baselines.
I mentioned the dummy classifier and dummy regressor here.
These are both Scikit models,
just because they make it easy for you to
build the dummy classifier into your standard experimental pipeline.
So maybe you're testing like five different models.
You don't want to have a separate code base for doing the dummy classifier,
because it might- you might introduce bugs there or change the evaluation
somehow or it might make it just very
hard to make sure all your models are seeing the same data.
With dummy classifying regressor,
they have the same interface as all the other Scikit models and
and in turn same interfaces all the models that we've
used for PyTorch and so forth in this class.
Um, it's just that they don't make
intelligent use of the features that you give them essentially.
Um, but that's kind of nice in terms of automating your workflows.
This is a subtler issue.
This is kind of already pushing us beyond standard thinking about evaluation,
I've called this task specific baselines.
It's worth considering whether your problem suggests a baseline that
will reveal something about the way the problem is posed or modeled.
We've seen one example of this.
So in NLI, we saw that hypothesis only baselines can be very strong,
much stronger than random guessing.
Uh, and I gave you an argument for why that is,
I think there's a scientific reason at least in part for why these can be so strong.
Uh, another story that had some prominence
a few years ago in some -there's a Story Close dataset,
and this is where you are given a passage,
and you have to choose between a coherent and incoherent ending for that passage.
Um, and people reported a lot of progress on this,
you know, numbers that looked pretty good compared to human performance.
Um, but then it all kind of got qualified by the fact that if you turned a
classifer loose just on the endings without showing it the story,
it could come pretty close to those top numbers.
And I don't said- have such a hopeful story
about that one as I do about the NLI baseline.
This might indicate that there's a real problem with the dataset.
But separately from that,
even if there is an issue,
you can kind of partly overcome that by just saying
my baseline is this classifier that doesn't look at the story.
It gets 70%,
and then let's see how much further we can get from that with
the smart model and more intelligent use of the data, right?
So all is not lost even in a two type situation here. Yeah.
[inaudible] task specific baseline,
do you essentially kind of implicitly encoding
some prior knowledge about the problem in, in doing that, right?
Do you think there is any potential like risks when you take that approach?
What would be the risks that you'd be making the problem too hard for yourself?
If, if you end up encoding kind of the wrong information about that.
I think I see what you mean.
So I don't- so do you think it holds for either of these two,
just to get a feel for what you're after?
Not, not particularly. No.
I recognize that it's a danger.
Um, so don't do it.
[LAUGHTER] I'm only part- partly joking in the sense that,
if you do take this step,
it's probably because you're giving a positive argument that you feel random,
it's just not a fair baseline for us to have picked.
That's certainly what you would do in both of these situations.
Um, I guess I'd be kind of discouraging about just going off and seeing if you
can cleverly construct task specific baselines that are really strong,
um, because you might end up introducing biases of the sort you're describing.
But a lot of what I'm trying to do here is say,
I don't have the full answer because I don't know what the world is gonna throw at me,
what task you're going to take on,
what your data will be like.
The idea here is to just be thoughtful about it,
and that like if you think about the really exceptional papers in our field,
they often have- first of all,
they're thoughtful about all of their choices in my experience.
Um, but they often also introduce these subtleties in a way
that moves us forward conceptually and not just quantitatively.
Okay. Another big topic is hyperparameter optimization.
Uh, this is where we're gonna start using those Cloud credits I guess.
I gave this argument before,
let me rehearse it and amplify it a little bit.
So just for some terminology here.
In machine learning, the parameters of your model are
those whose values are learned as part of our optimizing the model itself,
we often call those the weights or the coefficients.
Let's take that as given as a definition and just say that
the hyperparameters of your model are any settings
that are set by a process that is outside of the one that I described under 1.
Of course, the boundary between these two could get blurred, um, and moreover,
if you think about the space that I just defined for hyperparameters,
it could end up including everything, including, like,
what kind of computer you're running on and what you had for breakfast.
Um, but some clear examples here,
the regularization term for a classifier,
that's a clear hyperparameter.
We all have an intuition that those values
could pretty dramatically affect how your system performs.
Other examples are like the dimensionality of your network if its deep learning,
the activation functions that you use could have a huge impact,
the optimizer that you could use could have an impact,
especially for models that don't converge in a reliable way.
Um, you could also think as I've done here,
"What about, like, the way you normalize feature values?
Uh, did you apply TF-IDF or PSA or something like that?"
Um, we're kind of drifting farther and farther from the core parts
of the model but I think you could think of all of these things as hyperparameters,
and we'll construe them pretty broadly.
So here's the rationale for doing hyperparameter optimization.
There are lots of ways you could go at this,
but here I've picked one, kind of,
back and forth narrative.
So suppose you ran experiments with models A, B,
and C and for each,
you use the default hyperparameters as given by the implementations you're using, right?
So in scikit you just set up these models,
you initialize them with no arguments.
So I'm just at the mercy of what scikit picked.
And in doing that, you found that your model C performed best,
and you report that in your paper and you think "Ah, you know, victory.
I was rooting for C and it came out true."
Your reviewer is going to ask or certainly wonder about a few things.
So first, "Did you actually try
any other values for the hyperparameters without reporting that?"
They might just wonder about your practices.
"If not, would you have done that if your favorite model hadn't outperformed the others?"
Perfectly legitimate question, right?
You- maybe you didn't have to confront it,
but suppose B had won your competition,
would you have thought, "Well, wait a second.
I need to think about whether or not I'm doing the regularization correctly.
Let me fiddle around with some values a little bit"?
And then you'd be off and running on some hill,
climbing toward your favorite thing.
So the- what the reviewer might conclude here is that
all we've learned at this stage is that
there's some setting of
the hyperparameters that favors your model and disfavors the other one.
But the really cynical hard-beaten reviewer is gonna say, "Well,
I already knew that because I know that I can just craftily pick
these settings in a way that will make some model look
terrible and other models look good."
Right. If I'm given this as kind of behind the scenes fiddling,
then game over, we don't have a fair evaluation.
So in response, you could give every model its best chance to succeed, right?
Let your models explore a really wide range of hyperparameters.
For each model, choose the best settings
according to performance on your development data,
and then report how the models do on those settings on all your test sets.
Right? If you go through that whole discourse there,
your persistent reviewer might say, "Well,
you didn't pick the right space of hyperparameters.
You should have gone further,
you should have tried these new variations," and I grant that,
but I mean, come on,
you have to stop somewhere.
So that's one kind of rationale.
The other would be just that you are open-mindedly trying to figure out what's best.
Like, you've been placed in a situation where you have
to do a bake off for a class and you want to win, uh,
and then all of the same dynamics kick in,
it's just that you're not arguing with someone but rather just
trying to figure out how you can win the bake off.
Um, I think the same rationale applies cause you want to pick A,
B, and C and really see each one in the best possible light.
And that's kind of the spirit of this,
that we get a fair evaluation if we see A, B,
and C all at their best, which means, like,
dressing them up to be the best they can be,
and that comes back to the hyperparameters.
So here's the ideal for this kind of thing.
For each hyperparameter to your model,
identify a really large set of values for it.
Create a list of all the combinations of all the hyperparameter values,
this will be the full cross product of all the sets of
values for all the features you identified as relevant.
For each of the settings,
cross-validate it on the available training data
and then choose the settings that did best,
train on all the data,
and then evaluate your model in the usual way.
That's what you really ought to be doing I suppose by the logic that I gave you before.
However, this could be very demanding.
Suppose that hyperparameter 1 has five values and 2 has ten,
okay, now there are 50 settings.
If you add a third with two values,
now your number of settings has jumped to 100.
Okay. Now you wanna do five-fold cross-validation,
that means you're doing 500 runs,
and if you wanna do 10 random train test splits because you wanna, like,
test whether or not your model is
statistically different or look at confidence intervals,
now you're doing 5,000 runs of your model.
Boy, that got expensive fast, right?
Um, and if each one of these models takes eight hours of GPU compute time,
well you can do the numbers here,
um, because they're gonna get very expensive.
Yeah, we have to- we have to back off from this idea a little bit.
This is untenable as a set of laws for a scientific community.
If we actually said that the only people who could
publish were people who had made good on this entire thing here,
then only people with vast amounts of money would be
able to publish at all, and in addition,
we would have a distorting bias in favor of simple models because
those would be the models that you could really afford to do all these runs for.
And I pick this- [NOISE] there- more and more you see these kind of things,
but this is from a nice paper on doing NLP for
health care and they just note in their, uh,
methods section in an appendix,
the performance of all above neural networks were tuned automatically using
Google Vizier with a total of over 200,000 GPU hours.
So you figure like, "Okay.
At a dollar an hour,
plus whatever Google is gonna take from me,
this could cost a quarter of a million dollars very easily."
Um, we got you some Amazon credits,
but not close to that, I'm afraid.
So we can't have that be the- the law that we live by.
We can have it as an ideal,
but we shouldn't, um,
allow others to expect it from us nor should we require it of other people.
We need to introduce some space for being pragmatic about these choices.
So here is my view about how we could kind of alleviate this problem, and I've done this.
Again, this is very impressionistic,
but in descending order of attractiveness to
me as a reviewer or to my imaginary reviewers out there,
if I think about them being antagonists for me.
So first, you could do some randins- random sampling and guided sampling, uh,
to allow your- to explore a really large space based on a fixed budget of runs.
So the random sampling part would be just that,
I set up my grid as I did before,
and it has 500 settings,
but I decide that I'm going to look at 50 of them.
And what the random sampler will do is pick 50 random settings,
and what I'll do is kind of have an expectation
that that's a pretty good sample of the full space that I,
with only low probability,
missed a setting that was gonna be really transformatively different.
And then there's a kind of variation on that
that is cal- I've called guided sampling here,
which would be that you have maybe a separate model running that is trying to
intelligently travel around in the hyperparameter space and make choices on that basis,
and you might think that that's better than random.
The virtue of both of these strategies is you could just say, "Look,
my budget is 50 runs," and that you won't in having, in doing that have to delimit
your hyperparameter space artificially because it could be
wide and you're still just going to sample 50 times from it,
and I think this is pretty palatable to reviewers,
especially since there's published evidence that random sampling is a good strategy.
Uh, and I included those links in the notebook on the website,
and then guided sampling is another proven method and I'll show you- I'll link to,
uh, some software packages for doing that a little bit later.
Is there evidence that the, like,
space of hyperparameters is really non-linear?
That's a great question. I think it really is going
to depend on your model and your settings.
Um, yeah.
Do- do you have anything more to say?
[inaudible] So anything more than, uh, to ask.
You actually like have very, very convenience.
[NOISE] Like you- you really want to use [inaudible] something like random sampling with a meaning.
It is this idea that you keep focusing on small areas as- as you are advancing on the,
uh, uh, process of finding hyperparameters.
So you don't- you don't have to explore all things behavior, you kind of,
uh, constrain yourself to smaller and smaller areas.
Maybe you see that you don't have like being able to.
Based on these [inaudible] my fault.
We've talked about the second strategy before.
I'm sorry that these are all ones here.
[LAUGHTER] Um, you could do some search based on a few epochs of training.
Uh, so this is especially relevant I think in the context of deep learning models,
where your model may take 1,000 epochs to converge and that would be very
expensive to do all of those runs and then check
performance and select parameters on that basis.
But what you could do is have a background hypothesis
that the future is gonna be like the past,
that a setting that was really bad at the start is just gonna remain bad,
and in turn settings that are good in the start are gonna remain good.
Um, two things about that,
you might not be able to prove it but you might be able to
support it with some kind of learning curves that show what's
happening in those early things and maybe a few runs that
project out further and show that your assumption is basically correct.
So you could do that to kind of build your case.
Um, but you know,
if you don't have the capacity to do that,
even just acknowledging that this is the strategy that you
picked and giving a reason based on budget or other considerations,
I think will go a long way with reviewers because it kind of acknowledges
that your choices were done on somewhat limited evidence,
um, but at least you know that that qualification exists.
And again, this is kind of nice because you can control costs in this way
while still exploring a large space of parameters.
Search based on subsets of the data.
That's another strategy that we explored off and on throughout this course,
um, and that would be that, you know,
my actual dataset contains 500,000 examples but I'll do my tuning on 5,000
and then I have lots of flexibility to do lots of
experiments and then I'll just project that out to the larger set.
Um, the real problem with that is that I
think it's obvious that some of these parameters,
these hyperparameters, could be very dependent on dataset size.
So a classic case would be that
regularization parameters are really gonna be affected by the number of features you
have and the amount of evidence that you can gather for each
one and that's interacting badly with this approach to selection.
But still, certainly better than nothing.
You could also do some heuristic search.
We're getting kind of low down in this list but this is still quite respectable, right,
so via some experiments that you run just in a free-form way,
you figure out that some hyperparameters never change performance and others really
change it a lot and in turn you just fix
the ones that seem not to matter and explore the rest.
And this would be a case where absolutely you would want to justify this in the paper.
You'd want to say that for our problem, uh,
the dimensionality of the hidden layer didn't matter in the range 50 to 300
so we set it at 300 and explored all the regularization parameters,
and then, you know, your reviewer could say,
"in my experience, the hidden dimensionality really matters."
But now it's kind of on them to have that evidence, right?
And you, you sort of said heuristically that you didn't see it.
So I don't know.
It seems like you built a pretty good argument in that case.
You could find the optimal hyperparameters via
a single split and then use them for all subsequent splits,
and this would be justified on the idea that the splits are similar.
So that would be a case of saying like I'm going to run one experiment really
carefully and assume that it holds for all the other settings that I need to explore.
That could be pretty reasonable with
a large dataset that you know is kind of homogenous.
This is probably perfectly fine.
And then all the way at the bottom here,
but something that you still do see,
is just adopting choices that other people have made.
So you know like I'm going to train new word vectors on
a bajillion words from the web and I don't know how this model is going to behave.
I can afford to run one experiment,
so I'm just going to pick the parameters that were in
the originally published paper and use them,
uh, even though the data is different and my goals are different and so forth.
Um, you're probably going to get some complaints that the findings don't
translate that the- what was optimal for that paper isn't necessarily optimal for yours.
But in that setting, where you're trying to do
web-scale training of word representations,
this might be all you can afford and again,
I think we can't just arbitrarily expect that people will run
multiple runs across the whole web in order to tune these parameters.
So as a last resort,
other people's parameters and there's lots of papers
that do this and I think it works out okay. Yeah.
[BACKGROUND] Can you use the best results on some dataset?
Is it necessary to justify all this?
I mean if- you know, let's just say you just stuck some numbers out of
your- out of your hat and they work the best,
do you still have to say I did all of this stuff or is that almost not
self-sufficient evidence that you're beating the benchmark?
Where the- like the paper is really one of these pure papers that just says my model is
the best model for this dataset and my evidence is that
I found parameter settings that beat all the published numbers.
It might be a persuasive argument.
Yeah, I mean I personally would hope that you're trying to do more with this paper and
if you were trying to show us something more about this model than just that you won,
then this would all kick in.
Because I'd want to understand like what's the space of performance numbers and so forth.
Some of this I'm gonna address in a little bit.
But yeah, I want to allow them.
That might be all that's necessary.
I'm sorry. Why is sort of robustness
to different hyperparameters or at least trying a lot of them,
give you more insight into how the model's working?
I mean ultimately if you have a good hypothesis and the results are promising,
why is it necessary to,
to really get that last little bit of juice out of your model by hyperparameter solution?
Oh, I see. It might not be juice though.
Like I- first I want to just grant that your narrative might be perfectly fine,
which is just that, you know,
I found a setting where my model is the best
and that's evidence that it's a really good model for the task.
The hyperparameter exploration might be something that you explore more openly to say,
for example, how small could my network get,
uh, and still do well.
How- what are my performance drop-offs,
or how much does it matter how it was regularized,
or like how much does it matter even how it was
initialized with GloVe or with Word2Vec or with ELMo?
And what I- I was just injecting
my values there that I find all of that really illuminating.
Because after all, I don't really care that you won.
I mean congratulations in this case that you won.
But what I really want to know is can I use your model for other tasks?
How sensitive is it to various settings?
Um, can I form an expectation about how it will
work in an entirely new domain? That kind of thing.
Here's um, a hyperparameter optimization tool.
So scikit has grid search and randomized search.
Goods, Stan bias here,
and randomized search will be the one where you can fix your budget.
And then scikit optimizes a package for doing more guided search through these spaces.
And it's kind of nice because it plays well with the existing scikit models.
And it might offer you
some additional gains when you do this kind of thing on a fixed budget.
Let's do one more thing here.
Classifier comparison.
Because we've seen this before.
This is a nice way to round out this first lecture.
So suppose you've assessed two classifier models.
Well, we'll keep it for two classifiers because I think a lot of you are doing
classifiers the same considerations extend to other kinds of models.
And we can talk about how that would, would work.
But suppose it's classifiers,
they probably differ in some way.
Um, what could we do to establish that they're different in some meaningful sense?
And again, if you think about publishing reviewers will ask this,
how much can I trust that differences I'm seeing are really and truly differences?
So first of all you might think practical differences.
This is the ultimate, right?
Um, if your test set has 1000- say that your test set has a million examples,
then 1% will correspond to 10,000 cases,
and that seems like it's sure to matter, right?
If this is lives saved or something or
disastrous consequences averted then
10,000 you shouldn't need to say much more than that.
You're making a meaningful impact on the world, right?
Uh, so there's that one case.
In the other case, if the test set has 1000 examples,
then a 1% difference in accuracy or F1 will be like 10 examples.
And your readers might not know whether that matters at all.
And then hovering in the background here is this other question,
how much variation is there across runs?
That is in the case where it was 10 examples for your reported number,
if I make a small change does it go in the other direction, right?
It seems like it's just on the edge.
And in principle that could be true even of these models here because maybe
with some runs you save 10,000 lives but in others you lose 5,000.
We don't know, right?
Because we don't know the amount of variation there is between runs for your system.
And so all those things mean that we probably want to go
beyond just practical differences.
So just quickly a few ways you could do this.
First, you could report confidence intervals around the means that you report.
Uh, I've given here a kind of traditional way to do this.
That you could use.
My sense from NLP systems is that this traditional way here, um,
is pretty conservative in the sense that your system might actually
show less variation than is being assumed by this test.
And so you might see confidence intervals that show values that are
way outside anything you've ever seen in running experiments.
If that happens you could think about bootstrapping the confidence intervals,
and I link to a library for doing that.
And that would just make much more use of
your actual data when it comes to deciding what the interval of variation is.
But this is a really nice way to do this.
You report the mean and the confidence interval,
and what your reviewers are likely to do is say,
"Okay those two intervals don't overlap.
I'm pretty confident that these systems are different."
Or if there's a lot of overlap you might say, "Um, you know,
numerically they're different but in practical terms these might be the same system."
So this is a powerful step,
but it presupposes that you can do a lot of runs.
Because you want to do like 10 and, and, you know,
better would be like 20 different runs to get a bunch of values for the mean in
slightly different scenario- settings so that you can
get a good picture of what the confidence interval is like.
And you might just be limited in your ability to do that.
Uh, related kind of thing that you could do then- and again I'm just kind of drawing on
best practices as I've learned them from
the literature is to run the Wilcoxon Signed-Rank Test,
which is a kind of variant of a T-test,
um, that doesn't make any assumptions about the underlying statistical distribution.
So it does not presuppose in particular that your scores are normally distributed.
Um, it requires a lot of different runs.
So you wanna do at least 10,
ideally 20 or something like that.
Um, I think that you want to pick
the Wilcoxon Signed-Rank Test over this very closely related one,
the rank sums test which is often called the Mann-Whitney U Test.
I think the Wilcoxon one will be just slightly more conservative in the sense that it's
more appropriate in situations in which
your systems are trained and evaluated on the same underlying data.
And given the best practices as I laid out before,
you want them to be trained and test on the same data for
these comparisons but that just means that you
violated a lot of independence assumptions.
So pick the Wilcoxon Signed-Rank Test, it's a rank based test.
So it- all it's caring about is
the relative ordering of the scores that your system reports.
I think this is a good step to take.
And I think it will go a long way to people of- for people who are evaluating your work.
But a few qualifications are in order.
First, like all tests of this form it says nothing
about the practical importances of any of the differences that you see.
So like a small p-value is not a larger effect size.
Uh, and a large p-value doesn't mean that you have a failed result.
That just means that you lack evidence for claiming that there's a difference.
So these qualifications are to be kept in mind.
This is kind of very weak evidence for some kind of detectable difference.
But I would ideally balance it against something
establishing that your systems are different in practical terms.
Um, but I grant you that this could help you in sort of advocating for a system,
especially in situations in which the numerical differences appear to be small.
Because here you are saying,
they're small but I have some evidence that you can count on them.
And then just a final test so we can wrap this up here.
McNemar's test, is a test that you can run to compare
two classifiers that depends only on one run of the data.
Um, because it operates on this kind of collapsed,
um, confusion matrix for your two systems.
Um, and it's a kind of variant of the chi-squared test.
I think this can be pretty volatile for
systems that report different values under different conditions.
So I would resort to this just in situations in which you absolutely cannot afford to
do the number of runs that it would take to run a responsible Wilcoxon test.
In this case, this is a kind of fall back.
I think again, it's better than nothing for your reviewers.
And especially if you can compare- if you can pair it with
practical differences you probably got a pretty good case for the models being different.
There are a bunch of other big issues that I want us to
confront that are kind of particular to the deep learning era.
Um, but since we're out of time I'm going to save those for next time.
So next time it'll be these juicy issues that we're all facing today.
A discussion of metrics.
And then, this is going to take us beyond
all this traditional stuff that I've been talking about.
But thanks everyone for your questions.
 My plan for the day is to finish up that stuff on methods.
We don't have much left,
but I think they're really interesting topics.
And then I have a notebook on metrics.
And I think I won't go through the entire notebook,
there's a lot of material there.
What I thought I would do is just go through the section that's
on kind of accuracy, precision and recall.
Because I think if I do that,
not only are those probably going to be
the most used metrics for projects in this course,
but also you can kinda get a feel
for the higher level thing that I'm trying to do which is
to get you to think in a kind of structured way about what your metrics are doing,
what they're not doing,
where they might have faults, and so forth.
So I'll do that. And then Atticus is gonna take over.
Atticus has a very exciting mini-lecture on generalization and adversarial testing,
as I've called it here.
Um, and we'll let that take whatever time it takes and then just wrap up.
Uh, and if there's a little bit of time that's
great you guys can just start to work on your projects,
uh, and chat with the members of the teaching team who are here.
Uh, and then next week, contextual word representations,
I'm gonna try to make you, uh,
responsible users of BERT and ELMo and the transformer.
Uh, and then next Wednesday we'll have a panel discussion on NLU in industry.
And then Memorial Day.
And then we kind of wrap up with some other loose ends again in the spirit
of trying to introduce topics and content that will help push your projects forward,
that's basically what we're all about from here on out.
Oh, we had a wonderful discussion.
I really appreciated all the comments that people made last time about the material.
There was, you know, a bunch of stuff that I had never thought about,
and that deserves further thought.
Um, in the meantime,
have any questions or concerns occurred to you about the things
we covered which is kind of like we did all of this stuff,
we covered a lot of ground.
Data organization, cross-validation, baselines,
hyper-parameter optimization, and classifier comparison.
So Goog- Google has that theme this year
which is their hyper-parameters selection
I guess that's maybe what they use for AutoML too.
Is there some sense that those things have kind
of helped with the hyper-parameter tuning or is that still
kind of privy only to these big companies that have so much money
that they can just blast away on hyper-parameters for whenever,
it was 400,000 hours or something?
I mean, there's definitely an element of blasting away.
And kinda impressionistically for SNLI,
which is a leaderboard I kinda follow closely.
Um, there is a split,
you see sometimes that simple models with
relatively few parameters have really high scores,
and I think that strongly correlated with
them being- them originating in companies like Google,
that could do really wide cross- cross, you know,
hyper-parameter validation to find settings of the model that were really really good.
And they correspondingly, you know,
these really articulate intricate models that are
guided a lot by people's human intuition about the data,
they do well with less hyper-parameter optimization because they are guided by intuition.
So it's an interesting kind of juxtaposition.
I myself don't know much about Google Vizier.
Is that a public product or something they just use internally?
I think they had published a paper a little bit about it.
It was part of their like learning to learn I think like quantity or something in
2017 they had published about it but I wasn't sure how widely applicable it was.
Yeah, I don't know, I mean,
I remember it from that quote I gave from the healthcare paper
where they spent 200,000 hours doing,
uh, optimization using Vizier,
but I guess I don't really know much about what it's all about.
It would be great if they alleviated this burden by making resources
available. Other things on your mind?
All right. Let's do this final bit here, which is,
assessing models without convergence,
which you could think of as another way of calling out a lot of deep learning models.
And the background here is kind of like in the simpler era of
2011 for this course when everybody was using logistic regression, um,
convergence issues rarely arose,
because those models I mean, you might not even notice it but they do have thresholds,
they do run for a number of iterations typically,
um, but they tend to converge, right?
So you just kind of don't think about the fact that this
is one of your hyper-parameters because they just
finish up possibly before whatever threshold was set on the number of epochs.
So we didn't talk about this too much until recently but
now with neural networks kind of at center stage,
convergence too has taken center stage,
because these models rarely converge.
Um, they can converge at different rates between runs.
And then the other side of this is that maybe
running to convergence is not really what you want because what you
might implicitly be doing with kind of early stopping
is some regularization that and that is benefiting your solution.
Um, and then the final point here is just
that the performance of these models on test data is
often heavily dependent on when you decided to stop and how it related to convergence.
So the bottom line here is kind of like
sometimes a model with a low final error turns out
to be great and sometimes it turns out- it turns out to be
worse than one that finished with a higher error.
Who knows, right? That's I guess what I was gesturing at with
this notion of like stopping early as a way of
regularizing or as a way of avoiding over fitting to the training data.
But this poses a whole new set of problems, right?
Because it's at least one more hyper-parameter but it's a very
complicated one to deal with because now
you're thinking about having to run systems to
varying degrees interacting with all your other hyper-parameters.
And of course the more they need to run,
the more expensive this all gets in some generalized sense.
So one class of responses that you could have to
this I've grouped under the heading of incremental Dev set testing.
So basically the idea here is that as your model is training,
uh, incrementally say every 100th
epoch or every 10th or every 1000th, whatever you can afford,
um, run an evaluation on a Dev set and see how performance is doing.
Uh, and I just wanna mention that all the PyTorch models for
this course that are included in your repo, they can do this.
So if you give fit an optional X_dev and dev_itr set of arguments,
uh, then they will run evaluations on X_dev.
And you can kind of collect that information and make use of it.
You could show a learning curve um, or you could
use it to make a decision about where you want to stop.
And I will say that in my view at least given my understanding,
TensorFlow is kinda ahead of the curve in all of this.
If you use the TensorFlow classes for this course,
then I was able to just piggyback on the new estimator model framework and
its capabilities and they have lots of sophisticated things in
terms of like automatically writing your model to disk,
so that if you want to rewind back to
an earlier state of really good performance it's easy to do that.
And the PyTorch models are not currently set up for that kind of thing.
But the spirit of all of this is just that you have your Dev set, uh,
you might as well see how you're doing on it and use that to inform test set evaluation.
It's worth emphasizing again that absolutely
under no circumstances should you do this with your test set.
That would just be like an incredibly blatant form of cheating
where you decided to stop opportunistically when you saw the best performance.
But that's what your Dev set is for,
uh, so I encourage a lot of this.
I mentioned before here that, you know,
the low error rate doesn't always correspond to good performance.
Here's an actual pair of learning curves.
I have macro F1 here and the number of iterations.
And for whatever reason on this run of the model,
performance got really good and then kind of leveled off.
Meanwhile, over here error versus iterations,
the error just kept getting lower and lower.
But obviously this was arguably having a negative effect or no effect at all,
even as this error went way down,
and that's just because at a certain point you start to over-fit to the training data.
[inaudible]. [NOISE]
I mean, I- I guess it's a wild world that you might see that.
[LAUGHTER] Um, but versions of this
might be something that you actually use, this kind of debugging.
So for example, if your learning curve- your- sorry,
your error doesn't look approximately like this,
then there might be something wrong
in the optimization setting that's causing it to oscillate,
or even like have the error get larger,
and it would surprise me if that corresponds to
good behavior kind of over here on this left panel.
Um, and I guess the lesson there is just you might observe both of these things.
That's another advantage of TensorFlow is that with- with, um,
TensorBoard monitoring, you can see all of this,
and kind of get a feel for what your model is doing.
But certainly look at both and don't take one as a proxy for the other.
That's kind of my- my overarching point here.
Here's what I think the best response to all of this is, right?
Because of- the way I was just describing things before,
we were still kind of in the mode of saying,
I know that my model varies by epoch.
I know that that affects test set performance,
but still what I wanna do for you is report one single number on a test set.
And if you think open mindedly about this,
that's kind of misleading because what I'm telling you is that there's lots of
variation along these learning curves as you- as you train on more data.
So I think the best response is to accept that incremental performance plots,
like the one here,
are really how we should be assessing these models.
Because that exposes much more information about the variation that you actually observe,
and can give you actually new ways of
arguing for your model as I'll show you in a second.
The other thing I would say is that in deep learning, you know,
in principle if you think about the theory,
these models should be capable of learning anything.
So what we're really thinking about is how efficiently they can learn,
or how stable their learning patterns are and so forth,
and that's probably, ultimately,
where your real argument lies.
And here's the kind of an example of this from my own work
from a paper that I did last year with Nick Dingwall.
So the background here just quickly is,
we were arguing for this model called Mittens.
Mittens is a warm start for GloVe.
The argument is that Mittens are warmer than GloVes.
And really the pitch here is that what you could do is
start from like vectors that you had downloaded from the web,
and then run GloVe on them with a kind of
retrofitting learning objective to update them for a specialized domain.
And in the paper, we do this kind of updating for a bunch of different domains.
And what we're trying to do is show that you can do well,
if you do that kind of updating.
But the way that we actually evaluate them is kind of extrinsic.
So the evaluation is that they are
inputs to various kinds of other machine learning models.
And in this case,
we're doing a sequence prediction task for healthcare where
you're kind of talking about people being diagnosed with different diseases.
And the model is an RNN that for each token is predicting whether
it's like talking about a concern for a diagnosis,
or ruling out a diagnosis,
whereas in other tag or whether it's
a positive diagnosis for the disease in kind of clinical texts,
the kind of thing that you'd see in a- in an electronic health record.
So these are RNNs fundamentally.
The full story here is that we didn't do a lot of
hyper-parameter tuning because our rationale was that would be expensive,
first of all, for these RNNs because they need to run for a long time for convergence.
And the two models are,
our main competition is like Mittens versus clinical text GloVes.
So GloVe just trained on the text we have available,
and not given a warm start from the downloaded vectors.
That's our main competition.
And all we really care about is the Delta between them.
Really, we're trying to see whether Mittens is better.
And otherwise these RNNs are kind of the same.
Like in your- and the inputs are scaled in similar ways,
Mittens and these various flavors of GloVe.
So we used that as an argument to just set the hyper parameters,
like the dimensionality to the network and its activation function and so forth.
But we did observe a lot of variation in how these models would
learn because of randomization in their parameter initialization and so forth.
So we felt it was important to show these learning curves,
and that's what we're doing here by category,
along the x-axis is the number of training epochs and the y-axis is F1.
And I like this really full picture because if I step back,
I can say that we- if you look at macro averaged,
have a kind of modest argument overall,
that for this data,
Mittens learns a bit faster.
So if you can only afford to do like 1,000 training epochs,
then the warm start for this domain is actually helping.
But over time if you can run to 10,000,
the difference between those two models has disappeared
between clinical text GloVe and Mittens.
But you can also see that,
no matter how long you run,
these things have kind of leveled out,
and both of those models are better than generic GloVe vectors,
or random initialization like that's the yellow and the- and the gray.
So that's the kind of full picture,
but it gets pretty nuanced if you look at these different categories.
So like, I'm not sure that that model really-
that argument really holds for concern, for example.
There, it's very hard to see a difference between the red and the blue.
Maybe it's a bit stronger for positive up here where the error bars are tighter.
[NOISE] I like this,
you know, kind of like in early phases of learning,
Mittens is good, but in the fullness of time a lot of the differences disappear,
at least between those two competitors.
It would be a very different picture.
I would be giving you a very different story here if we had decided to just pick
the best point of performance for each one of
these categories or the best point of performance from macro averaging, right?
There it might look like a stronger cut and dry argument in favor of Mittens.
But you can see that would be a bit opportunistic, right?
And so I kind of, you know,
I think things like open an honest reporting of how these models are doing.
This is like our attempt to do that.
And so I don't know, I guess I'm saying that I would love to see
plots like this in papers you all produce.
And I recognize that there's often a lot of pressure in terms of
information overload and in terms of culture to give
a single Macro F1 number, or something like that.
But if you feel there's a room for this kind of more nuanced thing,
then go for it.
You have the teaching team's full support, I would say.
And this plot here,
since all that's differing here is
the random initialization and other kind
of random things about how these models are learning.
That brings us to this other point here,
the role of random parameter initialization, right?
So, you know, you- when you set up the weights for your model in these settings,
you do it with random initialization in these kind of non-convex optimization problems,
that initial setting might be determining a lot about how the model ultimately fares.
Um, but I will say that even simpler models can be impacted by this.
If there's multiple optimal solutions,
then they could get steered in one direction or another.
A kind of startling result from a couple of years ago,
is this paper that kind of did a bunch of analysis
of recently proposed neural, um, language models.
And the striking finding of this paper is that,
if you ran these systems a lot of times,
then the differences between them essentially disappeared because the way in
which they had been randomly initialized was having
a big impact on the final outcome for the models.
And it will kind of put a damper this paper on the feeling that
recent proposals were vastly better than
earlier ones because kind of when it all washed out,
the differences were small or nonexistent.
And this was all tracing to different initializations.
And this is sort of disturbing also for statistical testing.
Different initializations can lead to statistically significant differences.
And there's a related thing here which is kinda the extreme version of this,
which goes under the heading of catastrophic failure.
So you have an unlucky initialization of your model,
and it leads to not only bad performance,
but like effectively zero performance.
Um, my feeling is that,
the smart, the good,
and open thing to do is to report the number of times that that happened,
maybe in the pros of your paper,
so that you don't destroy the results table,
but it is important, I think,
to be forthcoming about how often this was happening,
and how much it was shaping the final means and
confidence intervals that you reported. Sorry.
So since the- the initializations can have these big differences,
and people aren't going to be publishing, you know,
results where they're not beating state-of-the-art.
Do you think that there is this kind of, like,
just random hill-climbing that happens because people all around the world are- are
basically doing these random initializations every time they attempt one of these papers,
and occasionally people will get lucky,
and beat some benchmark,
and they will get published and someone else will get even luckier.
Like is that what you think is happening,
or is that not really a big concern?
I think it is happening,
and I think it's a concern,
and that's sort of the story not to impose too much on these authors,
but that's kind of the story of this paper here.
Uh, and that- all of it brings me to
the idea that we should be reporting things that look more like this,
that show us the impact of all of these different choices,
and not so fixated on a single summary number
because it's probably overstating the degree to
which we're making inexorable progress on these problems.
It's kind of- like, again,
to bring up the SNLI leaderboard,
there are so many systems that have been entered onto that leaderboard now,
that you can do the thing of kind of informally doing a meta study,
and saying, okay, broadly speaking,
which classes of models are doing well?
And like here, there the clear answer is- it's some kind of
ensemble of deep learning modular pieces.
Um, so that even if the individual numbers are kind of being overstated,
the overall trend is clear.
And to substantiate this a little bit in the notebook I didn't reproduce it
here but there's a famous problem in, in neural networks.
The XOR problem which is just the logical connective that
gives a 1 to the cases where p and q have different truth values, otherwise 0.
Um, and it's famously not a problem that a linear classifier can learn.
The other one is if and only if where
the values have to be the same to get a truth value of 1.
And like one of the original arguments by the deep learning founders,
um, just shows that you can learn XOR with a kind of a shallow neural network.
And that's great. And it's cool.
And you definitely can theoretically do this thing that you can't do
with a linear model without adding a bunch of interaction terms.
Okay. Cool. But in reality,
if you run my little feed-forward network it succeeds at this 8 of 10 times.
[LAUGHTER] And that's- and
that's entirely owing to the fact that this is a small problem.
And so the initialization really matters.
And you could just run that and get a feel for yourself
that the extent to which this has actually solved empirically.
[LAUGHTER] So response here yeah,
this kind of summarizes what I've been saying.
So report scores for multiple complete runs with different randomly chosen
initializations and then summarize
that variation with confidence intervals or statistical tests.
And then I wanna return to this pragmatic note that I keep sounding here.
So arguably these observations are incompatible with
things like McNemar's test which depend on one run.
And I think that's true.
But I think we have to balance that against
the fact that if your system takes two weeks to
train then more or less starting now you have the opportunity to do just a few runs.
And then McNemar's test might be better than
nothing in terms of informing us about differences.
So let's wrap this up.
Uh, I think I can summarize everything that we did last
time and this time with a few things here.
So your evaluation should be based around a few systems that are related in
ways that illuminate your hypotheses
and help to convey what the best models are learning.
That's the kind of baseline stuff that we talked about.
Every model you assess should be given its best chance to shine, right?
Uh, that's like hyper parameter tuning and all of
that stuff is trying to make the strong argument in this way.
And then the pragmatic note we need to be realistic
about how many experiments we can actually afford to run,
afford in terms of dollars or time or resources.
The test shet- the test set should play
no role whatsoever in optimization or model selection.
This is a note that we've sounded a lot because we did all those bake-offs.
The best way to ensure that this,
this is to have the test set locked away until
the final batch of experiments that will be reported in the paper.
Um, and the other thing that you can do if you don't have a test set is,
is simulate that by careful cross-validation set-ups.
This would be where okay,
you have a fixed body of examples,
you do cross-validation and report the mean.
And the way you try to make a case that you're kind of
hands-off about all of that is that you set up
your models with a lot of hyper-parameter tuning and let
them run hands-off and then just report the final number.
And try to be honest with yourself about the extent to which you're
going back and specially advantaging the model that you're favoring.
You kind of want this to be as hands-off as possible.
And then of course just report this regime very carefully.
Strive to base your model comparisons in multiple runs
on the same splits especially if you're doing deep learning stuff.
Um, because a single model can perform in very different ways on
the same data depending on optimization and moon,
you know, sunspots and whatever else is happening in the universe.
All right. Uh, did that raise any new questions for you or concerns or comments or? Yeah.
Um, I'm still a little bit caught up on the random initialization portion.
I'm just wondering ethically speaking I guess- so I,
I understood I guess one major point of
the analysis and different the performance of various models to learn like
sort of meta-learn what architectures are of high level like
features of the model are good for like good performance on a certain task.
But at the same time is it necessarily bad that people
get lucky and do happen to do the random hill climb and,
and get like successful models that way?
Oh, I think it's not- so the only bad part.
Yeah, like getting lucky could be great.
And it does show us something there's a setting in which you win.
I think the part that at least I was worried about and maybe two is
like if I do that special stuff only for the model that I want to advocate for,
because that's where you're stacking the deck in your favor.
Uh, like I run these models hundreds of times.
And most of the time my model loses and I
report the situation in which my model happened to win.
And you, you probably know in your heart that that's not
an honest reporting, um, that kind of thing.
And I think he's asking,
how often is that happening out there in the world?
And I guess I was saying more than I would like as my guess.
But what we can do is a kind of push the community with
our own choices in the direction of reporting
the mean and confidence intervals on all of those runs.
[inaudible] on that note,
is there any empirical value from those weights that get initialized when you get lucky?
Is it may be something that could be,
could be indicative of a good initialization system and
like could that actual initialization be later considered like the hyper,
hyper parameter like there is- uh,
have there initialization there's other types like that.
Uh, I get that like, see tuning as an idea probably isn't that great but, uh,
is there insight that could be gleaned
from finding what weights worked out or which ones didn't?
I'm sure there is.
Yeah, now that's a great point.
Uh, and I feel like as we get better at getting analytic insights those good
settings where you got lucky we'll
provide more and more intelligence because you could s- what,
what if you did report that for your problem
the standard thing was this kind of, um, initialization.
And the best settings were the ones that deviated the most from it.
But what if they had different statistical properties?
And what if you then took that, uh,
lesson and applied it in a systematic way and found that it
reproduced a lot then you could get an initialization scheme named after you.
[LAUGHTER] Which is real fame
because then you get your name in all of these deep learning packages.
[LAUGHTER] Okay.
Let's do the metrics thing but keep asking questions on Piazza and office hours.
Uh, yeah, I think it's, it's great to be discussing all this.
[NOISE]
But let's do one thing about metrics.
So this is a massively long notebook.
[NOISE] Um, and that's why I say like let's talk about basically
just the first part of this classifier metrics thing
because that'll be enough to give you a feel for my thinking about this,
and kind of get that discussion going.
And then, if you're doing a problem with regression or sequence prediction,
then you can refer to these later sections here, um,
which introduce all of these metrics in just the way I'm going to today,
and talk about their trade-offs.
Yeah. Um, here's the overview of this.
So different evaluation metrics encode different values, and the,
the different values in the sense of like what do we value, uh,
of a system, and they also have different biases and weaknesses.
No- none of these metrics is perfect because
all of them are encoding different values and values can vary.
So you should choose your metrics carefully,
and motivate these choices when writing up and presenting your work.
Um, I would love to see passages where even if you've,
even if you're doing something that's the default [NOISE] for your problem you
still do articulate why it's the choice that you made.
[NOISE] The notebook reviews some of the most prominent metrics in NLP,
and I tried to define them,
but also articulate what values they encode and what weaknesses they have,
and also report relationships between them.
Because one way in which this can be kind of overwhelming
is that basically the same metric is described,
um, and given different names in different ways,
um, when really the differences don't mean much.
In your own work,
you should not feel confined to the metrics in this notebook, right?
Per my first item.
You have the freedom to motivate new metrics and use
cases for existing metrics depending on what your goals are.
Now, culturally, if you're working on an established problem,
then you'll feel a lot of pressure from readers and
referees to use metrics that have already been adopted for the problem,
and there are even these little communities within NLP that
have totally settled on a single metric,
uh, no matter how problematic.
And so you can get in this trap of feeling like you know there's a problem,
but you still have to report it.
Um, you should feel free to argue against those norms and motivate new ones.
You might in your paper want to report the old metrics as
a kind of foundation and then talk about why a new metric,
uh, is a better one for your problem.
I kind of feel like I don't really work in this area,
but for language modeling,
I feel like everybody is in this state with reporting perplexity.
So it's like everyone does it because everyone
expects that everyone else will expect that they do it,
but nobody believes in it.
Still report it.
[NOISE] Uh,
and the other thing I'll say just by way of high level is
the scikit model evaluation usage guide here is great.
It has lots of notes about how to make responsible use
of the metrics they've provided and they've provided lots of them.
In the notebook that I'm going through here,
I've hidden a lot of this,
but I defi- redefine the metrics just so that you can see how they work,
but m- my advice to you would be to use
the scikit ones because they had been more heavily tested,
and tend to have more options for doing things.
So but let's just do this classifier metric thing and then kind of linger over this.
And this is kind of nice because this is
the stuff that you've probably encountered the most and feel like you
already fully understand and that the- my idea would
be that I wanna kind of problematize this for you.
So let's start with the confusion matrix.
Uh, this gives a complete comparison of how the observed,
that is the gold labels here,
and the predicted labels relate to each other.
I'm sorry that that's a bit small, um,
but I have like a simple sentiment problem,
pos, neg, neutral, pos, neg, neutral.
And the idea here is that like the system that made this got 15 positive examples right,
15 negative, and 1,000 neutral correct.
Those are the diagonal elements.
And then the off-diagonal is saying like,
for 10 of the true positive cases,
the model predict negative,
and for 100 of the true positive cases,
the model predicted neutral, right?
You guys have seen these before.
I'm gonna call this example 1.
You should remember when you think about these things that
your classifier probably did not predict into this space.
It predicted a probability distribution over those three labels pos, neg, and neutral.
And to construct this table,
you have to impose a threshold.
You have to say that I'm gonna, for example,
pick the highest of the probability values and just say that is the true label,
and that is what shaped this final picture here.
But that is, of course,
a meaningful thing to have done.
If your classifier say it predicted, [NOISE] uh,
like fro- [NOISE] to make it easy to think of
a binary problem where the threshold will be 50%.
Suppose your classifier reliably predicts for the zero label that it's like 0-20%,
and then 20-40 for the positive label,
you're confusing ma- confusion matrix will look like you've got everything wrong,
when in fact with a different threshold,
your model would be perfect.
Um, so keep that in mind and that, this can happen,
especially with very imbalanced classes that the classifier
just kind of never predicts high values for things,
even though it is discriminating.
And I'm not gonna go into this today,
but metrics like average precision and precision-recall
curves they can expose exactly that kind of behavior in your classifier,
and give you a, a really strong argument that you're doing well
even though the default ma- confusion matrix would look bad.
And the other thing I would say here is that we might care about the full distribution,
and that's being completely hidden by this confusion matrix,
but this is nonetheless the basis for the metrics that I want to go over with you now.
So accuracy. This is the metric
that we all probably think of at some level when we think about assessing the,
how good a system is, at least before you enter
this field to think accuracy will be the right metric.
This is just the sum of the correct predictions divided by the sum of all predictions.
And I have the same confusion matrix here and I just highlighted
the diagonal because what accuracy is doing is summing those values,
and dividing by the total of all the values in the table.
And so here this,
with this classifier you get an accuracy of 81%.
[NOISE] Yeah.
It's useful to just know the bounds,
I did this for all the metrics,
um, and here obviously at 0 to 1.
0 the worst, 1 the best.
What value is encoded by accuracy?
You might argue that it encodes a kind of core value that we have for classifiers,
that is how often they're correct.
Um, and in addition as I'll show you in a second,
the accuracy of a classifier on a test set will be negatively correlated with
the negative cross entropy loss which is a common loss for classifiers.
So in, in that important sense,
your classifier is probably optimizing accuracy or
rather an inverse value of accuracy and that does make it a very natural fit.
And I, I wanna return to that when we think about how
this is all interacting with, um, Hyper-parameter tuning.
But just keep in mind that even though we don't favor accuracy as the metric,
it is what your classifier is doing, probably.
So weaknesses of accuracy.
It does not give a per-class metric for multi-class problems,
uh, and we might want that kind of nuance.
[NOISE] But the more important thing is that accuracy just completely
fails to control for size imbalances in the classes.
So for example, consider this variant here of the above classifier,
ex1, where the classifier only ever predicts neutral.
Because neutral is a large category,
it looks like a really good classifier from the point of view of accuracy.
Its accuracy is 0.87 for this,
for this confusion matrix versus 0.81 for the one before.
But I really feel like this is a better model.
If you think about actually discriminating between the labels,
this looks better than this one,
which is just stupidly always guessing neutral and
benefiting from the fact that neutral is a massive category.
And this is why we have never used accuracy in this class because you could only
ever trust it even a little bit for a completely balanced problem.
[NOISE] Makes sense?
And then the other part of this little framework I've
constructed here is just what's related to accuracy.
And so I didn't want- just want to state that accuracy is
inversely proportional to the cross-entropy loss,
um, and the cross-entropy loss and in turn accuracy can both be,
be related to KL divergence.
And the value of that is just that if
your classifier is predicting a probability distribution,
you might also think about learning from a probability [NOISE] distribution,
and kind of really embracing
the probabilistic nature of your [NOISE] classifier end of the data,
in which case you'd probably use KL divergence as your metric and it would
also be what your model was optimizing in some re-scaled sense.
[NOISE] So this is kind of like,
like nice little network of
related things that very deeply connect with what your model is doing,
and then we have this kind of mismatch that it's what your model is doing,
but it's probably not what you want to report as your metric.
You know we want to report something like macro F1.
But you can't in a straightforward way optimize for macro F1.
So that's worth keeping in mind,
and in my experience it's worth keeping in mind
because when you do hyper-parameter tuning,
you might do that tuning against your true metric which is macro F1,
uh, and pick the model that maximizes that value.
But you should remember that your model is
doing something different and might actually be
steering you outside of the space that you actually wanna be in for your problem.
That's a kind of limitation of this,
this mitch- mismatch that we're encountering.
Make sense? Any questions about that?
[NOISE] This, oh, go ahead.
Is it variables function for F1 macro average?
[NOISE] I've- um-
I'm afraid not because the macro F1 is this kind of holistic thing about
how you're doing on an entire subset of your data and so it would have to be,
correct me if I'm wrong here, [NOISE] but some kind of loss
that was more like a reinforcement learning loss,
and not something that was directly like
stateable as a differential loss on your classifier.
Yeah. That kind of problem though with accuracy you know like imbalanced classes.
This is what leads to precision and recall and the reason I like that we're doing
this one in particular is that again and again through the notebook,
you see that precision and recall is what
people have in their back- in the back of their minds.
So like when you get to like BLEU scores,
BLEU scores are kind of precision-recall, you know, um,
word-level acc- it's what's sometimes called accuracy is also a precision-recall balance
and even some of the regression metrics are kin- trying
to do something that's like bouncing precision and recall.
So this is kind of the heart of it.
So precision in these things is the sum of
the correct predictions divided by the sum of all the guesses.
Is a per-class notion and in our confusion matrices,
it's the diagonal values divided by the column sums.
That's what I've indicated with this coloring here
that we're kind of operating column-wise.
So here are the precision values for this,
uh, example one that we've been dealing with.
So high neutral, low negative, and modest positive.
For our problematic all neutral classifier,
precision is kind of undefined because we made zero guesses going down.
So we end up dividing by zero.
The standard thing is to map those to zero but
you should keep in mind that it was strictly speaking undefined and this,
this affects the bounds.
If you think very carefully and mathematically,
you can't quite say that the bounds are 0 to 1 without this caveat
about what you do in the case that you have an all 0s column like this.
Then we have high precision for this neutral [NOISE] category.
There's that caveat there about the bounds.
What value is encoded by precision?
The way I would articulate this is that i- it encodes
a conservative value in that it penalizes incorrect guessing on a per-class basis.
The weaknesses of precision.
Precision's dangerous edge case is that you can achieve
very high precision for a category by rarely guessing it.
So consider this one here; example three,
what I did is just make a minor change where I have it
predict once for each of these and it happens to be correct.
I think this is not a good classifier in some intuitive sense.
But its precision is perfect for positive and negative because it just kind
of withheld judgment for all but these two cases where it could do well.
Compare that with example one where, you know,
the precision was pretty low here but it's intuitively
kind of got a better grip on the positive and negative categories.
So the counter par- part to precision is recall.
Recall is the sum of correct predictions divided by the sum of all true instances,
again per-class; and in our confusion matrices it's the diagonal divided by the row sums.
[NOISE] So here are the recall values for example one.
Important point about all of these metrics that are related to
F scoring is that recall trades off against precision,
and that's the way in which each of them is kind of
making up for the faults of the other.
So here's that example three again.
Remember, we make one guess for positive, one for negative.
So precision is perfect but recall is dismal for both of
those categories because in withholding its guesses for those classes,
it ends up making a bunch of
recall mistakes and as a result like the F1 for this is going to be pretty bad.
[NOISE] Recall is straightforward bounds,
and I would say this is a nice balance here.
So recall encodes a permissive value in penalizing only missed true cases.
Whereas precision was conservative, recall is permissive.
Weaknesses of recall.
Its dangerous edge case is that you can achieve very high recall for a category
by always guessing it and that could mean lots of incorrect guesses.
But since recall kind of intuitively,
if you think about the calculations,
it sees only the correct guesses.
So it doesn't really mind about this over guessing.
Um, you can see this here actually.
So for the neutral category, uh,
it made a lot of guesses about neutral, um,
but it missed none of them,
and so its recall is actually very high, down here it's 1.
[NOISE] Yeah, just always guess a category and you're sure to have perfect recall.
That's the sum of it. But in turn your precision will be hit somewhat.
F scores which we've used pervasively in this course,
they kind of codify this balance between precision and
recall and so the standard way of doing this is to
combine precision and recall via their harmonic mean with
a value beta that can be used to emphasize precision or recall,
and we've done the default thing of always balancing them equally.
And that's F1 down here,
where you just pick the value 1 and it simplifies down to this expression.
But you know again you should be thoughtful about
which one- whether you default to this equal balance,
uh, and one way to think about this is like,
so a bunch of people in this course are working on things like hate speech and toxicity.
So just as a quick use case.
Suppose you are developing a system that's going to
help with interventions for that problem.
It's an open question whether you want the system
to be biased in favor of precision or recall.
Um, if you have
very few hum- human resources for doing manual review of your model predictions,
then you want to favor precision as you want to make really good use of
those humans and have them see only things that
your model has high confidence are like toxic.
But, that's going to have the tradeoff that a lot of
messages that really ought of have been filtered
out of your community are going to make it in.
Because what you are optimizing for there,
at a higher level it's like saving human time.
Conversely, if you have a whole fleet of humans who,
who can help you with this task,
then you could be biased in favor of recall if they don't mind reviewing a lot
of score- a lot of texts where actually it was fine to just let them onto the platform.
It's really going to depend on what you're trying to do
but I think whatever your actual situation is,
it's unlikely that precision and recall are perfectly balanced.
So I guess the only justification for us,
perfectly balancing them all the time,
is that we don't know what else you're trying to accomplish in the world.
So here's an example for- for our basic example,
these are the F scores here.
And I think they kind of align with what we
think of about how the model is doing on these tasks.
And there's for example two and and strictly speaking
both for positive and negative because you had undefined precision for uh,
for positive and negative,
uh the F Score is also undefined but typically that will be mapped to
0 or like for scikit that will be
mapped to 0 and it'll print out a bunch of warnings for you.
And here's example three where we had
such high precision for positive and negative but very low recall.
And that's nicely reflected in the fact that
the F1 scores for pos and neg are also very low.
Bounds are 0 and 1,
and you have a guarantee that it will be between precision and recall.
What's the value encoded by F Scores?
Here's one way that I tried to articulate it.
So it's an attempt to summarize how well your classifiers
predictions for a certain class K align with the true instances,
where what I'm trying to bring out with align is
like not only giving you credit for the hits,
but also detracting for the misses,
in the kind of in the case of F1 in a symmetric way.
And intuitively for all these things precision and recall are
keeping each other in check somehow for these calculations.
And as- and if you go through the notebook on your own,
you'll see that theme recur again and again outside in the classification context.
What are the weaknesses of F Scores?
So there's no normalization for the size of
your data-set within the class that you're focused on or outside of it.
Um, and then the other thing that you might highlight here is that for a given F score,
if you think about like focusing on pos here,
pos F1 pays attention to all the row values and all the column values,
but it ignores everything that's off of those rows and columns, all the values.
And to kind of illustrate that what I did is here I have example
one where down here in the corner you have 1,000.
And then for this dataset I changed that to 100,000.
For pos and neg,
F1 is the same across these two data-sets.
And that's worth keeping in mind because obviously these are very different problems.
These are very different datasets and the classifier is doing
very different things because I made this so massive.
But, F1 was insensitive to that.
Only the neutral F1 changed because of
course that value is- is on its row and its column.
But keep that in mind, right?
When you do this F1 you're kind of ignoring one aspect of the data,
when you think about a per class basis.
Related to F Scores,
so dice similarity for binary vectors which you might remember from the first unit uh,
and is often used to assess kind of how well you uh,
a model's predictions align on a set theoretic way,
that's equivalent to F1, especially,
if you think on a kind of per token basis for a vector of predictions.
Um, and then as I said the intuition of F Scores is common.
Let's do just a few of these macro averaged things.
So macro averaged F scores.
That's the mean of all the F scores for each category.
That's the one that we've always used.
Here's a quick example.
You guys know about this one.
I mean basically what we're doing here is just taking
all the per class F values and averaging them.
And that kinda goes here to the weaknesses.
So I typically- I default to macro F1 because I like
the fact that it's giving each class no matter
its size equal weight in the final calculation,
and the justification for that is that in NLP
very often it's the smallest classes that we care about the most,
and so it makes no sense to give more weight to
the ones that are large cause maybe they're easy and uninteresting.
But you should keep in mind that in- in thinking about macro averaging,
your system's metrics might look out of step with how it actually performs in the world.
Because if I just turn it loose on
real data that have the same class balance as the data-set it was developed on,
then you know the fact that it makes a lot of
correct predictions is gonna be
a meaningful thing in terms of people experiencing the system.
And just because it's a large class doesn't mean that it's
any less meaningful if you think about performance in the world.
So in a funny way,
your macro average score could end up overstating how well you
actually do because you know a tiny class that you
never experience as a user with the system was contributing just as
much to the kind of example that it sees all the time in practice.
So that's worth keeping in mind in terms of practical applications.
And then I think these two kind of balance each other.
So you can both over and understate how well
your system is doing by doing macro averaging.
But nonetheless, I think when we think about system evaluation in this context,
the macro average choice is a really good one.
There are also weighted F Scores.
Scikit reports these which just gives a weighted average based on the class size.
And then there are micro-averaged scores and micro-averaged scores
with just a few qualifications are exactly identical to accuracy.
And so they inherit all the problems of accuracy and that
makes them a kind of disfavored option and in fact by and large.
Why not just report accuracy as- as
opposed to choosing the more convoluted micro averaging approach.
But I did include it here for completeness.
And I think I'll stop there cause I wanna leave plenty of time for Atticus,
and I also feel like now you're armed with a kind of framework for thinking about this.
What value does it encode,
what bounds does it have,
and what are its strengths and weaknesses.
It's worth asking that for any metric that you encounter and certainly if you decided to
propose your own metric you'd wanna
kinda fully command that space of different considerations.
Okay. Unless there are questions.
Yeah.
Why are macro-averaged F1 scores better than weighed F scores?
The weighted scores will have that fact that they'll favor large classes,
just like micro-averaging would.
That might be what you wanna do.
Uh, it really depends- I guess I'm encouraging you
to think about what you're trying to achieve,
what your system is trying to do,
and then choose a metric accordingly.
And my- my justification for macro averaging is
just that we often do care about those small classes.
We really wanna get traction on them.
And that- that can be kind of dispiriting if you're doing really well
improving on the small classes but you chose weighted or micro F1,
and you just don't see any impact on the number.
It's like I doubled the performance,
I doubled my F score on this tiny class and weighted- weighted F1 says, ''I don't care.
It's too small for me to care.
I go up by a little bit,'' whereas the macro one will reflect all of those gains. Yeah.
If you wanted to develop say like a conversational agent that appears human,
I just basically pass the Turing test.
What type of metric would be best suited for something like that?
That's so funny to think though, you know.
Turing's answer was the Turing test.
And he did have some kind of accuracy calculation, right?
Wasn't it like fooling it like two out of three times or something,
I forgot. Was it two out of three?
And it's specifically like the game is you,
since it's a classification where there's a human and a machine,
they have to call the machine the human and the human machine,
like they have to get the label swapped.
So that was his metric but the lesson of the time is that
that people confuse humans for robots and the reverse all of the time.
Did we tell that story?
I guess we didn't. From the first ever real Turing Test.
Um, the human who is most often rated as a machine was a Shakespeare expert.
She could answer any question about Shakespeare,
and people's rationale was,
''No human could know that much about Shakespeare.''
[LAUGHTER]
All right. Let me turn it over to Atticus [NOISE] to continue these themes.
[NOISE]
Hello, I'm Atticus.
I'm one of your course assistants.
And I'm gonna give you a little presentation on
evaluating NLU models with harder generalization tasks.
All right. So going through this,
I'm gonna start by just kind of overviewing
the framework that Chris has been talking about which is
just a standard generalization framework of arbitrarily setting
aside training examples and testing examples from a large dataset.
And then I'm going to introduce you to
some adversarial testing literature where people are trying to develop
more challenging generalization tasks to probe at the capabilities of NLU models,
and then I will conclude by sharing some of my research with you.
[NOISE] Cool.
So this should be a framework you're very familiar with by this point.
It's what we've been doing in our homeworks and in most of our bake offs,
where we find a dataset for an NLU task.
And then we just arbitrarily split this dataset into training and testing.
And we trained models on the training set and evaluate on unseen testing examples.
But actually in bake-off 3,
we did something a little bit different than this.
So in bake-off 3,
if you remember, we did NLI on single words.
We were just trying to predict an entailment relation for pairs of words.
And in our edge-disjoint case,
we followed this standard framework where we
arbitrarily set aside some examples for testing and some for training.
But in our word-disjoint case,
we did something a little bit different,
where we ensured that in training and testing,
no words would be shared between these two sets which creates
a harder generalization task as it expects models to generalize to unseen vocabulary.
So kind of like the high level theme of this presentation is that I'm
going to encourage you to consider breaking from the standard evaluations,
and try to create generalization tasks that are difficult,
well-motivated, and that answer specific questions about model capabilities.
[NOISE] So to get introduced into the adversarial testing literature,
we're gonna start with the example of question answering.
So consider the research question,
can a model learn to comprehend a passage of text?
So this is a very high level and ambitious research question.
And people here have actually tried to answer
it by creating the Stanford Question Answer Dataset,
which is an awesome resource that you might wanna use in your projects.
So examples in this dataset look like this,
where there is a passage,
a question, and an answer.
The input is going to be the passage of text in the question,
and the output will be the answer.
In this case, it's about a passage about football,
question about a quarterback,
the answer's John Elway.
So you might think to answer our research question,
can a model learn to comprehend a passage of text,
that if a model achieves human level performance on this dataset,
then it is able to comprehend passages of text.
So if you do think that,
then you're in luck.
Plenty of models have beaten humans at this task.
So, you know, we're done.
Question answering solved like everything's great and no more work to do.
But unfortunately, it's not actually quite that simple.
So [NOISE] you might have a suspicion that
these models don't understand language quite as deeply as you might hope they do.
And Jia et al.
actually devised an experiment to see whether this is the case.
They take training examples from the SQuAD dataset,
and they systematically perturbed them to create
a new adversarial test set that they then evaluate models on,
uh, and using this as a new evaluation metric.
So our example before with the football.
We have a model prediction of John Elway which is the correct answer.
But what they do is they append a single misleading sentence to the passage
to trick the model into thinking that
Jeff Dean is the quarterback that the question is asking about,
and which is just incorrect.
So [NOISE] when they generate a new adversarial test set using this technique,
they find that 16 published models trained on SQuAD dropped from a 75% test,
a 75% F1 score on the original test set,
down to a 36% F1 score on this new adversarial test set.
So sad; question answering is not solved,
which you might have suspected.
So a natural idea is we've identified a hole in the
generalization capabilities of models trained on the SQuAD dataset.
So to patch this hole you'd say, "Okay,
let's just take these types of examples and include them in training."
And this does work,
when we include these types of examples in training,
those models learned to ignore the final sentence,
and they now- this patched model will now make the correct prediction of John Elway.
[NOISE] But these models- this new patched
model trained on these types of examples is
now vulnerable to a different adversarial testing set,
one where we just prepend the sentence instead of appending it.
And so now, we see that
this problem seems to be deeper than just throwing in more training data,
and having the model become more robust that [NOISE] different perturbations,
being trained on one perturbation of the data,
doesn't generalize to even similar perturbations like this.
And so- oh, yeah.
Are you- are they algorithmically inserting this? Like did-
Yes.
Or they didn't do it manually?
So there- it's a process of algorithmically,
and then I think it's verified by Mechanical Turks,
that it's consistent with the passage.
So they have like
an algorithmic generation process for these types of sentences that are misleading,
but then I believe that they have their own Mechanical Turks that go
in and verify whatever sentence they put in is going to be consistent with everything.
I see.
Yeah.
But they didn't do it based of the model itself,
like they didn't look at the way the model was working and then insert [OVERLAPPING]
I think they did do another experiment,
it's not this one.
It's- I think it's in the same paper where they just add tokens arbitrarily,
don't have to be grammatical or anything just to try to make the sen- the models fail.
And they get the F1 score down to 6% on that adversarial testing data.
But you also might have some like, I mean,
it- I think this one's a little more convincing in that this is English,
and the other experiment that gets down to
6% is kind of like throwing at random stuff.
That's not even really going to be grammatical English.
Yeah. And also something you can look at is SQuAD 2.0 tries to
address this make it a little harder by adding the option to say,
"Oh, there is no answer to the question in this passage."
Cool. Any more questions?
Okay. So we're gonna move from question
answering to NLI where in the last couple of years.
Um, [BACKGROUND] oh, sorry.
Yeah, that's all right. Um-
This is, like, a big question, but, like,
if you could include- I think all possible perturbations, um,
[NOISE] train on those, like,
what use- would you be able to say that,
like, you solved that test?
Uh, no, I, I think that's a good question.
That's an actually an experiment that I was really
interested when I read this paper is not pre-pending, appending,
but what if you had a training and testing data where you inserted
this misleading sentence at every possible location in the paragraph?
Because then you could think that the- you know,
I'm not sure if the tech models would even be able to,
like, generalize across those two things.
But yeah, no, I think that's really interesting.
Because- well, also it's important to remember that this is, like, one way to,
like, mess with the model, but of, like,
an infinite classes [LAUGHTER] of ways to mess with this model.
So me, I, I definitely think that's like an interesting question.
Cool. All right.
So moving on to NLI.
[NOISE] So in the last couple of years,
there have been, like,
a growing number of more difficult generalization tasks that are trying
to expose the fragility of models trained on the SNLI,
and MultiNLI datas- dataset.
[NOISE] So one of these experiments tries to
isolate the capability of models to perform lexical semantic reasoning.
So what they do to create
adversarial testing examples is they take a premise from the SNLI dataset,
they take a single word in that premise,
they exchange it for another word to generate a hypothesis sentence,
and then that is the adversarial example.
So you can see, there are three examples here,
and each of these examples,
the label for the sentence is directly from a relationship between a pair of words.
And in this paper,
they expose that models don't have
quite the robust range of lexical semantic reasoning that you would hope they would,
when being trained on these massive naturalistic data sets. Yeah.
[inaudible] first one in,
how do we know that some are holding a saxophone can also [inaudible] guitar, like one chance.
So that actually gets in more less about
their paper and more into the nature of how SNLI was created.
So SNLI was created with image captions,
deri- describing a literal scene,
and what Turkers were told to do is generate a caption that must be true.
So they're given a caption of an image without that image.
They're to generate an entailment relationship, they say,
they are asked to describe a sentence that
must be true of this scene for a contradiction relationship,
they are asked to generate a sentence that is not true of this scene.
And so, that kind of,
like, results in this more, like,
fuzzy idea of contradiction,
then on kind of, like, a strict logical idea of contradiction.
And so give an example, like,
what makes it different from an SNLI sampling rather than,
like, what's adversarial about it?
So what's adversarial about it is that- so these,
these pairs don't occur in the SNLI dataset.
Only the first sentences do.
And then, they just take a single word,
and they exchanged that word for something else.
So this could be something that might occur in SNLI,
but I guess, that's kind of the idea around adversarial testing.
Is that, like, it should be similar enough to
what we're training on that it's a justified task,
but the interesting part is they fail on these.
Compared to the SNLI test set.
So I guess what makes it adversarial is they're doing worse than they
are on the actual SNLI test set. So yeah.
Okay.
Yeah.
They accidentally made it do better.
Accidentally made it do better. So yeah.
So actually in this paper,
they have a bunch of different- they break it down to
different classes of words they work with, they have, like,
planets, like fruit, animals,
and so PyClass, some of the classes as they do very well on.
And then some of the classes they seem not to be exposed to those types of words as much,
and so, you can actually see a full breakdown.
So if you were to actually use, like,
for example, they have, like,
one thing they do is planets.
So like, replacing Venus with Pluto.
And it seems S- that models trained on SNLI just are really bad at that,
I guess because it's down to
like, 10% or something,
but in in other classes of words that I guess are much more familiar,
they get above what they get on the test set for SNLI.
So yeah.
I, I think it's a good idea to really, like,
take a look at the details in these types of papers because they, like,
really can expose, like,
a range of nuanced capabilities.
[NOISE] So moving on from lexical semantics.
There's also experiments that try to determine
whether models are capable of compositional semantics.
So here's two examples from this paper by a Nie and Wang.
So the first one,
they just take the subject and the object,
and they swap those two.
And so, woman is pulling a child on a sled in the snow,
child's pulling a woman on a sled in the snow.
And then in this example,
what they do is they take a premise sentence from SNLI,
and they take an adjective in that sentence,
and just move it to a different noun.
And so, we both- well,
we all understand that these sentences are not in
the entailment relation because moving words matters a lot.
[NOISE] That's kind of the whole thing around compositional semantics.
But what they found is by doing this,
they trick the models that are trained on SNLI,
and the- they often mistake these types of examples for entailment relationships.
Because they think, uh,
yellow- yellow those are similar words, entailment.
Yeah. [NOISE] And so another experiment isolates a specific compositional frame,
it's I think a little more, [NOISE] like,
specific than the other ones I was talking about, where it's just, like,
the X is more Y than the Z, and so,
it's just found that as models trained on SNLI just can't do these kind of examples.
And this one makes,
like, a little more sense,
isn't that surprising to me because it's like
a very particular compositional frame that I don't think
would occur that much when with literal image captions.
So yeah. [NOISE] So yeah.
So something you might be wondering is,
like, if they don't learn
in lexical semantics,
and they don't learn in compositional semantics,
what, what are we really doing here, right?
And I guess what I would guess is what I would say is
that models are learning a specific slice of lexical semantics,
and/or compositional semantics, that is kind of hand tailored to
the SNLI test set which the community has been hill climbing on since its release.
And so I think this is actually an exciting thing for your guys' projects because
it's kind of intimidating to try to get a new state-of-the-art score on SNLI dataset,
but for these adversarial testing data sets,
no one's been hill climbing on them.
So I think there's a lot more room for innovation and
improvement that you could do in your projects.
[NOISE] All right.
So now, I'm gonna move on from
adversarial testing onto and share a little bit of my own research.
So in my own research,
I've been constructing artificial natural language inference datasets.
And they have examples that look like this.
So like, every tall human does not kick
any large rock contradicts no human angrily kicks some rock.
So they're like, pretty complicated and hard to parse as humans,
but this is kind of how they look.
There's two quantifiers, negation, adjectives, adverbs.
And my kind of original intent with this dataset was to stress test,
and align models with learning these kind of, like, logical sentences.
And so, [NOISE] oh, also,
I designed a task-specific model which is I call the CompTreeNN model,
which kinda jointly composes a premise and hypothesis together
by aligning all their words and then composing them together up this tree structure.
And so there's just a task specific model that should be
particularly good at doing [NOISE] this kind of reasoning.
And so, the first thing I did was just do a standard evaluation on my data.
Where I arbitrarily set aside some examples for testing,
and some examples for training.
And as you can see from this results table, standard neural models,
and my task-specific model,
all achieved very high accuracy on this standard evaluation split.
And while when I did some digging,
I found that the standard neural models fail to encode the identity of verbs,
nouns, adverbs, and adjectives,
which is a pretty serious flaw,
and my task-specific model performs perfectly,
what I realized was that my-
the standard evaluation on my dataset was far easier than I expected it to be.
The- when I thought about it more,
I realized that every combination of quantifiers and negation,
and modifiers occur in my training dataset.
Because I have 100 nouns,
100 verbs, 100 adjectives.
So all these complex function words are going to
appear in every possible order during training.
And so a model that could do well is just one that
memorizes these quantifiers and negation,
and then just assigns them labels.
So what I decided to do is I wanted to construct a
more challenging training and testing split, that really, um,
precisely [NOISE] asked a question about what a model is able to do.
And we're gonna just keep it very high-level on
[NOISE] what a baseline model that performs natural logic reasoning is,
but I constructed a simple baseline model that does perform natural logic reasoning,
and I use this model to kind of implicitly define an idea of fairness.
So I have a simple baseline model,
and I consider a training dataset to be fair,
if this simple baseline can learn a perfect solution from the training data.
With the idea being that neural models,
if they're able to perform the type of reasoning my baseline model is able to form,
should succeed at the same generalization tasks.
And what I found was that standard neural models
fail miserably at this new more challenging generalization task,
and that even my task-specific model fails to achieve perfect performance,
which was pretty interesting that even something with
a hard-encoded tree structure wasn't able to solve this task.
And so, I guess the kind of narrative here is that at first,
I used the standard evaluation which was interesting,
but by breaking away from that kind of standard framework, I was able to ask,
like, a far more deep question about the capabilities of my models.
And so that's kind of just the high-level moral of the story is you should, like,
think deeply and carefully about what you're learning from your experiments.
Because often, they're going to be far easier than you expect them to be.
And you should consider breaking away from this kind of
standard evaluation of arbitrarily creating a training and test
set to create more challenging generalization tasks
that answer interesting specific questions.
Cool. Thank you.
 I propose we get started,
we've got lots that we want to accomplish today.
Um, this is a tweet that I saw yesterday from a
research group at UMass, uh,
and I thought this is a nice bridge between last week and today.
So last week we talked a lot about hyper-parameter tuning and kind of
very expensive testing regimes
that the field has moved toward in the era of deep learning.
Uh, and I kind of preached a pragmatic approach to all of this,
where you balance your own budget in terms of
money and time and resources and everything else,
when you design your experimental paradigm and I thought,
inevitably, you're going to have to compromise, and the,
the real thing that you want to do in the paper
is just be upfront about where you had to compromise,
which just means being open and honest about your experimental regime.
One ingredient, one factor that I didn't include in that discussion but I
think I will for future years is just the environmental,
uh, concerns that we might have around very ambitious hyperparameter tuning regimes,
and that's what this paper is about.
I haven't seen the paper itself but this seems to
be one of the core tables from the paper.
Uh, and these numbers are quite shocking, right?
So you've got along this column here, CO2 emissions.
And I guess for some baseline things,
it gives you like air travel,
human life average and then American life average.
That already is a kind of shocking statistic there.
Uh, car lifetime.
And then down here, and this is kind of striking because, I mean,
training a state of the art NLP model for tagging,
um, it's 13, CO2.
Uh, but with experimentation,
that number jumps up to almost one year for
the average American in terms of CO2 emissions.
And we're gonna- this is the reason this is a nice bridge is we're gonna
talk about the transformer today among other things.
And again, kind of, a modest but eye-opening
amount of energy consumed for the transformer large,
but once you start to do hyperparameter optimization,
you're talking about more than 10 years of
average American life CO2 emissions to do this hyperparameter optimization.
Um, this is a kind of environment- environmental disaster, right?
Are you interested in deep learning for NLP but also concerned
about the CO2 footprint of training? You should be.
[LAUGHTER] Um, yeah, I really think this is,
this is food for thought,
and frankly, I think this should be part of our thinking about
the kinds of experiments that we run and where we invest our resources.
So I encourage you to check out this paper once it's available.
Onto the transformer.
I guess if I'm successful in my lecture today it will be a, uh,
environmental disaster, so we'll all go forth and train large transformers.
So what we wanna to do for the first part of class today until probably
about 5:30 is talk about contextual word representations,
and then Cindy is gonna take over and do a mini-lecture
on practical approaches to dealing with very long text learning,
dense representations of long text,
essentially which I think could be useful for a lot of your projects, so it's well-timed.
But contextual word representations,
I do think this is one of the exciting things that happened in recent times in NLP.
I'm gonna try to motivate that,
I'll do a little bit of like guided and
high-level insights about why I think this is a wonderful development,
in addition to all the, um,
practical gains that it's led to on a wide variety of tasks.
That's kind of the core idea behind this, contextual word representations.
Then, we're gonna talk about three approaches to learning such representations,
ELMo, the transformer and BERT.
Uh, and then the final section which we might just leave for your own self study,
just introduces you to that notebook that I posted at the start of
the quarter which shows you some easy and practical ways that you could bring
ELMo and BERT into your project even without having to do
a complete code overhaul so that you're embedded in their PyTorch or TensorFlow code.
There's a bunch of associated notebooks.
So in add- in addition to contextualreps.i, the notebook, um,
there's also Smith to 2019 which is a nice overview paper that kind of does more of
providing high-level insights about how we
move from word vectors to contextual word representations,
but also kind of the guiding linguistic and empirical ideas behind this whole movement.
I also wanted to call out that the CS224n lectures on these topics are really good,
and they are somewhat different from my lecture.
So they, they offer much more of the history and
many more connections with different tasks whereas given my limited time,
I've decided that I'm gonna try to just convey to you how these models work,
and with luck, we'll get a sense not only for how
they work but also kind of why they work.
And this is perfectly timed because as you'll
see we're going to build on a lot of stuff that we've already done.
It's like we're assembling raw materials in
a really creative and interesting way to achieve some big gains.
And then, for ELMo transformer and BERT,
I just mentioned here the core papers and
also the project sites which have code
and also pre-trained representations available for them.
In that context I thought I would call out
especially this wonderful kind of notebook paper called,
The Annotated Transformer by Alexander Rush,
who's on the faculty at Harvard.
Um, it's the actual transformer paper.
So verbatim copy with a few comments from him.
But woven into the paper is PyTorch code,
really nice, efficiently written clear PyTorch code,
for every one of the technical concepts so that by the end he has fully
implemented the model and also tested it in a bunch of ways.
And I just think that, I mean,
what a wonderful kind of contribution to make.
It's like with all due respect to the original authors of the paper,
and it's quite a nice paper,
this is strictly better,
like the combination of Alexander Rush plus
that original author team is strictly more valuable because
it- it saves you from
this thing that I experienced all the time in reading NLP papers that like,
I can't quite tell from the language how the pieces
fit together or what the dimensionalities of things are,
and I can't quite do the matrix multiplications in my head.
And then, I screw them up when I try them with pad and paper.
There it's just all there in code,
like every single ambiguity is fully resolved there.
So hats off to him and I encourage that
as a way to study what's happening in the transformer paper.
And I also frankly encourage you to do these things
yourself because not only is it a wonderful contribution but
I'd say there's no better way to deeply learn
a paper than to do something like this, quite an achievement.
Let's move to the high-level motivation as I see it kind of with my linguist hat on.
So I'm just going to show you some examples here.
This is just around the verb break.
The vase broke, dawn broke,
the news broke, Sandy broke the world record,
Sandy broke the law,
the burglar broke into the house,
the newscaster broke into the movie broadcast and we broke even.
So they all involve a word form that sounds like break.
I don't know how many words senses there are here, right.
In the sense that I don't know whether we're dealing with
11 different lexical items or 2 or 6 or what.
I kind of have the feeling in looking at paradigms like this that
the linguist notion that we can cu- cut up the world into different word senses,
fully differentiate them, uh, is breaking down.
The nature is not quite cut up in that way.
It feels like we're drawing on a bunch of
metaphorical connections and linguistic connections that are being shaped by
the morphosyntactic environment- environment in which
this verb is being used together with like its particles,
and what we know about the world.
I think a lot of these sentences are strictly speaking ambiguous in some sense and we
perceive one reading but not others because of
what we know about the world, and on and on.
That feels like the real picture of what's
happening with the verb break in this paradigm.
Here are a few more, flat tire,
flat beer, flat note,
flat surface, or throw a party,
a fight, a ball, a fit.
How many senses of the verb flat are in 2A or throw in 2B?
I don't know. Are they even related?
Are there four lexical items or two or one?
I have no answer to this question but I can
tell that the sense of these verbs is pretty well cut up
once I put it in
a particular morphosyntactic context and bringing
kind of world knowledge about what I expect the readings to be.
And all of this I think points towards contextual representations as the right way to go.
Because if you think back to the first unit of
this course where we represented words in isolation as vectors,
what are we gonna do for one?
We're gonna have to smoosh together all of those senses into a single vector.
or we're gonna have to do the really hard work of kind of
breaking up this break into a bunch of different vectors.
And I think that neither of those is gonna work out.
And moreover, even if it did work out,
we have this kind of average or static representation.
And what we know from these examples is that,
that vector ought to be influenced by the context in- that it's sitting in, right?
That's our intuition that the vase broke
means what it means or dawn broke means what it means.
Because of other words that are interacting with this verb break.
Here's some more examples.
And these are subtly different.
So I caught, uh, a crane caught a fish.
You probably intuited that's a bird.
A crane picked up the steel beam.
You probably intuit that that's a machine.
And for I saw a crane.
You might just be uncertain about what kind of crane it is; bird or machine.
I think that for 3A and 3B,
there are readings where a machine catches a fish or a bird picks up a beam.
But they're just highly unlikely given what we know about world knowledge.
But I don't wanna lose sight of the fact that
you were steered toward one reading or another,
because of the syntactic environment.
The semantic environment.
And also what you know about the world.
All of that is shaping the sense- the word sense for crane here.
And maybe less shaped by 3C.
And then I think we could extend this to examples that I showed you,
when we talk about discourse and dialogue.
So are there typos?
I didn't see anything. We think about the sense of any there at the end.
It's shaped by the preceding discourse?
Are there bookstores downtown?
I didn't see any. Same sentence.
But the sense of that entire sentence and especially any I think there
then maybe its elliptical content is being shaped by the preceding environment.
And all of this points to the idea that static word vectors are just not the way to go.
This is sort of, I was reflecting on this like, if you plucked,
uh, your average theoretical linguist off the street.
And tried to convince them that words could be represented as vectors of real numbers.
I think it will be tough going.
But I think after a little while,
you could convince them that this is a smart way to go.
And in fact, it's quite close to what they think about when they think about
word meanings as kind of very high dimensional objects with lots of relationships, right?
Like, if you think about WordNet,
you could represent those things as vectors.
And kind of, you're not very far from what linguists are already doing.
But I think once you convince the linguist of that,
they would say, Wait a second.
Are these vectors just for words in isolation?
And that's where you would lose them.
Because many linguists think that word senses are
kind of shaped by their morphosyntactic environment.
And then if you talk to them about usage, they would say, well,
that's going to be shaped by the whole discourse context and everything else.
And that's where your idea that words could just be single vectors.
You'd never convince them of that.
And that to, I think ELMo and BERT.
I think you can get them on board with that, that view of meaning.
So an interesting twist.
There's of course lots of
empirical motivation that I've kind of left out of this slideshow.
Because you can find those tables in the papers and all over the web.
It truly is a breakthrough on lots of tasks.
You might have experienced this yourselves.
I think some of our bake-off wins
traced to BERT and ELMo and things like that.
So you guys know about that picture.
So I thought I would fill in kind of this linguistic one here.
Another perspective that I thought,
we might think about here,
just by way of high level motivation is kind of this notion of model structure,
linguistic structure, and the biases that these are encoding.
Right? So this is, like up here for example,
the rock rules where I just take all the vectors for those words.
Those could be like GloVe.
So isolated vectors and maybe sum them or
average them to get a representation of the whole sentence.
That model is very high bias in the sense that it's making a lot of
presumptions about how word meanings will interact to form a meaning for the whole.
Because you basically hard code, right?
And anything that deviates from GloVe and sum is gonna be out of step with your model.
Over here on the right,
we have a model that's less biased in the sense that you can learn
a very free-form function for combining
these three meanings into a whole meaning for the sentence.
We've imposed linear structure.
But if it turns out that the linear structure ought to be
just the sum function as on the left, we can learn that.
But we could of course learn lots of other functions.
And one thing that we're doing in terms of bias is
creating a bias for a kind of linear flow through the sentence,
which is pretty natural if you think about sentence processing.
But it is a- as a substantive thing to say,
about how to represent the meanings.
And then down on the left here,
you do that in a different way.
So here, the bias is that there is tree structure,
that the rock forms a unit in a way that rock rules does not.
And that the composition ought to proceed kinda from bottom up,
paying attention to what the linguists called the constituency of this example.
Very high bias, right?
If we're wrong about this tree structure,
we might be in real trouble in terms of what the model is gonna do.
If we're right about the tree structure,
this could be a huge gain in terms of telling
the model how to combine things and what to pay attention to.
So these are all kind of very high bias.
And then if we take this RNN here,
and run it bidirectionally,
and we add a bunch of these attention mechanisms,
that we're gonna talk a lot about today.
That is pretty far along the spectrum to being
unbiased about what kind of interactions we're gonna see.
No more presumption on left,
right, because we're gonna go on both directions.
And then not, not only that,
but we're gonna have all these attention mechanisms that are
kind of allow us to leap around in the sentence,
if that's what the data tell us to do, essentially.
And that's kind of very free form.
And I would say that all of the models that we talk about today are in this category,
pushing in the direction of assuming that we
don't know anything about what this data is gonna be like.
But we know they're gonna be kind of lots of dependencies that we need to learn.
So let's just learn them in a very free form way.
Give the model as much capacity to learn them from
data and not bias it in any one particular direction.
And then I think like, of,
of the models we look at today,
BERT is the farthest along on this continuum,
BERT and the transformer.
Okay. A few more things by way of setting the stage,
because I did wanna impress upon you that you kind of
already have all the raw ingredients for all of these models.
They're incredibly complicated, because there's like tons of stuff happening in them.
But at their core,
these are things that are familiar to you.
So one team will be attention.
You'll see this in the Transformer and in Bert.
And so I thought I'd just remind you that we talked about attention.
Here is an NLI example.
Every dog danced.
Some poodle danced.
You have all those word representations down here.
And then these hidden states,
for an RNN for the premise,
and an RNN for the hypothesis.
And the state to watch is HC up here.
In the simplest form of global attention that we considered,
what I do is calculate a dot product score by
multiplying this vector by all the vectors in the on-premise.
That's what happens here. Those get normalized via a softmax function.
And those scores then down here are used to weight everything in the premise.
And in the standard approach,
we take the mean.
So this Kappa here is a vector that kind of
summarizes everything that was in the premise as weighted by attention.
And then you combine that into the classifier,
so that the ultimate classification decision here,
this hidden representation is Kappa.
And this final state over here,
multiplied through some weights.
There's a couple ways you can do that.
And then finally, you get the classifier on top of that hidden state.
Remember that? Simplest form of global attention with dot products.
And the thing to keep in mind is,
the core mechanism which we applied
just for the final state looking back at all the premise states.
When we get to the transformer,
you're gonna see this all over the place.
That's the idea. But it's exactly the same calculation.
I'll try to call that out.
Another thing that we haven't really talked about a lot
since the first unit is this notion of sub-word modeling.
It came up in the first unit,
because we built these vector spaces.
And I think some of you saw that,
if you kind of broke them down into
the character level and then built up representations that way,
you could make up for some of the deficiencies in the original space by kind of taking
advantage of strengthened information that's shared across the sub-parts of words.
In the papers that we'll look at,
especially ELMo, that's done in a somewhat more sophisticated way.
So I thought I would just introduce this impressionistically,
um, and, uh, let you go off and study it in more detail on your own.
So starting down here at the bottom.
I just have the word rules,
operating at the character level.
I'm assuming that we learn embeddings for each one of the characters,
just like you would for words.
But now, things get interesting.
So what I've tried to signal here are a bunch of convolutional filters.
This is not an idea that we've talked about in this class.
They are not so common in NLP.
But they're used by the ELMo team to get word representations.
So the idea here is,
I have a bunch of filters like,
this is a filter of length 1,
this is of length 2,
and this is of length 3.
And what the line signal is that I'm doing a kind of moving window across this example.
And I get representations via a dense layer that connects
the characters that I'm connecting up into this representation.
And then those are combined via what's often called max pooling.
So take this top one here which had a window of 3.
If these are the vectors that I formed at each one of those steps,
the max pooling representation is just taking
all the max values from all three of
the different vectors to form a final representation.
For convolutions in general,
this could be the mean or the sum or some other operation like that.
Uh, but max pooling is common,
and that's what the ELMo team uses.
And then to get a representation for the entire sentence,
you just piece together all the representations
that you got from max pooling and through all of those filters.
So you get something that looks like this where
this would be the part that goes to this first filter,
blue for the second,
and green for the third.
And that's kind of at the lowest level how the ELMo team starts to represent words.
They actually add a few more layers as you'll see.
But I thought I would call this out as a separate contribution because this is
one approach to kind of gathering strength from the parts of words.
The transformer, uh, does this in a somewhat different way.
And that's this next idea which again,
we haven't discussed but I think this will be really intuitive for you.
So part of what you do for the transformer
and for BERT is have what's called positional encodings.
And I've signaled that here.
So I have word vectors,
the Rock and rules and they have their usual embeddings.
But in addition, I'm gonna learn
a separate embedding space for different positions in the sentence.
That's what signaled by this dark red here one, two, three.
They each have their own vector representations.
And then it's common to just sum the positional vector and the word
vector to get a representation of the word that is context sensitive.
That knows what position it was in the linear flow.
And you could learn those representations,
but a nice idea that's kind of very free,
that comes from the transformer paper is that instead of learning
them you might just construct them to encode some positional information.
So what they do is do this kind of
sinusoidal thing where the position is correlated with the,
um, periodicity of these waves.
And that means that you could generate them for basically any length that you wanted.
So that if you saw a length at test time that you'd never seen in training,
you could generate the positional vector for it.
Sum it with your word vector and you'd be able to do something useful.
Whereas, if it's a learned embedding space,
you're kind of stuck with what you've seen during training in terms of example lengths.
So this is a nice free-form idea that you could apply anywhere.
If you fit one of those sum of vectors classifiers,
you might have positional encoding as a way of
infusing them with a little bit of word order information.
And that would be very easy to test that experimentally.
It might really give you gains because of this added sensitivity to position
that your model would then have. Yeah.
You have those instances like the internal eight.
Will you use the same positional vectors
as like the Rock rules for each one of the positions,
or would it because the later two words are
different would each of those words have its own separate positional encoding?
I think the positional encodings would be the
same but the word vectors would be different,
and that's how you'd get both commonality between
the first words of those two sentences but also a difference.
And you might do something different yet again,
if it was like an NLI pair where you might wanna be sensitive to position
within the example as well as position across the,
uh, premise and hypothesis.
You'll see that they do that for BERT.
Thank you.
Yeah.
So to clarify, you take the sinusoid [NOISE]
as weight and do you convolve that with the vector,
er, dimension wise or what exactly do you do with the weight?
Yeah, they have a procedure for generating
these vectors that follow these sinusoidal curves that are sensitive.
You can see it's like dim four,
five, six, and seven,
and I think four is the one with the laziest pattern.
And then what do you do with that sinusoidal vector?
You add it to your word vector.
That's what's happening up here.
And that's a way of encoding in a high-dimensional vector,
the position that you're in. Yeah.
Why does addition sort of work?
It always feels to me weird that you add two vectors,
because it seems like you just kinda gobble off both of them.
Like, why not concatenate or is it just purely empirical?
I think it's a good question.
Throughout all these papers it's some.
I think it keeps the dimensonalities low and allows
some freedom in terms of how you're combining different pieces in these networks.
But yeah, I kind of.
I would for- if I was trying this just as a, you know,
taking a simple model and adding positional encoding,
I would try both sum and concatenation.
Because it sure seems like concatenation would be just as good if not better,
assuming that I didn't have to worry about dimensionality stuff.
Those are the high level ideas.
Let's dive into these models a little bit.
[NOISE] And again, I think that this can't be a substitute for reading the papers.
I think you should think of this more as a kind of guided tour,
um, that might help you as you work through them.
But there's no substitute for that, you know,
hours spent banging your head against these papers and reflecting on them.
Let's start in with ELMo, cause I think this
builds the most on stuff that we've already done.
This is a pretty simple idea that I think they just executed on beautifully.
So for ELMo, the idea is,
down here I have my- my standard example,
the Rock rules with a start symbol.
And each one of these- them has a standard embedding.
But now, I'm gonna transfer that into two RNNs,
one traveling forward and one traveling reverse.
That's kind of orange and green here.
So standard RNNs of the sort you've seen before they use,
I think LSTM cells.
Um, the paper I think it's deeper than this network. I forget the details.
I've shown you just two layers.
But obviously, you can imagine that this could be as deep as you wanted.
And then once I've got those two RNN- RNNs running
for the classification decisions that are gonna produce the,
uh, next token, uh, I share parameters.
So up here shared parameters,
down here shared parameters.
And these two are separate RNNs with their own parameters to allow them
to run in both directions without any masking or anything like that.
Does that makes sense? Yeah.
Okay. So the two RNNs are
language models that are trying to predict the underlying sentence?
Yeah. So if you follow just this left one here you do the start symbol.
You hope it produces The as its prediction.
And then the next one,
Th- The gets inserted here and you hope it produces Rock and so forth.
And then over here it's just gonna run in
the reverse direction doing those same predictions.
And I think the other important thing that seems meaningful,
they point this out in the paper is that the- there are shared parameters for
the m- map that takes you from the embedding into the RNN,
and then shared parameters that take you into the softmax layer.
Then for actual word representations,
conceptual- contextual word representations, like,
if I wanna represent rules here,
that word, the final word,
then I can do that, um,
by combining its embedding and canca-
concatenating the two hidden states that go along with it for each of these RNNs.
So it's like these are the two states that I drew from here and from here,
and I drew these two states from here and from here.
They get concatenated.
And then part of ELMo learning, uh,
when you apply this to another task is learning a task-specific weighting on those vectors.
So these are softmax normalized scores that you can learn from data that
tell you how to weight the various levels that you've learned from the core ELMo model.
Those all get summed together after the weighting.
And then I have this single representation up here in yellow which represents rules,
but notice it represents rules not on its own,
but rather from all the surrounding morphosyntactic context.
So, um, rules will be different if
the preceding words are different fron- from another example.
[NOISE] I mentioned before that they do some other,
ah, tricky stuff in terms of representing words.
And this is gonna draw directly on that convolutional stuff that I reviewed before.
So imagine I wanna represent just some anonymous word up here.
The way they build that up is first at the character level that's down at the bottom.
And then they have a bunch of these convolutional filters,
like up to seven of varying lengths.
Those get combined via maxpooling in the way that I showed you before,
and all those representations from all the different filters get
concatenated into what's quite a wide representation at this point.
And then they have what they call highway layers.
Which is a separately developed machine-learning intuition about how essentially you can
learn some gating information across different layers in a deep network.
Um, so like in the extreme case you could actually learn parameters that would tell
you to skip a layer and just pass up the lower representation.
But the actual highway layer idea is more flexible, it allows you to learn essentially
gating information that will tell you what percentage
to allow to pass on and what percentage not.
That's what I've put in yellow here.
They're just more dense representations that function as these gates. Yeah.
Gatings are different than the LSTM version of gating or is it just sort of a simplified?
It is a very similar idea.
It's drawing to- it's, it's from,
um, one of the same authors,
the developers of LSTMs and it draws on a very similar intuition,
just now applied in potentially very deep layers.
The ELMo layers are only- only two is their max.
But I think the idea is experimentally,
you can have like 25 layers and you would learn how to skip around in that space.
And then finally they do
a dimensionality reduction down to get the final word representation.
[NOISE] And you might be able to tell from back here that you're kind of constrained.
The word representation has to be
double the length of the two hidden states so that I can do this summing.
And that kind of adds some constraints to this model.
And that's why you have this final projection layer,
because these are of much higher dimensionality than you'd
wanna accommodate for the rest of the ELMo model.
But finally, that gives you your word representation up here
and that's what gets passed into the two RNNs.
And then here's some information about the models that they released.
Small, medium, original, and original trained on
much more data than this simple original one here.
Uh, shows you the parameter counts,
the hidden sizes for these networks. Some information.
Thi- this is both LSTM information and
then the number of highway layers in the word representations.
That's a kind of summary of how big these models get.
And one tip I have for you if you wanna know even more about how they're set up,
the way AllenNLP works is you have these options files,
which more or less setup the hyper parameters to the model.
And so you can just look at them at the website and
see which activation functions were used,
what the dimensionalities of everything were,
what the filters were like, and so forth.
Which is an even deeper dive on the kind of
mechanics of the specific models that they released.
And that's ELMo. Any questions about ELMo before I move on? Yeah.
Are there forward RNN and reverse RNNs,
are they trained, like, separately?
[NOISE] Or like, yeah.
Because I definitely don't understand how if you're trying
to predict a word from one RNN and that you've seen from the other one.
They are trained separately and that gets them around this problem,
which you'll see come up when we talk about the Transformers and BERT.
That you can't have these things run in both directions without doing some masking,
otherwise words kind of see themselves.
So I think, imagine that these RNNs are being trained separately
against what might as well be separate, uh, output states.
I just drew them this way as a kind of reminder that the parameters are shared,
that it's kind of doing this multitask learning from the same data. Yeah.
Wouldn't it.
[BACKGROUND] Just sort of trivially talking.
Like you take the vector representation and instead of doing all this
[inaudible] you just kind of know that you can just get straight back out what you put back in.
Like, do they block out words?
Wha- what is it that makes this task ultimately hard to the RNN to learn?
Well, I think each of these RNNs is doing the hard task of language modeling.
So it doesn't have those kind of degenerate solutions.
It's just as hard as that task is.
Um, it's just that you have tied parameters up here. Yeah.
So the end goal then is to use these representations in another task, right?
Yeah. In the context of the paper,
the idea that they're really pitching is that I can learn
task spe- task specific word representations that could be the input to another model.
So in the simplest case,
and this is kind of what you see in my notebook,
I have these contextual word representations.
They could be input to a further RNN,
that's about your task trained against your labels,
and then you're just benefiting from all this other stuff ultimately going
down to the web-scale data that these representations were trained on.
So is there any taking
your actual end task performance and propagating that back onto your RNNs,
or we train the RNNs and we freeze them,
uh, and then we use them on our actual task?
Oh, this is a good question.
I, I guess I assume that the task-specific learning,
the fine tuning that you did,
was just on this Softmax normalize, uh, weights here.
But there's no reason in principle why you couldn't have that propagate
down further to the ELMo parameters themselves.
Now, I'll have to check to see what's normally done.
Do you propagate all the way down? Anyone know?
Or just these weights here?
[NOISE] Both of them make sense as far as I can tell. It's a good question.
I think what you might have in mind though is that for transformers and BERT,
in principle, we can propagate down to all the weights as part of- part of fine tuning.
Let's skip to that now.
So the transformer.
Again, okay so let's build up this core model structure.
There's a lot here, and I just want to kind of make sure I highlight what
I think are the really innovative ideas and how they come together.
So first down at the bottom,
this will be familiar.
Because what I've done is for our example the rock rules,
positional encoding exactly the way I showed you before,
and they don't learn these embeddings in the paper,
they rather do them in that kind of s- um, sinewave based way.
So that they can generalize to example lengths that they didn't see in training,
which I think is a really nice idea.
But they point out that you get
very similar results if you learn the embedding for this dark gray.
So those get summed,
and that gives you the word representations for the rock and rules.
And I just wanted- I'm gonna call this out with these equations
here because they're presented in a kind of non-linear way in the paper,
and I wanted to organize them.
So c-input is the sum of these two vectors.
I'll concentrate on the c,
but all of this is true for a and b as well.
Then the next layer is the one where the real action lies.
This is where the pap- the title attention is all you need comes from.
So to form the next representation,
the ones I've got in orange here,
you do that dot-product attention that I showed you before.
I want to call out that that is exactly what I showed you for the NLI case with
the tweak that they do this normalization by the length of the representations,
which they find just helps with scaling as learning proceeds.
But other than that, it's like the scores Softmax normalized on they
happen to do the sum in the paper of the weighted other vectors.
It's called self-attention because instead of looking
back from the hypothesis into the premise,
now we're just looking around.
When we're at c, we looked at b and a,
and formed attention weights based on them,
and that's what gives us this weighted view,
uh, attention weighted view of the input vector.
But the mechanics are ones that are entire- entirely familiar as you can see here.
[inaudible] the length of the string?
No, by the length of the representations.
K here is the dimensionality of everything that we're gonna see as we build up.
Make sense? So good so far.
Self-attention is just attention all around me.
[LAUGHTER] Okay, then we keep building.
So then they have these two kind of sublayer structure pieces,
where to form what I've called a layer here,
I sum the attention weighted representation
that I got here with the dropout on the input,
and this is like- this is familiar too.
If you think back on the attention lecture,
a lot of what we do in attention is kind of take the representation we care about,
and mix in the attentional values,
and I take it that that's what's happening here.
So the orange representation is actually
just this kind of attentional thing on what's around me.
It's not really a proper representation of c. So to mix them together,
they do sum and then the dropout thing is just a bit of regularization,
and that gives you c_alayer here in this, um, yellow.
Uh, and I've tried to draw this here,
this kind of sublayer structure where it's kind of like you elevate
something below with something that was an intermediate representation,
and again that's done by sum,
and that's constraining the fact that all these dimension, uh,
these, uh, representations have to have the same dimensionality.
[NOISE] Then they do layer normalization.
I think, again, this is a kind of optimization trick that helps the network to learn.
I've given the calculations here.
This is a kind of version of z scoring,
where I just kind of make sure that
the vectors for each of one of these layers are scaled in a sensible way.
Then you get two layers of a feed-forward network.
The first with a ReLU activation function in the paper.
But this is the- a kind of similar thing where s- this, um,
a- this nor- this value here is here multiplied by some weights and a bias,
that gets a non-linear activation,
and then another dense layer over here;
and that gives you what I've called c_ff in blue.
Then one- again, one of these sub layer tricks.
So take c_anorm, and kind of elevate it and combine it with c_ff,
having done some dropout on c_ff,
and that gives you c_fflayer.
This is the kind of, uh,
darker or lighter yellow up here.
It's kind of similar to this yellow down at this point.
That's why I gave them the same color. Same trick.
It's just that here where we steered around attention,
now we steer around the dense representation.
[NOISE] Then finally, a normalization step;
same one as the one we showed you before,
and that's the output of this block of the Transformer.
[NOISE] That was a mouthful.
Any questions about it?
[inaudible] earlier [inaudible] decay [inaudible] layers?
No, no. So decay would be like whatever we picked here,
like 50 for the green. Isn't that right? Yeah.
Is the dimensions of c input like a concatenation of the x,
word embedding, and positional embedding.
I think it follows from this network structure that every single one of
these representations from the gray on up has to have the same dimensionality.
Because here I sum, so these have to be the same here. Here I sum.
Here I sum. I don't see any space for kind of growth and shrinkage.
So if we pick 50 down here.
It's 50 all the way up.
And then this normalization is the square root of 50.
Hope I have that right. [NOISE] Yeah, all these sums,
at least simplify some of these design choices because it's- at least you
don't have to do tuning over the dimensionality of each one of these things.
And it's only gonna get worse.
If you've read the transformer paper,
you know that I've just started here.
There's two more ways in this net- in which this network is gonna get [NOISE] bigger.
And the first, is what's called multi-headed attention.
This isn't gonna be anything new.
It's just we're gonna do what we did before a lot more.
Okay, so start with our example and lo- follow the green here.
So what we'll do is map this into its own attentional space.
And the [NOISE] reason that's meaningful is that,
this is that attention calculation before but now we have parameters W1Q,
W1K, and W1V, that are
particular to this head of the multi-headed attention that we're building.
But it's the same calculation.
It's just that each one of the representations is,
is fed through these dense layers.
So we do that, call that H1.
That gives us three vectors over here.
And then you do it, you start to build that up.
Do it for H2 as well.
Same calculation except now it's W2 for all three of those Ws,
and that gives us these representations.
Do it for W3,
same calculation but new parameters and so forth,
all the way up to what they do in the paper is eight, I think.
So they have eight heads for these attention layers.
What they do is like, if I picked 800 as my dimensionality,
the kind of conceptual dimensionality for my transformer network.
Then each one of these would have dimension 100.
So that the overall network didn't grow.
What they're trying to do is kind of provide more ways of
ensuring that you have lots of parameters to lots of interactions,
and avoid kind of I think the network ossifying in certain ways, right?
Really flexibly learning how to map into this attentional representation. Makes sense?
And then the final thing here,
is that there's not just one transformer block,
in the paper there are six.
So six repetitions of everything that I have in gray.
And I've just reminded you here that we do multi-headed attention at the bottom
of each one [NOISE] of these layers and then repeat the whole calculation.
[inaudible]
The eight can be done in parallel.
A huge advantage of the transformer over like RNNs,
is that a lot of this stuff can be parallelized,
which leads to real speed gains.
And that's one of the advertising points for the paper,
in addition to kind of being very free-form
and unbiased about how you learn how words interact with each other.
You get a lot of optimization gains with this.
And I think that's why this network can be so
massive and still train in reasonable times.
Yeah.
[inaudible] um, because there's this kinda like each word to
word has its kind of [inaudible].
Well, I drew it in a way that was trying to reveal
which parameters were shared versus which, which were split up.
But I might as well have copied all these inputs and just
had a bunch of different attention things.
And that kind of reveals to you the independence of these calculations.
And then if you think about gradient information flowing back, you know,
it's all gonna be kind of independent and just brought together in the end.
[NOISE] I think that's why you can think of these as separate calculations.
[NOISE] So now I feel like having done that,
I at least I'm [NOISE] in a position to understand the diagram in the paper,
which I absolutely did not understand the
first time I read the paper but now I kinda get it.
So the left side is,
of course repeated for every state in the encoder.
They've only shown one state for compactness
but like the rock rules would be three repetitions of this.
Each decoder state self attends,
with all the decoder states and with all the encoder states.
That's something that I didn't quite confront in my core discussion.
But just imagine everything that I was doing on our single example,
if we were doing like a dialogue agent or summarization or translation.
All the decoder states, like in NLI,
the hypothesis could attend back to everything
in the premise as well as attending to all things that were,
that were in their own space.
So that's what this line is meant to signal here.
Self-attention in both sides and also attention backwards.
The right side of this, of course,
repeated for every decoder state.
Um, yeah, I think that's all I have to say there.
And then finally, I just wanted to point out.
This is a complexity that I'm not going to talk about.
But in the decoder self-attention is limited to preceding words,
so that words can kind of see themselves.
So there's a bunch of masking that has to
happen in order for us to do this on the decoder side.
But for things like NLI,
you don't have to confront this.
Because you're just trying to predict a label and you can attend everywhere if you want.
Yeah. That's it for the transformer.
Questions about the transformer?
Great. So now BERT.
This will be comparatively easy,
if you have the transformer in mind.
Because it's obvious- it's called Bidirectional
Encoder Representations from Transformers.
Okay. Once more we build up a model.
Down here I have the rock rules.
You have a special class token that BERT has.
And it's got its own embedding.
And then the three familiar tokens.
They all have positional embeddings just as before.
And now they also have a sentence-level embedding.
Which is just repeated in this example in dark red.
And the idea there is that if I've got a two example thing like premise and hypothesis.
Then I have two embeddings;
one for the first. One for the second.
So each word is infused with that high level positional information.
But again, it's just summing those representations to get the green embedding here.
Which you might think of as the token representation.
And then, you just do a bunch of transformer stuff, right?
And the important thing I think to signal there,
is that you do a bunch of self-attention on
these representations and that get- that flows into the transformer.
And then once you've done all the self-attention stuff,
it's that kind of norming and dropout and
sublayer structure with a feed-forward network at a certain point,
you know, that mixture of ideas.
But surely the important thing here is that,
at the start of each one of these transformer blocks,
we do all of this attention all around us, right?
So it's not an RNN and it's not a back and forth RNN.
It's more like this dense thicket of connections that we
can make to form those initial representations.
And you can repeat the transformer blocks.
I think they do two in the paper?
So it's not as deep as the transformer but some number of them get repeated.
And then to use this model to make predictions.
Since it is flowing in both directions,
you can't just do a standard language modeling task.
So what they do instead is a test they call masked language modeling.
So in this case, uh,
you just make predictions about certain words that have the mask value.
So just block out an actual word and then learn to predict the word that was there.
And they do that on about 15% of the tokens,
the rest are shown to the network.
And that's the sense in which it can train against these missing points and
learn a bidirectional model without any of
those problems or words seeing their own future and past.
So it's a kind of standard language modeling thing just trained in
a really innovative way. That makes sense?
And then the other piece is for doing transfer learning and fine tuning,
the convention that they adopt in the paper is,
that this class token here,
is used as the summary representation of the whole example.
And what that means is that,
in the simplest case you could just add a classifier on top of it.
The way we did for NLI for example,
where we took the last state or the first and
the last states and use them for a classifier.
Here, you just use this final transformer representation
[NOISE] associated with the class label.
So that's the simple case of just doing like transfer learning.
And then they advocate doing a lot of fine tuning of the parameters.
So you put this classifier on top,
and then not only do you update the parameters for that classifier
but allow that gradient information to flow throughout the network.
And in that way, all the BERT parameters potentially get updated for your task.
And if you've done BERT fine tuning,
I think you did that.
And that's what the, um,
code that they released makes very easy to do.
In addition to that masked langua- uh,
language modeling task they train the network, the one that parameters that they
released against a second prediction task which is binary sentence prediction.
So, for this they can generate their own data
again just like they could by masking out certain tokens.
Here the positive instances are actual sequences of examples from their corpus like,
CLS, the man went to MASK store SEP.
He bought a gallon MASK milk SEP and that's labeled IsNext
because those two sentences did actually occur in a corpus that they're learning from.
And then for the negative examples,
they choose some random second sentence and label that NotNext.
And so then training proceeds in
the way that you would expect using this class label here
and you're fine tuning the whole network against
this high level next sentence prediction task.
And so that's the sense in which it's,
it's been jointly trained against
the language modeling task and the sentence prediction task.
And I guess that's like local coherence and
more global or discourse coherence for the model.
So two notions of context dependence which seems
again really right to me in terms of the way you'd want for learning representations.
A funny and delightful twist
about the BERT code release and the BERT data assets,
uh, is that the vocabulary is tiny.
If you download a, a BERT model here I did the uncased one from
the small model and just read in the vocabulary it's only got 30,000 tokens in it,
30,000 word things in it.
And you might think, how on Earth is BERT gonna be
so good at new text if it's got this tiny little vocabulary?
And by contrast I think the ELMo vocabulary is like huge.
So this is a real puzzler,
"How is it gonna do well with, well with only 30,000 tokens?"
Well, if you look at those tokens as I've done here you've got like folder, that's a word.
But then, uh, you know,
##gged, and principles, then moving.
And then this other funny word fragment.
And then if you use their tokenizer which you know,
if you just download their code and load in the tokenizer here,
then this isn't too surprising,
that does a normal thing, basically, except
maybe it's unusual about how it handles
this contraction but more or less this looks like words but then if you
say does BERT knows Snuffleupagus which is another Sesame Street character.
It break- it does not know that word, that is not in
the vocabulary and so it breaks it up into a huge number of
smaller tokens with those boundary symbols and
those boundary symbols tell the model that it's word internal.
And the actual tokenizer uses an underscore for whitespace.
So no information is lost as you do tokenization with
this tool and you get a whole lot of strength from sub words.
So this is great and this also shows
you why you shouldn't do your own tokenization with BERT,
because you wanna let it kind of do
its own divisions and use its own proper embedding space.
But then wow, talk about an efficient representation of the vocabulary.
I would not have guessed if you just asked me two years ago
that this would be an effective thing to do but wildly successful.
And then here's some basic information about the models they released.
So BERT base.
This is manageable for your computer,
I mean it's large but you can work with it.
Oops, this should say large here.
This is the large one.
This is truly enormous.
And if you try to work with this on your laptop everything might fall apart,
I guess that's the lesson here.
And they are limited to sequences of 512 tokens that's
longest you can get because that's how many positional embeddings they learned,
um, which reminds me of the, um,
transformer paper where they gave themselves
the flexibility to learn for longer sequences.
That's BERT. Any questions about BERT?
Lots of transformers.
BERT is just mostly the encoder part of the transformer right?
Yes.
It doesn't have the decoding part that's kinda handled implicitly by
the way you're doing this multi like back and forth approach?
I think that's exactly right.
Yeah, dropped the decoder it was a hassle,
it was getting in the way of being truly bidirectional.
And instead changed to this masked language modeling task.
Yeah. Surely that's one of the shining insights of the paper.
Next generation using BERT if you were to
give it like a second sentence that was a pure mask.
Yeah, sure. Yeah, no.
It will learn to- yeah if you just mask out it
will learn to make predictions about those tokens.
Yeah. Okay. I have a few minutes before I wanna,
wanna turn the stage over to Cindy.
So I, I thought I would just point out the guiding idea for
that notebook is whatever architecture you're developing for your project,
[NOISE] it might benefit from contextual word representations.
Speaking as a linguist I would say surely it's going to because
these representations were trained on a lot of data and they're very good.
And whatever your problem is word order and contexts matter.
So, might as well try to bring them in.
Now it might be hard for you to bring them in to whatever codebase you're working with,
because maybe you're using scikit or you're using PyTorch in
your own way that would get in the way of doing the fine tuning that they advocate.
But don't let that stop you because you could just use
the notebook tools to initialize your network using these representations.
That's a kind of twist on the standard thing for RNNs right.
So for RNNs usually we have this fixed embedding space that
makes that mistake of not having context for its representations.
Uh, and you just look up your tokens, get their indices,
look them up in the embedding and that finally gives you
the vector sequence that the RNN is actually processing as its inputs.
With these contextual representations you just skip all those intermediate steps and go
directly from your vocabulary to sequences of vectors.
And now whereas a and a here had to be identical and b and b had to be identical.
Now they can be changed depending on their context.
And that might give your network a real head-start.
It's really easy to do this.
This is a complete use of the SST sentiment framework.
It's a whole experimental framework with ELMo.
And you'd turn, you'd get impressive results with ELMo if
you compare back to the baselines for this ternary task for this,
um, Torch RNN big boost.
And then for BERT similarly very easy to bring these things and you've gotta do
a little bit more careful preprocessing of BERT vectors because they are very large,
uh, but still straight forward to bring it into the paradigm.
And then you're off and running, BERT is even better
according to this experimental run for these parameters.
So I hope that's empowering.
Um, any questions about any of that? Yeah.
If you look at the task of producing, uh,
like let's say the vector representation,
uh, the single word in context, uh,
like I guess I'm wondering if there's anything else like a next step or what would be
the next challenge to overcome as
far as making these representations really, uh, predictive?
Oh I don't know if you have ideas that can lead to a major contribution.
Uh, I mean surely more discourse sensitivity like
going beyond maybe the binary sentence task that's something even higher level.
Because I think that all my work meetings are
being influenced by everything that's happening in the discourse around us.
So the more of that we can bring in the better will be.
And if you could bring in stuff like tasks,
sensitivity that would be even better and on and on.
Yeah. You know, why,
why stop at linguistic context, the physical context matters,
gesture matters, visual salience matters, bring those things in.
Uh, maybe there's a Sesame Street character out there waiting for you.
[LAUGHTER]
All right. Good. That was a giant gulp from this very rich fountain of ideas,
um, go forth and read the papers and let me know if you have any questions.
I think what I'll do now is turn it over to Cindy for a related topic.
And be sure to wear this.
Okay. Um, so this short talk is just going to be about um,
representation learning for long texts.
So so far in the course we've mostly talked
about kind of learning distributed representations um,
for words or for sentences but like mostly limited to sequences of a few words, right.
If you think about the assignments that we've done um,
for like word similarity or like NLI um, or sentiment classification,
that's kind of been capped out at sentences that are
like just a single sentence, right,
we haven't really thought past that to a paragraph or even like a long document.
But for some of your projects you might be interested in doing um,
these sorts of NLU tasks for longer texts.
Um, so we're going to talk about a couple of ways, um,
you can kind of get the same distributed, ah,
representation um, but for um, something longer.
Um, so yeah. So so far,
you know like we've been doing representation learning at
the word or sentence level and this is good because we can kind of
get like a vector that represents our input that we can pass
into a classifier or we can compute distance,
um, using some distance metric,
um and those things are like very useful for a wide variety of NLU tasks.
Um, yeah.
But our goal for today is how can we
apply NLU methods like the ones that we've been using to longer texts.
So for example, news articles,
scientific papers or even books um,
or like transcripts of conversations um, things like that.
Um, so some example tasks that we might want to do are like document classification.
So for example, you have um,
like the transcript of a parole hearing and you wanna predict um,
whether ah, that transcript, ah,
you know, ah, ended up in a positive or negative decision.
Um, you might want to like cluster documents together.
So for example, you have like um,
hundreds of different news articles and you want to cluster together the ones that um,
are talking about the like same event or concept.
Um, maybe that's ah, something that you wanna do.
Um, or even reading comprehension.
So, um, if you're familiar with SQuAD that's um,
a reading comprehension dataset um, that kind of takes like
a paragraph long context and tries to find the answer um,
and within some span of that paragraph,
um, but there are even reading comprehension um,
data sets that look at like you know, longer inputs.
So for example, the NewsQA dataset takes
an entire CNN article and kind of does a similar thing to SQuAD or um,
you know you ask a question and try to find
the answer somewhere within the whole article.
So you can see how that would be a significantly more difficult task because you
kind of have to look across this much longer context for the answer.
Um, and then the final example will be summarization.
Um, where you know, you wanna go from a very long um,
text to a shorter representation um, but still capturing all the meaning.
So um, yeah.
I just want to talk about a couple of methods um,
that are like hopefully practically useful um,
for you guys on while you're thinking about um,
these things and like working on your projects.
Um, so just to start off,
I wanna refresh, um,
on like vector representations of words,
we've seen a ton of different methods to do this,
you know from the very basic um,
just using like one-hot representations um,
to represent one word in your whole vocabulary um,
or to use some of these like VSM um,
type approaches or even from today um,
using something like BERT or ELMo um,
to get word representations, right?
But kind of like the question, um,
like when you wanna go from word vectors to something longer is like, you know,
if I have vectors that are re-
representing on like each of the individual words in my input,
how can I like kind of compose those into
representations for my whole paragraph or document?
Um, so does anyone have any like ideas of something that they would try, just, you know,
as like maybe a baseline or like something just off the top of your head. Yeah.
[NOISE] Like average the vectors of the all the words in the text.
Yeah, um. Yeah, so ah,
if people didn't catch that it was just like take all your word vectors and average them.
And that's actually a really good approach um,
which is like, you know,
often done like not even as a baseline but just like as the primary approach um,
to get like a document ah,
vector, given a bunch of word vectors, right?
Um, and especially if you have like kind of ah,
context aware representations, like BERT vectors maybe that's good enough. Um, yeah.
So that's like the first good baseline method um,
just to do sort of like a bag of word vector sort of thing.
Where you sum or average or kind of do a max pool over the word vectors um,
in your paragraph or document.
Um, but can anyone think of like some drawbacks to using this um,
as like the only thing um,
to create your like document embedding? Yeah.
A word in the first sentence can
be very different context than a word from the last sentence.
So it doesn't really regularize the,
[NOISE] the whole document and what you are looking at.
Yeah, yeah definitely.
You kind of lose um, like the, ah,
like the structure of the sentence or like,
um, the ordering of the words, yeah for sure.
Um, so a one kind of like, you know,
step above doing that um,
is to instead of just like do a pure averaging to um,
use like the structure of the sentence itself um, you know,
to like do these sort of like matrix operations but in a- like a structured way.
So like, you know, maybe you can parse your sentence first, um,
and then combine the words based on that parse tree that comes out.
Um, and that's also like a pretty good approach,
um, but it does like rely on your parsing to be
accurate and doesn't work as well once you have more than one sentence.
Um, so another thing that you might ah,
wanna try is like using an RNN as your document encoder, right?
So like something on but you could potentially um,
think of doing is like training an RNN as
an autoencoder for the paragraph or document that you have um,
or even you know, like having the loss
propagate all the way back from whatever downstream task,
so like maybe you are doing a classification.
Um, and then something that you could do is use the output at
the last time step as kind of like an embedding for your whole document.
And like, we've seen this when we're doing like sentence embeddings, right?
Um, I guess I already have this on the slide here but,
ah, what's kind of like um, you know,
a situation where like doing this um,
would be like not the best solution? Yeah.
I mean maybe your most important information is in
the first sentence and then you have a bunch of just kinda filler information.
Ah, so if you use the very last thing maybe,
you basically have the vanishing gradient problem where
all of your signal has gotten diluted along the way.
Yeah. That's exactly right.
Um, so especially for the cases that we're thinking about um,
now where you have a very very long text.
Um, if your document you know,
is like many sentences long um,
then like and you're taking kind of
the output of the last time step as your representation.
Um, then yes exactly.
So through the vanishing gradient problem, you know,
a lot of the signal from the beginning gets lost.
Um, and maybe that context from the beginning is important, right?
And you'll like- often in a lot of documents the context
from the beginning like for example the
abstract or like the first sentence of an article,
like that's um, like sending a lot of
the signal for whatever tasks you might want to do [NOISE].
And so one approach that, uh,
was developed specifically kind of for this task,um,
of representing long texts like document-length texts is called Doc2vec,
and this might look familiar because it's from the authors of word2vec.
Um, and so like, uh,
to kind of put this in context since the Doc2vec algorithm
is actually quite close to the ones used for word2vec.
I'm just do a- I'll just do a quick like brief review
of kind of what is going on when we do train word2vec vectors.
Um, so essentially, uh,
we have kind of like a sliding window,
uh, over the current word.
Um, so here the current word would be jumped and our window,
uh, like lets say the length is 5, right,
so it's including like this whole section of the sentence.
And we're using the surrounding words of our current word to try to
predict our current word and that's kinda like the objective that we're optimizing for.
Um, and then through backpropagation this word
embedding matrix will get updated and then by the end of training,
we'll have a matrix containing vectors,
um, for all of the words in our vocabulary.
Um, each of which is like
a distributed representation of that word and hopefully, you know,
embeds some sort of context information, um,
er, and like is something that is semantically meaningful.
And so going from word2vec to Doc2vec,
it's actually very, very similar.
Um, so the only thing we're really adding here, um,
is this extra vector that gets combined with, um,
the words in our context window,
um, which is the document vector.
So in addition to our word embedding matrix
we also have a document matrix which has you know,
like a row for, um,
each document ID in our training corpus.
And so when we are trying to predict the kind of like center word in our context,
we're also incorporating the document vector,
which you can kind of think of,
um, as like a memory, ah,
for like the concept of the document overall.
Um, so in this kind of paradigm,
we're training this word embedding matrix simultaneously with the document matrix.
Um, and, you know, you can get like,
your document representation by simply extracting,
um, a row from the document matrix.
Does anyone have any questions about that? Yeah.
Does that mean that the doc ID is going to be more influenced
by like the words at the beginning of the document or like where in
the, like is it [NOISE]
does the doc ID like is that a token that is somewhere like in the document?
Um, no. So, um,
you can kind of just like think about it as a memory for like the entire document,
um, as a whole.
So like for each context window within the whole document,
that document ID vector will be incorporated. Yeah.
Does the document ID never appear like in the middle position as the thing
that you are predicting other things relative to or
only the kind of as an addition to the context window?
Oh like, will it ever be,
uh, like in the middle of these?
[OVERLAPPING] Oh I see like will that ever be the thing that we're trying to predict?
Um, according to the paper, no.
I guess that would be like an interesting thing to try but, no. Yeah.
Could this actually this be the worst word embeddings?
Because I can see how it's sort of allowed the word embeddings to
cheat a little bit but not actually learn as much about the language.
Um.
Might be an article about jumping so now, it's all over, right?
Like even imagine that it can just kind of
guess that it's jumping even though the sentence is not.
Oh like you could just like kind of be
lucky and always like predictive regardless of the context.
Um, yeah, that's possible,
um, and like, I think it's like one of the trade-offs.
And the so like,
so something that's commonly done is like you might want
to initialize your word embedding matrix with like
some word vectors that are already pretrained and then
you're kind of like using this as more of like a retro-fitting sort of scheme.
Um, yeah.
Like I think that -that's definitely possible but it might be more of an edge case.
Because this way, uh, like one could argue you even get
like better word embeddings because you kind of have
like both the overall context of
the document as well as like the words that are trained.
Um because even though these word embeddings are, um,
like used for all of the documents, um,
like you could think of the representation for one word in one document as kind of
being like the combination of both the document vector and the word vector.
Well, has there ever been any work done on sort of like- so this is
sort of like the micro-level of words and then like the macro-level global documents.
But has there been work done on like doing
some medium level like the beginning middle and end of the document.
However, that's defined like the first,
second, and third, third [NOISE]
I don't know, [OVERLAPPING] of anything like that.
I mean, I don't know of work in this mode but there's
old work on what was called text tiling.
And that was the unsupervised method for
discovering which parts of the document were important.
And I think you might be drawing on
similar insights but maybe learning them in a more free form way.
And I think the idea that Cindy is pointing out here has
enough generality that you could do it at the paragraph level,
or hierarchically, right, you could learn a lot of
these different embedding letters. Yeah, its a good idea.
Yeah. Um, should I recap for the recording?
I think people can hear.
Okay. Cool. Um, yeah.
So trying to summarize the Doc2vec, uh,
approach, uh, simultaneously learns, um,
a word vector for every word and also a document vector for every document.
Um, and some of the benefits are that it's kind of like
this unsupervised training paradigm so you can get,
um, good document vectors, uh, without,
you know, having to train for some specific downstream task.
Um, a-and it's kind of like purely based on like the word distributions,
um, within your corpus.
Um, and then one thing that I didn't talk about
was what kind of happens at inference time.
Um, so like, you know,
if you use, uh,
like the Doc2vec kind of approach to get document vectors like, you know,
what happens when you have a document that wasn't part of your original um,
training corpus so like isn't present in that document matrix.
Um, so what the paper actually proposes, um,
for inference time is when, ah,
we're trying to do this, we will fix the word matrix.
So like those, the word vectors are no longer updated
but we'll augment the document matrix, um,
with this new document and then actually like retrain for a few epochs,
um, to get the, um,
document vector for that new document.
Um, so one potential issue with this, um,
is that it's actually like non-deterministic because of that,
ah, kind of preexisting state like within the model.
Right, so, um, if you kind of do this like twice for the same,
um, like input document, uh,
you might actually get two different vectors.
Um, and like you can solve this by just like training for more epochs,
um, that'll make it a little bit more stable,
but that is kind of a risk.
Cool. And just to wrap it up, um,
there- here are some good resources that
you guys can turn to if this is something that you're tackling,
um, in your own project.
Um, kind of like when doing something involving
long texts that needs representations for that.
Um, so the gensim package of which we've seen
before with word2vec provides a very good Doc2vec,
um, implementation that you can just kinda like use out of the box,
so that's really helpful.
Um, and then for general document embedding,
um, the flair library, ah,
has like some good classes that you can use to,
uh, do like various different types of embedding,
so you can kind of love- have like a whole pipeline, um,
to kind of extract uh, you know,
like BERT embeddings and like
word2vec embeddings and then like combine them in a variety of ways.
Whether that be through like, you know,
simple pooling or like also, um, RNN methods.
So yeah, if, uh,
if you guys are working, um, on something that, ah,
is using long texts, um,
I'd like to definitely point you guys towards these resources.
So hopefully, it's helpful.
[APPLAUSE].
Any last questions though?
Have people tried using transformers for this?
Um, that's an excellent question.
Um, yeah, so I was talking with Chris earlier about like how effective it would be,
um, to use transformers or like to even use BERT as like an encoder,
um, for like a document length, um,
like sort of input, and Bert for sure [NOISE] kind of has like,
like a length cutoff.
At least for the pretrained models, um,
the maximum sequence length is like 512 I believe.
So there could be long enough for something that you want,
um, but it might be too short.
So kinda just depends on your task.
[BACKGROUND].
Yeah, you can do that, yeah.
And then you kind of have to make a design decision as to like how
you want to combine the representations for different sentences.
But yeah, I think like at the sentence encoding level, um,
there's been a lot of work done and there's like a whole plethora of things,
um, yeah, BERT just being one of them.
The only thing I would say for that is that I feel that it's not in
the spirit of the BERT paper to sentence tokenize.
But I feel like that should come.
Certainly, you should just be putting set symbols but learning continuously
as opposed to doing a kind of hard aggregation at each sentence.
So I feel like even if BERT is practically useful in that setting,
you know, judging by sentences,
there's still an opportunity there to think about applying it,
you know, at the document level.
But I think that would lead to a bunch of obstacles right now,
not just because of the pretrained representations but also
because of the amount of compute power required.
[NOISE] Well, thank you again Cindy,
that was great. [APPLAUSE]
 I'm a little bit sick today so I have a scratchy throat.
So forgive me for that,
but I think we'll,
we'll be able to manage without it.
Uh, today, I'm gonna talk a little bit about writing up and presenting your work.
But before I jump into that, uh,
I wanna jump over to our syllabus and orient us a little bit.
We're actually gonna cover three topics today.
I'm gonna talk about writing up and presenting your work,
but we're also gonna have a couple of mini-lectures.
Min and Jayadev are gonna talk about data augmentation,
and Akhila is gonna talk about probing black-box models.
Uh, so we'll have those, uh,
in the second half of, of today's session.
Um, also before we get into that,
I wanna make an announcement.
Um, Chris mentioned this on Piazza,
he posted this on Piazza.
But just in case anyone missed it,
I wanna make sure to draw attention to the- to this.
Um, we want you in your final paper,
at the end of the paper,
to include a short section that describes,
uh, the authorship of the paper,
so an authorship statement.
And the idea is that this is a very concise statement that
just says who contributed what to the final paper.
So this can be very brief.
It could say, for example, uh,
person A was mainly responsible for,
uh, gathering the dataset.
Person B was mainly responsible for model optimization.
Person C was mainly responsible for writing the paper.
Or it could say,
um, person A focused mainly on this approach.
Person B focused on that approach,
and they're both presented in the paper.
Or it could say all of the authors contributed equally to,
uh, collecting the data,
conducting experiments and writing the paper.
It's really up to you.
It should be a truthful reflection of
the relative contributions of the respective authors.
And the motivation for this is that we think that this is just scientific good practice.
In fact, it's one that we'd like to see become
more and more a part of the norms of the research community.
Uh, PNAS is already using this,
and there's a link here to the guidance that's given to the- to,
to PNAS authors, that's the Proceedings of the National Academy of Science.
And we're hopeful that this will become more and more
widespread in more and more, uh, venues.
For example, in, in ACL which is the, uh,
dominant, uh, NLP conference.
We do not intend or expect to use this information in assigning grades.
Um, our expectation is that
the team members who are collaborating together will all receive the same grade.
We reserve the right to make distinctions and not to assign the same grade to,
to all members of a team,
but we'll do that only in the most unusual cases,
and we wouldn't do that without discussing it
first with the people involved in the project.
Any questions about that? Yeah.
[inaudible].
Yeah. [LAUGHTER] For a solo project,
I think it can just be one line that says,
"I'm the sole author and I am responsible for everything."
I think for a solo project it's,
it's a moot point.
Um, I suppose it's to- just to meet the formal requirements,
a single sentence is,
is still appropriate, but it doesn't need to say very much.
Any other questions about the final project before we go on?
Note that, uh, today's class section- session is the last regular class session.
Next week, we encourage you to schedule time to meet with your mentors for the project,
to talk about their progress on the project.
But this, this class would be the last regularly scheduled session. Yes.
You mentioned you think it was a good practice and you want it to spread.
Um, I was kind of confused by this practice on a couple of NLP papers,
and I was wondering why you think it's a good practice.
Um, my philosophy, and
this would be an interesting question to ask Chris, Chris Potts as well,
but my philosophy on this is that it's about responsibility and accountability.
That it's about, um,
making it clear who's accountable for which parts of the paper.
Um, I think it's unlikely in this class,
but there are exceptional situations where,
um, accountability winds up really mattering.
Uh, for example, if there are problems with the paper, if there- if, um,
concern arises that, like,
some results were falsified or something like that,
um, you wanna know who's responsible for that.
And so I think, uh,
the authorship statement amounts to the authors making it clear,
I am responsible for this part of the work and I stand behind this part of the work.
[NOISE] Okay.
So let me switch into the topic of writing up and presenting your work.
And I think first, I wanna say a little bit about why we're talking about this.
Um, to me this topic- this is a little bit unusual because this is not a technical topic.
This is about how to communicate the results of your work.
And, um, it's about finding a way
to communicate the results of your work clearly and convincingly,
and in a way that's gonna have impact.
Yes. [NOISE] Thank you.
I don't know why it does that.
Maybe if I just do this, it won't do that.
Um, communicating your work both in written form and in spoken form,
so in an oral presentation.
I think this, um,
I think this topic is unfortunately one that sometimes,
um [NOISE] is undervalued by scientists and engineers.
I think there's, um, sometimes, uh,
an unfortunate tendency to think that great work will speak for itself,
and that it doesn't matter that much how you present it.
And I have to forcefully disagree
with the idea that great work will always speak for itself.
Um, actually something happened this week that kind of,
um, sharpened this for me.
Um, Murray Gell-Mann died on Friday.
Murray Gell-Mann is a great physicist.
You've probably heard his name.
Uh, he's responsible for inventing the quark,
and introducing the theory that hadrons are made up out of quarks, in 1964.
Um, when I was reading bi- the,
the biography or the,
uh, what's the word?
Obituary, thank you, of Murray Gell-Mann, uh,
one of the- the obituary was- I was reading pointed out that there's another guy,
George Zweig, who the same year independently arrived at the theory of quarks.
I had never heard of George Zweig before.
I don't know if- do you guys know this name?
I had never heard of his name.
I certainly had heard of Murray Gell-Mann. You're a physicist.
[inaudible] known.
I don't know the
reason why, though.
Well, maybe part of the reason,
and this is something that I learned from reading those obituaries.
Zweig named his concept aces,
and Gellman named his on- the concept that he came up with quarks.
And quarks somehow was a much more memorable name.
It was a sticky name.
It was good marketing.
It was good marketing for his ideas.
And so everybody knows who Gellman is.
A lot of people don't know who Zweig is.
I didn't know who he was.
There may be lots of other reasons, right?
They had- these guys had long careers.
They did lots of other stuff. Gellman is responsible for
tons of other advances in physics.
So I'm not saying this is the only reason why Gellman is well known,
but it doesn't hurt that he found a way to market his ideas in a very effective way.
I think a lot of times as, um,
scientists and engineers, we tend to think of marketing as a bit icky,
like respectable people, you know,
don't soil the- soil their hands with,
with marketing and thinking about how to present their ideas,
but it really does make a difference.
And it would be a terrible shame if you invested
a huge amount of work in collecting data,
in staying up late at night running experiments,
and being very innovative in your ideas and your thinking,
[NOISE] but then didn't follow through on that by
finding a way to communicate your ideas in a way that will have a,
a lasting impact on the community.
And it will let others benefit from your learning and your hard work.
So that's really what I want to focus on today.
And our hope is that,
by spending a little bit of time talking about this topic,
that this is one of the unique benefits of this class.
That this class can be a very practical introduction,
uh, not only to natural language understanding,
but in how to be an effective researcher.
And part of being an effective researcher is
communicating your work in a way that's going to have impact.
We also want to inspire you to think bigger than just this class.
Of course, your immediate goal is to finish your final project,
make a video presentation,
which I'll say a little bit more about at the end,
and write a final paper.
Um, but you can aim higher than that as well.
Um, the work that you're doing right now could lead to
insights and ideas that wind up having lasting influence.
I think Chris mentioned that there was a student in,
I forget whether it was last year or two years ago, uh,
had this insight about the hypothesis only baseline for natural language inference,
specifically for the NL- SNLI dataset.
And that was a pretty, um,
impactful insight that showed the community something really meaningful about this task,
or at least about that dataset that so many people had been optimizing on,
on, on the, the SNLI leaderboard.
Um, so it really is possible for, uh,
class projects to wind up having, uh,
a larger impact on the research community.
And there have been many papers from this class
over the last several years that have been turned into conference papers,
and thereby had- a had a broader reach.
Another reason to think about the topics that we're looking at today is that,
um, oftentimes the next step in
your career will involve presenting your research to others.
This is certainly true if you want to go on to an academic career,
but it's even true for many positions in industry.
Certainly, uh, any research oriented position in industry,
very commonly, as part of the interview process,
they'll invite you to give a research talk,
uh, where you go and- at Google or at Apple or even at a startup,
uh, talk about some research project that you've,
you've been involved in.
And so finding a way to communicate that effectively not only helps to, um,
disseminate your research but also helps to,
um, build your credibility and open doors for you.
So today, I'm gonna talk about four different topics.
We'll talk about how to write a research paper,
specifically an NLP research paper.
Uh, I'll talk a little bit about the conference submission process.
Just so that you have sort of, uh,
an overview of how papers wind up getting accepted to conferences.
I'll give you some advice on oral presentations,
and then I'll give you some very tactical tips on
the video presentations that we're looking for, for this class.
Here is a New Yorker cartoon about writing papers,
sometimes it feels like this.
Okay. So here's an outline of a typical NLP paper.
Um, there's, uh, the,
the field of NLP has evolved some conventions about how papers are typically structured.
And this kinda give you,
it gives you a sense of what those conventions are.
Um, and so when you see papers that are presented at ACL, or EMNLP,
or NAACL, or EACL,
they tend to follow these conventions.
I'll say more in a- in a few minutes about what the conventions are,
but first I wanna talk about why these conventions.
And there's really two reasons why it's a good idea to follow these conventions.
One is that the conventions have evolved for a reason.
The community has- the research community has converged on these conventions
because they're effective- because they are- an effective way to communicate your ideas,
and I'll explain more why in a moment.
The other reason to follow conventions is that,
because they're conventions,
they make it easy for your audience to understand what they're reading.
They make it easy to grok.
Typically, the content of your papers will be technically dense.
Your audience will already have a challenge understanding the technical content.
You don't wanna double that challenge by making them also
struggle to understand the rhetorical structure that you're trying to follow.
If you follow established conventions,
that won't be an issue.
They'll know what to expect, why they're reading
what they're reading, and it will make it easier for them to understand your paper.
So the, uh, the structure typically looks like this.
I'm not gonna dwell here because the next slide is somewhat redundant with this.
But I'll just say, um,
this is kinda of a conventional ordering of sections of an NLP research paper.
You- you do have the,
the freedom to play with this, to violate conventions,
but if you're gonna violate the conventions,
you should do it very consciously and you should do it for a good reason,
not just sort of haphazardly.
And the reason, this is shown with eight pages and that's because typically,
NLP conference papers are eight pages with two columns, single-spaced.
Because part of the convention is a convention for laying out and formatting,
which you will follow in your final papers for this class.
So this is somewhat redundant with the previous slide.
It's just sort of a different way of presenting it.
What should go into a paper?
Uh, this is actually a question of formatting-
Yeah.
Why the double columns?
There's, do different journals have different views of this? Like,
NIPS is very for single, like, column.
Uh, but it sounds like,
NLP's against that.
I don't have a great answer for you on this.
Um, that one might be somewhat arbitrary convention.
I think some of the conventions are very well-motivated.
There's good reasons for them.
Maybe some of them are arbitrary,
and that one might be an arbitrary one.
But here's what's a lot less arbitrary.
I think a great paper should have a narrative arc,
and the arc needs to include these elements.
What is the problem that you are trying to solve?
Why is this problem important?
Why should we care about it?
What have others done in this problem?
What approaches have others explored,
and why are those approaches not fully satisfactory?
Why is there still room to do something more?
That's- those three things are kind of the introduction to your paper.
That's kinda the preamble,
it launches you into your story.
Then you need to talk about, what's new?
What are you doing?
And this is really the meat of your story.
What is the, uh,
approach that you're gonna follow?
What is the model that you're gonna develop?
What distinguishes that from other things that have been done before?
And this is probably the longest part and the most technically dense part of your paper.
Uh, and sometimes it will include a description of a dataset if,
if your- if the dataset that you're working with is not completely standard,
something that lots of other people have worked on.
If it's something new,
if you collected data you need to explain how you did that.
Then you need to talk about how you're gonna evaluate the success of what you did.
Then you need to talk about the evaluation itself,
the results of the evaluation.
Then some discussion of the significance of those results.
Um, so what does it mean?
What can we learn?
Um, what are the strengths?
What are the limitations of the approach that you took?
Uh, and then perhaps,
finally, some pointers to future work.
So, uh, almost assuredly the work that you've done is not complete,
it's not the last word.
There's probably still limitations in whatever you did.
And if you can highlight what those limitations are,
and point to opportunities for future work,
you're doing the community a service by helping them
identify what the next steps are for,
uh, further progress in this field.
So those are kinda of the key elements of the story that you wanna tell.
I did that off the top of my head,
but hopefully it aligns really well.
I think it does align really well with what's here,
because it's all- there's- it's all- there is a reason for all of these pieces.
You kinda need all of these pieces in order to have a complete story.
What are you working on? Why does it matter?
What have others done?
What are you doing?
How are you going to evaluate it?
What was the result of that- that evaluation?
And what does it all mean?
That's a complete story of your work.
Um, later, I'm gonna talk
about how to write a good abstract and also how to give a good talk.
And I'm gonna kinda repeat what I just said,
because no matter what format you're telling your story in,
it might be written or spoken.
It might be eight pages long or it might be a single paragraph,
but you kinda wanna have those key elements.
Uh, Stuart Shieber is- yes.
Right. You know, in the metric section where you describe the model,
I've seen both approaches.
One is like is if you're more or less various with [NOISE] components across to
describe the details of each component and then how they all fit together,
and there are other approach being you first describe, you work on,
like, the overview of the model and then going to depths.
So I mean, I- what- what is it that dictates how you would write that particular part down?
Like in some papers, you first describe,
how you did feature engineering and then in the model was the
classifiers that you used, or you could do just the other way around.
[NOISE].
[NOISE] I think there's room for variation here,
um, and room for personal preference.
I think my inclination would be to, uh,
sort of do a two-pass approach;
first a top-down and then a bottom-up.
So first, start with a high level and say,
"Here's the overall picture of the system," um,
then dive into individual pieces and describe the individual pieces,
but then come back up to the top level and say,
"How all those pieces come together."
[NOISE] But I think, um,
[NOISE] there- the- there's less of,
uh, community convention or prescriptive answer on how to answer that question.
I think there's a lit- a little bit more room for variation.
[NOISE] So Stuart Shieber is, uh,
an NLP professor and researcher at Harvard,
and he has this, um,
document which we have a link to here,
which is about writing great papers and he
talks about the rational reconstruction format.
He describes- there's actually kind of, uh,
uh, Hegelian, um, [NOISE] what's the word?
Di- dialectic. Thank you.
[NOISE] There's a Hegelian dialectic here, uh,
with a thesis and antithesis and synthesis, okay?
So we start with the continental style.
He describes the continental style as one that's extremely concise,
and opaque, and just kind of lays the answer
out but doesn't really tell you how it got there.
When I read this, I- I'm reminded of, um, John Nash.
John Nash's PhD thesis was 26 pages long.
My PhD thesis was 120 pages long,
that's because I'm not as smart as John Nash.
Um, by the way,
John Nash had two citations in his PhD thesis.
If you're John Nash,
you don't cite other people,
other people cite you.
[LAUGHTER].
Uh, my PhD thesis had,
like, 150 citations because I'm not John Nash.
Um, if you're John Nash,
you can get away with this.
If you're just about anybody else, it's not recommended.
This is not the best way to communicate, uh,
your ideas and how you got to them and make them understandable to others.
It's too cryptic, it's too dense,
um, and doesn't provide a way in.
[NOISE] Uh, the antithesis, the historical style.
So this is where- this is kind of a narrative of your research process.
You describe the whole research process in laborious detail,
and it's kinda like you start out,
it was a dark and stormy night when I began to
investigate the problem of natural language inference,
um, and you go on from there, right?
And, uh, describe all the things that didn't work,
getting all the places where you got stuck.
Um, this has the benefit of being more
understandable [NOISE] than the- the continental style,
but it's gonna be long-winded and include
lots of information that doesn't actually help anybody,
um, and it's also not really the recommended approach.
And really the ideal approach is what
Stuart Shieber calls the rational reconstruction approach.
So in the rational reconstruction approach,
you, um, [NOISE] uh,
lay out the problem in a way that makes the-
the approach that you arrive at seem almost inevitable.
Makes it seem like lo- a logical and natural,
uh, consequence of the constraints of the problem.
And makes the- the- the whole approach and,
um, what you did and why you did it seemed very straightforward.
As- as Stuart Shieber says, um,
"Convince the reader that your solution is trivial."
[NOISE] Um, unfortunately, the conventions of the community, these- these conventions,
this kind of conventional narrative arc that I described,
part of the reason for these conventions is that
they naturally bring you toward this rational reconstruction style.
This is not a narrative,
and it's also not an opaque John Nash style PhD thesis,
it's an explanation of, uh,
a way of approaching a problem that makes sense,
and it's driven by the motivations and the constraints of the problem.
Uh, some hints on mathematics.
Oftentimes you'll have formulas or other mathematical notation in your paper,
Dave Goss has some good hints on how to do that effectively.
[NOISE] Okay.
Let me spend a couple of minutes talking about, um,
conferences, um, but first,
an XKCD cartoon. This is one of my favorite.
This is an old XKCD cartoon,
this one is number 541.
If you're a regular XKCD reader,
you know that these are numbered sequentially and I think we're up to
like 1,700 now or something like that.
So this is an old one,
but this is one of my favorites because it's about how to put smileys inside parentheses,
and I struggle with this.
Don't you struggle with this?
We all struggle with this.
And I think he's the first guy who,
you know, points out that we all struggle with this.
I also like this because he starts out by saying, "Hi, I'm Randall.
Welcome to my TED talk."
And welcome to my TED talk is kind of a meme now.
But as far as I know, he's the first person who like used it in this way to be funny.
So I love this,
um, cartoon. [NOISE] Okay.
So I wanna talk a little bit about, um,
what it's like to submit a paper to
an NLP conference and what the review process is like and how
your paper actually gets accepted because this is the information that
you'll need to know if you actually submit a paper to an NLP conference.
Um, I'm- by the way,
I'm sure there are other people in the room, uh,
from the teaching team and maybe others as well who have been through
this process and I hope you'll speak up and help fill in the gap,
whatever gaps I leave or- or share your own insights on this process.
Um, [NOISE] so here's kind of an overview of
the typical process for getting a paper accepted.
First, you write the paper.
You guys are doing that now,
um, so that's great.
Then you submit the paper through, uh,
there's a- usually a web interface where you upload your PDF,
and as part of the submission process,
they give you a bunch of keywords that you can choose from.
And the keywords will be things like machine translation
or computational semantics or natural language inference.
They sort of correspond to subfields of NLP.
They may correspond to specific techniques like reinforcement learning,
um, [NOISE] they may, um,
be things like, um, you know,
hu- human annotation or something like that, um,
techniques, strategies, uh, topics,
subtopics of NLP that are particularly salient for your paper.
So hundreds of people,
uh, upload lots and lots of titles.
The reviewers, actually the organizers,
the conference chairs, um.
Oh, yeah. Um, [NOISE] a conference typically has several different areas, um,
the areas each will have area chairs and the chairs are responsible
for recruiting reviewers to help review papers in that area.
And they'll organize the papers into areas in large part based on the keywords,
and then they'll send, um,
they'll put on a webpage lists of the submitted papers and
invite all the reviewers to come look at the list of papers,
[NOISE] and submit bids on which ones they want to be responsible for reviewing.
So as a reviewer,
you'll see this webpage with 100 papers on it.
You'll see the titles.
You'll be able to click through to get the abstract of the paper.
What you won't see is author names or affiliations or anything like that.
Um, you will see the keywords that are on the paper.
And based on that, you need to choose which papers you wanna review.
And so naturally your,
your decision on which papers to bid on will be largely
influenced by [NOISE] the titles that you're seeing.
Which is, uh, a good reminder that choosing a good title for your paper really helps.
Um, good titles help to attract attention,
um, and get people interested in your paper.
They're like an advertisement for your paper.
The ideal title is one that
gives a very clear indication of what's going to be in the paper,
but in a way that makes it interesting and enticing.
Um, here's a bad title that is
from my- this is a title that I used from my own experience.
It was actually not a title of a paper,
but it was a title of a talk that I gave one time.
And the title was two aspects of the problem of natural language inference.
It's a terrible title.
Ah, it tells you that it's about natural language inference, okay,
but two aspects of the problem- okay but which two aspects
and what are they- why should I care, we have no idea?
There's nothing, there's nothing interesting.
I mean actually now that I think about it it's kind of like those, um,
listicles on the web that are like seven things you should do to enhance your whatever.
Uh, not a great model to follow though.
Um, a much better example of a title- one of my favorite titles of a recent,
uh, it's not NLP,
but a machine-learning paper at NIPS in 2016.
There was a -ah,
paper that was titled.
Um, Learning to Learn by Gradient Descent, by Gradient Descent.
Maybe somebody- this was from DeepMind.
Maybe some of you read this paper, this paper.
And what I love about this title is that it's really descriptive.
You know what's going to be in that paper.
It's going to be about learning to optimize the gradient descent process itself.
So you know what the- what the paper is
about and you know what you're gonna learn from this paper,
and you know whether you're going to be interested in this paper.
But then the title is also a little bit whimsical because it has
this recursive structure and nerds love recursion and,
um, probably you can't get away with making every title into a recursive title like that.
But if you have a title that's both very descriptive of what it's about
and also somehow intriguing or with a twist or something appealing about it,
that's a great way to attract attention.
Okay. So, so all the reviewers submit all their bids on which papers they wanna
review, the program chairs then assign reviewers,
um, based on their bids.
The reviewers actually read the papers,
write comments, and supply ratings.
And I'll tell you a lot more about the ratings in just a moment.
Then there's a response period.
So the, um, authors are allowed to respond to the reviews.
So the reviews- the comments from the reviews are sent back to the authors.
Not the ratings the- I'm actually not sure.
The ratings too. Okay. So the authors will
see the ratings and they'll see the comments that the reviewers have
supplied and they have the opportunity to
respond to those comments for some short period of time.
Um, and this can stimulate some discussion amongst the reviewers.
Um, and this is the first time typically that
the reviewers are no longer anonymous to each other.
So there may be three different reviewers who have read
a particular paper up, until this point they're anonymous to each other,
but now their identities are revealed.
The authors of the paper are still anonymous and we don't know who they are.
But the reviewers, uh,
become known to each other and you can have some interesting discussion around the paper.
Um, and finally the program committee does some magic
to figure out which papers get in to the conference and which ones don't.
Probably largely based on the scores that have been assigned.
Um, [NOISE] so let's talk a little bit more now about the scores.
Um, this is, um,
when reviewers are asked to assign scores for papers,
they're asked to evaluate on many different dimensions.
This is a list that's typically used for ACL and I think
other NLP conferences have lists that are like this but with minor variation.
So let's- let me talk a little bit about each of these.
Um, appropriateness basically means is
this p- is this paper a good fit for this conference?
Does it sort of fit within the theme and
the topics that are appropriate for this conference?
It might be a great paper that doesn't belong in this conference,
and so I get a, a low score for appropriateness.
Clarity I think is pretty self-explanatory.
Can you understand what the people actually did in this,
um, in this investigation?
Replicatabil- replicability is a criterion that's growing in importance.
Um, I think five or ten years ago people didn't really think about this that much.
More and more it's becoming part of
a community standard that results should be replicable.
This is really healthy.
And a big part of replic- replicability is making code and data
available online so that others can
go and repeat the experiment and see if they get the same results.
Originality, I think it's pretty self-explanatory.
Soundness and correctness were the decisions,
um, well- well-motivated decisions is- or the things that I try.
Were the things that I tried the right things to try? Do my,
um, sort of logical steps that I followed in my investigation make sense?
Meaningful comparison.
Did I, um,
include comparisons to all of the work that I should have compared to,
or is their work out there that's very relevant but I seem to be unaware of it?
If I'm unaware of related work or if in my quantitative results,
I don't make comparisons to closely related work,
um, that's a miss.
Thoroughness. Did I, um,
did I explain everything I need to explain?
Sometimes a lack of thoroughness can impair replicability.
If I haven't actually specified all of the details of all of the steps,
it makes it hard to re- to replicate my results.
Um, impact of ideas or results.
Ah, I think that's pretty self-explanatory.
Impact of resources.
Um, this is particularly relevant if your work
introduces a new dataset to the community that might be of use to others.
Not introducing a new dataset is not a problem.
Um, it doesn't mean that there's anything wrong.
But if you have introduced a new dataset that's going to be valuable to the community,
that can really help to imp- increase the impact in the appeal of your work.
And finally, the overall recommendation which is undoubtedly the most important score.
It's sort- sort of, ah, intended to be a distillation of all the others.
So all of the other scores should inform the overall evaluation.
Those are the scores and then typically they ask for
a couple of other bits of metadata. They ask.
Do you- as a reviewer do you recommend this work be presented as
a poster in a poster session or as an oral presentation? So as a talk.
Um, it's definitely more
prestigious to be selected for an oral presentation than a poster,
but on the other hand there's nothing wrong with presenting your work
as a poster and it's a great way to get exposure for
your work and sometimes presenting your work as a poster is actually
appealing because it facilitates discussion with your audience.
Ah, what sort of, one on one discussion makes it
very easy for interested people to ask questions.
Um, [NOISE] sometimes by the way
some conferences have both short and long oral presentation.
So long oral presentation is typically a 20-minute presentation.
Sometimes they're also short oral presentations
which are like five minutes or seven minutes.
Um, of course as an author you'd rather have
a long form presentation than a short form presentation,
but they'll often ask that here.
Um, and they may ask whether
this paper should be considered as a candidate for Best Paper.
Um, typically a conference will have at least one Best Paper Award.
Sometimes they'll have like, um,
Best Paper Overall and best,
um, Best Student Paper or something like that.
[NOISE] Oh and some conferences have both long-form papers which are at the eight pages,
the eight page convention that we looked at before and also
short-form papers which are four pages.
You may have submitted a paper as an eight-page paper but,
a reviewer might have an opportunity to say
actually I think this one would be better as a short paper,
if it were reworked a little bit, um,
and typically the bar is a little bit lower for short papers.
[NOISE] Is the Best Paper possibility something that the reviewer is saying or the paper is saying, because like, poster talk as well,
like, is that something that we have to put on as the authors, is that part of the form?
Um, no.
This is coming from the reviewers,
and it will be the,
the program chairs will make the final determination on the Best Paper Award,
but reviewers have the opportunity to effectively nominate
a paper to- for consideration as a best paper.
[inaudible]
And the, and the Best Paper Award [inaudible] it's confidence value.
That's something that even program chairs in filling it up to get- get together.
[NOISE]
Yes.
[inaudible] work reproducible. You mentioned making the codebase like open-source,
and making your data available within the codebase,
maybe you've set that random seeds, and so on and so forth.
But beyond that, within the code structure in itself,
what are the things that you think make,
like, make research reproducible?
Um, I think first and foremost just making the code available, um,
which 10 years ago unfortunately was not the norm or 20 years ago was not the norm.
These days [NOISE] standards to put all the code on GitHub.
Um, and I think just putting the code on GitHub is,
uh, a huge proportion of what you need to do to make the code reproducible.
Now of course, um,
adopting good software engineering practices and making sure that the code is readable,
and documented and not a massive spaghetti that nobody could, you know,
possibly, um, understand, uh, helps as well.
Um, but making it available is by far the most important thing.
[NOISE].
Um, a slide with the names of some notable NLP conferences.
I won't talk about each of these but,
um, these are kind of the,
the biggest and most reputable NLP conferences,
um, some additional conferences.
These conferences tend to be a little bit more f- more focused on,
uh, the web and information retrieval rather than NLP.
And these conferences are a little bit more focused on
machine learning and include lots of other topics beyond NLP,
like vision for example,
but NLP tends to be an important sort of
sub current or sub-theme in those conferences as well.
So if you're thinking about,uh,
trying to bring, uh,
a paper to a conference,
these are kind of the, the likeliest suspects to look at.
I won't talk about this because I'm not knowledgeable about this.
Um, the abstract of your paper is a little,
um, paragraph that summarizes your paper.
And the best way to think about it is that it's an advertisement for your paper.
People will read the abstract to decide whether they
want to make the investment in reading the eight-page paper.
Sometimes reviewers will read it to decide on
whether to bid on- to bid to review your paper.
And so just like with the title,
you want the abstract to be both informative and engaging.
And you kind of want the sa- most of the same key elements that I described before.
You wanna say what problem you're working on,
you may not have enough room in the abstract to say why it's
an important problem or what previous approaches have been employed,
but you definitely want to say what your contribution is.
And you want to communicate the most important result of an evaluation in your abstract,
um, and why it's significant.
Really, the abstract should summarize what are the key contributions of your paper,
and why should I as a reader be interested in learning more.
And you have to find a way to do this in only about 100 words.
Abstracts are very concise and they should be polished like a,
like a, like a gem.
Uh, it's really valuable to get feedback from other people on your abstracts, um,
on whether they are clear,
and whether there's any way to pack more,
um, into a very tight space.
Okay. A little bit of advice on giving talks.
Um, actually I have nothing new to say here,
this is kind of exactly what I said about writing a paper.
Um, the format is different but the content is
gonna be largely similar to what I said for writing a paper.
Um, Geoff Pullum has some great guidance for giving talks,
um, that I encourage you to go read.
This is a very short document, uh,
where he gives [NOISE] some great rules.
The one that I want to emphasize here is number 5.
Remember that you are an advocate not the defendant,
um, so kind of a metaphor from law, right?
Your- when you present your work orally,
it's not you that's on trial.
It's some ideas that are on trial.
It's, uh, some, some work,
an investigation, an approach, a model.
Don't take it personally,
don't get wrapped up,
don't get wrapped up in it.
And if people question it,
don't take that as an attack on you.
Don't let your ego be part of it.
It's okay if the work is incomplete.
It's okay if the work has flaws.
Be forthright about that,
um, be dispassionate in your presentation of it.
Do your best to present the work in its best light,
but don't take anything about it personally.
And that really helps to make the presentation convincing and effective.
And it kind of echoes this insight from, from Patrick Blackburn.
He says, "Where do good talks come from?
From honesty."
Be honest about the work that you've done.
As long as you're honest,
you kind of can't go wrong,
um, in presenting the work.
Um, a little bit about slides and different styles of creating slides.
Uh, there's kind of two very different approaches to creating slides.
One is the minimalist approach,
the other is the maximalist approach.
The minimalist approach is, um,
to have very little on each slide,
maybe just a couple of words.
The slides go very quickly.
You kinda blaze through them.
You're like click click, click, click,
click as you talk.
Um, if you were to look at the slides in isolation,
if somebody just gave you the deck but you didn't see the oral presentation,
they might be hard to understand because there's just a couple of
words or maybe a diagram or something on each slide.
Um, but there- they tend to be more exciting and interesting as oral presentations.
The maximalist approach, by the way this slide is more like the maximalist approach.
The maximalist approach, has lots of words on the slide.
They tend to go slower as you go through them,
um, but if you look at them in isolation,
they're a lot easier to understand because all of
the words or most of the words that you need are there on the slide.
A shortcoming of the maximalist approach is that
the audience is torn between reading the slide and listening to you.
And if you have a lot of words on the slide,
when the slide first comes up they- they're probably reading the slide,
and they're probably not listening to you.
So there are pros and cons to these two different approaches.
I'm, uh, preparing to speak at WWDC next week.
And if you know anything about Apple,
you that Apple takes the minimalist approach.
We have slides that have like one word on them,
and they go at a pace of one slide every five seconds or something like that.
Um, and then there's like graphics, and,
you know it's a very different style.
Um, there's no right answer here.
You need to find a style that works for you and
works for the content that you're trying to present,
but I think it's valuable to think about the pros and cons of these different styles.
Um, Edward Tufte and Peter Norvig have thoughts about PowerPoint,
those thoughts are not flattering but if- but this
is kind of amusing if you wanna go click on these links later.
Uh, more mundane things,
um, I won't talk about all these but there's a bunch of really good advice here.
It's really good to test your setup in advance,
typically when you're presenting at a conference,
there will be, um,
like, five talks in a row.
Before that there will be a break,
if you're presenting during that block test your setup in the break before that block.
I one time didn't do that,
made that mistake and I spent- I had 20 minutes for my talk and I
spent the first five minutes of my talk fumbling with my setup,
standing in front of a room of 300 people,
like, fumbling with my setup.
And it was incredibly embarrassing,
and then when I finally did get started with my talk I was all
flustered and my talk was a disaster.
So learn from my mistake,
test your setup before you launch into your talk.
Um, also make sure you don't have any windows in
the background with embarrassing stuff on it.
I have not made that mistake,
but I've seen other people who've made that mistake.
Um, and the question period.
Typically when you give a talk at a conference, uh,
there'll be a- each talk is 20 minutes,
and there's usually five minutes for questions.
The question period is really important because, um,
this is when you get feedback from the audience about what they want to know more about.
Questions are great.
If you get questions in your question period,
especially thoughtful questions, that's a sign that you've really
connected with your audience and they're interested in your topic.
If you don't get any questions,
you should be a little bit bummed out.
That somehow you failed to get your audience interested in your topic.
But questions are also scary because you have no idea what's going to come at you.
The talk you can rehearse and rehearse and rehearse,
and feel really well-prepared.
The questions there is no way to prepare.
You just have to think on your feet and do your best.
Um, you'll do better if you really know your topic well which you will.
But even so, you have no idea what's gonna come at you.
Sometimes questions will come that you don't know how to respond to. That's okay.
It's okay to say, "I don't know,
but I would love to talk to you more about this afterward."
And actually that can be a great opportunity to make a new connection with
somebody in a research community that can lead to anything,
it could lead to a future collaboration.
So treat that as an opportunity and as a,
as a welcome thing.
Okay. Just a couple of tactical things about your presentations.
For this class, you're gonna prepare a video presentation.
It's due on June 5th.
That's coming up soon.
It's going to be a four minute talk.
You're gonna create a short deck in Google Slides or PowerPoint or Keynote.
You're gonna record yourself [NOISE] talking through this deck.
You can do that, uh, on a Mac using QuickTime or if you're not on a Mac,
you'll have to figure it out [NOISE] but maybe,
uh, other people in the class will have helpful suggestions for you.
Um, you'll upload the video to YouTube.
You can make it private if you want to, um,
and then you'll send us the link for this four minute video.
You don't have to be in the video.
Your, your face doesn't have to be in the video.
The video will just show your slides and the,
and the audio will be you talking through the deck.
Any questions about that before I hand off?
[inaudible] you said about-
I think the answer is yes.
Slightly unrelated, but I'm curious to know your advice on like reading papers,
so as you said, you must be reading like a lot of papers.
Yeah.
So how do you do that efficiently?
Do you do it like in one go or you'd go to the
abstract and like introduction and conclusion and then do like a second pass later?
It's because, I mean it takes me like one and a half to two hours and it's pretty dense.
I don't have any specific strategy to suggest,
but I share your experience like I can't
absorb an eight-page dense paper in one sitting typically.
So if it's a paper that- I mean sometimes I'll-
if it's the paper that I don't think it's that important for me to really get,
I'll sort of like read the abstract
and introduction carefully and then kind of skim the rest,
but if it's a paper that I really wanna get,
I typically need to read it in more than one sitting in order to fully absorb it.
Beyond that I don't have any specific strategy to recommend.
Yeah.
You can't actually record it like live as like with a video camera,
or does it need to be just like slides and voiceover?
I think that's okay. It's not what people usually do, but I think it's okay.
Yeah. Okay, I'm gonna hand off at this point to Min and Jayadev.
Right. So I'm gonna be giving a short presentation
on data augmentation for natural language processing.
So what is data augmentation?
So data augmentation is a technique to uh,
to increase the amount of relevant data.
In machine learning and especially in deep learning,
more data usually means better accuracy.
So if you don't really have a lot of training examples,
you can try data augmentation techniques to uh,
to create more synthetic training examples.
Data augmentation is actually very popular in computer vision, uh,
people create new images by shifting the original images or uh,
creating zoomed in or zoomed out versions,
rotating, flipping, and etc.
So how do we do such transformation in natural language?
We can't flip a sentence or rotate a sentence.
So we need to come up with different approaches.
So before we actually talk about the actual data augmentation techniques,
we ran some simple experiments.
So we wanted to do um,
sentiment classification of IMDB movie reviews using logistic regression,
and we wanted to see the correlation between number of
one possible- number of training examples and the model test accuracy.
As you can see, if we only have 1,000 training examples,
model- our model accuracy was only about 79%.
But with more training examples they actually see- increased significantly.
So it's usually the case that
the small number of- with small number of training examples,
you can't really have a really high-performance model.
So in those situations uh,
data augmentation can really help boost the performance.
So for this presentation,
I'm gonna assume that we only have 1,000 examples and we are going to
apply different data augmentation techniques to see if we can boost the uh,
the accuracy of 79%.
Feel free to uh, ask any questions.
So how do we actually augment natural language training data?
So we're gonna be discussing two main approaches.
The first one is easy data augmentation uh, EDA,
which uses uh very simple heuristics to uh,
to transport sentences to create um, new relevant examples.
We're also gonna talk about back translation,
which uses neural machine translation to,
um, to give noise to your sentences.
So as we mentioned before um,
the baseline is going to be the IMDB 1,000 examples trained logistic regression model,
which has an accuracy of about 79% and we're gonna apply uh,
EDA and back translation to um,
to augment the training samples.
Okay, so we're gonna look at the sentence.
The final project is the main assignment of course,
to explore different data augmentation models.
We're going to transform this sentence using back translation and EDA to give you uh,
some sense of how sentences can be transformed.
So let's first talk about easy data augmentation.
Uh, it's a paper written by Wei and Zou in 2019.
So it's a relatively new research paper,
and it uses four very simple techniques.
First, it uses synonym replacement.
So it randomly selects n words from a sentence and it
replace- replaces those words with its synonyms- with their synonyms.
Random insertion is something very similar, uh,
but instead of replacing it inserts uh, the synonyms.
A random swap and random deletion are pretty self-explanatory uh,
random swap swaps two random words and random deletion uh, deletes a random word.
So um, let's look at our example sentence.
The final project is the main assignment of the course.
So if we apply um, synonym replacement,
we can replace main with principal because um,
principal is a synonym of main.
So the resulting sentence can be um,
the final project is the principal assignment of the course.
If we apply random insertion we can insert last,
because last is a synonym of final and the resulting sentence now becomes,
the final project is the main assignment of last the course,
which doesn't make a lot of sense as a sentence,
but it still roughly um, retains the original meaning.
And if we perform random swap,
we can swap main with assignment,
and we can also delete a random word by using random deletion and in this case,
um, the word project is deleted.
So if you look at all these sentences you can see that um,
it doesn't necessarily preserve the original meanings,
but they're roughly similar and in line with the original sentence.
So the authors of the EDA paper actually ran
latent space visualization of the augmented sentences and the original sentences.
And you can see uh,
the red dots roughly coincide with the red circles,
and the blue dots also roughly coincide with the blue triangles.
Meaning that um, they roughly have similar meanings.
So EDA can be very useful in
synthesizing new sentences that are relevant to our training samples.
So we apply EDA to our 1,000 training sample dataset
and we use the GitHub repository that was published and released by the uh, EDA authors.
It's relatively simple to set up and it doesn't- it's relatively easy to um,
to perform the EDA operation using the GitHub repository.
So we created 10,000 more EDA augmented examples,
and then we used those examples in addition to the original examples into
our logistic regression model and we were able to see
about 1.2% um, accuracy improvement.
Which we thought was pretty significant because um,
EDA merely performs very simple heuristics.
Okay? So let's talk about back translation.
So back translation uses neural machine translation to augment training data.
So more simply put, when you have an English sentence,
you translate that into an intermediate language such as German,
French or anything you can think of and then you translate back- translate that back into
English and the resulting sentence is going to
be kind of a paraphrased version of the original sentence.
There's gonna be some noise into the sentence.
So let's take a look at an example.
So this is an example of using um, Google Translator.
So our sentence um,
the final project is the main assignment of the course.
If we translate that into German and translate it back into English, um,
the resulting sentence now becomes,
the final product is the main task of the course.
As you can see uh, the assignment is swapped with task.
So it's kind of- kind of like a paraphrased version of the original sentence.
We can use different intermediate languages. I tried Korean.
Um, if we- if we translate to Korean and back into um, English,
the sentence now becomes,
the final project is a major challenge for the course,
which is a bit of a more paraphrased version.
So um, using Google's translator we discovered that it's
possible to create paraphrased versions of sentence um, using back translation.
So we applied back translation to our 1,000 examples.
So we used um, Google Translate API um,
to translate from English to German and German back to English.
And we created 10- we created 1,000 um, back translated examples.
So when we combined those back translated examples to
our original training data and used it to train our logistic regression model,
we were able to see a pretty impressive um, 1.8% accuracy improvement.
So lastly, uh, we also used back translation in addition to EDA to see,
uh, how things are gonna work out,
and we trained our logistic regression model on the 1,000,
um, original training examples,
and 10,000 augmented examples from EDA,
and also 1,000 augmented examples from back translation.
Then when we combine all of them,
and then when we train a logistic regression model,
we're able to get an accuracy of a- of around 81%,
which is about 2% increase from the baseline model.
So these are some few, uh,
data augmentation techniques we can probably use.
Um, they're relatively simple to implement.
For the back translation,
you just have to set up a Google Translate API and just call it from- from your code.
And if you're working on your final project and,
um, if you don't have a lot of data,
and you think that having more data is gonna help boost your model's performance,
I recommend, um, looking into, um,
data augmentation techniques including EDA or back translation.
Even if you do have a lot of data,
I think it's still worth, um,
experimenting to see how things go,
and you can always write down the results.
So, um, on your final paper or your final project,
ev- even if it doesn't work out.
Cool. That's all I have.
[inaudible] techniques like that
might give like behavior gains or is it always kind of like just a little bit of an increase?
So from the paper that I read, um,
it seems that data augmentation is still relatively new in natural language processing,
so, um, I couldn't really find any more on [NOISE] comprehensive or complicated,
[NOISE] um, model for data augmentation.
But these are some of the most widely used some augmentation techniques.
Any more questions? [NOISE] Cool.
Thank you. [APPLAUSE]
Hi, everyone, I'm Akhila.
I'll be talking about, um,
probing blackma- black box models today.
Um, so essentially this- the-
the whole concept of probing is to be able to understand what models are actually doing.
It's some sort of introspection technique.
Um, today we'll be talking specifically with regard to like,
um, contextualize world whe- word embeddings and sentence embeddings.
But this is something which can be applied to a lot of other models as well.
Like, um, like I'm sure everyone's heard about the OpenAI GPT-2 lang- huge language model.
It's- it, it produces these contextualized word embeddings,
which is trained on a simple objective of language modeling,
but they do encode a lot of other information,
and it's really interesting to see how they encode this information,
and how it can be effectively applied to a lot more downstream tasks.
[NOISE] Um, so, an
informal definition of what a probing task is that it's
a classification problem which focuses
on some simple linguistic properties of these embeddings.
Um, this is an informal definition from this paper which is by Facebook AI Research.
Uh, I would highly recommend reading this paper if you wanna
go [NOISE] more into like how probing architectures work,
and that sort of stuff.
[NOISE] Um, and so these probing methods are designed to evaluate to what extent,
um, like they, they encode this sort of linguistic properties.
Um, so when you see probing tasks,
[NOISE] most of these probing tasks are gonna be very simple,
so that you don't have like interfering [NOISE] results.
You're gonna be, be probing for something very specific, um,
and these probing architectures are really
simple in the sense that there would be like a linear transformation,
or like a two-layer NLP.
Um, essentially the main idea is when you
do have these sort of embedding representations,
you want to understand what sort of information they encode rather
than forcing some sort of classifier to learn this on its behalf.
Um, and, uh, it's [NOISE] easier to control for biases in
these probing tasks since it's simple because I- you
can't really control this in like the downstream tasks.
And this is gonna be, uh,
an architecture which is agnostic of where do you get this embedding representation from,
which makes it really, uh,
useful in my opinion because it's very applicable
and one sort of probing task which is designed to,
you know, probe for certain properties can be applied to different models.
[NOISE] Uh, so one example is, uh,
given that you have a pre-trained encoder which is trained on some objective, um,
you want to get some one vector representation from this encoder,
and then you would be passing that to- through your probing model,
and as I already mentioned that this probing model is designed to be like really shallow,
like it's gonna be like a one-layer transformation,
or a two-layer, and then you- uh, at the end of this,
it would be a classification task based on
what sort of linguistic properties you are trying to,
um, see is encoded in this sort of, um, word representation.
[NOISE] Um, so today,
we'll be talking about two sorts of probes.
One is like sentence embedding probes,
and the other one is word embedding probe.
They both go hand in hand,
but I, I kind of want to, um,
highlight some of the characteristics which you can
actually probe for when you're looking at
a sentence level versus [NOISE] like a word level.
So given a sentence level embedding,
you can- you- I just named two of them,
but you can obtain like a sentence level embedding from a BiLSTM, or a Gated ConvNet,
or any other encoder which is trained on any objective,
like it can be machine translation.
It can be SNLI. It can be SkipThought.
SkipThought is a special kind where you're training
this encoder such that enabling sentences have- are most similar,
um, or, uh, the- there's a lot of literature
on how encoders i- initialize with random weights.
They can also encode,
um, properties of the sentence, or the word.
So, uh, given that you do get an embedding, uh,
at the- like when you do pass a sentence through this encoder,
and you get an embedding at the end of it,
there is a couple of properties you can probe for;
like surface level information.
When I say surface level information,
it can be like, um,
does this embedding have some sort of
information about what the actual length of the sentence was.
Um, word content, like can you actually
get information about what sort of words are in the sentence?
Um, syntactic information is whether- uh,
this is more on an encoder level,
where you're trying to understand whether this encoder is actually
sensitive to some synta- like semantic information.
Like when I say bigram shift, given a sentence,
if I swap two bigra- like if I take a bigram, and I swap that,
and I pass it through my encoder,
is my encoder gonna be sensitive to the swap or not?
Um, and then semantic is,
uh, for example, is tense.
You can find the tense of the,
the main clause verb.
Uh, I actually like the last task which is the Semantic Odd Man Out task,
where basically what you do is,
um, you're gonna replace a,
a noun or a verb randomly with another noun or a verb,
but the unigram frequency of
this particular noun or verb which is swapped out is similar to the original word.
Um, and when you do pass it through-
so you're trying to- you're trying to see if the sentence
is- the sentence embedding is just based on superficial like tone frequency properties,
or is the actually embedding some form of context.
Does it make sense or? Yeah. [NOISE] Um.
So the next topic is- ah,
rather the probe is like the Contextual Word Embeddings,
which seems to be, uh, hip- been happening.
Place right now that like BERT and OpenAI.
So reiterating the same idea about a probe,
it's just that- so when I say Contextual Word Embeddings,
they're very different from say GloVe embeddings,
because in GloVe it would be like one vector for one word,
but Contextual Word Embeddings is- is when you do parse a sentence,
you get like a word embedding for every token based on
the context in which it- in- of that sentence [NOISE].
So the idea is the same that, uh,
if you have a simple model which is trained
to predict like some sort of linguistic information,
um, and this is a simple model.
Then you can infer that this information is actually encoded in these Word Embeddings,
and not really a property of this probing architecture.
Uh, so some of the tasks you can do for a word level,
um, embedding is whether it has syntactic information again.
Like the part of speech,
the named entity recognition,
uh, given two tokens whether you can
actually get like the dependencies between these tokens.
Uh, the constituency labeling meaning that given
a sentence you want to induce like a phrase structure on it,
so whether you can identify a spine of tokens as a verb phrase or a noun phrase.
Um, semantic information is basically anything you can get from WordNet like, uh,
what about the Semantic Role, Entailment, Concreteness,
the Coref, Sentiment, Relation, Extraction.
Um, then the next sort of thing you can
probe for is like local and long-range dependencies.
Like if- if you were to take the task of dependency labeling,
um, given two tokens which are,
um, like two words apart and given two tokens which are like ten words apart,
can you still identify the dependencies?
Then, um, that would be like a fine-grained experiment to check
whether like long-range dependencies are
actually encoded in these contextualized word embeddings.
Um, I guess- so most of my talk was more about like word embeddings,
probing them, sentence embeddings,
and probing them, but this is something which needs to be
applied or can be applied to any of the,
uh, any of the NLP tasks.
Uh, like I, I guess,
coming back to the word representations,
you are trying to see what sort of information is encoded in these embeddings,
but another space is what sort of information is not.
Um, you're trying to analyze like what sort of information does it encode more?
Is it syntactic or semantic?
Um, if it was language modeling,
uh, you can have different objectives for language modeling,
but it's interesting to see how, um,
if you were to encode a sentence using a language model,
how does that representation vary with the different objective?
Um, then you're- you're- so given a language- okay,
if I were to take an LSTM as a language model,
and then I had a bunch of layers in my LSTM,
would each of these layers encode the same sort of information, or different information?
Um, how do I effectively, ah,
transfer this sort of information from my encoder to my downstream task?
So like, there's this notion of that lower layers encoding more syntactic information,
and higher layers en- encoding more semantic information.
There's- there's also this paper which talks about you want to do like, okay,
so ELMo embeddings has a bunch of layers,
and the same sort of norm follows there that the lower levels are more syntactic,
and the higher levels are semantic.
But effectively- if you wanna improve on downstream tasks,
if you want to use them as out of the box,
then a weighted average of these embeddings
would be a nicer way than just using one of the layer's embeddings.
So probing what layer encodes what sort of information,
and, um, I think this is interesting.
So if you were doing a task of say, um, machine translation,
then you can see how your Seq2Seq model objectives can be changed,
and how this normal encoder would,
uh, differ in properties based on the objectives.
And I guess even just not like- even- not even linguistic properties,
but any if- if you're training like a deep neural architecture, um,
sure, bigger seems to be better,
but you need to- you need to have interpretable models.
You need to understand why models are doing the way they are,
and this is a good place to start off be- by
seeing what sort of linguistic information do they capture?
Is it just based on huge amounts of
data or is it some inherent property of the architecture?
Uh, so I guess I just wanna finish up by saying,
there's a lot of work which can- is being done and
can be done in this whole probing style architecture,
because it'll be a trade-off between what sort of task do you wanna probe?
What sort of architecture do you wanna- um,
do you wanna use, and what sort of hypotheses do you wanna test on?
Um, [NOISE] Again, as I mentioned,
[NOISE] there's a lot of interp- like there's
a lot of need for interpretability of models.
So this will be a nice place to explore especially for your final projects to
see- given that you've trained your model to improve on some sort of evaluation metric,
but does it- does it
do just the objective it's been trained on or is it doing something else as well?
Um, then keep probing.
Uh, yeah. Thank you.
Ah, do you have any questions before- Yes, sorry.
Where that, um, like the random initialization-
Yeah- yeah.
-can it give you information? Like why would that be?
Uh, so I haven't- I haven't,
uh, read much about it, but it does.
So randomly ini- initialized neural networks
do see- do serve as a strong baseline at least,
um, as opposed to like a randomly initialized vector.
Like if I wanted to see- and they- actually they do- for some of the probing task, um,
that regard to contextualized word embedding, uh,
getting an embedding from this randomly initialized encoder,
does on par with like GloVe embeddings, which is very interesting.
So it- it, ah,
as to why it is happening that way?
I- [NOISE] I'm not too sure about it maybe, uh,
maybe we- maybe we can talk about more after the lecture, but,
um, yeah.
Any other- yes.
Is there, uh, also research on like probing
or interpretability of like non-contextualized embeddings?
Like just GloVe or look.
So you can't probe
GloVe for what sort of information as a thing encoded because if you look at GloVe,
I think there is some research on- that GloVe is
inherently encoding some form of frequency,
like based on whatever corpus it is being trained on.
It does inherently encode like
the unigram probabilities like
the- so frequency is one of the properties which is being encoded,
but I guess you can- you could actually do a trial for different properties as well.
Does this answer your question? Yeah. Did you have a question? Yeah.
I was gonna ask, you mentioned the infer for ELMo.
Yeah.
I just wondered if you can say anything more
like type of information is that's encoded by it.
things like more recent models.
Uh, so if- from my understanding, uh,
I think ELMo specifically does encode more syntactic information than semantic.
Like it does poorly on
co-reference resolution and semantic role labeling, those sorts of tasks.
But syntactic information, it does do better.
Um, but again, if you do look at syntactic task,
I think like POS tagging,
that is a relatively easier task.
So I think even GloVe does well on that task.
And so it's hard to say what specifically is being encoded,
but this is definitely an active field of research at the moment from what I've read.
Syntactical information is something which is encoded a lot,
and long-range dependencies is something which is encoded,
but semantic little bit.
Any other questions? Okay, thank you.
Thank you. [APPLAUSE]