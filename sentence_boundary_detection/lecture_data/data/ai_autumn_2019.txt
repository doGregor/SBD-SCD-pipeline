 All right. Let's get started.
Please try to have a seat if you can find a seat and let's, uh, get the show on the road.
So welcome everyone to CS221,
this is Artificial Intelligence.
Uh, and if you're new to Stanford, welcome to Stanford.
Um, so first let's do some introductions.
So I'm Percy, I'm gonna be one of your instructors.
I'm teaching this class with Dorsa over there.
So if Dorsa wants to say hi, stand up.
Hi guys, I'm Dorsa. Um, I'll be co-teaching [NOISE] this class with Percy.
I'm a professor in robotics and robotic interactions.
Super excited about teaching this class and [inaudible].
Great. So we're going to be trading off throughout the quarter.
And we also have a wonderful teaching team.
So these are your CAs.
So if all the CAs could stand up and I'll give you
each person an opportunity to say three words about what you're interested in.
So um, let's start with [inaudible] because you're the head CA.
Hello. My name is [inaudible].
I'm a PhD student,
and I'm interested in natural language processing.
Yay. [LAUGHTER]
Hi. My name is [inaudible].
I'm a second year masters student.
I'm interested in, um,
machine learning and data mining.
Hi. I'm [inaudible]. I'm a second year masters student
and I'm interested in machine learning and natural language processing.
Hi everyone, my name is [inaudible].
masters student and I'm interested in
computer vision.
[BACKGROUND] [NOISE]
Let's go over there.
[BACKGROUND] [NOISE]
Great. Now, any new TAs in the back?
No. Well, um, well,
they're all on the slide.
Okay. So uh, as you can see,
we kind of have a very diverse team and so when
you're thinking about kind of final projects later in the quarter,
you can tap into this incredible resource.
Um, so three quick announcements.
Um, so there's going to be a section every week which will
cover both kind of review topics and also advanced, uh, uh, topics.
So this Thursday there's gonna be an overview.
Um, if you're kinda rusty on Python or rusty on probability,
come to this and we'll get you up to speed.
Um, homework, the first homework is out,
it's posted on the website.
It's due next Tuesday at 11:00 PM.
So remember the time, that matters.
Um, all submissions will be done on Gradescope.
There's gonna be a Gradescope coast- code that will be posted on, uh, Piazza.
So look out for that, um, later.
Okay. So now let's, let's begin.
So when I first started teaching this class, uh, seven years ago,
I used to have to motivate why AI was important
and why if you study it you'll have a lot of impact in the world.
But I feel like I don't really need to do this.
Now it's kind of inescapable that you pick up
the news in the morning and you hear something about, you know, AI.
And indeed we've seen a lot of success stories, right?
AIs that can play Jeopardy or play Go,
Dota 2, pro- even poker,
all these kind of games at super human level performance.
It can also, you know,
read documents and answer questions,
do speech recognition, uh,
face recognition, um, even kind of medical imaging.
And all these tasks are, uh,
you read about how successful these,
uh, technologies have been.
Um, and then if you take a look at outside the kind of the technical circles,
there's a lot of people, um,
in policy, um, and trying to ask what is going on with AI.
And you, you hear about, uh,
these kind of very, uh,
broad claims of how transformative AI will be, um,
to the future of work and,
um, to society and so on,
and even some kind of bordering on, uh,
pretty castro- you know, catastrophic consequences.
So what's gonna happen in the future,
no one knows, but it is fair to say that AI will be transformative.
Um, but how do we get here?
And to do that, I wanna take a step back to the summer of 1956.
So the place was Dartmouth College, John McCarthy,
who was then at MIT, and then,
uh, after that he founded the Stanford AI Lab, um,
organized a workshop at Dartmouth College with, um,
some of the best and brightest minds of the time;
Marvin Minsky, Claude Shannon, and so on.
And they had this not so modest goal of trying to think that every aspect of
learning or any feature of intelligence could be precisely
captured so that a machine can be just, uh, simulated.
So they were after the,
the big question of how do you kind of solve, um, AI.
So now they didn't make that much,
uh, progress over the, the summer,
but a lot of programs and interesting artifacts came about from that time.
Um, there were programs that could play checkers or prove, uh,
theorems, and sometimes even better than what,
um, you know, the human proof will look like.
Um, and there was a lot of optimism.
People were really, really excited,
and you can see these quotes by all these excited people who
proclaimed that AI would be solved in a matter of years.
But we know that didn't really happen and there's this kind of folklore example,
um, people are trying to do machine translation.
So you take an English sentence like 'The spirit is willing but the flesh is weak',
you translate into Russian,
which is what, um,
the choice language by the US government was at that time, and you could,
uh, translate back into English;
and this is what you get, 'The vodka is good but the meat is rotten'.
Um, so the government didn't think that was too funny,
so they cut off the funding [LAUGHTER] and,
um, it became the first AI winter.
Um, so, so there was a period where, you know,
AI research was not very active and was not well- very well funded.
Um, so what went wrong here?
Um, these were really smart people, right?
Um, they just got a little maybe ahead of themselves.
So two problems; one is that the compute was simply not there, right?
It was millions or even billions of order of
magnitude compared less than what we have, uh, right now.
And also, the problems,
the way they formulate them,
intrinsically relied on camp- exponential search which, um,
no matter how much compute you have, you're never going to,
you know, um, win that race.
Um, they also have limited, you know, information,
and this is maybe a kind of a more subtle point that if I gave
you infinite compute and I asked you to translate,
I don't think you would be able to figure it out because it's not a computation problem.
You just need to learn the language and you need to
experience all the subtleties of language to be able to,
you know, translate [NOISE].
But on the other hand, AI wasn't solved,
but a lot of interesting,
um, contributions to computer science came out of it.
Lisp was- is- uh,
had a lot of ideas that
underlay ma- many of the high level programming languages we have,
garbage collection, um, time-sharing, allowing, uh,
multiple people to use the same- one computer at the same time,
which is something that,
uh, we kind of take for granted.
And also this paradigm of separating what you want to compute,
which is modeling, and how you do it,
which is inference, which we'll get to a little bit later.
Okay. So um, people forget quickly and, um,
in the '70s and '80s,
there was a renewed generation of people getting excited about AI again.
Um, and this time it was all about knowledge, right?
Knowledge is power and,
um, there were a lot of expert systems which were created.
And the idea is that if you could encode expert's knowledge about the world,
then you could do kind of amazing things,
and at the time the knowledge was encoded in generally a set of rules.
Um, and there were a lot of programs that was written,
and you'll notice that the, the scope is much narrower now.
The goal isn't to solve it- all of AI,
but to really focus on some choice and problems like
diagnosing the diseases or converting customer's order parts into parts,
and, uh- customer orders into parts and, uh,
this was the first time that AI, I think,
really had a real impact on industries.
So uh, people were actually able to make useful,
you know, products out of this.
And knowledge did actually play a key ingredient in curbing this,
you know, exponential growth that people were worried about.
But of course, um,
it didn't last long.
Um, knowledge as deterministic rules was simply
not rich enough to capture all the kind of nuances of the world.
It required a lot of manual effort to maintain and, um, again, um,
a pattern of over-promising and under-delivering that seems to plague, um,
AI people, led to the collapse of the field and the kind of a second AI winter.
Um, okay, so that's not the end of the story either.
But actually it's not kind of really the beginning either.
Um, so I'm going to step back further in time to 1943.
So what happened in 1943?
So there was, um,
a neuroscientist, McCulloch; and logician, Pitts,
who were wondering and marveling at how
the human brain is able to do all of these kind of complicated things.
And they wanted to kind of formulate a theory about how this could all happen.
So they developed a theory of,
um, artificial neural networks, um,
and this is kind of you can think about the root as of,
you know, deep learning in some sense.
Um, and what's interesting is that they looked at, um,
neurons and logic, which are
two things that you might not kind of necessarily associate with each other,
and showed how they were kind of connected mathematically.
And a lot of that early work in this era were of- around artificial neural networks,
was about studying them kinda from a mathematical perspective.
Um, because at that time, the compute wasn't there,
you couldn't really run any kind of training new models or um.
And then 1969, something interesting happened.
So there's this book by Minsky and Papert called Perceptrons.
And this book did a lot of mathematical analysis.
And it also showed that linear models, one of the results of many,
was showing that linear classifiers couldn't solve the XOR problem.
Um, the problem is- another way to think about the problem is basically given two inputs,
can you tell whether they are the same or not, or different.
And, um, so it's kind of not a-
shouldn't be a hard problem but linear classifiers can do it.
And for some reason,
which I don't quite understand,
it killed off neural nets research even though they
had said nothing about if you had a deeper network, what it could do.
Um, but it's often cited that this book, ah,
swung things from people who were interested in
neural networks to the field of AI being very symbolic and logic driven.
Um, but there was always this kinda minority group, um,
who were really invested in and believed in, um,
the power of neural networks,
and I think this was always just kind of a matter of time.
So in the '80s, there was a renewed interest.
Um, people kind of discovered or
rediscovered the backpropagation algorithm which allowed a kind of,
for a generic algorithm that could train these multilayer neural networks
because single layer remember was insufficient to do a lot of things.
And then one of the kind of the early success stories,
as Yann LeCun in 1989,
applied a convolutional neural network and
was able to recognize hand digit- written digits,
and this actually got deployed, um,
by the USPS and was reading kind of zip codes.
Um, so this was, you know, great, ah,
but it wasn't until this decade that the,
um, this area of neural networks really kind of took off,
um, under the moniker deep learning.
Um, and, you know,
AlexNet in 2012 was kind of a huge transformation, um,
where they show gains on the,
kind of ImageNet ba- benchmark and overnight transformed the computer vision community.
Um, AlphaGo as, you know, many of you know,
and many kind of other,
um, and there were kind of the rest is history.
Okay, so- so there's this kind of two intellectual traditions.
Um, you know, the name AI has always been
associated with the kind of John McCarthy logical tradition,
that's kind of where it started.
But, um, as you can see that there is also
kind of this neuroscience inspired tradition of AI,
and the two are kind of really had
some deep philosophical differences and over
the decades fought with each other kind of quite a bit.
But I want to pause for a moment and really think about,
[NOISE] maybe if there were actually kind of deeper connections here.
Remember McCulloch and Pitts,
they were studying artificial and neural networks,
but the connection was to logic, right?
So from even in the very beginning,
there is kind of this synergy that, you know,
some- some people can kind of often overlook.
And if you take a look at AlphaGo,
which [NOISE] if you think about the game of Go or many games,
it's a mathematically, you can write down the rules of Go in logic in just a few lines.
So it's a mathematically well-defined logical- logic puzzle in some sense.
But somehow, the- the power of neural networks
allows you to develop these models that actually play Go really- really well.
So this is kinda one of the deep mysteries that has, kind of,
uh, I think is kind of o- opens standard challenge, you know, in AI.
Um, as with any story it's not a full picture,
and I want to point out on this slide that,
AI has drawn from a lot of different, you know,
fields, many of the techniques that we're gonna look at,
for example, maximum likelihood,
came from your statistics or games came from economics, optimizations,
gradient descent, hence from- was, you know,
in the '50s completely unrelated to AI.
But these techniques kind of developed in a different context.
And so AI is kind of like,
you know, it's kind of like a New York City.
It's- it's like a melting pot where a lot of the- these
techniques that kind of unified and apply to kind of interesting problems.
And that's what makes it, I think really interesting
because of the- the new [NOISE] avenues
that are opened up by kind of unique combinations of, um, existing techniques.
Okay, so- so that was a really bre- brief history of,
you know, where- how we got here.
Um, now I want to pause for a moment and think about,
you know, what is- what is the goal?
What- what AI people are trying to do?
And again this- this is kind of there's two ways to think about
this which and- sometimes the conflation of these causes a lot of confusion.
Um, so I like to think about it as AI as agents,
and AI as tools.
So the first view asks the kind of standard question of,
how can we create or recreate intelligence?
And the second one asked, you know,
how can we use technology to kind of benefit, you know, society?
[NOISE] And these two are obviously very related and they have, ah,
a lot of shared technical,
um, overlap, but, you know,
philosophically they're kind of different.
So let me kind of explain this a little bit.
So the idea with AI agents is,
and this is, I think a lot of what,
um, um, gets associated with AI,
um, and especially as, you know, with science fiction.
That kind of, ah, po- portrayal certainly kind of
encourages this kinda view where [NOISE] you're human- we're human beings.
And what you do is you look in the mirror and you say,
wow, that's must- that's a really smart person.
And you think okay, how- how- what- what- what can humans do that is,
you know, so amazing.
Well, they can, um,
they can see and they can perceive the world, recognize objects.
Um, they can grasp cups and drink water and not spill it.
[NOISE] Um, they can communicate using language as I'm doing to you right now.
Um, we know facts about the world,
[NOISE] declarative knowledge such as what's the capital of
France and procedural knowledge like how to ride a bike.
We can reason with this knowledge and maybe ride a bike to the capital of France.
And then, really importantly,
we're not born with all of this, right?
We're born with basically nothing,
none of these capabilities,
but we are born with the capacity and
potential to acquire these over time through experience.
And learning it seems to be kind of this critical ingredient,
which drives a lot of the success in AI today but also with,
um, you, know, human intelligence it's clear that learning plays
such a central role in getting us to the level that we're operating at.
So each of these areas has kind of spawned entire sub-fields,
and people in it are kind of wondering about how you
can make artificial systems that have the language,
or the motor, or the visual perceptual capabilities that, you know, humans have.
But are we there yet?
Um, and I would- I would like to think that we are, ah, very far.
So if you look at the way that machines are, have been successful,
it's all with a narrow set of tasks and, you know,
millions or billions of examples,
and you just crunch a lot of computation,
and you can really kind of optimize,
um, every- any tasks that you're going to come-come up with.
Whereas humans operate in a very different regime.
[NOISE] They don't necessarily do any, you know,
one thing well, but they are have such a kind of
diverse set of, you know,
experiences, can solve a diverse set of tasks and
learn from each individual tasks from very few examples.
And still it's a kind of a grand challenge,
in from a, uh, cognitive perspective,
how you can build systems with this level of capability in that humans have.
So the other view is, you know, AI tools.
Basically we say okay well, you know,
it's kind of cool to think about how we can,
uh, you know, recreate intelligence.
But, you know, we don't really care about making more,
um, things like humans.
We already have a way of, you know,
doing that, that's called babies.
[LAUGHTER].
Um, so when instead what we'd really like to do is not
making something that's like a human but making systems that help humans.
Because, you know, after all,
we're- we're humans, I guess it's a little bit selfish but,
um, we're in charge right now.
Um, and- and a lot of this- this view and a lot of
the success stories in AI are really different from the things that you expect,
you know, this, uh, this humanoid robot to come into your house and be able to do.
For example this is a project from Stefano Ermon's group.
Um, there's a lot of poverty in the world and, um,
part of it is- is just kind of understanding
what's- what's going on and they had this idea of using,
uh, computer vision on satellite imagery to predict things like,
you know p-, uh, GDP.
Um, so this is obviously not a task that, you know,
the- our ancestors in Africa were like,
you know, getting really good at.
Um, but nonetheless it uses
convolutional neural networks which is a technique that was inspired by,
um, you know the brain and so that's- that's kind of interesting.
Um, you can also have another application for saving
energy by trying to figure out when to cool on datacenters.
Um, as AI, is, uh,
being deployed in more kind of mission critical s-, uh,
situations such as self-driving cars or authentication.
There are- there are a f- few th- new issues that come up.
So for example, there are- thi- this phenomenon called adversarial examples,
um, where you can take,
um, these cool-looking glasses,
you can put them on your face,
and you can fool the computer, um,
as- of- save our-
our face recognition system to think that you're actually, you know, someone else.
Um, or you can post these, uh,
s- stickers on stop signs and you'd get this,
uh- s- save our system to think that it's a,
um, a speed limit sign.
So there's obviously- there's- clearly these are, you know,
big problems if we think about that the widespread deploy- deployment of AI.
Um, there's also a less catastrophically but also p- pretty, um,
you know, upsetting which is, uh,
biases that you- many of you probably have read in the news about.
So for example, if you take Malay which is a language that, uh, doesn't distinguish,
um, in this writing form between he and she and you stick it into Google Translate.
Um, you see that she works as a nurse but he
works as a programmer, which is encoding certain,
uh, societal biases, um,
in the actual models.
And one kind of an important point I wanna bring up is that,
you know, it's -- it's how is machine learning and AI kinda working today?
Well, it's, um, you know, society exists.
Society is generating a lot of data.
We're training on this data,
and kind of trying to fit the data and try and mimic
what it's doing and then using predictions on it.
What could possibly go wrong, right?
Um, and so- so certainly people- a lot of people have been thinking about, um,
how these biases are kind of creeping up and is an open and active area of research.
Something a little bit more, uh,
kind of s- sensitive is, you know,
asking well, these systems are being deployed to
all these- all these people whether they kinda want it or- or want it or not.
Um, and this, uh, this actually touches on,
you know, people's, uh, you know, livelihoods.
It actually impacts people's lives in a serious way.
Um, so Northpointe was this company that developed a- a software called
COMPAS that tries to predict how risky,
um, criminal risk or how someone- how risky someone is essentially.
Um, and ProPublica this organization realized whoa, whoa, whoa, whoa.
You have this system that, uh,
given an individual didn't reoffend is actually, um,
more- twice as likely to classify blacks as incorrectly as, you know, non-blacks.
So this is, uh, seems pretty problematic.
And then Northpointe comes back and says actually,
you know, I think we- I think we're being fair.
Um, so given a risk score of 7, uh,
we were fair because 60% of whites reoffended and 60% of blacks reoffended.
Um, the- the point here is that there's- there's- there's actually no,
um, solution to this in some sense sadly.
Um, so people are finding or formulating different notions
of fairness and equality between,
um, how you predict or record it on different kind of, um, groups.
But, um, or you can have different notions of fairness
and which all seem reasonable from first principles but mathematically they can be,
um, incompatible with each other.
So this is- this is again an open area of
research where we're trying to figure out as a society how,
um, to deal with the schema that
machine learning might be using these in kind of critical situations.
Okay. So summary so far,
um, there's an agent's view.
Um, we're trying to really kind of dream and think about how do you get
these capabilities like learning from very few examples that humans have into,
you know, machines and a whole- maybe opening up a kind of
a- a different set of technical capabilities.
But at the same time,
and we really need to be thinking about how
these AI systems are affecting the real world.
And things like security,
and biases, and fairness all kind of show up.
It's also interesting to note that, you know,
a lot of the challenges in deployment of an
AI system don't really have necessarily to do with,
um, you know, humans at all.
I mean, humans are incredibly biased but that doesn't mean we want
to build systems kind of in our- in,
um, that mimic humans and kind of inherit all the kind of the flaws that humans have.
Okay. Any questions about this?
Maybe I'll pause for a moment.
So let's go on.
Um, so what I wanna do next
is give an overview of the different topics, um, in the course.
Um, and the way to think about all this is that,
um, in AI we're trying to solve really complex problems.
The real world is really complicated.
And- but at the end of the day we want to produce
some software or maybe some hardware that actually runs and does stuff, right?
And so there's a very considerable gap between these things.
And so how do you even approach something like self-driving cars or,
um, you know, d- diagnosing diseases?
You probably shouldn't just like go sit down at a terminal and start typing because then,
um, there- there's no kind of- no overarching structure.
So what this class is going to do is to give you one example of
a structure which will hopefully help you approach hard problems,
and think about how to solve them in a kind of more principled way.
Um, so this is a paradigm that I call the,
um, modeling inference and learning paradigm.
Um, so the idea here is that there's three pillars which I'll explain in a bit.
And, uh, we can focus on each one of these things kind of in turn.
So the first pillar is modeling. So what is modeling?
The modeling is taking the real world,
which is really complicated and building a model out of it. So what is a model?
Model is a simplification that is mathematically precise so that you can,
you know, do something with it,
uh, on a computer.
Um, one of the things that's necessary is that modeling, um,
necessarily has to simplify things and,
you know, throw away information.
Um, so one of the kind of,
uh, the, you know,
the art is to figure out
what information to pay attention to and what information to keep.
Um, so this is going to be important for example when you
work on your final projects and you have a real world problem,
you need to figure out, um,
you can't have everything and you have to figure out judiciously how to,
um, manage your- your resources.
So here's an example. If you want to for example build a- a system that can find, uh,
the best way to get from point A to point B in a graph- in a- in a city you can
formulate the model as a- a graph where nodes are points in the city,
and edges rep- represent ab- ability to go between these points with some sort of cost,
um, on the edges.
Okay. So now once you have your model you can do, uh, inference.
And what inference means is asking questions about your model.
So here's a model you can ask for example how- what is the shortest path from,
um, this point, uh, to this point.
Right. And that's because now your model land is a mathematically well-defined, uh,
problem now you can- it's within the realm of,
uh, you know, deve- developing algorithms to,
you know, solve that problem.
And most of the inference is ki- being able to
do these computations, um, really efficiently.
And finally learning addresses the problem,
where does this model come from?
So in any kind of realistic setting,
um, the model might have a lot of parameters.
Maybe it has, you know, millions of parameters and how do you
s- if it- if it- wants to be faithful to the,
you know, real world that how do you get all this, uh, information there.
Um, manually p- encoding this information turns out not to be a good idea.
This is, um, in some sense what,
um, AI from the '80s was trying to do.
Um, so the learning paradigm is as follows.
What we're gonna do is specify a model without parameters.
Think about it as a skeleton.
So in this case we have a graph but we don't know what the edge weights are.
Um, and now we have some data.
So maybe we have data of the form people tried
to go from X to Y and they took 10 minutes,
or an hour, or so on, um,
and then from this data we can learn to fit the parameters of the model.
We can assign, um,
costs to the edges that kind of are representative of what the data is telling us, okay?
So now in this way,
we can write down a model without parameters,
feed the data, apply a generic learning algorithm and get a model with parameters.
And now we can go back and do, um,
inference and ask questions, you know, about this.
Okay. So this is kind of the- the- the paradigm.
And I want to really emphasize that, you know,
learning is not- as I've presented is really not about
any one particular algorithm like nearest neighbors or neural networks.
It's really a kind of a philosophy of how you go about approaching problems by
defining a model and then not
having to specify all the details but filling them in later.
Okay. So here is the plan for the course.
We're gonna go from low-level intelligence to high-level intelligence;
and this is the intelligence of,
um, of the, of the models that we're gonna be talking about.
So first we're gonna talk about machine learning,
and like I've kind of alluded to earlier,
machine learning is going to be such a kind of
an important building block
of- that can be applied to any of the models that we kind of develop.
So the central tenet in machine learning is you have data and you go to model,
its main driver of a lot of su- successes in AI because it allows you to,
in software engineering terms,
move the complexity from code to data.
Rather than having, you know,
a million lines of code which is unmanageable,
you have a lot of data which is collected in kind of
a more natural way and a smaller amount of code that can
operate on this data and this paradigm has really been,
it's really been powerful.
One thing to think about in terms of machine learning is that it,
it is, requires a leap of faith, right.
So you can go through the mechanics of down- downloading some machine learning code
and you train them all but fundamentally it's about generalization, right.
You have your data,
you fit a model, uh,
but you don't care about how it performs on that data;
you care about how it performs on new experiences.
And that leap of faith is something that's, um,
I think gives machine learning its power but it's also a little bit,
um, at first glance perhaps magical.
Um, it turns out you can actually formalize a lot of this using,
um, probability theory and,
and statistics but that's kind of a topic for another time.
Okay. So after we talk about machine learning,
we're going to go back and talk about the,
the simplest of models, right.
So a reflex model is this.
So here's a quiz.
Okay. What is this animal? Okay, zebra.
How did you get it so fast?
Well, it's kind of a reflex, where your human visual system is so good,
um, at, at doing these things without thinking.
Um, and so reflex models are these, um,
are models which just require a fixed set of computations.
So examples like are linear classifiers,
deep neural networks, um,
and most of these models are the ones that people in machine learning um, use.
Models is almost synonymous with,
um, reflex on- in machine learning.
The important thing that there's no feed for it.
It just like you get your input bam,
bam, bam, and here's your output.
Okay, so that's, that's great because it's fast.
But there's some problems that require a little bit more than that.
Right. So for example here's another problem.
Okay, quick, white to move. Where does she go?
Okay, there's, there's probably like a few of you who are like chess geniuses,
um, but for the rest of us,
um, I have no idea.
I don't know, wait, who's moving again?
Um, so, so in these kind of situations,
we need something perhaps a little bit more powerful than a reflex.
We need agents that can kind of plan and think, um, ahead.
So the idea behind state-based models is that we model
the world as a set of states which capture any given situation like, uh,
a position in a,
in a game and actions that take us between states which correspond to things that,
um, you can do in the, in this game.
Um, so a lot of game applications fall in this as
category of robotics, motion planning, navigation.
Um, also some things that are might not be- you might think of,
um, planning as such as gen- you know, generation, um.
In natural language or generating an image,
um, you are, uh,
can be cast in this way as well.
So there's three types of state-based models each of which we'll cover in,
um, you know weeks of time.
So search problems are the classic, uh,
you control everything so you're just trying to fi- find the optimal path.
There are cases where there's randomness.
For example if you're trying to go from point A to point B,
maybe there's traffic that you don't,
you know, don't know about or, um,
in a game there might be dice that are- die which are rolled, and, uh,
there's a third category which are adversarial games which
is cases where your playing an opponent who's actively trying to destroy you.
So what are you gonna do about it?
Um, so one of the games that we're gonna,
uh, be talking about, uh,
when we talk about games is Pac-Man;
and one of the assignments is, um,
actually building, um, a Pac-Man agent such as this.
So, uh, while you're looking at this,
think about how- what are the states and what are the actions and how would you go
about you know devising a strategy for
Pac-Man to eat all the dots and avoid all the ghosts?
So that's something, uh,
to maybe look forward to.
There's also gonna be a competition.
So we'll see how- who ends up at the top.
Okay, so state-based models,
um, are very powerful and a value to kind of have foresight.
Um, but some problems are not really most naturally cast as state-based models.
For example, you know, how many of you play Sudoku or have played it before?
So as the goal of Sudoku is to fill in these, uh, um,
blanks with numbers so that, um,
every row and column and three-by-three sub-block has the digits 1 through 9.
So there's a bunch of constraints.
Um, and there's no kind of sense in which you have to do it in a certain order, right.
Whereas the, the order in how you move in chess or something is,
you know, pretty important.
Um, so, so these type of problems, uh,
are captured by these variable-based models where you kind of think about
a solution to the problem as
an assignment to the individual variables, under some constraints.
So constraint satisfaction problems,
we'll spent a week on that,
um, these are hard constraints.
For example two people can be- or
a person can't be in the two places at once for example.
Uh, there's also Bayesian networks which we'll talk about which
are variable-based models with, uh, soft dependencies.
For example if you're trying to track, um, you know,
a car over time,
these are the positions of the car.
These variables represent the position of the cars and these,
uh, E's represent, uh,
the- the sensor readings of the position of the car at that particular position
and inference looks like trying to figure out
where the car was given all this kind of noisy sensor reading.
So that's also gonna be another assignment where you're going to deal with.
Okay. So finally, um, now we get to high-level.
What's- so what is high-level intelligence here?
Um, and I put logic here, um,
for a reason that you'll see clear. Yeah, is there a question?
The Sudoku, can you explain why it's not a state-based model?
Yeah, so the question is why is not
the- why is the Sudoku problem not a state-based model?
Um, you can actually formulate this as a state-based model,
um, by just thinking about the sequence of, uh, assignments.
But it turns out that, um,
you can formulate in a kind of
more natural way as a variable-based model which allows you to,
uh, take advantage of some kind of more efficient algorithm to solve it.
Right, it's- think about these models as kind of different,
um, analogy as like a programming language.
So yes, you could write everything in you know
C++ but sometimes writing in you know, Python or,
or SQL for some things might be more- might be easier.
Yeah.
[inaudible] state based problem where you have both adversarial elements and an element of randomness?
Yeah, so the question is how do you categorize
state-based models where there is both randomness and an adversary?
Um, we're also gonna talk about those as well.
Um, and those would be- I,
I would classify them as adversarial but
there is also a random component that you have to deal with,
games like backgammon. Yeah, question.
[inaudible]
Yeah, so the question is about whether, uh,
some of these are more continuous and some of them are more discrete.
Uh, I don't necessarily think of, uh,
so a lot of the reflex models actually can work
in continuous state spaces, for example images.
Um, actually it's, it's almost a little bit of the opposite where, um,
the logic-based models are in some sense more, you know,
discrete but you can also have continuous elements,
you know, in there, um, as well.
Um, so in this class,
we're mostly going to focus on kind of discrete objects
because they're just going to be simpler to work with.
Okay, so what is this logic?
So the motivation here is that suppose you, um,
wanted a little companion who, um,
you could boss around and, um,
help or help you do things,
let's say; that's a better way to say it.
Um, so you'd like to be able to say okay,
you know, tell us some information, um,
and then later you wanna be able to ask
some questions and have the system be able to reply to you.
Um, so, um, you know how- how would you go about doing this?
One way you could think about is building a system that you
can actually talk to using natural language, okay.
So I'm actually going to show you a,
a little demo, um, which, uh,
is going to come up in the last assignment on logic;
um, and well, let's see what you think about it.
Uh, okay, so this is going to be a system that is, um,
based on logic that I'm going to,
um, tell the system a bunch of things and I'm going to ask some questions.
So, um, I want you all to follow along and you see if you can,
you know, play the role of the agents.
Okay. So I'm going to teach you a few things like, um,
Alice is a student, okay.
So it says I learned something.
Now let's, let's quiz,
um, is Alice a student?
Okay. Good. So that worked.
Um, is Bob a student?
What should the answer be? I don't know who's Bob. Um, okay.
So now let's do, um,
students are, uh, people.
Um, Alice is not a person.
I don't buy that [LAUGHTER] okay.
So, um, okay it's,
you know, it's doing some reasoning, right?
It's using logic, it's not, uh, just, um.
Okay. So now, let's do, um,
Alice is from Phoenix.
Phoenix is a hot city.
I know because I've lived there.
Um, cities are places,
and if it is snowing,
uh, it is, um, then it is cold.
Okay, got it. So, um,
is it snowing? I don't know.
Um, so how about this?
Okay. So if, um,
a person is from a hot place and it is cold,
then she is not happy, okay.
True. Right, um.
I guess those of you who have spent all your
live in California would maybe appreciate this.
But, um, okay, so ho- is it snowing now?
How many of you say yeah, it's snowing?
How many say no? You don't know? Okay.
[inaudible]
Ah, ah, [LAUGHTER] um, how about if I say Alice is, ah, happy.
Okay, so is it snowing now?
No, it should be no. Okay. So you,
you guys were able to do this.
Okay. So this is kind of an example of a interaction which, um,
if you think about it has is ve-
very different from where you would see kind of in a typical,
um, you know, ML system where you have to show it millions of
examples of one particular thing and it can do a kind of one task.
This is much more of a very open-ended set of,
um, I wish to say that the,
the experiences are super rich but they're definitely diverse.
I teach- I just give one statement.
I say it once and then all of a sudden it has all the ramifications
and kind of consequences that
built in and it kind of understands in a kind of a deeper level.
Of course this is based on,
you know, logic systems.
Um, so it is brittle but this is kind of just a proof of concept
to give you a taste of what I mean when I say logic.
So, ah, these systems need to be able to digest
this heterogeneous information and reason deeply with that information.
And we'll see kind of how, um,
logic systems can do that.
Okay. So that completes the tour of the topics of this class.
Um, now I want to spend a little bit of time on course logistics.
Uh, so I wanna- all the details here are online.
So I'm not going to be complete in my coverage, um,
but I just wanna give you a general sense of what's going on here.
Okay. So what are we trying to do in this course?
Um, so prerequisites, um,
there's programming, um, discrete math and, ah, probability.
So you need t be able to code and you need to be able to, um,
do some math and,
uh, some kind of basic proofs.
Right? So these are the classes that are, um,
required or at least recommended that you-
or if you have some equivalent experience that's, you know, fine too.
Um, and what we- what should you hope to get out of this course?
Right. So one had- the course is meant to be giving
you a set of tools using the modeling inference learning paradigm.
It gives you a set of tools and a way of thinking about problems that hopefully will
be really useful for you when you go
out in the world and try to solve real world problems.
Um, and also by- as a side product I also want all of you
to be more proficient at
your math and programming because those are kind of the core elements that,
ah, enable you to do kind of interesting,
you know, things in AI.
So a lot of AI and you, you read about it,
it's very flashy but really the foundations are still,
um, just you know math and programming in some sense.
Okay. So the coursework is homeworks,
exam, and a project.
That's what you have to do, um,
Homeworks, there's eight homeworks.
Each homework is a mix of writing- written and programming problems centered on
a particular application covering one particular type of model essentially.
Um, like I mentioned before there's a competition for extra credit.
There's also some extra credit problems in the, in the homeworks,
um, and when you submit code,
we're gonna run- we have an auto-grader that runs.
It's gonna run on all the test cases but you get a feedback of only a subset.
So you can, um,
it's like, you know, in machine learning,
you have a train set, and you have a test set.
So don't train on your test set.
[LAUGHTER] Okay.
So um, the exam is,
ah, testing your ability to use the knowledge that you learn to solve new problems.
Right. So there's, um,
I think it's worth taking a look at exam because this,
this kind of surprises people every- the exam is
a little bit different than the types of problems that you see on,
on the homework and there are kind of more problem, you know, solving.
So the exam isn't going to be like a multiple choice like,
okay, you know, um,
you know, when was Perceptrons published or something like that.
It's gonna be, here's a real life problem.
How do you model it and how do you come up with a solution?
Um, they're all going to be written.
It's closed book except for you have a one page of
notes and this is a great opportunity to actually,
um, review all the material and actually learn the ah,
the content in the class.
Um, so the project I think is a,
a really good opportunity to take
all the things that we've been talking about in the class and,
um, try to find something you really care about and try to apply it.
Work in groups of three and I really recommend finding a group early,
um, and as I emphasize it's your responsibility to find, you know, a good group.
Right? Um, don't come to us later like one week before the project deadline and say,
"Oh, you know, my group members they,
um, they ditched me," or something.
We really try to,
try to nail this down use Piazza to- or your other social networks to find a good group.
So throughout the quarter there's going to be these milestones for the projects.
So, um, to prevent you guys from procrastinating into the very end, um,
so there's gonna be a proposal where you try and brainstorm some ideas, progress report,
a poster session which is actually a whole week before the final report is due,
um, and the project is very open.
So this can be, um,
really liberating but also might be a little bit daunting.
Um, we will hopefully give you a lot of structure in terms of saying okay,
how do you define your task?
How do you implement different,
um, baselines or oracles?
Which I'll explain later.
How do you evaluate? How do you,
um, analyze what you've done?
And each of you will- each project group will be assigned a CA mentor,
ah, to help you, ah,
through the process and you're always welcome to come to
my office hours or Dorsa's, or any of the CAs to get additional, um,
help either brainstorming or figuring out what the next step is.
Ah, some policies, ah,
all assignments will be submitted on Gradescope,
um, there are seven total late days
you can use, and most two per assignment. After that there's no credit.
Um, ah, we're gonna use Piazza for all communication so don't email us directly.
Leave a post on Piazza.
If- I encourage you to make it public if it's,
it's not sensitive, but if it's, you know,
personal, then obviously make it private,
um, and try to help each other.
We'll actually award some extra credit for students who help answer,
um, other student's questions.
So all of the details are on the course website.
Okay. So one last thing and it's really important and that's the Honor Code.
Okay. So especially if you're,
um, you know, you've probably heard this if you've been at Stanford.
If you haven't, then I wanna really kind of make this clear.
So I encourage you all to have- collaborate, discuss together.
But when you- when it comes to actually the homeworks,
you have to write up your homework and code it independently.
So you shouldn't be looking at someone's writeup.
You shouldn't be looking at their code.
Um, and you definitely shouldn't be copying code off of GitHub.
Um, um, that's hopefully should be,
you know, obvious and maybe less obvious,
you should not- please do not post your homework assignments on GitHub.
I know you're probably proud of the fact that your Pac-Man agent is doing really well
but please don't post on GitHub because then that's going to be our Honor Code violation.
Um, when debugging, um,
with- if you're working together,
it's fine to as long as
it's kind of looking at input-output behavior so you can say to your partner,
"Hey, I put in this,
um, input to my test case and I'm getting a 3.
What are you getting?" So that's fine but you can't.
Remember don't look at each other's code.
Um, and to enforce this, we're gonna be running MOSS,
which is a software program that looks for code duplication, um, to,
to make sure that,
ah, the rules are being followed and,
you know, changing one variable name is- or you'll be so- anyway enough said.
[LAUGHTER] Just don't, don't, don't do that.
Okay? Any questions about this?
I wanna make sure this is important or about any of those logistics. Yeah.
[inaudible]
The final project, ah,
you can put on GitHub.
Yeah.
Yeah.
Yeah, private GitHub repos,
uh, is fine. Yeah, question in the back?
Is it necessary to have a group or can you do a solo project?
Uh, the question is can you,
can you do a solo project?
You can do a solo project,
you can do a project with two people,
or you can do a project with three.
I would encourage you to try to work in, uh,
groups of three because you'll be able to do more as a group,
and there is definitely,
uh, you know, it, it, it's not like if you do
a solo project we'll be expecting like one third of the, the work.
So okay.
Anything else? All right.
Okay. So in the fi- final section,
I want to actually delve into s- some technical details.
Um, and one thing we're going to focus on right now is, um,
the, kind of inference and learning components of, of this course.
So I'm going to talk about how you can
approach these through the lens of, you know, optimization.
So this is going to be,
uh, it might be a review for some of you but hopefully,
it's gonna be a, a good,
um, you know, way to get everyone on the same page.
Okay. So what is optimization?
There's two flavors of optimization that we care about.
There's, uh, Discrete Optimization,
where you're trying to find the best, uh, discrete object.
For example, you're trying to find the best, uh,
path or the path P that minimizes the cost of that path.
Um, we're going to talk about one algorithmic tool, um,
based on Dynamic Programming which is a very powerful way of solving these,
um, complex optimization problems.
Um, and the key, you know,
property here is that the set of paths is huge and you can't just,
uh, trial them and compute the cost and choose the best one.
So you gonna have to choose something clever.
The second brand of optimization is continuous optimization and formally this is just
finding the best of vector of
real numbers that satisfies or minimizes some objective function.
So a typical place this shows up is in learning where you define, uh,
objective function like the training error and you're trying to
find a weight vector W. So this notation just means it's a list of numbers,
D numbers that minimizes the training error.
And we're going to show that gradient descent is, uh, uh,
easy and a surprisingly effective way of solving these,
um, continuous optimization problems.
Okay. So to introduce these two ideas,
I'm going to look at two, um,
problems and trying to kind of work through them.
So this might be also a good, um, you know,
way to think about how you might go approach a,
you know, homework problems.
And I'll try to kind of talk you through this, um, in a bit more detail.
Okay, so the first problem is,
um, you know, computing edit distance.
Um, and this might not look,
you know, like an AI problem, but a lot of, ah,
AI problems have this as kind of a, you know,
building block if you wanted to do some sort of matching between,
um, you know, two words or two, um, biological sequences.
So the input is you're given two strings.
Um, we're gonna start writing over here on the board just to work this out.
So given two strings, um,
S and T. Um,
so for example, um,
a cat and um, the cats.
Okay. So these are two strings and you wanna find the minimum number of edits that is
needed to take transform S into T. And by edits I mean you can insert,
um, a character like you can insert S,
you can delete characters,
I can delete this A and you can substitute one character for another.
So you can replace this A with a T. Okay.
Um, so here's some examples.
What's the edit distance of cat and cat?
It's 0, you don't have to do anything.
Cat and dog is 3,
cat and at is 1,
you insert the A or insert a C. Um,
cat and cat is 1, um,
and a cat and the cats is 4.
Okay. So the challenge here is that there are,
ah, quite a different number of ways to insert and delete.
Right, so if you have a string of- that's very
long there's just way too many things to like just try out all of them.
Okay, so then, how do we,
how do we go about,
um, coming up with a solution?
So any ideas? Yeah.
[inaudible] simplify the output in terms of saying that
the substitution tells us we considered [inaudible] deletion peoples who considered
a substitution or vice-versa by saying like an empty character.
Yeah, yeah. So let's try to simplify [NOISE] the,
the, the problem a bit.
And building up on your what you, um, what was said.
So, um, one thing to note is that okay,
where so the general principle,
let me just write the general principle,
um, is to, you know,
reduce the problem to
a simpler problem because then you can hopefully solve- it is easier to solve,
and then you can maybe keep on doing that until you get something that's trivial.
Okay. So there's maybe two observations we can make.
One is that well,
we're technically saying we can, um, you know,
insert into S right but if we insert into S,
it makes the problem kind of larger in some sense, right?
I mean that's not, that's not good.
That's not reducing the problem.
But, but whenever we insert into S, um,
we probably want to insert things which are in
T. We wanna like cancel something out, right?
So we wouldn't insert a K there for any reason.
We probably wanna insert a S in which case no S
matches that and then we've reduced that problem, right?
So we can actually think about, you know,
inserting into S to S as equivalent to kind of deleting from,
um, from T. Okay, does that make sense?
All right. So another observation we can make is that,
you know, we can start inserting anywhere.
We can start inserting here and then jump over here and to this.
But this just introduces a lot of, um,
you know, ways of doing it which all kind of result in the same answer.
So why don't we just start more systematically at
one end and then just proceed and try to chisel-off the problem,
um, kind of let's say from the end.
Okay, so start at the end?
Okay, so, so now we have this problem and to draw a problem in a little box here.
Um, so let's start at the end. Yeah, question.
What's the reasoning used to reach that principle start at the end?
[NOISE].
[NOISE] the question is why are we starting at the end as oppo- well,
the idea is that if you start at the end then you
have kind of a more systematic and consistent way of,
you know, reducing the problem.
So you don't have to think about all the permutations of where I can delete and substitute.
Why is it more systematic to go from
the right to the left than from the left to the right?
We can also do it left to right.
So the end or the start is both fine.
This is just- I just picked the end. Yeah.
Are we not starting at the end and then give us the optimal strategy?
Yeah, the question is how do we know that starting,
um, at one end can give you the optimal strategy?
Um, so, you know,
if you wanted to prove this more rigorously there's some work but,
um, I'll just try to give you a,
you know, an intuitive answer.
Um, suppose you didn't start at the end,
and you just made a sequence of steps like I insert here,
I delete here, and then I went over here and um,
did all those operations to S. I
could have equivalently also just sorted those by, you know,
where it was happening and then just proceeded from
one end to the other, and I would arrive at the exact same answer.
So without loss of generality,
I can start at that. Any other questions?
Okay. So yeah.
Instead of doing this wouldn't the more viable [NOISE] approach be that
trying to recognize some patterns instead of doing this.
I think between the two strings "s" and "t" like some form of- some sort of
[NOISE] pattern [inaudible] string.
Yeah. So the question is, maybe you can recognize some patterns.
Uh, it's like okay, oh, cat.
That's- that's- maybe those should be lined up.
Um, I guess these examples are chosen so that these patterns exist,
but we want to solve the problem for cases where,
um, the pattern might not be obvious.
So it could be- we want to work it for- it to work for all strings.
Maybe there is no pattern,
and we still would want to- kind of an efficient algorithm to do it. Yeah.
Can't we just like use dynamic programming?
Like we go one by one,
there was always like [inaudible] -
Yeah.
Either we're doing, um, substitution,
or, um, otherwise it's like the same character. Or we have to insert-
Yeah.
- um, and then we keep going,
and you just like [NOISE] remember
each like to- to strings that we have at one point-
Uh-huh.
-so that if we calculated that we don't have to do it again.
Yeah. Yeah.
That's it. Yeah.
Yeah. Yeah. Great idea.
Let's do dynamic programming.
Um, so that's what I'm kind of trying to build up from- uh, build up to.
Okay so, um, so if you look at this- so dynamic programming is a kind of
a general technique that essentially allows you
to express this more complicated problem in terms of a simpler problem.
Uh, so let's start with this problem.
If we start at the end,
um, if the two match then,
well we can just immediately, um, you know,
delete these two and that's- it's gonna be the same, right?
So we can get- we are gonna get some free rides there.
Okay, but when they differ,
um, now we have many options.
So what we could- what could we do?
Well, we could, um, um, you know substitute.
Okay, we can change the "t" to an "s".
So what does that leave us with?
So I can do a cat,
[NOISE] "t" is the- the cat,
the- [NOISE] Okay, so I can substitute.
[NOISE] Um, [NOISE] okay.
Um, what else can I do?
[NOISE] Someone say something I can do.
[NOISE] So I can insert,
um, insert where into-
[OVERLAPPING]
So I can insert an "s", right?
Yes. [NOISE]
But that's the same as, you know,
[inaudible] deleting from "t".
So by, uh- you can basically also just delete this "s".
Um, so this is our cat,
[NOISE] and I deleted this "s" from "t".
Okay, so this is,
um, let's call it, uh, you know,
um, I guess let's call this insertion- it's technically insertion [NOISE].
And then finally what can I do?
[NOISE] I can also remove "t".
So [NOISE] a, ca, the, cats.
Okay, so this is delete.
[NOISE] And right now you're probably looking at this like,
well, obviously, you know,
you sho- you should do this one.
But in general it's hard to tell.
What if I just give you some arbitrary strings,
you know, who knows what the right answer is.
Um, so in general how do you pick? Yeah.
In the second one, the "t" is supposed to be for cats.
[NOISE] [inaudible]
You mean this one?
Yeah.
So here I inserted an "s", right?
But then because there's two s's here,
I just canceled them out and [NOISE] what was left [inaudible]
So you can think about this as really deleting from-
What if I'm considering [NOISE]
[inaudible] Like in the original problem you said we're transferring "s" to "t".
Yeah. Yeah. Yeah. So, um, um,
because of this I'm kind of trying to re-frame the [NOISE] problem a little bit. Okay,
so which one should I choose? Yeah.
What about the substitution the other way?
Um, the substitution the other way meaning change-
"s" to "t".
Sorry there's too many s's and t's here which
[LAUGHTER] is going to be a bit unfortunate.
And then replace the last s in cats with "t".
Oh, you could-
How do we eliminate that [inaudible] [NOISE]
Um, that's- you can think about that as kind of equivalent.
So, if you identify two letters that you want to make the same,
then [NOISE] you could- you can replace the one to be the other,
or the other to be that.
I mean if- officially we've been kind of framing it as we're
only editing "s" which is the reason that it's asymmetric.
[NOISE] Okay, so which one of these?
Door "a" door "b" or door "c"? Yeah.
Would you look [inaudible] between "s" and "t" for
every step [NOISE] [inaudible] because there's "cat" in both of them?
Yeah, so you could try to look inside but,
um, but remember these are- might be really complicated.
So you- we wanna kind of a simple mechanized procedure to tell.
[NOISE] What about the next letter?
The next letter.
"t" [inaudible]
Um, yeah let's- let's pretend these are-
you- you can't see inside them. Okay. [LAUGHTER].
Keep going with each of the different cases.
Yeah, okay, so let's keep on going.
[NOISE] So, I'm not going to draw everything,
but you can also try to break this down into- maybe there's three actions here,
and three actions here. All right.
Um, and at the end of the day you hopefully have a problem that's simple enough,
that, um, where "s" equals "t' or something then you're done.
Um, but then, you know,
how- how do I- how do I know?
Suppose I've solved this.
Suppose if someone just told you, okay,
I know this cost,
I know this cost, I know this cost. What- what should you do?
[inaudible]
Yeah, you should take the minimum, right?
Like remember we want to minimize the edit distance.
So, um, there's three things you can do.
Each of them has some costs of doing that action which is, you know, one.
Every edit is the same cost.
And then there's a cost of,
you know, continuing to do whatever you're doing.
And so we're just gonna take the minimum over those. Yeah.
[inaudible] How do we know that that's,
like- that's the maximum amount of distance that we have to take?
Yeah, so I was trying to argue that, um,
with- if you're going to right to left,
it's, uh, without loss of generality.
Because if you've- went left to right,
or in some other order,
you can also replay the edits, um, in order.
[inaudible] [NOISE] one letter
that you needed one assertion like [inaudible] like upstream.
But if you went from like the left it looks like as if you're [inaudible]. [NOISE]
Yeah.
[inaudible] Okay.
Yeah. I think it works.
[NOISE] Um, okay, so- so let's, um,
try to code this up and see if we can make this program work.
Okay, so, um, I'm gonna do editDistance.
Can everyone see this?
Okay, so, um, so I'm gonna define a function that takes two strings,
and then I'm going to um, define a recurrence.
So, recurrences are- are,
I guess, one word I haven't really used,
but this is really the way you should th- kind of think about, uh,
dynamic programs, and this idea of taking complex problems and breaking it down.
It's gonna show up, in you know,
search problems, MDPs, and, you know, games.
So, I guess it's something that you should really be comfortable with.
So, let's um, define recurrence, uh, as follows.
Um, so remember at any point in time,
I have, uh, let's say a sub problem,
and since I'm going right to left,
I'm only considering the first,
um, "m" letters of "s" and the first letter "n" letters of "t".
Okay, so recurse is going to return the minimum edit distance between two things,
the first "m" letters of "s",
and the first "n" letters of "t".
Um, I'm gonna post this online so you guys don't have to,
like, copy- try to copy this.
Um, okay, so, um,
okay, suppose I'm gonna- I'm gonna define this function.
Uh, if I have this function what should I return?
Recurse of-.
[inaudible]
So "m" is an integer, right?
So "n" is an integer,
so I'm going to return the length of "m" and the length of "n".
Okay, so that's kind of, uh,
the initial state.
[OVERLAPPING]
Sorry. Yup. Okay. Um, All right.
So now you need to fill out this function.
Okay, so let's- let's um,
consider a bunch of cases.
So here's some easy cases.
Suppose that, um, "m" is zero, right?
So I have- comparing an empty string with something that has "n" letters.
So, what should the cost of that be?
[NOISE] I heard some mumbling.
[OVERLAPPING].
It should be "n" [NOISE] and symmetrically if "n" is 0 then result should be "m",
um, and then if now we come to the kind of
initial case that we consider which is the end [NOISE] match a match.
So, if "s" um,
the last letter of "m",
you know, this is 0-based indexing.
Um, so that's why there's a minus 1.
So, this matches.
[NOISE] Then what should I do?
[NOISE] So, now we reduce this to a sub problem, right?
[inaudible]
So, I have "m" minus 1 and "n" minus 1.
Okay. And now comes the fun case which we looked at.
So there's- um, in this case the last letter doesn't match.
So, I'm gonna to have to do some sort of edit, can't just let it slide. Yeah. Question.
Would you- do you need a full "s" to "t" compare
or "s" through "m" and then "t" through "n" to compare?
Worse than doing a full s, a compare.
[OVERLAPPING] rather than waiting until, um, first-
Yeah.
-stream at the last slide than that.
There- there's probably a way you can make this more efficient.
I'm just gonna try to get the basic thing in there.
Okay. So substitution.
Okay. So what's a cost of a substitution?
I pay 1 to do the substitution,
but and in- as a reward I get to, um,
reduce the problem to n minus 1 and n minus 1, right?
So I lop off a letter from s and I lop off a letter from t. So what else can I do?
So I can, um, you know, delete.
[NOISE] So that also costs 1.
And when I delete,
I delete from s and then n. So this remains the same.
And then now you can think about the insertion,
um, is n minus 1, right?
Because remember insertion into s is deletion from t,
that's why this is n minus 1.
Okay. And then the result is just gonna be a minimum of,
uh, all these things. Okay. Return result.
Okay. So just, uh,
and then, how do I call this function?
Um, a cat, the cats.
[NOISE] So let me print out the answer.
Um, let's see if it works.
Okay. Print out 4.
Therefore, I conclude it works now.
[LAUGHTER] I mean if you were doing this, uh,
you would probably want to test it some more,
but in the interest of the time,
I'll kind of move on.
So let me just kinda refresh.
Okay. So I'm computing this at a distance between
two strings and we're gonna define a recurrence that works on sub problems,
where the sub problem is the first m letters of s and the first n letters of
t. And the reason I'm using integers instead of,
um, strings is to avoid like string copying,
um, implementation detail, but it doesn't really matter.
Um, so base cases.
So you wanna reduce your problem to a case where it's- it's trivial to solve.
Um, and then we have the last letter matches.
And then we have a letter doesn't match and you have to pay some sort of cost.
I don't know which action to take.
So I'm gonna take them, you know, minimum of all of them.
And then I call it by just calling, you know, recurse.
Okay. So this is great, right?
So now I have a working thing.
[NOISE] Um, let's try another test case.
So I'm gonna make this.
Um, so if I do times 10,
this, uh, basically, uh,
replicates this string 10 times.
So it's a- it's a long string-longer string.
[NOISE] Okay.
So now I'm gonna run it.
[OVERLAPPING] Maybe I shouldn't wait for this.
Is there a base case?
Um, there is a base case, I- I think that it expanded-
it's- what- what's wrong with this code?
Very slow.
Um, yes, it's very slow. Why is it slow?
[BACKGROUND] Yeah, right?
So- so I'm recursing.
[NOISE] Every point recurses three times.
So you kind of get this exponential, you know, blob.
Um, so there's kind of a- how do you solve this problem?
[BACKGROUND] Yeah.
You can memo I think I heard the word memoize,
which is another way to kind of think about.
Memorize plus, um, I guess,
recurrences is dynamic programming, I guess.
Um, so I'm gonna show you kind of this,
um, way to do it which is pretty, uh, uninvasive.
Um, and generally I recommend people.
Well, get the slow version working [NOISE] and then try to make it faster.
Don't try to be, you know,
too slick at once.
Okay. So I'm gonna make this cache, right?
And I'm gonna say if m,
n is in the cache,
then I'm gonna return whatever's in the cache.
So cache is just a dictionary mapping.
Um, the key which is, um,
identification of the problem I'm interested in solving,
and the result which is the answer that I computed.
So if I already computed it, I don't need a computer again, just return it.
And then at the end, if I have to compute it,
then, um, I have to put this in the cache.
[NOISE] Okay?
So three lines or four lines, I guess.
Yeah. [BACKGROUND] [NOISE] Yeah.
That's a great point.
Uh, this should be outside of the recurse object.
Yeah. Glad you guys are paying attention.
Um, otherwise, yeah, it would do basically nothing.
Any other mistakes? [LAUGHTER] Yeah.
Um, there is also function decorators that like implement memoizing for you.
In this class, are you okay if we use that or would you
rather us like make our own in this case?
Um, you can use the deco- you can be fancy if you want.
Okay.
Um, yeah. But- but I think this is,
you know, pretty transparent.
Easy for learning purposes.
Okay. So let's run this.
So now it runs instantaneously as opposed
to- I actually don't know how long it would have taken otherwise.
Okay. And sanity check for t is
probably the right answer because there's
four was the original answer and multiply by 10.
Okay. any other questions about this?
[NOISE] So this is an example of,
you know, kind of basic, uh,
dynamic programming which are, uh,
you'd solve a problem trying to formulate it as
a recurrence of a complicated problem in terms of smaller problems.
Um, and like I said before this is gonna kind of show up,
um, um, over and over again in this class.
Yeah. [BACKGROUND] Yeah.
So the question is why does this reduce, uh, redundancy.
[NOISE] Is that right?
Um, so maybe I can do it kinda pictorially.
Um, if you think about, let's say, you have a,
um, a problem here, right?
And this gets, um,
you know, reduced to,
um, um, I'm just making kind of a arbitrary, um, diagram here.
So this problem gets reduced to these two.
And this problem gets reduced to these two, um,
and- and so on, um, right?
So if you think about- if you didn't have memoization,
you will just be paying for the number of paths.
Every path is a kind of you have to compute from scratch.
Whereas, if you do memoization,
you pay the number of nodes here,
which a lot of this has shared like here.
Um, you know, once you compute this,
no matter if you're coming from here or here,
you're kind of using the same value.
Okay. So let's- let's move on.
So the second problem, um,
we're gonna talk about is,
uh, has to do with continuous optimization.
[NOISE] And the motivating question here is how do you do, um, regression?
Which is a kind of a bread and butter of,
um, you know, machine learning here.
[NOISE]
So here we go.
Regression. Okay. So imagine you get some points.
Okay, so I give you a point which is 2, 4.
Then I give you another point,
let's say 4, 2.
And so these are data points, you want to, let's say,
predict housing price from,
you know, square footage or something like that.
You want to predict health score from,
um, your blood pressure and some other things.
So this is pretty common in machine learning.
And the question is how do you fit a line?
I'm going to consider the case where
your line has to go through the origin, just for simplicity.
Um, so you might want to like find, you know, a fit.
Two points is maybe kind of a little bit degenerate,
but that's the simple example we are going to work with.
In general you have lots of points and you want this to fit the line that best kind of,
uh, is close to the points.
Okay, so how do you do this?
So there's a principle called least squares, which says, well,
if you give me a line which is given in this case by a slope w,
I'm going to tell you how bad this is.
And badness is measured by looking at all the training points,
and looking at these distances.
Right. So here I have, you know,
this particular, uh, a particular,
let's say point x_i.
If I hit it with a w,
then I get, basically the,
uh, you know, the y-intercept here,
not the y-intercept but the- like the y value here.
That's my prediction. The real value was y_i,
which is, you know, up here.
And so if I look at the difference,
I want that difference to be zero.
Right. So in, in least squares, I square this,
and I say, I want this to be as small as possible, right.
Now, this is only for one point.
So I'm going to look at all the points.
Let's suppose I have n points,
and that's a function that I'm going to call f of w,
which basically says, for a given weight vector,
which is a slope, give me a number that characterizes how bad of a fit, um, this is.
Where 0 means that I fit everything perfectly,
and large numbers mean that I fit poorly.
Okay? All right.
So, so that's your regression.
So how do I solve a regression problem?
So how do I optimize this?
Can you do this in your head?
So if I actually had these two points, what should w be?
Okay, it doesn't matter. We'll, we'll compute it.
So how do we go about doing this?
So one principle, which is maybe another general takeaway is,
abstract away the details.
Right. Um, this is also true with the dynamic programming, but sometimes, you know,
you get- if you're too close to the board, and you're looking at,
oh man, these, these points are here and I need to fit this line.
How do I do that? You kind of get kind of a little bit stuck.
Why don't we think about this f,
as say some function?
I don't, I don't really care what it is.
And let's plot this function.
Okay. So now this is a different plot.
Now, this is, ah, the weight,
and this is f of w. [NOISE] Always label your axes.
And let's say this function looks like this.
Okay. So which means that for this slope,
I pay this, you know, amount,
for this slope, I pay this amount and, and so on.
And what I want to do, I want to minimize f of w,
which means, I want to find,
um, the w which,
um, has the least value of f of w, right?
Question? Okay. So you take the derivative.
So what is the derivative giving you?
It tells you where to move, right?
So if you look over here,
so you can- in general,
you might not be able to get there directly,
in this actually particular case you can because you can solve it in closed form,
but I'm going to try to be more general.
Um, so you start here.
This, this derivative tells you,
well, the function is decreasing if you move to the right.
So then you should move to the right.
Whereas over here, if you end up over here,
the derivative says, the function is decreasing as we move to the left.
So you should move to the left, right?
So what I'm going to introduce is this,
uh, algorithm called gradient descent.
It's a very simple algorithm.
It basically says, start with some place,
and then compute the derivative,
and just follow your nose.
Right? If the derivative says it's negative,
then just go this way.
And now you're on a new point,
you compute the derivative again,
you descend, and now you compute it again.
And then maybe you compute the derivative and it
says keep on going this way and maybe you overshoot, and then you come back.
And then, you know, hopefully you'll end up at the minimum.
Okay. So let's try to see what this looks like in code.
So gradient descent is one of the simplest algorithms,
but it really underlies
essentially all the algorithms that you people use in machine learning.
So let's do points.
We have two points here.
Um, and I'm going to define, um, some functions.
Okay, so f of w, so what is this function?
So I'm going to sum over all the different,
um, you know, and basically at this point it's converting math into Python.
So I'm going to look at all the points.
So for every x, y,
what the model predicts is w times x minus y.
And if I square that,
that's going to be the error that I get on that point.
Then, if I sum over all these errors then I get my objective function.
Okay. Array of- so yeah.
So you can put array here if you want,
but it doesn't matter.
It's, it's actually fine.
Okay. So now I need to compute the derivative.
So how do you compute the derivative?
So if your calculus is a little bit rusty,
you might want to brush up on it. So what's the derivative?
Re- remember we're taking the derivative with respect to w, right?
There's a lot of symbols here.
Always remember what you're taking derivative with respect to.
Okay. The derivative of the sum is the sum of the derivative.
So now I need to take the derivative of this.
Right. And what's the derivative of this?
Something squared, um, you bring the two down here,
and now you multiply by the derivative of this.
And what's the derivative of this?
Should be x. Right? Because this is a- y, this is a constant,
and w derivative- w times x with respect to w is x. Okay. So that's it.
Okay, so now let's do gradient descent.
Let's initialize with w equal 0.
Then I'm going to just, um,
you know, iterate a hundred times.
Normally, you would set some sort of stopping condition,
but let's just keep it simple for now.
Okay, so for every moment, I'm going to- I have a w,
I can compute the value of the function,
and also take the gradient of the derivative.
Gradient just means derivative in higher di- dimensions,
which we'll want later.
Um, okay. And then, what do I do?
I take, uh, w,
and I subtract the, the gradient.
Okay. So remember- okay, I'll be out of here.
Okay. So, uh, I take the gradient.
Remember I want to have the gradient.
Uh, gradient tells me where the function is increasing,
so I want to move in the opposite direction.
And eta is just going to be this, uh,
step size to, um,
keeping things under control.
We'll talk more about that next time.
Okay, so now, I want to do, print out what's going on here.
So iteration, print out the function, and t value.
Okay. All right, so let's compute the gradient.
And, um, so you can see that the iteration, we first start out with w equal 0.
Then it moves to 0.3, then it moves to
0.79999999 and then it looks like it's converging into 0.8.
And meanwhile, the function value is going down from 20
to 7.2 which happens to be the optimal answer.
So the correct answer here is 0.8.
Okay, so that's it.
Next time we're going to keep,
uh, we're going to start on the machine learning lecture.
 Okay. So let's, uh,
get started with the actual, ah, technical content.
So remember from last time,
we gave an overview of the class.
We talked about different types of models that we're gonna explore: reflex models,
state-based models, variable based models,
and logic models which we'll see throughout the course.
But underlying all of this is, is, you know, machine learning.
Because machine learning is what allows you to take data and, um,
tune the parameters of the model,
so you don't have to, ah, work as hard designing the model.
Um, so in this lecture,
I'm gonna start with the simplest of the models,
the reflex based models, um,
and show how machine learning can be applied to these type of models.
And throughout the class, ah,
we're going to talk about different types of models
and how learning will help with those as well.
So there's gonna be three parts,
we're gonna talk about linear predictors, um,
which includes classification regression, um,
loss minimization which is basically stating an objective function of how you, ah,
want to train your machine learning model,
and then stochastic gradient descent,
which is an algorithm that allows you to actually,
ah, do the work.
So let's start with, ah, perhaps the most,
um, cliched example of,
uh, you know, machine learning.
So you have- we wanted to do spam classification.
So the input is x,
um, an email message.
Um, and you wanna know whether an email message is spam or not spam.
Um, so we're gonna denote the output of the classifier to be Y which is in this case,
either spam or not spam.
And our goal is to, ah,
produce a predictor F, right?
So a predictor in general is going to be a-
a function that maps some input x to some output y.
In this case, it's gonna take an email message and
map it to whether the email message is spam or not.
Okay. So there- there's many types of prediction problems, um,
binary classification is the simplest one where the output is one of two,
um, possibilities either yes or no.
And we're gonna usually denote this as plus 1 or minus 1,
sometimes you'll also see 1 and 0.
Um, there's regression where you're trying to predict a numerical value,
for example, let's say housing price.
Um, there's a multi-class classification where Y is, ah,
not just two items but possibly, um, 100 items,
maybe cat, dog, truck,
tree, and different kind of image categories.
Um, there's ranking where the output,
um, is a permutation of the input, this can be useful.
For example, if the input is a set of, um, articles,
or products, or webpages,
and you want to rank them in some order to show to a user.
Um, structured prediction is where Y,
ah, the output is an object that is much more complicated.
Um, perhaps, it's a whole sentence or even an image.
And it's something that you have to kind of construct,
you have to build this thing from scratch,
it's not just a labeling.
Um, and there's many more types of prediction problems.
Um, but underlying all of this,
you know, whenever someone says I'm gonna do machine learning.
The first question you should ask is, okay what's the data?
Because without data, there's no learning.
So we're gonna call an example.
Um, x, y pair is something that specifies
what the output should be when the input is x, okay?
And a training data or a set of examples,
the training set is going to be simply a list or a multiset of, er, examples.
So you can think about this as a partial specification of behavior.
So remember, we're trying to design a system
that has certain- certain types of behaviors,
and we're gonna show you examples of what that sum should do.
If I have some email message that has CS221
then it's not spam but if it has,
um, lots of, ah, dollar signs then there might,
um, um, be spam.
Um, and, ah- so remember this is not a false specification behavior.
These, ah, ten examples or even a million examples
might not tell you what exactly this function is supposed to do.
It's just examples of, ah,
what the function could do on those particular examples.
Okay. So once you have this data,
so we're gonna use D_train to denote, ah, the data set.
Remember, it's a set of input output pairs.
Um, we're going to,
ah, push this into a learning algorithm or a learner.
And what is the learning algorithm is gonna produce?
It's gonna produce a predictor.
So predictors are F and the predictor remember is what?
It's actually itself a function that, um,
takes an input x and maps it to an output y.
Okay? So there's kind of two levels here.
And you can understand this in terms of the,
uh, modeling inferences of a learning paradigm.
So modeling is about the question of what
should the types of predictors after you should consider are.
Ah, inference is about how do you compute y given x?
And learning is about how you take
data and produce a predictor so that you can do inference?
Okay. Any questions about this so far?
[NOISE]
So this is pretty high level and abstract and generic right now,
and this is kinda, kind of on purpose because I wanna highlight how, um,
general machine learning is before going into the specifics of,
uh, linear predictors, right?
So this is an abstract framework.
Okay. So let's dig in a little bit to this actual,
um, an actual problem.
Um, so just to simplify,
ah, the email problem,
let's, eh, consider the task of, um,
predicting whether a string is an email address or not.
Okay. Um, so the input is an em-,
ah, is a string and, ah,
the output is- it's a binary classification problem,
it's either 1 if it's an email or minus 1 if it's not, that's what you want.
Um, um, so the first step of,
um, doing linear prediction is,
um, known as feature extraction.
And the question you should ask yourself is,
what properties of the input x might be relevant for predicting the output y?
Right, so I say, I really highlighted might be, right?
At this point, you're not trying to encode the actual set of rules that solves a problem,
that would involve no learning,
and that would just be trying to do it directly.
But instead of- for learning you're kind of taking a,
um, you know, a more of a backseat and you're saying,
"Well, here are some hints that could help you."
Okay. Ah, so formally,
a feature extractor takes an input and outputs a set of feature name,
feature value pairs, right?
So I'll go through an example here.
So if I have abc@gmail.com,
what are the properties that might be useful for determining
whether a string is an email address or not?
Well, you might consider the length of the string,
if it's greater than 10,
maybe long strings are less likely to be email addresses than shorter ones.
Um, and here, the feature name is length greater than 10.
So that's just kind of a label of that feature,
and the value of that feature is 1,
ah, representing it's true.
So it will be 0, if it's false.
Here's another feature, the fraction of alphanumeric characters, right?
So that happens to be 0.85 which is the number.
Um, there might be features that test for a particular,
um, you know, letters for example,
that it doesn't contain an "at" sign or that has a, you know,
feature value of 1 because there is an "at" sign,
endsWith.com is one, endsWith.org is a 0 because that's not true.
So, um, and there you could have many,
many more features, ah,
and we'll talk more about features on next time.
But the point is that you have a set of properties,
you're kind of distilling down this input which is could be a string,
or could be an image, or could be something more complicated into kind of a
um, you know, ground-up fashion that later,
we'll see how a machine learning algorithm can take advantage of.
Okay. So you have this, ah,
feature vector which has- is a list of
feature values and their associated names or labels.
Okay. But later, we'll see that the- the names don't matter to the learning algorithm.
So actually, what you should also think about
the feature vector is simply a list of numbers,
and just kind of on the side make a note that all this, you know.
position number three corresponds to contains "@" and so on.
Right, so I've distilled the-
the email address abc@gmail.com into the list of numbers 0- or 1,
0.85, 1, 1, 0.
Okay. So that's feature extraction.
It's kind of distilling complex objects into lists of numbers which we'll
see is what the kind of the lingua franca of these machine learning algorithms is.
Okay. So I'm gonna write some concepts on a board.
There's gonna be a bunch of, um,
concepts I'm going to introduce,
and I'll just keep them up on the board for reference.
So feature vector is again an important notion and it's denoted Phi,
um, of x on input.
So Phi itself- sometimes, you think about it, er,
you call it the feature map which takes an input and returns, um, a vector,
and this notation means that returns in general,
ah, d-dimensional vector, so a list of d numbers.
And, um, the components of this feature vector we can write down as Phi_1,
Phi_2, all the way to Phi_d of x.
Okay. So this notation is,
eh, you know, convenient, um,
because we're gonna start shifting our focus from thinking about
the features as properties of input to features as kind of mathematical objects.
So in particular, Phi of x is a point in a high-dimensional space.
So if you had two features,
that would be a point in two-dimensional space,
but in general, you might have a million features,
so that's a feature, ah,
it's a point enough, a hundred- ah,
uh, million dimensional space.
So, you know, it might be hard to think about that space, but well,
we'll see how we can, you know,
deal with that in a later in a, in a bit.
Okay. So- so that's a feature vector,
you take an input and return a list of numbers.
Okay. Um, and now,
the second piece is a weight vector.
So let me write down a weight vector.
[NOISE]
So a weight vector is going to be noted W.
Um, and this is also,
uh, a list of D numbers.
It's a point in a D-dimensional space
but we're gonna interpret it differently, as we'll see later.
Okay. So- so a way to think about a weight vector is that,
for each feature J.
So for example, frac of Alpha, um,
we're gonna have a real number WJ,
that represents the contribution of that feature to the prediction.
So this contribution is 0.6.
So what does this 0.6 mean?
So, so the way to think about this is that you have
your weight vector and you have a feature vector of a particular input,
and you want- the score of, uh, your prediction
is going to be, uh, the dot product between the weight vector and the feature vector.
Okay. So um, that's written W dot a phi of X um, which is um,
written out as basically,
looking at all the features and multiplying the feature of the value
times the weight of that feature and summing up all those numbers.
So for this example,
it will be minus 1.2,
that's the weight of the first feature,
times 1, that's the feature value,
plus 0.6 times 0.85 and so on.
And then, you get this number of 4.51 which is- happens to
be the score for this example.
Question?
So the feature extraction which is phi of X, is that, uh,
supposed to be like an automated process or is it a part of
manual extraction classification procedures?
Yeah. So the question is,
is the feature extraction manual or automatic?
So uh, phi is going to be implemented as a function like encode, right.
Um, you're going to write this function manually.
But you know, the function itself is run automatically on examples.
Um, later we'll see how you can actually learn features as well.
So you can slowly start to do less
of a manual effort but uh, we're going to hold off until,
next time for that.
Question?
So we're talking about weight gaining
and I know that in certain tests of regressions like,
uh, the weights being, uh, a percentage change,
[inaudible] weights to percentage change of the
outcome it doesn't, it doesn't mean the sphere?
Yeah. So the question is about interpretation of weights.
Sometimes weights can have a more precise meaning.
In general, um, you can,
you can try to read the tea leaves but it I don't think there is maybe, uh,
in general a mathematically precise thing
you can say about the meaning of individual weights.
But intuitively, and the intuition is important,
is that you should think about each feature as you know,
a little person that's going to make a vote on this prediction, right?
So you're voting either plus, yay or nay?
And the weight of a particular feature is-
specifies both the direction level whether- if positive weight means that,
um, that little person, um,
is voting positive and negative weight means, that it's voting negative.
The magnitude of that weight,
is how strongly that little person feels about the prediction, right?
So, you know, contains add as three because maybe like
"@" signs generally do occur in
email addresses but you know the fraction of alphanumeric characters,
it's you know, less.
So at that level,
you can have some intuition but the precise numbers and y is 0.6 versus 0.5.
Um, that's, um, you can't really say much about that. Yeah. Another question?
Does, uh, [inaudible] [NOISE] is it the same dot product for deeper networks.
They can feel like more weight vectors afterwards.
It's still like it's, like just more than one products. [NOISE]
So right now we're focusing on linear classifier.
So the question is what happens if you have a neural net with more layers?
Um, there's gonna be more dot products but there's
also goin- it's not just adding more features.
There's gonna be other uh, components which we'll get to in a later lecture.
Yeah?
Do the weights have to add up to a certain number or
how do you normalize it, so the weights, like you have to change the score value
[inaudible] .
Yeah. So the question is,
do the weights have to add up to something? Short answer is.
No. There's obviously restricted settings,
where you might want to normalize the weights or something but we're not gonna,
you know, uh, consider that right now.
Later, we'll see that the magnitude of weight does tell you, you know, something.
Okay, so, so just to summarize it's important to note that the weight vectors,
there's only one weight vector, right, you have to find
one set of parameters for every- everybody.
But the feature vector is per example.
So for every input, you get a new feature vector and
the dot product of those two weighted combination of features is the uh, is the score.
Okay, so, so now let's try to put the pieces together and define,
um, uh, of the actual predictor.
All right, so remember we had this box with f in it,
which takes x and returns y.
So what is inside that box?
Um, and I'm hopefully giving you some intuition.
Let me go to a board and write, uh, a few more things.
So the score, uh,
remember is w dot phi of x.
And this is just gonna be a number, um, and uh, the predictor.
So linear predictor actually let me call this linear.
To be more precise, it's a linear classifier not just a predictor.
Classifier is just a predictor that does classification.
Um, so a linear classifier
um, denoted f of w.
So f is where we're going to use, you know, predictors.
W just means that this predictor depends on a particular set of weights.
And this predictor is, uh,
going to look at the score and return the sign of that score.
So what is the sign?
The sign looks at the score and says, is it a positive of a number?
if it's positive then we're gonna return plus 1.
If it's a negative number, I'm gonna return minus 1.
And if it's 0 then you know, I don't care.
You can return plus 1 if you want, it doesn't matter.
Um, so what this is doing the remember the score is either, is a real number.
So it's either gonna be kind of leaning towards um, you know,
large value, large positive values or leaning towards,
uh, s- large small- negative values.
And the sign basically says, okay you gotta commit are you- which side are you on?
Are you on the positive side or you on the negative side?
And just kind of discretizes it.
That's what the sign does.
Okay.
Okay, so, so let's look at a simple example
because I think a lot of what I've seen before is kind of more the,
uh, formal machinery behind and the math behind how it works but it's
really useful to have some geometric intuition because then you can draw some pictures.
Okay, so let's consider this, uh, case.
So we have a weight vector which is 2, 1,
2 minus 1, and a feature vector which is a 2, 0,
and another feature vector which is x0, 2 and 2, 4.
Okay. So there's only two dimensions so I can try to draw them on a board.
So let's try to do that.
Okay, so here is a two-dimensional plot.
Um, and let's draw the fea- the weight vector first.
Okay so the weight vector is going to be at 2 minus 1.
Okay. So that's this point.
And the way to think about the weight vector is not the point.
Um, but actually um,
the, the, the vector going from the origin to that point
for reasons that will become clear later.
Okay so that's the, that's the weight.
Okay. Um and then what about the other points so we have 2, 0, 0, 2.
So 2, 0 is here,
0, 2 is here and 2, 4 is, uh, here.
Right? Okay, so we have three points here.
Okay, so, um, how do I think about what this weight vectors is, is doing?
So just for just for reference remember the classifier is looking
at the sign of W dot, uh, phi of x.
Okay. Um, so let's try to do uh, classification on these three points.
Okay so w is um,
let me write it out formally, so 2, 1.
Um, and this is 0, 2.
So what's the score when I do W dot phi of x here? It's 4, right?
Because this is um, uh,
2, 0, 0, 2 um, 2, 4.
So this is just a dot product that's 4,
um, and take the sign what's the sign of 4?
One.
Okay. So that means I'm going to label this point as a positive, right?
Positive point, okay what about 0, 2?
Actually, sorry, this is just be a minus 1, right?
Okay. This is 2, minus 1.
Okay, so if I take the dot product between this,
I get minus 2 and then the sign of minus 2 is,
is minus 1, okay, so that's a minus.
Um, and what about this one?
So what's the dot product there? It's gonna be 0.
Okay. So, um, so this classifier will classify this point as a positive.
This is a negative and this one I don't know.
Okay. So we can fill in more points.
Um, but, but, you know,
does anyone see kind of um, maybe a more general pattern?
I don't wanna have to fill in the entire board with classifications.
Yeah?
Orthogonal, everything to the right of it is positive and to the left of it is negative.
Yeah so so let's try to draw the orthogonal.
Uh, this needs to go through that line.
Okay, [NOISE] okay, so let's draw the orthogonal.
So this is a right angle. Okay.
And, ah, what that gentleman said is that,
the points- any point over here because it has acute angle width w,
is going to be classified as positive.
So all of this stuff is um, you know,
positive, positive, positive, positive, positive,
and everything over here because it's an obtuse angle with w is going to be negative,
so everything over here is negative.
And then, everything on this line is going to be 0.
Okay? So, so I don't know.
Okay, and this line is called, um,
the decision boundary, which is the concept not just for linear classifiers,
but whenever you have any sort of classifier
the decision boundary is the separation between
the regions of the space where the classification is positive versus negative.
Okay? And in this case, um,
it's, it's separate because uh, we have linear classifiers,
the decision boundary is straight,
and we're just separating the, the space into two halves.
Um, if you were in three-dimensions, um,
this vector would still be just a you know vector,
but this decision, um,
boundary would be a plane.
So you can think about it as you know coming out of the board if you want,
but I'm not gonna try to draw that.
Um, and that's, that's kind of the geometric interpretation of how linear classifiers,
ah, you know, work here. Question, yeah?
It seems like your weight could be any values here.
Right?
Yeah.
So we have one last [inaudible].
Yeah.
[inaudible] .
Yeah. So that's a good point. So the, the observation is that,
no matter, if you scale this weight by 2,
it's actually gonna still have the same decision boundaries.
So the magnitude of the weight doesn't matter it's the direction that matters.
Um, so this is true for just making a prediction.
Um, when we look at learning, ah,
the magnitude of the weight will matter because we're going to,
you know, consider other more nuanced loss functions.
Yeah. Okay. So let's move on.
Any questions about linear predictors?
So, so, far what we've done is,
we haven't done any learning.
Right. If you've ah, you know, noticed,
we've just simply defined the set of predictors that we're interested in.
So we have a feature vector,
we have weight vectors, multiply them together, get a score and
then you can send them through a sign function and you get these linear classifiers.
Right. There, there's no specification of data yet.
Okay. So now, let's actually turn to do some learning.
So remember this framework,
learning needs to take some data and return a predictor and our predictors are ah,
specified by a weight vector.
So you can equivalently think about the learning algorithm as
outputting a weight vector if you want for linear classifiers.
Um, and let's unpack the learner.
So the learning algorithm is going to be based on optimization which we started ah,
reviewing last lecture um,
which separates ah, what you want to compute from how you want to compute it.
So we're going to first define an optimization problem which specifies
what properties we want a- a classifier to have in terms of the data,
and then we're going to figure out how to actually optimize this.
[NOISE] And this module is actually really really powerful um,
and it allows people to go ahead and work on different types of
criteria for and different types of models
separately from the people who actually develop general purpose algorithms.
Um, and this has served kind of the field of machinery quite well.
Okay. So let's start with an optimization problem.
So this is an important concept um,
called a loss function and
this is a super general idea that's using the machine learning and statistics.
So a loss function takes a particular example x, y and a weight vector, um,
and returns a number and this number represents
how unhappy we would be if we used the predictor
given by W to make a prediction on x when the correct output is y.
Okay. So it's a little bit of a mouthful but, um,
this basically is trying to characterize, you know,
if you handed me a classifier,
and I go on to this example and try to classify it,
is it gonna get it right or is it gonna get it wrong?
So high loss is bad ah,
you don't wanna lose and
low loss is good.
So normally, zero loss is the- the best you can then hope for.
Okay. So let's do figure out the loss function for binary classification here.
Um, so just some notation,
the correct label is, ah,
denoted y and, um,
the predicted label remember is um, the score, ah,
sent through the sign function and that's going to give you some particular label.
Um, and let's look at this example.
So w equals 2 minus 1 phi of x equals ah,
2, 0 and y equals minus 1.
Okay. So we already defined the score as, um,
one example is a w dot phi of x which is,
um, how co- confident we're predicting minu- plus 1.
That's the way to, uh,
you know, interpret this.
Okay. So um, what's the score of this,
for this particular example again?
It's 4.
Right. Um, which means I'm kind of,
kinda positive that it's ah,
you know, a plus 1. Yeah. Question?
Ah, I was wondering, is the loss function generally 1-dimensional or,
or the output of the loss function?
Yeah. So the- the question is whether the output of
loss function is usually a single number or not.
Um, in most cases it is for
basically all practical cases you should
think about the loss functions outputting a single number.
The inputs can be, you know,
a crazy high-dimensional. Yeah.
Why is it not 1-dimension?
[NOISE] Um, there are cases where you might have
multiple objectives that you're trying to optimize at once ah,
but in this class it's always gonna be, you know, 1-dimensional.
Like maybe you care about, you know,
both time and space or accuracy but robustness or something.
Sometimes you have multi-objective optimization.
But that's way beyond the scope of this class.
Okay. So we have a score.
Um, and now we're gonna define a margin.
So let me, um.
Okay. So let's, let's actually do this.
So we're talking about classification.
I'm gonna sneak regression in a bit.
So score is w dot phi of x.
This is how confident we are about plus 1, um,
and the margin is the score ah, times y.
Um, and this relies on y being plus 1 or minus 1.
So this might seem a little bit mysterious but let's try to,
you know, decipher that, um here.
Um, so in this example,
the score is 4.
So what's the margin?
You multiply by minus 1.
So the margin is, ah, minus 4.
Right. And the margins interpretation is how correct we are.
Right. So imagine the correct answer is ah,
if, if the score in the margin had the same sign,
then you're gonna get positive numbers and then the,
the confident, the more confident you are then the more correct you are.
Um, but if y is minus 1 and the score is positive,
then the margin is gonna be negative which means that
you're gonna be confidently wrong um, which is bad.
[LAUGHTER]
Okay. So just to to see if we kind of understand what's going on.
Um, so when is a binary classifier making a mistake on a given example.
Um, so I'm gonna ask for a kind of a show of hands.
How many people think it's, it's when the margin is, uh, less than 0.
Okay. I guess we can kind of stop there.
[LAUGHTER]
I used to do these online quizzes where it
was anonymous but we're not doing that this year.
Okay. So yes, the margin is less than 0.
Um, when the margin is less than 0 that means y and
the score are different signs which means that you're making a mistake.
[NOISE]
Okay. So now we have the notion of a margin.
Let's define ah, something called the
zero-one loss and it's called zero-one because it returns either a 0 or a 1.
Okay. Very creative name.
Um, so the loss function is simply,
did you make a mistake or not?
Okay. So this notation let's try to decipher a bit.
So if f of x here is the prediction when the input is x,
um, and not equal y is saying, did you make a mistake?
So that's, think about it as a Boolean,
and this one bracket is um, just notation.
It's called an indicator function that takes a condition and returns either a 1 or 0.
So if ah, if the,
the condition is true,
then it's gonna return a 1 and if the condition is false, it returns a 0.
Okay. So all this is doing is basically returning a 1,
if you made a mistake and 0,
if you didn't make a mistake.
Okay. And we can write that as follows.
We can write that as um,
the margin less or equal to 0.
Right. Because pre- on the previous side of the margin is less than or equal to 0,
then we've made a mistake and we should incur ah,
a loss of 1 and if the margin is greater than 0,
then we didn't make a mistake and we should incur a loss of 0.
Okay. All right so, um,
it will be useful to draw these loss functions,
um, pictorially like this.
Okay, so on the axi- x-axis here,
we're going to show the margin, right?
Remember the margin is how, uh, correct you are.
And on the, uh,
y-axis we're gonna show the-
the loss function which is how much you're gonna suffer for it.
Okay, so remember the margin,
if the margin is positive,
that means you're getting it right which means that the loss is 0.
But if the margin is less than 0,
that means you are getting it wrong and the loss is 1.
Okay, so this is a 0-1 loss.
That's, uh, thi- this thing- the visual that
you should have in mind when you think about zero-one loss. Yeah.
[NOISE] Like less than 0
because we are not defining the event actually 0 [inaudible] classified as correct.
Yeah, so there is this kind of boundary condition of when ex- what happens exactly
at 0 that I'm trying to sweep under the rug because it's not, um, terribly important.
Um, here, it's less we go to 0 to be kind of on the safe side.
So if you don't know you're also,
uh, gonna get it wrong.
Um, otherwise you could always
just return 0 and then you, that, you don't want that.
Okay. So is it- uh,
any questions about, uh,
kind of binary classification so far.
So we've set up these linear predictors and I've defined
the 0-1 loss as a way to capture, um,
how unhappy we would be if we had a classifier that was,
ah, operating on a particular data point x, y.
So, um, just to-
I'm gonna go on a little bit of a digression and talk about linear regression.
Um, uh, um, [NOISE]
and, and the reason I'm doing this is that
loss minimization is such a powerful and general framework,
and it go- transcends, you know,
all of these, you know,
linear classifiers, regression, setups.
So I want to kind of emphasize over- the overall story.
So I'm gonna give you a bunch of different examples, um, classification,
linear regression side-by-side so we can actually see how they compare and hopefully,
their- the common denominator will kind of emerge more, um, clearly from that.
Okay, so we talked a little bit about linear regression in the last lecture, right?
So linear regression in some sense is simpler than
classification because if you have a linear,
uh, uh, predictor, um,
and you get the score w dot phi of x,
it's already a real number.
So in linear regression,
you simply return that real number and you call that your prediction.
Okay? Okay so now we- let's move towards defining our loss function.
Um, so there's gonna be, uh,
a concept that's gonna be useful,
it's called the residual, um, which is,
as- against kind of trying to capture how,
uh, wrong you are.
Um, so here is a particular linear, uh, predictor, um,
linear regresser, um, and it's making predictions all along,
you know, for different values of x.
Um, and here's a data point of Phi of xy.
Okay? So the residual is the difference between,
um, the true value y and the predictor value y.
Okay, um, and in particular it's the amount by which,
um, the prediction is overshooting the, you know, target.
Okay, so this is- this is a difference.
Um, and if you square the [NOISE] difference you get something called,
uh, the squared loss.
[NOISE]
So this is something we mentioned last lecture.
Um, residual can be either negative or [NOISE] positive.
Um, but errors, either,
if you're very positive or very negative,
that's bad and squaring them makes it so that you're gonna, you know,
suffer equally for, um,
errors in both, you know, directions.
Okay, so the square loss is the residual squared.
So let's do this kind of simple example.
So here we have our weight vector 2 minus 1.
The feature vector is 2, 0. What's the score?
It's 4, y is minus 1.
So, uh, the residual is 4 minus minus 1 which is 5 and,
uh, 5 squared is 25.
So the squared loss on this particular example is 25.
Okay, so let's plot this.
So just like we did it for a 0-1 loss.
Let's see what this loss function looks like.
So the, the horizontal axis here
instead of being the margin is going to be this quantity,
uh, for regression called the residual.
Um, it's going to be the difference between the prediction and the, the true target.
And I'm gonna plot the loss function.
Um, and this loss function is just,
you know, the squared function, right?
So with- if the residual is 0,
then the loss is 0.
If as a residual grows in either direction,
then I'm going to pay,
uh, something for it.
And it's a quadratic penalty which means that,
um, it actually grows,
you know, uh, pretty fast.
So if I'm, you know,
the residual is 10 then I'm paying 100.
Okay, so, so that's the squared loss.
Um, there's also another loss.
I'll throw in here,
um, called the absolute deviation loss.
And this might actually be the last thought, if you didn't know about regression you might,
uh, immediately come to.
It's basically the absolute difference between the prediction and,
um, the, the actual true target.
[NOISE] Um, turns out the squared loss.
The- there's a kind of a longer discussion about,
you know, which loss function,
um, you know, makes sense.
The- the salient points here are that
the absolute deviation loss is kind it has this kink here.
Um, and so it's not smooth.
Sometimes it makes it harder to optimize,
um, but the squared loss also has this kind of thing that blows up,
which means that it's, uh, uh,
it really doesn't like having outliers or, uh,
really large values because it's gonna,
you- you're gonna pay a lot for it.
Um, but at this level,
just think about this as, you know, different losses.
There's also something called a Huber loss which kind of, uh, um,
combines both of these, is smooth,
and also grows linearly instead of quadratically.
Um, okay, so we have both classification and regression.
We can define margins and residuals.
We get either, uh,
different loss functions out of it.
Right? Um, and now we want to minimize the loss.
Okay? Um, so it turns out that for one example and this is really easy, right?
So if I- if I told you,
okay, how do I minimize the loss here?
Well, okay, it's 0. Done. [NOISE] So that- that's not super interesting.
And this corresponds to the fact that,
you know, if you have a classifier,
you're just trying to fit one point, um, it's really not that hard.
So that's kind of not the point.
[NOISE] The point of machine learning is that you have to fit all of them.
Remember, you only get one weight vector,
you have all of these examples,
you have a million examples.
And you want to find one weight vector that kind of balances,
uh, errors across all of them.
And in general, you might not be able to achieve loss of 0, right?
So tough luck . Life is hard.
Ah, so you have to make trade-offs, you know,
which examples are you going to kind of sacrifice for the good of other examples.
And this is kind of actually a lot of where, you know,
issues around fairness of machine learning actually come in
because in cases where you can't actually make a prediction that's,
you know, equally good for everyone.
You know, how do you actually, you know,
responsibly make these trade-offs.
Um, but, you know,
that's a- that's a broader topic.
Let's just focus on trade-off defined by the simple sum over all the loss examples.
So lets just say we want to minimize the average loss over all the examples.
Okay, so once we have these loss functions,
if you average [NOISE] over the training set,
you get something which we're gonna call a train loss.
Um, and that's a function of W. Right?
So loss is on a particular example.
Train loss is on the entire data set.
[NOISE]
Okay. So any questions about this, uh, so far?
Okay. So there is this, uh,
discussion about which regression loss to use, which I'm gonna skip.
Um, you can feel free to read it in the notes if you're interested.
The punchline is that if you want things that look like the mean square loss,
if you want things that look like the median,
use the absolute deviation loss.
Um, but I'll skip that for now. Yeah?
[inaudible] regression like this.
Uh, when do people start thinking of regressions like in terms of loss minimization?
Yeah.
Uh, so regression has,
Least Squares Regression is from like the early 1800s.
Um, so it's been around for is- you know, kind of,
you can call it the first machine learning that was ever done,
um, if you- if you want, um,
I guess the loss minimization framework is,
um, it's hard to kind of pinpoint a particular point in time, you know,
it's kind of not a terribly, uh,
um, er, er, you know, it's not like,
uh, um, you know,
innovation in some sense.
It's just more of a- at least right now it's kind of a pedagogical tool to organize,
um, all the different methods that exist. Yeah.
Say I'm training on mean and median.
Do you mean that like, uh, in that particular training, training set,
the median would be the [NOISE] highest accuracy and the most confident,
whereas like with, uh,
loss [inaudible] deviation would be the median instead of the mean?
Yeah. So, um, I don't wanna get into these examples but, uh, bri- briefly,
if you have three points that you- you can't exactly f- fit perfectly,
um, you- if you use absolute deviation,
then you're gonna find the median value.
You're gonna basically predict the median value.
And if you use the square loss,
you're gonna predict the mean value.
But, um, I'm happy to talk offline [NOISE] if- if you want.
[NOISE]
Okay. So what we've talked about so far is we have
these wonderful linear predictors which are driven by feature vectors and weight vectors,
and now we can define a bunch of different loss functions that capture,
you know, how we care about,
um, you know, regression and classification.
And now let's try to actually do some real, uh, machine learning.
How, how do you actually optimize these objectives?
So remember the learner is going, uh,
so now we've talked about the optimization problem which is minimizing the training loss.
Um, we'll come back to that next lecture.
Um, and then now we're gonna talk about optimization algorithm.
Okay? So what is a optimization problem?
Now, remember last time we said, okay,
let's just abstract away from the details a little bit.
Let's not worry about if it's,
uh, the square loss or s- you know,
some other loss. [NOISE]
Um, let's just think about as a kind of abstract function.
So one-dimension, the training loss might look something like this.
You have a single weight and for
each weight you have a number which is your loss on your training samples.
[NOISE] Okay? And you want to find this point.
So in two dimensions,
um, it looks something like this.
Yeah. Let me try and actually draw this because I think it'll,
[NOISE] uh, be, um, useful
in a bit to solve, let me pull this up. [NOISE]
Okay. So in two dimensions,
um, what optimization looks like is as follows.
So I'm gonna- I'm now plotting,
um, W_1 and W_2 which are the two components of this two-dimensional weight vector.
For every point I have a weight vector and
that value is gonna be the loss, the training loss.
Um, and it's, er, you know,
[NOISE] it's pretty standard in these settings to draw what are called level curves.
Um, so let's do this.
So each curve here is a ring of points where,
uh, the function value is identical.
So if you, uh, look at terrain maps, those are level curves.
So you know, kind of what I'm talking about.
So this is the minimum and as you kind of grow out you get larger and larger, um.
Okay. I'll keep on doing this for a little bit.
Okay. [NOISE] All right.
Um. [NOISE] And, uh, the goal is to find the minimum.
Okay. All right.
So how are we gonna do this?
So yeah, question.
Assuming that there is a single minimum.
Yeah, why am I assuming,
uh, there is a single minimum.
[NOISE] in general for arbitrary loss functions,
there is not necessary a single minimum,
I'm just doing this for simplicity.
It turns out to be true for, um,
you know, uh, many of these linear classifiers.
[NOISE]
Okay. So last time we talked about gradient descent, right?
And the idea behind gradient descent is that well, I don't know where this is.
So let's just start at 0,
[NOISE] as good as any place.
And what I'm gonna do at 0 is I'm gonna compute the gradient.
So the gradient is this vector that's,
uh, perpendicular to the level curves.
So the gradient is gonna point in this direction.
That says, hey, in this direction is where
the function is increasing the most dramatically.
Um, and gradient descent says,
um, takes- goes in the opposite direction, right?
Because remember we wanna minimize loss.
Um, so I'm gonna go here.
And, um, now I'll hopefully reduce my, uh,
function value, not necessarily but,
um, we hope that's- that's the case.
Now, we compute, uh, the gradient [NOISE] again.
The gradient says, um,
you know, maybe it's pointing this way.
So I go in that direction and maybe now it's,
uh, pointing this way.
And I keep on going.
Um, this is a little bit made up.
Um, but hopefully, eventually I get to the,
um, the [NOISE] origin.
And you know, I'm, I'm kind of simplifying things quite a bit here.
So in- there's a whole field of optimization that studies exactly what kind of
functions you can optimize and how gradient descent when it works and when it doesn't.
Um, I'm just gonna kind of go through the mechanics now and defer
the kind of the formal proofs of when this actually works until, um, later.
Okay. So that's kind of the- the schema of how gradient descent works.
So in code this looks like this.
So initialize at 0 and then loop in some number of iterations,
um, which let's- for simplicity just think there's a fixed number of iterations.
And then, I'm gonna pick up my weights,
compute the gradient, move in the opposite direction,
and then there's gonna be a step size that, uh,
tells me how fast I want to,
you know, make progress.
Okay? And we'll come back to,
you know, uh, what,
uh, the step size, uh, does later.
Okay. So let's specialize it to a least squares, uh, regression.
So we kind of did this last week,
but, um, just to kind of review, um.
So the training loss for least squares regression is this.
So remember it's an average over the loss of individual examples,
and the loss of a particular example is the residual squared.
So that's this expression.
Um, and then all we have to do is compute the gradient.
And you know, if you remember your calculus,
it's just I've used the chain rule.
So this two comes down here.
You have the, um, you know,
the residual times the derivative of what's inside
here and the gradient with respect to W is, uh, phi of x.
Okay. So last time we did this in Python in 1-dimension.
So 1-dimension, and hopefully all of you should feel comfortable doing
this because this is just kind of basic, um, calculus.
Um, here we have w is a vector.
So, uh, we're not taking derivatives but we're taking gradients.
Um, so there's, you know,
some things to be, uh,
wary of but in this case it's often kind of useful to double-check that.
Well, um, the gradient version actually matches, uh,
the, the single-dimensional version
as well because last time remember we have the x out here.
Um, and one thing to note here is that,
um, there's a prediction minus target, and that's the residual.
So the gradient is driven by,
um, you know, kind of this quantity.
So if the prediction equals the target, uh, what's the gradient?
It's going to be 0 which is kind of what you want.
If you're already getting the answer correct,
then you shouldn't want to move your, uh, your weights, right?
So often you know we can do things in the abstract and everything will work.
But you know it's, it's often a good idea to write down some objective functions,
take the gradient and see if gradient descent on using
these gradients that you computed is kind of a sensible thing because there's
kind of many layers you can understand and get intuition for this stuff at
the kind of abstract level optimization or kind of at the algorithmic level.
Like you pick up an example is it sensible to update when
the gradient other than when the prediction equals the target.
Okay, so so let's take the code that we have from our, from last time,
and I'm going to expand on it a little bit,
and hopefully set the stage for doing stochastic gradient.
Um, okay.
So, so last time we had gradient descent.
Okay, so remember last time we defined a set of points,
we defined the function which is the train loss here.
Um, we defined the derivative of the function, and then we have gradient descent.
Okay, um, so I'm gonna do a little bit of housecleaning and I'm just,
uh, um, don't mind me.
Um, okay so I'm gonna make this a little bit more explicit,
what this algorithm is.
Gradient descent depends on, um, a function,
a derivative of a function and let say, um, you know,
the dimensionality, um, and I can call this gradient FDF
and in this case it's, uh, D where D equals 2.
Okay, and I want to kind of separate.
This is the kind of algorithms and this is, you know, modeling.
So this is what we want to compute and this is,
you know, how we compute it. [NOISE]
Okay and this code should still work. Okay, um.
All right, so what I'm gonna do now is,
um, upgrade this to vector.
So remember the x here is just a number, right?
But we want to support vectors.
Um, so in Python,
um, we're going to import NumPy so which is this, uh,
nice vector and matrix library um, and,
um, I'm gonna make some,
you know, arrays here,
um, which this is just going to be a one-dimensional array.
So it's not that exciting.
So this, this w dot x becomes,
uh, the actual dot I need to call.
And I think w needs to be np.zeros(d).
Okay. All right.
So that's just- should still run actually,
sorry, this is 1-dimensional.
Okay. So remember last time we ran this,
uh, this program and, um,
it starts out with some weights and then it
converges to 0.8 and the function value kind of keeps on going on.
Okay. All right, so let's,
let's try to, um,
you know it's really hard to kind of see you whether
this algorithm is any, doing anything
interesting because we only have two points, it's kind of trivial.
So how do we go about,
um, you know, because I'm going to also implement stochastic gradient descent.
How do we have kind of a test case to see if this algorithm is, you know, working?
Um, so there's kind of this technique which I,
I really like [NOISE] which is to call,
generate artificial data and ideas that, you know, what is learning.
You're learning as you're taking a dataset and you're trying to fit- find the,
the weights that best fit our dataset.
Uh, but in general if I generate some arbitrary,
if I downloaded a dataset I have no idea
what the right kind of quote unquote right answer is.
So there's a technique where I go backwards and say,
okay let's let's decide what the right answer is.
So let's say the right answer is,
um, 1, 2, 3, 4, 5.
So it's a 5-dimensional problem.
Okay. Um, and I'm going to generate some data based on that so that this,
uh, weight vector is kind of good for that data.
Um, I'm going to skip all my breaks in this lecture.
Um, so I'm going to generate a bunch of points.
So let's generate 10,000 point.
The nice thing about artificial data is you can generate as much as you'd want.
Um, there's a question, yeah?
A true w?
So true w just means like the, the correct,
the ground truth, the w.
The true y, true output or?
So w is a weight vector.
So this is kind of going backwards.
Remember, I want to fit the weight vector
but um, I'm just kind of saying this is the right answer.
So I want to make sure that the algorithm actually recovers this later.
Okay, so I'm going to generate some random data.
So there's a nice function,
random.randn which generates a random d-dimensional vector and y.
I'm gonna set- what should I set y to?
Which side of w you want?
Yeah. So I'm gonna do regressions.
So I want to do, uh,
true_w dot uh, x, right?
So I mean if you think about it,
if I took this data and I
found the, the like true one- w is the right thing
that we'll get 0 loss here.
Okay. But I'm going to make your life a little bit
more interesting and we're gonna add some noise.
Okay, so let's print out what that looks like.
Also I should add it to my dataset.
So okay, so this is my dataset.
Okay, I mean, I can't really tell what's going on but,
but you can look at the code and you, you can assure yourself that,
uh, this data has structure in it. [NOISE]
Okay, so let's get rid of this print statement and let's train and see what happens.
So let's.
Okay. Oh, one thing I forgot to do.
Um, so if you notice that the objective functions that I've, uh,
written down they haven't divided by the number of data points.
I want the average loss, not the, the sum.
Um, it turns out that, you know if you have the sum,
then things get really big and you know, blow up.
So let me just normalize that.
Okay. So let me lock it.
Okay, so it's training, it's training.
Um, actually so let me,
uh, do more iterations.
So I did 100 iterations, let's do 1000 iterations.
Okay. So when the function value is going down,
that's always something to- you know, good to check.
Um, and you can see the weights are kind of slowly getting to,
you know, what appears to be 1,
2, 3, 4, 5, right?
Okay. So this is a hard proof but it's kind of
evidence that this learning algorithm is actually kind of doing the right thing.
Um, okay so now let's see if I add,
you know more points.
So I now have 100,000 points.
Now, you know, obviously it gets slower,
um, and you'll, you know, hopefully get there you know,
one day but I'm just gonna kill it.
Okay, any questions about,
uh, oops, my terminal got screwed up.
Okay. So what did I do here,
I defined loss functions, took their derivatives.
Um, the gradient descent is what we implemented last time and the only thing different
I did, this time is generated data sets so I can kind
of check whether gradient descent is working. Yeah question.
So the fact that the gradient is just the residual [inaudible]
a algorithm to learn from overpredictions versus like underpredictions?
The question is whether the fact that the gradient is residual
allows the algorithm to learn from under or over predictions.
Um, yeah. So the gradient is if you think about it,
yeah that's good intuition.
So if you look at, um,
if you're over-predicting, right?
That means the gradient is kind of- assume that this is like 1.
So that means this is going to be positive which means that, hey if you opt that way,
you're going to over-predict more and more and incur more loss.
So, um, by subtracting a gradient,
you're kind of pushing the weights out in
the other direction and same for when you're, um, you're under-predicting.
Yeah, so that's good intuition to have.
Yeah.
What is the effect of the noise when you generate [inaudible]
What is the effect of the noise?
Um, the effect of the noise,
it makes the problem a little bit, you know,
harder so that it takes more examples to learn.
Um, if you shut off the noise then it will- you know, we can try that.
Um, I've never done this before,
but presumably you'll learn, you know,
f- faster, but maybe not.
Um, the noise isn't, you know,
that much. But, um, okay.
So, so let's say you have,
you know, like 500 examp- 1000 examples.
You know, that's quite a few examples.
As in now, you know, this algorithm runs,
you know, pretty slowly, right?
And in- in modern machine learning you have,
you know, millions or hundreds of millions of examples.
So gradient descent is gonna be, you know, pretty slow.
So how can we speed things up a little bit,
and what's the problem here?
Well, if you look at the- the- what the algorithm is doing, it's iterating.
And each iteration it's computing the gradient of the training loss.
And the training loss is,
um, average of all the points,
which means that you have to go through all the points and you
compute the lo- gradient of the loss and you add everything up.
And that's what is expensive and, you know, it takes time.
So, you know, you might wonder,
well, how, how can you avoid this?
I mean, you- if you wanted to do gradient descent you have to go through all your points.
Um, and the, the key insight behind stochastic gradient descent is that,
well maybe- maybe you don't have to do that.
So, um, maybe- you know,
here- here's some intuition, right?
So what is- what is this gradient?
So this gradient is actually the sum of
all the gradients from all the examples in your training set.
Right? So we have 500,000 points adding to that.
So actually what this gradient is- is, um,
it's actually kind of a sum of different things which are maybe
pointing in slightly different directions which all average out to this direction.
Okay. So maybe you can actually not average all of them,
but you can, um,
average just a couple or maybe even in an
extreme case you can just like take one of them and just,
you know, march in that direction.
So, so here's the idea behind stochastic gradient descent.
So instead of doing gradient descent,
we are going to change the algorithm to say for each example in the training set,
I'm just going to pick it up and just update, you know.
It's- instead of like sitting down and
looking at all of the training examples and thinking really hard,
I'm just gonna pick up one training example and update right away.
So again, the key idea here is,
it's not about quality it's about, uh, quantity.
May be not the world's best life lesson,
but it seems to work in- it works in here.
Um, and then, there's also this question of what should the step size be?
And in- generally, in stochastic gradient descent,
it's actually even a bit more important because,
um, when you're updating on each- each individual example,
you're getting kind of noisy estimates of the actual gradient.
And, uh, and people often ask me like,
"Oh, how should I set my step size and all."
And the answer is like there is no formula.
I mean, there are formulas,
but there's no kind of definitive answer.
Here's some general guidance.
Um, so if step size is small,
so really close to 0,
that means you are taking tiny steps, right?
That means that it'll take longer to get where you want to go,
but you're kind of proceeding cautiously.
so it's less likely you're gonna,
you know- uh, if you mess up and go in
the wrong direction you're not gonna go too far in the wrong direction.
Um, conversely, if you have it to be really,
really, large then, you know, it's like a race car.
You, kind of, drive really fast,
but you might just kind of bounce around a lot.
So, pictorially what this looks like is that, you know,
here's maybe a moderate step size,
but if you're taking steps,
really big steps, um,
you might go over here and then you jump around
and then maybe, maybe you'll end up in the right place but maybe
sometimes you can actually get flung off out of orbit
and diverge to infinity which is a bad situation.
Um, so there's many ways to set the step size.
You can set it to a, you know, constant.
You can- usually, you have to,
um, you know, tune it.
Or you can set it to be decreasing the intuition
being that as you optimize and get closer to the optimum,
you kind of want to slow down, right?
Like if you- you're coming on the freeway, you're driving really fast,
but once you get to your house you probably
don't want to be like driving 60 miles an hour.
Okay. So- actually I didn't implement stochastic gradient.
So let me do that. So let's, let's try to get stochastic gradient up and going here.
Okay. So, so the interface to stochastic gradient changes.
So- right? So the- in gradients then all you need is a function.
And it just kind of computes the sum over all the training examples.
Um, so in stochastic gradient,
I'm just going to denote S as for stochastic gradient.
I'm gonna take an index I,
and I'm going to update on the Ith point only.
So I'm going to only compute the loss on the Ith point.
And same for its derivative.
Um, you can look at the Ith point,
um, and just compute the gradient on that Ith point.
Okay? And this should be called SDF.
Okay. So now instead of doing gradient descent,
let's do stochastic gradient descent.
And I'm going to pass in sf, sdf, d, and,
um, the number of points because I need to know how many points there are now.
Um, copy gradient descent,
and it's basically kind of the same function.
I'm just going to stick another for loop there.
So stochastic gradient descent,
it's going to take the stochastic functions,
stochastic gradient, the dimensionality and- Okay?
So now, before I was just going through, um,
number of iterations and now, right,
I'm not going to try to compute the value of the- all the training examples.
I'm going to, um,
loop over all the points
and I'm going to call just evaluate the function at that point I,
and compute the gradient at that point I instead of the entire, you know, dataset.
And then everything else is the same.
I mean, one other thing I'll do here is that I'll use a different step size schedule.
So um, 1 divided by number of updates.
So I want it so that the number of,
uh, the step size is gonna decrease over time.
Okay, so I start with a equals 1 and then it's half,
and then it's a third, and it's a fourth, and it keeps on going down.
Um, sometimes you can put a square root and that's more typical in some cases,
but, um, I'm not going to worry about the details too much. Uh, question?
The point I is the chosen randomly but here we just [inaudible].
Yes. The question is- the word
stochastic means that there should be some randomness here.
And, you know, technically speaking,
the- the stochastic gradient descent is where
you're sampling a random point and then you're updating on it.
I'm cheating a little bit,
um, uh, because I'm iterating over all the points.
You know, in practice if you have a lot of points and you
randomize the order it's kind of- it's- it's
similar but it's- there is a kind of a technical difference that I'm trying to hide.
Okay. So- so this is stochastic gradient descent.
Um, to iterate, you know,
go over all the points and just, you know update.
Okay? Um, so let's see if this works.
Um, okay.
I don't think that worked.
[LAUGHTER]
Maybe- let's see what happened here?
I did try it on 100,000 points. Maybe that works.
And, nope, doesn't work either.
Um, anyone see the problem?
[inaudible]
So I'm printing this, um, out, uh,
at the- at the end,
um, of each iteration.
So that should be fine, um.
Really, this should work.
So gradient descent was working, right?
Maybe I'll, I'll try-
It's probably not the best idea to be debugging this live.
Okay. Let's, let's make sure gradient descent works.
Um, okay, so that was working right.
Okay. So stochastic gradient descent.
I mean, it's really fast and converges,
[LAUGHTER]
but it doesn't converge to the right answer.
I think [inaudible].
Yeah, but that should get incremented to 1.
So that-
It might be true.
Okay, so I do have a version of this code that does work.
[LAUGHTER]
So what am I doing here, that's different.
Okay, I'll have some water. Maybe I need some water.
[LAUGHTER]
Okay, so this version works. Yeah.
[inaudible]
Yeah, that's- that's probably good.
That's a good call. Yeah. okay.
All right. Now, it works. Thank you.
[LAUGHTER]
Um, so yeah.
Yeah, this is a good lesson.
Um, it's that when you're dividing, um,
these needs to be one- actually in Python 3,
this is not a problem but I'm so- on Python 2 for some reason.
But this should be, uh, 1.0 divided by numUpdates.
Otherwise, I was getting-
So how is it faster?
Okay. So why is it faster?
[LAUGHTER].
Yeah, okay.
Okay. Let's- let's,
uh, go back to 500,000, okay.
Okay. So one full sweep over the data is the same amount of time.
But you notice that immediately,
it already converges to 1, 2,
3, 4, 5, right?
So this is like way, way faster than gradient descent.
Remember, I just, uh, kind of compare it.
Um, gradient descent is,
um, you run it.
And after one stop, it's, like,
not even close.
Right. Yeah?
What noise levels you have to have until gradient descent becomes better?
What noise levels you have to have until gradient descent becomes better?
Um, so it is true that if you have more noise,
then gradient descent might be, uh,
stochastic gradient descent can be unstable.
Um, there might be ways to mitigate that with step size choices.
But, um, yeah, probably,
you have to add a lot of noise for stochastic gradient to be, um, really bad.
Um, I mean, this is in some sense, you know,
if you take a step back and think about what's going on in this problem,
it's a 5-dimensional problem.
There's only five numbers and I'm feeding it half a million data points, right?
There, there aren't- there's not that much to learn here.
And so there's a lot of redundancy in the dataset.
And generally, actually, this is true.
I go into a large dataset,
there's gonna be a lot of, you know, redundancy.
So, uh, going through all of the data and then try to make an informed decision is,
you know, pretty wasteful, where sometimes you can
just kind of get a representative sample from, um,
one example or more as common to do the
like of kind of mini-batches where you maybe grab a hundred examples
and you update on that which is- so there's
a way to be somewhere in between stochastic gradient and gradient descent.
Okay, let me move on. Um.
Okay.
Summary so far, we have linear predictors,
um, which are based on scores.
So linear predictors we include both classifiers and regressors, um,
we can do loss minimization,
and we can, uh,
if we implement it correctly,
we can do, uh, SGD.
Okay. So that was- I'm kind of switching things.
I hope you are kind of following along.
I'll introduced binary classification and
then, I did all the optimization for linear regression.
So now, let's go back to classification
and see if we could do stochastic gradient descent here.
Okay. So for classification, remember,
we decided that the zero-one loss is the thing we want.
We want to minimize the number of mistakes.
You know, who can argue with that?
Um, so rem- remember, what is zero-one loss look like? It looks like this.
Okay? So what happens if I try to run stochastic gradient descent on this?
Um, I mean, I can run the code,
but [OVERLAPPING] yeah, it's- it won't work,
right? And why won't it work?
[inaudible].
Yeah. So two popular answers are it's not differentiable,
that's- it's one problem.
Um, but I think that the- the bigger problem and kind of deeper problem is that,
what is the- what is the gradient?
Zero.
Zero. It's like zero, basically everywhere except for this point,
which are, you know, it doesn't really matter.
So, um, so as- as we learned that if you try to update with a gradient of 0,
um, then you, you won't move your weights, right?
So gradient descent will not work on the zero-one, uh, loss.
Um, so that's- that's kind of unfortunate.
So how should we fix this problem? Yeah?
[inaudible]
Yeah, let's, let's make the gradient non-zero. Let's skew things.
Um, so there's one loss,
which I'm gonna introduce called the hinge loss,
which, uh, does exactly that.
Um, so let me write the hinge loss down.
And the hinge loss,
um, is basically, uh,
is zero here when the margin is greater than or equal to 1 and rises linearly.
So if you've gotten it correct by a margin of
1 so you're kind of pretty safely on the err side of,
um, getting it correct, then we won't charge you anything.
But as soon as you start,
you know, dip into this area,
we're gonna charge you a kind of a linear amount and your loss is gonna grow linearly.
Um, so there's some reasons why this is a good idea.
So it upper bounds the zero-one loss, um, it's, uh,
it has a property called- known as convexity,
which means that if you actually run the gradient descent,
you're actually gonna converge to the global optimum.
Um, I'm not gonna get into that.
And so that's, you know,
that's a hinge loss.
Um, so what remains to be done is to compute the gradient of this,
you know, hinge loss, okay?
So how do you compute this gradient?
So in some sense, it's a trick question because
the gradient doesn't exist because it's not,
um, you know, differentiable everywhere,
but we're gonna pre- pretend that little point doesn't exist, okay?
So, so what is this hinge loss?
The hinge loss is actually two functions, right?
There is a zero function here and then there's like this,
uh, 1 minus x function.
So what am I plotting here?
I'm plotting the- the margin and, uh, the loss.
Okay? So this is,
uh, the zero function,
and this is, uh,
1 minus, uh, w dot phi of xy.
And the hinge loss is just the maxima of these two functions.
So at every point,
I'm just taking the top function.
So um, that's how I am able to trace out,
uh, this- this curve.
Okay? All right.
So if I want to take the gradient of this function,
you know, you, you can try to do the math.
Well, let's think through it. You know,
what- what should the gradient be?
Um, we're, we're here,
what should the gradient be? It's zero.
And if I'm here, what should the gradient be?
It should be the- whatever the gradient of this function is, right?
So in general, when you have a gradient of this- of this kind of max,
uh, you have to kind of break it up into cases.
Um, and depending on where you are,
um, you, you have a different case.
So loss is equal to- if I'm over here,
and what's the condition for being over here?
If the margin is greater than 1, right?
And then otherwise, I'm going to take the gradient of this with respect to w,
which is gonna be minus phi of x y, you know, otherwise.
Okay? Um, so again,
we can try to interpret the, the gradient of the hinge loss.
So remember your stochastic gradient descent, you have a weight vector,
and you're gonna pick up an example and you say,
Oh, let's compute the gradient move away from it.
So if you're getting the example right,
then the gradient zero don't move, which is the right thing to do.
And otherwise, you're going to move in that direction because you're minus,
minus of phi of x y,
which kind of imprints this example into your weight vector.
So- and you can formally show that it actually increases your, uh,
margin after you do this.
Okay? Yeah?
What's the significance of the margin being 1?
What's the significance of the margin being 1?
Um, this is a little bit arbitrary,
you're just kind of sending a non-zero value.
Um, and, and, you know,
in support vector machines, you set it to 1,
and then you have regularization on the weights and
that gives you, uh, some interpretation.
So I don't have time to go over that right now,
but, uh, feel free to ask me later.
There's another loss function.
Uh, do you have a question?
Yeah. Why is the or why do we choose the margin if it's
a loss function that's supposed on the square or another loop?
Yeah. So why do you choose the margin?
So in classification, we're gonna look
at the margin because that tells you how comfortable when you're predicting,
uh, co- you know, correctly.
In regression, you're gonna look at residuals and square losses.
So it depends on what kind of- what problem you're trying to solve.
Um, just really quickly,
some of you might have heard of logistic regression.
Logistic regression is this, uh,
yellow loss function, right?
So the point of this is saying that this loss minimization framework is, you know,
really general and a lot of things that you might have heard of
least squares logistic regression are a kind of a special case of this.
So if you kind of master how to do loss minimization,
you kind of, uh, can do it all.
Okay. So summary, um,
basically, what's on the board here?
If you're doing classification,
you take the score which comes from the, uh,
w dot phi of x and you drive it into the sign,
and then you get either plus 1 or minus 1.
Regression, you just use a score.
Now to train, you have to assess how well you're doing.
In classification, there's a notion of a margin.
Res- uh, in regression,
it's the residual, and then you can define loss functions.
And here is we only talking about five loss functions but there's many others, um,
especially for a kind of structure prediction or ranking problems,
there's all sorts of different loss functions.
But they're kind of based on these simple ideas of,
you know, you have a hinge,
the upper balance is zero-one if you're doing classification and,
[NOISE] um, some sort of square-like error for, you know, regression.
And then, once you have your loss function, provided it's not zero-one,
you can optimize it using, um, SGD,
which turns out to be a lot faster than, you know, gradient descent.
Okay. So next time, we're gonna talk about, uh,
Phi of x, which we've kind of left as,
you know, someone just hands it to you.
And then we're also gonna talk about what is
the really true objective of machine learning?
Is it really to optimize the training loss?
Okay, until next time.
 Okay. [NOISE] Uh, welcome back everyone.
This is the second lecture on machine learning.
Um, so just before we get started,
a couple of announcements.
Um, homework 1 foundations is due tomorrow at 11:00 PM.
Note that it's 11:00 PM, not 11:59.
Um, and please I would recommend everyone try to do a test submission early, right.
Um, it would be unfortunate if, uh,
you wait until 10:59 and you realize that your computer,
uh, you can't login to the website.
Um, if that happens,
please don't just bombard me or- or with emails.
Just- just wondered- so there is- you can- you can
resubmit as much as you want before the deadline?
So there's no penalty to just submitting something and checking to make sure it works.
Yeah. So just to remind you,
you're responsible for any technical issues you encounter,
so please do the test submission early.
So you have peace of mind,
and then you can go back to finishing your, um, your homework.
Okay? Uh, homework 2, sentiment is out.
This is the homework on machine learning,
um, and it will be due next Tuesday.
Um, and finally, there's a section this Thursday which will talk about, uh,
back propagation and nearest neighbors,
and maybe a overview of scikit-learn which might be useful for your projects.
So please, uh, come to that.
Okay. So let's jump in.
I'm gonna spend a few minutes reviewing what we did last time.
It's kind of starting at the very abstract level and drilling down into the details.
So ab- abstract level,
learning is about taking a data-set and outputting a predictor F,
which will be able to take inputs x, for example an image,
and output a label or output y,
for example whether it's a cat or a truck or so on.
And if you unpack the learner,
we talked about how we want to frame it as
a optimization problem which captures what we want to,
uh, optimize, what properties a predictor app should satisfy.
And apart from the optimization algorithm,
which is how we accomplish our, um, objective.
So the optimization problem that we talked about last time was,
uh, minimizing the training loss.
Um, and in symbols,
this is the training loss which depends on a particular weight vector.
Is the average over all examples in
the training set of the loss of that particular example,
uh, with respect to the wave vector w. Okay.
And we want to find the w that minimizes the training loss.
So we want to find the single w that,
um, makes sure that on average,
all the examples have low loss.
Okay. So looking at the loss functions,
um, now, this is where it depends on what we're trying to do.
If we're doing regression,
then the pertinent thing to look at is the residual,
which remember, is the model's prediction minus the true label.
So this is kind of how much we overshoot.
And the loss is going to be zero if the residual is zero,
and in-increases either quadratically for the square loss,
or linearly for the absolute, uh,
deviation depending on how much we want to penalize large, uh, deviations.
Um, for classification or binary classification more specifically, um,
the pertinent quantity to look at is the margin,
which is the score times,
uh, the label y, which remember is plus 1 or minus 1.
So the margin, um,
is a single number that captures how correct we are.
So a large margin is good.
In that case we, uh,
obtain either a 0 or a near 0, uh, loss.
And margin less than 0 means that we're making a mistake.
So the 0 in loss captures that we're making a mistake of a loss 1.
Um, but, uh, the hinge loss and logistics loss kind of grow linearly,
because it allows us to optimize the function better. Question?
So I have a question about residuals.
Yeah.
Like, I know that I see the regression curve,
the loss squared curve there with the
residual- what- what would a residual look like on a graph?
Would it be just a point away from the resid- away from the regression curve?
Or what would the residual look like on this graph, if you were to put it?
Um, so there are multiple graphs, uh, here.
So remember last time we looked at residual.
If you look at, um,
x or rather Phi of x over y,
so here's the line.
Um, here is a particular point Phi of x,
um, Phi of x, uh, y.
And the residual is the, uh,
basically the difference between the model's prediction and,
uh, the actual point here.
This graph is different.
This graph is, um, visualizing,
um, in- is- um,
in a different space.
Right. I'll show you another graph that might make some of these things,
uh, a bit clearer in a second.
So the residuals won't look exactly like that on this curve of velocity graph? Correct?
Um, well okay, I guess one way to think about the residual is,
um, the residual is a number.
So if your residual is 2,
then you're kind of here,
and this is the loss that you pay,
which is, uh, 2 in this case.
Oh.
And if the residual is minus 2,
then you pay, uh, 2.
So the residual is the x-axis? Okay.
Yes. The residual is the x-axis here.
Oh, okay. Okay.
And the margin is the x-axis over here.
All right.
Yeah. Okay. Any other questions about this?
When would you use the absolute value? [BACKGROUND]
Um, yeah. The question is,
when would you use absolute value versus the square loss?
Um, there is a slide from the, uh,
previous lecture which I skipped over which talks about when you would want it.
Um, most of the time people tend to use
the square loss because it's easier to optimize,
but, um, you also see absolute, um, you know, deviation.
Um, um, the- the square loss will penalize large outliers a lot more.
Which means that it has kinda mean- uh, mean-like, uh, qualities.
Whereas the absolute deviation, um, penalizes less,
so it's more like a median,
uh, just for kind of intuition.
Um, but the general point is that all of
these loss functions capture properties of a desired predictor.
They basically say, hand me a predictor,
and I'll try to assess for you how good this is, right.
This is kind of establishing what we want out of it.
And, um, you know,
also another comment is that, you know,
I'm presenting this loss minimization framework because it is so general.
Anything basically that you see, um,
in machine learning can be viewed as some sort of,
you know, loss minimization.
If you think about PCA or deep neural networks, um, different, um,
types of auto-encoders, they can all be viewed as some sort of
a loss function, um,
which you're trying to minimize.
So, um, tha- that's why,
uh, I'm kind of keeping this ge-framework somewhat general.
Okay. So let's, uh,
go to the opposite direction of generality.
Let's look at a particular example,
and try to put all the pieces together.
Um, so suppose we have a simple regression problem.
We have three training examples: 1, 0,
the output is 2,
1, 0, the output is 4,
and 0, 1 the output is minus 1.
Right. Um, so, um,
how do we visualize what learning on this,
uh, training set looks like?
Um, so let's try to form the training loss.
The training loss, remember,
is the average over the losses on the indivi- individual examples.
So let's look at the losses on individual examples.
Um, so we're doing linear regression,
so and x is two-dimensional,
and Phi of x equals x.
So, uh, in this example, so,
um, we're basically trying to fit two numbers, w_1 and w_2.
Um, so if you plug in these values for x and y into this loss function,
then you get the following quantities. So the dot
product between w and x is just w_1, right.
Um, because x- x2 is 0.
And you minus 2 and you square it because we're looking at the square loss.
Um, the same thing for, uh,
this point instead of 2 you have a 4.
Um, and then for this point, um, uh,
w.Phi of x minus y is w_2 now because, um,
now the- the x2 is, uh,
active, uh, minus minus 1 squared.
Okay. So these are the individual loss functions.
Each of which tells what I kind of want out of w. So if here I'm looking at this,
if w_1 is 2,
then that's great, I get a loss of 0.
This one says if w_1 is 4,
then that's great, and I get a loss of 0.
And obviously you can't have both.
And the goal of the training loss is trying to look at the average,
so that you can pick one w that works for,
as kind of on average,
is good for all the points.
Okay? So now, this is a function in two dimensions.
It depends on uh, w_1 and w_2.
So let me try to draw this on the board to give
you some more intuition what this, uh, looks like.
Okay. So I'm gonna draw a w_1, uh, w_2.
And so the first, uh,
function is, uh, w_1 minus 2.
Okay. So, um, so what does this function want to do?
It wants w_1 to be close to or,
uh, close to 2, and it doesn't care about w_2.
Right? So, um, I'm not really sure how to draw this function,
but it it really requires something in 3-D.
So you can think about a ball-shape kind of coming out of the board, uh,
like this, if this direction is meant to be the- the loss.
Okay. So I'm gonna try to do,
uh, um, well let's- let's try it this way.
So it's going to be like I have kind of a bunch of, um,
problems that look like this coming out of the board. Okay? Uh-
Um, okay. So what about the second one?
The second one is,
uh, w1 minus 4 squared.
So that's going to be basically the same thing [NOISE],
but kind of centered, uh, around 4.
So around this axis.
Okay. So again, there is gonna be some parabolas coming out of the board.
Um, and then finally,
the other point is, uh,
w2 minus, minus 1.
So it's going to be, um,
happiest when, um, um,
w2 is minus 1.
Um, so it's going to be kind of a bunch of,
uh, parabolas coming out of the board here, okay?
So you add all three functions up,
and what do you get?
You get something that is, um,
has- first of all,
where do you think the minimum should be?
One of the two intersections of the [NOISE] on the-
One of the two intersections.
Yeah. Like the first, like the first,
uh, vertical and horizontal or the second square vertical and horizontal.
Oh, the red lines, I mean.
Oh, yeah. There's gonna be some sort of intersection here.
So if you look at, um, the w2 axis,
right, um, it should definitely be minus 1,
because this is the fun- only function that cares about w1.
So it's gonna be somewhere here and both by symmetry,
while this one wants it to be a 2,
this one it wants it to be a 4,
so the average is somewhere between.
You can work all of this kinda actually mathematically out,
I'm just kinda giving the rough intuition.
Uh, and now let me draw the level curves here.
The level curves are going to be something like this where, um, ag- again,
if you draw it in 3D, it's like a parabola, uh,
or- coming out of the- a board here,
um, where here's the lowest point.
Um, and as you venture away from this point,
your loss is going to increase.
[NOISE] Right?
Okay. Yeah.
Can you explain that bit again, that middle point?
Uh, how do I get this middle point?
Um, [NOISE] so one way is that if you add these two functions up and it kind of,
um, just, you know,
plot it, uh, it turns out to be a 3.
Intuitively, um, the, the square loss when you average,
uh, it acts kind of like,
uh, um, a mean.
So kind of, you know,
it's gonna be somewhere in between.
It's also, um, related to one of the homework problems.
So hopefully, you'll h- have a better appreciation for that.
Um, okay.
So, so I guess- yeah, question.
Once we have the 3, how do we merge it with the negative 1 as well?
Do we need to do another addition?
Um, so the question is, once we have the 3,
how do you merge it with a minus 1?
Um, so the 3 is regarding w1 and the minus 1 is regarding w2.
So you just add them together.
They kind of don't- in this particular example,
they don't interact. In general, they will.
I still like this example, could you quickly
summarize exactly what's going on with this example.
Yeah. So this plot shows for every possible wave vector w1, w2,
you have a point and the amount that the function
comes out of the board is the loss, right?
And the loss function is defined on- in the slides, right there.
And all I'm doing is trying to plot this loss function.
Okay. So it's actually w1 and w2 points,
the loss is coming out of the board you're plotting. Yes? No?
Um, so unfortunately, it's hard to kind of draw it in 3D here. So-
Okay.
What I'm trying to do here is taking each of the pieces and
trying to explain what each piece is trying to do.
All right.
Yeah. Okay. So, um, in general,
the, the training loss,
you don't have to think about kind of how exactly it composes the individual losses.
Um, this is probably as complex of an example we'll have to,
you know, we'll get to right trying to understand it.
Um, but this kind of gives you an idea of how you
connect these pictures where you see kind of these are parabolas,
um, with, uh, the picture which is actually the- of the, you know, training loss.
Okay. But for now,
let's assume you have the training loss.
It's a function of the parameter- it's some function.
And how do you optimize this function?
So you do some sort of gradient descent.
So last time we talked about how you can just do a
v- vanilla gradient ascend where you'd initialize with 0.
And then you compute the gradient of that entire training loss.
And then you update once.
And the problem with that is the, uh,
up to computing the gradient requires going through all the training examples,
and if you have a million training examples that's really slow.
So instead we looked at stochastic gradient descent which allows you to pick up
an individual example and then make a gradient step right away, right?
And, um, empirically we SHA encode how it can be a lot faster, you know.
Of course there are cases where,
um, it can also be less stable.
So there's kinda in general going to be some, you know, trade off here.
But by and large, stochastic gradient descent it kind of really dominates, um, you know,
machine learning applications today because
you- there's only way to really kind of scale to large,
um, you know, datasets. Okay. Yeah.
Is there any other benefit of stochastic gradient descent or gradient descent?
Um, so apart from being able to scale up,
is there any advantage of stochastic gradient descent?
Um, another besides computation,
another advantage might be that, um,
your data might be coming in an online fashion, like over time,
and you want to,
you know, update kind of on the fly.
Um, so there are cases where you don't actually have all the data at once.
Okay. So that was a quick overview of the general concepts.
Um, now to set the stage for what we're gonna do in this lecture,
I wanna ask you guys the following question.
So can we obtain decision boundaries, um,
remember a decision boundary is the- the- it's kind of a line that or the
curve that separates the region of the space which
is classified positively versus negatively.
Um, can we obtain decision boundaries which are circles,
um, by using linear classifiers?
Okay. So, um, does that make sense?
So we want to get something like this, um,
where you have- now we're going into,
uh, Phi1 of x, um,
you know, Phi2 of x.
And we want decision boundaries that look like this
where you classify maybe these as positive and these as negative.
Okay. Is that possible? Yeah.
If you map, if you like take a square of those inputs.
Then you get something to be linear. You [inaudible]. [OVERLAPPING]
Yeah, yeah. Okay. [LAUGHTER] So you're saying, yes?
Yeah.
Okay.
[inaudible].
Okay.
Uh, okay. Well, there's a punchline there.
Um, so it turns out, um,
that you can actually do this which maybe on the surface seems kinda surprising, right?
Because we're talking about linear classifiers.
But as we'll see it really depends on what you mean by
linear classifiers and hopefully that will become clear soon.
Okay. So we're gonna start by talking about
features which is going to be able to answer this question.
Then we're gonna shift gears a little bit and talk about neural networks which is, uh,
in some sense an automatic way to learn features.
And we're gonna, ah, show you how to train neural networks using
back propagation hopefully without tears.
And, um, then talk about nearest neighbors which is
another way to get really expressive models which is gonna be,
um, much simpler in a way.
Okay. So recall that we have the score.
So the score is a dot product between the wave vector and the feature vector,
and the score drives prediction.
So if you're doing regression, you just output the score as the number.
If you're doing classification,
binary classification then you output the sign of the score.
Um, and so far we've focused on learning which is how you
choose the wave vector based on a bunch of data and how you optimize for that.
And so now what we're gonna do is focus on phi of x, um,
and talk about how you choose these features in the first place.
And this actually, feature extraction is
such a really critical important part of kinda
a machine learning pipeline which
often gets neglected because when you take a class you're saying,
Okay, well there's some feature vector and then let's focus on all of these algorithms.
But whenever you go and apply machine learning in the world, um,
feature extraction, um, turns out to be kinda the main bottleneck.
And neural nets can mitigate this to some extent
but it doesn't completely make feature extraction um, obsolete.
So recall that a feature extractor takes an input, um,
such as this uh,
string and outputs a set of properties which are- are useful for prediction.
So in this case,
it's a set of um,
named feature values okay.
And last time, we didn't really say much about this.
We just kinda waved our hands and say,
okay here's some features.
So you in general how do you approach this problem,
what features do you include?
Um, do you just like start making them up and how many features you have?
We need maybe a better organizational principle here.
Um, and you know in general a feature engineer is gonna be someone of art.
So I'm not gonna give you a recipe,
but at least some framework for thinking about features.
So the first notion um,
is a feature template,
and a feature template is informally
just a group of features that are all computed in the same way.
Um, this is kind of a somewhat pedantic but kinda,
um, a terminology point that I want you all to kinda be aware of.
Um, so a feature template is basically a feature um, name with holes.
So for example length greater than blank.
So remember the concrete feature has length greater than 10.
Now, we're gonna say length greater than blank,
where blank can be replaced with 10,
9, 8 or any kind of number.
And it's a template that gives rise to multiple features.
Last week, characters equals blank, contains character blank.
These are all examples of feature templates.
So when you go in your project or whatever and you
describe your features or when you think
about kind of grouping these features in terms of,
you know, um, these blanks.
Another example is pixel intensity of position.
So even if you have what you consider to be like a raw input,
like an image, right?
There's still implicitly some sort of way to think about it as a feature template, um,
which corresponds to the pixel intensity of position,
blank comma blank, ah,
is a feature template where it gives a rise
to the number of features equals to the number of pixels in the image.
And this is useful because maybe your input isn't just an image.
Maybe it's an image plus some metadata.
Then having this kind of language for describing
all the features in a unified way is really important for clarity.
Okay. So as I alluded to,
each feature template maps to a set of features.
So by writing last three characters equals blank,
I'm implicitly saying, well,
I'm going to define a feature for each value of blank and
that feature is gonna be associated with
a value which is just the natural evaluation of that feature on the input.
Okay. So all of these are 0 except for ends with .com is 1.
Okay. So, um, and in general you are going to have
each feature template that might give rise to many, many features, right?
Um, the number of
possible three-letter characters, you know
some number of characters to a cube which is a large number.
Ah, so one question is how do we represent this, right?
First vector.
Yes, first vector.
[LAUGHTER].
Yeah good answer. So mathematically, it's really useful.
Just think about this vector as a d-dimensional vector, right.
Just d numbers just laid out, right?
And because that's mathematically convenient but when you
go to actually implement this stuff you might not represent things that way.
In particular, you know, what are the ways you can represent a vector?
Well, you can say,
I'm going to represent it as an array which is just this list of numbers that you have.
But this is inefficient if you have a huge number of features.
But in the cases where you have sparse features which means
that only a very few of the feature values are non-zero,
then you're better off representing as a map or in Python, a dictionary,
which you specify the feature name um,
is a key and the value is, you know,
the value of that feature, right?
And all the,um, the- the home
homework two will basically work in this sparse feature framework.
Um, and you know, just kind of a note,
a lot of, um,
especially in NLP and we have discrete objects and traditionally,
it's been common to use kind of these sparse feature maps.
Ah, you know one thing that has happened with
the rise of neural networks is that often um,
you take basically your inputs and embed them into some sort of
fixed dimensional vector space and
dense feature representations have been more um, dominant.
But sparse features if you wanna use linear classifiers is still kinda a good way to go.
So it's important to understand this.
Okay. So now instead of storing possibly a lot of features now you just
store the key and the value.
All right. Um, So this was the feature templates.
The overall point is that it's kind of organizational principle,
um, and you know,
um, okay so now let's switch gears a little bit.
So which features or feature templates should you actually write down?
And to get at that,
I wanna introduce another notion which is pretty
important especially if you think about the theory of machine learning,
and that's the notion of a hypothesis class.
Okay. So remember we have this predictor.
So for a particular wave vector,
that defines a function that maps inputs into some sort of score or prediction.
And the hypothesis class is just the set of all predictors that you can
get if you vary the wave vector.
Okay. So- so let me give you um,
we're gonna come back to this slide.
Let me give you a kinda example here.
So suppose you are doing a regression and you're doing linear regression in particular.
So you're in one dimension.
Here is x and, um,
here is, ah, I guess y. Um,
so if your feature map is just identity,
so maps x to x, um,
then this notation just means the set of all, ah,
linear functions like this.
Then the set of functions you get you can visualize um, as this, right?
So you have one function here and for every possible value of w1,
you have, ah, a slope.
You also have 0.
They should all go through the origin.
Um, so you have- these are your functions, right?
So your hypothesis class F1 here is essentially all lines that go through the origin.
Okay. So just wanna think about it when you write down
a feature vector you're implicitly committing yourself to saying hey,
I want to think about all possible predictors defined by this feature map.
Okay. So here's another example.
Suppose I define the feature map to be x comma x squared.
Okay. So now what are the possible functions, you know, I'm gonna get?
So does anyone wanna say what, read off this slide what it is [LAUGHTER].
It's gonna be all quadratic functions, right?
Okay. So in particular,
because they don't have a bias term,
it's gonna be all quadratic functions that go through the origin.
So let me actually draw another.
[NOISE]. Um.
So it's gonna be all quadratic functions that go through the origin,
which look like this,
but it could be upside down.
Um, and do it like that,
I'm not gonna draw all of them.
Um, in particular, it also includes the linear functions, right?
Because I can always set w_2 equals 0,
and vary w_1 which means that I also get all the linear functions too, right?
So this means that w- f_2,
if you think about the set of functions is a larger set than f_1, it's more expressive.
That's what we mean by expressive.
That means that it can represent more things.
Okay. So for every feature vector,
you should think also about the set of functions that you can get by,
uh, that new feature vector.
Okay. So let's- is there a question?
Yeah. We need to assess the time here- the best set of w's,
are- are the more expressive sets harder to optimize over?
The question is, are the more expressive sets harder to optimize over?
In terms of, ah,
you know, the short answer is not necessarily.
Um, um, in terms of- sure,
you have more features so that it req- is more expensive.
Yeah. At that level, um,
but the difficulty optimization depends on a number of different, you know, factors.
Um, and sometimes, adding more features can be easier to optimize because it's easier to
figure out training data, um, okay.
So now, let's go back to this picture.
Okay. So, uh, this is- on the board is concrete examples of feature or,
or ah, uh, hypothesis classes.
Um, now, let's think about this big blob as the set of all predictors.
Any predictor in your wildest dreams,
you know, they're in this, this set.
Okay? And whenever you go and you define a feature map,
that's going to carve out, uh, you know,
much smaller set of, um,
you know, uh, functions, right?
And- and then what is learning doing,
learning is choosing a particular element of that,
um, function family based on the data.
Okay. So this picture shows you
kind of the full pipeline of how you're doing machine learning.
Is, you know, there- you first declare structurally a set of, ah,
of functions that you're interested in,
and then you say [NOISE] okay, now,
based on data, let me go and search through
that set and find the one that is, you know, best for me.
Okay. So now, there are,
you know, two places where things can go wrong.
Well, for feature extraction,
maybe you, um, didn't have enough features.
So now, yours- your- your, uh,
purple set is too small.
Then, no matter how much learning you do,
you're just not going to get good accuracy.
Right? And then conversely,
even if you define a nice,
um, you know, uh,
hypothesis class, if you don't optimize properly,
you're not gonna find the element of that null hypothesis class that fulfills your,
um, your goals. Question?
The function F- the feature function is extracted to get from the input,
since that, you know,
self as a function,
how come you can assume that your weights,
will be able to compute,
um, that function also?
So the question is- so you're defining a function Phi,
right? This is fixed.
Um, and then learning sets weights,
and together jointly, they specify a particular function or predictor.
There's something that saying that if you don't choose Phi appropriately,
you're limiting the space they will be able to predict.
Yeah.
But so I'm wondering like why my under- my
intuition tells me that the whole point of learning is that, uh,
regardless of the Phi that you choose,
the actual model that you choose should be able to,
you know, learn the function Phi that you would have picked.
Ah, I see. So the question is, ah,
does- doesn't learning kind of
compensate and just figure out the Phi that you would have picked.
Um, so the answer is- short answer is no.
The- the Phi is really kind of a bottleneck here.
For example, it just- if you define Phi to be,
um, X, so that's the linear function.
Linear function is all you're going to get.
Right? So if your data moves around, um,
in a sinusoidal way, you're just gonna, like,
fit a line through that and you'll get,
you know, horrible accuracy.
And no amount of learning,
um, can, you know, fix that.
The only way to fix that is by, um,
changing your, you know, feature representation.
So does that assume that W is- is a linear model though?
So yes. So all of this assumes that W- we're talking about linear predictors here.
Okay.
But of course, the same general idea
applies to any sort of function family in neural nets.
Um, the arc- so the equivalent there would be not just,
ah, the feature map, but also the neural network architecture.
It's a constraint on what kind of things you can express.
So if you have in your- only a two-layer neural network,
then there's just some things that you just, you know, ah,
with ah, with a fixed size,
there's just some things you just can't express. Yeah. Another question.
Just to follow onto that as well as an alternative interpretation of the question,
I felt it was more of a question of why for
a visualization rather than kicking in the raw data,
and have like a neural net it still functions linear classifiers,
but it has enough complexity that it can strive for non-linear behavior.
Yeah. So the question is why bother doing feature in engineering?
Hasn't neural nets kind of basically, solved that?
Um, so to some extent,
the- the amount of feature engineering you have to do today is,
you know, much less.
One thing that I think it's still important to think about in feature engineering is it's
really- think about it as what sources of information you want to, you know, predict.
For example, if you want to predict, um, you know,
this, uh, you know,
some property about a movie review.
And you know what- what if- part of
the first-order bits are like what even goes into that.
Does it- the text go into that?
Do you have metadata? Do you have other star ratings?
And those are [NOISE] you know,
features you can- there's- I guess no such thing as like raw, um,
because there's always some code that takes, you know,
the- the, you know,
the world and distills it down into something that fits in memory.
So that's you can think about it as feature extraction.
Thank you.
Yeah. Okay. One last question and then I'll go.
What is the problem with, uh,
too many features, don't you want your hypothesis class to be too big, is it like an overfitting thing?
Yeah. Yeah. Um, so the question is why don't you just make Phi as large as possible,
throw on all the features,
and overfitting is, um, you know,
one of the main concerns there which,
you know, we'll come back to in the next lecture.
Okay, great questions. Um, so let's,
um, let's actually skip over this, um.
So there's another type of, uh,
feature function you can define,
but in interest of time, I'm going to skip over that.
Um, okay, so now let's come back to this question,
this linear- I keep on saying near linear predictors.
So what, what, what is linear, right?
Uh, so remember the prediction is driven by the score.
Right. So here's a question.
Is this score linear in w?
Yes, right?
Because, um, what is a, you know,
linear function is basically some kind of weighted,
er, combination of your inputs.
Okay, so is it linear in Phi of x?
By symmetry, it should be because it's just a dot-product.
So is it linear in x?
No, in fact this,
this question doesn't even make sense because think about x. X remember was a string.
Right, it's not a- it's not even a number.
So, um, and that's when you know the answer should be no
because you know it doesn't, there's a type error.
Um, okay so here's,
here's kind of the cool thing now is, um,
you know these predictors can be
expressive nonlinear function and decision boundaries of x,
you know, in the case where x is, uh,
uh, is actually a real vector.
Um, but the score is a linear function of w, okay?
So this is cool because, you know,
from a pr- there's two perspectives, right?
From the point of actually doing prediction,
you know, you're thinking about like wh- how does this function operate on x?
And you can get all sorts of you know, crazy functions coming out.
Um, we just looked at quadratic functions was
clearly non-linear but you can do all sorts of, you know, crazy things.
But from the point of view of learning,
it doesn't care about x.
All it sees is Phi of x.
In particular, your learning asked the question how does this function depend on w?
Right? Because it's tuning w. And from that perspective,
it's a linear function w and,
um, for reasons I'm not gonna, you know,
go into, um, these functions, er,
permit efficient learning because the loss function becomes convex,
um, which I'll, that's all I say about that.
Okay. So, um, so one kind of
cool way to visualize what's going on
here is when you're going back to our circle as example.
So remember we want this, um,
two-dimensional classification problem where the true decision boundary is,
you know, let's say a circle.
So how do we fit that and what does it mean for
a linear thing because when you think linear it like, should be a line, right?
Um, so here's a kind of a cool graphic. So, okay.
So here is, um,
these points inside the circle and,
you know, it can't be classified.
But the point is when you look at
the feature map it actually lifts these points into a higher dimensional space.
Now I will have three features, right?
And- and you know,
in this higher-dimensional space,
I can actually- things are linear.
I can slice it with a kind of a knife.
And then, you know,
in that high dimensional space if things are cut and what that
induces in the lower-dimensional space is, you know, this circle.
Okay. Okay, I don't wanna- Okay,
so hopefully that was, er,
a nice visualization that shows how you can actually get
nonlinear machine functions out of kind of essentially linear machinery, right?
So someone- the next time someone says, well, you know,
um, you know, linear classifiers are really limited, um,
and you really need neural nets, um, you know,
that's technically false because you can actually get really expressive models out of,
er, you know, neural networks- sorry,
out of linear models.
The point with neural networks is not that they're not- you're more necessarily more
ex- expre- They can be more expressive but the fact that they have other advantages,
for example, the inductive bias that comes with the architectures and, um,
the fact that they are more efficient, ah,
when you go to more expressive models and so on.
Okay, so- so to kind of wrap up all things,
I want to kind of do a
you know, simple exercise.
So here's a task.
So imagine you're doing a final project and you want to,
um, predict, you know,
whether two consecutive messages in some forum or a chat are,
um, where the second one is a response to the first.
So it's binary classification,
input is two messages,
and you're asked to predict whether a second is a response to the first.
Okay, so we're gonna go through this exercise of coming up with, um, you know,
features that might be or feature templates
might be useful to pick out properties of x that might be useful.
Um, and we're gonna assume that we're dealing with linear predictors.
Okay. So what are some features that might be useful?
Let's, um, you know,
let's- here's- let's start with a few.
Okay. So how about time elapsed, um,
between the two messages,
is that a useful feature or not? How many of you say yes?
Okay, so this information is definitely good.
Um, one subtle point is that this time elapsed is a single number,
and this number is going to go into the score kind of in a linear fashion, okay?
So what does- what does that mean?
That means, um, you know,
if I double the time,
then the score is going to or that can p- the contribution to
the score is going to like multiply by 2, right?
So think about it, it's, it's kinda like saying them,
as I increase the time, you know, the,
it becomes linearly more likely that I'm
going to be let's say not a response or- or a response.
So this is, you know,
maybe it's kind of not what you want because, you know,
the difference and from that perspective like if you,
the time elapsed is like a year then that really kind of dominates the- the score function.
Um, and it's like way more likely that it's going to be
a response than if it were like one minute,
which is kind of not what you want. Yeah, question?
Can you normalize it to teach them?
Yeah, so the question is, can you normalize it?
Um, so you have to be careful with normalization.
So you have- if you normalize let's say the span of like over one year.
Now, now, there's no difference between like, you know,
five seconds and one minute because everything gets squashed down to 0, right?
So, er, one way to kind of approach that is to,
um, discretize the features.
So one trick that people often do is if you
have a numerical value which you really kind of want to,
um, treat kind of in a sensitive way,
you can kind of break up into pieces.
So the feature template would look something like time elapsed is between blah and blah.
So you can do things like okay is it between
zero seconds and five seconds and is it between
five seconds and like a minute and between
a minute and an hour and an hour and a year or something?
And then after that, it doesn't matter.
Um, because that will give you kind of more, um,
it's more domain knowledge that tells you kind of what things to look out for.
That difference between let's say a year and a year plus two seconds is really,
you know, it doesn't matter, right?
Whereas the difference between one second and five seconds might be significant.
So this is all a long way of saying you know
if you're using linear classifiers or even if you're using neural networks,
I think it's really important to think about how
your raw features are kind of entering the system and think about like,
if I change this feature by like scaling it up,
does the prediction change in a way that, you know,
I expect? Yeah, you got a question?
So if we approve that second feature right there,
er, what prevents us from having let's say, er,
if we had a whole sort of 35 seconds from
30 to 40 seconds and maybe so on what prevents us from getting just the entire time?
[NOISE]
Yeah, so, er, the question like if you have
every possible range isn't that like an infinite number of features?
Um, so, er, there's two answers to that.
One is that even if you did that,
you might still be okay because there's probably some, um,
if you think about it like discretizing the space of, you know,
here is your time elapsed, time, um, elapsed.
And you're basically saying for every bucket I'm going to have a feature.
Um, it is true that you have an infinite number of, you know,
features but, you know,
at some point you might just cut it off.
And if you didn't cut it off and use a sparse feature representation, um,
you don't have to, um,
pra- have a pre-set,
you know, maximum because remember,
most of these features are gonna be zero
because the chances of some data point being like,
you know, 10 years is going to be essentially you know, nil.
Um, another answer is that, um,
in general when you have features that, er,
have multiple timescales, um,
you want a kind of space that will work kind of logarithmically.
Um, so you know, one to two,
two to four, four to eight, um,
so that you can have both kinds of sensitivity in lower events but also,
um, kind of cover a large, um, magnitude.
Yeah, in the back.
Is it possible to learn,
like how to discretize the features make it the most important?
Um, question is, is it possible [NOISE] to learn how to discretize the, the features?
Um, there are- there's definitely more automatic things you can do besides,
you know, just like spans specifying them.
Uh, at some level though,
you have to kind of input the value in a
form, like, er, if you've inputted into x versus,
let's say log of x, um,
those choices often can make a,
you know, big difference.
Um, but, um, if you use more expressive models like neural networks you can,
you know, mitigate some of this. Yeah.
I see the value in changing time elapsed, uh,
from a number to like a Boolean whether it falls between a range.
Why would you wanna retain a,
a numerical value for teaching?
When would you not wanna discretize it?
Yeah, good question. So when would you actually want to not discretize it?
Um, [NOISE] so there are- um,
essentially when you expect kind of the- the scale of that feature to,
um, [NOISE] really kind of matter in,
in some, in some sense.
So, so, so certainly when you think that some things behave linearly,
um, then you just wanna preserve the linear.
Or if you think that it behaves quadratically,
then you wanna keep the feature but also add a squared term to it.
Okay. I wanna maybe move on, um.
Uh, these are all good questions,
happy to discuss more offline.
Um, so some other features might include,
the first message contains blank where blank is a string.
Right. So maybe things like, you know,
question marks are more indicative of you know, things being
the second message being a response.
Second message contains certain words.
Um, um, two messages both contain a particular word.
Um, you know, there's cases where, um,
it doesn't really m- it's not the presence and absence of
particular words in the- in individual, um, messages.
But like the fact that they both share a common word,
you know, that might be useful.
Um, here's another feature which is,
you know, two meshes have the,
um, some number of common words together.
Um, so this feature is kind of interesting because it's,
um, there's, you know, the,
the- for example you look at this feature,
it's how the number of- when I say feature,
I actually mean feature template.
Um, so for this feature template, um,
there are many, many features,
one for possibly any number of words.
And this again leads to cases where you might have a lot of,
um, you know, sparsity and you might not have enough data to fit all the features.
Whereas, this one is very compact.
That says, I just have to look at the,
um, number of overlap.
So, er, the, the two messages might contain a word that I've never seen before,
but I know it's the same word and I can kind of recognize that pattern.
Um, so, you know, there's quite a bit of things you can do
to play around with features that capture,
um, you know, the intuitions about what might be relevant to your task. Question. Yeah.
We have a lot of these sparse features like the working different point here.
Is that when we want to do like dimensionality reduction,
like knockout some of those many, many features?
Um, so the question is when you have a lot of sparse features,
do you wanna do dimensionality reduction?
Um, not necessarily.
Um, so in terms of computation,
having sparse features it doesn't necessarily mean that it's gonna be,
you know, really slow, um,
because there's efficient ways of, um, you know,
representing sparse features, um,
in terms of, you know, expressivity,
one thing that, um,
in a lot of NLP applications,
you actually do want a lot of features.
Um, and you can have a lot more features than you might think you can handle.
Um, and because you really wanted, the first orbit is just to,
you know, be expressive enough to even fit the data.
Yeah. Okay. Let me move on,
um, since, you know, I'm running short on time.
Okay. So summary so far,
you know, uh, we're looking at features.
We can define these feature templates which organize these,
uh, features, uh, in a kind of meaningful way.
And then we talked about hypothesis classes which are,
are defined by features.
And this defines what is possible f- out of, uh, from learning.
Um, and all of this in the context of linear classifiers
which incidentally can actually produce
these nice non-linear decision, you know, boundaries.
[NOISE] So at this point you can actually have kind of enough tools to,
um, you know, do a lot.
Um, but in the next section,
I wanna talk about neural networks because, um,
these are even more expressive models which can be,
you know, more powerful.
Um, um, one thing I,
I often recommend is that,
um, you know, when you're given a problem,
you know, always try the simplest thing.
I will always try kind of a linear classifier and just
see where it gets because sometimes you'd be surprised at how,
uh, far you can get with linear classifiers.
And then, and then go and kind of increase the complexity as you need it.
I know there's sometimes this temptation to, you know,
fire the fancy new shot, um, you know,
uh, hammer, but, um,
sometimes keeping it simple is,
you know, really, really good.
Okay. So neural nets, um.
There's a couple of ways of motivating this, um,
one motivation is, um,
you know, comes from the brain.
Um, I'm going to use a kind of slightly different, um,
motivation which comes from,
um, kind of this idea of decomposing a problem,
you know, into parts, right.
So this is a somewhat contrived example, but hopefully,
it'll allow us to build up the intuitions for, you know,
what's going on in a neural network. Um, okay.
So suppose I am building some sort of,
uh, system to detect whether two cars are gonna collide.
Okay. So the way it works is I have this car at position x_1 and it's,
you know, driving, uh, this way.
And then I have another car at position x_2 and it's driving this way.
And I want to determine whether it's safe,
um, which is positive or it's- if it's gonna collide.
Okay. And let's suppose for, uh,
simplicity that the true function is as follows.
Okay. So it's just measuring whether the distance is at least 1 apart.
Now th- this is kind of a little bit,
uh, you know, s- like what we did in, uh,
the last lecture where we
suppose there was a true function and then see if learning can recover that,
um, where in practice,
obviously we don't know the true function,
but this is for- kind of pedagogical purposes.
Okay. So just to kind of making sure we understand what function we're talking about.
So if, um, x_1 is 1 and x_2 is 3,
um, kind of like that on the board,
then here, plus 1.
So this is like driving in the US.
This is like driving in the, er,
UK. Um, and that's fine too.
Um, but if you're,
um, uh, you know,
too close together then that's bad news.
Okay? All right.
So let's think about decomposing the problem, right.
Because if you look at this, you know, this,
this could be a kind of a complicated, um,
you know, function, but let's try to break it down into kind of linear functions, right.
Because at the end of the day,
neural networks are just a bunch of linear functions with,
um, which are stitched together with some nonlinearities.
So like there are a kind of linear components that are,
um, critical to neural nets.
Okay. So one subproblem is detecting if car 1 is to the far right of car 2.
Okay. So x_1 less x_2 is greater than or equal to 1.
Um, another problem is testing whether car 2 is far right of car 1.
And then, um, and then you can put these together by saying, um,
if at least one of them is, you know, 1,
then I'm going to predict safe,
um, otherwise I will predict, uh, not safe.
Okay. So here's the kind of concrete examples.
So for 1, 3, uh,
car 2 is far right of car 1. So that's a 1.
You add these up, take the sign, that's plus 1,
in the opposite direction it's still fine.
And in this, this case both h_1 and h_2 are 0,
so that's, uh, bad news.
Okay. So this is just kind of trying to take
this expression which is a true function and kind of write it in,
uh, kind of a more modular way,
where you have different pieces corresponding to different competitions.
Okay. So now, um, we,
we could just write this down,
obviously to solve this problem but th- we already knew what the right answer was.
But suppose we didn't know what the true function is and we just had data.
So, so we don't actually know what these functions are.
So can we kind of learn,
learn these functions automatically?
So what I'm gonna do is I'm,
I'm gonna define a feature vector now,
um, of x which is gonna be a 1, x_1, x_2.
Okay. Um, and then I'm going to rewrite this intermediate subproblem as follows.
So x_1 is x_2 greater than 1,
is going to be represented as this,
uh, vector v_1.v of x,
where v_1 is minus 1 plus 1 minus 1.
So you can- you pause for a second.
Um, you can verify that this is x_1 minus x_2,
you know greater than equal to 1.
Okay. So this is just another way of writing, um, you know,
what we wanted in terms of this like dot product and you can see kind of
how this is maybe moving more towards something that looks more general. Yeah.
Why is that 1 there?
So the question, why is there this 1 here?
Um, so this 1 typically is known as a bias term which allows you to,
um, not just, uh, you know,
threshold on 0, but threshold on,
uh, any arbitrary number.
So in the linear classifiers that I've,
you know, talked about, I've kinda swept this under the rug.
Generally, you always have a bias term that allows you to kind of modulate
how likely you're gonna pre- predict 1 versus, uh, minus 1.
Okay. So you can also do it for h_2.
It's the same thing, but just, um,
you know, switching the roles of x_1 and x_2.
Um, and now also the first sign of final sign prediction,
you can write it as follows.
Um, now th- these are just weights on,
um, h_1 and h_2.
Okay? So now, here is the,
the kind of the punchline, is,
you know, for a neural network,
we're just going to leave v_1, v_2,
and w as unknown, uh,
quantities that we're going to try to,
uh, fit through training.
Right. We motivated this problem by saying, okay,
in this case, there is some choice of v_1,
v_2, w that works.
But now we're kind of generalizing.
If we didn't know these quantities,
we can just leave them as variables and we can actually still fit them- fit these parameters.
Okay. So, um, before we were just tuning w,
and now we're tuning both V and w. V specifies the choice of
the hidden problems that we're interested in and
w governs how do we take the results of the hidden problems and,
uh, come to a final prediction.
Okay. So there's one problem here,
which is that if you look at the gradient of h1 with respect to v1,
um, it happens to be 0, okay?
So if you look at, um, the, uh,
horizontal axis is v1 dot Phi of x and the vertical axis is h1,
um, that function, um,
is- looks like the step function, right?
Because indicator function of some quantity greater than or equal to 0.
It's 1 over here, 0 over here.
Um, and remember, we don't like 0 gradients because SGD doesn't work.
So the solution, um,
here is to, um,
take some sandpaper, um,
and you, you know,
sand out this function to smooth it out and,
uh, then you get something that is,
um, you know, differentiable.
So, uh, the logistic function is this function which is,
um, a smoothed out version of this, which, um, rises.
So it doesn't hit 1 or 0 ever,
but it becomes extremely close.
But it kind of, um, goes up in the, in the middle.
And you could think about this as, um, a differentiable,
um, or I, I guess a smooth version of,
uh, the step function, okay?
So it kinda behaves and looks like the step function.
It serves kind of the same intuition that you're
trying to test whether some quantity is greater than 0,
but it doesn't have 0 gradients anywhere, okay?
And you can double-check. If you take the derivative,
then this is actually- has this kind of really interesting nice form,
which is the value of the function times 1 minus the value of the function.
And the value of the function never hits 0,
so this quantity never hits 0, okay?
So, so now we can define,
uh, neural nets in contrast to linear functions.
So remember, linear functions, um,
we can visualize it as, um,
inputs go in, um,
and each of the inputs gets, um,
weighted by some, uh,
w and you get the score, okay?
[NOISE] So this is what a linear- what a linear function looks like.
Now, neural networks with one hidden layer and two hidden units,
1, 2, looks something like this where you have, um,
these intermediate hidden units,
which are the sigmoid function, um,
applied or logi- logistic function in this case in, uh,
to be concrete, um, applied to,
um, this wave vector Vj times Phi of x.
So h1 is, uh,
going to be taking the input and multiplying it by
a vector of- and you get some number here,
and then you send it through this,
um, logistic function to get some number.
And then finally you take the output of h1 and h2 and you, uh,
take the dot product with respect to w,
and then you get the final score, okay?
So again, the intuition is that neural nets are trying to
break down the problem into a set of,
you know, subproblems where you- the subproblems are the kind of the,
the result of these intermediate computations.
[NOISE] And you can think about these as like, you know,
h1 is really kind of the output of a mini linear classifier.
h2 is the output of a mini linear classifier.
And then you're taking those outputs and then you're, you know,
sticking them through another linear classifier and getting the score.
So this is what I mean by,
you know, at the end of the day,
it's kind of linear classifiers packaged up and strung together.
And their expressive power comes from,
from the kind of the composition. Um, yeah, question.
Phi h sub j when there's like multiple Phi, like, how do you combine them?
Uh, the question, how do you get h sub j when there's multiple Phis?
There's only one Phi of x. Oh,
so this is, this is the first component of Phi of x.
So this vector, this,
this is a three-dimensional vector,
which is Phi of x.
And it has three components.
Yeah. Yeah.
[inaudible] uh, isn't that effectively features, kind of?
Yeah.
Then they're like, they're like the- like, I mean,
some kind of function of the original features that you've put
in and they make the new features that are better than the ones before?
Yeah. [NOISE] Yeah. So that's my- kind of my next point,
which is that, um,
one way you can think about it is that the hjs are actually just, you know,
features which are learned automatically from data as opposed to having,
a fixed, uh, set of your features Phi, right?
Because at this layer,
w always sees these, you know,
hs which are coming through which look like,
you know, uh, features.
Um, and for deeper neural networks,
you kind of just keep on stacking this.
So, you know, this output of one set of classifiers becomes the features to
the next layer and then the output
of that class sort of becomes the features to the next layer, and so on.
Um, and the intuition for,
you know, deeper networks, um,
is that, you know,
as you proceed you can, uh,
derive more abstract, you know, features.
For example, images.
You start with pixels and then you find kind of the edges,
and then you define kind of object parts,
and then now you define kind of, uh,
things which are closer to the actual classification problem.
Yeah. [NOISE]
What if you wanted h2 to develop the exact same value,
like, do you have to have a bias to start with?
Ah, yeah. That's a good question.
So why don't h1 and h2 do, uh,
basically end up in the same place because,
you know, because of symmetry?
Um, if you're not careful that will happen.
So if you initialize all your weights to 0 and, uh,
or initialize these weights the same way then,
um, they will be kinda moving in locks- lockstep.
Um, so what is typically done is you randomly initialize.
So they're, kinda, you break the symmetry.
And then what the network is going to do is it's trying to, um,
use- learn auto- it kind of automatically learns these subproblems to,
uh, be kind of complementary because you're doing this joint learning.
[NOISE] Yeah. Final question then.
How do you choose the Sigma function? [NOISE]
Uh, how do I choose the Sigma function?
Um, so this is- so in general,
sigmoid functions are these or activation functions are these nonlinear functions.
So the important thing, uh, it's, it's a nonlinear function.
Um, I chose this particular logistic function because it's kind of the classic, um,
neural net and it looks like the step function,
which is kind of, uh,
takes the score and outputs,
uh, a classification result.
I should, you know,
responsibly note that, um,
these are, um, maybe, uh,
less in style than they used to be.
And the, the cool thing to do now is to use,
uh, what is called a ReLU or a rectified linear,
which looks like this.
Um, and you might ask, like, why this one?
Um, well, there's no one reason, but, um,
this, um, this function has less of a kind of this,
um, gradient going to zero problem.
It's also simpler because it doesn't require exponentials.
Um, but there's, um, um,
I'm gonna just leave it at that.
[NOISE]
[BACKGROUND]
What- the benefit of this function is, uh,
pedagogical reasons and it's a little bit of a throwback too.
[NOISE] Um, okay.
[NOISE]
Yeah, if you read the notes in the lecture slides,
there's more details on, like,
why you would like change, choose one versus another.
Okay, so now we're kind of ready to do neural net learning, right?
So- okay, remember we have this optimization problem,
it's, the training loss now depends on both V and w,
and a training loss remember,
is averaged over the losses of individual examples,
uh, the loss of the individual example,
let's say we're doing regression,
is the square difference between y and the function value,
and remember the function value is the summation over the- the weights at the last layer,
times the activations of the hidden layer,
and- and that's the basic idea, okay?
And now all I have to do is compute this gradient.
Um, so you look at this and you say okay, well,
if you get- have,
enough scratch paper, you can probably like, work it out.
Um, I'm gonna show you,
a different way to do this,
um, without grinding through the chain rule.
Um, so this is going to be based on the computation graph,
which will give you, um,
insight- more additional insight into the kind of the structure of computations,
and visualize what it means,
what does a gradient kind of mean in some sense?
And it also happens that these computation graphs,
is really at the foundation of all of
these modern deep learning frameworks like TensorFlow and PyTorch.
So, um, this is a real thing.
Um, it turns out that we've taught this it,
many people still kinda prefer,
uh, to grind out the amount.
I can't really tell why,
except for maybe you're more familiar with that,
and so I would encourage everyone to kind of at least try to, um,
think about the computation graph as a way to understand your gradients,
even though initially it might not be faster.
And it's not to say that you always have to draw a graph, um,
to compute gradients, but doing a few times
might give you additional insight that you wouldn't otherwise get.
Okay, so here we go. Um, so functions,
we can think about them as just boxes, right?
The boxes you have some inputs going in,
and then you get some output.
That's all a function is, okay?
And partial derivatives or you know, gradients asked the question- the following question,
how much does the output change if the input changes a little bit?
Okay? So for example if we have this function,
that just computes two times in1 plus in2in3.
Um, you ask the question like,
you take input one and you just add a little epsilon.
So like 0.001.
And you ask hmm,
and- and you sti- uh,
read out the output,
and you say, "Well, what happens to the output?
While in this case, uh,
the output changed by 2 epsilon additively.
Okay? So then you conclude that the gradient of
this function with respect to in1 is, is what?
[NOISE].
2.
2, right?
Because the gradient is kind of the amplification.
If I put an epsilon, then I get 2 epsilon out,
the gradient is 2, or the partial derivative.
So okay, let's do this one.
So if I add epsilon to in2,
then I- simple algebra shows I get a- a change in,
in3 epsilon, so what's, um,
the partial with respect to in2?
In3, right? Okay, good.
So you know, you could have done basic calculus and gotten that,
but I- I really kind of want to stress the kind of interpretation of, you know,
perturbing inputs and witnessing the output,
because I think that's a useful, um, interpretation.
Okay, so now, um,
all functions are- well,
not all functions are made out of building blocks,
but most of the functions that we're interested in,
in this class are going to be made out of these- these five pieces, okay?
And so for each of these pieces,
it's you know, it's a function,
it has inputs, a and b,
and you pump these things in and you get some output.
Um, this, so there is a plus,
minus, times, max, and the logistic function.
Okay, so on these edges,
I'm going to write down in green
the partial derivative with respect to the input that's going into that function.
Okay? So let's do this.
So if I have the function a plus b,
the partial derivative with respect to a is 1,
and the partial derivative with respect to b is 1, okay?
And if you have minus,
then it's 1 and minus 1,
um, if you have times,
then the partial is b and a, okay?
Everyone follow so far, okay?
Okay so max, uh, what is this?
This is maybe a little bit, you know, trickier.
Um, so remember we kind of experienced the max last time.
So when the max, um,
example you have, uh,
a formula, just refresh.
Uh, uh,
so- so remember our last time we had the- we saw the max in the context of,
um, uh, the- the hinge loss, right?
So you have the max of these two functions, which is this,
which means that, um, you know,
let's say one is- one is a and the other is b. Um,
so if a is greater than b, then the, um,
then we need to take the derivative with, uh- sorry,
then, uh- Okay, let me do it this way.
Okay, um, ig- ignore that thing on the board.
So I just have max a of b, okay?
So suppose a is,
uh, 7 and b is, uh, 3.
Uh, okay, so max, uh,
a and b and let's say this is 7 and this is 3,
so that means a is greater than b.
So now, if I change,
um, a by a little bit,
then that change is going to be reflected by an output of a max function, right?
Because this, uh- this region is small and it doesn't matter.
And, um, in this case,
if I change b by a little bit,
then does the output change?
No, because like, you know, 3.1,
2.9 is all, the output doesn't change,
so the gradient is going to be 0 there.
So the max function is partial derivatives, look like this.
So if a is greater than b,
then this is going to be a 1,
if a is less than b,
this is going to be a 0 and you know, conversely over here,
if b is greater than a,
then this is going to be a 1,
if b is less than a,
then this is going to be a 0.
Okay? So the partial of maximum,
there's always 1 or 0 depending on this particular co- you know, condition.
Okay, and then the logistic function, um,
this is just a fact you can derive it in your,
you know, free time but I had on a previous slide.
It's just like the sigmoid, um,
uh, logistic function, times 1,
minus the logistic function.
Okay so now you have these building blocks,
now you can compose and you can build castles out of them.
It turns out like all- basically all functions that you see in,
you know, you know, deep learning are just
basically bail- built- built out of these blocks.
Um, and how do you compose things?
Um, there's this nice, uh,
thing called, the chain rule, which says that,
"If you think about input going to one function
and that output going into input in a new function,
then the partial derivative with
respect to the input of the output is just the product of the partial derivative."
This is just the chain rule, right?
And you can think about as like- you know, think about amplification.
So this function amplifies by two times,
and this amplifi- this function amplifies by 5,
then total amplification is going to be 2 times 5, okay?
All right, so now let's take an example,
we're going to do, uh,
binary classification with the hinge loss, um,
just as a warm-up, um,
and I'm going to draw this computation graph,
and then compute the partial derivative with respect to w. Okay,
so what is this graph?
Um, so I have w times Phi of X,
that's a score, times y, that's a margin,
1 minus margin, um,
max of 1 minus margin 0 is a loss, okay?
So now for every edge I can draw the partial, uh, derivative, okay?
So here remember the partial derivative here is,
uh, left-hand-side greater than d or the right- the right branch.
So 1 minus margin greater than 0.
Um, for minus, this is a minus 1.
For a times, this is going to be whatever is over here.
Uh, for this times,
it's going to be whatever is over here.
And by the chain rule,
if you multiply what's on all the edges,
then you get the gradient of the loss with respect to w.
Okay. So this is kind of a graphical way of doing what you,
you know, probably wha- what I did last time, which is,
um, if the margin is, um, uh,
less than- greater than 1,
then it's- everything is 0.
And if the margin's less than 1 then I'd perform this, uh, particular update.
Okay? So in the interest of time,
um, I'm not going to do it for the simple neural network.
Uh, I will do this in section.
But, you know, at a high level,
you basically do the same thing.
You multiply all the, you know, blue edge, uh,
the edges and you get the- the, uh, partial derivatives.
Okay. So- so now,
you know, we've kind of done everything kind of manually.
I wanted to kind of systematized this
and talk about an algorithm called back-propagation,
uh, that, um, allows you to compute gradients for arbitrary computation graph.
That means, any kind of, uh,
function that you can build out of these building blocks,
you can actually just get the derivatives.
So, you know, one nice thing about these packages like PyTorch or TensorFlow is that,
you actually don't have to compute the derivatives on your own.
It used to be the case that,
you know, uh, before these,
people would have to crank- implement this derivatives by- by,
um, hand, which is really tedious and error prone.
And part of why it's been so easy to kind of
develop new models is that all that's done for you automatically.
Okay. So back-propagation is gonna compute two types of values;
a forward value and a backward value.
So f_i for every, um,
node I is the simply the value of that expression tree.
And, um, the backward value,
g_i,is going to be the partial derivative with respect to output of,
uh, that- the value at that node.
Okay? So for example,
f_i here is gonna be,
um, w_1 times, uh,
um, Sigma v_1 times, uh, phi of x.
And g of that node is going to be, uh,
the, basically the product of all these edges.
Basically, how much does this node change the output at the fin- uh,
at- at the very top.
Okay. So the algorithm itself is- is,
you know, quite straight forward.
There is a forward pass which computes all the f_i's,
and then there's a backward pass that computes all the g_i's.
So in the forward pass,
you start from the leaves and you go to the root,
and you compute each of these,
uh, values kind of recursively.
Where the computation depends on,
you know, the sub-expressions.
Um, and in the backward pass,
um, you, similarly have a recurrence that, uh,
gives you the value of a particular- a g_i of a particular node is equal
to the g_i of its parent times whatever is on, um, this edge.
Okay? So it's like you take a forward pass,
you fill in all the f_i's and then you take a backward pass,
and you fill in all the g_i's that you care about.
Okay? All right.
So section will go through this in, uh, detail.
I realize this might have been a little bit quick.
Um, one quick note about optimization is that,
now, you have all the tools that you can do,
you can run SLG on in which doesn't really care about whether you're,
um, you're, you know, what the function is.
It's just like a function. You have it,
you can compute the gradient, that's all you need.
But one kind of, eh, important thing to note is that just
because you can compute a gradient doesn't mean you optimize the function.
So for a linear function,
it turns out that if you define these loss functions on top,
you get these convex functions.
So convex functions are these functions that you can hold in your hand,
and, eh, um, and have a one global, uh, minimum.
And so if you think about SLG,
it's going- going downhill.
You converge to the global minimum and you solve the problem.
Whereas neural nets, it turns out that the loss functions are non-convex,
which means that if you try to go downhill,
you might get stuck in local optima.
And in general, optimization of neural nets is hard.
In practice, people somehow managed to do it anyway and it works.
There's a gap between theory and practice which is
an active area of research.
Okay. So in one minute,
I I have to do nearest neighbors.
[BACKGROUND] Um, it will actually be fine because nearest neighbors is really simple,
so you can do it in one minute. So here it goes.
Um, so let's throw away everything we knew about linear classifiers in neural nets.
Here's the algorithm. You're training as you store your training examples.
That's it. And then,
the predictor of a particular example
that you get is you're gonna go through all the training examples
and find the one which is closest- has input which is closest to your- uh,
your, um, input x prime.
And then you're just gonna train- you're gonna return, um, y.
Okay? So, um, and the intuition here is that
similar examples- it's similar inputs should get similar outputs.
Okay? So here's an, uh, pictorial example.
So suppose we're in two dimensions and you're doing classification and [NOISE] you have,
a plus over here.
Um, let's do this plus and you have,
um, you know, [NOISE] a minus here.
Okay? So if you are asking what is the pro- uh,
label assigned to that point,
it should be plus because this is closer.
Um, this should be minus.
This region should be minus.
This should be plus. And, [NOISE] you know,
one kind of cool thing is that is, where's the decision boundary?
So if you look at the point that is equidistant from these,
and draw perpendicular, um,
that's the decision boundary there, um,
same thing over here, um, and, uh,
so you have basically carved out this region where this [NOISE] is minus and,
[NOISE] um, everything here is [NOISE], you know, plus.
Okay? [NOISE] Um, in general, this is, um,
what I've drawn is an instance of
a Voronoi diagram which if you're given a bunch of points,
um, the defined regions of points which are closest to that point.
And everything in a particular region like
this yellow region is assigned the same label as,
um, this point here.
And this- this is, um,
what is called a non-parametric model which means that,
the number- it doesn't mean that there's no parameters.
It means that the number of parameters is not fixed.
The more points you have,
the more kind of each point has its own parameter.
Um, so you can actually fit really expressive models, um, using that.
It's very simple, uh,
but it's kind of computationally expensive because you
have to store your entire training examples.
Okay. So we looked at three different, um,
models and, you know,
there's a saying that well, and,
uh, I guess in school,
you- there's three things study, sleep,
and party or something, and you have to only pick two of them.
Well, so for learning, it's kinda the same.
It can either be fast to predict for linear models and neural nets.
Um, it can be easy to learn for linear models and
uh, um, nearest neighbors or it could be powerful.
For example, like neural networks and nearest neighbors but there's
always some sort of compromise and exactly what method you choose,
um, will depend on kind of what you care about.
Okay. See you next time.
 Homework 1, hard deadline, uh, section.
Tomorrow, um, we're gonna go through
the backpropagation example which I went through very briefly in the last lecture.
Talk about diverse neighbors,
which I did in one minute.
And also, we're gonna talk about scikit-learn,
which is- it is a really useful tool for doing
machine learning which might be useful for your final projects, good to know.
So, uh, please come to section.
All right, let's, ah, do a little bit of review of where we are.
So we've talked about, ah,
we're talking about machine learning in particular supervised learning where we
start with feature extraction where we take
examples and convert them into a feature vector,
um, which is more amenable for our learning algorithm.
Um, we can take either linear predictors or neural networks which gives us scores, um,
and the score is either defined via a
simple dot product between the weight vector and the feature vector,
or some sort of more fancy non-linear combination.
At the end of the day,
we have these model families that gives us you know, score functions
which then can be used for classification regression.
We also talked about loss functions as
a way to assess the quality of a particular predictor.
So in, ah, linear classification,
ah, we had the zero,
one loss and the hinge loss as example of loss functions that we, ah, might care about.
Ah, the training loss is an average over the losses on individual examples.
And to optimize all this,
ah, we can use, ah,
the stochastic gradient algorithm which takes an example x, y,
and, ah, computes the gradient on that particular example and then just updates,
ah, you know the weights based on that.
Okay. So hopefully this should be all, you know, review.
Okay. So now, I'm going to ask the following question.
You know, let's be a little bit philosophical here?
So what is the true objective you know, of machine learning?
So how many of you think it is to minimize the error on the training set?
Show of hands. No one?
This is what we've been talking about, right?
We've been talking about minimizing your own training sets.
Okay, well, uh, maybe that's,
um, maybe that's not right then.
Um, what about minimizing,
ah, training error with regularization?
Because regularization is probably a good idea.
How many of you think that's, ah, that's the goal?
What about minimizing error on the test set?
Okay, seems like it's closer, right?
You know, the test sets.
Test accuracies are things maybe you care about.
Um, um, what about minimizing error on unseen future examples?
Okay. So the majority of you think that's the right answer.
What about, ah, learning about machines,
and that's the true objective.
Who doesn't want to learn about machines?
That's actually the true objective.
Now, um, so the correct answer is minimizing error on unseen future examples.
So I think all of you have an intuition
that we are doing some machine learning, we're learning on data,
but what we really care about is how this predictor performs on in the future.
Because we're going to deploy this in a system,
and it's going to be the future, it's going to be unseen.
Um, and then- but then- okay,
so then how do we- do you think about all these other things?
You know training set, regularization tests.
So that's going to be something we'll come back to, um, later.
Okay. So there's two topics today,
I wanna talk about generalization,
which is I think a pretty subtle but important thing
to keep in mind when you're doing machine learning.
Um, and then we're going to switch gears and talk
about unsupervised learning where we don't have labels,
but we can still, um, do something.
So we've been talking about training loss, right?
We've- you know, I've made a big deal about,
you write down what you want,
and then you optimize.
So the question is like,
ah, is this training loss a good objective function?
Um, well, let's take this literally.
Suppose we really wanted to just minimize the training loss, what would we do?
Well, here- here's an algorithm for you.
So you just store your training examples, okay.
And then you're going to define your predictor is as follows.
So if you see that particular,
ah, example in your training set,
then you're just going to output, um, ah,
the out- the output that you saw in the training set.
And then otherwise, you're just going to segfault.
And this is going to crash,
[BACKGROUND] right? So this is great.
It minimizes the training error perfectly.
It gets zero loss assuming your training examples don't have, ah, conflicts.
But you know you're all laughing because this is clearly a bad idea.
So somehow purely following this minimizing training set objective is,
ah, error objective is not really the right thing.
Um, So this is an example- a very extreme example of overfitting.
So overfitting is this phenomena that you see where you, ah,
have some data and usually the data has some noise,
and you are trying to fit a predictor,
but you're really trying too hard, right?
So if you're fitting this,
um, green squiggly line,
you are fitting the data and getting zero training error,
but you're kinda missing the big picture which is you know this black curve,
or in grant regression.
Some of you you've probably seen examples where you
have a bunch of points usually with noise,
and if you really try hard to fit the points and you're gonna get zero error,
but you're kinda missing this general, ah, trend.
And overfitting can really kind of bite you if you're not careful.
So let's try a formula- formalize this a little bit more.
How do we assess whether a predictor is good?
Because if we can't measure it,
we can't really, um, you know optimize it.
Okay. So, um, the key idea is that we
really care about error on unseen future examples, okay?
So this is great as you know,
uh, aspiration to write down.
But the question is how do we actually you know, optimize this, right?
Because it's the future and it's also unseen.
Um, so you know high-definition, we've had, get a handle, get a handle on this.
Um, so typically what people do is, ah,
the next best thing which is you gather a test set,
which is supposed to be representative of all the types of things you would see.
And then you guard it, uh, carefully,
and make sure you don't touch it too much, right?
Because, you know what happens if you, ah,
start looking at the test set and- you know,
or the worst case that you train on a test set, right?
So you know, the test set being a surrogate for unseen future examples,
um, just completely goes away, right?
And even if you start looking at it and you're- you're really trying to optimize it,
um, you can get you know,
into this overfitting regime, right?
So really be careful of that.
And I want to emphasize that the test set is
really a surrogate for what you actually care about.
So, um, don't blindly just, you know,
try to make test accuracy numbers go up at all costs, okay?
Okay. So, um, but for now let's assume we have a test set though,
you know, we have to work with.
So there- there's this kind of really peculiar thing about machine learning,
which is this leap of faith, right.
You- the training algorithm,
um, is only operating on a training set.
And then all of a sudden,
you go to these unseen examples or the test set and you're expected to do well.
So why is there- why would you expect that, you know, to happen?
And as I alluded to on the first day of class,
there is some kind of actually pretty deep mathematical reasons
for why this might happen,
um, but, you know,
rather go and get into the math,
I just kinda wanna give you a maybe intuitive picture of how to think about,
um, this- this gap.
Okay. So remember, ah,
we had this picture that it's,
ah, of all predictors.
So these are all the functions that you could possibly want in your wildest dreams.
Um, and then when you define,
um, a feature, ah,
extractor or a neural net architecture or any sort of,
um, you know, a- a structure.
You're basically saying, "Hey I'm only
interested in these sets of functions, not all functions."
Okay. And then learning is, um,
trying to find some element of the- the class of functions that you've,
ah, set out, ah, to find.
Okay. So there's a decomposition which is useful.
So let's take out this point G. So G is going to be the best,
um, function in this class.
The best predictor that you can possibly get.
So if some oracle came and set your neural net weights to something,
how well could you do?
Okay. So now there's two gaps here.
One is approximation error.
Approximation error is the difference
between F star which is the- the true, ah, predictor.
So this is the thing that always gets
the right answer and G which is the best thing in your, ah, class.
Okay. So this really measures how good is your hypothesis class.
Remember last time we said that,
we want hypothesis class to be expressive.
If you only have linear, ah,
functions and your data looks, ah,
sinusoidal then that is not expressive enough to capture, um, the data.
Okay. So the second part is estimation error.
This is the difference between the best thing in
your hypothesis class and the- the function you actually find.
Right. And this measures how good is
a learned predictor kind of relative to the potential of the hypothesis class.
You define this hypothesis classes,
um, here are things that I'm willing to, you know,
consider but at the end of the day based on a finite amount of data,
you- you can't get to G. You only can kind of estimate,
um, you know, some- you do a learning and you get to some F hat.
So, um, in kind of more mathematical terms,
if you look at the error of the thing that your learning algorithm actually
returns minus the- the era of the best thing possible which,
you know, in many cases is, you know, zero.
Um, then this can be written as follows.
So all I'm doing is minus- subtracting error of G and
adding error of G. So this is the same quantity as this,
um, but then I can look at these two terms.
So the estimation error is the difference in error
between the thing that your learning algorithm produces minus
the best thing in the class G. And then
the difference- approximation error is the difference between
the error of G and error of F star.
Okay. So this is going to be useful as a way to kind of conceptualize,
um, the different trade-offs.
Right. So, you know,
just to kind of explore this a little bit.
Suppose I increase the hypothesis class size, right,
so I add more features or I, you know, add,
ah, increase the dimension of my neural networks.
Um, what- what happens?
So the approximation error will go down. And why is that?
Because we're taking a minimum over a larger set.
So, um, G is always the minimum possible error
over the set F. And if I make the set larger,
I have just more possibilities of driving the error down.
Okay. So the approximation error is gonna go down,
um, but the estimation error is going to go up, right?
As I make my hypothesis class more expressive.
And that's because it's harder to estimate something more complex.
So I'm leaving it kind of vague right now.
There's a mathematical way to formalize this which,
um, you can ask me about offline.
Okay. So you can see there's kind of tension here.
Right, you really want to make your hypothesis class large so you can, um,
drive down the approximation error but you don't want to make it too
large that it becomes impossible to, um, estimate.
Okay. So now we have this kind of abstract frame work.
What are some kind of knobs we can tune?
How do we control the size of the hypothesis class?
So we're gonna talk about two essentially classes of,
um, types of ways.
So strategy one is, um, dimensionality.
So remember for linear classifiers,
a predictor is specified by a wave vector.
So this is D numbers, right?
And we can change D. We can make D smaller by removing features.
We can make D larger by adding features.
And, ah, pictorially you can think about as
reducing D as reducing the dimensionality of your- of your hypothesis class.
So if you are in three dimensions,
you have three numbers, three degrees of freedom,
you have this kind of a ball and you- if
you re- remove one of the dimensions now you have this,
ah, ball or a circle in two dimensions.
Okay. So concretely what this means, is, ah,
you can manually add uh, you know,
this is a little bit heuristic.
You can add features if they seem to be, you know,
helping and remove features if they don't, ah, help.
So you can, um, kind of modulate the dimensionality of your, ah, weight vector.
Or there are also automatic feature selection methods, um,
such as boosting or L1 regularization,
um, which are outside the scope of this- this class.
Um, to be- if you take a machine learning class you'll learn more about this, um,
this stuff but the main point is that, ah,
you can determine by setting the number of features you have.
You can, um, vary the expressive power of your hypothesis.
Okay. So the se- second strategy is, um,
looking at the norm,
or the norm, or the length of a vector.
So this one is maybe a little bit less, um, obvious.
Um, so again for linear, ah, predictors,
the wave vector is just, ah,
a d-dimensional vector and you look at how long this vector is.
So what is - and the length,
um, pictorially it looks like this.
So if you have, um,
let's say all the weight vectors in,
um, each W can be ex- thought about as a point as well.
So this circle contains all the weight vectors up to a certain length.
And if by making this smaller,
now you're considering, you know,
a smaller number of weight vectors.
Okay. So at that level it's, um, perhaps intuitive.
Um, so what does this actually look like?
Um, so let's suppose we're doing one-dimensional linear regression and here's the board.
Um, and we're looking at,
um, x y. Um,
so remember what- and in one-dimension, um,
we're- all we're looking at is,
um, you know, W is just a single number.
Right? And the number represents the slope of this line.
So by saying, um,
you know, let's draw some slopes here.
Okay. Um, so by saying that, ah,
the weight vector or the weight is a small magnitude,
that's basically saying the slope is,
ah, you know, smaller or closer to 0.
So if you think about, um, you know,
slope equals- let's say this is slope equals 1, so W equals 1.
So anything- anything let's say, ah,
less than 1 or greater than minus 1 is fair game.
And now if you reduce this to half,
now you're looking at a kinda a smaller, um,
window here and if you keep on reducing it,
now you're basically, um,
converging to, you know,
essentially very flat and constant functions.
Okay. So you can understand this two ways.
One is just that the total number of, um,
possible weight vectors you're considering,
it's just shrinking because you're putting more constraints.
They have to be, you know, smaller.
From this picture you can also think about it as,
what you're really doing is, um,
making the function, you know, smoother.
Right? Because, um, a flat function is kinda the smoothest function.
It doesn't kind of, you know,
vary too much and,
ah, a complicated function is one that can go, you know,
very- jump up very steeply and,
you know, for quadratic functions can also come down really quickly.
So you get a kind of very, ah, wiggly functions.
Those are- tend to be more complicated.
Okay. Any questions about this so far? Yeah?
Um, trying to not overfit.
So like what if we had like latent structures within the data set that sensor tra- that
says if you try to like not overfit we're really just kind of like this tricking
ourselves like a perpendicular set of like distributions that we say, "Okay.
This data must have like come from like something normal,
it must have come from something reasonable."
But saying that we're like not really capturing
the full- the full like scope of our data sets.
Um, I'm not sure, so let's see.
So the- so the question is if there's a
particular structure inside your data set, for example,
if some, um, things are sparse or low rank or something,
um, you know, how do you capture that with a regularization?
Regularization. But you have like,
perhaps not even just like price spikes like this.
Like if you have a causal model inside, inside
between your like parameters like how would
you like, would a regularization like impede some of those relations?
Oh yeah so um,
so all of this is kind of very generic, right?
You're not making any assumptions about
the, the what the classifier is or the features is.
So they're kinda like big cameras that you can just apply.
So if you have models where you have
more structural domain knowledge or if you, um, which we'll see.
For example if you have you know, Bayesian networks later in the class
then there's much more you can do.
And this is just kind of you know, two techniques
for as a kind of a generic way of controlling for overfitting.
Yeah.
Making sure I'm understanding correctly.
This approach is actually creating constraints on each element in
the vector W that the magnitude of it versus
the other one was actually counting elements in a potential vector W?
Yeah so um, so let's look at W here.
So let's say you're in 3-dimensions.
So W is W1, W2, W3.
So the first method just says okay let's
just kill some of these um, elements and make it smaller.
This one is saying that I mean formally,
it's looking at the squared values of each
of these and looking at the square root, that's what the norm is.
So it's saying that each of these should be,
you know, small according to this particular metric.
Yeah.
[NOISE] Yeah so that's what I'm going to get to.
So this is just kind of giving you intuition
for in terms of hypothesis classes and how you want them to be small.
How do you actually implement this?
Um, you know there's several ways to do this but
the most popular way is to add regularization.
And by regularization what I mean is take your original objective function which
is train loss of W and you just add this, um, penalty term.
So Lambda is called the regularization strength.
It's just a positive number let's say- let's say 1.
And this is the squared length of W. Okay,
so what is this doing is by adding this it's saying that,
"okay optimizer you should really try to make
a train loss small but you should also try to make this small as well."
Okay. And there's uh,
if you study convex optimization there's kind of this duality between, um,
this- this is called the Lagrangian form where you have a penalize objective where you
add a penalty on the weight vector and
the constraint form where you just say that I want to minimize
training loss with subject to the norm of W being less than some value.
But this is more of that kind of a typical one that you're going to see in practice.
Okay, so here's objective function.
Great. How do I optimize it?
Yeah, I think i will use the same W leading into the train
Yeah.
Okay, so the process of minimizing train loss is further minimized.
Yeah so it's important that these be the same W and you're optimizing the sum.
So the optimizer is going to make these trade-offs.
If it says, "Oh okay,
I can drive the training loss down.
But if this is shooting up then that's not good and it'll try to balance these two."
Yeah, [NOISE] it's basically saying try to fit the data but not at the expense of,
uh, having huge weight vectors.
[NOISE] Yeah, so if there's another way to say it is that,
um, kind of think about Occam's razor.
It's saying if there is a simple way to fit your data then you should just do that.
Instead of finding some really complicated weight vector that fits
your data, so prefer simple solutions. Okay.
So once you have this objective you know we have
a standard crank we can turn to turn this into an algorithm.
You can just do gradient ascent.
Um, and the you know if you just take the derivative of this then you have this gradient.
And then you also have Lambda W which is the gradient of this term.
So you can understand this as basically
you're doing gradient descent as we were doing before.
Um, and now all you're doing is you're shrinking the weights towards 0 by lambda.
So Lambda is a regularization strength.
If it's large that means you're trying to really kind
of push down on the magnitude of the weights.
So the gradient optimizer is basically going to say,
hey I'm going to try to step in a direction that makes the training loss small but then
I'm going to also push the weights towards 0.
Okay. In neural nets literature this is also known as a weight decay.
And in optimization and statistics it's known as
L2 regularization because this is the Euclidean or 2-norm.
Okay so here is another strategy
which intuitively gets at the same idea but it's in some sense you know, more crude.
So it's called early stopping.
And the idea is very simple.
You just stop early instead of going and training
for 100 iterations you just train for 50.
Okay. So why, why does this, why is this a good idea?
Um, the intuition is that if you start with the weights at 0, so that's the smallest
you can make the norm of W, right?
So every time you update on a training you know, set,
generally the norm goes up, you know there's no guarantee
that it will always go up but generally this is what happens.
So if you stop early that means you are giving less
of an opportunity for the norm to grow, grow.
So fewer updates translates to generally a lower norm.
You can also make this formal mathematically but the connection is not
as tight as the explicit regularization from the previous slide.
Okay, so the lesson here is you know try to minimize
the training error but don't try you know, too hard.
Yeah, question?
It depends on how we initialize the weights?
Question is, does this depend on how we initialize the weights?
Most of the time you're going to initialize the weights from you know,
some sort of weights which is kind of
a baseline either 0, or for neural nets maybe like
random vectors around 0 but they're pretty small weights and usually the weights
grow from outside from small to large.
There's other cases where if you think about
your pre-training, you have a pre-trained model you
start with some weights and then you do gradient descent from that.
Then you're saying basically don't go too far from your initialization.
Yeah.
This means that like want to [inaudible] like focus on the train loss
[inaudible] ?
Right. So the question is why aren't we focusing on minimizing the train loss,
or why focus on w?
It's always going to be a combination.
So the optimizer is still trying to push down
on the training loss by taking these gradient updates, right?
Notice that the, the gradient with respect
to the regularizer actually doesn't come in here.
It kind of comes in explicitly through the fact that you are stopping it early.
But it's always kind of a balance between, uh,
minimizing the training loss and also making sure your, um,
class- classifier weights doe- doesn't get too complicated. Yeah.
How do you decide what value of lambda of T to set as?
Yeah. So the question is how you decide the value of T here,
and how you decide the value of lambda?
[NOISE] So these are called hyperparameters,
and I'll talk a little bit more about that later.
Okay. So here's the kind of the general philosophy,
uh, that you should have in machine learning.
So you should try to minimize the training error,
because really, that's the only thing you can do.
That's your data, and that's,
you know, you have your data there,
but you should try to do so in a way that keeps your hypothesis small.
So try to minimize the training set error,
but don't try too hard.
I guess it's the, it's the lesson here.
Okay. So now, going back to the question earlier.
If you notice through all these, um, my presentation,
there's, there's all sorts of properties of the learning algorithm, you know,
which features you have,
which regularization parameter you have,
the number of iterations, the step size for gradient descent.
Um, these are all considered hyperparameters.
So, so far, they're just magical values that are given to the learning algorithm,
and the learning algorithm runs with them.
But someone has to set them,
and how do you set them?
[inaudible]?
Yeah. You can ask me,
uh, I don't know the answer to that.
[LAUGHTER] Um, okay.
So here- here's an idea.
So let's choose hyperparameters to minimize the training error.
So how many of you think that's a good idea?
Okay. Not too many.
So why is this a bad idea?
Yeah. You can over-fit, right?
So suppose you took, uh,
lambda and you say,
"Hey, um, you know,
let's choose the lambda that will minimize the training error."
Okay. And the, the learning algorithm says "Well,
okay, you know, I wanted to make this stat go down.
What is this doing in the way?
Let's just set lambda to 0,
and then I don't have to worry about this."
So it's kind of, um,
you know, cheating in a way.
And also, early stopping would say like,
don't stop, just keep on going because you're always going
to drive the training error lower and lower.
Okay. So that's not good.
So how about, um,
choosing hyperparameters to minimize the test error?
How many of you say, "Yeah, it's a good idea"?
Yeah. Not, not so good,
it turns out. Um, so why?
And this is again stressing the point that the test error
is not the thing you care about.
Because what happens when you look at that- uh, we,
we try to use a test set,
then it becomes an unreliable estimate of the actual unseen error.
Because if you're tuning hyperparameters on the test set, that means that,
um, it's no longer- it becomes less and less unseen and less future. Yeah.
[inaudible].
Yeah. So we could do cross-validation which I'll describe in a second.
Okay? So I want to emphasize this point.
When you're doing your final project,
you have your test set,
you have it sitting there, and,
uh, you should not be, you know,
fiddling with it too much or else,
um, it becomes less reliable.
Okay. So you can't use the test set, so what do you do?
So here's the idea behind, uh,
a validation set, it's that you take your training set,
and you sacrifice some amount of it, maybe it's, you know,
10% maybe 20%,
and you use it to estimate the test error.
So this is a validation set, right?
The test set is, you know,
off to the side, it's locked in a safe, uh, you're not gonna touch it.
And then, um, you're just gonna tune hyperparameters on the validation set,
and use that to guide your model development.
So, the, um, the proportion itself is not a hyperparameter?
The proportion itself, uh,
[LAUGHTER] is a hyper, hyperparameter.
You know, I- us- yeah,
you know, I usually don't tune that.
I mean, usually, it's- how you choose it is, um,
kind of this balance between you want
the validation set to be large enough so it gives you reliable estimates,
but you also want to use most of your data for training. Yeah.
How do you choose like lambda and like the other like T?
How do we choose those hyperparameters?
Yeah. So how do you choose the hyperparameters?
Um, so the, the answer is you try a particular value,
so, so you- for example,
try let's say lambda equals, um,
0.01 and 0.11, and then
you run your algorithm and then you look at your validation error,
and then you just choose the one that has the lowest.
Yeah. It's pretty crude but [NOISE] yeah.
[inaudible] and I got a hyperparameter without just doing like a,
like a, like a search, like try this one then try this one, then try this one.
Yeah. So how- is there a better way to search for hyperparameters?
Um, you could do, uh, your, er,
grid search generally is fine,
random sampling is fine.
There's fancier things based on Bayesian Optimization which might give you
some benefits but it's actually the jury's out on that and they're more complicated.
Um, there's also you can use better, um,
learning algorithms which are less sensitive to the step size.
So you don't have to nail it like, "Oh,
0.1 works but 0.11 doesn't."
So you don't- you don't want that.
But in all of the high-level answer is that there's no, um,
real kind of principled way of like here's a formula
that lambda equals and you just evaluate that formula,
and you're done, um,
because there's this is,
you know, the kind of the- uh, I don't know,
the dirty side of machine learning, there's always this tuning that needs to happen to get your,
you know, good results. Um, yeah. Question over there.
[inaudible] is this process usually automated or is this manual?
So the question is, uh,
is this process automated?
Increasingly, it becomes much more automated.
So, um, it requires a fair amount of compute, right?
Because usually, if you have a large data-set,
even training one model might take a little while.
And now, you're talking about, you know,
training let's say 100 models.
So it can be very expensive and there's things that you can do to make it, uh, faster.
But I mean, in general,
I would advise that don't hyperparameter tune kind of blindly,
especially when you're kind of learning the ropes.
I think doing it kind of manually and getting intuition for what, uh,
step size, um, like factor of step size algorithm is still valuable to have.
And then once you kind of get a hang of it,
then maybe you can automate.
But I wouldn't try to automate too,
you know, early. Yeah?
Small changes of hyperparameters need to vary big changes in prediction accuracy,
is that considered [inaudible]?
Yeah. So your question is, if you change the hyperparameters a
little bit and that causes your, um,
training or, or model performance to change quite a bit,
does that mean your model's not robust?
Uh, yeah, it means your model is probably not as robust.
And sometimes, you actually don't choose a hy- hyperparameter set at all,
and you still get varying,
you know, model performances.
Um, so, you know,
you should always check that first because there could be just inherent randomness,
especially if you're doing neural networks that could get stuck in local optimum,
there's all sorts of, um, you know, things that can happen.
Okay. Final question now so we can move on.
So we found out that the optimal hyperparameter, is it [inaudible]?
Uh, so how do you choose,
uh, an optimal hyperparameter?
So you basically have like a for loop that says for lambda in,
you known, 0.1, 0.011, whatever values for
t equals, uh, you know, something.
Um, you train on this- all these training examples with manual validation,
and then you test the model on the validation, you get a number,
and you just use, uh,
whichever setting gives you the lowest number.
[inaudible]?
I'm sorry?
We- we do have know the numbers it's not like the uh, [inaudible]?
Yeah. Usually, you just have to be in the ballpark.
You don't have to get like 99 versus 100.
The, the things I would just advise is like you know,
let's say what kind of orders of magnitude.
Because if it- if it really matters like being down to a precise number,
then, um, you probably have other worry- things to worry about.
Okay. Let's move on. So what I'm gonna do now is
go through a kind of a sample, uh, problem, right?
Because I think the, the theory of machine learning and the practice of it,
are actually kind of quite different in
terms of the types of things that you have to think about.
Um, so here's a simplify named entity recognition problem.
So named entity is this, uh,
recognition is this popular task in NLP where you're trying to find names of,
uh, people and locations and, um, organizations.
So the input is a string,
um, where, which has, you know,
a particular, potentially named with,
uh, the left and right context words.
Okay. And the, the goal is to predict whether, um,
this x contains, you know,
if they're a person,
um, which is plus 1 or not.
Okay. So, so here's the,
the recipe for success.
Um, when you're doing your fin- final project or something,
um, you get a data set, um,
it have- if it hasn't been already split,
split it into train, validation, and test,
and lock the test set away.
And then, first, I would try to look at the data to get some,
you know, intuition, you know.
Al- always remember, you want to make sure that you understand your, your data.
Don't just immediately start coding up the most fancy algorithm you can think of.
Um, and then you repeat.
You implement some, you know, feature,
maybe change the architecture of your network,
um, and then you tune some, you,
you set some hyperparameters and you run the learning algorithm,
and then you look at, uh, the,
the training error and validation error rates,
um, to see, you know,
how they're doing, if you're underfitting or overfitting.
Um, in some cases,
you can look at the weights for linear classifiers,
um, in, for neural nets it might be a little bit harder.
And then you- I will recommend look at the predictions of your model.
I always have- I always try to log as much information as I have.
You can, so that you can go back and
understand what the model is, you know, trying to do.
And then you brainstorm some improvements,
and you kind of do this until, uh, you either,
are happy or you run out of time,
and then you run it on that final test set and you get
your final error rates which you put in your, uh, report.
Okay? So let's go through an example of what this might, uh, look like.
Um, so this is going to be based on the code base for the sentiment homework.
Um, so, okay, so here's where we're starting.
We're reading, uh, a training set.
Let's look at this training set.
So there are, you know, 7,000 lines here.
Each line contains the label which is minus 1 or plus 1,
along with the input,
which is going to be,
uh, you know, remember,
the left context, the actual entity and the, the right context.
Okay? All right.
So you also have a development or validation set.
Um, and what this code does is,
eh, it's gonna learn a predictor,
which, uh, takes the training set and a feature extractor which we're gonna fill out.
Um, and then it's gonna output either, uh, both the,
the weights and, um,
some error analysis which you can look- use to look at the predictions.
And finally, there's this test which I'm gonna not do for now.
Okay. So, um, so the first thing is,
uh, let's define this feature extractor.
So this feature extractor is,
uh, Phi of x.
And we're gonna use the sparse, uh,
you know, map representation of, of features.
So Phi, Phi is, um,
there's this really nice community structure called defaultdict.
So this is kind of like saying,
you have, uh, you know,
uh, you know, a map,
but, um, you can't,
you know, access it.
And if the element is in there,
then you return zero.
Um, okay. So Phi equals that, and then you return Phi.
Okay. So this is the,
the simplest feature vector you can come up with.
Um, the dimensionality is zero because you have no features.x
Okay. So- but, you know,
we can run this and see how we do on this.
Okay. So let's run this.
Um, okay.
So over a number of iterations,
um, you can see that learning isn't doing anything because there's no way it's still updating.
Okay. So- but, you know,
it doesn't crash, uh, which is good.
Um, okay. So I'm getting 72%, uh,
error, which is, you know,
pretty bad but I haven't really done anything.
So, um, that's to be expected.
Okay. Where did my window go?
Okay. So now, let's, um,
start defining some, you know, features.
Okay. So remember, what is x?
X is something like, uh,
um, took Mauritius into, right?
So there's this entity on left and right.
So let's break this up.
So I'm going to, tokens equals x.split.
So that's gonna give me a bunch of tokens,
and then I'm going to define left entity, right equals.
So this is the- token zero is the left, that's gonna be took.
Um, tokens 1 through minus 1 is gonna be everything until the last token,
and then tokens minus 1 is the last one.
Okay. So now, I can define,
um, a feature template.
So remember, a good- nice way to go about it is to define a feature template.
So I can just say entity is, um, ent- blank.
Um, that's how I would've write- written it as a feature template.
In code, um, this is actually pretty, you know, transparent.
It's saying, I'm defining a feature which is going to be one,
um, for this, uh, you know, feature template.
So entity is gonna be some value,
I plug it in, I get a,
a particular feature value or feat- feature name.
And I'm gonna set that feature name to be- have a feature value of 1.
Okay. So let's, uh, run this.
Okay, so, um, let's go over here, run it.
Uh, oops. Um, so entity is,
uh, a, a list.
So I'm going just gonna turn it into a string.
[NOISE] Okay.
So now, I'm getting, uh, what happened?
So the, um, the training error is,
uh, pretty low, right?
I'm basically fitting the training error pretty,
uh, for training set pretty well.
But, you know, noticed, I, I don't,
we don't- I don't care about the training,
so I care about the, uh, tester.
So just one note,
it says test here but it's really the,
the validation, um, should probably change that.
Um, it's just what- whatever non training set you passed in.
Okay. So this is still a 20% error which is not great.
Okay so, uh, at this point, remember,
I wanna go back and look at, um,
some, you know, get some intuition for what's going on here.
So let's look at the weights.
Okay. So this is the weight vector that's learned.
So for this weight, er, uh,
feature, the weight is 1,
and all of these are 1.
And this, you know,
corresponds to the names that,
um, the people names that have been seen at training time.
Because whenever I see a person name then I'm going to,
um, you know, give that feature a 1,
so I can get that training example right.
And if you look at the bottom,
these are the entities which,
uh, are not people names.
Okay. So this is a sanity check that it's doing what it's,
um, you know, supposed to do.
Um, so the nice thing about with these kind of really interpretable features of that,
you can kind of almost compute
the- what the weight should be in your- in your head, yeah.
[inaudible] one feature for every,
almost every example that you learn?
Yeah.
Okay.
Yeah. Yeah. So I have one- essentially,
one feature for every entity,
which has almost, you know, number [OVERLAPPING].
Most of them are unique.
Yeah. So there's 3,900 features here.
[inaudible] [NOISE].
Uh, so we're gonna change that.
But, um, we- we'll get. We're not done yet.
Okay so, okay, so the other thing we wanna look at is,
um, the error analysis.
Okay. So this shows you- here is an example, Eduardo Romero.
Um, the ground truth is positive but we predicted minus 1.
And why do we predict minus 1?
It's because this feature,
uh, has weight 0.
And why does it have weight 0? Because we never saw this name at training time.
Okay? Um, we did get some right,
we saw Senate at training time and we just rightly,
uh, predicted that was minus 1.
Okay. But, you know,
you look at these errors and you say, "Okay.
Well, you know, this is,
um, maybe the- we should add more features."
Okay? So if you look- remember,
this, um, you know, example here.
Maybe the context helps, right?
Because if you have governor, blank,
then you probably know it's a person because only people can,
you know, be governors.
Uh, so let's add a feature.
So I'm gonna add a feature which is,
uh, left is left.
[NOISE] And for symmetry,
I'll just add right is right.
Okay. So this, eh, defines some indicator features on, you know, the context.
So in this case, it will be took him into.
[NOISE] Okay. So now,
I have three feature templates.
Let's go and train this model.
Um, and now I'm down to just,
uh, 11% error.
Okay. So I'm making some progress.
Um, oops, um, let's look at the error analysis.
Okay? So now, I'm getting this correct.
Um, and let's look at what else am I getting it wrong.
So Simitis blamed, um,
you know, Felix Mantilla.
And, you know, again,
it hasn't seen, um, this exact, uh,
actually, maybe it, uh,
did see this string before, but it still got it wrong. Um,
uh, you know, I think there's kind of a general intuition though that,
well, if you have, you know,
Felix, um, you know,
even if it- you've never seen Felix Mantilla.
If you see Felix something, you know,
chances are it probably is a person, um,
not always but, ah, as- as
we noted before features are not meant to be like deterministic rules.
They're just pieces of information which are useful.
So let's go over here and we want to define let's say a feature for every,
ah, possible word that's in- in entity.
So word and entity.
Remember, entity is a list of tokens which occur between left and right.
And I'm gonna say entity contains a word.
Okay? So now let's run this
again and now I'm down to 6% error which is, you know, a lot better.
Um, if you look at the error analysis,
um, so I think the F- maybe the Felix example and now I get this right.
Um, and, you know,
what else- what else can I do?
Um, so you know what I'm- kind of this general strategy here I'm, ah,
following here is, um, you know,
which is not always the- necessarily the right one but you start with kind of very,
uh, very specific features and then you try to kind of generalize,
you know, as you go.
Um, so how can I generalize this more, right?
So if you look at,
um, worker, so Kurdistan, right?
If your word ends in stan, um,
or then- I mean may- maybe it's,
ah, less likely to be a person.
I actually don't know but, you know,
maybe like suffixes and prefixes,
um, you know, are helpful too.
So, um, I'm going to add features.
Let's say entity contains prefix and then I'm going to let's say just, you know,
heuristically look at the first four tokens,
um, and suffix the last four tokens.
Um, and then run this again and now I'm down to,
you know, 4% error.
Um, okay. I'm probably gonna,
you know, stop right now.
Um, at this point, you can, um,
actually run it on your test set and we get,
um, you know, 4% error as well. Yeah.
[BACKGROUND]
Oh, yeah. I guess, um, this was, um,
all planned out so that the test error would go down.
But actually more often than not,
you'll add a feature that you really,
really think should help.
But it doesn't help for whatever reason, so.
[inaudible] cause of that certainly not get worse.
We agree that the cause of due date, sorry,
not get [inaudible] with whatever cause, would it get worse?
Yeah. You- you s- yeah,
some of the time, you- yeah, it doesn't move.
Uh, that's kind of probably the more of the time but sometimes it can go up,
if you add a really, you know,
bad feature or something.
[inaudible] don't consider this at all, you know, it says here.
So the more features you add generally the training error will go down, right?
So all the algorithm knows is like it's driving training error down,
so it doesn't know that.
It doesn't, you know, generalize.
Yeah. Okay. So this is definitely the happy path.
I think when you go and actually do machine learning,
it's going to be more often than not,
ah, the test error will not go down.
So don't get too frustrated.
Um, just keep on trying. Yeah.
Are we expected to keep optimizing after like 5% error? [NOISE]
Um, are you expected to optimize after 5% error?
Um, it's- it really depends,
um, um, you know,
there's kind of a limit to every data set.
So data sets have noise.
So sometimes, you- you shouldn't definitely not optimize below the noise, ah, limit.
So one thing that you might imagine is, for example, um,
you have an oracle which,
um, let's say it's, uh, human agreement.
Like if your data set is annotated by humans
and if humans can't even agree like 3% of the time,
then you can't really do better than 3% of the time, as a general rule.
There are exceptions, but- okay. Any other questions? Yeah.
Uh, kind of like through all your training,
you're happy and then you- you see the kinds of errors, um,
hence in fair view applications,
say in the advent you try and test that and you find it's not good,
um, what do you do?
Oh, yeah. What happens if you accidentally,
ah, if you train on the test that and it's not good.
Um, that's you- to say that it's not good [LAUGHTER] in some level.
So there's many things that could happen.
One is that your test set might actually be different for whatever reason.
Maybe it was collected in a different day and, um,
your performance just doesn't hold up on that test set.
Um, in that case,
well, that's your test error, right?
Remember, the test error is just- if you didn't look at it,
it's really a honest representation of how good this model is.
And if it's not good,
well, that's just the truth.
There wasn't- your model is not that good.
In some cases there are some like bug,
like something was misprocessed in a way and it wasn't really fair.
So, you know, there are cases where you want to
like investigate if it's like way off the mark.
If I had gone like 70% error,
then maybe you- something was wrong and you would have to go investigate.
But if it's in the ballpark and whatever it is,
that's kind of what,
um, you have to deal with, right?
So what you wanna do also is make sure your validation error is kind of
representative of your- if your test error so that you don't have,
you know, surprises at the end of the day, right?
I mean it's- I think fine, er,
to run it on a test set, um,
just to make sure that there's
no catastrophic problems but the- the kind of
aggressive tuning on a test set is something that would,
you know, have uh, warned against. Um, yeah.
Is there any sort of standard as to how you should split
the data into train that and like validation testing.
Generally, like what percentage of your data you should allocate to each one,
just randomize it or-
Um, yeah.
So the question is how do you split, uh,
into train, validation and test?
Um, it- it depends on how large your data set is.
So generally people, um, you know,
shuffle the data and then randomly split it into test validation and- and train.
Um, maybe let's say like 80%, 10%,
10% just as a kind of,
ah, a rule of thumb.
There are cases where you don't wanna do that.
Um, there's cases where you, for example,
wanna train on the past and test on the future
because that simulates the more realistic settings.
Um, remember, the test set is meant to kind of be representative as possible of
the situations that you would see at- in the- in the real world. Yeah.
Have like, uh, some examples or something like labeled plus one and minus one.
Do you have to do that manually?
So the question is that dataset was labeled.
There's 7,000 of them.
Um, I personally did not label this dataset.
[BACKGROUND] This is a center dataset that, uh,
someone labeled, um, you know,
sometimes these data-sets come from, um, you know,
crowd workers, sometimes they come from, you know, experts.
Um, yeah, it varies.
Um, yeah, sometimes they come from grad students.
It's actually a good exercise to go and label.
I've labeled a lot of data also,
in my life, um.
[BACKGROUND]
Yeah, exactly.
Okay, let's go on.
So switching gears now,
let's talk about unsupervised learning.
So, so far we've talked about supervised learning where
the training set contains input-output pairs.
So you are given the input and this is the output that your predictor should output.
Um, but, you know,
uh, this is very, uh, timely.
Um, we were just talking about
how fully labeled data is very expensive to obtain because,
you know, 10,000 is actually not that much, you know,
you can often have, you know,
100,000 or even a million examples which,
uh, you do not want to, um,
be sitting down and annotating yourself.
Um, so here's another possibility, so unsupervised learning.
Unsupervised learning, the training data only contains inputs and
unlabeled data is much cheaper to obtain in certain situations.
So for example if you're doing text classification,
you have a lot of text out there.
People write a lot on the Internet and you can easily download,
you know, gigabytes of text and all that is unlabeled.
And yeah, you can do something with it.
That would be, you know,
you turn that into gold or something.
Um, and also images,
videos, um, and so on.
Um, you know, it's not always possible to obtain unlabeled data.
For example, if you have, you know,
some device that is producing, uh,
data and you only have one of that device that you built yourself,
then, you know, you're not going to be able to get that much data.
But we're gonna focus on a case where you do have basically infinite amount of,
uh, data and you want to do something with it.
Um, so here's some examples I want to share with you.
This is a classic, uh,
example from NLP that goes back to,
um, you know, the early 90s.
So if these ideas were clustering,
the input you g- will have a bunch of raw text,
lots of news articles and you put it into this algorithm,
which I'm not going to describe,
but I'm going to look at- we're going to look at the output.
So what is this output?
It returns a bunch of clusters where for each cluster,
it has a certain set of words associated with that cluster.
Okay, and when you look at the clusters, they're pretty coherent.
So this is roughly- the first cluster is days of the week,
second cluster is months, um,
third cluster is some sort of, uh,
you know, m- m- materials, um, um,
fourth cluster is, uh,
synonyms of like, you know,
big, and so on.
And, you know, one th-, one thing though,
the critical thing to note is that the input was just raw text.
Nowhere did someone say, "Hey, these,
these are days of the month, learn them and I'll go test you later."
It's all unsupervised.
So this is actually,
um, you know, on a personal note,
the kind of th- th- the, uh,
example when I was doing a Masters, uh,
that got me into doing NLP research because I was looking at this and I was like, "Wow,
you can actually take unlabeled data and actually
mine-" really interesting kind of signals, you know, out of it.
Um, more recently, there's these,
uh, things called word vectors, uh,
which do something very similar instead of clustering words, they embed words in,
uh-, into a vector space.
So if you zoom in here, um,
each word is associated with a particular position and, uh,
s- words which are similar actually t- happened to be close by in vector space.
So for example, these are country,
um, names, these are,
uh, pronouns, these are,
you know, years, months, and so on.
Okay? So this is kind of operating on a very similar principle.
Um, there's also contextualized word vectors like,
um, Elmo and Bert if you've, you know,
heard of those things which have been really taking
the NLP community by storm m- more recently.
On, on the v- vision side,
you also have, uh,
the ability to do unsupervised learning.
Um, so this is an example from 2015 where you run, um,
a clustering algorithm which is also jointly learning the features during this kind
of deep neural network and it can identify, um,
different types of digits: zeros, and nines,
and fours that look like nines,
threes and- or fives that look like three's and so on.
So remember this is not doing classification, right?
You're not, um, uh,
telling the algorithm, "Here's our fives, here's our twos."
It's just looking at examples and finding the structure that,
"Oh, these are kind of the same thing and these are also the same thing."
And sometimes but not always,
these clusters actually correspond to labels.
Um, so here's another example of,
um, um, ships, planes,
and birds that look like planes.
Um, so you can see kind of this is not doing classification,
it's just kind of looking at visual similarity, okay?
All right so the general idea behind supervised learning is that, you know,
data has a lot of rich latent structure in that, in that.
And in that, by that mean- I mean there's,
there's kind of patterns in there.
Um, and we want to develop methods that can
discover this structure, you know, automatically.
So there's multiple types of unsupervised learning.
There's clustering, dimensionality reduction.
Um, um, but we're going to focus on, you know,
clustering- in particular K-means clustering for, um, this lecture.
Okay. So let's get into it more formally.
So the definition of clustering is as follows.
I give you a set of points,
so x_1 through x_n and you want to output an assignment of each point to a cluster,
and the assignment variables are going to be z_1 through z_n.
So for every data point,
I'm going to have a z_i that tells me which of the K clusters I'm in,
1 through K, okay?
So pictorially this looks like this on the board here where I have,
uh, let's say, uh, let's say I have seven points.
Okay. And if I gave you only these seven points and I tell you, "Hey,
I want you to cluster them into two clusters, " you know,
intuitively, you can kind of see maybe there's
a left cluster over here and a right cluster over here, okay?
Um, but how do we formulate that kind of mathematically?
So, um, here's the, K-means objective function.
So this is the principle by which we're going to derive, um, clusterings, okay?
So K-means says that, uh,
every cluster, there's going to be two clusters,
is going to be associated with a centroid, okay?
So I'm gonna draw a centroid and,
um, uh, a red square here.
And the centroid is a point in the space along with the,
uh, you know, the data points.
And, um, I'm gonna th- this is kinda representing where the cluster is.
And then I'm going to associate each of the points with a particular centroid.
So I'm going to denote this by a blue arrow pointing from the point into the centroid,
um, and, you know,
these two quantities, um,
are going to kind of represent the clustering.
I have the locations of the clusterings in red and also
the assignments of the points into the clusters in, in blue.
Okay, so of course neither the red
or the blue are known and that's something we're going to have to optimize.
Okay, so, but first we have to define, um,
what the optimization, uh, objective function is.
Um, so intuitively, what do we want?
We want each point, uh,
Phi of X_i to be close to the centroid, right?
For the centroid to be really representative like,
of the points in that cluster,
that centroid should be close to all the points in that cluster, okay?
So this is captured by this objective function where I look at all the points.
For every point, I measure the distance between that point and,
um, the centroid that that point is associated with it.
So remember z_i is a number between one 1 K. So that indexes which of the Mu,
uh, Mu 1 or Mu 2 I'm talking about.
I'm looking at the squared distance between those;
two, the centroid and the point. Yeah?
How does each point get assigned to a centroid?
Yeah, how does each point get to, assigned to a centroid?
So that's going to be specified by the z's which,
um, is going to be optimized overall. A priori, you don't know. Yeah?
The holes have a pretty good idea of how many labels they can support, I guess-
How many clusters.
-clusters it could be?
Yeah, the question is do we know how many clusters there are?
In general, no.
So there are ways to select.
It's another hyperparameter.
So it's something that you have to set before you run the k means object function.
So when you're tuning,
you try different number of clusters and see which one kind of works better.
Okay, so we need to choose the centroids and the assignments jointly.
So though this- this hopefully is clear,
you just want to find the assignment z and the centroids
mu to make this number as small as possible.
So how do you do this?
Well, let's- let's look at a simple one-dimensional example,
and let's build up some questions, okay?
So we have 1d now,
and we have four points and the points are at- they are going to be at 0,
2, 10, and 12, okay?
So I have our points,
four points at these locations.
Okay, I want to cluster, and intuitively you think I want two clusters here.
There's going to be two centroids.
And suppose I know the centroids, okay?
So just- someone told you magically that the centroids of
this example is- are going to be like at 1 and 11, okay?
So someone told you that and now you have to figure out the assignments.
Yeah, how would you do this?
Let's assign this point, where should it go?
You look at this distance, which is one.
You look at this distance,
which is 11. Which are smaller?
One is smaller. So you say,
"Okay, that's where I should go."
Same with this point,
1 is smaller, for these, 11 is smaller.
And that's it, okay?
So mathematically, you can see it's comparing the distance from each point
to each of the centers and choosing the center which is closest, okay?
And you can convince yourself that that's the way
to- if the cluster centroids were- centroids would fix,
how you would minimize the objective function.
Because if you choose a centroid which is farther away,
then you get just- a larger value and you want the value to be as
small as possible, okay?
I don't know why this is two.
I think this should be one, right?
Okay, so let's do it the other way now.
Suppose I now have the assignments.
So I know that these two should be in some cluster.
These two should be in a different cluster, cluster two.
And now I have to place the- the centers.
Where- where should I place it? Should I place it here?
Should I place it here?
Should I place it here? Where should I place it?
And if you look at the slide here,
what you're doing is you're saying, "Okay,
for the first cluster, I know 2 and 0 are assigned to that cluster.
And I know that the sum of the distances to this- this centroid mu is this,
and I want this number to be as small as possible."
Okay? And if you did the first homework you know
that whenever you have one of these kind of squared of some objectives,
you should be averaging the points here.
So you can actually solve that in closed form,
and you- given the assignments here,
you know the center should be there,
which is average of 0 and 2.
And for these- these cluster,
you should average the two points here,
and that should be at 11. Yeah.
[inaudible].
Okay, so what's the difference between centroid and assignment?
So when you're clustering,
you have k clusters,
so there's k centroids.
So in this case there's two centroids.
There- those are the- the red.
The assignments are the association between the points and the centroid.
So you have n assignments.
And these are the things that move.
Is the k a hyperparameter or is that somehow [OVERLAPPING].
Yeah, so k here is a hyperparameter,
which is the number of clusters which you can turn.
Okay, so here's a chicken and egg problem, right?
If I knew the centroids,
I could pretty easily come up with assignments.
And if I knew the assignments,
I could come up with the centroids.
But I don't know either one.
So how do I get started?
So the key idea here is alternating minimization,
which is this general idea in optimization which is usually not a bad idea.
And the principle is well, you have a hard problem,
maybe you can solve it by tackling kind of two easy problems here.
So here's a k-means algorithm.
So step one is you're going to- you're given the centroids,
now you kind of go into more general notation,
mu 1 through mu k. And I want to figure out the assignments.
So for every data point,
I'm going to assign that data point to the cluster with the closest centroid.
So here I'm looking at all the clusters, 1 through k,
and I'm going to test how far is that point from that centroid,
and I'm just going to take the smallest value,
and that's going to be where I assign that point, okay?
Step two, flip it around.
You're given the cluster assignments now, Z_1 through Z_n.
And now we're trying to find the best centroids.
So what centroids should I pick?
So now you go through each cluster 1 through k,
and you're going to set the centroid of
the kth cluster to the average of the points assigned to the cluster, right?
So mathematically this looks like that.
You just sum over all the points i which have been assigned to cluster k,
and you- you basically add up all the feature vectors.
And then you just divide by the number of things you summed over, okay?
So putting it together,
if you want to optimize this objective function the K-means reconstructor and loss.
First you initialize mu 1 through mu K randomly.
There's many ways to do this.
And then you just iterate,
set assignments given the clusters,
the centroids, and then set the centroids given the cluster assignments.
Just alternate. Yeah.
Yeah this makes sense for like coordinates, for like images,
where like if you read in a similar image by bytes it looks the same,
but like words, where words that are spelled totally
differently can have like these same like semantic meanings.
How would you accurately map them to like a same location to cluster essentially around?
Yeah, so the question is like maybe for images distances
in pixel space makes kinda more- more sense.
But if you have words,
then- two words which- you shouldn't be looking at it like the edit distance between,
you know, the, the words and two synonyms like big and large,
look very different, but they are somehow similar.
So this is something that word vectors,
you know address, which we're not going to talk about.
Basically you want to capture the representation of a word by its context.
So the contexts in which big and large occur is going to be kind of similar.
And you can construct these context vectors that give you a better representation.
We can talk more offline. Yeah.
[inaudible] things where you can get stuck at like a
local minima or you're guaranteed if you do it enough times [inaudible].
Yeah, you can get stuck and I'll show you an example.
Any other questions about the general algorithm? Yeah.
Unstable in that they say you get stuck,
and then like you like kind of [inaudible] multiple.
Yeah, I'll- maybe I'll- I'll answer that. I'll show you an example.
Make sure you show using a fixed number of iterations,
but some kind of criteria like doesn't change anymore as the stopping condition?
Yeah, so this is going up to a fixed number of iterations t. Typically,
you would have some sort of- you would monitor this objective function.
And once it gets below,
stops changing very much, then you just stop.
Actually, this is that the k-means algorithm is guaranteed
to always converge to a local minimum.
So why don't I just show you this demo,
and I think it'll maybe make some things clear.
Okay, so here I have a bunch of points.
So this is a JavaScript demo.
You can go and play around and change the points if you want.
It's linked off the course website.
And then I'm going to run K-means.
Okay, so I initialize with these three centroids,
and these regions are basically the points that would be assigned to that centroid.
So this is a Voronoi diagram of these- these centroids.
Okay, and this is the loss function which will hopefully should be going down.
Okay, so now I iterate- so iteration one,
I'm going to assign the points to the clusters.
So these get assigned to blue,
this one gets assigned to red,
these get assigned to green.
And then the step two is going to be optimizing the centroids.
So given all the blue points,
I put the center in the smack in the middle of these blue points.
And then same with green and red.
Notice that now these points are in the red region.
So if I reassign,
then these become red,
and then I can iterate, and then, you know,
keep on going, and you can see that the algorithm, you know,
eventually converges to clustering,
where these points are blue,
these points are red, and these are green.
And if you keep on running it,
you're not going to make any progress,
because if assignments don't change,
then the cluster centers aren't going to change either.
Okay. Um, so let me actually,
you know, skip this since I'm- I was just gonna
do it on the board but I think you kind of get the idea.
Um, so let's talk about this local minima problem.
So K-means is not guarantee is, is,
is guaranteed to converge to a local minimum,
um, but it's not guaranteed to find the global minimum.
So if you think about this as a coy visualization of the objective function,
you know, by going downhill, we can get stuck here but it won't get to that point.
So you- so you take an example for different random seeds.
You can- let's say you initialize here.
Okay, so now all the three centers are here and if I run this and I run this,
now I get this other solution which is actually a lot worse.
Remember the other one was 44 and this is 114,
and that's where the algorithm converged and you're just stuck.
So in practice, people typically try different initializations,
run it from different random points and then just take the best.
Um, there's also a particular way of initialization called K-means plus
plus where you put down a point and you put down a point which
is as farthest away as possible and then as far away as possible.
And then that kind of spreads out the centers.
So they don't, kind of, inter- interfere with each other
and that generally works pretty well.
But still there's no necessary guarantee of converging to a global optimum.
Okay, any questions about K-means?
Yeah.
[inaudible]
How do you choose K? You guys love these hyper-parameter tuning questions.
Uh, so, uh, one thing you can,
kind of, draw is the following picture.
Um, so K then your loss that you get from K. And usually,
if you have one cluster,
the loss is gonna be very high and that at some point, it's, you know,
going to go down and you generally, uh,
you know, lop it off when it's,
you know, not going down by very much.
So you can monitor that curve.
Another thing you can do is you have a validation set,
um, and you can measure reconstruction error on the, you know,
validation set and choose the minimum based on that,
which is just another hyper-parameter that you can turn. Yeah.
How is the training loss calculated [inaudible]
How's the training loss calculated?
Uh, so the training loss is this quantity.
Um, so you sum over all your points and then you look at
the distance between that point and
the sign centroid and you square that and you just add all those numbers up.
Okay. So to wrap up,
um, oh actually I have- actually,
I have more slides here.
[LAUGHTER] So, um,
unsupervised learning you're trying to leverage a lot of data and we can,
kind of, get around this difficult optimization problem by,
you know, doing this alternating minimization.
So these will be quick.
Um, so just to, kind of, summarize the learning section,
we've talked about feature extraction.
And I want you to think about the hypothesis class that's defined by a set of features.
Um, prediction which boils down to kind of
what kind of model you're looking at for classification and regression.
Supervised learning, you have linear models and neural networks,
and for clustering you have a K-means object- objective loss functions which,
you know, in many cases all you need to do is compute the gradient.
Um, and then there's generalization which
is what we talked about for
the first half of this lecture which is really important to think about.
You know, the task set remember is, kind of,
only a surrogate for future examples.
Um, so a lot of these ideas that we presented are actually quite old.
So the idea of least squares, you know,
the- for regression goes back to, you know,
Gauss when he was,
you know, solve- trying to solve some astronomy problem.
Logistic regression was, you know, from statistics.
For an AI, there was actually some learning that was done even in the,
you know, in the '50s for playing checkers.
As I mentioned, the first day of our class,
there was a period where learning kinda fell out of favor but it came back
with back-propagation and then much of the '90s actually a lot more, kind of,
rigorous treatment of optimization and formalization of
when algorithms are guaranteed to converge um that- that happened in the '90s.
And then in the 2000s,
we know that people looked at kind of structure prediction and,
um, there was a revival of neural networks.
Um, some things that we haven't covered here are,
you know, feedback loops, right?
So learning assumes kinda the static view where you take data.
You train a model and then you go and generate predictions.
But if you deploy the system in the real world,
those predictions are actually gonna come around and beat data.
And those feedback loops can also cause
problems that you might not be aware of if you're only thinking about,
ah, here's- I'm doing my machine-learning thing.
How can you build classifiers that don't discriminate?
So, um, we, uh,
often have classifiers,
you're minimizing the training set- average of the training set.
So by- by a kind of construction,
you're trying to drive down the losses of,
you know, kind of common examples.
But often you get these situations where minority groups actually get, you know,
pretty high loss because they look different and almost look like
outliers but you're not really able to fit them.
But, um, the training loss doesn't kind of, you know, care.
So there's other ways.
Um, there's techniques like distribution and robust optimization that tries to,
um, you know, get around some of these issues.
Um, there's also privacy concerns.
How can you learn actually if you don't have access to an entire dataset?
So there are some techniques based on randomization that can help you.
And then interpretability, how can you understand what,
you know, the algorithms are doing especially if you have a deep neural network.
You've learned a model and there's, you know,
work which I am happy to discuss with you offline.
So the general- so we've concluded three lectures on machine learning.
Um, but I wanted you, kind of,
to think about learning in the most general way possible,
which is that, you know, programs should improve with, you know, experience.
Right. So I think we've talked about, you know,
linear classifiers and all these kind of nuts and bolts of basically reflex models.
But in the next lectures,
we're gonna see how learning can be used in state-based models and also,
you know, variable-based models.
Okay. With that, so that concludes.
Um, next week, Dorsa will be giving the lecture on state-based models.
 Hi everyone, I'm Dorsa, uh,
and this week I'll be teaching
the state-based models and the plan is for the next couple of weeks for me to,
to teach the state-based models MDPs, uh,
and games and then and after that Percy,
we'll come back and talk about the later,
some of the later topics.
So a few announcements.
Uh, so homework 3 is out.
So just make sure to look at that.
And then the grades for homework 1 will be coming out soon.
So just yeah, be aware of that.
All right. So, so let's talk about state-based models,
let's talk about search.
So just to start,
I was thinking maybe we can start with this question.
Uh, if you can,
let me reset this.
So basically, okay, let me tell you what the question is and then think about it,
and then after that I will get this working.
So, so the question is you have a farmer and the farmer has a cabbage,
a goat, and a wolf,
and it's on one side of the river.
Everything is on one side of the river.
So you have this river.
We have a farmer.
We have the farmer with a cabbage,
with a goat, and with a wolf, okay.
And the farmer wants to go to the other side of the river and take everything with, with,
with himself, um, and- but the thing is the farmer has
a boat and in that boat can only fit two things.
So the farmer can be in it with,
with one of these other things, okay?
So the question is how many crossings can,
can the farmer do to take everything on the other side of the river?
And there are a bunch of constraints,
the constraint is if you leave
the cabbage and goat together the goat is going to eat the cabbage.
So you can't really do that.
If you leave wolf with the goat,
the wolf is going to eat the goat, you can't really do that.
How many crossings should you take to take everything to the other side?
Think about it, talk to your neighbors, I'll get this working.
Everyone clear on the question? Okay.
So the link doesn't work because,
uh, we can't connect to Internet,
but all right so.
Okay. So how many people think it is four?
Four crossings. Five, five crossings.
Six, six.
Some people think six.
Seven? More people.
No solution? No solution.
Okay. So the point is actually not like what the answer is,
we'll come back to this question and try to solve it,
but I think the important points to,
to think about right now is how you went about solving it.
So, so what were you thinking and what was the process
that you were thinking when you were trying to solve, solve this problem.
And that is kind of the commonality that search problems have and,
and we want to think about those types of problems where it's,
it's more challenging to answer these types of
questions and let's say reflex based type of questions.
So, so that's kind of just a motivating example that we'll come back later.
And here's an XKCD on this.
So basically one potential solution is the farmer takes the goat,
goes to the other side, comes back,
takes the cabbage, goes to the other side and just
leaves the wolf because why would he need a wolf,
why would a farmer need a wolf.
So [LAUGHTER] if you answered four,
you probably were thinking about this.
[LAUGHTER] And I guess it has like
an interesting point in it because sometimes maybe you should change the problem.
Your model is completely wrong.
Maybe, maybe sometimes you should rethink and go back to your model and try to fix that.
But anyways. So we'll come back to this question.
So all right.
So this was our guideline for the class,
and, and we have already talked about the reflex-based model.
So we have talked about machine learning and how that can get applied,
and now we want to start talking about state-based models.
This week, we're going to talk about search problems,
next week, MDPs, and then the week after we're going to talk about games.
If you remember the kind of the guideline that,
that we had for the class was, uh,
we were thinking about these three different paradigms of,
of modeling, all right,
we talked about this already.
So modeling, inference, and learning.
So for, for reflex-based models we talked about this already, right?
So what would the model be,
well, it can be a linear predictor or it can be a neural network.
So, so that was a model.
And then we talked about inference but in the case
of reflex-based models it was really simple,
it was just function evaluation.
You had, you had your neural network and you would just
go about evaluating it and that was inference.
And we also spent some time talking about learning.
So how would we use like let's say gradient descent
to try to fit the parameters of the model, okay.
So similar thing with search-based models.
You want to talk about these three different paradigms that we have in the class, and,
and the plan is to talk about models and
inference today and then on Wednesday we'll talk about learning.
We kind of have the same sort of format next week too.
So we're going to start talking about modeling and inference on Mondays,
Wednesdays are going to be about learning.
So, so just to give you an idea of what the plan is.
All right. So, so what are search problems?
Let's start with a few motivating examples.
So, so one potential example one can think of is, is route finding.
So you might have a map and you want to go from point A to point B on the map,
and you have an objective.
So you want to maybe find the shortest path or the fastest path or most scenic path.
That is your objective and the things you can do is you can take a bunch of actions.
So you can do things like go straight,
turn left, turn right,
and then the answer for the search problem is going to be a sequence of actions.
If, if you want to go from A to B with the shortest path,
the answer that one would give is maybe turn right
first and then turn left and then right again or any,
any of these sequences.
Okay so, so this is just a canonical example of what a search problem is.
There are a few other examples.
So for example you can think of robot, robot motion planning.
So if you have a robot that wants to go from point A to point B,
then it might want to have different objectives for doing that.
So again the question might be what is the fastest way of doing it
or what is the most energy efficient way of getting the robot to do that or,
or what is the safest way of doing it.
Like another question that we are interested in is what is the most expressive or,
or legible way of robot doing it so,
so people can understand what the robot really wants.
So you might have again various types of objectives you can formalize that,
and then the actions that,
that you can take in the case of
the robot motion planning is the robot is going to have different joints,
and each one of the joints can translate and can rotate.
So translation and rotation are the type of actions that you can take.
So, so in this case I have a robot with seven,
seven joints and then I need to tell what each one of
those joints should do in terms of translation and rotation.
That's your robot?
This is my robot, yes.
[LAUGHTER] It's a fetch robot.
[LAUGHTER] All right.
So, so let's look at another example.
So games is, is a fun example.
So you might, uh,
think about something like Rubik's cube or,
or this 15-puzzle, and again what do you wanna do as a search problem?
Well, you wanna, you wanna end up in configuration that's desirable, right?
So you wanna end up in a configuration where,
where you have this type of ah, configuration on Rubik's cube or,
or the 15 puzzle.
So that, that is the goal, that's the objective.
And then the action is you can move pieces around here.
So, so the sequence of actions might be how you're moving these pieces
around to get that particular configuration of the 15 puzzle, okay.
So again another example of what a search problem is.
Um, machine translation is,
is an interesting one if it's not necessarily
the most natural thing you might think about when you think about search problems,
but what it is actually you can think about it as a search problem again.
So imagine you have a phrase in
a different language and you want to translate it to English.
So what is the objective here?
Well you can think of the objective as going to fluent English and preserving meaning.
So, so that is the objective that one would have in machine translation.
Um, and, and then the type of actions that you're taking is you're appending words.
So you start with the and then you're
appending blue to it and you're appending house to it.
So, so as you're appending the- these different,
different words, those are the actions that you're taking.
So, so in some sense you can have any complex sequential task and,
and the sequence of actions that you would get to get
to your objective is there's going to be the answer for,
for your search problem and you can pose it as a search problem, okay?
All right. So, so what is different
between let's say reflex-based models and, and search problems?
So, so if you remember, reflex-based models the idea was you'd have
an input x and then we wanted to find this f for example a classifier that,
that would output something like,
like this y which is labeled, it's a plus 1 or minus 1.
So, so the common thing in,
in these reflex-based models was we were outputting this,
this one label, this one in this case action being minus 1 or plus 1.
Again in search problems,
the idea is I'm given an input,
I'm given a state,
and then given that I have that state,
what I wanna output is a sequence of actions.
So I do want to think about what happens if I take this action like
how is that going to affect the future of my actions.
Okay. So, so the key idea in search problems is
you need to consider future consequences of,
of the actions you take at the current state. Yes.
Is this like not equivalent to like just outputting
one thing and then like rerunning the function,
on like the updated state?
So if you rerun it.
So, so the question is, yeah, is it not the same as like I'm rerunning it,
I output a thing and then I rerun it again.
And you could do that, but that ends up being a little bit of a- that would be some-
similar to a greedy algorithm where
like let's say I want to get to the door and I want to find,
find the fastest way and right now if I just look at like
my current state maybe I think the fastest way of getting there is going this way.
But if I actually think about a horizon and I think about how
this action is going to affect my future
I might come up with a different sequence of actions.
Okay? All right.
Okay. So and, and you've already seen this paradigm so let's start
talking about modeling and inference during this class.
So this is the, the plan for today.
So we're going to talk about three different algorithms for,
for doing inference for search problems.
So, so we're going to talk about tree search which is
the most naive thing one could do to solve some of these search problems,
but that's the simplest thing we can start with.
And then after that you want to look at improvements of that doing dynamic programming or,
or uniform cost search.
So, um, the difference between search-based problem and reflex-based problem,
the very fact that in a reflex-based problem,
the output that you gave does not influence a string,
and it doesn't search?
Yeah. Tha- that's true. Yeah so,
so the output that you get in search problem it
is an action that actually influences your future.
Yeah, that's a good way of actually thinking about it.
Yes. All right.
So, so let's talk about tree search.
So let's go back to our favorite example.
Um, okay so we have the farmer,
cabbage, goat, and wolf.
So let's think about all possible actions that one can take,
when we have this farmer,
cabbage, goat, and wolf.
Okay. So, so a bunch of things we can do is a farmer
can go to the other side of the river with the boat alone.
So, uh, this triangle here just means like going to the other side of the, uh, the river.
The farmer can take the cabbage.
So C is for cabbage G is for,
ah, goat, W is for wolf.
So another possible action is the farmer takes a cabbage or the farmer
takes the goat or the farmer takes a wolf and goes to the other side of the river.
We also have a bunch of other actions.
The farmer can come back.
The farmer can come back with the cabbage,
come back with the goat, come back with the wolf.
So I'm basically numering- enumerating all possible actions that,
that one could ever do.
And sure none of- like not- some of these might not be possible in
particular states but I'm just creating this library of actions things that are possible.
Okay. So then when we think about the, ah,
this as a search problem,
we could create a search tree.
Which, which basically starts from an initial state of where things are and
then we can kind of think about where we could go from that initial state.
So the search tree is more of, ah,
what if- what if tree which,
which allows you to think about what are the possible options that, that you can take.
So, um, conceptually what- what it looks like is you're starting with your initial state,
where everything is on one side of the river.
So those two lines are the riv- the river the blue lines.
Um, and you can take a bunch of actions,
right like one possible action is you can take the cabbage
and go to the other side of the river and you end up in that state.
And that state is not a good state.
I am making that red. Well, why is that.
Because the wolf is going to eat the goat. That's not that great.
Okay. Um, and, and every action,
every crossing let's say ma- let's say every crossing takes cost of one.
So that one that you see on the edge is the cost of that action.
Okay. So that didn't really work that well.
What else can I do? Well, I can,
I can do another action.
I can, I can- from the initial state,
I can take the goat and go to the other side of the river,
that ends up in this configuration.
From there the farmer could come back,
take the cabbage, go to the other side,
end up in this configuration,
the farmer can come back.
That's again, not a great state because
cabbage and goat are left on the other side of the river,
goat is going to eat the cabbage.
That's not great. What else can I do?
Well, the farmer can come back with the goat.
And then once the farmer comes back with the goat,
the farmer leaves the goat, takes the wolf, goes to the other side,
comes back gets the goat again.
And then boom, you're done.
Okay. So- so how many steps does this take?
Well, one, two, three,
four, five, six, and seven.
So- so the ones who answer seven that was the right answer.
Um, and that is kind of the idea of getting to this end state. Yes.
So to be specifically, ah,
not include the option that the going back to the previous state even
though that's a valid next step just because we know that there's something-
So you could have this giant tree where you
go to different states but we can actually have like
a counter that tells you if I have visited that state and
if you have visited that state maybe you don't want to go there again because,
because you have already explored all the possible actions from there.
You're not done with this tree though, right?
Like I've, I've found that this good state here,
but maybe there's a better way of, like getting there.
I don't know yet. I haven't explored everything.
So, so what I can do is,
I can actually explore all these other things that, that one could do.
And I'm not gonna go over them.
But there is another solution,
and turns out that other solution also takes seven steps.
So it's not necessarily a better solution, but,
but you've got it for all of that because there could be another solution later on that.
That is, uh, better than the seven steps.
Okay. All right. Yes.
Are these slides up?
They are, they should be.
Okay.
Slides are up. Okay. Um, all right.
So, so this is how the search tree looks like. Yeah.
I'm just asking [inaudible]
Oh, that's a very good point.
Thank you for- [LAUGHTER] thank you, so for SCPD students I'll try to repeat the questions.
I always forget this.
Um, I'll try to repeat the question.
The question was, ah, was the slides, uh,
the slides aren't up, they're up, they should be up.
So okay. All right.
So, uh, going back to our search problem.
Ah, so we can try to formalize this search problem.
So, so let's actually think about it more formally.
So what are the things that we need to keep track of.
So, so we have a start state.
So let's defined a start to be the start state.
In addition to that we can,
we can define this function called actions
which returns all possible actions from states.
So actions as a function of state.
If I'm in a state, that basically tells me what are the actions I can take from there.
I can, I can define this cost function.
So this cost function,
takes a state and action and tells me what is the cost of that and in this example,
the cost of crossing the river was just one but
you can imagine having different costs values.
Ah, we can have a successor function that basically takes a state and action and,
and tells us where we end up at.
So if I'm in state S and I take action A where would I end up at?
And that's the successor function.
And then we're going to define an IsEnd function,
which basically checks if you're in an end state where
we don't have any other possible actions that you can take. Yes.
So these are the [inaudible] I got a call?
You can, you can think of it as, yeah, as a way of like finite state machine type of,
type of, uh, way of looking at it.
Yeah. So like we- we use a similar type of formalism,
uh, for MVPs and games too.
So this is good idea to get like all these formalisms right.
But start state, transitions, costs.
Those sort of things. Okay. Yes.
What's the [inaudible] like [inaudible].
Ah, say it again so.
A cost [inaudible] like.
Cost?
Position and action, and action already concerns the state.
So then- so- so the action,
okay, so action depends on state.
So you start from start state where you haven't taken any actions right,
and then from that start state then you can think about all possible like right up there.
So you're under that start state,
and there you can think about all possible actions you can take,
and then those actions depend on
current state but they don't depend on the future state, right.
So based on like the current state,
everything is on one side of the river.
I can think about all possible actions I can take and where I know- where I end up at.
And then, after that like the next action depends on that.
Yeah, that's it. So it's a sequential thing. Okay. Yes.
You have all the information on the actions and the cost that you could do beforehand,
how is this conceptually different than like a min cost flow convex optimization?
You can think of it. Okay. So- so how- how is it
different from a kind of convex optimization type of role?
So- so we have- we have an objective here and then you can think
of what that objective is and based on what that objective is,
we can have different methods for solving it, right?
So- so you can basically formulate this as an optimization problem where you
saw- you look for the solution to
a search problem as an optimization problem too that's perfectly,
a perfect way of doing it.
And, and we're going to talk about various types of
methods for- for solving this problem today.
Okay. All right.
So- so let's look at another example.
So, um, this is, um, transportation problem.
Now I'll just move this.
So, um, okay.
So basically, what we wanna do is we have street blocks from 1 through N. So 1,
2, 3, 4, so on.
So these are street blocks and N is here.
And what we wanna do is we basically want to travel from,
from 1 to, to some N number.
And we have two possible actions.
So at any state,
let's say I'm in state S. At any state,
I can either walk,
and if I walk I end up in S plus 1.
So if I'm in 3,
I'm going to end up in 4.
And walking takes one minute.
Or I can take this magic tram.
And this magic tram, takes any state S to 2 times S.
So if I'm in 3, then I am going to end up in 6 by taking the magic tram.
And the magic tram always takes two minutes, doesn't matter from where to where.
So, so if I'm in 2, I will end up in 4,
if I'm in 5 I can end up in 10 by taking the tram.
Okay. So, so I have two possible actions in any of these states.
And what I want to do is, I want to go from 1 to N and then
I want to basically do that in the shortest, uh, time possible.
Okay. So with the- with the least amount of costs. That's the problem, makes sense?
Okay. All right.
So, so this is kind of like,
what the search problem is.
So what we wanna do is first off, you want to just formalize it.
Uh, and I'm gonna do that here.
I'm not gonna do live solutions because I'm not Percy,
and I did that once and it was a disaster.
So [LAUGHTER] we are going to,
uh, yeah I taped these in 2018.
Uh, but, uh, basically,
we're going to go over it together.
So, so let's just do that.
Um, so we're going to define the search problem, this tram problem.
So we're gonna define a class for transportation problems.
So we're going to separate our search problems from
our algorithms because remember modeling is separate from inference.
So let's just have a constructor for this transportation problem.
It takes N, because we have N blocks.
Okay. So N is the number of blocks.
Okay. All right.
So, so then you have- we still have a start state.
We're starting from 1 so block 1.
And then we need to define IsEnd state.
So IsEnd state basically checks if you've reached N or not.
Because, because we have to get to the Nth block.
Okay. All right. So what else do we need?
So we have a successor function.
We also have a cost function.
I'm gonna put both of them together,
because, because that is just easier.
So the successor and cost function,
I'm saying let's just give it state S. And then given a
state it's going to return this triple of action, new state, cost.
So I give it a state, let's say initial state,
and then it just returns all possible actions,
within new states I can end up at and how much does that cost.
Okay/. So what are my options?
Well, if I'm state S, I can walk to s plus 1 that costs 1.
If I'm in state S,
I can take the tram, I can end up in 2S,
and that costs 2.
Okay. So that's how I'm creating my triples.
And, and I need to check if I don't pass the Nth block.
Remember, like we have N blocks so we don't want to pass the Nth block.
Okay. So, so that's just to make sure that we don't pass it.
So we are still below the Nth block.
And, and this is what my successor and cost function will return that, the triples.
Okay. So let's just return that.
Okay. So that is my transportation problem.
Let's make sure it does the thing the way we want it.
So let's say we have 10 blocks,
and now I wanna print my transportation- my successor and costs function.
Let's say I'm returning successor and cost for 3. What should I get?
So from 3, I can have two actions, right.
I can either walk or I can take the tram.
If I walk, uh, it costs 1.
If I take the tram, it costs 2.
I'll end up in 4 or 6.
Let's just try. I don't know 9.
If I'm in state 9, I can only do one thing, I can walk, right?
Because remember, the, the block is- number of blocks is 10 and I can't go beyond that.
So- all right.
Um, okay. So that was,
um, [NOISE] yeah, let's go back here.
So that was just defining,
uh, the search problem, [NOISE] okay?
And, and I haven't told you guys like how to solve it, right?
This is- we are just doing the modeling right now.
So we just modeled this problem. We just coded it up.
Modeling it means, what is this- what are,
what are the actions, what is a successor function,
what is a cost function,
defining an is end function,
saying what, what the initial state is, okay?
So, so now I think we are ready to think about the algorithms in terms of,
like, going and solving these types of search problems, okay?
So the simplest algorithm we want to talk about is, is backtracking search.
So the idea of backtracking search is- maybe I can draw a tree here,
is you're starting from
an initial state and then you have a bunch of possible actions.
And then you end up in some state and you have a bunch of other possible actions.
[NOISE] Let's say you have two actions possible.
And this can become- [NOISE] this exponentially blows up so I'm going to stop soon.
[LAUGHTER] All right.
So, so we create this tree and this tree has some branching factor.
That's the number of actions you have at,
at every, at every state.
And then it also has some depth.
[NOISE] So that is how many levels you go down.
[NOISE] So let me just define that with D, okay?
And now there are solutions down in these notes, right?
So, so we wanna figure out what those solutions are.
And backtracking search just does the simplest thing possible.
What it does is, it starts from
this initial state and it's going to go all the way down here.
And if it doesn't find a solution,
it's gonna go back here and then try again and try again.
And it's gonna go over all of
the tree because there might be a better solution down here too.
So it needs to actually go over all of the tree, okay?
So I'm gonna have a table of algorithms because we're gonna talk about a few of them here.
Algorithms, [NOISE] what sort of costs they allow,
in terms of time,
how bad they are, in terms of space, how bad they are.
So if you've taken an algorithms course,
like, some of these are probably familiar.
So, er, all right.
So we talked about backtracking search, [NOISE] backtracking search.
That is basically this algorithm that goes through pretty much everything,
and it allows any type of cost.
So I can have [NOISE] any cost, right?
I can have pretty much any cost I want on
these edges because I'm going over all of the tree.
It doesn't matter what these costs are, okay?
So, um, how-, how bad is this in terms of, in terms of time?
So in terms of time,
I'm going over the full tree.
By going over the full tree, then, then this,
this is going to have this exponential blowup where I'm looking at order of b to the d,
where b is, again, my branching factor and d is the depth of the tree, okay?
Cause in terms of time,
this is not a good algorithm.
Like, in terms of time, I have to go over everything in the tree.
And that's the size of my tree, okay?
And in terms of space,
in terms of space, what I mean is,
I need to figure out what was,
what was the sequence of actions I needed to take to get to some solution.
So let's say that my solution is down here.
If my solution is down here, then for me,
in or- like, I need to store a bunch of things to know how I got here,
and the things I need to store are the appearance of
this node and that is depth of D. So in terms of space,
this algorithm takes order of D, okay?
Because, because that is, like, the things that I need to
store in my memory to be able to recover,
like, the solution when I get there. Yes. [NOISE]. Question.
Because we need to look at everything,
shouldn't this space be big or here D to the D as well?
Because until you get to that,
you need, you need to have the space to have everything, right?
You can prove that, but [NOISE] no. So actually,
we'll talk about breadth-first search later,
which does require you have a larger space.
So, so the reason you can forget it is the only history that I
need to keep track of is this particular branch, right?
I don't need to figure out, like,
I don't need to keep track of, like,
actually the history of all these other nodes.
I can, I can throw it- [NOISE] those out.
But for something else like breadth-first search where we'll talk about in a few slides,
you actually need to keep track of,
like, the history of everything else.
So, so let me get back to that in a few slides.
But for this one, basically the idea is,
um, yeah, like, I wanna know how I got there.
To, to know how I got there,
I just need to know the parents. Yes.
[inaudible] like the minimum cost to reach a point or is it to find whether,
like, you can or cannot reach a certain point in your search.
So it depends on what your objective is.
Like, it really depends on what the search problem is asking.
So, so in the case of that farmer-goat example, uh,
the search problem is asking,
you wanna move everything to the other side of the river.
So you have that criteria.
And you wanna find the minimum cost one,
so you also have that other cri- criteria.
So it really depends on what the search problem is asking.
And some of these nodes might be solutions.
Some of them might not be solutions.
So, so it really depends, okay? All right.
So, so let's just look at these on the slide.
So the memory is order of D. It's actually small. It's nice.
In terms of time, this is not a great algorithm, right?
Because even if your branching factor is 2,
if the depth of the tree is 50,
then this is gonna blow up, like, immediately.
So a lot of these tree search algorithms that we're gonna talk about,
like, they have the same problem.
So, so they pretty much have the same time complexity.
We're going to just look at very minimal improvements of them.
And then after that, we'll talk about, uh,
dynamic programming and uniform cost search,
which are polynomial algorithms that are much better than these, okay? All right.
So let's actually- let's go back to the tram example and let's try
to write up what backtracking search does.
So- all right. So we defined our model.
Our model is the search problem,
this particular transportation search problem.
It could be anything else.
Um, and now we're going to kind of have
this main section wi- where we're going to put in,
like, our algorithms in it.
And we're gonna write them as general as possible so,
so we can apply them to other types of search problems, okay?
So let's define backtracking search.
It takes a search problem.
It can take the transportation problem, okay? All right.
So- and then we're going to- basically in backtracking search,
what we're doing is we're recursing on every state given that you have a history of,
of getting there and the total cost that it took us to,
to get there, okay?
So, so at the state,
having gotten some history and some accumulated costs so far,
we are going to basically recurse on
that state and look at the children of that state, okay?
So, so we're going to explore the rest of the subtree from,
from that particular state, okay?
All right. So how do we do that?
[NOISE] Well, we gotta make sure that we're not in an end state.
Or if you're in an end state, like,
we can actually update the best solution so far, okay?
So let's put that for to do.
So, so, so the bunch of things that we need to do.
We need to figure out if you're in an end state.
If we are, well,
we got to, we gotta update our best solution.
If you're not in an end-state,
then we're going to recurse on children, okay? All right.
So we can do that later.
And then in general, this recurse function is, is going to,
uh, we're going to call it on on the, on the start state.
So let's actually do that too.
So, so what backtracking search does is it calls this recurse function
on the initial state that we have with history of none, right?
Like, we don't have any history yet, and,
and cost is 0 so far because we haven't really gone anywhere.
So, so we start with a start state.
We call recurse on it, okay?
[NOISE] And how do we recurse on children?
Well, we have defined this,
this successor and cost function.
So by calling that successor and cost function on state,
then we can get action,
new state, and cost.
So, so we get this triple of action,
new state, and cost, okay?
And then we can basically recurse on the new state.
Um, I'm not putting the histories right now in this code.
So, so we need to keep track of the history too,
but, but let's just not worry about the history.
Oh, I guess I'm putting it in this one.
[LAUGHTER].
In the later ones I will not put them.
But, but basically the history is keeping track of, like, how you got there.
And to- total cost is going to be [NOISE] what,
what you've got so far plus the cost of this,
this new state, action pair, okay?
Okay. So we need to keep track of the best solution so far.
So I'm just going to find a dictionary here just to
make sure that we keep track of it and for Python scoping reasons.
Okay. And then the place
we're going to update our best solution so far is that to do that is left, right?
So, so if you're in an end state,
then we can actually update the best solution so far, okay?
And what do we want in our best solution?
Well, we wanna know what the cost is.
So, so we can start with cost of infinity.
And anything below infinity is better.
[NOISE] And then we're going to start with a history of empty,
but we're going to fill up that history too, okay?
So that's the initialization of best solution so far.
Then, we're going to update that, right?
If you're in an end-state,
if the total cost that we have right now is smaller than the best solution so far,
then we're going to update that best solution.
And, and you're going to update its history with whatever its history is, okay?
All right. And, and that's it,
that's backtracking search, okay?
So let's just make sure it does the thing.
So maybe- so to do that,
[NOISE] we are going to- actually, no,
we gotta return the best solution so far.
Mm-hmm.
All right. So now we have defined a transportation problem.
Now, what I want to do is,
I want to call backtracking search on the transportation problem, okay?
So that all sounds good.
I need to write a print function also to- to be able to print things.
So I'm gonna just write a generic print function that we can
call on any of these types of problems.
So let's- let's define a print solution function that just like,
prints things the way we want them.
So we get the solution,
and we're gonna just unpack that cost and
history and just print the cost and history nicely.
Okay. All right.
So I can- I can use this print solution for pretty much all the other algorithms,
we'll talk about today too.
Okay. And we're gonna talk about how we get there- to the history.
So now I have my print function,
I have my backtracking search algorithm,
I've defined my transportation problem.
I can just call it on this transportation problem with 10 blocks.
So as you guys can see here,
so the total cost is 6.
So what this means is for going from city 1 to city,
city 10, then this is the best solution.
I- I gotta walk walk, walk,
walk, and then after that ta- take the tram.
Because like I end up in 5,
and then after that it's actually
worth taking the tram and paying the cost 50.
Um, let's try it out for 20.
What do you think is the answer for 20?
So [LAUGHTER] similar to before, walk, walk,
walk until we get to 5,
then we take the tram, then we take the tram again.
The cost is 8. And then if,
if it is 100,
it's a little bit more interesting if you have 100.
So you are walking and then you're taking
the tram and you get to 24 and you what- you have
that in one step to get to 25 which is
the good state because then you can just multiply that by 2.
So you walk for that one step and take the tram again, okay.
So what if I want to try out a much larger number of blocks?
So is this gonna work?
No, because, because remember,
that time was order of b to the d. That wasn't that great.
So let's try that.
Well, we got maximum recursion then, we can fix that.
So [LAUGHTER] let's try fixing that.
[LAUGHTER] So you can, you can set your recursion limit to be whatever.
So you can try that. Is this gonna work?
[LAUGHTER] Now, it's just gonna take a long time, right.
So, so it's not going to give you an answer
[LAUGHTER] And it's gonna just take a long time.
So all right.
[LAUGHTER] Actually, how do I view? Okay.
Let's go back here. All right.
So that was backtracking search, right?
So all it was doing was just going over all of this tree and it was taking
exponential time as you saw and we just tried it
out on that transportation problem that we defined.
So we just defined a search problem, we used this
really simple search algorithm to find solutions for that,
and- and then that's what we have so far.
So, so now what we want to do is,
we want to- we want to come up with
a few better improvements of this backtracking search.
Again, don't get your hopes up,
it's not that big of an improvement.
But, but we can do some- something better.
So, so the first improvement you want to make
is by using this algorithm called depth-first search,
as some of you might have heard of it.
DFS or depth-first search, okay?
So the restriction that DFS put in,
is, is that your cost has to be 0.
So your cost has to be, let me leave that.
Um, let me actually draw a line between them.
So you don't get.
Okay, so, so we are talking about DFS now,
and the restriction is the cost has to be 0.
So, so what DFS does,
is it basically does exactly the same thing as backtracking search,
but once it finds a solution down here then it is done.
It basically doesn't like explore the rest of the tree.
And the reason it can do that is the cost of all these edges is 0.
So if the cost of all these edges are 0,
then if I find a solution I found a solution.
I don't need to like find this better solution.
Because, because that, that is good enough like anything that I find also has a cost of 0,
so I might as well just return the solution.
Like, an example of that is if you have Rubik- Rubik's cube uh,
like if you find a solution then you have found a solution, right?
There are a million different ways of like getting to a solution,
but like you just want one.
And then if you find one,
then you're happy, you're done.
Okay. So as you can see, this is a very,
very slight improvement to backtracking search.
Um, what happens is in terms of,
in terms of space it's still the same thing.
So it's order of D. So in terms of space nothing has changed.
It's pretty good, it's order of D. In terms of time,
in practice it is better, right?
Because in practice if I find a solution,
I can just be done,
don't worry about the rest of the tree.
But, but in, in general,
if you want to talk about it in theory then
the worst case scenario is just trying out all of the trees,
so you write it as worst case scenario,
it's order of b to the d. So,
so nothing has really changed in terms of- in terms of exponential blow up. Yes.
I've been thinking of how you draw that tree,
it seems that you imply that the sub problems do not overlap, right?
Because you're kind of [inaudible] but in fact the sub-problem could overlap.
So you- somebody with a training problem, you can get to
the same place through different history but the rest is the same.
Yeah, so you can- so,
so the question is yeah, do sub-problems overlap here or they don't.
So you could actually have it in a setting where sub-problems do overlap,
but you could actually add this,
this extra like constraint that says if I visited the state,
then don't add it to the tree.
So, so you have that option or you have the option of like going down to tree with some,
like particular depths and not trying out everything.
In the setting that we have here, yeah,
like we're basically trying out all possible.
Like, I'm talking about the most uh, like,
general form where you're going over
all the states and all possible actions that could come out of it, okay?
All right. So that was DFS.
Okay. So the idea of DFS again as you're doing
backtracking search and then you're just stopping when you find
a solution because- because cost is 0, okay?
So in terms of s- space order of D,
in terms of time,
it's still order of b to the d, okay?
All right. So that was DFS.
We have another algorithm called breadth-first search BFS.
And this is useful when cost is some constant but it doesn't need to be 0,
it's just some, some, some positive constant.
So what that means is all these edges have the same cost
and that cost is just C. So I have the same cost pretty much everywhere, okay?
So the idea of breadth-first search,
is we can- we can go layer by layer.
Like, like we're not going to try out the depth.
Instead what we can do is,
we can go layer by layer,
try out this layer and see if we find a solution here.
Remember the tree doesn't need to go all the way down here.
The tree could end here or like at any of these and any of these nodes.
Like, like I can have like a tree that looks maybe like this.
I have a solution here.
Like this tree doesn't need to be like this nicely formed.
Like I can have a tree that looks like this, okay?
So if I have a tree that looks like this,
with breadth-first search, I'm gonna try out this layer.
See if this guy is a solution.
If it's not, I'm gonna try this guy, see
if this is the solution.
If not I'm gonna try here, here,
and then when I find a solution when I get here, I'm done, right?
Because like if I find a solution here,
I know it took 2C to get here.
Like two of these C values.
And if there is any other solution anywhere else in this sub-tree or in this sub-tree,
those solutions are going to be worse than this.
Because they are gonna just like take like,
they- they're going to have a higher cost, okay?
So because the cost is constant throughout.
Okay. So then it's,
it's useful if your solutions are somewhere like
high up in this tree and then you can find it.
So in terms of time,
I get some improvements here because I can call this depth,
this shorter depth the small d. I'm gonna
call this shorter depth small d. And in terms of time,
it's still exponential but it's order of
B to the small d. And this is actually a huge improvement,
because if you think about it,
the tree has exponentially become larger.
So these like lower levels are a lot of things that you need to, you need to explore.
If we have like branching factor of 10,
the next layer has 100 things in it, right?
So- so going down these layers is actually pretty bad.
So, so the fact that with bre- breadth-first search I can improve the timing and,
and limited to a particular depth, that's pretty good.
Still exponential, but pretty good. Yes.
[inaudible] negative cost at that point,
you can also assume this is best solution.
Yeah, you can assume that this is the best solution. Yeah, exactly.
So you are assuming that there are no negative cost.
So at this point, I know this is the best solution, I'm done.
Like I call it and and I don't like explore anything else.
The problem with breadth-first search is um, there's a question there, sorry.
Are you also assuming all the costs are the same?
Yeah, we're assuming all the costs are the same.
Because maybe you like all the costs are 1,
if- if I don't assume that,
if all of these costs are 100 and then like there might be like some,
some other like um.
[inaudible].
Yeah, you need to explore the rest if they're not the same basically. That's what I mean.
All right. So, so the the problem with BFS is,
in terms of memory we are losing.
In terms of memory,
you need to actually keep track of the history of all these other,
like all the nodes that you have explored so far.
So uh, in terms of memory,
this is going to be order of b to the d,
kind of similar to the time.
And, and the reason is,
I have explored this guy.
And then after exploring this guy,
I need to still have like a history of where it's going to go,
because next time around when I try out this layer,
I need to know everything about this parent.
And I,- like when I- when I explore here and this is not a solution,
I need to store everything about this,
because maybe I don't find a solution in this,
in this level and I need to come down.
And when I come down, I need to know everything about these nodes.
So I need to actually store pretty much like
everything about the tree until I find my solution.
And then that's where you lose like in breadth-first search.
In terms of space, it's not going to be that great.
So in terms of space, it's now order of b to the d. It's a lot worse than what we've had.
In terms of time, it is, it is better.
It's still exponential, but it is better, okay? All right.
Okay, so now um,
let's talk about one more algorithm and then afterward we,
we jump to dynamic programming.
There is a question back there.
One thing though, the small d can be the same as the big D, right?
It can. Yeah. So, it is exponential. I agree.
Small d can be the same as big D. But in practice,
if small d is not the same as big D,
we are- we are winning a lot because, because, yeah,
these lower layers are so bad that,
that people actually like to call it- call the fact that we,
we are order of b to the small d rather than big D. Yes?
Is there a reason for why DFS would be the worst case scenario for the time enough for DFS?
Uh, so DFS needs to go all the way down to these lower, lower levels.
But BFS can stop at every level because it's doing level by level.
That can be the worst case scenario [inaudible].
Yeah. So the reason is- yeah, so like you were saying, okay,
so in DFS we were also saving some time, right?
Like why aren't we are calling that out.
And then the reason is with DFS you still need to get to these like lower layers,
and that is the, like,
that is the place that you're losing on time.
So, so the fact that you're still, like,
losing on time and surely you haven't explored these other ones,
but you have already got to these lower trees,
like, so far, um, that's pretty bad.
So, so that is why we are calling it order of b to the d in a worst case.
Okay. All right.
So this, this last algorithm I wanna to talk about is,
is an idea that tries- it's a cool idea.
It actually tries to combine the benefits of BFS and DFS.
And, and this is called,
uh, DFS Iterative Deepening.
So what this algorithm does is it basically goes level by level,
same as BFS, because then that way i- if you find a solution,
you're done, everything is great, right?
Uh, but what, what it does is for every level,
it runs a full DFS.
And, and it feels- it's like it's gonna take a long time.
But, but it's actually good because, again,
if you find your solution, like,
early on, it doesn't matter that you have ran like a million DFSs so far.
So, um, so it's kinda like an analogy of it is,
is imagine that you have a dog,
and that dog is DFS,
and it's on a leash, and you have like a short leash.
And when it is on that leash,
it's going to do a DFS and try out and search all the space,
and it doesn't find anything.
So it comes back, and then you're going to extend the leash a little bit,
and it's gonna do everything, and, like,
search everything, and do a DFS.
Comes back, doesn't find anything you extend the leash again.
So, so that's the idea.
Like extending the leash is this idea of extending your, your levels, okay?
So, uh, so how does,
how does DFS iterative deepening be? Yes?
Um, if what we're looking for in following the tree is even worse [inaudible]
Uh, say that again, say that.
So if, if what we're looking for in following the tree,
is that gonna be worse than-
Yes, exactly. Yes, that's, that's okay. That's a good point.
So the point is, uh,
the, the point that, um,
I mentioned is, if your solution is,
like, here, you are screwed.
It's worse than BFS or DFS, right?
You're doing all these DFSs through like a bigger, like,
higher-level BFS and you're- and,
and it's, it's a terrible situation.
But again, in practice, like,
we are hoping the solutions are not gonna end up like down this tree.
But yeah, if the solutions are down the tree,
then you're not, like, winning anything by, by using DFS.
What exactly, like what problems do you think DFS iterative deepening would be, like, useful?
In general, if you- okay. So the question is, yeah,
so what problems do we think DFS iterative deepening is useful?
Uh, in general, if like,
there are problems that I think BFS is going to be useful,
usually, DFS iterative deepening is useful.
The reason I would think that is, like,
there is some structure about the problem that I
would think I would find my solution earlier.
So if I, if I have some reasons or some,
some reasons about the problem,
about the structure of the problem,
and I think solutions are low depth,
I should use some of these algorithms.
And in DFS with iterative deepening in terms of space,
it helps too, so might as well use that. All right.
So, so in terms of space,
it's going to be order of small d. So in terms of
space order of small d. And then in terms of time,
you'd get the same benefits of,
uh, it gets the same benefits of, uh, BFS.
So, so that's, that's nice.
And then again, like, because it's has this BFS out of the loop,
it has the same sort of constraint on the cost.
That's gotta be a, uh,
constant constraint that cost, right?
So that is our table.
And again, in looking at this table in terms of time,
you're just not doing well, right?
Like you have this exponential time algorithms here.
And, um, we cou- could avoid
the exponential space with using something like DFS iterative deepening.
But still, this time thing is- it's just not that great, okay?
And what we wanna do now is we wanna talk about search algorithms
that bring down this exponential time to polynomial time somehow.
And then there is no magic,
we'll talk about how.
[LAUGHTER] And dynamic programming is,
is the first algorithm, okay? Yes?
You might give us ideas b to the d time in term of d space.
Uh, yeah. So it- so,
so the way iterative deepening works is,
it sets the lev- or say level is one.
So if level is one,
I'm gonna do a full DFS, okay?
Because I'm doing a full DFS in terms of space,
uh, I- it's the same as DFS in terms of space.
I just- it's just the same as the length where we find a solution.
Let's say the length where I find the solution is small d. So now,
I say level is two, my new level is two,
I'm gonna do a full DFS, okay?
[NOISE] So when I do a full DFS,
then in terms of space,
I need to- I need to just remember my pairings,
so that's why it's order of d in terms of space.
And in terms of time, it's,
it's order of b to the d because if I find my solution here,
I'm done, I don't need to,
like, explore anything else.
And, and that is exponential but exponential in,
in this smaller depth as opposed to the longer depth similar to,
similar to BFS. Yes?
I'm sorry. I still don't understand why, let's say, like,
the small d is the same as the big D, right? And-
That's a- okay. So that's a very good question. So you- I think I know it.
So you're asking small d,
if small d was the same as big D. If I had my solutions down here,
why am I, like, differentiating here between a small d and big D, right?
Is that what you're asking or am I-
I'm just gonna ask if it's, like,
the depth is quite large, like,
small d is large,
and why is it, like,
why do we need to find also a function of d?
As in why wouldn't it be, like,
d times b to the d?
Um, Oh, I see where you're saying.
So, so you're saying, okay,
like, when I'm doing,
when I'm performing DFS iterative deepening,
then I'm doing DF- DFSs.
So sure, it's order of b to the d for each of them,
but then I'm doing d of them.
And if d is really large, I should put that here.
Sure, I, I do agree that is the right time.
But again, I'm- like, in, in,
in the, in the case of this exponential,
this is so bad that that we are just dropping that,
like, we don't even worry about that,
the extra d that comes in.
But it is true, you need to have that extra d,
like, in, in general if you want to talk about it.
Kind of wanna move on to dynamic programming, but last question there.
First of all, I'm after that,
presumably though you're saving the work that you've done during the prior iterations,
so you're not really computing anything larger than O to the B, capital D, correct?
Yeah, that's right. The worst-case scenario is O to the B,
capital D. All right.
So let's move to dynamic programming.
Okay. So, so what does dynamic programming do?
So maybe I can- I'll,
I'll still use this because I might need to use this thing later.
Okay. So I'm gonna erase my parameters up on here.
Okay. So the idea of dynamic programming,
we have already seen this in the first lecture,
is I have a state s,
and I wanna end up in some end state.
But to do that, I can take an action that takes me to S-prime, right?
I can, I can end up in s-prime by cost of s and a. I can take an action that,
that ends up in s-prime.
And then from there, I can do a bunch of things.
I don't know what. But I'll end up in some end state, okay?
And, and what I'm interested in actually computing is for
this state s is to find what is future cost of s, okay?
And this part of it,
is future cost of
S prime and I don't know what it is but I can just leave it as future cost of S prime.
So if I wanna find what future cost of S is,
maybe I should make this a little bit to the right one cycle.
I'm gonna write cost of s,
a for this edge.
I'm gonna erase this.
What I'm interested in finding is future cost of my state S. So what is that equal to?
Well, that's going to be equal to this cost of s, a.
Right? Like a state S, I'm going to take action a.
So it's going to be cost of s,
a plus future cost of S-prime.
Again, I don't know what that is but that's future Dorsa's problem.
So this is future cost of S prime.
And then you might ask well what is a?
Where does a come from?
How do I know what a is?
I don't know. I'm gonna pick an a that minimizes this sum.
I'm gonna put this around it.
Okay? So future cost of S is just going to be equal to minimum of cost of s,
a, plus future costs of S-prime over all possible actions.
And it's going to be 0, if you are in an end state.
If is End of S is true.
Okay? So if I already know I'm in an end state,
then there is no future cost.
That's going to be equal to 0.
Otherwise, future cost is just going to be,
cost of going from S to the next state and then future cost computed from there.
Okay? So that is just how one would go about
formalizing this problem as a dynamic problem and they're
not a dynamic programming problem, okay?
And then how do I find what S prime is?
Well, I wrote this successor and cost function [NOISE] in my code.
Remember like we know how to find the successor given
that we are in state S and we are taking action a.
So S prime is just calling that successor function over s and a.
All right. So let's go back to some route finding example.
So, so this is slightly different route finding example.
So let's say that we want to find the minimum cost path
from going from city 1 to some city n in the future,
moving forward, we can always just move forward and it
costs c_ij to go from city i to city j.
Okay? So this this is my new search problem.
Okay? So, so this is kind of how the tree would look like.
So, so if I wanna draw this research for this,
I can start from city one,
I can end up in a city two or three or four.
Then if I'm in city two,
I can end up in three or four.
If I'm in three, I can end up in four like this is how it will look like.
Ah, I can have a much larger version of it.
If I'm talking about going to city seven,
then I have this type of tree.
And by just like looking at this tree,
you see all these sub-trees just being repeated like throughout.
If you just look at five like future cost of five,
it's gonna be the same thing.
Right? It's just gonna be the same thing throughout.
And if I use like something like tree search that we have talked about,
then I have to like go and explore like
this whole tree and then it's gonna be really time-consuming.
So, so the key insight here is future cost,
this value of future cost,
only depends on state.
Okay? So it only depends on where I am right now.
And because of that maybe I can just store that the first time that I
compute future cost of five and then like in the future,
I just called that and, and,
and I don't like recompute future costs of five.
Okay? So, so the observation here is,
future cost only depends on current city.
So, so my state in this case is current city and,
and that state is enough for me to compute future cost.
Okay? All right.
So, so if you, if you think about what we have talked about so far,
like we have thought about like these these search problems where the state we think
of it as the past sequence of actions and
the history of actions you have taken and all that.
But right now for this problem,
like state is just current city and that's enough.
Okay? So and and because of that,
you are getting all these exponential savings in time and space because again,
I can compute future cost of five there and collapse that whole tree into
this graph and just go about solving
my search problem on this graph as opposed to that that whole tree.
Right. So, so that's that's where you get the savings from, from dynamic programming.
Um, and I just wanna emphasize that again of,
let me actually do this.
So, so the key idea here is,
like I was saying there is no magic happening here.
The key idea here is is how to figure out what your state is.
It's actually important to think about what your state is.
In this case we are, we're assuming a state is summary of all parts,
all past actions that we've taken sufficient for us to choose the optimal future.
Okay? So, so that's like a mouthful but ah,
basically what that means is,
the only reason dynamic programming works.
And for this particular example we just saw,
is the state the way we define it is enough for us to plan for the future.
Like I might have a different problem where the state.
Like I define a state in a way that it's not enough for me to do a plan for future.
But if I wanna use dynamic programming,
then I gotta be smart about choosing my state because,
because that is the thing that,
that decides for the future.
So, so for example for this problem,
like I might visit city one, then three, then four, and then
six, and for solving this particular search problem,
I just need to know that I'm in city six.
That is enough. Okay? But like maybe I have some other problem that requires knowing one,
three, four, and six and and because of that maybe I need to know the full tree.
Okay? So so this is where the saving comes from like
figuring out what the state is and and defining that.
Right? All right.
So so we will come back to this notion of state
again and I think about the state a little bit more carefully.
But maybe before that maybe we can just implement
dynamic programming real quick. All right.
So let's go back to our tram problem.
I'm back to the tram problem and let's implement dynamic programming.
Okay. So how do we do this?
We're basically just writing that like
math over there into code. That, that's all you're doing.
So, so we're going to define this future cost.
If you're in an end state,
we're going to return 0.
If you're not in an end state we're just going to
add up cost plus future cost of S prime.
How do we get S-prime?
Well, we're gonna call this successor success and cost function.
So we can get action new, new, new state and costs.
And then you're gonna take the minimum of them over, over all possible actions.
So minimum of cost plus future cost of new state.
That is literally what we have on the board.
Okay? All right.
And we're returning the result.
So that is future cost.
What's your dynamic programming there?
It should, it should return a future cost over initial state. Right? Start state.
And you will return the history if you want.
In this case, I'm not returning [LAUGHTER] the history.
Okay. So how do I get savings? Well, I gotta put a cache.
Right? That's the only way I'm gonna get savings.
So um, that is where I put the cache.
And if I, if the state is already in the cache.
I'll just call my cache.
Otherwise I don't. Any question there?
[inaudible].
What's that?
Are we getting future costs?
How are we getting? Uh, say that again. Sorry, I didn't hear.
So future cost takes some states,
but what actually- is there like- uh,
do we actually have, like, a function in the menu to calculate
future costs or is that like [inaudible].
So future cost is going to be,
uh- yeah, so, so we have this function, right?
Future cost over state.
But you're going to call future cost- so, so,
so future cost over state is going to be equal to cost of state and actions,
in this function I'm saying all possible actions,
try that out, plus future costs of S prime.
And S prime comes from the successor and, and, and cost function, uh,
successor and cost function. All right.
So- and then, yeah- and so,
so we do the caching,
the proper caching type of way of doing this too.
And now we have dynamic programming.
So we can basically call this over,
uh, our tram problem.
So I'm gonna, I'm gonna move forward.
Okay. So let's do print solution,
dynamic programming over our problem.
Uh, you can, again, play around with this.
The only way I'm checking this is if it gives me
the same solution as backtracking search because I knew how that works, right?
So let's just call it on ten.
And, yeah, it gave me the same, the same answer.
So I can play around with this, okay? All right.
So, uh-huh, let's go back.
Okay. So one assumption that we have here,
to just point out, is we are assuming that this graph is going to be acyclic.
So, so that's, that's an assumption that we need to make
when we are solving this dynamic programming problem.
And, and the reason is,
[NOISE] well, we need to compute this future cost, right?
For me to compute future costs of S,
the S, S prime,
I need to, like,
have thought about- sorry.
For me to compute future costs of S,
I need to have thought about future costs of S prime.
So there is, kind of, this natural ordering that exists between my state.
So if I think about an example where there are cycles,
then, then I don't have that ordering, right?
If I want to compute, let's say,
I want to go from A to D here,
and on B, C. So if I want to compute future cost of B,
I don't really know if I should have computed future costs of A before
or C before or what order should I have gone to compute,
like, future costs of B?
So, so you actually need to have some way of
ordering your states in order to compute these future costs and,
and apply dynamic programming.
So that's why, like, we can't really have cycles,
like, when we, when we think about this algorithm.
But we are going to talk about, uh,
uniform cost search which actually allows us to have cycles,
like, in a few slides. Yes.
So when is the run time of the dynamic programming?
So the run time of this is actually polynomial time in the order of states.
So order of n.
O of n?
Yeah O of n, where n is the number of states.
Yeah. Okay. All right.
So- all right.
So let's talk about the idea of states a little bit
more because I think this is, this is actually interesting.
All right. So, so let's just reiterate. What is a state?
State is a summary of all past actions
sufficient to choose future actions optimally, okay?
So, so everyone happy with what state is?
So now, what we want to do is,
we want to figure out how we should define our state space.
Because, again, this is an important problem, right?
Like, how we we're defining state space is
the thing that gets the dynamic programming working.
So, so we got to, we got to think about how to do that.
So, so let's go back to this example,
and let's just change that a little bit.
So, so this is the same example of,
I'm going from city one to city n,
I can only move forward,
and it cost C_i_j to go from any city i to city j,
and I'm going to add a constraint.
And the constraint is,
I can't visit three odd cities in a row, okay?
So what that means is,
um, [NOISE] maybe I'm in state one.
And then, I went to state three,
or city one, I went to city three.
And then after that,
can I go to city seven or- no,
based on this constraint that I've added,
I, I, like, can't do that, right?
So I want to define a state space that allows me to keep track of these things,
so I can solve this new search problem with this new constraint.
So, so how should I, how should I do that?
[NOISE] So in, in the previous problem,
when we didn't have the constraint,
our state was just a current city.
Like previously, we just cared about the current city.
And the reason we cared about the current city is like,
is like we are solving the search problem, like, we end up in a city.
We need to know how I'm going- where I should go from three.
So I should, I should have my current city in general, right?
So, so for the previous problem without the constraint,
current city was enough.
But, but now current city is not enough, right?
I actually need to know, like,
something about my past, okay? Yes.
[inaudible] have a count of how many that's odd states.
Yeah. That's actually a very good point. [NOISE] Yeah.
And so, so one suggestion is,
have a count of how many odd states.
Not only maybe, like- and the-
maybe the first thing that would come to our mind is something simpler.
So maybe we say, well,
the state is- maybe I'll write previous city just to be similar to the slide.
The state- like, when we say, well,
the state is previous city and current city.
Okay? So this is one possible option for, for my state, right?
Because, because if I have this,
if I have this guy as my state,
and then that is enough, right?
Like if I- my current city is three,
I know my previous city was one.
I know I shouldn't go to seven,
like that's enough for me to make,
like, future decisions, okay?
But there is a problem with this.
Well, what is the problem?
So I have n cities, right?
So, so current city can take n possible action and n possible states,
previous city can also take n possible options,
has n possible options.
So if I think about the size of my state space,
it is n squared.
If I decide to choose the state, okay?
If I, if I decide to choose the state,
I'm going to have n squared states.
And remember, we are doing this dynamic programming thing, like, we need to actually,
like, write down, like,
all the- like, how to get from all those states.
That's gonna be big. But there is an improvement to this.
And that's an improvement that you suggested, which is,
I don't actually need to have this whole giant previous city which has n options.
I can just have a counter to just know whether the previous city was odd or not.
Like, that's enough, right?
Like if I- I don't care if it was one or three or whatever.
Like, I just care to know if previous city was odd or not.
So, so another option for- I'll write it here.
Another option for my state is to know if previous was odd or not, okay?
And then I need to know my current city again, right?
Current city we need that because,
like, we need to know how to get from there.
And then this brings down my state space,
like, how does it bring down my state space?
Because, well, what's the size of my state space?
This guy can take n possible, uh, states.
If my previous city was odd, that's two, right?
Like, so I just brought down my state space from something that was n squared to 2n,
and, and that's a good improvement.
So in general, when you're picking these state spaces,
you should pick the minimal, like,
sufficient thing for you to make decisions.
So it's got to be a summary of all the previous actions and
previous things that you need to make future decisions,
but pick the minimum one because you're storing these things,
and it, it actually matters to pick the smallest one.
So, so here is an example of, like, exactly that.
So, so my state is now this tuple of whether
the previous city was odd or not, and my current city.
So if I start at city 1,
well, like, I don't have a previous city,
and I'm at city one, I could go to city three,
and I end up in odd and three.
I could try to go to city seven, well,
that's not possible because now I have listed three states, and,
and I end up here,
and there are, like,
the rest of the tree, you can have any other examples. Yeah.
[inaudible].
So, so the way I'm counting this is, how my- so,
so my state is a tuple of two things, right?
If the previous city is odd or even,
I have two options here.
It's either odd or even, that's two.
And then my current city. And I have n possible options for my current city.
It could be city one, city two, city three,
so that's n. So I have n options here.
I have two options here.
That's why I'm saying my whole state space is two times n, okay?
All right. Okay. So let's try out this example.
Let's not put it in.
Uh, just talk to your neighbors about this,
and then maybe, if you have ideas just let me know in a minute.
So- okay. So what is the difference here?
So we're traveling from city one to city n,
and then the constraint is changed.
Now, we want to visit at least three odd cities.
So that's what we wanna do.
And then the question is, what is the minimal state?
Talk to your neighbors. [NOISE]
All right. Any ideas?
Any ideas? [BACKGROUND] What is a possible state?
Like it- don't worry about the minimal even, like for now.
Like what do I need to keep track of?
Number of odd cities.
Number of, number of odd cities?
Yeah.
Okay. So- and is that it?
Do I need to just know the number of odds cities?
Um, or number of odd is about your, uh, [OVERLAPPING]
So number- so, so what I meant is I also need to have current city, right? So, okay.
So one possible option for this new example,
I'm gonna write that here,
is I want to visit at least three odd cities,
I also need my- to know my current city,
for any of these types- like,
not any of these types of problems,
for these particular problems that I've defined here,
I need to know where I am.
So I need to know what my current city is.
So- so that is, like,
that is given what I need to have that, okay?
So I want to see at least three odd cities.
So one possible option is to just have a counter and keep
counting number of odd cities, okay?
So this could be one potential state, okay? Yes?
Do the cities have to be different or it could be one, three, one?
So, um, okay, so the question is do the cities need to be different?
The way we are defining the problem is we are moving forward.
If I'm in one, like, I can just just move forward.
I can't like stay at one or I can't, like, go back.
So- so we're always moving forward.
But when we talk about the- the state space,
we are talking about the more general, like, setting.
Like, some- some of that 2N might not even be possible,
but- but that's the way we are counting, okay?
All right. So- so this is one option,
but I can actually do better than this. Yes?
[inaudible] you need at least three odd cities,
and then you need at least two odd cities,
then you need at least one odd city and then you're-
And then you're done. Right. So- so a suggestion there is we can- we can have,
like, you can- you can start, like,
saying you need at least three odd cities,
then you need at least two odd cities,
then you need at least one- one odd city and then you're done.
And one way of formalizing that, that's exactly right, right?
I only care if I have four odd cities now,
or five odd cities, like,
as long as I have like above three,
that's- that's good enough, right?
One odd city, two odd city, three odd city,
above that is just three plus,
like- like that's enough for me, okay?
So if I have this,
then the state space here is going to be N options here,
and number of odd cities,
it's around N over 2, so it's going to be N squared over 2.
But if I use this- this new suggestion,
where I don't keep track of four, five, six,
seven, I just keep track of one,
two, and three plus,
then my state space ends up becoming 3 times N,
and I- I can formally write that as S is equal to minimum of number of odd cities,
and three, and then current city,
you need the current city.
And with this state space,
then the size is equal to 3N, okay?
So I just, again, brought down N squared to N,
and that's- that's a nice improvement. Yes?
Do you not also need an option for zero odd cities specific to [inaudible]
Zero. We're starting from city one,
so we're already counting that in, but yeah, like,
if you have zero odd cities,
that is a good point too.
All right. So I've gotta move.
Okay, so, um, that was that.
This is how it looks like.
Like you can think of your state space like this again as a tuple of I visited one,
two, three, and- and then the cities.
I have another example here,
you can think about this later and yeah,
like, work, work it at home.
But, uh, basically the question is, again,
you're going from city one to N,
and you want to visit more odd cities than even cities.
What would be the minimal state space?
But we can talk about it offline.
So the summary so far,
is- is that state is going to be a summary of
past actions sufficient to choose future actions optimally.
And then dynamic programming,
it's not doing any magic, right,
it's using this notion of state to bring down
this exponential time algorithm to a polynomial time algorithm,
and then, with the trick of using memoization,
and with a trick of choosing the right state, okay?
And we have talked about dynamic programming and how it doesn't work for acyclic graphs.
And now, we want to spend a little bit of time talking about uniform cost search, uh,
and how that can help with the- with the cycles.
So if you guys have seen Dijkstra's algorithm,
this is very similar to Dijkstra's, like, yeah.
So- so it's basically Dijkstra's. But- all right.
So let's- let's actually talk about this.
So- so the observation here is that when we- when we think about
the cost of getting from start state to some s prime,
well, that is going to be equal to cost of going from s
to s prime and then some past cost of s, okay.
And then when dynamic programming,
let's make sure that we have this ordering and these things are computed in order,
so we're not worried about, like,
visiting the state, like, multiple times.
But- but in- in uniform cost search,
we might visit a state multiple times,
and if you have cycles, we don't know what order to go.
But the order we can go is we can actually compute a past cost- a suggested past cost,
and- and basically, go over the states based on increasing past cost, okay?
So, um, let me actually- yeah,
so- so uniform cost search,
what it does is it enumerates states in an order of increasing past cost.
So- and- and in this case,
we need to actually make an assumption here,
we need to assume that the- the cost is going to be non-negative.
So- so I'm making this assumption for uniform cost search.
So here is an example of uniform cost search running- oh, we don't have internet,
I just- yeah, there is a video of uniform cost search running in action.
If I have time, I'll connect to internet and get it working.
But- so- so let's talk about the high level idea of uniform cost search.
So in uniform cost search,
we have three sets that we need to keep track of.
One is explored set,
which is the states that we have found the optimal path.
These are the states that we are sure, like, how to get to,
we have computed the best path possible to get there,
we are, like, done with them, okay?
Then we have another set called a frontier,
where this frontier are the states that we have seen,
we have computed like a cost of getting there,
like we know, somehow,
how to get there and what would be the cost,
but we're just not sure about it, like, like,
we're not sure if that was the best way of getting there, okay?
So- so the frontier,
you can think of it as a known unknown.
I know they exist, but, like,
I actually, I'm not sure what's the optimal way of getting there.
And then finally, we have this unexplored part of states.
And these unexplored part of states,
I haven't even seen them yet,
I- I don't even know how to get there,
and you can think of it as more of an unknown unknown.
So- so that's, like,
how you would think about these three.
So let's actually work out an example for uniform cost search.
I'm actually going to do this one.
So- so I'm just gonna show how uniform cost search runs on this example.
So I said we are going to keep track of three sets: unexplored,
frontier, and then explored.
Explored. Okay? All right.
So everything ends up in unexplored at the beginning, A, B, C, and D.
And what I wanna do is I wanna go from A to D,
that- that's what I wanna do, okay?
So I wanna find the minimum path cost- path- minimum cost path to get from A to D,
given that I have this graph, okay?
So what I'm gonna do is I'm gonna take my initial state,
that's A. I am going to put A on my frontier,
and it costs zero to get to A because I'm just starting at A, okay?
So that's on my frontier, then in the next step,
what I'm gonna do is I'm going to pop off the thing
with the lowest cost from my frontier.
There's one thing on my frontier,
I'm just gonna pop off that one thing off my frontier,
I'm gonna put that to explored,
the cost of getting to A is 0.
And then, what I'm going to do is after popping it off from my frontier is,
I'm gonna see how I can get from A to any other state.
So from A, I can get to B,
that's one option, and with the cost of 1.
So from A, I can go to B with a cost of 1.
Where else can I go? I can go to C with a cost of 100.
Okay? So what I just did is I moved B from unexplored to frontier,
and then I- I know how I- to get there from A,
and I moved C to the frontier,
and I know how to get from there.
Okay? So now it's the next round,
I'm looking at my frontier,
A is not on my frontier anymore, it's in explored.
And I'm going to pop off the thing with the best cost off my frontier.
Well, what is that? That's B.
So I'm going to move B to my explored.
The way- the best way to get to B,
I already know that, right?
That's from A to B. Everything is good.
Okay? So now that I've popped off B from my frontier,
I'm gonna look at B and see what states I can get to from B.
From B, I can go to A,
but A is already in explored, like,
I already know the best way to get to A,
so- so there is no reason to do that.
From B, I can get to C,
and if I want to get to C,
then I can actually get to C with the cost of
1 plus whatever cost of B is already, 1.
So what I'm gonna do is I'm going to erase this,
because there is a better way of getting there,
and that's from B, okay?
And then, from B, I can get to D. So I'm gonna move D from unexplored to frontier.
I can get to it from B.
And then, how do I get to it from B?
There's a cost of 101, right?
Because 100 plus cost of getting to that, okay?
All right. So I'm- I'm done exploring everything I can do from B.
Going back to my frontier again.
So these two are not on my frontier.
I just have C and D on my frontier.
I'm gonna pop off the thing with the best cost,
that is C. I'm gonna move that to explored with a cost of two,
and the way to- the best way to get that is from B, okay?
So we're done with C. And then,
we're gonna see where we can go from C. From C, I can go to A.
Well, that's done, that's already on
the explored- in- in the explored set, I'm not gonna touch that.
Similar thing with B, already in the explored,
don't need to worry about that.
From C, I can get to D, right?
And if I want to get to D from C,
well, what would be the cost of that?
It would be 2 plus 1.
So I can update this and have 3.
And I can update the way to get to D from here.
And then, we're done,
we go to frontier.
The only thing that's left on the frontier is- is D. I'm going to just pop that off,
and then I'm going to add that to explored.
And that is 3. And that's what I have in my explored.
So the way to get from A to D is- is by taking this route, and it costs 1.
So A, B, C, and D. Okay?
Is that- is that clear? All right.
Okay. So there are two slides left and they're probably gonna kick us out soon,
so I'll do this next time.
So- so yeah, the two- two slides left is one is
going to just go over the- the pseudo-code.
So take a look at that, the code is online.
And there's a small theorem that says,
this is actually doing the right thing.
I'll talk about that next time.
 Okay. So, Hi, everyone.
So, uh, our plan for today is to continue talking about search.
So, so that's, uh,
what we're going to start doing,
finish off some of the stuff we started talking about last time,
and then after that, uh,
switch to some of the more interesting topics like learning.
So a few announcements.
Um, so the solutions to the old exams are online now.
So if you guys wanna start studying for the exam, you can do that.
So, so start looking at some of those problems,
I think, that would be useful.
Um, actually, let me start with the Search 2 lecture because I think that might be,
like, that has a, a review of some of the topics we've talked about.
So it might be easier to do that.
Also, I'm not connected to the network,
so we're not gonna do the questions, uh,
or show the videos because I have,
I have a hard time connecting to the network in this room.
Okay. All right.
So, so let's start- continue talking about search.
Uh, so if you guys remember, uh,
we had this, this city block problem.
So let's go back to that problem and let's just try to do a review of some of the,
some of the search, search algorithms we talked about last time.
So, uh, so suppose you want to travel from City 1 to City n only going forward,
and then from City n you wanna go backwards,
so and back to City 1 going only backwards, okay?
So, so you- so the problem statement is kind of like this.
You're starting in City 1,
you're going- you're going forward and you're getting to some
City n. So maybe we're doing that on this.
And then after that,
you wanna go backwards and get to,
get to City 1 again.
So you go into some of these cities, okay?
So, so that's the goal,
and then the cost of going to- from any city i
to city j is equal to cij, okay? So, so that's it.
So, the question is: What- which one of
these following algorithms could you use to solve this problem?
And it could be multiple of them.
So- so we have depth-first search,
breadth-first search, dynamic programming,
and uniform cost search.
And these were the algorithms we talked about last time.
So, uh, maybe just talk to your neighbors for a minute
and then we can do votes on each one of these. Yes, question?
Just needed to ask [inaudible]
The [OVERLAPPING]?
Okay. Let me check that again. Thank you. Thank you for.
[BACKGROUND]
All right, so let's maybe start talking about this.
So how about depth-first search like how many people think we can use depth-first search?
How many people think we can't use depth-first search?
There's -a very like good split.
[LAUGHTER] So, some of the people think we can't use depth-first search,
what, what are some reasons maybe just like call it out.
The depth first-search, the assumption was that based upon the cost is zero.
Yes, that's right. Yeah, so here we are basically going from
City 1 to city n. Each one of these edges had a cost of cij.
I'm just saying cij is greater than or equal to 0.
That's the only thing I'm saying about cij.
But if you remember depth-first search,
you really wanted the cost to just be equal to 0 because if you remember that whole tree,
like the whole point of depth-first search was
I could just stop whenever I could find a solution.
And we were assuming that the costs of all the edges is just equal to zero.
So we can't really use depth first search here,
because, because our cost is not 0.
So assuming, like now that you know that reasoning,
how about breadth first-search?
Can we use breadth-first search? Yes?
All of that moving from one city to city n that is not the city n.
So that's a good point.
So, so what suggesting is can we think about the problem as
going from City 1 to City n? And then after that,
like introduce like a whole new problem that
continues that and starts from City n and goes to City 1.
Let me get back to that point like in a second,
because like you could potentially think
about that -actually like that might be an interesting way of thinking about it.
But, but irrespective of that I can't use depth first-search.
So I'm -so far I'm just talking about depth first-search.
Irrespective of how I'm looking at the problem,
the costs are gonna be uh, non-zero.
So because the costs are going to be non-zero,
I can't use depth-first search.
So, so let's talk about that first.
So how about breadth first-search?
Can I use breadth-first search?
[inaudible]
That's exactly right. So we cannot use
breadth-first search here because for breadth first-search.
If you remember, you really wanted all the costs to be the same.
They didn't need to be 0,
but they needed to be the same thing because then you could just go over the levels.
And here I'm not- like I'm not saying I'm not putting
any restrictions on cij being the same thing.
Okay? So now let's talk about dynamic programming.
How about dynamic programming?
Can we use dynamic programming?
All right, so that looks right,
right you like we could use dynamic programming here.
Everything looks okay, cij's are positive, looks fine.
Um, how about, um, actually one question?
So, so don't they have cycles here? We kind of,
briefly talked about this already. So, don't I have like this cycle here?
Uh, we can think about possibly going from one to n and then n to one.
Yes, so this is a suggestion that,
that we have already like heard twice.
So we could actually use dynamic programming here
even if it kinda looks like we have a cycle and the reasons we
can kinda use this trick were we can basically draw this out again.
And for going forward basically go all the way here,
and then after that we're going backwards, kind of include the directionality too.
So all I'm doing is I'm extending the state,
the state space to not just be the city but
be the city in addition to that, it would be direction that we're going.
So if I'm in City 4 here,
it's City 4 going forward.
And if at some point in the future I'm in City, I don't know, 4 again,
it's City 4 going backwards.
So I'll keep track of both the city and the directionality.
And when I do that then I'm kind of breaking the cycle.
Like I'm not putting any cycles here and I can actually use dynamic programming, okay?
Does that make sense? And then uniform cost search.
That, that also sounds good too, right?
Like Uniform cost search, you could actually use that.
Doesn't matter if you have cycles or not.
And then we have positive, positive, non-negative costs.
So we could use uniform cost search.
Okay? All right, so this was
just a quick review of some of the things we talked about last time.
And, um, another thing we talked about last time was this notion of state.
Okay, so, so we started talking about tree search algorithms and at some point, uh,
we switched to dynamic programming and uniform cost search where we are,
uh, like we don't need to- like we don't need to have this exponential blow up.
And the reason behind that was we have memoization.
And in addition to that we have this notion of state.
Okay? And so, what is a state?
A state is a summary of all past actions that are
sufficient for us to choose the future optimally.
So, so we need to be really careful about choosing our state.
So in this previous question,
uh, we looked at past actions.
So if you look at like all cities that you go over it can be in City 1,
then 3, then 4, 5,
6 and city 3 again.
So in terms of state,
the things that you wanna keep track of is what city you are in.
But in addition to that, you wanna have the directionality because you,
you need to know like where you are and how you're getting back.
Okay? So, and we did a couple of examples around that trying to figure out what is,
what is like a specific notion of state for various problems. All right.
So, so we started last time talking
about search problems and, and we started formalizing it.
So if you remember our paradigm of modeling and
inference and learning we started kind of modeling
search problems using this formalism where we defined a starting state, that's s start.
And then we talked about the actions of s,
which is a function over our states which returns all possible actions.
And then we talked about the cost function.
So the cost function can take a state and action and tell us what is the cost of that,
that, that, that edge.
And then we talked about the successor function which takes
a state and action and tells us where we end up at.
And again, we had this end function that was just checking if you're in an end state or not.
So these were all the things that we needed to,
to define a search problem and we kind of
tried that and a couple of examples to try an example.
The City example, all of that. Okay?
And then after talking about these,
these different ways of, um,
thinking about search problems, um,
we started talking about various types of inference algorithms.
So we talked about tree search.
So depth first search, breadth first search,
depth first search with iterative deepening, um, backtracking search.
And then after that we talked about some of these graph search type algorithms like,
uniform cost search an- and, uh, dynamic programming.
So last time we did an example of,
um, uniform cost search but we didn't get to prove the correctness of it.
So I want to switch to some of the last,
er, last, last time's, um,
slides to just go over this,
this quick theorem and then after that just switch back to, to this lecture.
Okay. So uniform cost search.
Like, if you remember what we were doing in uniform cost search,
we had three different sets.
We had an export set which was basically the set of states that we have visited,
and we are sure how to get to them,
and we know the optimal path,
and we know everything about them.
We had this frontier set which was a set with,
with a set of states that we have got to them,
but we're not sure if,
if the cost that we have the best cost, cost.
There might be a better way of getting to them and you don't know it.
Like you're not sure yet.
And then we have the unexplored, er,
set of states which are basically states that we haven't seen yet.
So we did this example where we started with all the states in
the unexplored set and then we moved into the frontier and then from the frontier,
we move them to the explored set.
So, so this was the example that we did on the board.
Okay? And, and we realized that,
like, even if we have cycles,
we can actually do this algorithm and then we,
we ended up finding the best path being from A to B to C to D and that costs 3.
So, uh, let's actually implement uniform cost search,
uh, so I think we didn't do this last time.
So going back to, um,
our set of, ah, so,
so we started writing up these algorithms for search problems.
So we have, we have written dynamic programming already and backtracking search.
So now we can, we can try to kind of implement uniform cost search.
And for doing so,
we need to have this priority queue data structure.
So this is in a util file.
I'm just showing you what it like what functions it has,
it has an update function,
and it has a remove min function.
So, so it's just a data structure that I'm gonna use for my frontier.
Because like, my frontier I'm popping off things off my frontier.
So I'm going to use this data structure. All right.
So let's go back to uniform cost search.
So we're going to define this frontier,
where we are adding states to- from unexplored sets,
you're adding states to the frontier.
Okay? And it's going to be a priority queue so,
so we have that data structure because we've just imported util.
And you're going to basically add the start state with a cost of 0 to the frontier.
So that's the first thing we do.
And then after that, like,
while the frontier is not empty.
So while true, what we're going to do is, uh,
we're going to remove the minimum, uh,
past cost element from the frontier.
So, so basically just pop off the frontier that the best thing that exists there,
and just move that to the explored set.
Okay. So when I pop off the thing from the frontier,
basically I get this past cost and I get the state.
Okay? All right.
So, so if, if,
er, you're in an end-state,
then you're just going to return that past cost with the history.
I'm not putting the history here for now, I'm just returning the cost.
Okay. So after popping off this state from the frontier,
the thing we were doing was you were adding the children of that.
So, um, the way we do that is we're gonna
use this successor and cost function that we defined last time.
So we can basically iterate over
action new state and costs and this successor and cost function.
And, and basically update our frontier,
by adding these new states to it.
Okay. And then the cost that you are going to add
is cost plus past cost if, if that is better.
So, um, so that's what the update function of the frontier does.
And that's pretty much it.
Like that is uniform cost search.
You add stuff to the frontier,
you pop off stuff from the frontier.
And, and that way you explore and remove things from unexplored set to the explored set.
So let's just try that out.
Looks like it is doing the right thing.
So it got the same value as dynamic programming.
So, er, looks like it kinda works okay.
[NOISE] So, um, this code is also online.
So if you want to take a look at it, um,
later, actually it's not what I wanted.
Um, yeah.
Okay. All right.
So, so that was- and here's also the pseudo-code of uniform cost search.
Okay? Okay. So we have- is there a question right there?
What's the runtime of uniform cost search [inaudible].
That's a good point. So so what's- the question is
what's the runtime of uniform cost search?
So the runtime of uniform cost search is order of n log n,
where the log n is because of,
like, the bookkeeping of,
of the priority queue,
uh, and you're going over all the edges.
So, so if you can think of n here as
the edges and worst-case scenario if you have a fully connected graph,
it's technically n squared log n. But in practice, er,
we have [inaudible] graph so people usually refer to that just
n log n where n is the number of states that you have explored.
And it's actually not all of the states.
It's the states that you have explored.
Okay? And dynamic programming,
it's order of n. So technically, like,
dynamic programming is slightly better but really depends.
Yeah, certainty. Actually go first and then I'll get you back.
Is the only difference between this and Dijkstra's is that you
just don't have all [inaudible] beginning?
That wasn't- the question is what's the difference
between this and Dijkstra's algorithm,
they're very similar, the only difference is,
this is trying to solve a search problem.
So you're not like exploring all the states.
When you get to the solution,
you get to the solution and then you just return that Dijkstra,
you're going from- you're basically exploring all of,
all of the states in the- in your graph. What's your question?
[inaudible].
All right. Sounds good.
Okay. So, uh, I just want to quickly,
er, talk about this correctness theorem.
So, so for uniform cost search we actually have a correctness theorem
which basically says uniform cost search does the right thing.
So, uh, what basically this theorem says is,
if you have a state that you are popping off the frontier and removing
it from the frontier to the explored, then it's priority,
that value which is equal to past cost of s is actually the minimum cost of getting to,
to, to the state s. So what this is saying is,
let's say that this is my explored set.
So this is my explored set,
and then right here is my frontier,
and I have a start state, okay?
And then I have some state s,
that right now I have decided that I am popping off s from
the frontier to explored because that is the best thing that has the best past cost.
So what the theorem says is, this,
this path that I have from s_start to s,
is the shortest path possible to get to get to the state s. Okay.
So the way to prove that is to show that the cost
of this path is lower than any other path,
paths that go from s_start to s. So let's say there is some other path,
this green one, that goes from s_start to s some other way.
And, and the way that it goes to s is it should probably leave the,
the explored set of states from some state called t maybe to
some goes- go to some other state u and
then from u go to s. u and s can be the same thing.
But what the point of it is,
if I have this other path that goes through- to s,
it needs to leave the explored set from some state t. Okay.
So what I want to show is I want to show that the,
the cost of the green line,
I want to show that that is greater than the cost of the black line.
Okay. All right.
So the cost of the green line,
what is the cost of the green line?
It's gonna be the cost to here,
and then cost of t to u,
and the cost of u to s. So I can say well,
this cost is actually greater than or equal to,
um, priority of t,
because that is the cost of getting to t,
plus cost of t to u.
And I'm just dropping this, this last part.
The u to s, I'm just dropping it.
Okay. So cost of green is at least equal to priority of t plus cost of t. t, t to u.
Okay. Well, what does that equal to?
Priority is just a number, right?
It's just a number that you are getting off the,
the, the, priority queue.
So that is actually equal to past cost of T,
plus cost of t to u.
Okay. And, and this value is going to actually be greater than or equal to priority of u.
Well, why is that?
Because if u is in my frontier,
I've, I've visited u.
So I already have some priority value for u.
And, and the value that I've assigned for the priority of u,
is either equal to this past cost of t plus cost of t,
t to u, because I've like,
seen that using my explored, using my frontier.
So I've definitely seen this or it is
something better that, that I don't know what it is.
Right? So, so priority of u is going to be less than or
equal to this past cost of t plus cost of t to u.
Okay. And well, what do I know in terms of priority of u and priority of s?
Well, I know priority of u is going to be greater
than or equal to priority of s. Well, why is that?
Because I already know I'm popping off s next, I'm not popping off U,
like, like I've- I know I'm popping off the,
the thing that has the least amount of priority,
and the least value here,
and that's s, and well, that is equal to,
er, cost of the black line, black line. Okay.
All right. So that was just a quick,
like proof of why the uniform cost search always returns
kind of the best minimum cost path type [NOISE].
All right. So let's go to the slides again.
So, um, just a comparison,
quick comparison between dynamic programming of uniform cost search.
So, uh, we talked about dynamic programming.
We know it doesn't allow cycles,
but in terms of, uh,
action cost, it can be anything like,
like you can have negative costs, you can have positive costs.
And, er, in terms of, um,
complexity is order of n,
and then uniform cost search,
you can have cycles. So that is cool.
But the problem is, the costs need to be non-negative,
and into order of n log n. And if you have- if you end up in
a situation where you have cycles and your costs are actually negative,
there is this other algorithm called Bellman-Ford,
that we are not talking about in this class,
but you could actually like have
a different algorithm that addresses those sort of the things.
Okay. All right, how am I doing on time?
Okay. So that was,
that was this idea of inference.
Right now we have like a good series of ways of going about doing inference,
uh, for search problems, you have to formalize them.
And now the plan for this lecture is,
is to think about learning.
So how are we going to go about learning when we have search problems?
[NOISE] And when our search problem is not fully specified,
and there are things in the search problems that are
not specified and you want to learn what they are,
like the costs, okay.
So, uh, so that's going to be the first part of the lecture,
and then towards the end of the lecture,
we're going to talk about a few other algorithms that make things faster.
So, so smarter ways of making things faster.
We're going to talk about A star and some sort of
relaxation type strategies, okay. All right.
So, um, so let's go back to our transportation problem.
So, so this was our transportation problem where,
er, we had a start state and we can either walk,
and by walking we can go from state s to state s plus 1,
and that costs one, or we can take a tram,
a magic tram that takes us from state s to state 2s,
and that costs 2, okay,
and we want to get to state n. So,
uh, we can formalize that as a search problem.
We can like we saw it- we saw this last time,
we can actually try to find what is the best path to get from
state 1 to any state n like we saw- like path- like walk walk,
tram tram tram, walk tram tram.
This is one potential like optimal path that one can get, okay?
But the thing is, uh,
the world is not perfect like,
like modeling is actually really hard,
like it's not that we always have this nice model with everything.
And we could end up in scenarios where we have a search problem,
and, and we don't actually know what the costs of our actions are.
So we don't actually know what the cost of walking is,
or what the cost of tram is.
But maybe we actually have access to,
to this optimal path.
Like, maybe I know the optimal path is walk walk tram tram tram,
walk tram tram, but I don't know what the costs are.
So the point of learning is,
is to go about learning what these cost values are based on this,
this optimal path that we have.
So, so I want to actually learn the costs of walking is 1,
and the cost of tram is 2.
And this is actually a common problem that we have like in machine learning in general.
So like for example, um,
you might have data from, uh,
how a person does something or like how a person,
let's say, like grasps an object.
And I, I have no idea what was the cost that
the person who was optimizing to grasp an object,
right, but I have like the trajectory I know like what,
what the path they took when they picked up an object.
So what I can do is,
if I have access to that path of how they picked up an object,
then from that I can actually learn what was the cost function that they were optimizing,
because then I can put that cost function maybe on a,
on a robot that does the same thing. Question?
[inaudible] like five or something.
That's a good question. So the question is,
is it possible to have multiple solutions here?
Yes, so we are gonna actually see that like later,
like what sort of the solutions that we gonna get, are there,
ther- there could be cases where we have multiple solutions.
The ratio of it is the thing that matters.
So if you have like, walk is 1, tram is 4,
if you get to an 8, you kind of get the same sort of behavior.
Uh, and then it also depends on what sort of data you have.
Like if your data allowed you to actually recover the,
the, the true solution.
So, so we're gonna actually talk about all these cases, okay? All right.
Okay. So if you think about it,
when the way- the search problem we were trying to solve,
this, this was the inference problem,
was when you are,
you are given kind of a search formulation and you are given a cost,
and, and our goal was to find the sequence of actions,
this optimal sequence of actions,
that was the shortest path or the best path and,
and some path or some way,
and this is a forward problem.
So search is this forward problem,
where you're given a cost and you want to find a sequence of actions, okay.
So it's interesting because learning in some sense is,
is an inverse problem.
It's the inverse of, of search.
So the inverse of search is,
if you give me that sequence of actions,
the, the best sequence of actions that you've got,
then can you figure out what the cost is?
So, so in some sense you can think of learning as this inverse problem of,
of search and, and we are going to kind of address that.
So I'm going to go over one example to,
to talk about, er, learning.
Um, and I'm actually going to use the notation of, uh,
the machine lea- learning lectures that we had,
um, at the beginning of like last week basically.
So, um, let's say that we have,
ah, maybe I can draw this.
[NOISE] Um, yeah, I will just draw the scheme.
So let's say we have a search problem without costs,
and, and that's our input.
So if- so, so we are kind of framing this problem of learning as a prediction problem.
And if you remember prediction problems,
in prediction problems we had, ah, an input.
So our input was x, okay.
And in, in this case you are saying our input is a search problem,
search problem without costs, okay?
So that is my input.
And then we have outputs.
And in this case my,
my output y is this optimal sequence of actions that one could get- gets,
so it's the solution path,
so it's a solution path, okay.
And what I wanna do is,
I wanna- like, like if you remember machine learning,
the idea was, I would wanna find this predictor, this f function,
f that we take an input, f of x,
and then it would basically return
the solution path in other settings and it would generalize.
So, so that was kind of the idea that we explored in machine learning,
and you kinda wanna do the same thing in here.
So, uh, let's start with- um,
I'm going to draw that here.
So let's start with an example where we are in city 1,
and then maybe we walk to city 2,
so we can walk to city 2.
And then from there,
maybe I have two options.
I can keep walking to get to city 4.
So I can do walk walk walk.
Or maybe I can take the tram and end up in city 4, okay?
And, and the thing is I don't actually know what the costs of these,
these actions are, I don't know what the cost of do- uh,
walk is, what the cost of tram is.
Okay? But one thing I know is that my, my solution path,
my y is equal to walk, walk, and walk.
So, um, so one way to go about this is
to actually start with some initialization of, of these costs.
So the way we're defining these costs are going to be,
uh, I'm going to use the word,
um, I'm gonna write here maybe.
I'll just write up here.
I'm going to use w like,
because I want to use the same notation as as the learning lectures.
So w is going to be the weights that o- of,
of each one of my actions.
I have two actions.
In this case I can either walk or I can take the tram so I'm going to call them action 1.
So w of action 1 is w of walking.
And then w of action 2 is w of taking the tram.
So action 2 is taking the tram.
So I'm defining these w values,
and the way I'm defining these weights is just as a function of actions.
This could technically be a function of state and actions but right
now I'm just simplifying this and I'm saying the w's is
this values, the costs of walking just depend-
the cost of going from 1-2 just depends on my action.
It doesn't depend on what state I'm in.
You could imagine settings where it actually
depends on like what city you are in too, okay?
So, so then under that scenario what is the cost of, cost of y?
It is going to be w walk,
plus w walk, plus w walk.
Okay? So what I'm suggesting is let's just start with something.
Let's just start with- yeah,
like let's just start with these weights.
So I'm gonna say walking costs 3.
And it's always going to cost 3.
Again, the reason it's always going to cost 3 is
I'm basically saying my weights only depend on the action,
they don't depend on state.
So it's always going to cost three.
And I'm going to say well why not let's just say, the tram takes the cost of 2.
Okay? So this doesn't like look right but
like let's just say I assume this is the right solution, okay?
So now what I wanna do is I want to be able to update these weights,
update these values in a way that I can get this optimal path that I have,
this, this walk, walk, walk.
Okay? So how can I do that?
So I started with these random initializations of what the weights are.
Okay? So now that I've done that I can,
I can try to figure out what is the optimal,
optimal path here based on these weights.
So what is my prediction,
so that is y prime.
That is my prediction based on these weights that I've
just set up in terms of like what the optimal path is.
Well, what is that? That is walk tram because this costs 5 and this costs 9.
So with these weights,
these random weights that have just come up with I'm going to pick walk and tram.
And that is my prediction.
Okay? So now what we wanna do is you want to update our
w's based on the fact that our true label is walk,
walk, walk and our prediction is walk, tram.
Okay? And, and the algorithm that kind of
does this, this does like the most like silliest thing possible.
So, so what it does is it's going to first look at the truth value of W. Okay?
So it's going to look at- so, so,
so the weights are starting from- so I decided that
this guy is 3 and I decided that this guy is 2, and I'm gonna update them.
So I'm going to look at every action in this path.
And for every action in this path I'm going to down-weight the, the weight of that.
Well why am I going to do that?
Because I- I don't want to penalize that, right?
This is the true thing.
I want the weight of the true thing to be small.
So I see walk.
I'm like okay so I see walk.
The weight of that was 3.
I'm going to down-weight that by 1.
I'm gonna make that two. I see walk again.
So I'm gonna bring that with 1.
I see walk again, I'm going to subtract one again. I end up at 0.
Okay? Now I'm gonna go over
my prediction and then for every action I see here I'm going to bring it up,
bring the cost, uh, the,
the weight up by 1.
So I see you walk again here,
I'm going to bring it up by 1.
So, so, these were subtract, subtract,
subtract, bring it up by one because it's over my y prime.
And then I see tram.
And then because I see tram,
I'm going to bring this up by 1.
And that ends up in 3.
So my new weights here are going to be three- the, the, the,
the weight of walk just became 1 and then the weight of tram just became 3.
Okay? And, and now I can kind of repeat doing this and see if that gets me this,
this optimal solution or not.
So I'm going to try running my search algorithm.
If I run my search algorithm this path,
this path costs 3,
this path costs 4.
So I'm actually going to get this path and this path.
So my new prediction is just going to be walk, walk, walk.
They're going to be the same thing.
My weights are not gonna change.
I'm going to converge. Yes.
Is it always one?
So I'm talking about a very simplified version of this but yeah it is always one.
So the very simplified version of this is this version where I'm
saying the w's just depend on, on actions.
If you, if you make the weights depend on state and actions,
there is a more generalized form of this.
This is called the stru- er, the structure pe- er, perceptron algorithm,
we'll talk about- briefly talk about the,
the version where there is a state action too,
but for this case we are just depending on action.
You're literally just bring it up by one or by whatever like by
whatever you bring it up here, you gotta bring it down by the same thing.
So, so it's plus and minus a whatever a is. There's a question.
[inaudible] why we do the plus 1 after we do all the minus 1s?
So why am I doing the minus 1s? So I'll get to that.
So, so when I look at y here, right?
Like this is the thing that I really wanted.
So if I- so when I see walk I realize that walking was a good thing,
so I need to bring down the weight of that.
But if, if the weights that I already had like knew that walking is pretty
good then like the weights that I already had knew that walking is pretty good,
I should like cancel that out.
So, so that's why we are doing the plus 1 because like at this stage
like I knew walking is pretty good up here like like my prediction also said walk.
So if, if I'm subtracting it,
I should add it to, to kind of like get them cancel that.
But like right here,
like I didn't know walking is good so I'm going to bring down the weight of
that and then bring up the weight of, uh, tram.
[inaudible].
Yeah. So, so I mistakenly thought tram,
uh, is the way to go.
So to avoid that next time around,
I'm going to make the cost of tram higher so I don't take that route anymore.
And there's a question there.
So here- only like the only reason why [inaudible] in the second- in the,
the y prime is because we know the y prime is different from y.
Yes.
But then, like what if like we have like a long sequence and y prime is only different in
like one small location and like would that change the weights sufficiently?
Yeah. So if, if, er, so you're asking.
Okay, if my y and y prime,
prime are kinda like the same thing walk,
walk, walk or something and then at the very end
this last one they're going to be different.
Yeah. So like we were just and for that last one we are just adding one, right?
So, so it does like weighted, er,
it does actually address that and it just run- you can run it until you
get the sequences to be exactly the same thing so you don't have any mistakes.
Yeah. There's a question back there.
Does it matter if our new cost become negative?
Uh, does it matter if our new costs become- it depends on what,
sort of, search algorithm you are using.
Uh, at the end of the day it's fine if you're using
dynamic programming so I can have like a negative cost here and I'm just calling,
uh, like dynamic programming at the end of the day with that and that is fine.
Yeah, it's fine if the cost becomes negative. There's a question.
In this problem we want to find the true cost for walk and tram,
but we ended up converging to 1.
So this becomes a problem.
Sorry, did not supposed-
Just like the end result for this algorithm we got is 1 for walk and 3 for tram.
And the real result, like in the previous example was 1 and 2.
1 and 2. Right, yes. Yeah. So the,
so the question is, er,
we got here 1 and 3.
Is this actually right?
Like, like if you remember like when we define this tram problem,
we said walking costs 1 and tram costs 2 but we never got that.
Well, the reason we never got that is the solution we are going to
get here is just based on our, our training data.
So if my training data is just walk, walk, walk,
this is like the best thing I can get and I can kind of
like converge to this solution where,
where the two end up being equal.
I don't have any mistakes on this.
If I have more like data points then I'm going to do this
longer and actually try it out on other training data and,
and then I might converge to a different thing.
Is there any rule for as far as initializing the weight?
Is- I, I, I, I,
I'm assuming when- the fu- uh,
further when we are from the actual truth,
the longer it's going to take to, uh, actually converge.
It's- o- okay so the question is how do we initialize?
So in na- in a natural algorithm you're just initializing with 0.
So we're initializing everything by 0.
It's actually not that bad because you just,
you just basically have this sequence and in the-
for the more general case you're computing
a feature value that you just compute
the full thing and you just do one single subtraction.
So it is not that costly actually
to do this.
Yeah.
[inaudible] know the path for a given cost.
If you have that input can we incorporate that into the algorithm?
So, you're saying if we have some prior knowledge about the cost can we incorporate it?
Yeah.
Um, that is interesting.
So, uh, in this current format.
So if you have some prior algorithm
maybe you'll like then your prediction is going to be better, right?
So if you have some knowledge about it maybe you'll get
a better prediction and then based on that you don't update it as much.
So maybe you can incorporate into the search problem.
But again this is the most like general form of this algorithm.
The simple- kind of,
like the simplified version of it also like even like for the action.
So not doing anything fancy.
It's not doing something that hard either, honestly.
Are we worried about overfitting at all?
[BACKGROUND] Yeah.
So it is going to- it can too- you're- yeah,
so I'll show some examples on this.
Like we are going to code this up and then we'll see overfitting, kind of, situations.
So- so I'll get back to that actually. All right.
All right. So, um,
all right, so let's move on. Ah, okay.
So- so this is just like the things that are on
the slides are what I've already talked about.
So, uh, yeah, so here's an example.
So we start with,
3 for walk and 2 for tram.
And then the idea is like how are we going to change
the costs so we get the- the solution that we're hoping for.
Um, and- and as I was saying, well,
we can assume that the costs only depend on the action.
So I'm assuming cost of s, a is just w of a,
and in the most general form it- it can depend on- on the state too.
Um, okay.
So then if you take any candidate output path,
then what would be the cost of the path?
It would just be the sum of these W values over- over all the edges.
So it would just be W of a_1 plus W of a_2 plus W of a_3.
And as you've seen in this example,
the cost of a path is just W of walk, plus W of
walk, plus W of walk, or W of walk plus W of tram.
So- so that's all this slide is saying.
So- so that's how we compute the cost.
All right, so- so now, uh,
let's actually look at this algorithm like running in practice.
Um, okay, let me actually go over the pseudocode.
So- so, you start initializing W has to be equal to 0.
And then after that we're going to iterate for some amount of
T and then we have a training set of examples.
It might not be just one here.
I just showed this one example like- like,
the only training example I had was- was that walk, walk,
walk is a good thing, but you can imagine having
multiple training examples for a search problem.
And then what you can do is you can compute
your prediction so that is y prime given that you
have some W and the-then you can start with this W equal
to zero and then-then just compute your prediction y prime,
and then basically, you can do this plus and minus type of action.
So for each action that is in your true y that is in your true label,
you're going to subtract 1.
So to decrease the cost of true y.
And then for each action that is in your prediction you're going to add- add one to- to,
kind of, increase the cost of the predicted y.
Okay. All right.
So let's look at implementing this one.
And let's try to look at some examples here.
All right. So let's go back to the tram problem.
So this is again the same tram problem.
We just want to use the same, sort of, format.
Uh, I actually went back and wrote up the history here.
If you remember the last time I was saying I'm not returning the history.
Now we have a way of returning history of each one of
these algorithms cause we are going to call dynamic programming and we need the history.
All right. So let's go back to our transportation problem.
So we had a cost of 1 and 2 for walking and tram,
but what we wanna do is we wanna put parameters there.
So you wanna actually put this weight and we can give that to our transportation problem.
So in addition to the number of blocks,
now I'm going to actually give like the weight of different actions.
Okay. All right.
So then walking has a weight and,
um, [NOISE] tram has a weight.
So now I have updated
my transportation problem to generally take different weight values.
So- so, now we wanna be able to generate
some- some training examples. So that's what I wanna do.
I wanna generate different types of training examples
that- that we can call so we can get these true labels.
So let's assume that the true weights for our training example is just 1 and 2.
So- so that is what we really want.
Okay. And- and we're going to just wri- write this prediction function
that we can call up later to- to- to get different values of y.
So the prediction function is going to get the number of blocks.
So- so it's going to get,
um, N, the number of blocks here.
And it is going to act with this path that we want.
So it's going to output these- these y values, this different path.
Okay. So, all right,
so the whole point of prediction is- is basically,
like running this f of x function.
Um, and we can define our transportation problem with- with n, n weights.
And the way we are going to get this is by calling dynamic programming.
So someone asked you earlier could the costs be negative?
Well, yes because now I'm calling dynamic programming and
if like this problem has negative cost, that is fine too.
Um, So and the history is going to get and the action new state and- and costs, right?
So but the thing that I actually wanna return
from my predict function is a sequence of actions.
So I'll just get the action out of this history that I get from dynamic programming.
So I'm calling dynamic programming on my problem that
is going to return a history or get the sequence of actions from that,
and that is my predict function and I can just call that later.
So let's go back to generating examples.
So, um- [NOISE] so,
I'm just going to go for, uh,
try out n to go from 1-10.
So 1 block to 10 blocks and we are calling
the predict function on these true weights to get the true y values.
So these are my true labels, okay?
And those are my examples.
So my examples are just calling generate examples here.
Okay. So let's just print out our examples.
See how it looks like.
We haven't done anything like in terms of like the algorithm or anything.
We're- we're just creating these training examples,
um, by calling this predict function on- on the true weights.
I have a typo here,
[LAUGHTER] generate examples and I need parentheses,
oh, fix the typo.
Okay, so that kinda looks right, right?
So that's my training example 1 through 9.
And then what is- what is the path that you would wanna
do if- if you have these two weights, the 1 and 2.
Okay. So now I have my examples.
So I'm- I'm ready to write this structured Perceptron algorithm.
It gets my examples.
It gets the training examples which are these paths.
Um, and then we're going to iterate for some range.
And then, um, we can, um,
basically go over all the examples that we have in our true- true y values.
And then we can- we can basically go and update
our weights based on- based on that and based on our predictions.
So let's initialize the weights to just be 0.
So that's for walking and tram, they're just 0.
And, uh, prediction actions,
this is when we're calling predict based on the- the current weights.
So if my current weights are 0 then pred actions is just that y prime.
So pred actions is y prime,
true actions is y, like the things that we had on the slides.
If- okay, and- and I wanna count the number of mistakes I'm making too.
So if the two are not equal to each other then I'm
going to just keep a counter for number of mistakes.
If- if the two become equal then- then my number of mistakes is zero.
I'm going to break then maybe I'm happy then.
Okay. So I make a prediction.
And then after that I'm going to update the weight values.
Okay. So how do I update?
Well, basically subtract.
If you're in true actions which is y,
the labels that I've created from my training examples and then,
uh, do plus 1
if you're in prediction actions based on the current weight values.
And- and that's pretty much it.
Like- like that is structured perceptron.
Okay. So let's just print things nicely so we can print
the iteration and number of mistakes we have
and what is actually the weight values that we have.
And I'm just breaking this, um,
whenever I have like no mistakes.
So if number of mistakes is 0,
I'll- I'll just break this. Okay. Okay.
That sounds good.
So if number of mistakes is 0, then I'll break.
[NOISE] Okay.
So all good.
Uh, I'm gonna run this,
it's not gonna do anything because I didn't call it.
So I'll go back and actually call it.
I have another typo here,
I don't know if you guys can guess,
like where is my typo.
This is gonna give an error [LAUGHTER].
Well, I called it weights, not weight.
[LAUGHTER] So, I'll go and fix that. Okay, this should run.
Okay. So and then- then, this is what we get.
So let's actually look at this.
So what we got is the first iteration number of mistakes was 6,
and then, uh, we ended up actually,
at the fir- first iteration,
we ended up converging to 1, 2.
So then the second iteration,
the number of mistakes just became 0,
and then we just got 1, 2,
which is- which is the- the weights that we were hoping for.
Okay? So that kind of,
looks okay to me, that's my training data.
Everything looks fine. There's a question actually.
[inaudible] more like integers. Is that right?
Yeah. So in this case, yeah,
we are summing all the weights as integers, and you're adding them.
Given our update model as well,
Well, we're- we're assuming that the number
of walks and the number of trams were different.
What if tram was in a different location but the number of walks to the tram can be correct?
You would still- So- so I see what you're asking.
No. It should- it- like,
it should figure- figure it that out.
So, um, we- we- we can go over an example after- after
the class and I'll show you like how- how it actually does it. All right.
So- okay.
So let's try 1 and 3.
So with 1 and 3 takes a little bit longer,
and, uh, but it does recover.
So 1 and 4 is actually the interesting one,
because it does recover something.
It does recover 2, 8.
It doesn't recover 1 and 4.
But like given my data, actually, 2,
8 is- is like- like,
there is no reason for me to get 1- 1 and 4.
Like the ratio of them is the thing that that I actually care about.
So even if I get 2 and 8,
like- like that is a reasonable set of weights that one could get.
Um, I'm gonna try a couple of more things.
So let's try 1 and 5.
So I'm gonna try 1 and 5,
and this is what I get.
So I get the weight of walk to be minus 1,
and the weight of tram to be 1.
Now, my mistake is 0.
So why is this happening?
Yeah. Your training data is all walking.
So it's learning to just walk.
Yeah, that's right. So- so what's happening here is,
if you look at my training data up here,
my training data is just has like walk, like all walks.
It hasn't seen tram ever,
so it has no idea like what the cost of tram is with respect to the cost of walk.
So it's not going to learn that.
So we're gonna fix that.
Like one way to fix that is to go and
change the training data and actually like get more data.
So, uh, we can kind of do that.
Um, so like just one thing to remember is,
this is just going to fit your training data,
whatever it is. Um, so yeah.
So when we fix that,
then walk becomes two and tram becomes 9,
which is not 1 and 5.
But it- it is getting there,
like it's a better ratio.
Uh, a number of mistakes is still 0.
So it really depends on what you're looking for.
Like if you're trying to like match your data and your number of mistakes is 0,
and you're happy with this, you can just go with this.
Um, and even though like it hasn't like actually recovered the exact value,
the ratios, that's fine.
Or maybe you're looking for the exact ratios and you should like run it longer.
More iteration questions?
Structured perceptron like suspect to getting stuck in local optima,
like maybe, all we need is different initializations?
Sorry. Like I was looking at the- can you repeat that?
Oh, sorry. Um, does the,
uh, structured perceptron, like,
have a risk of getting stuck in local optimum,
like k-means, so we need different initializations?
Um, that is a good question.
So in, um, actually, lemme think about that.
Um, do you see this in NLP?
Do you actually know if this gets into local optima?
I haven't experienced it personally,
but I feel like there's [inaudible]
There is reasons for it to do this.
It's still in this kind of- I mean, let me think about this.
I'll think about this, because even in the more general form of it, uh,
it's commonly used in like- like the matching,
like sentence- like words and sentences.
So I haven't experienced that either but,
um, I can look into that and back to you. Question?
I was gonna ask, are you just being at all of the optimal paths, currently?
Yes. Yeah, yeah, yeah.
But if we do figure all the optimal paths then technically,
it should be complex, right?
Because like you just match paths.
Um, if you're feeding it all the optimal paths,
uh, it should- you- you're just matching path, you're saying is-
[inaudible]
Yeah. So- so in terms of- okay- so, yeah.
So in terms of like bringing down the number of
mistakes then- then it should always match it.
But if you have some true like weights that you are looking for,
and it's not represented in your dataset,
then it's not necessarily like- like learning that.
So- so in those settings,
you could find the local optima.
So kind of like a- another version of this is, uh,
when you are doing like reward learning and-
and you- you actually have this true reward you wanna find.
Like in those settings, you can totally fall into like
local optima because you want to find what your reward function is.
But you're right, like if you're just matching, uh, the data.
Just in the reward function, you are on the scaling two, you still get like the optimal policies.
So the scaling would be a different problem, right?
So the scaling is kinda- yeah,
so you can have reward shaping,
so you can have different versions of the rewards function,
and if you get any of them, that is fine.
Uh, but, uh, but you might still get
into local optima that's not explained by reward shaping.
So okay. So that we- we can talk about these things offline.
Maybe, I should just move on to the next topics because we have some more stuff going on.
Okay, so I was actually going to skip these slides because we have stuff coming up,
but this is a more general form of it.
So remember I was saying,
this w is a function of a.
Ah, but, um, [NOISE] um,
you could- you could have a more general form, ah,
where your cost function is not just w as a function of a,
it is actually w times the set of features.
Ah, and then the cost of a path is w times the features of a path.
Uh, and that's just the sum of features over the edges.
So- so you can have this more general form.
Go over this slides later on,
maybe, because we've gotta move to the next part.
But just real quick to update here is-
is this more general form of updates which is update
your w based on subtracting the features over
your- your true- true path plus the features over your predicted path.
So- so a more general form of this is called Collins' algorithm.
So Mike Collins was working on this in- in natural language processing.
He was actually interested in it in the setting of part of speech tag- er, tagging.
So- so you might have like a sentence, uh,
and- and you wanna tag each one of the- each one of the labels
here as- as a noun, or a verb, or a determiner, Or a noun again.
So- so he was think- he was basically looking at this problem as a search problem.
Uh, and he was using like similar type of algorithms to- to
try to figure out like- like match what- what the value,
like match noun, or like each one of these,
um, part of speech tags to the sentence.
So he has some scores and then based on the scores and his dataset,
he goes like up and down.
He moves the scores up and down which uses the same idea.
You can use the same idea again in machine translation.
So you can have, like if you have heard of like Beam Search.
Um, and you can have multiple types like- like a bunch of translations of- of
some phrase and then you can up-weight and down-weight them based on your training data.
Okay? All right.
Okay. So now let's move to ai's- ai's- a star,
not ai star. A star search.
All right. So, um, okay.
So we've talked about this idea of learning costs, right?
So we have talked about, uh,
search problems in general doing inference and then doing,
uh, learning on top of them.
And then now, I wanna talk a little bit about, um,
kind of making things faster using
smarter ideas and smarter heuristics. There's a question.
[inaudible] see what is the loss from [inaudible] in this structure?
In this structure? So, so in,
in- this is, this is a prediction problem, right?
So, so in that prediction problem,
we are trying to basically figure out what w-
w's are as closely as possible as we are matching these w,
w- this y prime to y, right?
So, so basically, like,
like the way we are solving this is,
is not necessarily as an optimization,
the way that we have solved other types of learning problems.
The way we are solving it what- is by just like tweaking
these weights to try to match my y as closely as possible to,
to y, okay? All right.
Okay. So let's get- talk, talk about a A-star.
So I don't have internet so I can't show these.
Um, but I think the link for this should work if- when you go to the, to the file.
So the idea is,
if you go back to uniform cost search,
like in uniform cost search,
what we wanted to do was,
we want to get from a point to some solution,
but we would uniformly,
like increase, uh, explore the states around us until we get to some final state.
The idea of A-star is to basically do uniform cost search,
but do it a little bit smarter and move towards the direction of the goal state.
So if I have a goal state,
particularly like in that corner,
maybe I can, I can move in that direction in a smarter way, okay?
So here is like an example of that pictorially.
So I can start from S-start,
and, and if I'm using uniform cost search,
again I'm uniformly kind of exploring all the states possible until I hit my S-end.
And then I'm happy, I'm done,
I've solved my search problem, everything is good.
But the thing is, I've done all these,
like wasted effort on this site which is,
which is not that great, okay?
So uniform cost search in,
in that sense has this problem of just exploring a bunch of states for no good reason,
and what we wanna do is we want to take into
accounts that we're just going from S-start to S-end,
so we don't really like need to do all of that.
We can actually just try to get the- to get to the end state, okay?
So, um, so going back to maybe,
um, I'm going to go on this side.
So, um, [NOISE] going back to how these search problems work,
the idea is to start from S-start and then get to some state S,
and then we have this S-end, okay?
And what uniform cost search does is,
it basically orders the states based on past cost of s, okay?
And then explore everything around it based on past cost of
F- S until it reaches S-end, okay?
But when you are in state S,
like there is also this thing called future cost of s, right?
And ideally, when I'm in state S,
I don't wanna explore other things like this side.
I actually want to- wanna move in the direction of kind of reducing my,
my future cost and getting to my,
to my end state, okay?
So, so the cost of me getting from S-start to S-end is really just like past cost of
s plus future cost of s. And if I knew what future cost of s was,
I would just move in that direction.
But if I knew what future cost of s is,
well the problem was solved, right?
Like I had the answer to my search problem.
Like I'm, I'm solving a problem still.
So in reality, I don't have access to future cost, right?
I have no idea what future cost is.
But I do have access to some- like I can potentially have access to something else and
I'm gonna call that h of s. And that is an estimate of future cost.
So I'm going to add a function called h_s,
and this is called a heuristic,
and the- and this heuristic could estimate what future cost is.
And if I have access to this heuristic,
maybe I can update my cost to be something as what the past cost is.
In addition to that,
like I can add this heuristic and that helps me to be a
little bit smarter when I'm running my algorithm, okay?
So, so the idea is,
ideally like what I would wanna do is,
I wanna explore in the order of past cost plus future cost.
I don't have future cost or if I had future cost,
I had the answer to my search problem.
Instead, what A-star does is it's- it explores
in the order of past cost plus some h_s, okay?
So remember uniform cost search, it,
it explores just in the order of past cost.
So in uniform cost search,
um, like we don't have that h_s, okay?
And h_s is, is a heuristic,
it's an estimate of the future cost.
All right. So what does A-star do?
Actually that's something really simple.
So, so a A-star basically just does uniform cost search.
So all it does is uniform cost search with a new cost.
So before I had this blue costs costs of s and a,
this was my cost before.
Now I'm going to update my cost to be this cost prime of s and a,
which is just cost plus the heuristic,
over the successor of s and a minus the heuristic.
So, so that is the new cost and I can just run uniform cost search on this new cost.
So, so I'm gonna call it cost prime of s and a.
Well, what does that equal to?
That is equal to cost of s and a,
which is what we had before when we were doing uniform cost search,
plus heuristic over successor of s and a,
minus heuristic over s. So why do I want this?
Well, what this is saying is,
if I'm at some state S, okay, and there is some other state,
successor of s and a,
so I can take an action a and end up in successor of s and a,
and there is some S-end here that I'm really trying to get to.
Remember h was my estimate of future cost.
What this is saying is,
my estimate of future cost for getting from successor to S-end,
minus my estimate of, er,
getting from, er, future costs of S to
S-end should be the thing I'm adding to my cost function.
I should penalize that. And, and what this is really enforcing is,
it basically makes me move in the direction of S-end.
Because, because if I end up in some other state that is
not in the direction of S-end, then,
then that thing that I'm adding here is basically going to penalize that, right?
It's going to be saying, "Well,
it's really bad that you've- you are going in that action.
I'm going to put more costs on that so you never going that direction.
You should go in the direction that goes goes towards your S-end."
And that all depends on like what your H function is and how good,
like of an H function you have and how you're designing your, your heuristics.
But that's kind of the idea behind it.
So here is an example actually.
So let's say that we have this example where we have A,
B, C, D, and E and we have cost of 1 on all of these edges.
And what we wanna do is we wanna go from C to E. That's our plan, okay?
So if I'm running uniform cost search, well what would I do?
I'm at C, I'm going to explore B and D
because they have a cost of 1, and then after that,
I'm going to explore A and E. And then finally, I get to,
get to E. But why did I spend all of that time looking at A and D?
I shouldn't have done that, right?
Like A and B are not in the direction of getting to S-end.
So instead, what I can do is if someone comes in and tells me,
well, I have this heuristic function,
you can evaluate it on your state and
this heuristic function is going to give you 4, 3, 2, 1,
and 0 for each one of these states,
then you can update your cost and maybe you'll have a better way of getting to S-end.
So this heuristic, in this case,
is actually perfect because it's actually equal to future cost.
Like the point of the heuristic is to get as close as possible to the future cost.
This is exactly equal to future cost.
So with this heuristic,
what's going to happen is my new cost is going to change. How is it going to change?
Well, it's going to become the cost of whatever the cost of the edge was before,
which was 1, plus h of- in the case of,
for example, the cost of going from C to B.
If you look at C to B, it's the old cost, which was 1,
plus heuristic at B, which is 3,
minus heuristic at C, which is 2.
So that ends up giving me 1 plus 3 minus 2,
that is equal to 2.
And then similarly, you can compute like all these,
like new cost values,
the purple values and,
and that has a cost of two for going in
this direction and cost of zero for going towards E. And,
and if I just run uniform cost search again here,
then I can get to E like much easier, okay? Yes.
Does an A-star like kinda result in greedy approaches,
where you put these opportunities,
like go back with [inaudible].
Does A-star result in-
Like greedy approaches.
Like where you sort of- greedy.
Greedy?
Yes.
Um, yeah. So okay.
So, so in all, ah, so,
so the question is, is A-star like causing greedy approaches?
So, no. Actually, we are going to talk about that a little bit.
A-star, depend- depends on the heuristic you are choosing.
So depending on the heuristic you are choosing,
A-star is actually going to be like returned to optimal value.
But yeah, it does depend on the heuristic.
So it actually does the exact same thing as
uniform cost search if you choose a good heuristic.
Why is cost of CB 1 here?
Uh, what-
Why is cost of CB 1?
Why is cost of C- CE 1?
CB.
CB. Hold on. [LAUGHTER]. I'm like, really bad, my ears are really bad, so speak up. So cost of CB.
Oh because- oh, I see what you're saying.
That's what we started with.
So this is like the graph that I started with.
So I started with the cost,
like the blue costs being all 1,
but now I'm saying those costs are not good,
I'm going to update them based on this heuristic so I can get closer to the goal,
like as fast as possible.
[inaudible].
You return like the actual cost of not, like you wouldn't count the heuristic in there, because it can be like wrong.
That's, that's right.
So, so the question is what costs are you going to return at the end?
And you do want to return the actual cost.
So you're returning the actual cost,
but you can run your algorithm with this heuristic thing added
in because that allows you to explore less things and just be more efficient.
Okay. Oh, I gotta move on.
All right. So, um.
Okay. So a good question to ask is well, what is this heuristic?
How does this heuristic look like?
Like can any- does any heuristic like work well?
So turns out that not every heuristic works.
So here's an example.
So again, the blue things are the costs that are already given.
These are the things that I already have,
and I can just run my search algorithm with it.
The red things are the values of the heuristic,
someone gave them to me for now.
In general we would want to design them.
So someone comes in and gives me these, these heuristic values,
and, uh, then what I wanna do is I wanna compute the new cost values.
So the question is, is this heuristic good?
So I get my new cost values.
They look like this. Like does this work?
We don't have time so I am going to answer that. It's not gonna work.
[LAUGHTER] So the reason this is not gonna work is,
uh, well we just got a negative edge there, right?
So I'm running uniform cost search at the end of the day,
like A_star is just uniform cost search.
Um, and I can't have negative edges.
So, uh, I'm not- like that was just not a good heuristic to have here.
So, so the heuristics need to have specific properties and,
and you, you should think about what those properties are.
So one property that you would want to have
the heuristics to have is this idea of consistency,
this is actually the most important property really.
So, um, so when we talked about heuristics,
I'm gonna talk about properties of them here.
Heuristics h. They should be consistent.
So a consistent heuristic has two conditions:
The first condition is it's going to satisfy the triangle inequality.
And, and what that means is like the cost that- your,
your updated cost that you have should be, should be non-negative.
So, so this cost prime of s,
s and a, this should be positive.
So, so that means that the old constant s and a plus h of,
um, successor I'm gonna use s prime for that minus h of s is greater than or equal to 0.
Okay. So that is the first condition.
And then the second condition that you are going to put is that, uh,
future costs of s_end is going to be equal to 0, right?
Because the future cost of the end state should be 0.
So then the heuristic at the end state is also equal to 0.
So, so these are kind of the properties that we would want to
have if you want to talk about consistent heuristics.
Okay. And they're kinda like natural things that we would want to have, right?
Like, like the first one is basically saying, well,
the cost you are going to end up at should be,
should be greater than or equal to 0 and you can run uniform cost search on it.
But it's really like talking about this triangle inequality that you want to have, right?
Like, er, h of s is kind of an estimate of this future cost.
So if I'm going to- from s take an action
that cost of s and a that added up h of successor of s,
s and a should be greater than just h of s,
the estimate of future costs.
So that's, so, so that's, that's all it is saying.
And then the last one also makes sense, right?
I do want my future cost of s_end to be zero, right?
So then the heuristic at s_end should also be equal to 0,
because again heuristic is just an estimate of the future cost.
Okay. All right.
So, so what do I know about A_star beyond that?
So one thing that we know is that,
um, if, if h is consistent.
So if I have this consistency property,
then I know that A_star is correct.
So that there is a theorem that says,
A_star is going to be correct if h is consistent.
And well, we can kind of look at that through an example.
So, so let's say that I am at s_0 and I take a_1 and
I end up at s_1 and I take a_2 and end up at s_3 and,
uh, a 0 at s_2,
take a_3 and I end up at s_3.
So let's say that I have,
I have kind of like a path that, that looks like this.
Okay. So then, uh,
if I'm looking at the cost of each,
each one of these, right?
I'm looking at cost of- cost prime of s_0 and a_1.
Well, what is that equal to?
That's- that's my updated cost.
Updated cost is old cost,
which is cost of s_0 and a,
plus heuristic value at s_1 minus heuristic value at s_0.
Heuristic value s_1 minus heuristic value at s_0.
Okay. So, so that is the cost of going from s_0 and taking a_1.
I'm gonna to just write all the costs for,
for the rest of this to figure out what's the cost of the path.
The cost of the path is just the sum of these costs.
So s_1, a_2 is cost of s_1,
a_2 plus heuristic at, um, what is it?
S_2 minus heuristic at s_1,
so that is the new cost of this edge.
And the new cost of the last edge which is cost prime of s_2, a_ 3,
and that is equal to the old cost of s_2,
a_3 plus heuristic at s_3 minus heuristic at s_2.
Okay. So I just wrote up all these costs.
If I'm talking about the cost of a path,
then it's just that these costs added up, right?
So if I add up these costs, what happens?
Bunch of things get canceled out. All right.
This guy gets canceled out by this guy,
this guy gets canceled out by this guy, right?
And what I end up with is,
is sum of these new costs,
these cost primes of,
um, s_i minus 1,
a_i is just equal to sum of my old cost of s_i minus 1,
a_i plus my heuristic,
I guess last state whose end state minus heuristic at s_0.
Okay. I'm saying my heuristic is a consistent heuristic.
So what is a property of a consistent heuristic?
The heuristic value at s end should be equal to 0.
So this guy is also equal to 0.
So what I end up with is is if I look at a path with the new cost,
the sum of the new cost is just equal to the sum of the old cost minus some,
some constant, and this constant is just the heuristic value at s_0.
Okay. So, so why is this important because when we talk about the correctness,
like remember we just proved at the beginning of
this lecture that uniform cost search is correct,
so the cost that it is returning is optimal.
That is, that is this cost.
A_star is just uniform cost search with a new cost.
So A_star is just running on this new cost.
But this new cost is the same thing that they have as old cost minus a constant.
So if I'm optimizing the new cost,
it's the same thing as optimizing the old cost.
So it is going to return the optimal solution.
Okay. All right.
So that is basically the same things on the slide like,
like I basically did that.
So, so that's one property, right?
So, so we talked about heuristics being consistent.
We have now just talked about A_star being correct,
because it's uniform cost search.
It's, it's correct only if the heuristic is consistent, right?
Like only if we add that property.
Because, because that consistency gets us,
gets us the fact that this guy is equal to 0 and gets us the fact
that these guys are going to be positive and I can run uniform cost search on them.
Um, the next property that we have, uh,
here for A_star is A_star is actually more efficient than uniform cost search,
and we kind of have already seen this, right?
Like, like the whole point of a A_star is to not
explore everything and explore in a directed manner.
So, um, if you remember uniform cost search like,
how does it explore?
Well, it explores all the states that have
a past cost that are less than the past cost of s_end.
So again, remember, uniform cost search,
you're exploring with the,
with the order of past cost of states,
and then we explore all those states that have past costs less than the end state.
Okay. A_star like- the thing that A_star does is it explores less states.
So it explores states that have a past cost less
than past cost of the end state minus the heuristic.
So, so if you kinda look at the right side,
the right side just became- becomes smaller, right?
Like, like the right side for uniform cost search was just past cost of s_end.
Now it is past cost of s_end minus the heuristic,
so it just became smaller.
And then why did it become smaller?
Because now I'm doing this more directed search.
I'm not searching everything uniformly around me.
And then that's the whole point of the heuristic.
Okay. And that makes it actually more efficient.
So- and then kind of the interpretation of this is if h is larger then,
then that's better, right?
Like if my heuristic is as large as possible,
well that is better because then I am kind of exploring a smaller like area to,
to get to the solution.
Uh, the proof of- this is like two lines so I'm gonna skip that.
So let me actually show,
uh, how this looks like.
So if I'm trying to get from s_star to s_end, again,
if I'm doing uniform cost search, I'm uniformly exploring.
So like all states around me,
and that is equivalent to assuming that the heuristic is equal to 0,
like it's basically uniform cost search is A-star when the heuristic is equal to 0.
So what is the point of the heuristic?
The point of the heuristic is to estimate what the future cost is.
If I know what the future cost is,
then, then h of s is just equal to future cost.
Uh, and then, that would be awesome
and I only need to like explore that green kind of space.
And then the thing I'm exploring is,
is just the nodes that are on the minimum past cost and co- uh,
cost path, and I'm not exploring anything extra, right?
Like that's the most,
like efficient thing one can do.
In practice, like I don't have access to future costs, right?
In, in practice if I had access to future costs,
like the problem was solved.
I have access to some heuristic that is some estimate of the future cost.
It's not as bad as uniform cost search,
it's getting close to future costs, like,
like the value of future costs,
and you're kind of somewhere in between.
So it is going to be more efficient than uniform cost search in some sense.
Okay. All right.
So, so basically the whole idea of A_star is it kind of distorts edge,
edge costs and favors these end states.
So I'm going to add here that A_star is efficient too.
So that is the other thing that,
that we have about A_star.
Okay. All right.
So, so these are all cool properties,
um, one more property about heuristics and then after that,
we can talk about relaxation.
So um, so there's also this other property called admissibility,
which is something that we have kind of been talking about already, right?
Like we've been talking about how this heuristic should get
close to FutureCost and should be an estimate of the FutureCost.
So an admissible heuristic is a heuristic
where H of S is less than or equal to FutureCost.
And then the cool thing is,
if you already have consistency,
then you have admissibility too.
So if you already have this property,
then you have admissibility too.
So another property is admissible.
Which means H of S is less than or equal to FutureCost of s, okay? All right.
So the proofs of these are again like just one liners,
so this one is
more than one line but- [LAUGHTER] but it's actually quite easy, it's in the notes.
So you can use induction here to prove, uh,
to prove that if you have consistency,
then you're going to have admissibility too.
Okay, so, so we've just talked about how A-star is a sufficient thing.
We've talked about how we can come up
with- we haven't talked about how to come up with heuristics,
but we have talked about consistent heuristics
that are going to be useful and they are going to give us
admissibility and they're going to give us correctness
and how like A-star is going to be this very efficient thing.
But we actually have not talked about how to come up with heuristics.
So let's spend the next, yeah,
couple minutes talking about,
uh, talking about how to come up with heuristics.
And in the main idea here,
is just to relax the problem. Just relaxation.
So, so what are- so,
so the way we come up with heuristics is,
we pick the problem and just make it easier and solve that easier problem.
So, so that is kind of the whole idea of it.
So remember the H of S is- is supposed to be close to FutureCost,
um, and, and some of these problems can be really difficult, right?
So the- so if you have a lot of constraints and it becomes harder to solve the problem,
so if you relax it and we just remove the constraints,
we are solving a much easier problem and that could be used as a heuristic,
as a value of heuristic that estimates what the FutureCost is.
so, um, so we want to remove constraints
and when we remove constraints,
the cool thing that happens is,
sometimes we have closed form solutions,
sometimes we just have easier search problems that we can solve and
sometimes we have like independence of problems and we can find the solutions to them,
and that gives us a good heuristic.
So, so that is my goal, right?
Like I would want to find these heuristics.
So let me just go through a couple of examples for that.
So, so let's say I have a search problem
and I want to get the triangle to get to the circle,
and that is what I wanna do and I have
all these like walls there and that just seems really difficult.
So what is a good heuristic here?
I'm going to just relax the problem.
I'm gonna remove like all those walls,
just knock down the walls and have that problem.
That- that just seems much easier, okay?
So- so well, like now,
I actually have a closed form solution for getting the triangle,
get to the- get to the circle.
I can just compute the Manhattan distance and I can use that as a heuristic.
Again, it's not going to be the- like actually like what FutureCost is,
but it is an approximation for it.
So- so usually, you can think of the heuristics as,
as these optimistic views of what the FutureCost is,
like, like it's an optimistic view of the problem.
Like what if there was like no walls.
Like if- if there are no walls here,
then how would I get from one location to another location?
The solution to that is going to give you this FutureCost-
this estimate of FutureCost value which is- which is H of S. Okay?
Or the tram problem, let's say we have the tram problem but we have
a more difficult version of it where we have a constraint.
And this constraint says,
"You can't have more tram actions than walk actions."
So now this is my search problem,
I need to solve this.
This seems kind of difficult.
Like we talked about how to come up with states word
last time and even that seemed difficult,
like I need to have the location,
I need to have the difference between the walk and tram.
That seems kind of difficult,
like- like I have an order of N squared states now.
So instead of doing that,
well, let me just remove the constraint.
I'm- I'm just gonna remove the constraint, relax it.
And after relaxing it,
then I have a much easier search problem I need to deal with.
I only have this location,
and then I can just go with that location and,
and everything will be great.
Okay? All right.
So, so the idea here was like where,
where, where this middle part is,
if I- if I remove these constraints,
I'm going to have these easier search problems, these relaxations.
And I can compute the FutureCost of these relaxations
using my favorite techniques like dynamic programming or uniform cost search.
But- but one thing to notice is,
I need to compute that for 1 through
N. Because is heuristic is a function of state, right?
So I actually need to compute FutureCost for this relaxed
problem for all states from 1 through N. Uh,
and that allows me to have like a better estimate of this.
There are some, uh,
like engineering things that you might need to do here.
So, so for example, um,
you might- so, so here we are looking for FutureCost,
so if you plan to use uniform cost search for whatever reason,
like maybe Dynamic Programming doesn't work in this setting,
you need to use uniform cost search,
you need to make a few engineering things to make it work.
Because if you remember,
uniform cost search would only work on past costs,
doesn't work on FutureCost.
So you need to like, create
a reverse problem where- where you can actually compute FutureCost.
So, so a few engineering things but beyond that,
it is basically just running our search algorithms that we know,
uh, on, on, uh, these relaxed problems.
And that will give us a heuristic value,
and we'll put that in our problem and we will go and solve it.
Okay? Um, and another cool thing that heuristics give us,
is, is this idea of having independent subproblems.
So, uh, so here's another example.
I want to solve this- this eight puzzle and I move
blocks here and there and come up with this new configuration,
um, that seems hard again.
A relaxation of that is just assume that the tiles can overlap.
So the original problem says,
the tiles cannot overlap.
I'm just gonna relax it and say, "Well,
you can just go wherever and you can overlap."
Okay? So that is again much simpler and now I
have eight independent problems for getting each one of
these points from one location to another location and I have
a closed form solution for that because that's again just Manhattan distance.
So that gives me a heuristic,
that- that's an estimate.
That's not perfect, it's an estimate.
And then I can use that estimate in
my original search problem to solve the search problem.
So here were- it was just some examples of this idea of
removing cons- removing constraints and coming up with better heuristics.
So like knocking down walls,
like walk and tram freely, overlapping pieces, er,
pieces and that allows you to kind of solve this new problem, uh, and,
and the idea is you're reducing
these edge costs from infinity to some finite- finite cost.
Okay? All right.
So, um, yeah, so, so I'm gonna wrap up here, uh,
and I guess we can always talk about these last few slides next time, uh,
since we're running late, uh,
but I think you- you guys have got like the main idea.
So let's talk next time.
 Okay. Let's start guys.
Okay. So our plan for today is to catch up.
So we're a little behind.
So, uh, it's okay.
So today, I want to talk about MDPs,
Markov decision processes, and my plan is to talk about that for the first hour.
And then after that, I want to talk, uh,
for 10 minutes about the previous lecture.
So remember, like we went over relaxations kind of quick,
so maybe we can go over that again.
And then, the last 10 minutes I want to talk about the project and,
kind of the plan for the project,
how we should think about it,
this is coming up, so we should start talking about that.
So this is an optimistic plan, so,
[LAUGHTER] uh, let's see how it goes with, this is the current plan.
Okay. All right.
So. Okay, let's get into it.
So Markov decision processes.
So let's start with a question.
Um, let's actually do this just by hand,
so you don't need to go to the website.
So the question is,
it's Friday night and you wanna go to Mountain View and you have a bunch of options but,
what, what you wanna do is you want to get to Mountain View
with the least amount of time, okay?
Which one of these modes of transportation would you use?
Like, how many of you would bike?
No one would bike. A couple of you would bike.
How many of you would drive?
This is, this is popular in Mountain View, would be good.
Caltrainers? Some people would take Caltrain,
sounds good. Uber and Lyft?
We have like a good like distribution.
Fly? [LAUGHTER] Yes, yeah, a good number of you want to fly,
uh, as flying cars are becoming a thing,
like this could be an option in the future.
There are a lot of actually startups working on flying cars.
Um, but, but as you think about this problem like the way you think about it is,
is there a bunch of uncertainties in the world,
like it's not necessarily a search problem, right.
You could, you could bike and you can get
a flat tire and you don't really know that right,
you have to kind of take that into account.
If you're driving, there could be traffic.
Uh, if you are taking the Caltrain,
there are all sorts of delays with the Caltrain,
uh, and all sorts of other uncertainties that exist in the world and,
and you need to think about those.
So it's not just a pure search problem
where you pick your route and then you just go with it,
right, there are, there are things that can happen,
uh, that can affect your decision.
So, and that kind of takes us to Markov decision processes.
We talked about search problems,
where everything was deterministic,
and now you're talking about this next class of state-based functions,
which are Markov decision processes.
And the idea of it is, you take actions but you might not actually end
up where you expected to because there is this nature around
you and there's this world around you that's going to be
uncertain and do stuff that you didn't expect, okay.
So, so, so far we've talked about search problems.
The idea of it is you start with a state and then you take
an action and you deterministically end up in a new state.
If you remember the successor function,
successor of S and A would always give us S prime,
and we would deterministically end up in S prime.
So if you have like that graph up there,
if you start in S and you decide to take this action one,
you're going to end up in A,
like, there's no other option.
But that's how you're gonna end up in it, okay.
Uh, and the solution to these search problems are these paths.
So we have the sequence of actions because I know if I,
if I take action one, and action three, and action two,
I know like what is the path that I'm going to end up at and that would be ideal, okay.
So when we think about Markov decision processes,
that is the setting where we have
uncertainty in the world and we need to take that into account.
So, so the idea of it is, you start in a state,
you decide to take an action but then you can randomly end up in different states.
You can randomly end up in S_1 prime or S_2 prime.
And again, because there's just so many other things
that are happening in the world and you need to,
you need to worry about that randomness and make decisions based on that, okay.
And, and this actually comes up pretty much like every run- every application.
So, uh, this comes up in robotics.
So for example, if you have a robot that wants to go and pick up an object,
you decide on your strategy, everything is great,
but like when it comes to actually moving the robot and getting
the robot to do the task like the actuators can fail,
or you might have all sorts of obstacles around you that you didn't think about.
So there is uncertainty about the environment or
uncertainty about your model like your actuators that,
that you didn't necessarily think about and in reality,
they are affecting your decisions and where you're ending up at.
This comes up in other settings like resource allocation.
So in resource allocation,
maybe you're deciding what to produce,
what is the product you would want to produce and,
and that kind of depends on what is the customer demand and,
and you might not have a good model of that and,
and that's uncertain, right?
It really depends on what,
what products customers want and what they don't.
And you might have a model but it's not gonna be like accurate and, and you need,
you need to do resource allocation under
those assumptions of uncertainty about the world.
Um, similar thing is in agriculture.
So for example, you want to decide, uh,
what sort of, uh, what, what to plant but,
but again, you might not be sure about the weather,
if it's gonna rain or if the, if the,
the crops are going to yield or not.
So there's a lot of uncertainty in these decisions that we make and,
and they make these problems to,
to go beyond search problems and become problems where,
where we have uncertainty and we need to make decisions under uncertainty.
Okay? All right.
So let's take another example.
So this is a volcano crossing example.
So, so we have an island and we're on one side of the island and what we wanna do,
so we are in that black square over there.
And what we wanna do is,
you want to go from this black square to this side of the island and here we have
the scenic view and that's gonna give us a lot of reward and happiness.
So, so my goal is to go from one side of the island to the other side of the island.
But the caveat here is that there's
this volcano in the middle of the island that I need to actually pass, okay.
So, and, and if I fall into the volcano,
I'm going to get a minus 50 reward,
more like minus infinity.
But, but for this example like imagine you are getting a minus 50 reward if,
if you fall into the volcano, okay. So. All right.
So, so, if I have this link here in this side,
so if my slip probability is 0 which is- I'm sure I'm not gonna fall into the volcano,
should I cross the island?
No or yes?
Well, I should cross the island uh,
because I'm not gonna fall, right,
like I'm, I'm not gonna fall into that minus 50.
Uh, slip probability is 0,
I'll get to my 20 reward, everything
will be great, okay.
But the thing is like we've been talking about how the world is,
is stochastic and slip probability is not gonna be 0.
Maybe, maybe it's 10%.
So if there's 10% chance of falling to, into the volcano,
how many of you would,
would still cross the island?
Good number, yeah.
So, um, the optimal solution is actually shown by these arrows here.
And yes, the optimal solution is still to cross the island.
Like your value here,
we're going to talk about all these terms,
but the value here is basically the value you're gonna get, uh,
at the beginning like state which is the,
kind of- we'll, we'll talk about it,
it's the expected utility that you're gonna get.
It's gonna go down because there is
some probability that you're going to fall into a volcano,
but still like the best thing to do is to cross the island. How about 20%?
How many of you would do it with 20%?
Some number of people, [LAUGHTER] it's less.
Um, still turns out that the optimal strategy is to cross.
30% percent?
One person. [LAUGHTER]
So with 30%,
that's actually the point that you kind of you'd rather not,
not cross because there's this volcano and then with a large probability you could,
you could fall into the volcano and the value is going to go down.
Okay. So these are the types of problems we're gonna,
we're gonna work with. Yes.
The value like with respect to two because two is like what you can do with them.
So two is like the value- the reward that you are going to get at,
at that state, and then value you compute that you propagated back.
We'll talk about that in details on,
on how to compute the value, [NOISE] okay? [NOISE]
All right. Okay. So that was just an example.
So, so that was an example of a Markov Decision Process.
What we wanna do in this lecture,
is we are going to, like, again, model these, er,
types of systems as Markov decision processes,
then you are going to talk about inference type algorithms.
So how do we do inference?
How do we come up with this best strategy path?
Um, and in the middle,
I'm going to talk about policy evaluation,
which is not an inference algorithm but it's kind of a step towards it.
And it's basically this idea,
if someone tells me this is a policy,
can I evaluate how good it is?
And then we'll talk about value iteration which tries to figure out
what is the best policy that I can take, okay?
So that's the plan for today.
Then next lecture we're going to talk about
reinforcement learning where we don't actually know what the reward is,
and we don't know what the- where the transitions are.
Uh, so, so that's kind of the learning part of- part of these, er, MDP lectures.
So Rita is going to actually do the- do the lecture next,
next- on, on Wednesday, right?
Okay. So let's get into- let's get into Markov decision processes.
So we have a bunch of examples throughout this lecture,
so this is kind of another example.
So all right so actually I do need volunteers for this.
So in this example, uh, we have a bunch of rounds,
and the idea is you can at any point in time,
you can choose two actions.
You can either stay or you can quit, okay?
If you decide to quit,
[NOISE] I'm going to give you $10, I'm, uh,
actually I'm not going to give you $10,
but imagine I'm gonna give you $10 [NOISE],
and then we'll end the game, okay?
And then if you decide to stay,
then you're gonna get $4 and then I'll roll the dice.
If I get one or two, we'll end the game [NOISE].
Otherwise, you're going to continue to the next round,
and you can decide again, okay?
So who wants to play with this?
Okay. All right. Volunteer. Do you want to stay or quit?
Quit. [LAUGHTER]
[LAUGHTER] so that was easy.
You got your $10. [LAUGHTER]
Does anyone else want to play?
Stay, stay again.
Oh, you've got 8, $8.
Sorry. [LAUGHTER]. The dice is still.
Um, so you kind of get the idea here, right?
So, so you have these actions and then with one of them,
like if you decide to quit,
you deterministically you will get your $10 and you're done.
Uh, with the other one, it's,
it's probabilistic and you kind of wanna see which one is better and what,
what would be the best policy to take in this setting.
So we'll come back to this question.
We will formalize this, and, and we'll go over this.
I have a question. Is like, I think I see a similar example.
Is it better to always, like, just continue once and then quit?
Like, isn't it better to switch or?
So when, when not.
Okay so, so then you need to actually compute what is the-
Yeah.
-expected utility, right?
So- and that's what we wanna do, right?
So, so [NOISE] you might say, "Oh, I wanna,
I wanna stay and then I get my $4,
and then I want to quit and then I get 14,
and maybe that is the way to go.
Um, that could be a strategy, but for doing that, right?
Like we are going to actually talk about that.
For doing that, we are going to define what would be the optimal policy.
One other thing that, uh, for this particular problem,
you're going to keep in mind is,
I'll, I'll talk about it when, when I define a policy.
But, but the policy the way we,
we define it is it's a function of state.
So if you decide to stay, that is your policy.
If you decide to not stay, that is your policy.
Like, you're not allowing switching right now.
Like, as I talk about this later in the lecture.
But, but I'll come back to this problem, okay?
So if you- if you decide that your policy,
the thing you want to do is to just stay.
Uh, keep staying, this is the probability of,
like, the total rewards that you are gonna get.
So you're gonna get four with some probability.
And then if you're lucky, you're gonna get 8.
And then even if you're luckier, you're gonna get 12,
and if you're luckier, you're gonna get 16.
But, but the probabilities are going to come down pretty much like really quickly.
So the thing we care about in this setting,
is, is the expected utility, right?
In expectation, like if I- if I- if I run this,
and if I average all of these possible paths that I can do,
what would be the value that I get?
And for this particular problem,
it turns out that in expectation if you decide to stay, you should get 12.
So, so you got really unlucky that you got 8.
But [LAUGHTER], but in general,
in expectation, you should decide to stay, okay?
And, and we actually want to spend a little bit of time in
this lecture thinking about how we get that 12,
and and how to go about computing this expected utility.
And, and based on that, how to decide what policy to use, right?
Okay. And then if you decide to,
to quit, then, then expected utility there is kind of obvious, right?
Because that, that, you're quitting and
that's with probability of 1 you're getting $10,
so you're just gonna get $10 and that is the expected utility of quitting. Yes.
[inaudible]. [NOISE]
Uh, [NOISE] so, so when you- when I say- when you roll a die,
I said if you get one or two-
You stay.
You, you, you, stay, yeah.
And then if you get the other,
so the two-thirds of it, you continue.
So, so it's a one-third,
two-third comes from there, okay?
All right. I'll, I'll come back [NOISE] to this example.
This is actually the, the running example throughout
this lecture [NOISE], okay? So [NOISE].
[inaudible] so how are
you able to do this calculation? We're going to talk about that next.
That is what the lecture is about.
Okay. So let's, let's actually,
uh- I do wanna finish it in an hour,
that's why maybe I'm rushing things a little bit.
But we are going to talk about this problem like throughout the class.
So, so don't worry about it.
If it's not clear at the end of it,
we can clarify things, okay?
All right. So I do want to formalize this problem.
The way I want to [NOISE] formalize this problem is, er, using an MDP.
So I wanna- I wanna formalize this as a ma- as a Markov decision process.
Maybe I can [NOISE] just use this [NOISE].
So in Markov decision processes,
similar to search problems,
you're going to have states.
So in this particular game,
I'm going to have two states.
I'm either in the game [NOISE] or I'm out of the game.
So I'm in an end state where everything [NOISE] we
ended you're out of the game, you're done, okay?
So, so those are my states.
Then, um, when I'm in these states,
I'm in each of these states, I can take an action.
And if I'm in an end state,
I can take two actions, right?
I can either decide to stay [NOISE], right?
Or I can quit [NOISE], okay?
And if I, if I decide to stay,
from in state, that takes me to something that I'm [NOISE] going to call a chance node.
So a chance node is a node that represents a state and action.
So it's not really like, like the blue things are my states,
but I'm creating this chance nodes as a way of kind of going through this example,
to, to see where things are going.
So, so the- these blue states [NOISE] are going to be my states.
I'm in S. These chance nodes are over state and action.
So basically, this node tells me that I started [NOISE] with in,
and I decided to stay, okay?
And the chance node here,
basically tells me that I started with in,
and I decided to quit [NOISE], okay? Yes.
Why do we still call it a chance node even though it's deterministically?
So I deterministically go through it,
but then from the chance node that's where I'm introducing the probabilities.
So from the chance node I can like
probablistically end up in the- these different states.
In the case of quit, it's also deterministic.
In the case of the quit in this case it's deterministic.
Yeah. So in the case of the quit,
we say [NOISE] with probability 1 [NOISE],
I'm going to end up in this end state.
So I am going to draw that with the no- with the- with the edge
that comes from my chance node, and I'm gonna say,
with probability of 1 [NOISE],
I'm going to get $10 [NOISE] and just be done, okay?
But if you are in this state,
this is actually the state where interesting things can
happen with probability two-thirds,
I'm going to go back to [NOISE] in,
and get $4,
or with probability one-third,
I'm going to end up in end,
and, and do I get still 4,
$4 [NOISE] , okay?
So, so that is my Markov decision process.
So, so I had maybe we can keep track
of a list of things we are defining in this lecture.
So we just defined states [NOISE], and then we said well,
we're gonna have these chance nodes [NOISE]
because from these chance nodes probabliistically,
we're going to come out of them depending on what happens in nature, right?
Like I end up- this is the decision I've made,
now nature kind of decides which one you're going to end up at,
and, and based on that we,
we move forward, okay?
All right. So, so more formally,
we had a bunch of things when we define an MDP.
Similar to search problems, we- like we,
we now need to define the same set of things.
So, so we have a set of states.
In this case my states are in and end, okay?
We have a start state.
I'm starting with in.
So that's my start state.
I have actions as a function of states.
So when I ask what are the actions of the state,
my actions are going to be stay or quit.
What are actions of end?
I don't have anything, great,
end state doesn't have any actions that come out of it.
And then we have these transition probabilities.
So transition probabilities more formally,
take a state, an action, and, and a new state.
So S, A, S prime,
and tell me what is the transition probability of that,
it's one-third in this case.
And then I have a reward which tells me how much was that rewarding,
that was $4, okay?
So, so I'm defining- so when I'm defining my MDP,
kind of the new things I'm defining is this transition probability,
which tells me if you're in state S,
and take action A, and you end up in S prime.
What is the probability of that?
I'm in in, I decide to stay,
and then end up in end.
What's the probability of that? That's one-third.
Maybe I'm in in, I decide to quit,
I end up in end.
What's the probability of that?
It's equal to 1, okay?
And then over the same state action state primes,
like next states we are going to end up at,
we're going to define a reward [NOISE] which tells me how much money did I get?
Or like how, how good was that.
So it was $4 in this case.
Or, or if I decide to quit,
I got $10, okay?
Um, and if you remember in the case of search problems,
we're talking about cost.
I'm just flipping the sign here,
we wanted to minimize cost.
Here we want to maximize the reward just a more optimistic view of the world I guess.
Um, so, so that is what the rewards are going to be defined, okay?
We also have this as end function,
which again similar to search problems just checks if you're in an end state or not.
And in addition to that,
we have something that's called a discount factor.
It's, it's this value Gamma [NOISE] which is between 0 and 1.
And I'll talk [NOISE] about this later don't worry about [NOISE] it right now.
But it's a thing to define for our search pro- er, for our MDPs, okay?
All right. So how do I compare this with search?
Again, these were the things that we had in a search problem.
We had the successor function that would deterministically take me to S prime and we had
this cost function that would tell me what was the cost
of being in state S and taking action A.
So, so the major things that are changed is that instead of the successor function,
I have transition probabilities these T's, that,
that basically tell me what's the probability of starting in S,
taking action A, and ending up in S prime.
And then the cost just became reward, okay?
So, so those are kind of the major differences between search and MDP.
Because things are- things are not deterministic here [NOISE], okay?
All right, so, so that was the formalism.
Now, now I can define any,
any MDP model- any Markov Decision Process.
And then one thing- just one thing to point out is this transition probability is this t,
basically specifies the probability of ending up in
state S prime if you take action A in state S. So,
so these are probabilities, right?
So, so for example again,
like we have done this example but let's just do it on the slides again,
if I'm in state in, I take action quit, I end up in end,
what's the probability of that?
1.
And then if I'm in state in,
I take action stay,
I end up in state in again,
what's the probability of that?
I end up in again, two-thirds.
And then if I'm state in,
I take action stay,
I end up in end, what is the probability of that?
One-third, okay?
And then these are probabilities.
So what that means is they need to kind of add up to 1, but one thing to notice is well,
just what is going to add up to 1?
Like, like all of the things in the column are not going to add up to 1.
The thing that's going to add up to 1is if you consider
all possible these- different s primes that you're going to end up at,
those probabilities are going to add up to 1.
So, so if you look at this, this sta- stable again,
if you look at deciding and being stay in and taking action stay,
then the probabilities that,
that we have for different s primes are two-thirds and one
third, and those two are the things that are going to add up to 1.
And in the first case, if you're in stay in and you decide to quit,
then wherever- whatever s primes you're gonna end up at,
in this case, it's just the end state,
those probabilities are going to add up to 1.
So, so more formally what that means is,
if I'm summing over s primes,
these new states that I'm going to end up at,
the transition probabilities need to add up to 1.
Okay, because they're basically probabilities that tell me
what are the- what are the things that can happen if I take an action, okay?
And then these transition probabilities are going to
be non-negative because they are probabilities.
So that's also another property, okay?
So usual six. All right.
So, so that's a search problem.
Let's actually formalize another search problem.
This is- let's actually try to code this up.
So what is a search problem?
This is the tram problem.
So remember the tram problem.
I have blocks 1 through n. What I wanna do is I have two possible actions,
I can either walk from state S to a state S plus 1.
Or I can take the magic tram that takes me from state S to state 2S.
If I walk, that costs one minute, okay?
Means reward of that is minus 1.
If I, if I take the tram that costs two minutes,
that means that the reward of that is minus 2, okay?
And then the question was how- like how do we want to travel from,
from 1 to n in the least amount of time?
So, so nothing here is, is probabilistic yet, right?
So I'm going to add an extra thing here which says
the tram is going to fail with probability 0.5.
So I'm going to decide maybe you take,
take a tram at some point and that tram can,
can fail with probability 0.5.
If it fails, I end up in my state, like I don't go anywhere.
And, and actually like in this case,
you're assuming you're still losing two minutes.
So if I decide to take a tram,
I'm gonna lose two minutes,
maybe you'll fail, maybe we will not, okay? All right.
So let's try to formalize this.
So we're gonna take our tram problem from two lectures ago.
So this is from search one.
We're gonna just copy that.
So all right.
So this was what we had from last time.
You had this transportation problem and we had
all of these algorithms to solve the search problem.
You don't really need them because we have a new problem so let's just get rid of them.
And now I just want to formalize an MDP.
So, so it's a transportation MDP, okay? The initialization looks okay.
Start state looks okay.
I'm starting from 1, this end looks okay.
So the thing I'm going to change is the- first off I need to add this actions function.
Okay? So what would actions do?
It's going to return a list of actions that are our potential actions in a given state.
So I just copy pasted stuff from down there to just edit.
So it's going to return a list of valid actions.
Okay? So what are the valid actions I can take?
I can either walk or I can tram.
So I'm going to remove all these extra things that I had from before and just
keep it to be I'm either walking or I'm taking the tram, okay?
As long as it's a valid state.
So, so that looks right for actions.
The other thing we had was a successor and cost function.
So, so now we want to just change that and
return these transition probabilities and end reward.
So, so it's basically the successor probabilities and reward.
Okay? So I'm putting those
two together, similar to before we had successor and cost.
Now I'm returning probabilities and reward.
Okay? So what this function is going to
return is it's going to return this new status S prime,
I'm going to end up at and the probability value for that and reward of that.
Okay? So, so given that I'm starting in state S and I'm taking action A,
then what are the potential S primes that I can end
up at and what are the probabilities of that?
Then what, what is T of SAS prime and what is the reward of that?
What is the reward of SAS prime?
I want to have a function that just returns these so I can call it later.
Okay? All right.
So I need to basically check like for,
for each one of these actions,
I can for, for action walk.
What happens for action walk?
What's the new state I'm going to end up at?
Well, I'm going to end up at S plus 1.
It's a deterministic action.
So I'm going to end up there with probability 1 and what's the reward of that?
Minus 1 because it's one minute cost,
so it's minus 1 reward.
Then for action tram,
we kind of do the same thing but we have two options here.
I can- I can end up in 2S. Tram doesn't fail,
I end up in 2S. The probability 0.5 that cause- that reward of that is minus
2 or the other option is I'm going to end up in
state S because I didn't go anywhere because we had probability of 0.5, the tram did fail.
And that, that- the reward of that is minus 2.
And that's pretty much it. That, that is my, my MDP.
So I can just define this for a city with let's say 10 blocks.
Oh, and we need to have the discount factor but we'll talk about that later.
Let's say it's just 1 for now, okay?
And they'll use right- I'm writing these other states function for later but, okay.
Does that look right? We just formalized this MDP.
So let's check if it does the right thing.
So maybe we want to know what are the actions from state three?
What are the actions from state three?
Oh, we need to remove this utility function
from before because we don't have it in the folder.
So remove that.
What, what are the actions from state three?
I have 10 blocks.
If I'm in state three,
I can either walk or tram.
Either one of them is fine, right?
So, so that did the right thing.
Maybe we want to just check if
this successor probability and the reward function does the right thing.
So maybe, maybe we can try that out for state three and walk.
So, so for state three and action walk,
then what do we get?
Well we end up in four and that is,
that is with probability 1 with the reward of minus 1.
Okay? Let's try it out for tram.
Again, remember tram can fail,
so I'm gonna get two things here.
So these are the things I'm going to get for tram,
I'm going to either end up in six with probability
0.5 with the reward of minus 2 or I will not go anywhere.
I'm still at three with probability 0.5 and that is with a reward of minus 2.
Okay? All right.
So that was just the tram problem and we formalized it as an MDP.
Again, the reason it's an MDP is,
is that the tram can fail with probability 0.5.
So we added that in,
then we defined our transition function and our problem- and our reward function.
Okay? All right, everyone happy with how we are defining MDPs?
Yeah? Okay. Pretty similar to search problems except for now
we have these probabilities, okay? All right.
So, so now I have defined an MDP, that's great.
The next question that in general we would like to answer is to give a solution, right?
So there's a question here.
So what is the Markov part of an MDP?
So the Markov part means that you just depe- so,
so when you just depend on the state and this current state,
like the way we define our state remember,
our state is sufficient for us to make optimal decisions for the future.
So the Markov part means that you're Markovian, it only depends on the current state and
actions to end up in the probabilistically end up in the next,
next state. So yeah.
So the interesting question we would like to do is well,
we want to find a solution, right?
I want to figure out what is the optimal path to actually solve this problem.
And again if you remember search problems,
the solution to search problems was just a sequence of actions,
said that's all I had, like a sequence of actions,
a path that was a solution.
And the reason that was a good solution was like everything was deterministic,
so I could just give you the path and then that was what you would follow.
But in the case of MDPs,
the way we are defining a solution is by using this notion of a policy.
So a policy- let me actually write that here.
So we have defined an MDP but now I want to say well,
what is a solution of an MDP?
A solution of an Markov decision pro- process is a policy pi of S.
So and this policy basically goes from states,
so it takes any state and it tells me what is the-
what is the potential action that I would get for that state.
Okay? So, so if a policy is a function,
it's a mapping from each state S in the set of all possible states,
to, to an action and the set of all possible actions.
Okay? So in the case
of the volcano crossing, like I can have something like this.
I can be in state 1,
1 and then a policy of that state could be going south, okay?
Or I can be in state 2,
1 and a policy for that state is east.
If, if this was a search problem, I would just give a path.
I would just say go south and then to- go east and go north, right?
So, so that would be my solution.
But- but again, like if I decide that well the policy at 1,
1 is to go south, there is no reason for you to end up at south, right?
Because this thing, this thing is probabilistic.
So, so the best thing I can do is for every state just
tell you what is the best thing you can do for that particular state and,
and that's why we are defining a policy as opposed to ge- giving like a full path, okay?
All right, so policy is the thing you're looking for.
And ideally, I would like to find
the best policy that would just give me the right solution.
But in order to get there,
I want to spend a little bit of time talking about how good a policy would be.
So and that's kind of this idea of evaluating a policy.
So in this middle section, I don't want to try to find a policy, I,
I just assume you give me a policy and I can evaluate it and tell you how good that is.
So, so that's the plan for the middle section, okay?
All right. Everyone happy with- so,
so far all I've done is I've defined an MDP,
which is very similar to a search problem, it's just probabilistic.
Okay? So so how would we evaluate a policy?
Okay? So if you give me
a policy which basically tells me at every state S, take some action,
then that policy is going to generate a random path, right?
I can get multiple random paths because nature
behaves differently and the world is uncertain.
So I might get a bunch of random paths and then those are all
random variables, uh, random paths, sorry.
And, and, and then for each one of those random paths,
I can, I can define a utility.
So, so what is the utility?
Utility is just going to be the sum of rewards that I'm going to get over that path.
I'm calling it as, as the discounted sum of the rewards.
Remember that discount, we'll talk about that but,
but you can- you can discount the future.
But, but for now just assume it's just a sum of the rewards on that path, okay?
So a util- the utility that we are going to
get is also going to be a random variable, right?
Because if if you think about a policy,
a policy is going to generate a bunch of random paths and
and utility is just going to be the sum of rewards [NOISE] of each one of those.
So it's a random variable.
So, so if you remember this example, right?
So I can, I can basically have a path that tells me
start in in, and then stay and then that ends.
Right? So so this is one random path,
and for this particular random path, well,
what is the utility I'm gonna get? I'm just gonna get $4.
That's one possible thing that can happen.
If my, if my, um, policy is to let's say stay,
like there is no reason for for the game to end right here.
Right? Like I can have a lot of different types of random path.
I can have a situation where I'm staying three times and then
after that ending the game and utility of that is 12.
We can have this situation where we have stay, stay, and end.
That's the situation it's all, like you had,
you had an utility of eight and so on.
So, so you're getting all these utilities for all these random paths.
So, so these utilities are also going to be just random variables.
Okay? So I can't really play around with the utility.
That's not telling me anything.
Although it's telling me something but it's a random variable.
I can't optimize that.
So instead we need to define something that you can actually play around with it and,
and that is this idea of a value which is just an expected utility.
So, so the value of a policy,
is the expected utility of that policy.
And then that's not a random variable anymore,
that's actually like a number and I can I can compute that number.
I can compute that number for every state and and then just play around with value. Okay, next question?
What is the value of the policy, does,
is that policy needs defined for all possible states or a particular state?
For all possible. So so the question is, yeah,
so when you say value of policy, uh,
is the policy basically telling me, um,
is a policy basically telling me,
uh, what- what is a strategy for all possible states?
Well, um, you're defining policy as a function of state, right?
So, and value is the same thing as a function of state.
I might ask what is the value of being in in?
So the value of being in in is, is, and, and, following, and following policy stay,
is, is going to be the, the value of fo- following
policy stay from this particular state which is the expected utility of that,
which is, which is basically that 12 value there.
I could ask it for about any other state too.
So I can be in any other state and then say well,
what's the value of that?
And, and when we do value iteration and you actually need to
compute this value for all states to kind
of have an idea of how to get from one state to another state but [OVERLAPPING].
[inaudible] will be in state in and
the policy given your state in taking the actions stay.
Yes.
Okay.
Yeah. And that is, that is what 12 is.
Okay? And 12 like we kind of
empirically we have seen, it's 12 but we haven't shown how to get 12 yet.
Okay? All right.
So, um, actually let me write these in my lists of things.
So we talked about the policy.
What else did we talk about?
We talked about utility.
So what is utility?
Utility, we said it's sum of rewards.
[NOISE] So if I get like reward 1, then I get reward2two.
It's a discounted sum of rewards.
So I'm gonna use this gamma which is that discount that I'll talk about in a little bit
times reward 2, plus gamma squared times reward 3, and so on.
So utility is, you give me a random path and I just sum up the rewards of that.
Imagine if gamma is 1,
I'm just summing up the rewards.
If gamma is not 1, I'm summing- I'm looking at this this discounted sum.
Okay, so, so that is utility.
But value- so this is utility,
value is just the expected utility, okay?
So you give me a bunch of random paths,
I can compute their utilities,
I can just sum them up and average them and that gives me value. Yes.
If the discount factor is 1, would that be bounded?
That's a very good question and we'll get back to that.
So, so, so in general, and, and, and. Okay.
If if it is acyclic, it is fine,
but if you have a cyclic graph you want your gamma to be less than 1.
And we'll talk about that when we get to the convergence of these algorithms.
All right, how am I doing on time? Okay. All right.
So so let's go to the, uh,
this particular volcano crossing example.
Um, so in this case, um, like I can run this game,
and every time I run it,
I'm gonna get a different utility because like I'm gonna end up in some random path,
some of them end up in the volcano,
that's pretty bad, right?
So I get different utility values,
utilities [LAUGHTER] but the value which is the expected utility is not changing really.
It's just around 3.7 which is just the average of these utilities.
So I can keep running this getting these different utilities, but
values is one number that, that I can, I can talk about
and, and that's the value of
this particular state and that tells me like what would be the best policy that I
can take and what's the best amount of utility
that I can get from in expectation from that state?
Okay? All right,
so we've been talking about this utility
I've actually written that already on the board.
So utility is going to be a discounted sum of rewards.
And then we've been talking about this discount factor.
And the ideal of the discount factor is I might
like care about the future differently from how much I care about now.
So, so for example, if if you give me $4 today,
and you give me $4 tomorrow,
like if that $4 tomorrow is
the same kinda amount and has the same value to me as as today,
then then I might,
it's kinda the same idea of having a discount counter of 1,
uh, discount of, of 1, gamma of 1.
So you're saving for the future, the values of things in the future is the same amount.
If you give me $4 now,
if you give me $4 10 years from now,
it- it's going to be $4.
I care about it like $4 amount and I can just add things up.
But it could also be the case like you might be in a situation,
in a particular MDP,
where you don't care about the future as much.
Maybe you give me $4 10 years from now and
that's that doesn't like, I don't have any value for that.
So, uh, if then that is the case and you just want to live in
the moment and you don't care about the values you're gonna get in the future,
then that's kind of the other extreme when- when this this gamma,
this discount is equal to 0.
So so that is a situation that if I get $4 in the future,
that they don't like val- like they don't have any value to me.
They're just like a 0 to me.
So, so I only care about right now
living in the moment what is the amount I'm going to get.
And then in reality you're like somewhere in between, right?
Like we're not this, this case where we are living in a moment,
we're also not this case that, that
everything is just the same amounts like right now or in
the future in- and like in balanced life as a setting where we have some discount factor,
it's, it's not 0, it's not a 1,
it actually discounts values in the future because future
maybe doesn't have the same value as now but,
um, but we still value things in
the future like $4 is still something in the future.
And, and that's where we pick like a gamma that's between 0 and 1.
So so that is kind of a design choice like depending on what problem you're in,
you might want to choose a different gamma. Question, yeah.
So is discounting utility, is it an assessment of risk or is there,
like, a different way we can assess how much risk you want to take?
Um, you could, you could think of it as
an, it's not really an assessment of risk in that way.
It depends on the problem, right?
It depends on like in a particular problem,
I do want to get values in the future or have like some sort of
long term like goal that I want to get to and I care about the future.
Like it it depends, like, if you're solving
a game versus you're solving like, I don't know,
mo- mo- mobile like a robot manipulation problem
like it might just be a very different like discount factor that you would use.
For a lot of examples we'd use in this class,
we just choose a gamma that's close to 1.
Like- like usually like for a, for a lot of
problems that we end up dealing with gamma it's like 0.9.
That's like the usual.
Okay, like for usual problems.
Like you might have a very different problem where we don't care about the future.
So, so then we just drop it. Yes.
[inaudible] is gamma a hyperparameter that needs to be
tuned and is a gamma 0 the same as a 3D algorithm?
Gamma. Okay. So so that's a good question.
So is- is gamma a hyperparameter that you need to tune?
I would say gamma is a design choice.
It's not a hyperparameter necessarily in that sense that,
oh if I pick the right gamma that will do the right thing.
You want to pick a gamma that kind of works well with your problem statement.
Um, and, and gamma of 0 is kind of greedy,
like you are picking like what is the best thing right now and I just don't
care about the future ever. Question right there.
Does gamma violate the Markov property because like
this kind of memory of what you save is.
It doesn't violate the Markov property.
It's just a discount of like your- it's about the reward.
It's not about how this state affects the next state.
It basically affects how much reward you're
going to get or how much value you reward in the future.
It doesn't, it doesn't actually like- it's still a Markov decision process.
[inaudible] and make your possible actions [inaudible]?
What you are getting with- it's affecting the reward yeah,
but it's Markov because if I'm in state s and I take action a,
I'm gonna end up in s prime and that doesn't depend on like gamma.
Okay. All right.
So. Okay. So, so in this section we've been talking
about this idea of someone comes in and gives me the policy.
So the policy is pi and what I want to do is,
I want to figure out what's the value of that policy,
and again value is just the expected utility.
Okay? So V pi of s is the expected utility
received by following this policy pi from state s. Okay?
So, so I'm not doing anything fancy.
I'm not even trying to figure out what pi is.
All I want to do is, I want to just evaluate.
If you tell me this is pi,
how good is that? What's the value of that?
Okay? So, so that's what a value function is.
So value of a policy is, is V pi of s. Okay?
That's expected utility of starting in some state, um,
let me put this here and then I'm going to move these up.
[NOISE] Um, yeah, yeah so V pi is,
is the value- the expected utility of me starting in some state S. Okay.
And state S has value of pi of S. And if someone tells me that,
well you're following policy pi,
then I already know from state S,
the action I'm going to take is pi of S. So that's very clear.
So I'll take pi of S. And if I take pi of S we'll- I'm going to end up in some chance node.
Okay. And that chance node is, is a state action node.
It's going to be S and the action- I've decided the action is pi of S. Okay.
And of this- define this new function,
this Q function, Q pi of S,
a, which is just the expected utility from the chance node.
Okay. So, so we've talked about value,
value is expected utility from my actual states.
I'm going to talk about Q values as expected utilities from the chance nodes.
So after you've committed that you,
you have taken action a, and,
and you're following policy pi.
Then, what is the expected utility from that point on, okay.
And well what is the expected utility from this point on?
We are in a chance node,
so many things that can happen because I have
like nature is going to play and roll its die,
and anything can happen.
And they're going to have in transition, S, a,
S-prime and with that transition probability,
I'm going to end up in a new state.
And I'm going to call it S-prime,
and the value of that state- again,
expected utility of that state is V pi of S-prime, okay. All right.
So, okay.
So what are these actually equal to?
So I've just defined value as expected utility,
Q value as expected utility from a chance node,
what, what are they actually equal to?
Okay. So I'm going to write
a recurrence that we are going to use for the rest of the class.
So pay attention for five seconds. There is a question there.
I understand how semantically how pi and v pi are different, in like actual numbers,
like expected value- how are they different?
So they're- both of them are expected value.
Yeah. So it's just- one is just a function of
state the other one you've committed to one action.
And the reason I'm defining both of them,
is to just writing my recurrence is going to be a little bit easier,
because I have this state action nodes, and I can talk about them.
And I can talk about how like I get branching from these state action nodes, okay?
All right. So I'm going to write a recurrence.
It's not hard, but it's kind of the basis of the next like N lectures,
so pay attention. So alright.
So V pi of S, what is that equal to?
Well, that is going to be equal to 0,
if I'm in an end state.
So if IsEnd of s is equal to true,
then there is no expected utility that's equal to 0. That's a easy case.
Otherwise- well, I took policy pi S. Someone told me,
take policy pi S. So value is just equal to Q, right?
So, so in this case, V pi of S,
if someone comes and gives me policy pi,
it's just equal to Q pi of S, a.
Okay. These two are just equal to each other.
So the next question one might ask is-
actually let me write this a little closer so I'll have some space.
Yeah. So this is equal to Q pi of S, a, okay.
So, so what is that equal to?
What is Q pi of S, a equal to?
So this is V pi S. So now,
I just want to know what is Q value,
Q pi of S, a. What is that equal to?
Okay. So if I'm right here then there are
a bunch of different things that can happen, right?
And I can end up in these different S-prime.
So if I'm looking for the expected utility then I'm looking for
the probability of me ending up in this state times the utility of this state,
plus the probability of me ending up in a new state times the utility of that.
So, so that is just equal to sum over all possible S-primes that I can end up
at of transition probabilities of S, a, S prime.
Transition probability of ending of
a new state, times the immediate reward that I'm going to get,
reward of S, a, S prime, plus the value here.
But I care about the discounted value.
So I'm going to add gamma V pi of S-prime,
because I've been talking about this, this next state.
Okay. There's this, does everyone see this?
Okay. So this is the recurrence that we are doing in policy evaluation.
Again, remember someone came and gave me policy pi.
So I'm writing this policy pi here.
Someone gave me policy pi,
I just want to know how good policy pi is.
I can do that by computing V pi.
What is V pi equal to?
Someone told me you're following policy pi,
so it's gotta be equal to just Q pi.
What is Q pi equal to?
It's just sum of all the- like the expectation of
all the places that I can end up at that sum over S-primes,
transition probabilities of ending up in S-prime,
times the reward- the total reward you're getting which is the immediate reward,
plus discounting in my future, okay. Yes.
What if Q values and then following policy pi starting from S-prime?
Yes. Yeah, yeah, yeah, starting from S-prime.
All right. So okay. So far so good.
So so that is how I can evaluate this policy, right?
So, so I have these two recurrences- if I have these two recurrences,
I can just replace this guy here,
and let's imagine we're in the case- maybe I can use a different color up here.
Um, I'm just replacing,
I'm just replacing this guy right here.
I don't know if it's worth writing it.
Imagine we we're not in an end state.
If you're not in an end state then V pi of S,
well, what is that equal to?
That is just equal to sum of transition probabilities S,
a, S-prime, over S-primes,
times immediate reward that I'm going to get,
plus discounting V pi of S-prime.
Okay. So this is kind of a recurrence that I have.
I, I literally just combined these two,
and wrote it in green, okay,
if you're not in an end state.
So if you're not in an end state,
this is the recurrence I have.
I have V pi here,
I have V pi on this side too.
So that is nice.
And, and that is kind of the, the placer.
I can compute V pi.
Maybe I can do it literally or maybe I can
actually find a closed form solution for some problems,
but that is basically what I'm going to do.
I have V pi as a function that depends on V pi of S-prime.
And I can just solve for this V pi.
Okay. It allows me to evaluate policy pi.
I haven't figured out a new policy.
All have done is evaluating what's the value of pi, okay. All right.
Okay, so let's go back to this example.
So let's say that someone comes in and tells me
well the policy you gotta follow is, is to stay.
So my policy is, is to stay.
Okay. I want to know- I want to just evaluate that,
I want to do policy evaluation.
When you're doing policy evaluation,
you gotta compute that V pi for all states.
So let's start with V pi of end,
oh that is equal to 0,
because we know V pi at end state is just equal to 0.
Now, I want to know what's V pi of in, okay stay, in.
What is that equal to? That's just equal to Q pi of in and stay, right?
V pi is just equal to Q pi of in and stay.
So I'm going to replace that,
that's just equal to one-thirds,
times immediate reward, which is 4,
plus value of the next state I'm going to end up at,
which is end in this case,
plus two-thirds, times the immediate reward I'm going to to get,
which is $4, plus value of the state I'm going to end up at, which is end.
Okay. So, so that is just that sum that we have there, right?
V pi of end is 0,
so let me just put that 0 there.
I'm going to put 0 there.
I only have one state here too, right?
So, so th- I just have this other function of this one, stay, in.
So having an equation,
I can find the closed form solution of V pi of in.
I'm just going to move things around a little bit.
And then I will find out that V pi of in is just equal to 12.
So, so that's how you get that 12 that I've been talking about.
So, so you just found out that if you tell me the policy to follow is stay,
if that is the policy,
then the value of that policy from state in is equal to 12.
Is it you always choose the same or- so you always choosing to state.
Yeah. So, so the policy is a function of state.
I only have this one state that's interesting here, right?
That, that one state is in.
So I need to- when,
when I defined my policy,
I need to kind of choose the same policy for, for that state, right?
My policy says, in in you've got to either stay or you've got either quick- quit.
Okay. All right.
So you can basically do the same thing using an iterative algorithm too.
So, so here like in the previous example,
it was kind of simple.
I just solved the closed form solution.
But in, in reality like you might have different states
and then the com- it might be a little bit more complicated.
So we can actually have an iterative algorithm that allows us to find these V pis.
So the way we do that is,
we start with the values for all states to be equal to zero.
And, and this zero that I- I've put here,
is the first iteration.
So, so I'm going to count my iterations here.
So, so I'm going to just initialize
all the values for all states to just be equal to zero.
Okay. Then I'm just going to iterate for some number of time,
whatever number I care, like I would like to.
Then, what I'm going to do is, for every state- again,
remember the value needs to be computed for every state.
So for every state,
I'm going to update my value by the same equation that I have on the board, okay?
And the same equation depends on the value at the previous time step.
So this is just an iterative algorithm that allows me to
compute new values based on previous values that I've had.
And I started like everything zero and then I keep
updating values of all states and I keep going, okay?
So basically, that equation but think of it as like an iterative update every round.
So you- you don't run this for multiple rounds.
Every round you just update your value.
Okay. So like here, is just pictorially you're looking at it,
imagine you have like,
five states here, you initialize all of them to be equal to 0.
The first round, you're going to get some value you're going to update it.
And then you're going to keep running this and then eventually,
you can kind of see that the last two columns are kind of
close to each other and you have converged to the true value.
So, so again, someone comes and gives you the policy,
you start with values equal to 0 for all the states,
and then you just update it based on your previous value.
Okay. So how long should we run this?
Well, we have a heuristic to- to kind of
figure out how long we should run this particular algorithm.
Uh, one thing you can do is you can kind of keep track of
the difference between your value at the previous time step versus this time step.
So, so if the difference is below some threshold you can, kind of,
call it- call it done and- and say,
well I've- I've found the right values.
And then in this case,
we are basically looking at the difference between value at
iteration T versus value at iteration T minus 1.
And then we are taking the max of that over all possible states,
because I want the values to be close for all states. Okay. Yes.
[inaudible]
Is this- so I'm going to talk about the convergence when we talk about
the gamma factor and- and- and the- the discount factor and acyclicity.
Um, also how long you should run this to get these
is also a difficult problem and it depends on the properties of your MDP.
So if you have an ergodic- if you have an ergodic MDP if this- this should work.
Okay, but in general,
it's a hard problem to answer for general Markov decision problem processes.
Okay. And another thing to notice here is,
I'm not storing that whole table.
Like the only thing I'm storing,
is- is the last two columns of this table because- because
that's V pi at iteration T and V pi at iteration T minus 1.
Those are like, the only things I'm storing,
because that allows me to compute and if I've converged then that kind of
allows me to keep going because I only need
my previous values to update my new values, right.
In terms of complexity,
well this is going to take order of T times S times S prime. Well, why is that?
Because I'm iterating over T times step,
and I'm iterating over all my states and I'm summing over all S primes, right.
So because of that- that's a complex idea yet,
and one thing to notice here,
is it- it doesn't depend on actions, right. It doesn't depend on the size of actions.
And the reason it doesn't depend on the size of actions as you have given me the policy,
you are telling me follow this policy.
So if you've given me the policy then I don't really need to worry about, like,
the number of actions I have.
Okay. All right.
Um, here is just another like the same example that we have seen.
So at iteration T equal to 1, in,
is going to get 4, end is going to get 0,
at iteration 2 it gets a slightly better value.
And then finally, like at iteration, like,
100 let's say, we get the value 12.
And then remember for this particular example, like,
this example we were able to solve it, like,
solve the closed form V- V- of, ah,
V- V of policy staying, uh, from state, in, but,
uh, but you could also run the iterative algorithm
and get the same value of 12. Okay. Yes.
Number of actions is just the size of S prime, right?
The number of, uh, actions is the size of S prime.
Uh, no because the size of S- you might end up in very different, different states.
This depends on your probabilities.
Oh, okay.
The size of S prime is actually the size of,
like, size of states is the same thing, right?
Like it's you can- worst case scenario,
you're going from every state to every state.
So just imagine the size of S. [NOISE]
Okay. All right.
So summary so far where are we?
So we have talked about MDPs.
These are graphs with states and chance nodes
and transition probabilities and- and rewards.
And you have talked about policy as the solution to an MDP,
which is this function that takes a state and gives us an action.
Okay. We talked about value of a policy.
So value of a policy is the expected utility of- of that policy.
So, so if you have like utility you- we have
these random values for all these random paths that you're going to get for every policy.
The value of utility is just an expectation over all those random, random variables.
And so far we have talked about this idea of policy evaluation,
which is just an iterative algorithm to compute what's the value of a state.
If you'd give me some policy, like,
how good is that policy what's the value I'm going to get at every state.
Okay. All right.
So- okay, that has been all assuming you'd give me the policy.
Now, the thing I want to spend a little bit of time on
is- is figuring out how to find that policy.
Uh, is that possible that the variable actions for problem that is
going to change the value of the policies.
We learn new actions.
So for example here, we only have stay or quit.
Uh-huh.
If you have a different problem that they can learn another action, like,
stay quit or something, uh, um, the trade.
Is it going to change the value of the policies because then we
had a new action and then we need to update our policies?
So in this case so, so far I'm assuming that a set of actions is fixed.
I am not like adding new actions,
like, the way- even with search problems, like,
the way we defined search problems or the way we are defining MDPs is I'm saying, like,
I'm starting with a set of states are fixed,
actions are fixed, I have stay and quit.
Those are, like, the only actions I can take.
Uh, the reward is fixed, uh,
transition probabilities are fixed under that scenario.
Then what is best- the best policy I can
take and best policies is just from those set of like,
def- already defined actions.
Okay. Um, next lecture we will talk about unknowing settings,
like when we have transition probabilities that are not known or
reward functions that are not known and how we go about learning them.
And, and that- that will be the reinforcement learning lecture.
So next lecture I might address some of those.
Okay. All right, so let's talk about value iteration.
So, so that was policy evaluation.
So like, that whole thing was policy evaluation.
So now, what I would like to do is I want to try to get
the maximum expected utility and find
the set of policies that gets me the maximum expected utility, okay?
So to do that I'm going to define this thing that's called an optimal value.
So instead of value of a particular policy,
I just want to be opt of S,
which is the maximum value attained by any policy.
So, so you might have a bunch of different policies,
I just want that policy that maximizes the value.
Okay. So and that is V opt of S. Okay.
So, um, so let me go back to this- this example.
So I'm going to have this in parallel to this example of policy evaluation,
I want to do value iteration.
Okay. So I'm going to start from state S again,
state S has V opt of S. Okay.
That is what I like to find here I have V pi of S. If I'm looking for V opt of S,
then I can have multiple actions
that can come out of here and I don't know which one to take,
but like, any of them- if I take any of them,
if I take this guy,
that takes me to a chance node of SA.
Okay. And then I'm looking for Q opt of SA.
And from here, it's actually pretty similar to what we had right here.
So I'm in a chance node, anything can happen, right?
Nature plays and with some transition probability of SA,
S prime I'm going to end up in
some new state S prime and I care about V opt of that S prime.
Okay. So if I'm looking for this optimal policy which comes from this optimal value,
then I need to find V opt.
And if I want to find V opt well,
that depends on what action I'm taking here.
But let's say, I take one of these.
And if I take one of these I end up in a chance node,
I have Q opt SA in that chance node.
And then from that point on with whatever probabilities I can end up in some S prime.
Okay. So I want to write the recurrence for
this guy similar to the recurrence that we wrote here.
It's going to be actually very similar.
So- okay, so I'm going to start with Q because that is easier.
So what is Q opt of SA that- that just seems very similar to this previous case.
What is that equal to? What was Q pi?
Q pi was just sum of transition probabilities times rewards,
right. So, so what is Q opt?
[inaudible].
Yeah. So, so it would just be basically this equation
except for I'm going to replace V pi with V opt.
So, so from Q opt,
I can end up anywhere like based on the transition probabilities.
So I'm going to sum up over S primes and all possible places that I can end up at.
I'm going to get an immediate reward which is RSA S-prime.
And I'm going to discount the future but the value of the future is V opt of S-prime.
Okay. So, so far so good that's Q opt.
How about V opt. What is that equal to?
Well, it's going to be equal to 0 if you are in an end state that's similar to before.
So if end of S is true then- then it is 0.
Otherwise, I have- I have a bunch of options here, right.
I can take any of these actions and I can get any Q opt.
So which one should I pick?
Which Q opt should I pick?
The one that maximizes, right?
Like, um, which actually I should pick an action from the set
of actions of that state that maximizes Q opt.
So, so the only thing that has changed here is before someone told me what the policy is,
I just took the Q of that.
Here I'm just picking the maximum value of
Q and that actually tells me what action to pick.
So what is the optimal policy?
What should be the optimal policy?
Hmm? I'm going to call it pi opt of S. What is that equal to?
It's gotta be the- the thing that maximizes V, right.
Which is the thing that maximizes this- this- this Q.
So because that gives me the action.
So it's going to be the argmax of Q opt of S and A.
Where A is an action of S. Okay?
All right, so this was policy evaluation.
Someone gave me the policy.
With that policy I was able to compute V,
I was able to compute Q,
I was able to write this recurrence,
then I had an iterative algorithm to do things,
This is called value iteration.
This is to find the right policy Iteration.
This is to find a policy. How do I do that?
Well I have a value that's for
the optima- optimal value that I can get and it's going to be
maximum over all possible actions I can take of
the Q values and Q values are similar to before.
So I have this recurrence now and at optimal policy is just an argmax of Q. Yeah.
It looks like there are two argmax, right?
Sorry? What?
Phi for argmax like just two argmax,
right, like there are two As?
Oh, yes. You could get two A's, So the question is,
yeah, like, what if I have two A's that give me the same thing?
I can return any of them. It depends on your implementation of max.
So you can return any of them.
How am I doing on time?
[NOISE] We are five minutes over and if you want.
[LAUGHTER] So good news is the slides are the same things that I have on the board.
So so Q_opt is just equal to the sum that we've talked about V_opt.
I just add the max on top of Q_opt same story, okay? And then if I want the policy,
then I just do the argmax of Q_opt and that gives me the policy.
Right. I can have and again an iterative algorithm that does the same thing.
It's actually quite similar to the iterative algorithm for policy evaluation.
I just start setting everything to equal to 0.
I iterate for some number of times.
I go over all possible states.
And then, I just update my value based on this new recurrence that has a max, okay?
So very similar to before, I just do this update.
One thing is the time complexity is going to be order of T times S times A times,
S prime because now I have this max value over all possible actions.
So I'm actually iterating over all possible actions versus in policy evaluations,
I- I didn't have A, because,
because someone would give me the policy.
I didn't need to worry about this.
All right. So let's look at coding this up real quick.
Okay, so we have this MDP problem.
We define it, it was a Tram problem,
it was probabilistic, everything about it was great.
So now I just wanna do an algorithm section and inference section where I code up value
iteration and I can call a value iteration
on this MDP problem to get the best optimal policy.
Okay. So I'm going to call value iteration later.
All right. So we initialize,
so all the values are going to become- I might skip things to make this faster.
So we're gonna initialize all the values to
just 0, right, because all these values are gonna be 0.
So I defined a states function.
So i for all of those the value is just going to be equal to 0.
So it's initialized with that.
Then you're just gonna iterate for some number of time.
And what we wanna do is you wanna compute this new value given old values.
So it's an iterative algorithm.
We have old values, we just update new values based on them.
So what should that be equal to?
So we iterate over our states.
If you are in an end state then what is value equal to? 0, right?
If you're not in an end state,
then you're just gonna do that- that- that recurrence there.
Okay, So new value of a state is going to be equal to max of,
what the Q values, okay.
So new V is just max of Qs of state and actions. Okay.
So now I need to define Q or what does Q do?
Q of state and action is just equal to that sum over- over S primes.
So it's gonna return sum and it's gonna return sum over S primes.
I define this successor probability and
reward function that gives me newState probability and reward.
So I'm gonna iterate over that and- and call that up here.
So given that I have a state and action I can get newState probability and reward.
What are we summing, you're summing the probability, the transition
probabilities times the immediate reward which is reward here
times my- plus my discount times my V
which is the old value of V over S prime, over my newState.
So that is my Q, that is my V, and that's pretty much done.
We just need to check for convergence. To check for convergence,
we kind of do the same thing as before.
We check if value of V and new V are close enough to,
to each other that we can call it done.
I'm gonna skip these parts.
So- so you can basically check if V minus
new V are within some threshold for- for all states.
And if they are then,
V is equal to new V. We need to read the policy.
So policy is just argmax of Q.
So I'm gonna make this a little faster.
So the policy is just going to be, well, none if we're in an end state and otherwise
it's just going to be argmax of- of our Q values.
So I'm just writing argmax here pretty much.
I'm just returning the action that maximizes the Q.
And then we spent a bunch of time getting the printing working.
So let me actually get. Yeah, okay.
All right actually right here. So I'm running this function.
I'm- I'm writing out, actually these are a little shifted grid.
States [LAUGHTER] values and then Pi
which is the policy K. So it starts off walk, walk, walk.
Remember this is the case where we have 50% probability
of tram failing and with 50% probability of tram failing,
these are the values we are gonna get.
And the policies still walk until state five.
And then take the tram from, from state five.
Okay, just kind of interesting
because the policy of the search problem was the same thing too.
Okay, so the thing we can do is,
we can actually, let me move this a little bit forward.
We can actually define this fail probability which becomes just a variable.
So you can play around with this.
If you pick different fail probabilities you're gonna get different policies.
So for example if you pick a fail probability that is large then probably like
that policy is going to be just, just walk
and never take the tram because the tram is failing all the time.
But if you- if you decide to take fail probability is
close to 0, then- then this is
your optimal policy which is close to the search problem.
It's basically the solution to a search problem.
So play around with this, the code is online.
This was just value iteration- value iteration,
um, on this tram problem.
Okay. So I'm gonna skip this one too.
All right, so yeah.
And- and this is also showing like how over
multiple iterations you can kind of get to the- get to
the optimal- optimal value and optimal policy using value iteration.
So in one iteration it hasn't seen it yet.
So it think that the value, optimal value is 1.85, it hasn't updated the values.
And so with like, I don't know, three iterations,
it gets better but it hasn't still
updated. It still thinks it can't get to the other side.
And remember this is with stick probability of 10%.
But if I get to like I think 10,
then it eventually learns the best policy is to get to 20 and the value is 13.68.
And if you go even like higher iterations after that point it's just fine-tuning.
So the values are around 13 still. So you can play around with the volcano problem.
Okay. So when does this converge? So if
the discount factor is less than 1 or
your MDP graph is acyclic then this is going to converge.
So if MDP graph is acyclic that's kind of obvious
you are just doing dynamic programming over your full-thing.
So- so that's going to- that's going to converge.
If you have cycles,
you- you want your- your discounts to be less than 1.
Because if you're, if you have cycles and your discount is
let's say 1 and let's say you are getting 0 rewards from,
then you're never going to change.
You're never going to move,
you move from your state.
You're always going to be stuck in your state.
And if you have non-zero rewards you're going to
get this unbounded reward and you keep going because you
have cycles and it's just going to end up becoming numerically difficult.
So just a good rule of thumb is pick a Gamma that's less than one.
Then you kind of get this convergence property.
Okay, all right, so summary so far is we have MDPs.
Now, you've talked about finding policies,
rather than path, policy evaluation is just a way of computing like how good a policy is.
And the reason I talked about
policy evaluation is there's this other algorithm called policy
iteration which uses policy evaluation and we didn't discuss that in the class.
But it's kind of like,
not equivalent but you could use it in a similar manner as value iteration.
It has its pros and cons.
So policy evaluation is used in those settings.
Do not leave please.
We have more stuff to cover. [LAUGHTER]
And then we have value iteration, uh, which, uh,
computes its optimal value which is the maximum expected utility, okay?
And next time, we're going to talk about reinforcement learning,
and that's going to be awesome.
So let's talk about unknown rewards. All right.
So that was MDPs [LAUGHTER] doing inference and,
and kind of defining them.
I'm going back to the last lecture just to kind
of talk about some of the stuff that we didn't cover last time, okay?
All right. So if you remember last time,
we were talking about search problems.
So big switch now.
Search problems, where we don't have probabilities,
and we talked about A-star as a way of just making things faster,
and we talked about this idea of relaxations which was,
uh, a way of finding good heuristics.
So A-star had this heuristic.
Heuristic was an estimate of future costs.
We wanted to figure out how to find these heuristics,
like, how do you go about finding these heuristics?
And one idea was just to relax everything, that
allows you to come up with an easier search problem or just an easier problem,
and that helps you to find what the heuristic is, okay?
So, um, [NOISE] so we talked about this idea of removing constraints,
and when you remove constraints,
then you can end up in nice situations.
Like in some settings, you have a closed-form solution.
In some other settings, you have just an easier search problem,
and you can solve that, and in some other settings,
you have like independent sub-problems.
So when you remove constraints then,
then you have this easier problem.
You can solve that easier problem,
and that gives you a heuristic.
You're not done yet, right?
You're- you have a heuristic.
You take that heuristic,
and then change your costs,
and then just run uniform cost search on your original problem.
So, so solving an easier problem
is like you're not done when you have solved the easier problem.
It just helps you to find a thing that helps for- with the original problem,
so it's kind of like a multi-step thing.
So examples of that is,
if you have walls, remove all the walls,
you have an easier problem.
If you solve that easier problem,
that gives you a heuristic, and in this case,
like when you knock down these walls,
that easier problem you have a closed-form solution for it.
You don't need to do anything fancy.
You don't need to do uniform cost search. Any of that.
You just compute the Manhattan distance and,
and then that gives you the heuristic.
With that heuristic, you go and solve the original problem. That was one example.
Another example is, when you remove constraints,
you have an easier search problem.
So you don't have closed-form solutions,
but you have an easier search problem.
So you might have a really difficult search problem
with a bunch of constraints that are hard to do.
Remove the constraints.
So when you remove the constraints,
you have a relaxed problem,
which is just the original problem without the constraint.
That's a search problem.
You can solve that search problem using uniform cost search or dynamic programming and,
and solving that allows you to find the heuristic.
Again, you're not done yet, right?
You take the heuristic,
and then you go to the original problem,
change the costs, and, and draw your uniform costs there.
And just one quick kind of example here was,
uh, when you're computing these relax problems,
the thing you want to find is the future costs of this,
this relaxed problem, and,
and to do that, you have this easier search problem.
You still need to run uniform cost search or dynamic programming.
In this case, if you decide to run uniform cost search,
remember, uniform cost search computes past costs.
In this case, I really wanna compute future costs.
So you need to do a bunch of engineering to get that working.
In this particular case,
the relaxed problem, you need to reverse it.
Because when you reverse it,
past costs of the reversed relax problem
becomes future cost of the relaxed problem, if that makes sense.
So, so the way I'm reversing this is I'm
basically saying start state is n. End state is 1,
and my walk action takes me to s minus 1, instead of s plus 1,
and my tram action takes me to s over 2 instead of S times 2,
and the whole reason I'm doing that is- is that the past cost of
this new problem is the future costs of the non-reversed version.
Okay. Because I, I need to use uniform cost search here, okay?
So I run my uniform cost search,
that gives me a heuristic,
and that heuristic gives me this future cost of the relaxed problem,
and everything will be great.
Another example is, I can have independent subproblems using my heuristic.
So in this case,
like we have these tiles,
they technically cannot overlap.
Instead, what we are allowing is,
you're allowing them to overlap.
So if we allow them to overlap,
I have eight independent subproblems that I can solve.
These subproblems give me heuristics,
and I can just go with them, okay?
So, so these were just a bunch of examples,
and kind of the key idea was reducing edge,
li- like when we are coming up in these relaxed problems,
we're reducing edge costs from infinity to some finite costs.
Okay. So I'm getting rid of walls before I couldn't cross, like it was infinity.
Cost of that was infinity,
but if I get rid of the wall and making it a finite cost.
So this type of method,
um, this is a general framework.
So the point I wanna make is, generally,
you can talk about the relaxation of a search problem.
So if you have a search problem P,
a relaxation of a search problem,
I'm going to call that PR, uh, Prel,
is going to be a problem where the cost of the relaxation for
any state action is less than or equal to cost of state and action.
I'll take questions afterwards. All right.
So, uh, so that is a relaxed problem, okay?
So the cool thing about that is,
if you're given a relaxed problem,
then you can pick your heuristic to be the future cost of the relaxed problem,
and that is called the relaxed heuristic, okay?
So, so this is kind of a recipe. A general framework.
Like, if someone asks you find a good heuristic,
find a relaxed problem,
future cost of the relaxed problem is a heuristic.
And the cool thing about that is it turns out that, that,
that future cost of the relaxed problem,
which you are deciding to be a heuristic,
is also consistent because we talked about all these consistency properties,
and how you want to find the heuristic to be consistent for the solution to be correct,
and how in the world am I gonna find a consistent heuristic?
Well, here is one. Here is one way of finding consistent heuristics.
Pick your problem, make it relaxed.
Making it relaxed means that pick your cost that's less- pick,
pick your relaxed problem where the cost is less than the cost of the original problem,
and then future cost of that relaxed problem is just going to be a heuristic,
and, and it's going to be consistent.
So proof of that is two lines, so I'm going to skip that.
And, and the cool thing about this like,
like note about this is, there is a trade-off here.
There is a trade-off between efficiency and tightness.
So, sure, like making things relaxed and removing constraints.
It's kinda fun, right? We have this easier problem,
and you just solved it, and everything is great about it.
But it's not like,
like there is kind of a trade-off between how tight you want your heuristic to be.
Like, you shouldn't remove too many constraints,
because if you remove too many constraints,
then your heuristic is not a good estimate of future costs.
Remember, your heuristic is supposed to be an estimate of future costs.
So, so if it is not a good estimate of
future costs and it's not tight, then it's not that great.
So, so there is a balance between how much you are
removing your cons- your constraints and,
and how that makes finding the heuristic easier,
versus the fact that you want your heuristics to be
tight and be close to your future costs,
so, so don't remove everything.
Leave some constraints [LAUGHTER] and then solve it.
Um, and you can also do things,
like if you have two heuristics that are both consistent,
you can take the max of that,
and if you take the max of that, it's,
it's a little bit more restrictive.
Maybe, maybe that is closer to your future costs,
and that is- and then you can actually show the max of that,
is also consistent, okay?
Uh, so we talked about,
uh, like relaxations A-star.
One other quick thing I want to mention because that wasn't very clear last time,
is structured das- perceptron.
We talked about that a little bit too,
and we talked about convergence of that.
So quick things on that.
Structured perceptron actually converges.
There was this question that, uh,
if we have- if that- if, if we have a path,
that is let's say walk,
tram, and, and we end up recovering another path.
That is tram, walk, is that bad, is that good?
Well, turns out that the cost of both of these paths are the same thing.
So if I end up getting this path,
well that's perfectly fine too.
Right? Like that, that is also with the same optimal weight.
In the example that we have shown, in a tram example,
I don't think we are able to get to a path that
looked like this because of the nature of the example.
So, so in general things to remember from structured ce- perceptron is, it does converge.
It does converge in a way that it can recover the two Ys,
but it doesn't necessarily get the exact Ws,
as we saw last time, right?
Like, you might get two and four,
you might get four and eight, like,
as long as you have the same relationships,
that, that is enough but,
but you are going to be able to get the actual Ys, and it does converge.
So with that, um,
project conversation is going to be next time.
Do take a look at,
do take a look at the website.
So all the information on the project is on the website.
So if you have started thinking about it,
look at the project page,
and that has something for you.
 So this lecture is going to be on reinforcement learning.
Um, I will, in the interest of time,
skip the, the quiz.
So, so the way to think about how
reinforcement learning fits into what we've done so far is,
you remember this class has this picture, right?
So we talk about different models and we talk about different algorithms,
inference algorithms to be able to predict using these models and answer queries,
and then we have learning which is,
how do you actually learn these models, right?
So every type of model we go through,
we have to kind of check the boxes for each of these [NOISE] pieces.
So last lecture, we talked about Markov decision processes.
This is a kind of a modeling framework,
allows you to define models.
For example, for crossing volcanoes or playing dice games or tram, taking trams.
Um, what about inference?
So what do we have here last time?
We had value iteration and which allows you to compute the optimal policy and policy,
uh, evaluation which eva- allows you to estimate the value of,
uh, a particular policy.
So these are algorithms that,
um, will operate on MDP, right?
And we sort of looked at these algorithms last time.
So this lecture is gonna be about learning.
Uh, I'll just put RL for now.
RL is not an algorithm, it's a kind of, uh,
refers to the family of algorithms that fits in, uh, this week.
Um, but that's the way you should think about it.
RL allows you to,
um, either explicitly or implicitly estimate MDPs.
And then once you have that,
you can do all these, um, uh,
inference algorithms to, uh,
figure, uh, what the optimal policy is.
Okay? [NOISE] So just to review.
Um, so what is the MDP?
Um, the clearest way- remember to think about it is- it's,
um, in terms of a graph.
So you have a set of states.
So in this dice game,
we have in and end.
So we have a set of states.
From every state, you have a set of actions coming out.
So in this case,
uh, stay and quit.
Um, the actions take you to chance nodes, uh,
where the- uh, you don't get to control what happens,
but nature does and there's randomness.
So out of these chance nodes are transitions.
Each transition takes you into a state,
it has some probability associated with it.
So two-thirds in this case.
It also has some reward associated with it which you pick up along the way.
So naturally, this has to be one-third,
four and remember last time,
this was probability 1:10.
Okay. So, um, and then there is, you know, uh,
the discount factor which Gamma,
which is a number between 0 and 1 tells you how much you value the future.
Uh, for default, you can think about it as 1, uh, for simplicity.
Okay. So this is a Markov decision process.
Um, and what do you do with one of these things?
[NOISE] We, um, have a notion of a policy and a policy,
um, [NOISE] see, I'll write it over here.
So a policy denoted Pi. Uh, let me use green.
Um, so a policy, Pi,
uh, is a mapping from states to action.
It tells you a policy when you apply it,
it says, "When I land here, where should I go?
Should I do stay or quit?"
If I land, well, I mean this is kind of a simple MDP.
Otherwise, there'd usually be more states and for every state,
blue circle will tell you where to go.
Um, and when you run a policy, uh, what happens?
Uh, you get a path,
um, which I'm going to call an episode. So what do you do?
You start in state S_0,
that's- that will be in.
In this particular example, um,
you take an action a_1, let's say stay.
Uh, you get some reward,
in this case it will be 4.
You end up in a new state, um, oops, S_1.
And suppose you go back to end and,
uh, then you take another action,
maybe it's stay, reward is 4 again and,
and so on, right?
So this sequence is a path or in RL speak,
it's, uh, an episode.
Um, let's see.
So let me- let me erase this comment.
Uh, so this is an episode.
Um, and until you hit the end state.
Um, and, uh, what happens out of the episode,
you can look at a utility.
We're gonna denote U which is the discounted sum of rewards along the way, right?
So if you, um, you know,
stayed three times and then went there, you would have, uh,
a utility of 4 plus 4 plus 4 plus 4, so that'll be 16.
Okay? So the last lecture,
we didn't really work with,
um, the episodes into utility,
um, because we were able to define a set of recurrences that,
uh, computed the expected utility.
So, uh, remember that we want to- you know,
we don't know what's going to happen.
So, uh, there's a distribution,
and in order to optimize something,
we have to turn it to a number,
that's what expectation does.
Um, so there's two,
uh, concepts that we had from last time.
One is the value function of a particular policy.
So V_Pi of S is the expected utility if you follow Pi from S. What does that mean?
That means, if you take a particular S, let's take, uh, n,
and I put you there,
and you run the policy,
so stay and you traverse this graph,
um, you will have different utilities coming out and the average of
those is going to be V_Pi of S. Similarly,
there's a Q value,
um, expect the utility,
if you first take an action from a state S and then follow Pi.
So what does that mean? That means if I put you on one of these, uh,
red chance nodes and you basically play out the game,
um, and average the resulting utilities that you get, what number do you get?
Okay? [NOISE] Um, and we saw recurrences that related these two.
So V_Pi of S is, um,
you, recurrence, the name of the game
is to kind of delegate to some kind of simpler problems.
So you first, uh,
look up what you're supposed to do in s, that's
Pi S [NOISE] and that takes you to a chance node which is s,
Pi S of S, and then you say, "Hey,
how much, um, utility am I going to get from that node?"
And similarly from the, the chance nodes,
you have to look at all the possible successors,
the probability of going into that successor, um,
of the immediate reward that you get along the edge plus the discounted, um,
reward of the kind of a future when you end up in, um, S-prime.
Okay. So any questions about this?
This is kind of review of, uh,
Markov decision processes from, um, last time.
Okay. So now we're about to do something different.
Okay. So, um, if you say goodbye to the transition and rewards,
that's called reinforcement learning.
So remember Markov decision processes.
I give you everything here and you just have to find the optimal policy.
And now, I'm gonna make life difficult by not even telling you,
um, what rewards and what are transitions you have to get.
Okay. So just to get a, kind of flavor of what that's like.
Um, let's play a game.
So, um, I'm going to need a volunteer.
I'll, I'll give you the game,
but this volunteer, you have to have a lot of, uh,
grit and, uh, persistence,
because this is not gonna be [NOISE] an easy game.
You have to be one of those people that even though you're losing a lot,
uh, you're still gonna not give up.
Okay. So here's how the game works.
Um, so for each round, r equals,
uh, 1, 2, 3,
4, 5, 6, and so on.
You're just going to choose A or B,
um, red pill or blue pill, I guess.
Um, and you, you move to a new state.
So the state is here and you get some rewards which I'm gonna show here.
Okay. And the state is 5,
0, that's the initial state.
Okay. So everything clear about the rules of the game?
[LAUGHTER] That's reinforcement learning, right?
[LAUGHTER] We don't know anything about how.
Okay. So any volunteers.
Um, how about you in the front? Okay.
Okay.
Okay. Let me, let me fix that. A.
A, A, [LAUGHTER] [NOISE] [LAUGHTER] B, B, A, [LAUGHTER] A.
It's a MDP, so,
uh, in that case that helps.
B, B, B, B,
B, just infinitely click B with an A, I guess.
[LAUGHTER] It's like I'm losing a point every time.
I warned you. [LAUGHTER] Okay.
A, A, A, A, B, A,
A, A, A, A, A. [LAUGHTER]
Okay. [APPLAUSE] I'm glad this worked because last time it took a lot longer [LAUGHTER].
Um, but, you know,
so what did you have to do?
I mean you don't know what to try so you try A and B.
And then hopefully you're building an MDP in your head, right?
Yeah, right? [LAUGHTER] Okay. Just smile and nod.
Um, and you have to figure out how the game works, right?
So maybe you noticed that hey, A is, you know,
decrementing and B isn't going up but then there's this other bit that gets flipped.
So, um, okay you figure this out,
and in the process you're also trying to maximize reward which, uh,
apparently I guess wasn't - doesn't come until
the very end because, um, it's a cruel game.
[LAUGHTER].
Okay. So how do we get an algorithm to kind of do this and
how do we think about, uh, us doing this?
So just to kind of make the contrast between MDPs and reinforcement learning sharper,
so Markov decision process is a offline thing, right?
So you already have a mental model of how the work- world works.
That's the MDP, that's all the rewards and the transitions and the states and actions.
And you have to find a policy to collect maximum rewards.
You have it all in your head,
so you just kind of think really hard about,
you know, what is the best thing.
It's like "Oh, if I do this action then I'll go here" and,
you know, look at the probabilities, take the max of whatever.
So reinforcement learning is very different.
You don't know how the world works.
So you can't just sit there and think because
thinking isn't going to help you figure out how the world works.
Um, so you have to just go out and perform actions in the world, right?
And in doing so you - hopefully you'll learn something but also you'll,
um, you'll get some rewards.
Okay so-so to maybe formalize the,
um, the paradigm of RL.
So you can think about it as an agent.
That's, uh, that's you.
Uh, and do you have the environment,
which is everything else that's not an agent.
The agent takes actions.
So that sends action to the environment and the
environment just send you back rewards and a new state.
And you keep on doing this.
Um, so what you have to do is figure out first of all how to - am I going to act.
If I'm in a particular state S_t minus 1,
what actions should I choose, okay?
So that's one, um, one question.
And then you're gonna get this reward and observe a new state.
How -what, what should I do to update my mental model of the world, okay?
So these are the main two questions.
I'm going to talk first about how to update the parameters and then
later in the lecture I'm going to come back to
how do you actually go and, you know, explore it.
Okay. So I'm not going to say much here but, you know,
in the context of volcano crossing, um,
just to kind of think through things,
every time you play the game, right?
You're gonna get some utility.
So you take -so this is the episode over here.
So a r s,
you're gonna -sometimes you fall into a pit.
Sometimes you go to a hut.
Um, and based on these experiences, um,
if I didn't -hadn't told you what any of
the actions do and what's a slip probability or anything,
how would you kind of go about,
um, kinda solving this problem?
That's a -that's a question.
Okay so there's a bunch of algorithms.
I think there's gonna be 1, 2, 3, 4.
At least four algorithms that we're going to talk about with different characteristics.
But they're all going to kind of build onto each other in some way.
So first class of algorithms is Monte Carlo methods, right?
So, um, okay.
So whenever you're doing RL or any sort of learning,
uh, the first thing you get is you just have data.
Let's, let's suppose that you run even a random policy,
you're just gonna -because in the beginning you don't know any better,
so you're just going to try random actions and, uh,
but in the process you're gonna see "Hey,
I tried this action and it led to this reward and so on".
So in a concrete example just to make,
uh, things a little bit more crisp,
it's gonna look something like in, uh,
and then you take,
uh, you know you did, um, let's see.
Let me try to color coordinate this a little bit.
Um, so you're in n,
you do, um, stay.
And then you get a reward of 4 and then you're back in n,
you do a stay,
and then you get 4 and then maybe you're done, you're out.
Okay. So this is an example episode just to make things concrete.
So this is s_0,
a_1, r_1, s_2, s_1.
I keep on incrementing too quickly.
Um, a_2, r_2, s_3, okay?
Okay so what should you do here?
Alright so, um, any ideas? Model-based Monte Carlo.
So if you have MDP you would be done.
But we don't have MDP, we have data.
So what can we do?
[NOISE] Yeah.
[inaudible].
Yeah. Let's try to build a MDP from that data.
Okay. So, um, the key idea is estimate the MDP.
Um, so intuitively,
we just need to figure out what
the transitions and rewards are and then we're done, right?
Um, so how do you do the transitions?
Um, so the transition says if I'm in state S and I take action A, what will happen?
I don't know what will happen, but let's see in the data what will happen.
So I can look at the number of times I went into a
particular S prime and then divide it over
the number of times I attempted any- this action from
that state at all and just take the ratio, okay?
And for the rewards, um,
this is actually fairly, you know, easy,
when I - because when I observe a reward,
um, from S, A and S prime.
I just write it down and say that's the reward, okay?
Okay. So on the concrete example what does this look like?
So remember now, here's the MDP graph.
I don't know what the -the, uh,
transition distribution or the rewards are.
Um, so let's suppose I get this trajectory.
What should I do? So I get stay, stay,
stay, stay, and I'm out, okay?
So first I, I can write down the rewards of 4 here,
and then I can, um,
estimate the probability of, you know, transitioning.
So three out of four times I went back to in.
One out of four times I went to end.
So I'm gonna estimate as three-fourths, one-fourths.
Okay. But then suppose I get a new data point.
So I have stay, stay,
end. So what do I do?
I can add to these counts, um.
So everything is kind of cumulative.
So two more times,
I'm sorry one more time I went into in and another time I went to end,
so this becomes four out of six, three out of six.
And suppose I see another time when I just go into end,
so I'm just going to increment, uh,
this counter and now it's three out of seven and four out of seven, okay?
So pretty, um, pretty simple.
Okay so for reasons I'm not going to get into,
this process actually, you know,
converges to the -if you do this kind of, uh, you know,
a million times, you'll get pretty,
um, accurate. Yeah, question?
Yes, the question is, you don't know the rewards or the transitions, uh,
but yes you do know the set of,
ah, states and the actions.
Set of states, I guess, you don't have to know them all in advance,
but you just observe them as they come.
The actions, you need to know because you-
you are an agent and you need to play the game.
Yeah, good question. Okay. So, yeah.
Does this work with variable costs?
Like, there is a probabilit- or variable reward around it.
There's a probability you get some rewards for probability [inaudible].
Yeah. So the question is,
does this work with variable, uh, rewards.
Um, and if the reward is not a function of, um,
sas prime, you would just take the average of the rewards that you see.
Yeah. Okay. So- so what do you do with this?
So after you estimate the MDP,
so all you need is the transitions and rewards.
Um, then now we have MDP.
It might- it may not be the exact right MDP
because this is estimated from data so it's not gonna match it exactly,
um, but nonetheless, we already have these tools from last time.
You can do value iteration to compute, um,
the optimal policy on it and then you just,
you know, you're done, you run it.
On- in practice, you would probably kind of interleave
the learning and the- the optimization but, uh,
for simplicity we can think about it as a two-stage where you gather a bunch of data,
you estimate the MDP and then you are off.
Okay. There's one problem here.
Does anyone know what the problem might be?
You can actually see it by looking on the slide. Yeah.
Well, with your based policy of all this thing,
you'll never explore the quick branch of the world.
Yeah, yeah. You didn't explore this at all,
so you actually don't know how much reward is here.
Maybe it's like, uh, you know, 100, right?
So- so this is this problem,
this kind of actually a pretty big problem that unless you have a policy that,
uh, actually goes and covers all the- the states,
you just won't know, right?
And this is kind of natural because there can always be, you know,
a lot of reward hiding under a kind of
one state but unless you see it you- you don't- you just don't know.
Um, okay.
So this is a kind of key idea,
key challenge I would say,
in reinforcement learning is exploration.
So you need to be able to explore,
um, the state space.
This is different from normal machine learning where data just
comes in passively and you learn on your nice function and then you're- you're done.
Here, you actually have to figure out how to get the data,
and that's- that's kind of one of the,
the key challenges of RL.
So we're gonna go back to this- this problem,
and I'm not really gonna, uh,
try to solve it now.
Um, for now you can just think about Pi as
a random policy because a random policy eventually will just,
you know, hit everything for, you know,
finite, uh, small, uh, state spaces.
Okay. So, um, okay.
So that's basically end of the first algorithm.
Let me just write this over here.
So algorithms, we have model-based, um, Monte Carlo.
And the model-based is referring to the fact that we're
estimating a model the- in particular the MDP.
The Monte Carlo part is just referring to the fact that we're using samples, uh,
to estimate, um, a model or you're
basically applying a policy multiple times and then estimating,
uh, the model based on averages.
Okay. So- so now,
I'm going to present a- a different algorithm and it's called,
uh, model-free Monte Carlo.
And you might from the name
guess what we might want to do is maybe we don't have to estimate this model, okay?
And why- why is that?
Well, what do we do with this model?
Um, what we did was we, you know, uh,
presumably use value iteration to,
um, you know, compute the optimal policy.
And the- remember this, uh, recurrence, um,
for computing Q_opt, um,
it's in terms of T and reward,
but at the end of the day all you need is Q_opt.
If I told you, um,
Q_opt (s, a) which is,
um, what is Q_opt (s, a)?
It's the, um, the maximum possible utility I could get if I'm in,
chance node sa and I follow the optimal policy.
So clearly if I knew that,
then I would just produce the optimal policy and I'd be done,
I don't even need to know- understand the- the rewards and transitions.
Okay. So with that, uh,
insight is model-free learning,
which is that we're just going to try to estimate Q_opt,
um, you know, directly.
Um, sometimes it can be a little bit confusing what is meant by model-free.
So Q_opt itself you can think about as a- as a model,
but in the context of MDPs in reinforcement learning,
generally people when they say model-free refers to the fact that there's no MDP model,
not that there is no, um, model in general.
Okay. So, um, so we're not gonna get to Q_opt, uh, yet.
Um, that will come later in the lecture.
So let's warm up a little bit.
Um, so here's our data staring at us.
Um, remember- let's, let's look at a related quantity, so Q Pi.
Remember what Q Pi is.
Q Pi (s, a) is an expected utility if we start at
s and you first take action a and then follow policy Pi, right?
So in, um, in- I guess another way to write this is,
um, if you are at a particular, uh,
time step t, you can define u_t as
the- the discounted sum of the rewards from that point on, which is, you know,
the reward immediately that you will get plus
the discounted part in the non- next time step plus,
you know, a square discounted and then,
uh, two time steps in the future and so on.
And, um, what you can do is you can try to estimate Q Pi from this utility.
Right? So this is the utility,
uh, that you get out to predict your time steps.
So suppose you do the following.
So suppose you average the utilities that you get only on
the time steps where I was in a particular state s and I took an action a.
Okay. So you have a- let's suppose you have a bunch of episodes, right?
So, um, here pictorially, um,
uh, let's see.
[NOISE]
Here's another way to think about it.
So I get a bunch of episodes.
I'm gonna do- do some abstract, um, drawing here.
Um, so every time you have you know,
s, a shows up here,
maybe it shows up here,
maybe it shows up here,
maybe it shows up here,
you're going to look at how much reward do I get from that point on?
How much reward do I get from here on?
How much reward do I get from here on?
And, um, average them, right?
So there's a kind of, a technicality
which is that if s, a appears here and it also appears,
uh, after it then I'm not going to count that because I'm
kind of- if I do both I'm kind of double counting.
Um, in fact it works both ways, but just,
conceptually it's easier to think about just taking of, uh, an s, a, uh,
of the same you don't kind of go back to the same position.
Okay, so let's do that on a concrete example.
So Q-pi, let's just write it.
Q-pi s, a is a thing where we're trying to estimate and this is,
uh, a value associated with every chance node s, a.
So in particular, I've drawn it here.
I need a value here and,
uh, a value here.
Okay? So suppose I get some data,
I stay and then I got- go to the end.
Uh, so what's my utility here? It's not a trick question.
4.
4, yes. Um, sum of 4 is 4.
Okay, so now I can say, "Okay it's 4."
And that's my best guess so far.
I mean, I haven't seen anything else, maybe it's 4.
Um, so what happens if I play the game again and I get 4, 4?
So what's the utility here?
8.
8? So then I update this to the average of 4 and 8,
do it again, I get 16 then I average,
uh, in the 16.
Okay? And, um, and again, you know,
I'm using stays so I don't learn anything about this, in practice you would
actually go explore this and figure out how much utility you're seeing there.
So in particular, notice I'm not updating
the rewards nor the transitions because I'm model-free,
I just care about the Q values that I get which
are the values that sit at the nodes not on the edges.
Okay, so one caveat is that we are estimating Q-pi not Q-opt.
We'll revisit this, um, later.
Um, and another, uh,
thing to kind of note is the difference between what is called On-policy and Off-policy.
Okay? So in reinforcement learning,
you're always following some policy to get around the world right?
Um, and that's generally called the exploration-policy or the control policy um,
and then there's usually some other thing that you're trying to estimate,
usually the- the value of
a particular policy and that policy could be the same or it could be different.
So On-policy means that, uh,
we're estimating the value of
the policy that we're following, the data-generating policy.
Off-policy means that we're not.
Okay? So um, so in particular is,
uh, model-free Monte Carlo, um, On-policy or Off-policy?
It's On-policy because I'm estimating Q-pi not Q-opt.
Okay? That's On-policy.
Um, and Off-policy ,
uh, what about model-based Monte Carlo?
[NOISE] I mean it's a little bit of a slightly weird question,
but in model-based Monte Carlo,
we're following some policy,
maybe even a random policy,
but we're estimating the transition then rewards,
and from that we can compute the- the optimal policy.
So you can- you can think about is, um,
Off-policy but, you know,
that's maybe not, uh, completely standard.
Okay. So any questions about what model-free Monte Carlo is doing?
So let me just actually write.
So what is model-based Monte Carlo is doing,
it's trying to estimate the, uh,
the transition and rewards and model-free Monte Carlo is trying to estimate,
uh, the, um, Q-pi.
Um, okay? And just as- as a note,
I put Hats on, uh,
any letter that is supposed to be a quantity
that is estimated from data and that's what, you know,
I guess statisticians do, um, to differentiate them between whenever I Q-pi,
that's the true, uh,
value of that, you know,
policy which, you know, I don't have.
Okay, any questions about model-free Monte Carlo?
Both of these algorithms are pretty simple, right?
You just, you know,
you look at the data and you take averages. Yeah.
So model free is not trying to optimize [inaudible] policy.
So the question is is model-free,
uh, making changes to a policy or is it a fixed policy?
So- so this version I've given you is only for a fixed policy.
The general idea of model-free as we'll see later,
uh, you can also optimize the policy.
Okay. So- so now what we're gonna do is we're gonna,
uh, do theme and variations on, uh, model-free Monte Carlo.
Actually where it's going to be the same algorithm but I just wanted to interpret
it in kind of slightly different ways that'll help us,
um, generalize it in the future. Yeah.
Are there certain problems where model-free does better than model base?
Are there certain problems where model-free is better than model base?
So this is actually a really interesting question, right?
So, um, you can show that if your model is correct,
if your model of the world is correct,
model-based is kind of the way to go because there'll be more sample efficient,
meaning that you need fewer, uh, data points.
But it's really hard to get the model correct in the real world.
So recently, especially with,
you know, deep reinforcement learning,
people have gone a lot of mileage by just going model-free because then, um,
jumping ahead a little bit, you can model this as
a kind of a deep neural network and that gives
you extraordinary flexibility and power without having to solve the hard problem of,
you know, constructing the MDP.
Okay. So- so there's kind of three ways you can think about this.
So the first, we already talked about it,
is, you know, this average idea.
So we're just looking at the utilities that you
see whenever you encounter an s and a, and you just average them.
Okay. So here is an equivalent formulation.
Um, and the way it works is that for every,
um, s, a, u that you see,
so every time you see a particular s,
a, u, s, a, u, s, a,
u and so on,
I am going to perform the following update on.
So I'm gonna take my existing value and I'm
going to do a- what- what we call a convex combination.
So, you know, 1 minus eta and eta sum to 1.
So it's, you know, a kind of balancing between two things.
Balancing between the old value that I had and the- the new utility that I saw.
Okay? And the eta is set to be 1 over 1 plus the number of updates.
Okay? So let me do a concrete example.
I think you'll make this very clear what's- what's going on.
So suppose my data looks like this.
So I get, uh, 4,
um, and then a 1 and a 1.
Um, so these are the utilities, right?
That's- that's a U here.
I'm ignoring the s and a,
I'm just assume that there are some- something.
Okay, so first, uh,
let's assume that Q-pi is 0, okay?
So the first time I do, um, uh,
let's see, number of updates,
I haven't done anything so it's 1, um, 1 minus 0.
So 0 times 0 plus 1 times 4 which is the first view that comes in.
Um, okay, so this is 4, okay?
So then what about the next data point that comes in?
So I'm gonna to take, um, one-half now times 4 plus one-half times 1,
which is the new value that comes in.
And that is, I'm gonna to write it as 4 plus 1 over 2, okay?
So now- okay just to keep track of things,
this results in this,
this results in this,
and then now, um,
I'm running out of space but hopefully we can- so now on the third one,
I do, um, uh, two-thirds,
so I have 4 plus 1 over 2 times two-thirds plus,
um, actually I- I guess I should do two-thirds to be consistent.
Two-thirds times 4 plus 1 over 2 which is
the previous value that's sitting in Q-pi plus one-third times 1,
which is a new value,
and that gives me,
um, 4 plus 1 plus 1 over 3, right?
So you can see what's going on here is that, you know,
each, uh, each time I have this, you know,
sum over all the tools I've seen over the number of times it
occurs and this eta is set so that next time I kind
of cancel out the old uh, count and I add
the new count to the denominator and it kind of all works out so that at
every time-step what actually is in
Q-pi is just a plain average over all of the numbers I've seen before.
All right, this is just kind of an algebraic trick to, um,
get this original formulation,
which is a notion of average,
into this formulation which is a notion of, um,
kind of you're trying to, um,
take a little bit of the old thing and add a little bit of a new thing.
Okay. So [NOISE], um,
I guess I'm going to call this, uh,
I guess, um, combination I guess.
So the- that's the second interpretation.
There's a third interpretation here which,
uh, you can think about is,
uh, in terms of stochastic gradient descent.
So this is actually a kind of a,
uh, simple algebraic manipulation.
So if you look at this expression, what is this?
So you have 1 times Q Pi,
so I'm gonna pull it out and put it down here
and then I'm gonna have minus eta times Q Pi,
that's this thing and then I also have a eta, a u,
so I'm going to put kind of minus a- u here and this is,
uh, inside this parenthesis.
So if you just, you know,
do the algebra you can see that these two,
you know, are equivalent.
Uh, so what's the point of this?
Right, so, um, where have you kind of seen this, uh,
before, something like, maybe not,
not this exact expression but something like that [NOISE].
Any ideas? Yeah, when you look down at a stochastic gradient descent in the context of,
uh, the square loss for linear regression.
Right, so remember, uh,
we had these updates that
all looked like kind of prediction minus target which was,
you know, the residual and that was used to kind of update.
So one way to interpret this is, uh,
this is kind of implicitly trying to do
stochastic gradient descent on the objective which is a squared,
uh, loss on, uh,
the, the Q Pi value that you, you,
you're trying to set and,
uh, u which is the new piece of data that you got.
So think about in regression this is the y,
this is, uh, y, you know,
the- what the output is and you- this is
the model that's trying to predict it and you want those to be close to each other.
Okay? So, so those are kind of three views on basically,
uh, this idea of averaging or incremental updates.
Okay. So it'll become clear why, you know, I,
I did this isn't just to, you know,
have fun. Uh, okay.
So now let's, uh, see an example of model- free Monte Carlo in action on this,
ah, the volcano games.
So remember here we have this, uh, you know,
volcanic example and, uh,
I'm going to, uh,
set the number of episodes to let's say 1,000, let's see what happens.
Uh, so here, okay.
So what does this kind of, uh, uh,
grid-like structure, a grid of triangles denote?
So this remember is a state,
this is 2, 1.
So what I am doing here is dividing
into four pieces which correspond to the four different action,
so this triangle is 2, 1 north,
this triangle is 2, 1 east and so on.
Okay. And a number here is the Q Pi or value that I'm estimating along the way.
Okay, so the, the policy I'm using, uh,
is a complete random,
uh, just move randomly, uh,
and I run this 1,000 times and we see that the average utility is,
uh you know, minus 18 which is, uh, obviously not great.
Okay. Uh, but this is an estimate of how well the random policy is doing.
So, you know, as advertised, you know,
random policy you would expect to fall into a volcano quite often.
Uh, okay.
Uh, and you can run this and sometimes you get slightly different results but,
you know, it's pretty much stable around minus 19, minus 18.
Okay. Any questions about this before we move on to, uh, different algorithms?
Okay. So model-based Monte Carlo we're estimating the MDP,
model-free Monte Carlo we're just estimating the Q values of a particular policy for now.
Okay. So, so let's revisit what model-free Monte Carlo is doing.
So if you use the policy Pi equals stay for the dice game,
um, you know, you might get a bunch of different,
uh, trajectories that come out.
These are possible episodes and in each episode you have a utility,
you know, associated with it.
Uh, and what model free Monte Carlo is doing is it's using these utilities,
uh, to kind of update,
uh, towards, uh, update u Q Pi.
Right, so in particular like for example this you're saying, okay, I'm in,
I'm in, uh, the in-state and I,
you know, take an action and stay,
when you're- what will happen?
Well, in this case I got, you know,
16 and, uh, this case I've got 12.
And notice that there's quite a bit of variance.
So on average, this actually does the right thing.
Right? So, um, just by definition,
this is our unbiased, you know,
estimate, if you do this a million times and
average you're just going to get the right value which is,
uh, 12 in this case.
But the variance is here, so if you,
for example if you only do this a few times,
you're not going to get 12, you might get something, you know, sort of related.
Uh, so how can we kind of counteract,
uh, this, this variance?
So the key idea, uh,
behind what we're going to call bootstrapping is,
is that, you know,
we actually have, you know,
some more information here.
So we have this Q Pi that we're estimating along the way.
Right? So, so this view is saying, okay,
we're trying to estimate Q Pi, um,
and then we're going to try to basically regress it against, you know,
this data that we're seeing but, you know,
can we actually use Q Pi itself to, uh,
help, you know, reduce the variance?
So, so the idea here is, uh, um,
I'm going to look at all the cases where,
you know, I started in and I take stay,
I get a 4.
Okay? So I'm going to say,
I get a 4 but then after that point I'm actually just going to substitute this 11 in.
Okay? This is kind of weird, right,
because normally I would just see,
okay, what would happen?
But what happens is kind of random.
On average it's going to be right but, you know,
on any given case,
I'm gonna get, like, you know, 24 or something.
And the, the hope here is that by using
my current estimate which isn't going to be right because if I were,
if it were right I would be done but hopefully it's kind of somewhat
right and that will, you know, be, you know,
better than using the, the kind of the raw,
rollout value. Yeah, question.
You, you would update your current estimate at the end of each episode, correct?
Uh, yeah. So the question is,
would you update the current estimate,
um, after each episode?
Yeah. So all of these algorithms,
I haven't been explicit about it, is that you've seen an episode, you update,
uh, after you see it and then you get a new episode and so on.
Yeah. Sometimes you would even update before you're done with the episode, uh.
[NOISE] Okay.
So, uh, let me show this, uh,
what, um, this algorithm.
So this is a new algorithm, it's called SARSA.
Does anyone know why it's called SARSA?
[inaudible].
Oh, yeah, right. So if you look at this,
it's spelled SARSA and that's literally the reason why it's called SARSA.
Uh, so what does this algorithm say?
So you're in a state s, you took action a,
you got a reward,
and then you ended up in state s prime and then you took another action a prime.
So for every kind of quintuple that you see,
you're going to perform this update.
Okay, so what is this update doing?
So this is the convex combination, uh,
remember that we saw from before, um,
where you take a part of the old value and then you,
uh, try to merge them with the new value.
So what is the new value here?
This is looking at just the immediate reward,
not the full utility,
just the immediate reward which is this 4 here
and you're adding the discount which is 1 for now,
um, of your estimate.
And remember, what is the estimate trying to do?
Estimate is trying to be the expectation of rewards that you will get in the future.
So if this were actually a q pi and not a q pi hat,
then this will actually just be strictly better because that would be,
uh, just reducing the variance.
Uh, but, you know,
of course this is not exactly right,
there's bias so it's 11, not 12 but the hope is that,
you know, this is not biased by, you know, too much.
Okay? So these would be the kind of the,
the values that you will be updating rather than these kind of raw values here.
Okay. So just to kind of compare them, well, okay.
Okay, any questions about what SARSA is doing before we move on?
So maybe I'll write something to try to be helpful here.
So Q pi model-free Monte Carlo estimates Q pi based on u,
and SARSA is still Q pi hat,
but it's based on reward plus,
uh, essentially Q pi hat.
I mean this is not like a valid expression,
but hopefully, it's some symbols that will evoke, uh,
the right memories, um, okay?
So let's discuss, um, the differences.
So this is- this- whenever people say, kind of, bootstrapping,
um, in the context of reinforcement learning,
this is kinda what they mean,
is that instead of using u as its prediction target,
you're using r plus Q pi,
and this is kind of you're pulling up yourself from
your bootstraps because you're trying to estimate q pi,
but you don't know q pi, but you're using Q pi to estimate it.
Okay. So u is based on one path,
um, er, in SARSA,
you're based on the estimate which is based on all your previous kind of experiences, um,
which means that this is unbiased,
uh, model for your Monte Carlo is biased,
but SARSA is biased.
Monte Carlo has large variance.
SARSA has, you know, smaller variance.
Um, and one, I guess, uh,
consequence of the way the algorithm is set up is that model-free Monte Carlo,
you have to kind of roll out the entire game.
Basically, play the game or the MDP until you reach the terminal state,
and then you can- now you have your u to update, whereas, uh,
SARSA when- or any sort of bootstrapping algorithm,
you can just immediately update because all you need to do is you need to see,
this is like a very local window of S-A-R-S-A,
and then you can just update, and that can happen,
kind of, you know, anywhere.
You don't have to wait until the very end to get the value.
Okay. So just as a quick sanity check.
Um, which of the following algorithms allows you to estimate Q opt,
so model-based Monte Carlo,
model-free Monte Carlo, or SARSA?
Okay. So I'll give you maybe ten seconds to ponder this.
[NOISE] Okay?
How many of you more- need more time?
Okay. Let's, uh, get a report.
I think I didn't reset it from last year,
so this includes last year's, uh, participants.
Um, so model-based Monte Carlo,
uh, allows you to get Q opt, right?
Because once you have the MDP,
you can get whatever you want. You can get Q opt.
Model-free Monte Carlo, um, estimates Q Pi;
it doesn't estimate Q opt and, um,
SARSA also estimates Q Pi,
but it doesn't estimate Q opt, okay?
All right. So, so that's,
uh, kind of a problem.
I mean, these algorithms are fine for, uh,
estimating the value of a policy,
um, but you really want the optimal policy, right?
In fact, these can be used to improve the policy as well because you can,
um, do something called policy improvement,
which I didn't talk about.
Once you have the Q values,
you can define a new policy based on the Q values.
Um, but there's actually a kind of a more direct way to do this, okay?
So, so here's the kind of the way mental framework you should have in your head.
So there's two values: Q Pi and Q opt.
So in MDPs, we saw that policy evaluation allows you to get Q Pi;
value iteration get- allows you to get Q opt.
And now, we're doing reinforcement learning,
and we saw model-free Monte Carlo and SARSA allow you to get Q Pi.
And now we need,
I'm going to show you a new algorithm called Q-learning,
that allows you to get Q opt.
So this gives you Q opt,
and it's based on reward, uh,
plus, uh, Q opt, kind of.
Okay. So this is going to be very similar to SARSA,
and it's only going to differ by,
essentially, as you might guess,
the same difference between policy evaluation and value iteration.
Okay. So it's helpful to go back to kind of the MDP recurrences.
So even though MDP recurrences can only apply when you know the MDP.
For deriving reinforcement learning algorithms, um,
it's- they can kind of give you inspiration for the actual algorithm.
Okay. So remember Q opt,
what is a Q opt?
Q opt is considering all possible successors of probability immediate reward plus,
uh, future, um, returns.
Okay. So the Q-learning is,
it's actually a really kind of clever idea, um,
and it's- it could also be called SARS, SARS, I guess, um,
but maybe you don't want to call it that,
and what it does is as follows.
So this has the same form,
the convex combination of the old,
uh, value, uh, and the new value, right?
So what is the new value?
Um, so if you look at Q opt,
Q opt is looking at different successors reward plus V opt.
What we're gonna do is, well,
we don't have all- we're not gonna be able to sum over
all our successors because we're in our reinforcement learning setting,
and we only saw one particular successor.
So let's just use that as a successor.
So on that successor,
we're going to get the reward.
So R is a stand-in for the actual reward of, I mean,
is the stand-in for the reward, the reward function,
and then you have Gamma times.
And then V opt,
I am going to replace it with, uh, the,
our estimate of what V opt is,
and what should the estimate of V opt be?
So what relates V opt to Q opt? Yeah?
I think the a that maximizes Q opt but [inaudible] V opt.
Yeah. Exactly. So if you,
define V opt to be the max over
all possible actions of Q opt of s in that particular action,
then this is V opt, right?
So Q is saying,
I'm at a chance node, um,
how much, what is the optimal utility I can get provided I took an action?
Clearly, the best thing to do if you're at
a state is just choose the action that gives you
the maximum of Q value that you get into, okay?
So that's just Q-learning,
so let's put it side-by-side with SARSA.
Okay. So SARSA, these two are very similar, right?
So SARSA, remember updates against r plus Q Pi?
And now we're updating against r plus this max over Q opt, okay?
And you can see that SARSA requires knowing what action I'm gonna take next,
um, kind of a one-step look ahead,
a prime and that plugs i- into here, whereas Q-learning,
it doesn't matter what a you took because I'm
just gonna take the one that maximizes, right?
So you can see why SARSA is estimating the value of policy because, you know,
what a prime, uh,
shows up here is a function of a policy.
And here, um, I'm kind of
insulated from that because I'm just taking the maximum over all actions.
This is the same intuition as for value iteration versus policy evaluation, okay?
I'll pause here. Any questions?
Q-learning versus SARSA.
So is Q-learning on-policy or off-policy?
It's off-policy because I'm following whatever policy I'm following,
and I get to estimate the value of
the optimal policy which is probably not
the one I'm following, at least, in the beginning.
Okay. So let's look at the example here.
So here's SARSA and run it for 1,000 iterations.
And like model-free Monte Carlo, um, this, um,
I'm estimated that an average- the average utility I'm getting is minus 20,
and in particular, the values I'm getting are all very negative because this is Q Pi.
This is a policy I'm following,
which is the random policy.
Um, if I replace this with q, what happens?
So first, notice that the average utility
is still minus 19 because I actually haven't changed my exploration policy.
I'm still doing random exploration.
Um, well, yeah.
I'm still doing random exploration.
But notice that the value,
the Q opt values are all around,
you know, 20, right?
And this is because the optimum policy, remember,
is just to- and this is,
uh, slip probability is 0.
So optimal policy is just to go down here and get your 20, okay?
And Q- and I- I guess it's kind of interesting that Q-learning,
I'm just blindly following the policy running, you know, off,
off the cliff into the volcano all the time but,
you know, I'm learning something,
and I'm learning how to behave optimally,
even though I'm not behaving optimally,
and that's, uh, the kind of hallmark of off-policy learning.
Okay. So, any questions about these four algorithms?
So model-based Monte Carlo, estimate MDP,
model-free Monte Carlo, um, estimate, ah,
the Q value of this policy based on, um,
the actual returns that you get,
the actual sum of the, ah, rewards.
SARSA is bootstrapping estimating the same thing but with kind of a one-step look ahead.
And Q learning is like SARSA except for I'm
estimating the optimal instead of,
um, fixed policy Pi. Yeah.
Is SARSA on-policy or off policy?
SARSA is on-policy because I'm estimating Q Pi. All right.
Okay so now let's talk about encountering the unknown.
So these are the algorithms.
So at this point if I just hand you some data, um,
if I told you here's a fixed policy,
here's some data, you can actually estimate all these quantities.
Um, but now there's a question of exploration which we saw was really important,
because if you don't even,
even see all the states,
how can you possibly act optimally?
So, um, so which exploration policy should you use?
So here are kind of two extremes.
So the first extreme is, um,
let's just set the exploration policy.
So, so imagine we're doing Q learning now.
So you have this Q_opt estimate.
So it's not a true Q_opt but you have an estimate of Q_opt.
Um, the naive thing to do is just take a- use that Q_opt,
figure out which action is best and just always do that action.
Okay. So what happens when you do this is,
um, you, ah, don't do very well.
So why don't you do very well?
Because initially while you explore randomly and soon you find the 2.
And once you've found that 2, you say, "Ah,
well, 2 is better than 0, 0, 0.
So I'm just gonna keep on going down to the 2 which is you know,
all exploitation, no exploration.
Right? You don't realize you that there's all this other stuff over here.
Um, so in the other direction,
we have no exploitation, all exploration.
Um, here, ah, you kind of have the opposite setup where I'm,
I'm running Q learning, right?
So as we saw before,
I'm actually able to estimate the,
uh, the, the Q_opt values.
So I learn a lot.
But the average utility which is the actual utility
I'm getting by playing this game is pretty bad.
In particular, it's the,
the utility you get from just,
you know, moving randomly.
So kinda what you really want to do is, uh,
balance you know, exploration and exploitation.
So just kind of a, kind of an aside or
a commentary is that I really feel reinforcement learning kind of captures,
ah, life pretty well.
Um, uh, because in life there's, you know,
you don't know what's going on.
Um, you want to get rewards,
you know, you want to do well.
Um, and, ah, but at the same,
time you have to, um,
kind of learn about how the world works so that you can kind of improve your policy.
So if you think about going to in restaurants or
finding the shortest path better way to get to, um,
to school or to work, or in research even when you are trying to figure out, um,
a problem you can work on the thing that you know how to do
and will definitely work or, you know,
do you try to do something new in hopes of you
learning something but maybe it won't get you as high reward.
So, um, hopefully reinforcement learning is,
um, I know, it's kind of a metaphor for life in the US.
Um, okay so, ah,
back to concrete stuff.
Um, so here's one way you can balance,
um, exploration and exploitation, right?
So it's called the Epsilon-greedy policy.
And this assumes that you're doing something like Q learning.
So you have these Q_opt values and ideas that, you know,
with probability of 1 minus Epsilon where Epsilon is,
you know, let's say like 0.1, you're usually gonna give exploit.
We're just gonna do,
give you- give it all you have.
Um, and then, um,
once in a while, you're also gonna do something random.
Okay. So this is actually not a bad policy to act in life.
So once in a while, maybe you should just do
something random and kind of see what happens.
Um, so if you do this,
um, what, what do you get?
Okay, so what I've done here is, uh,
I've set Epsilon to be starting with one.
So one is, ah, all exploration.
And then I'm going to change the value,
ah, a third of the way into 0.5.
And then I'm gonna, two-thirds the way I'm gonna change it to 0.
Okay. So if I do this then I actually estimate the values,
ah, really really well.
Um, and also I get utility which is, you know,
pretty good, you know 32.
Um, okay.
And this is also kind of something that happens, uh,
as you get older,
you tend to, um,
[NOISE] explore less and exploit more.
Um, it just happens.
Um, okay. All right.
So that was exploration.
So let's put some stuff on the board here.
Um, do I need this anymore?
Maybe [NOISE].
Okay. Um, okay.
So covering the unknown,
so we talked about, you know,
exploration, um, you know, Epsilon-greedy.
Um, and there's other ways to do this.
Um, Epsilon-greedy is just kind of the simplest thing that actually, you know,
works remarkably, you know, well,
um, even in the stabilized systems.
So the other problem now I'm gonna talk about is, you know, generalization.
Uh, so remember when we say exploration.
Well, if you don't see a particular state,
then you don't know what to do with this.
I mean you think about it for a moment, that's kind of unreasonable because,
you know, in life you're never gonna be in the exact same, you know, situation.
And yet we are [NOISE] we need to be able to act properly right.
So general problem is that a state-space that you,
you might deal with in a kind of a real,
ah, world situation is enormous.
And there's no way you're going to go and track down every possible state.
Okay. So this state space is actually not that enormous, um,
but this is the biggest state space I could draw on the- on the screen.
Um, and you can see that this, you know,
the average utility is, you know, pretty bad here.
Okay. So what can we do about this?
So, um, I guess let's talk about a large state space.
So this is the problem.
So now this is where
the second- the third interpretation of model-free Monte Carlo will come in handy.
So let's take a look at Q learning.
Okay. So in the context of,
ah, SGD, looks like this.
Right. So it's a kind of a gradient step where you take the old value
and you minus eta and something that kind of looks like,
ah, it could be a gradient,
which is the residual here.
Um, so one thing to note is that under the,
the kind of formulations of Q learning that I've talked about so far,
this is what we call a kind of rote learning.
Right. Um, which if we were,
you know, two weeks ago, we already said this is,
you know, kind of ridiculous because it's, uh,
not really learning or generalizing at all.
Um, right now it's basically for every single state and action I have a value.
If I have a different state and action, completely different value.
I don't- I don't- there's no kind of,
ah, sharing of information.
And naturally, if I do that,
I can't generalize between states and actions. Um, okay.
So here's the key idea that will allow us to,
um, actually overcome this.
So it's called function approximation in the context of reinforcement learning.
Uh, in normal machine learning,
it's just called normal machine learning.
Um, so the way it works is this,
uh, so we're going to define this Q_opt s, a.
It's not going to be a lookup table,
it's going to depend on some parameters here w. And I'm gonna define
this function to be w dot Phi s, a.
Okay. So I'm gonna define this feature vector very similar to how we did it
in kind of machine- in the machine learning section except for instead of s, a we had x.
And now the weights are going to be kind of, you know, the same.
Okay. So what kind of features might you have?
Ah, you might have for example,
um, features on, you know, actions.
So these are indicator features that say, "Hey,
maybe it's better to go east then to go west or maybe it's better to be in the fifth,
ah, row or as it's good to be in a six column and, you know, things like that."
So, um, you have a smaller set of features and you try to
use that to kind of generalize across all the different states that you might see.
So what this looks like is now with the features is
actually the same as before except for,
um, now we have something that really looks like,
uh, you know, the machine learning lectures,
is that you take your weight vector and you do, um,
an update of the residual times the feature vector.
Okay. So how many of you this looks familiar from linear regression?
Okay. All right.
So, so just to contrast,
so before we were just updating the Q_opt values,
um, but the residual is exactly the same and there's nothing over here.
And now what we're doing is we're updating not the Q values,
we're updating the weights.
The residual is the same and the thing that connects the, the,
the Q values with the,
the residual width, the,
the weights is, ah, the kind of the feature vector.
Okay. As a sanity check,
this has the same dimension.
This is a vector. This is a scalar.
This is a vector which has the same dimensionality s, a; w. Okay.
And if you want to derive this, um,
you can actually think about the implied objective function as,
ah, simply, you know, linear regression.
You have a model that's trying to predict a value,
um, from an input, um, s, a.
So s, a is like x and Q_opt is like kind of y.
And then your regre- sorry.
This target is like, uh,
the y that you're trying to predict and you're just trying to make this prediction close
to the target. Yeah, question.
Is the eta, you said that [inaudible] [NOISE]
Yeah. So a good question.
So what is this eta now?
Uh, is it the same as before?
So when we first started talking about these algorithms,
eta was supposed to be one over the number of updates and so on.
But once you get into the SGD form like this
then now this just behaves as a step size and you can tune it to your heart's content.
All right. So that's all I will say about these two challenges.
One is how do you do exploration?
You can use Epsilon-greedy which allows you to kind of balance exploration with
exploitation and then the second thing is that for large state spaces,
Epsilon-greedy isn't going to cut it because you're not
going to see all the states even if you try really
hard and you need something like function approximation to tell
you about new states that you fundamentally haven't seen before.
Okay. So summary so far, online learning.
We're in an online setting.
This is the game of reinforcement learning.
You have to learn and take actions in real world.
One of the key challenges is the exploration-exploitation trade-off.
We saw, um, four algorithms,
there's kind of two key ideas here.
One is Monte Carlo which is that from data alone,
you can basically use averages to estimate
quantities that you care about, for example, transitions, rewards, and Q values.
And the second key idea is this bootstrapping which shows
up in SARSA and Q-learning which is that you're
updating towards a target that depends on your estimate of what you're trying to predict.
Um, not just the kind of raw data that you see.
Okay. So now I'm gonna maybe step back a little bit and talk
about reinforcement learning in the context of some kinda other things.
So there's kind of two things that happen when we went from
binary classification which was two weeks ago to
reinforcement learning now and it's worth kind of decoupling these two things.
One is state and one is feedback.
So the idea about partial feedback is that you can only learn about actions you take.
Right. I mean this is kinda obvious in reinforcement learning.
If you don't, don't, quit in this game,
you never know how much money you'll get.
And the other idea is the notion of state which is
that new rewards depend on your previous actions.
So if you're going through a volcano, you have to, ah,
there's a kind of a different situation depending on where you are in, in the map.
Um, and there's actually kind of- so,
so this is kind of you can draw a two-by-two grid where you go
from supervised learning which is stateless and full feedback.
So there is no state, every iteration you just get a new example, ah,
and that doesn't have, you know,
there's no dependency and in terms of prediction on the previous examples.
Um, and full feedback in because in supervised learning,
you're told which is the correct label.
Even if there might be 1,000 labels for example in image classification,
you're just told which ones are the correct label.
Ah, and now in reinforcement learning,
both of those are made harder.
There is two other interesting points.
So what is called multi-armed bandits is kind of a,
you can think about as a warm up to
reinforcement learning where there's partial feedback,
but there's no state which makes it easier.
And there's also, you can get full feedback but there are states.
So instruction prediction.
For example in machine translation,
you're told what the translation output should be,
but clearly though actions depend on previous actions because,
you know, you can't just translate words in isolation essentially.
Um, okay, So one of the things I'll just mention very briefly is, you know,
this is deep reinforcement learning has been very popular in recent years.
So reinforcement learning, there was kind of a lot of interest in
the kind of '90s where a lot of the algorithms were kind of,
ah, in theory were kind of developed.
And then there was a period where kind of not that much, not as much
happened and since I guess 2013,
there has been a revival of reinforcement of research.
A lot of it's due to I guess at the DeepMind where they
published a paper showing how they can do- use raw reinforced learning to play Atari.
So this will be talked about more in a section this Friday.
But the basic idea of deep reinforcement learning just to
kind of demystify things is that you are using a neural network for Q_opt.
Essentially that's what it is.
And there's also a lot of tricks to make
this kind of work which are necessary when you're dealing with enormous state spaces.
So one of the things that's different about
deep reinforcement learning is that people are much more
ambitious about handling problems where the state spaces are kinda enormous.
So for this, the state is just the,
you know, the pixels,
right, so there's, you know,
a huge number of pixels and whereas before people were kind
of in what is known as a tabular case
which the number of states you can kind of enumerate.
So, um, there's a lot of details here to care about.
One general comment is that reinforcement learning is, it's really hard,
right, because of the statefulness and also the delayed feedback.
So just when you're maybe thinking about final projects, I mean,
it's a really cool area, but don't underestimate how much work and compute you need to do.
Some other things I won't have time to talk about is so far we've
talked about methods that are trying to estimate the Q function.
There's also a way to even do without the Q function and just
try to estimate the policy directly that's called,
um, methods like policy gradient.
There's also methods like actor critic that try to
combine of these value based methods and policy-based methods.
These are used in DeepMind's
AlphaGo, and AlphaZero programs for crushing humans at Go.
This will actually will be deferred to
next week's section because this is in the context of games.
There's a bunch of other applications.
You can fly helicopters, play backgammon,
this is actually one of the early examples TD-Gammon was one of the early examples in
the early '90s of kind of one of
the success stories of using reinforcement learning in particular,
you know, self play.
For non-games, reinforcement learning can be used to kind of do
elevator scheduling and managing data centers and so on.
Okay. So that concludes this section on
Markov decision processes which we- the idea is we are playing against nature.
So nature is kinda random but kind of neutral.
Next time, we're going to play against an opponent where they're out to get us.
So we'll see about that.
 All right. Let's start guys.
Okay. So a few announcements before we start.
So, um, if, you have- if you need OAE accommodations,
please let us know if you haven't done that already.
So you need to let us know by
October 31st because we need to figure out the alternate exam date.
So, uh, we'll get back to you about the exact like details around the alternate exam date,
but let us know by October 31st.
Um, project proposals are also due this Thursday.
So do talk to the TAs.
Do talk to us, come to office hours, all that.
Okay. All right.
So today, I wanna talk about games.
So, um, so we've started talking about this idea of state-based models, like,
the fact that if you wanna have state as a way of representing,
uh, everything about- everything that we need to plan for the future.
We talked about search problems already.
We have talked about MDPs where we have
a setting where we are playing against the nature and,
and the nature can play, uh, like probabilistically.
And then based on that,
we need to respond.
Uh, and today, we wanna talk about games.
So, so at the setup is,
is we have two players playing against each other.
So we're not necessarily playing against nature which can act probabilistically.
We're actually playing against another intelligent agent that- that's deciding for,
for his own or her own good.
So, so that's kind of the main idea of,
of games. All right.
So, so let's start with an example.
So this is actually an example that we are gonna use throughout the lecture. All right.
So the example is, we have three buckets.
We have A, B and C. And then you are choosing one of these three buckets.
And then I choose a number from the bucket.
And the question is, well, your goal here is to
maximize the chosen number and the question is,
which bucket would you use?
Okay. So, so how many of you would choose bucket A?
No one trusts me, okay [LAUGHTER] No one trusts me, good.
How many of you would choose B?
Okay. So now, now people don't trust me [LAUGHTER].
How many of you choose C?
Okay. So, so there's a number of people there too.
So, so how are you making that decision?
So the way you are making this decision is, if you choose A,
you're basically assuming that I'm not playing like,
like try- I'm not trying to get you.
I might actually give you 50.
And if I give you 50, that'll be awesome.
And you have this very large value that you are trying to maximize.
If you think I'm going to act adversarial,
and go against you and then try to minimize your,
your number, then you're going to choose bucket B, right, because,
because worst-case scenario, I'll choose the,
the lowest number of the bucket and,
and in bucket B, the lowest number is one which is better than minus 50 and minus 5.
So, so if you're assuming I'm trying to, like,
minimize your good, then you're gonna choose bucket B.
And if you have no idea how I'm playing and,
and you're just assuming maybe I'm acting ast- stochastically and maybe I'm, like,
flipping a coin and then based on that deciding like what number to give you,
you might choose C because in expectation,
C is not bad, right?
Like, C, like, if you just average out these numbers and
then pick the average values from A, B, C- A,
A and B and C, the average value for A is 0,
for B, it's 2 and then for C, is, um, 5.
Right, so, so, so if I'm playing it stochastically,
you might say, well, I'm probably going to give you something around 5.
So you would pick C. Okay.
So, so today we wanna talk about
these different policies that you might choose in these settings and
how we should model our opponent and how we formalize these problems as game problems.
So this is an example that, that we just started.
Okay. So, so to- the plan is to formalize
games, talk about how we compute values in the setting of games.
So we're gonna talk about expectimax and minimax.
And then towards the end of the lecture,
we're gonna talk about how to make things faster.
So we're gonna talk about evaluation functions as a way of making things faster, uh,
which is using domain knowledge to,
to, to define evaluation functions over notes.
We're also gonna talk about alpha-beta pruning,
which is a more general way of pruning your tree and making things faster.
Okay. All right.
So that's the plan for today.
Okay. So we just defined this game and a way to,
to go about the scheme is to create something that's called a game tree.
A game tree is very similar to a search tree.
So this might remind you of
search tree where we talked about it like two weeks ago, right.
So, so the idea is,
we have this game tree where we have nodes in the- in
this tree and each node is a decision point of a player.
And we have different players here, right,
like I was playing or you were playing or we have
two different people, like, playing here.
So these decision nodes could be for what one of the players, not both of them.
And then each root to leaf path is going to be a possible outcome of the game.
Okay. So, like, it could be that I'm choosing minus 50 and then your decision was to pick
bucket A so that path is going to give us one possible outcome of how things can go.
Okay. So, so that is what the tree is basically representing here.
Okay. So the, the nodes in,
in the first level are the de- decisions that I was making and then the,
the first node, the root node are the decisions that you were making in this setting.
So if we were to formalize this a little bit more,
we're gonna formalize this problem as,
as a two player zero sum game.
Okay. So, so in this class, a- at least, like, today,
we are going to talk about two-player games where
we have an agent and we have an opponent.
And then we are going to talk about policies and values and for all of those things,
think of you- yourself as being the agent.
So you're playing for the agent.
You're optimizing for the agent.
Opponent is this opponent that's playing against you.
Okay. So we are also going to, to, like,
today, we are going to talk about games,
uh, that are turn-taking games.
So we're going to talk about things like chess.
We're not talking about things like rock-paper-scissors.
We will talk about that actually next time when we have,
like, like, simultaneous games where you're playing simultaneously.
Today we are talking about turn-taking settings.
Two-player turn-taking settings.
Full observability, we see everything.
We are not talking about, like,
games like poker where you don't necessarily see,
like, you have partial observation and you don't
necessarily see the hand of your opponent.
Full observation, two-player and also zero-sum games.
And, and what zero-sum means is,
if I'm winning and if I'm getting, like,
$10 from winning, then my opponent is losing $10.
So, so the total utility is going to be equal to zero.
If I win some amount,
my opponent is losing the same amount.
Okay. All right.
So, so what are the things that we need when we define games?
So, so we need to know the players.
We have the agent, we have the opponent.
In addition to that, you need to define a bunch of things.
This should remind you of the search lecture or the MDP lecture.
So you might have a start state, as S start.
We have actions which is a function of state,
which gives us the possible actions from state S similar to before.
You have a successor function similar to search problems.
So a successor function takes a state and action and it
tells us what's the resulting state you're going to end up at.
And this- and, and you have an end- this
end function which checks if you're in an end state or not.
And the thing that's different here,
there are two things that are different here.
One is this utility function.
And the utility function basically gives us the agent's utility at the end state.
Okay. So one thing to notice here is,
is that the utility only comes at an end state.
So after you finish the game, like,
I've played my chess and I won chess now and this is this chess game.
And then, then I get my utility.
Like, as I'm making moves, like,
through my, my chess game, I'm not getting, getting any utility.
Like, you only get the utility at an end state.
And, and the way we're defining the utility,
is we're defining it for the agents because again we are,
we are replaying from perspective of the agent.
So, so what would be the utility of the opponent?
Minus that, right.
So, so negation of that would be the utility of opponent. Okay.
I've heard about partially observable Markov decision process.
Is this, like, kind of, what it is?
Like, is this partially observable?
Okay. So the question is, is this partially observable Markov decision process?
This is not a partially observable Markov decision processes.
Um, there are classes that talk about,
like there's- this decision under uncertainty by
Mykel Kochenderfer's class that actually teaches that.
So you should, you should, you should take classes on that.
This is not a partially observable Markov decision process.
This is fully observable.
You have two players playing against each other. It's a very different setup.
[inaudible].
So, so the, the question is,
are there any randomness here?
And, and so far, I haven't discussed any randomness yet.
Later in the lecture, I'll talk actually about the case where there might be
a nature in the middle that acts randomly and then how we go about it.
But so far, two players playing against each other.
Okay. All right.
And then the other thing that we need to define when you are defining a game,
um, is, is the player.
So, so, so player is a function of state.
And basically tells us who is in control, like, who is playing now.
So in the game of chess,
like, whose turn is it now.
And then that is the function that,
that you are going to define when we are formally defining, um, that game.
Okay. All right.
So, so let's look at an example.
So we have a game of chess.
Players are white and black.
Let's say you're playing for white.
So the agent is white,
the opponent is black.
And then the state S can represent the position of all pieces and whose turn it is.
So, so that is going to what the state is representing.
So whose player's turn it is and
then the position of all pieces.
So actions would be all the legal chess moves that player S can take.
And then IsEnd basically checks if the state is checkmate or draw.
That is what it is checking.
Okay. So, so then what would the utility be?
The utility will be,
will be if you're, like,
you're only going to get it when you win or when you lose or, or if there's a draw.
So the way we are defining it is,
it's going to be let's say, plus infinity if white wins because,
because the agent is white and,
and it's going to be zero if,
if there is a draw and then it's going to be minus infinity if black wins.
Okay. Yeah. So, so that was all the things that we would need to define. Yes.
[inaudible]
What- why do we have,
why do we have whose turn it is in the state.
Uh, so that's one way of actually, like,
extracting the player function.
So, so the way you can define a player function is a player is a function of state.
So the state already needs to encode whose turn it is.
So you can kind of extract that from the player.
You said the, the utility would kind of be negative utility for the p agent.
Is that assuming that they're both taking the same actions the whole time?
No. So, so, so this is turn-taking, right?
So I take an action and then the opponent
takes an action and then the agent takes an action.
The opponent takes an action and then at the very end of the game then then
you get the utility and then the opponent gets- gets the negative of that utility.
But the actions could be very different.
Policies could be very different.
And we'll talk about how to come up with that.
So why is that condition variable, so what happens if white wins, you get
plus infinity, but if black wins, if black wins, you get negative infinity, but like, when you lose-
you hav- you don't have zero-sum game.
We'll talk about that next lecture actually a little bit.
So, so I'm, I'm talking about zero-sum games here
because the algorithms you are talking about are for zero-sum games.
Like we are talk- going to talk about min- mini-max type policies.
Where I'm minimizing and the agent is maximizing.
So I'll get back to that if,
if I haven't answered that.
Like we can talk about it after the class but also next lecture,
we'll talk about more variations of games.
So- but for now, I'm assuming a bunch of simplifying assumptions about this game.
The assumption is that like if white wins, it's negative infinity, but if white wins, black gets 0 utility, [inaudible]. [NOISE]
Uh, yeah.
So these utilities need to add up to 0.
If white wins, maybe white gets 10,
but black gets minus 10.
So, so like they, they need to add up.
Okay. All right.
So and then kind of the characteristics of games that
we have already discussed are two main things.
One is that all utilities are at end state.
So throughout this path you are not getting
new utilities as opposed to like things like MDPs where we were,
we were getting rewards like throughout the path.
But here, like the utility only comes in at the very end.
At the end state. And then the other thing about
it is that different players are in control at different states, right.
Like if you are in state, you might not be able to control thing- control things.
It might be your opponent's turn and you might not be able to do anything.
Okay? So those are kind of the two main characteristics of games. All right.
So let's look at a game that you're going to play.
All right. So the game is a halving game.
So we start with a number N. And then
the player- the players take a turn and they can do two things.
They can either subtract 1.
So they can decrement N, or they can replace N with N over 2.
So they can divide or subtract.
Okay? And the player that's left with 0 is, is going to win.
Okay. So, so that is, that is the setup.
Is that- is everyone following that?
So, so let's try to formalize the game and then after that you
want to figure out what is a good policy to, to do it.
So, so right now let's just try to- let's just try to formalize this.
So you know like what are all the different things for the model are.
So let's just have a new file.
We are going to define this game.
So it's a halving game.
Okay, so let's, let's get this. All right.
So we're initializing with N. So we're starting with some number N. So what is our state?
Our state is going to encode whose player turn it is and that number N. Okay.
So we have a player.
Let's say our players are either plus 1 or minus 1.
That's how I'm defining like who's player it is.
So the start state. Let's say player plus 1 place with N. So so
that is plus 1 and N. And then we need to define its end.
Okay. So what you do is end check.
Well we take the state. We decouple it into player and number.
And if the number is equal to zero then then that is when the game ends.
That's our ending condition. Okay.
How about utility?
Well we get the utility at an end state.
So again I take a state.
I decouple it into player and number.
I make sure that we are in, in, in an end state so we assert
that number is equal to 0 because that kind of
defines if you're in an end state or not.
And then the utility I'm gonna get, if I'm winning I'm gonna get infinity.
If I'm not winning I'm gonna get minus infinity.
And the way I'm defining that here is by just doing player of times infinity.
Because player- I'm the agent, I'm the player plus 1.
The opponent is player minus 1.
That how- like if,
if minus 1 is winning I'm gonna get minus infinity.
Okay? The actions that we can do is we can subtract 1, or we can divide.
Divide by 2. I mean subtract and divide are the main actions.
And player, this player function again takes the state.
I'm gonna decouple the state into player and number and just return the player.
That's how I know who's player's turn is.
And then we need to define the successor function.
The successor function takes a state and
an action and tells us what state you're going to end up at.
So again a state.
I'm going to decouple that into a player and a number.
And then the actions I can take are two things.
I can either subtract 1 or I can I can divide by 2.
So if I'm subtracting then I'm going to return a new state which is minus player cause
now it's minus 1's turn or plus 1's- like it's minus whoever turn it is now.
And then I'm gonna do number minus 1.
If the action is divide,
we're gonna return the new player which is minus player,
and then number divided by 2.
Okay? That is it.
So, so we just defined this game, okay.
Yeah. All right.
So, so that was my game.
We're gonna play this game in a little bit.
But let's- quickly before playing it.
Let's talk about what is a solution to a game.
Like what are we trying to do in a game.
So if you remember MDPs the solution to a game was was the policy.
So a policy was a function of state.
It would return the action that you need to take in that state.
So similar to MDPs here we have policies.
But, but, the thing is I have two players.
So policy should should depend on the player too.
So I have Pi of P which is the policy of player P. And I can define it similar to before.
It can be a policy as a function of a state and it can return just an action.
And this would be a deterministic policy.
Like deterministically if I'm in state,
the policy is going to tell me what action to take, okay.
We can also define Stochastic policies.
So what Stochastic policies would do is they would
take a state and action and then they would
return a number between 0 to 1 which is the probability of taking that action.
So policy Pi of a state and action basically will return
a probability of player P taking action A in state S. So,
so if you remember the bucket example,
like maybe half the time I would pick the number
on the right and half the time I would pick the number on the, on the left.
That would be a stochastic policy, right.
I'm not deterministically telling you what the action is.
I'm coming up with the stochastic way
of telling you like what policy I'm following, okay?
So we have deterministic policies.
Stochastic policies. Like in our game we could follow either one of them.
Under what case would you want a
stochastic policy versus the deterministic policy?
Uh, can you speak up?
Yeah. Under what case would you want a
stochastic policy versus a deterministic policy?
So under what case do you want a stochastic policy versus a deterministic policy?
Again, we'll cover that a little bit more next time depending on what games you are in.
Like you have some properties of when stochastic policies are giving
us some some properties and deterministic policies are giving us some other properties.
Right now you're just defining them as things that could exist.
And, uh, we could think our opponent is acting deterministically if,
if you know exactly what they were doing.
Sometimes I've no idea.
Maybe you like I've learned it somehow and I have some randomness there.
And then I'm going to use some stochastic policy
for how my opponent is going to play against me.
But we are going to apply the- like what we get out of the
stochastic versus deterministic policy is a little bit more next time.
Okay. All right. So okay.
So now let's- okay so now that we know that it's the policy that we want to get.
Let's try to, let's try to write up a policy for this game.
And then I'm gonna define a human policy.
And what I mean by that is this is going to come from the human.
That means one of you guys or two of you guys.
So, um, so I need two volunteers for this but let's quickly actually write this up.
So what is a human policy?
It's just going to get the input from the keyboard.
So, so what I'm going to type up here is,
is get the action from the keyboard.
So get the input from the keyboard.
And that is going to be the action that we are picking.
Remember the actions are either divide or subtract.
Subtract 1. And if action is valid then return that action.
That sounds like a good, good pol- policy.
Okay. So that is a human policy.
So now what I wanna do is I wanna
have like this game that they're actually playing against each other.
So I want to have policies for my agent.
My agent is plus 1.
That's going to be a human policy.
And for my opponent,
I'm gonna say my opponent is also a human policy.
So I just want two humans to play against each other.
Okay. And the game is, let's say we are starting with 15.
So our number that we're starting with is 15.
Okay? All right, so that looks right to me.
So how do we, how do we ensure that we are progressing in the game.
So if you're in an end sta- if you're not in an end state you want to progress.
So let's print a bunch of things here.
Let's print out state.
Okay. Let's get the player out of the state cause again the state encodes a player.
Let's get the policy.
Because we have defined these policies for both of the players so
we can get the policy for whoever is playing right now.
And then the action comes from the policy in that state.
And then the new state you're going to end up at is
just the successor of the current state and action.
So th- I'm just progressing.
So, so this while loop here just figures out what state we are in,
what policy are we following,
and where are we going to end up at and that's the successor function.
Okay. And then at the very end I'm just going to print out the utility.
So that's either plus infinity or minus infinity.
And that sounds good.
So, all right.
So let's actually- All right.
So who wants to play this?
Okay that's one person.
You're the agent. You're player plus 1.
Opponent is three people [LAUGHTER].
I think you were first. By [inaudible] yeah.
Okay so you're minus 1.
All right so let's,
uh, play this game.
Is this large enough?
Yeah. Okay. All right so player 1.
Player plus 1. We are at number 15.
Do you wanna, uh, decrement.
Okay. So minus 1. So we are at player minus 1.
We're at 14. What do you wanna do?
Divide.
Divide. Okay. You have a policy [OVERLAPPING] [LAUGHTER]
[BACKGROUND]
Minus 1.
Divide.
Divide. [LAUGHTER]
[LAUGHTER] Yeah I don't really, yeah.
So yeah so you kind of get the point, right, so wait, did I make you lose now?
[LAUGHTER] Sorry. My bad.
But you get the utility at the end and then basically you
kinda can see this interface- actually does any- Oh I don't know.
We don't have that much time. I was going to try like another pair
but the code is online if you wanna play with it, just play with it.
We will have one other version playing it with an automated policy later.
Um, all right.
So, okay.
So we're back here.
Let me close this.
Um, all right. So we just saw how we can give
some human policies and human policies playing against each other.
And again, the policy,
you give it a state and action.
It gives you a probability or you give it a state and it gives you an action.
So a deterministic policy is just an instance of a stochastic policy.
Right? So if you have a deterministic policy,
you can kind of treat as a stochastic policy where
with probability 1 you're picking- you're picking an action.
So, all right.
So, so now we wanna talk about how we evaluate a game.
So, so let's say that someone comes in and gives me
the policy of an agent and an opponent,
and I just want to know how good that was.
And again if you remember in the MDP lecture,
we started with policy evaluation.
So in the MDP lecture,
we started with this idea of someone gives me the policy,
you just want to evaluate how good that is,
and you're kind of doing it analogous to exactly that.
Someone comes in and tells me that my agent is going to pick bucket A,
that is what my agent is going to just do all the time.
And someone comes in and says, "Well,
my opponent is going to act stochastically and,
and with probability one-half,
give me one of those numbers."
Okay? So, so these are the two policies that we are going to have.
So the question is; how good is this?
So going back to the, to the tree, the game tree,
what is really happening is my agent is going to pick,
uh, this one, right?
Because he's going to pick bucket A.
So with probability one,
we are going to end up here,
with probability zero we end up in any of these other buckets.
And then my opponent is going to stochastically pick either minus 50 or 50.
Okay? So if my opponent is picking minus 50 or 50,
then the value of this node is just the,
the expectation of that or it's just going to be 0.
So 50% of the time it's minus 50,
50% of the times it's 50, then the value of this node is 0.
And then if my agent is picking, picking A then,
then the value of this node is going to be 0.
Okay? So, so you kind of can see how the value is going to propagate up from the utility.
So we had the utilities at the leaf nodes,
but we could actually compute a value for each
one of these nodes if I know what the policies are.
Like if I know who's following what policies,
I can actually compute these values and go up the tree.
Okay? And so in this case,
I can say a value of a- of the start state,
if I'm evaluating this particular policy,
is going to be equal to 0.
Okay? All right.
So someone gave me the policy,
I evaluated the value at the start state.
So in general, as I was just saying earlier and this is,
this is similar to policy evaluation.
This is similar to the case that someone gives me the policies and I'll
evaluate wha - how good the situation is.
And you can write a recurrence to actually compute that.
So I'm going to write the recurrence here maybe.
So you want to compute this value.
And this value is evaluating a given policy and it's a function of state.
Well, what is that going to be equal to?
It's going to be equal to utility of S,
if you're in an end state.
So it's utility of S if we are already in an end state.
Otherwise, I have access to the policy of
my opponent and policy of my agent so I can just do
an expected sum over all possible actions of S. Let's say that I am - if,
if player S is agent,
I'm looking at policy of agent,
let's say its a stochastic policy times V of eval of the successor state.
Successor of S and A.
And this is if, if my player is agent.
So, so if is player - I'm just gonna write is player of S is equal to agent.
What happens if my player is opponent?
Um, I'm gonna do the same thing.
I'm just evaluating I have access to the po- policy of the opponent.
I'm again just doing- going to do a sum over
all possible actions on the policy of the opponent,
this is given to me- someone gave this to me,
of state and action times the value of the successor state.
And S and A and this is the case that my player is the opponent.
So this is a recurrence that we are going to just write and it's kind of intuitive.
Again, we have seen this in research too.
Like you start with the utilities at the leaf nodes and you just push that back
up based on what your policies are and what
your policies are telling you like which sides - like which,
which edges of the tree you are taking with what probability.
Okay? This makes sense? All right.
Okay. So that was evaluating the game.
But what if now I want to solve what the agent should do?
Like I'm the agent,
I care about doing - like figuring out what my Pi agent is.
I don't know what my Pi Agent is.
I need to figure out what sort of policy I should be following.
And that kind of takes us to this idea of expectimax which is
basically the idea of - if I'm in a scenario where I know what my opponent does,
so I'm still assuming what - I know what my opponent does,
what would be the best thing that I should be doing as an agent?
Okay? What, what would be the best thing I should do?
Like if you knew, like, in the bucket example,
I was trying - I was acting probabilistically, what would you do?
Pick the action that gives you the maximum value.
So you'd pick the action that gives you the maximum value because you're
trying to maximize your own, your own value.
So, so then if that is the case,
then this recurrence needs to change, right?
This recurrence- the way changes is,
I'm going to call this- that new value,
so I'm going to just do everything on top of this, I'm not gonna be writing it.
I'm gonna call this value,
value of expectimax policy.
Okay? So, so this value eval,
I'm not evaluating anything anymore.
I want to actually figure out what my agent should do.
So I'm gonna call it expectimax.
And if I know a policy of my opponent,
I'm not changing anything here because I know the policy of my opponent,
I'm just going to compute this.
But now I want to figure out what the agent should do and what should the agent do?
Well, the agent should do the thing that maximizes this value.
So I'm going to erase this sum with the policy because I don't have that policy.
And the agent should do the thing that maximizes this value over all possible actions.
So this should remind you of value iteration.
So if you remember value iteration in the MDP lecture,
like we weren't evaluating things, right?
We were trying to maximize our value.
And that's kind of analogous to what we are doing here.
We're trying to figure out what should be the policy that the agent should take that
maximizes the value under the scenario that I know what the opponent does.
So I still kind of know what the opponent does.
So going back to this example,
so let's say I know my opponent is acting stochastically. What should I do?
So if my opponent is acting stochastically with probability one-half,
then the values of each one of these buckets are going to be 0, 2 and 5.
And I'm trying to maximize my own util- my own values.
So I'm gonna pick the one that gives me five.
And, and that's shown with this upward triangle I'm trying to maximize.
So I'm gonna pick bucket C because I'm trying to
maximize under this knowledge that the other agent is stochastically acting.
Okay? And, and, and then you're calling this the value of
expectimax policy and the value of
expectimax policy from the start state is equal to 5.
Right? Because that's, that's evaluating
the thing I'm going to get. Question back there?
[inaudible]
Yes. This is assuming I know my opponent's policy and I'm,
I'm following - I guess I'm maximizing my own, er,
my own value knowing that my opponent
is following this policy and what the opponent would do in expectation.
Okay? All right.
So and then this is the,
this is the recurrence that we would get, we would just update the recurrence.
So if the agent is, uh,
playing then we maximize the value of expectimax.
Okay? All right.
So, okay, in general I don't know the policy of my opponent.
Right? So in general, like,
I know what gives me this pi opt.
So if that is the case, then what should we do?
So one thing that we could do is we could assume worst case.
So, so one thing that you could do is you could be like oh
the opponent is trying to get me in and
they're going to play the worst-case scenario and they are trying to minimize my value.
And, and that's the fair thing to do.
And we are going to talk about if,
if that is always the best thing we can do or not,
a little bit later in the lecture.
But for now, what we could assume that if I know nothing about my opponent,
I can just assume my opponent is acting adversarially against me.
So and that kind of introduces this idea of
minimax as opposed to expectimax that we just talked about.
So, so what would minimax do?
So in the case of a minimax policy, what I'm,
I'm assuming is I am this agent trying to maximize my,
my own- my own value and then I'm assuming my opponent is acting adversarially.
So my opponent is really trying to minimize my value.
And what that means is from this bucket,
I'm gonna get minus 50,
from this one I'm gonna get 1,
from this one I'm gonna get minus 5.
And under that assumption, well,
I'm going to pick the second bucket because that gives me the highest- the highest value.
So, so that is a minimax policy.
So how would I change my recurrence if I were to play minimax or I'm going to-
I'm going to call it V of- so let's look at the V of minimax of a state.
Well, the recurrence is going to be over
minimax, V of minimax, so I'm gonna change that.
If the agent is playing,
the agent is still trying to maximize the value.
So, so that is all good.
What if the opponent is playing?
The opponent is going to minimize, right?
So I don't have access to pi opt.
So what I'm gonna do is I'm going to remove
this and say well the opponent is going to take
an action that minimizes the value of the successor of S and A.
Okay? And this is how you would compute the value of a minimax policy.
Is this assuming that the adversarial agent consistently tries to minimize the utility of the agent?
Yes.
What happens when, um,
the adversarial agent doesn't always go with that selection but also becomes
stochastically.
Yes. So that's a good question.
So what happens like if the adversarial agent is not always adversarial, right?
So in that case, you have another stochastic policy
that kind of defines what- what the opponent is doing.
And if you have access to that,
you can do something similar to expectimax.
If you don't have access to that maybe you would want to act
worst-case and assume that they're always trying to minimize.
But- but that's some prior knowledge that you have that
allows you to- to act better or maybe evaluate,
ah, the value better for wherever you stay.
So we'll talk about evaluation functions a little bit in the lecture.
And maybe you'll look back and form your evaluation function, okay? All right.
So- so- so here the value of minimax from the start state is going to be 1,
right? Does everyone see that?
So I'm assuming my opponent is acting adversarially.
So we have minus 51 and minus 5.
If I am maximizing then the best thing I can get is 1.
And then that's how we compute V of minimax, okay?
And then there is really no analogy to this in MDP
setting because in MDP setting you don't really have this game.
We don't really have this opponent that's playing against us.
And what happens is, is that this is a recurrence that you're going to
get it which is what we already have on the board, right?
Okay. So- so what would the policy be?
So the policy is just going to be the argmax of this V of minimax.
So if you want to know what the policy of your agent should be, that's Pi max.
It's the arg max over v of minimax,
over successor of that state.
And if you want to know what's the policy of- of your opponent,
that state S should be- well,
that's argmin of- of b of minimax which is intuitive, right?
So- so then that way you can actually figure out what the action should be,
what the policy with the actual action should be, okay?
All right. So let's go back to this example, this halving game.
So what we wanna do is we wanna actually code up what
a minimax policy would do in this setting.
And maybe we can play with a minimax policy after that, okay?
So what would a minimax policy do?
So it's a policy,
so it's going to be a function of states, so let's give it state.
And you're going to just write this recursion that we have on the board.
So- so we're recursing over to state.
If you're in an end state then what are we returning?
Just the utility, okay?
So we're returning the utility of that state,
and there was no actions.
And then if you're not in an end state,
then you are either maximizing or minimizing over a set of choices.
So let's actually like create those choices so they can just call max and min on them.
So the choices we're going to iterate over all actions that- that we have.
And what is that going to be exactly?
Well, that's going to be doing a recursion over the successor states.
So we are going to recurse over the successor state.
So recurse over succ- game.successor of state and action.
And I'm going to return the action here too because I just want to get the policy later.
And the successor- does this recursive function returns a state and action.
So I just want to get the state from the first one and the action from the second one.
Okay. So if player is plus 1 that's the agent,
the agent should maximize the choices.
And if player is minus 1,
then- then that's the opponent,
the opponent should try to minimize over these choices.
And that's pretty much like this recursion that we have on the board,
and- and that's our recursive function, okay?
So we're going to recurse over- over our state and that
gives us a value and it also gives us- gives us an action.
So let's just print things out.
So you can refer to them.
So minimax gives us an action,
and it tells us this is the value that you can get [NOISE].
All right. And then it's a policy,
so let's just return the action.
Okay. So now what I'm gonna do is,
I'm going to say plus 1 agent is still a human policy,
and then it's playing against a minimax policy.
So all right.
So let's- who wants to play with this?
And it's a little scarier to play with the minimax policy [LAUGHTER].
Okay. All right.
So let's do this.
Python.
All right. So you are the agent. So you're player 1.
You're starting from 15. What do you want to do?
[BACKGROUND].
So you just lost the game [LAUGHTER].
So- so why do I know you lost the game?
Now it's player minus 1 playing, you are at 7.
And minimax policy took action minus, er,
and says action minus, um,
and- and it also,
yeah takes action minus. So we're at 6.
And then the value of the game is minus infinity.
So you're playing with a minimax policy,
you're already getting minus infinity.
So- so you just lost the game.
Anyone want to try this again [LAUGHTER].
You want to try it again maybe.
[BACKGROUND] Subtract.
[LAUGHTER] Okay.
So you- so you can win, right?
So the value is infinity right now.
And then yeah, so and then the minimax policy also did a minus.
So we're at 13 right now.
It's your turn, you're at 13 [BACKGROUND].
You just lost the game again [LAUGHTER].
So yeah, so minus infinity is- yeah actually you need to like alternate between them.
I think that is the best policy.
But play with this kind of get a sense of how this runs. The code is online.
So just feel free to play with it and figure out,
what is the best policy to use. All right.
So- okay.
So- so that was a minimax policy.
And then this is kinda the recurrence that we get for a minimax policy.
Now, what I wanna do is I wanna spend a little bit of time talking about,
um, some properties of this minimax policy.
And then we talked about two types of policy so far, right?
We have talked about expectimax,
which is basically saying,
"I as an agent,
I'm trying to maximize,
but I know what my opponent is going to do.
So I'm going to assume my opponent does whatever.
And then I'm going to maximize based on that."
So- so for example, I am following and I'm going to refer that to as Pi of expectimax,
which means that the agent and everything in red is for the agent,
everything in blue is for the opponent.
So I'm gonna say the agent is following this policy which says,
"I'm going to maximize assuming my opponent is doing whatever.
And here I'm calling Pi 7 as like some opponent policy."
It couldn't be like anything but Pi 7.
So let's say that, opponent is playing Pi 7,
I'm going to maximize based on that.
And- and the value we just talked about is the value of expectimax.
The other value we just talked about is the value of minimax which says, "I am the agent.
I'm going to maximize assuming the opponent is going to minimize."
And then the opponent actually is going to minimize and is going to follow pi min.
Okay. So- so these are the two values we have talked about so far.
I want to talk a little bit about the properties of this. But before that, let me-
So weight to like kinda like mix the two together.
And you say like just highlight the probability of
piping the minimum for like an expected max.
I give a probability distribution over like the actions, right?
Like why don't we just take the action that like
minimizes whatever our reward is and give it a higher weight,
in Expectimax.
Um-
[NOISE] I didn't fully follow what policy you were referring to, actually.
Is it- are you coming up with a new policy that you
do- you're saying would be a better policy to
[NOISE] between like expectimax and minimax in some sense?
So this might- this, this table might,
kind of, address that because it's,
it's considering four different cases.
It's actually not considering the two cases.
So this might actually refer to what you're, what you're proposing.
So, so let's actually go through this first and then maybe,
like, if it doesn't answer that.
So, All right, so,
so I want to talk about the setting.
So this table is actually not that confusing,
but it can get confusing.
So do pay attention to this part.
Um, all right, so where do I wanna- maybe, maybe I'll write over there.
So I'm gonna use red for agent.
Where is my blue, my blue? On the floor?
Hanging on the left.
Left?
Your right.
My right, [LAUGHTER] okay, all right, [LAUGHTER] okay.
And then I'm going to use blue for,
um, and I dropped this.
I'm going to use blue for,
um, the opponent policy.
Okay. So, so then for agents,
we're are going to have Pi max.
All right. An agent could play Pi max. What does that mean again?
I'm going to maximize assuming you're gong to minimize.
An agent could play Pi expectimax.
Maybe the policy 7, I'm gonna put 7 here,
which means I am going to maximize assuming you're going to follow this Pi 7.
So this is a thing that the agents can do.
[NOISE] Okay?
And then there are things that my opponent can do.
I'm going to write that here.
My opponent can actually follow Pi min which is I'm just going to minimize,
or my opponent could follow some other policy Pi 7.
Let's say Pi 7 in the bucket example right now is,
is just acting as stochastically.
So half the time pick one number,
half the time pick another number.
Okay? So, so that is what we have.
So I'm going to draw my- actually my tree so we can go over examples of that too.
So this was the bucket example.
They started at minus 50 and 50 in bucket A,
1 and 3 in bucket B,
minus 5 and 15 in bucket C. Okay?
So this was my bucket example.
I'm actually going to talk about that.
So- All right.
So I'm gonna talk about a bunch of properties of V of Pi max and Pi min,
which is what we have been referring to as the minimax value.
Okay? So, so I want to talk about this a little bit.
Okay? So the first property that,
that we can have is,
is that V of Pi max and Pi min,
it is- actually let me go back to the next slide.
It is going to be an upper bound
of any order value of any order policy.
Pi of- I'm going to just write Pi of expectimax for any other policy for the agent.
Assuming that my opponent is playing as a minimizer.
Okay. So, so what I'm writing,
so what I'm writing here is,
is that value is going to be an upper bound of any order value if
my agent decides to do anything
else under the assumption that my opponent is a minimizer.
So my opponent is really trying to get me.
If my opponent is really trying to get me,
then the best thing I can do is to maximize.
Okay? So, so that's kind of intuitive,
right? That's an upper bound.
Let's look at that example. So what is Pi- V of Pi mix- er,
Pi max and Pi min?
So, so we just talked about that, right?
So if this guy is a minimizer,
we're gonna get minus 50 here,
1 here, minus 5 here.
If this guy is a maximizer,
what is the value I'm gonna get?
You'll get 1, right? I'm gonna go down here and then I'm gonna get 1.
So V of Pi max and Pi min is just equal to 1.
That is this value that is just equal to 1.
Okay? What is this saying is that this is going to be greater
than maybe the setting where my opponent- so my,
my agent is following expectimax and my opponent is still doing Pi min.
So, so what would this correspond to?
What will this value correspond to?
So this is a value which says, well,
I'm going to take an action assuming my opponent is acting stochastically.
If my opponent is acting stochastically,
I'm gonna get 0 here,
I'm gonna get 2 here, and get 5 here.
If I'm assuming that and I'm trying to maximize my own,
my own value, which route do I go? I'm gonna go this route.
But it turns out that my opponent was not doing that.
My opponent was actually a minimizer.
So if my opponent was actually a minimizer and I went this route,
my opponent is going to give me minus 5.
So the value I'm going to end up getting is minus 5.
So this is equal to minus 5.
This is equal to minus 5.
Okay? So, so far I've shown that this guy is greater than this guy.
okay? All right.
So that's the first property.
First property is if my opponent is terrible and is trying to get me,
best thing I can do is to maximize. I shouldn't do anything else.
Okay? The second property is,
is that this is V of Pi max, again the same V,
V of Pi max and Pi min is now a lower bound of
a setting where your agent is maximizing assuming your opponent is minimizing.
But your opponent was actually not minimizing,
your opponent was following Pi 7.
So, so what this says is if you're trying to maximize assuming your agent,
your, your, your opponent is always minimizing, then,
then you're doing- like you'll come up with like
a lower bound and if your opponent ends up doing something else,
you can always just do better than this lower bound.
Okay? So what is,
what is this V equal to or we just showed that is, that is one, right?
That is this value.
Okay? What does this correspond to?
So this is value of Pi max which is I am going to assume you are trying to get me.
If I'm going to assume you are trying to get me I'm gonna go down
this route because that is the thing that gives me the highest, the highest value.
But you are not trying to get me,
you are following Pi 7.
So if you're following, following Pi 7,
you're just going to give me a half the time
1 and half the time 3 and that actually corresponds to the 2,
and I'm going to get value 2 instead of value 1.
So this is actually equal to 2 in this case.
And this corresponds to this value in the table which is again
the agent is following a maximizer assuming the opponent is a minimizer.
Opponent was not a minimizer,
opponent was just following Pi 7.
And this is just equal to 2 .
Okay. So so far,
the things I've shown are actually very intuitive.
They seem a little complicated but they're very intuitive.
What I've shown is that this value of minimax,
it's an upper bound.
If you're assuming our,
our opponent is a terrible opponent,
now it's going to be an upper bound because the best thing I can do is maximize.
I've also shown it's a lower bound if my opponent is not as bad.
So, so that's what I've shown so far.
A question.
So here the opponent's policy is completely hidden to the agent.
Yeah. So here, like,
because- Yeah, the agent actually
doesn't see the opponent- where the opponent goes, right?
Even in the expectimax case,
it thinks the opponent is going to follow Pi 7,
but maybe the opponent follows Pi 7, maybe not.
Right so, so like when we talk about expectimax and minimax,
it's always the case that the opponent doesn't actually see what the opponent does.
But the opponent can think- the agent can think what the opponent does, okay?
And I'm going to talk about one more property.
And this last property basically says if you know something,
actually goes back to your question,
if you know something about your opponent, right?
If you know something about your opponent,
then you shouldn't do the minimax policy.
You should actually do the thing that has some knowledge of what your opponent does.
So, so that basically says this-
we Pi max and some Pi of opponent,
you know something about Pi opponent.
You know that opponent is playing Pi 7.
That is going to be less than or equal to the case where you are following
the Pi of expectimax of 7,
uh, and the opponent actually follows Pi 7.
Okay. So what does this last equality- inequality saying?
Well, it is saying that the case where you're trying to
maximize and you think your opponent is minimizing,
but your opponent is actually not minimizing the,
value of that is going to be less than the case where you're maximizing under
some knowledge of your opponent's policy
and your opponent's policy actually ended up doing that.
Okay? So, so the first term is always the agent.
The second term is always the opponent, right?
So this value we have already computed,
that- that's equal to 2.
This value, what is this value saying?
It is saying you are going to maximize assuming your opponent is stochastic.
So if I'm assuming my opponent is stochastic,
then I'm assuming that this is 0,
this is 2, this is 5, right?
I'm trying to maximize.
So which one of my routes shou- should I go?
I should go this route because that gives me 5.
So this is the agent thinking the opponent is going to be stochastic,
thinking he's going to get 5.
And it gets here and the opponent actually ends
up following Pi 7 which is a stochastic thing.
So, so we are actually going to get 5.
So, so this guy is equal to 5.
And this is the last inequality that we have,
which is V of Pi expectimax of 7,
and Pi of 7 is greater than or equal to V of Pi max and Pi 7.
We just showed this is equal to 5 for this example.
Okay. All right. Question.
[inaudible] The actions of the opponents
always whether or not the [inaudible] [NOISE].
Uh, so-
So if, if you, so if you know something about the stochasticity, that's in order.
Like here, I knew that the opponent was
following the stochastic policy of one out, one out.
I might have known that the opponent is following
a deterministic policy in- and always is picking the left one.
So I could have like followed,
like same expectimax policy under that knowledge.
It could be anything else, but the whole idea of expectimax is,
I have some knowledge of what the policy of,
of the opponent is, it could be a stochastic policy,
it could be a deterministic policy under that,
how would I maximize?
Does that mean that like transitively,
that the bottom right is greater than the bottom left always?
Yeah. So the question is do we have- Yeah.
So we have what like this inequality, so transitively,
this guy is always greater than this guy.
And that kinda makes sense, right?
Like we're saying, like if you're following expectimax,
so this last one kinda makes sense, right?
It's, it's basically saying if you're following expectimax and you know
something about your opponent and your opponent actually ended up doing that,
though, though your value should be greater than pretty much anything, right?
Because you knew something about the opponent,
you played knowing that,
having that knowledge. Yes.
When you say knowing something about the opponent,
is that just knowing that it's asked stochastically or know what it's gonna take? [NOISE]
It's knowing what they're going to take.
Right? Like here, I knew what they'll point out.
I knew that half the time they're going to take this one,
half the time you are going to take the other one,
and then I use that knowledge, right? Yeah.
So you know exactly this? [OVERLAPPING].
Yes. Yeah, yeah, the expectimax.
Is the expectimax policy given that your opponent is following Pi min policy-
Given that, sorry.
Given that your opponent is following Pi min.
Is it- do you maximize it?
So the expectimax policy is,
is this policy when here we have a sum.
The expectimax policy, uh,
assumes your opponent is following Pi opponent
and assumes that it has access to Pi opponent and so it ends up doing this sum over here.
Yeah. If Pi opponent is Pi min? Like-
Uh, if Pi oppo- I see what you are saying.
So you're saying if Pi opponent is actually Pi min,
then do they end up being equal to each other in some sense?
So yeah, I guess so.
Yeah. So if, if you know that the oppo- it becomes minimax, right?
If you know your opponent is, is following min,
as acting as minimizer or just like call that minimax. All right.
So I'm gonna move ahead a little bit.
All right so- and then, this is like what we have already talked about.
Okay. So a few other things about modifying this game.
So, so we have- okay so we have talked about this game,
we have talked about properties of this game.
There is a simple modification one can do which is, bringing nature in.
So there was a question earlier which was like,
is there any chance here?
And then, yeah, you can like actually bring chance inside here.
So, so let's say that you have the same game as before,
you're choosing one of the three bins.
And then, after choosing one of the three bins,
you can flip a coin and if heads comes,
then you can move one bin to the left, with wraparound.
So what this means is 50% of the time,
tails comes, you're not changing anything,
you have this set up.
50% of the time you get heads.
And then, in those settings you're just gonna pick like
a neighboring bin as opposed to your original bin, okay?
So, so the- you're adding this notion of chance here and,
and it's kind of acting as a new player, so,
so it's not actually the making things that much more complicated.
So, so what happens is in some sense we have a policy of,
of coin which is nature here,
right and policy of coin is,
half the time I get 0,
I don't change anything,
half the time I just get the neighboring bin as opposed to my main bin.
And then I get this new tree where,
where I have like a whole new level for what- where the chance plays.
So we have- now we have max nodes,
we have min nodes, we also have these chance nodes here.
And the chance nodes again,
like sometimes they take me to the original bucket and then
50% of the times they take me to a neighboring bucket, okay?
But, but the whole story like stays the same, like nothing changes.
You can, you can still compute value functions,
you can still push the value functions further up.
It's the same sort of recurrence.
Nothing fundamental changes.
Just- it just feels like there are three things playing now, okay?
So, so then this is actually called expectiminimax,
so a value of expectiminimax here,
in this case for example,
is minus 2, because there is a mini node for the opponent,
there is an expectation node for what nature does,
and then there is a max node for what the agent should do.
That's why it's called expectiminimax.
And then, you can actually compute the same value.
So when the game is working out,
so there's like two players.
I pick a bin then you flip a coin,
and then shift it left or not shift it left,
and then I get to pick the number?
Yes. Well, not you, well the opponent.
The opponent.
So yeah. So, so there are still two players and then the third coin thing. Yes.
[inaudible]
All right. So, so yeah.
So the way to formalize this is you have players, so you have an agent,
you have an opponent, you have coin,
and then the recurrence changes a little bit I guess.
So, so what happens is,
the recurrence that we have had for minimax was just the max and
min and it would just return us the utility
if you're in an End function and in an End-state.
Now, if the- if it is the coins term,
we just do a sum over, uh,
an expected sum of the policy of
the coin which is what we were doing in expectiminimax.
But, but we just have like a new term for when coin placed.
So, so everything here kind of follows
naturally in terms of what we were expecting, okay?
All right so the summary so far is, uh,
what we've been talking about max nodes, we have been talking about chance nodes,
like what if you have a coin there and then also these min nodes.
Um, and, and basically we've been talking about composing
these sort of nodes together and creating like a minimax game or,
or an expectimax game.
And then value function, uh,
we- is- you just do the usual recurrence that we had been
doing in this class from the expected utility to,
to- from the utility to come up with
this expected utility value for all the nodes that we have.
So there might be other,
other scenarios that you might wanna think about for example,
for your projects or like in,
in general there are other variations of games that you might wanna think about.
So what if, like the case that you are playing with multiple opponents?
Like so far we have talked about like a two-players setting where we have
one opponent and one agent but what if you have multiple opponents,
like you can think about how the tree changes in those settings.
Uh, or for example, like the taking turns aspects of it, like is it sim- if,
if the game is simultaneous versus your turn-taking, uh,
or like you can imagine settings where you have
some actions that allow you to have an extra turn.
So, so you have two turns.
Uh, and then the next person takes t- takes a turn.
So, so you should think about some of these,
some of them come up in the homework.
So, uh, think about variations of games in general.
They are kind of fun. So to talk a little bit about the computation aspects of this.
Um, so this is pretty bad.
[LAUGHTER] Right, we talked about a game tree which is similar to tree search.
So we are taking a tree search approach.
Uh, if you remember tree search,
like the algorithms we're using,
like if you have branching factor of b and some depth of d then,
then in terms of time it's exponential in order of b to the 2d, in this case.
So I'm using d for the number of- how do I say this?
So, so it's 2d because the play- the,
the agent plays and then the opponent plays,
so that's how I'm counting it.
So every, every 2d like you have 2d plies but d depth.
Does that makes sense? All right.
And then in terms of space,
it's order of d in terms of time,
it's exponential that's pretty bad.
So for a game of chess for example,
the branching factor is around 35,
depth is around 50.
So if you compute b to the 2d,
then it goes in the order of like number of atoms in the universe,
that's not doable we should- we are not able to use any of these methods.
So, so how do we make thi- things faster?
So we should be talking about how to make things faster.
So there are two approaches that we are
talking about in this class to make things faster.
And the first approach is using an evaluation function.
So, uh, using an evaluation function and what we can do is you can use domain specific
knowledge about the game to define almost like
features about the game in order to approximate,
like the, the value th- this value function at a particular state.
So I'm going to talk about that a little bit.
And then, another approach is this approach which is kind of
simple and kind of nice which is called alpha-beta pruning.
And, and the alpha-beta pruning approach,
basically gets rid of part of the tree
if it realizes you don't need to go down that tree,
that part, that part of the sub-tree.
So, so it's a pruning approach that doesn't
explore all of the tree only explores parts of the tree.
So, so we're going to talk about both of them. All right.
So evaluation functions.
So let's talk about that.
Okay. So the depth can be really like
the breadth and depth of the game can be really large.
That's not that great. So one approach to go about solving the problem is,
is to kind of limit the depth.
So instead of like exploring everything in the tree,
just limit the depth and,
and get to that particular depth.
And then after that, when you get to that depth just call an evaluation function.
So, so if you were to search the full tree,
this was the recursion that,
that we had like we have talked about.
This was like if you're doing a minimax approach this is the recursion that you gotta do.
You gotta go over all the states and actions and,
and go over all of the tree.
But if you're using a limited depth tree search approach, what you can do is,
you can basically have this depth d and then decrement d every time you go
over an agent and opponent, like every time you go
down the tree and at some point d just becomes 0,
so you get to the po- some particular depth of the tree and when d becomes 0,
you're gonna call an evaluation function on the states that you get, okay?
And this evaluation function is almost of the same form of,
like future costs when we were talking about search problems, right?
So, so if you knew exactly what it was,
then, then you were done,
but you don't know exactly what it is because if,
if you knew that we were to solve like the whole,
uh, tree search problem.
But in general, it can have some sort of weak estimate of, of, um,
what, what the future costs would be.
So, um, yeah.
So, so an evaluation function Eval of s is a weak estimate
of V minimax of s. So it's a weak estimate of,
of your value function, okay?
All right. So, so analogy of that is future costs in search problems.
So how do we come up with an evaluation function?
So we do it in a similar manner that we had visited in the learning lecture,
where we're coming up with,
with features and, and weights for those features, right.
So, so if I'm playing like chess,
and like the way we play it, right,
like we think about a set of actions that we can take and where we end up at and,
and based on where we end up at um,
then you kind of evaluate how good that board is, right.
You have some notions of features,
and how good looking- like how good that board would be from that point on.
And that allows us to evaluate what action to pick,
right, like when we play chess that's kind of what we do.
We pick a couple of actions and we see how the board would look like after taking them.
An evaluation function kind of does the same thing, it tries to figure out
what are the things said we should care about in a specific game,
in this case in chess and tries to give values to them.
So, so it might be things like the number of pieces we have,
or mobility of those pieces,
or if our king is safe,
or if we have central control or not.
So, so for example,
for the pieces what we can do is,
we can look at the difference between the number of
pieces we have between what we have and what our opponent has.
So number of kings that I have versus number of opponents that I have.
Well, that seems really important thing because if I don't have a king and
our opponent has a king then [LAUGHTER] I've lost the game.
So, so you might put like a really large weight for
that and you might care about like differences between the number of pawns,
or number queens and other types of pieces that you have on the board.
So, so that allows you to care about- to think about how good the board is,
or number of legal moves that you have and the number of legal moves that your opponent has,
and then that gives you some notion of like mobility of that state.
Okay. All right.
So um,
so summary so far is- yeah,
so this is pretty bad,
order of B to the 2D is pretty bad,
and an evaluation function basically tries to estimate
this V minimax using some domain knowledge.
And unlike A star,
we actually don't have any guarantees in terms of
like error from these sort of approximations.
So um, but it's an approximation, people use it, it's pretty good.
We will talk about it a little bit later next time when,
when we think about like how- what sort of weights we should,
we should pick for each one of these,
for each one of these features.
So you should think learning when you think
about what are the weights we are using. All right.
So- okay, so now I want to spend a bit of time
on alpha-beta pruning because this is- yeah, important.
Okay. So alpha-beta pruning.
Yeah. The concept of alpha-beta pruning is also pretty simple,
but I think it's one of those things that was- it was kind
of that table you should pay attention to,
to kind of get what it is happening.
All right. So, so let's say that you want to choose between some bucket A and bucket B.
Okay, and you want to choose the maximum value,
and then you know that the values of A fall into like 3 to 5,
and the values of B fall into 5 to 10.
So, so they don't really have like any,
any intersections between each other.
So, so in that case,
you don't really care about your,
your- if you're picking a maximum right,
you shouldn't care about your bucket A,
or rest of your bucket A right,
because you already know that you are above 5,
you are happy with B, you shouldn't even look at A.
So, so kind of the,
the underlying concept of,
of um, alpha-beta pruning is,
is maintaining a lower bound and upper bound on values,
and then if the intervals don't overlap then basically
dropping part of the sub-tree that you don't need to work on because there is,
there is no overlap between them.
Okay. So here's an example,
so let's say we have these max nodes and
min nodes and you're going to go down and see 3,
and then this is a min node so,
so you're going to get 3 here.
So when I get to the max node here,
right, I- what, what,
I know is that the max node is going to get 3 or higher, right.
That- that's one thing that I would know without
even looking at anything on the, on the other side,
without even looking at the sub-tree on the left,
I already know that this max node should get 3 or higher, right.
Does everybody already agree with that? Okay. So, so then
when I go down to this min node and I see 2 here,
right, I know this is a min node,
it's going to get a value that's less than or equal to 2.
Less than or equal to 2 does not have any interval with greater than or equal to 3,
so I should not worry about that sub-tree.
Does everyone see see that? So maybe you'll like let me draw it out here.
[NOISE] So that's kind of like the whole concept of what happens in alpha-beta pruning.
So I have this max node,
this was three, this was what- five.
I found that the guy is 3,
this is a max node.
Whatever it gets, it- it's going to be greater than or equal to 3 because,
because it's already seen 3,
it's not gonna get any value less than 3, right.
So, so we know whatever value we are going to get at
this max node is going to be 3 or higher.
Okay. Then I'm going to go down here,
and then I see two here, right,
It's a min node whatever it gets is going to be less than or equal to 2.
So less than or equal to 2 is the value that's going to get popped up here.
I already know less than or equal to 2 has no interval with 3 or greater.
So I don't even need to worry about this like I,
like I can completely ignore this side of the 3,
I don't need to know whatever is happening down here,
I don't even need to look at that.
Okay. Because, because I- like this value should be greater than and equal to 2. Yes.
All right, we should get a value greater than or equal to 8.
Sorry.
[inaudible]
It's minimum- so it's a minimum,
it's a minimum node, right.
So it's going to be less than or equal [NOISE] to- right.
Yeah. It's a min node,
so I still have to, if I see 10 here or 20 here,
like I'm not going to pick that, like it's 2 or lower.
All right. So yeah- so if it is 10,
or 100, or whatever sub-tree it is there like we're not going to look at that.
So, so that, that is kind of the whole concept.
Um, All right.
So- okay.
Let me actually go to this slide, I think this would be.
So the key idea of alpha-beta pruning is as we are- with
the optimal path is going to get to some leaf node that has some utility,
and that utility is the thing that is going to be pushed up, right,
like- and then the interesting thing is if you pick the optimal path,
the value of the nodes on that optimal path are all going to be equal to each other,
right, like they're the- basically the utility that you are going
to get pushed up all the way to the top.
So, so because of that like we need to have like these, these like we,
we can't have settings where we don't have
any intersections between the intervals because we know if this is,
if this were to be the optimal path,
the value on this node should have been the same as the value at
this node- the same as the value at this node and, and so on.
So if they don't have any intervals then no way that they would have the same value,
and no way for that path to be the optimal path.
Okay. So, so that's kind of the reason that it works because
the optimal path you're going to have the same value throughout.
Okay. So-all right so how do we actually do this?
So the way we do this, is we're going to keep
a lower bound on max nodes, so I'm going to call it that a_s.
Let me [NOISE] get this up here.
So we are going to have a_s which is a lower bound on max nodes.
So we're going to keep track of that.
We're also going to keep track of b_s,
which is an upper bound on min nodes.
Okay. And then if they don't have any intervals,
we just drop that sub-tree.
If they have intervals we just keep updating a_s and b_s.
Okay. So, so here's an example,
so let's say that we start with this top node.
Somehow we have found out that this top node
should be greater than or equal to 6, right.
Somehow I know it should be greater than or equal to 6.
Okay. So that is my a_s value.
So my a_s is equal to 6, it,
it is, it is going to be a lower bound on my max nodes.
I know the, the valued- optimal value
is going to be something greater than or equal to 6.
Okay. Then somehow we get to this min node,
and then we realized that this min node should be less than or equal to 8.
So you're here, let's say 8 is here,
we still have some interval, we're all good, right,
so b_s is going to be equal to 8, right,
we have an upper bound on the min node,
and that tells us that upper bound is 8.
So the, the valued- optimal
value- the value and optimal path is going to be less than or equal to 8.
Okay. So far so good.
Then somehow I found out that that one is greater than or equal to 3.
Greater than or equal to 3 should be fine, right,
greater than or equal to 3 is still greater than or equal to 6,
my a_s in this case,
I'm going to call this S1, S2,
and S3 is equal to 3,
right, because I know I need to be greater than or equal to 3.
But like 6 already does the job, right,
like I don't need to worry about that 3.
So, so that's all, good so far.
And then for this last node,
I am at this min node,
and I realize that b_s4,
I'm going to call it b_s4,
is equal to 5.
And what this tells me is that your value should be less than 5 and less than 5.
So I'm going to update less than 8 to less than 5.
And now, we don't have any in- intervals.
So what that tells me is that path is not going to be the optimal path,
because there is no intervals.
So- so we're not going to find this- this one number that is going to be the utility.
And what that tells me is,
I can actually ignore that whole sub-tree because-
because that's not going to be in- my- my optimal path,
I can- I can get rid of it,
I can ignore it, okay. Yes.
We also ignore 3 if, uh,
the beta is equal to alpha,
if we already have something else, is that not the same thing?
Yeah. So- so we're ignoring 3 in a different way.
I- I- so- so yeah- so we're ignoring the value
of 3 because this is already encoded here.
But we're ignoring the subtree of 5,
like I'm not exploring it.
Like I need to explore things after the 3 already,
because I- like- like- like with the 3 we already had an overlap with the Beta.
So you're looking at- with the b value- we are looking at
the overlap between your upper bound of min node and lower bound of max node.
So that interval is the interval you're making sure it still has values in it.
One example of, uh, if the two or three extend;
do you just ignore them anyway
because you already had something else that's- that's [OVERLAPPING] is that optimal?
Yeah, yeah. So, uh, yeah, I think so.
Yeah, so- so if you already have like,
if 3 were 2, is that what you're saying?
Yeah so- so- th- you want to have non-trivial intervals basically, yes.
Yeah. So like if- if- if- it is the same value- you still- yeah,
you don't have non-trivial intervals.
And- and yeah question.
I was wondering how we got 6 and 8 and 3.
Oh, this is an example of that, imagine somehow [NOISE].
But we- we will talk about some examples whe- where we get them.
So I'll talk about one more example where we actually like get these,
but for now just assume somehow we have found this. Yes.
Um, on the top example,
I don't understand why,
uh, 3 is an upper bound or 2 is a lower bound.
So, um, so the- the actual values,
um, I'm not showing a full example here.
So the actual values are coming from somewhere that
I'm not talking about yet but- [OVERLAPPING]
[inaudible]
Oh, the one at the top. Okay, oh sorry.
Yeah. So the one at the top right?
So- so this is a min node,
a min node, this is a max node, right?
So at my min node,
I found out that minimum between 3 and 5 is 3, right?
So max node is maximizing between 3 and a bunch of other things.
That- that's what it is supposed to do, right?
So it's maximizing between 3 and a bunch of other things,
then it's at least going to be 3.
It's not going to be 2, there is no way for it to be
2 or it's not going to be 0, right?
Because it's- it's going to take maximum of 3 and something else.
So that's why I'm saying,
well this value whatever I'm going to get at this max node is going
to be greater than or equal to 3. Does that make sense?
So now I come down here, and I see like,
I see this 2; this is a min node.
So the value here is going to be the minimum between
2 and whatever is down this tree, right?
So it is going to be at least,
uh, I'm very bad with that, the least, and the most.
It's going to be- [LAUGHTER] it's going to be 2 or lower. Let me just use that.
So- so what we are getting here is going to be 2 or lower, right?
So I'm either going to get 2 or 1 or 0 or- or all that.
And that's the value that's going to be pushed up here, right?
So that's the value that's going to go down here,
it's going to be a value that is 2 or lower.
So if I'm maximizing between 3 and something that is 2 or lower,
then 3 is enough.
And I can like, kind of figure that out based on
these intervals and don't look at this side of the tree.
Like- like once I've- I've seen these two,
I already feel there is no- no trivial interval
between a value that's greater than 3 and a value that's less than 2.
So I can just not worry about stuff down there.
Okay. All right.
So one quick other implementation,
I think is we talked about these A's,
A values and B values.
You can- on- keep track of only one value.
And that would be this Alpha value and Beta value,
where Alpha value is just -I'm going to illustrate it here.
Alpha value- let me get it right.
So Alpha of s is the max of a_s for all these s primes that are less than s. Yeah.
So- so- is what this basically says is,
remember like when we saw 3 we said, "Well,
that's already included, like we already knew that."
That's kind of the same idea.
So Alpha of s is just going to be one value.
In this case, it's just going to be 6,
because like when I see 3,
like I don't really care about that 3, right?
Like I already know I'm greater than 6,
knowing that I'm greater than 3 is not adding anything.
So we keep track of one value;
Alpha of a- al- Alpha of s. In this case,
Alpha of s is just equal to 6.
And then similar thing for Beta.
We're going to keep track of Beta of s,
and Beta of s is just minimum of b_s's.
And then, what I'm writing here is just the ordering of the nodes that you have seen.
So- so Beta of s is 5.
And then, you're looking at the intervals like Alpha of s,
uh, and s- Alpha of s and above,
and Beta of s and below.
And if those intervals don't have any trivial intersections,
then you can- you can prune part of the tree.
Okay. So- so this is more of an- an implementation thing instead of keeping
track of all these a_s's and b_s's just keep one number,
one Alpha and one Beta.
Okay. All right.
Okay. So let's look at one- one other example.
Uh, so all right.
So I'm going to just do this example real quick.
Okay. So you're going to start from some top node,
we're gonna go to this node,
this is a min node between 9 and 7.
Between 9 and 7 right?
So it's a min node,
I'm going to get this guy; 7.
I'm going to realize that
this max node is going to be something that's at least 7, right?
It's going to be something that's greater than or equal to 7.
So my Alpha of s is going to be 7 right now.
I know whatever value I'm going to get is going to be 7 or higher.
Whatever value this start node is going to get,
It's got to be 7 or higher, okay?
So now I come down here, I am at a min node.
I see a 6 here, right?
I go here, it's a min node,
so whatever we get here,
is going to be less than or equal to 6, right?
So it's going to be 6 or something that is lower.
That tells me my Beta of s is equal to 6.
That tells me whatever I am getting in that min node is going to be 6 and lower.
That doesn't have any intersections with my Alpha of
s. So I can just not do anything about this- this branch.
Like I don't- like I don't need to go
over like- like I know like all these other things like,
I can kind of ignore like this whole branch.
Okay. All right.
So now I go back up.
I go down here, I'm at the min node.
So remember the way we were computing these Beta values,
were based on the nodes that we have seen previously.
So I have a new Beta now because I'm done with this branch, right.
So I- I need to get here.
Here I have a min between- what is it?
8. This Is 8?
8 and 3. So okay.
So- so I see my- maybe let me just write 8.
I see my 8 here,
it's a min node, so it's going to be less than or equal to 8.
So my new Beta value is going to be 8.
My Alpha is still 7 because that's for my top node.
So its 8 or lower.
We do have an interval,
overlapping interval, 7 to 8.
Everything is good. So I actually need to go and see what this value is.
This value is 3,
so I get 3 here,
or like, it's exactly equal to 3.
So that updates my Beta from 8 to 3.
We have already explored that part of the tree anyways,
but 3- you don't have an interval.
If there were a bunch of things below this three like I- I like when I somehow decide,
like I wouldn't need to explore it,
but we don't really have that.
And then we just find that our optimal value is 7,
so we just return 7, okay.
And we didn't explore this giant middle part of the tree.
Okay. One more slide and then I'll- two more- two more quick, one quick idea.
Okay. So [LAUGHTER] All right.
So the order of things actually matters,
so- so that the only thing I want to mention about
this idea of pruning is- is the order of things matters.
So- so when we look at this example,
remember we didn't explore anything about the 10,
because we already knew that this value needs to be greater than or equal to 3.
These are my buckets, right?
If I swap the buckets,
like if I just swap the order of buckets,
I move the 2-10 bucket to this side,
3-5 bucket to the other side,
I wouldn't be able to do that.
I actually need to explore the whole tree,
because my Alphas and Beta wouldn't have the same properties.
So the order that you are putting things on the tree
actually matters and- and you should care about that.
Um, so worse case scenario,
our ordering is terrible,
so we need to actually go over the full tree that's order of b to the 2d.
That's the worst-case scenario.
There is this best ordering where you don't explore like half of it.
So- so you can- like if you- if you had- if you have
a tree where you can explore up to depth 10,
then with the best ordering,
you can actually explore up to depth like 20.
So- so that's a huge improvement actually.
Uh, so the best ordering is going to be order of b to the
d. And then random ordering turns out to be pretty okay too,
so random ordering would be order of b to
the 2 times 3 fourth times d. So even if you had the random ordering,
it would be better than the worst case scenario.
And then, well, how do you figure out what is a good bordering- ordering?
Well, we can have this evaluation function.
Remember you- you are computing the evaluation function and- and what you can do is,
you can order, uh, your- so for max nodes,
you can order the successors by decreasing evaluation function,
and therefore, min nodes you can order the successors by increasing the evaluation functions.
That allows you to prune as much things as possible. All right.
So with that I'll see you guys next lecture talking about TD learning.
 Let's start guys. Okay, so,
uh, we're gonna continue talking about games today.
Uh, just a quick announcement,
the project proposals are due today.
I think you all know that. Um, all right, let's co-
Tomorrow.
Tomorrow. You're right [LAUGHTER].
Tomorrow [LAUGHTER] Just checking.
[LAUGHTER] Yeah. Today is not Thursday.
Yeah. [LAUGHTER] Tomorrow.
For a second, I thought it's Thursday.
Um, all right, so let's talk about games.
Uh, so we started talking about games last time.
Uh, we formalized them.
Uh, we talked about, uh,
non- we talked about zero-sum two-player games that were turn-taking, right?
And we talked about a bunch of different strategies to solve them,
like the minimax strategy or the expectimax strategy.
Uh, and today we wanna talk a little [NOISE] bit about learning in the setting of games.
So what does learning mean?
How do we learn those evaluation functions that we talked about?
And then, er, towards the end of the lecture,
we wanna talk a little [NOISE] bit about
variations of the game- the games we have talked about.
So, uh, how about if you have- how about the cases
where we have simultaneous games or non-zero-sum games.
So that's the, that's the plan for today.
So I'm gonna start with a question that
you're actually going to talk about it towards the end of the lecture,
but it's a good motivation.
So, uh, think [NOISE] about
a setting where we have a simultaneous two-player zero-sum game.
So it's a two-player zero-sum game similar to the games we talked about last time,
but it is simultaneous.
So you're not ta- ta- taking turns,
you're playing at the same time.
And an example of that is rock, paper, scissors.
So can you still be optimal if you reveal your strategy?
So lets say you're playing with someone.
If you tell them what your strategy is,
can you still be optimal?
That's the question. Yes.
[inaudible] It's a small [NOISE]
enough game space for- if they know exactly [NOISE] what you're going to play,
[NOISE] you won't be successful if you- for
a zero-sum real-time simultaneously being the larger scale,
I think you could still be successful if that approach
is like superior to the other approach taken. [NOISE]
So it's not- so, so,
so the answer was about the size of the game.
So rock, paper, scissors being small versus,
versus not being small.
So, so the question is more of a motivating thing.
We'll talk about this in a lot of details towards the end of the class.
It's actually not the size that matters.
It's the type of strategy that you play that matters,
so just to give you an idea.
But, like, the reason that we have put this I guess at,
at the beginning of the lecture is intuitively when you think about this,
you might say, "No.
I'm not gonna tell you what my strategy is, right?
Because if I say, I'm gonna play it, like,
scissors, you'll know what to play."
But th- this has
an unintuitive answer that we are gonna talk about towards the end of the lecture.
So just more of a motivating example.
Don't think about it too hard. All right.
So, so let's do a quick review of games.
So, um, so last time we talked
about having an agent and opponent playing against each other.
So, uh, and we were playing for the agent,
uh, and the agent was trying to maximize their utility.
So they were trying to get this utility.
The example we looked at was, uh,
agent is going to pick bucket A, bucket B,
or bucket C. And then the opponent is going to pick a number from these buckets.
They can either pick minus 50 or 50,
1 or 3 or minus 5 or 15.
And then if you want to maximize your,
your utility as an agent,
then you can potentially think that your opponent [NOISE] is trying to,
trying to minimize your utility,
and you can have this minimax game, kind of,
playing against each other and, and,
and based on that, uh, decide what to do.
So we had this minimax tree and based on that,
the utilities that are gonna pop up are minus 50,
1 and minus 5.
So if your goal is to maximize your utility, you're gonna pick bucket B,
the second bucket, because that's the best thing you can do,
assuming your opponent is a minimizer.
So, so that was kind of the setup that we started looking at.
And the way we thought about, uh,
solving this game by- was by writing a recurrence.
So, so we had this value.
This is V which was the value of a minimax, uh,
at state S. And if you're at the utility,
er, so if you're an- at an end state,
we are gonna get utility of S, right?
Like if you get to the end state,
we get the utility because we get the utility only at the,
at the very end of the game.
And if the agent is playing,
we- the recurrence is maximize V of the successor states.
And if the opponent is playing,
you wanna minimize the value of the successor states.
And so that was the recurrence we started with, and,
and we looked at games that were kind of large like the game of chess.
And if you think about the game of chess,
the branching factor is huge.
The depth is really large.
It's not practical to u- to do the recurrence.
So we, we started talking about ways to- for speeding things up, and,
and one way to speed things up was this idea of using an evaluation function.
So do the recurrence but only do it until some depth.
So don't go over the full tree.
Just do it until some depth,
and then after that, just call an evaluation function.
And hopefully your evaluation function which is kind of this weak estimate of
your value is going to work well and give you an idea of what to do next.
Okay. So, so instead of the usual recurrence,
what we did was we decided to add this D here, um,
this D right here which is the depth that un- until which we are exploring.
And then we decrease the value of depth,
uh, after an agent and opponent plays.
And then when depth is equal to 0,
we just call an evaluation function.
So intuitively if you're playing chess, for example,
you might think a few steps ahead,
and when you think a few steps ahead,
you might think about how the board looks like and
kind of evaluate that based on the features that,
that, that board has and based on that, you might,
you might decide to take various actions.
So similar type of idea.
And then the question was, well,
how are we gonna come up with this evaluation function?
Like where is this evaluation function coming from?
Uh, and, and then one idea that,
that we talked about last time was it can be handcrafted.
The designer can come in and sit down and figure out what is a good evaluation function.
So in the game of chase- che- and chess example is,
you have this evaluation function that can depend on the number of pieces you have,
the mobility of your pieces.
Maybe the safety of your king,
central control, all these various things that you might care about.
So the difference between the number of
queens that you have and your opponents number of queens,
these are things, these are features that you care about.
And, and potentially, a designer can come in and say, "Well,
I care about nine times more than I care about how many pawns I have."
So, so the hand- like you can actually hand-design these things and,
and write down these weights about how much you care about these features.
Okay. So I'm using terminology from the learning lecture, right?
I'm saying we have weights here and we have features here,
and someone can come and just handcraft that.
Okay. Well, one other thing we can do is instead of handcrafting it,
we could actually try to learn this evaluation function.
So, so we can still handcraft the features, right?
We can still say, "Well, I care about the number of
kings and queens and these sort of things that I have,
but I don't know how much I care about them.
And I actually wanna learn that evaluation function.
Like what the weights should be."
Okay. So to do that,
I can write my evaluation function,
eval of S, as,
as this V as a function of state parameterized by, by weights Ws.
And, and my goal is to figure out what these Ws,
what these weights are.
And ideally I wanna learn that from some data.
Okay. So, so we're gonna talk about how learning is applied to these game settings.
And specifically the way we are using learning for these game settings is to just
get a better sense of what this evaluation function should be from some data.
Okay. So, so the questions you might have right now is,
well, how does V look like?
Where does my data come from?
Because if I, if you know where your data comes from and your, your V is,
then all you need to do is to come up with a learning algorithm
that takes your data and tries to figure out what your V is.
So, so we're gonna talk about that at the first part of the lecture.
Okay. And, and that kind of introduces to this, this,
um, temporal difference learning which we're gonna discuss in a second.
It's very similar to Q-learning.
Uh, and then towards the end of the class,
we will talk about simultaneous games and non-zero-sum games. Okay.
All right. So, so let's start with this V function.
I just said, well, this V function could be
parameterized by a set of weights, a set of w's,
and the simplest form of this V function is to just write it as
a linear classifier as a linear function of a set of features, w's times Phi's.
And these Phi's are the features that are hand-coded and someone writes them.
And then- and then I just want to figure out what w is.
So this is the simplest form.
But in general, this, this V function doesn't need to be a linear classifier.
It can actually be any
supervised learning model that we have discussed in the first few lectures.
It can be a neural network.
It can be anything even more complicated than neural network that just does regression.
So, so we can- basically,
any model you could use in supervised learning could be placed here as,
as, as this V function.
So all I'm doing is I'm writing
this V function as a function of state and a bunch of parameters.
Those parameters in the case of linear classifiers are just
w's and in the case of the neural network,
there are w's and these v's in this case of what one layer neural network.
Okay. Or multilayer, actually. Yeah, one way.
All right. So let's look at an example.
So let's think about an example and I'm going to focus on
the linear classifier way of looking at this just for simplicity.
So, um, okay, let's pick a game.
So we're going to look at backgammon.
So this is a very old game.
Uh, it's a two-player game.
The way it works is you have the red player and you have the white player,
and each one of them have these pieces.
And what they wanna do is they want to move all their pieces from
one side of the board to the other side of the board. It's a game of chance.
You can actually, like, roll two dice and based on the outcome of your dice,
you move your pieces various,
various amounts to, to various columns. Uh, there are a bunch of rules.
So your goal is to get all your pieces off the board.
But if you have only, like,
one piece and your opponent, like,
gets on top of you, they can push you to the bar and you have to, like, start again.
Um, there are a bunch of rules about it.
Read it, read about it on Wikipedia if you're interested.
But you are going to look at a simplified version of it.
So in this simplified version,
I have Player O and player X,
and I only have four columns.
I have column 0, 1, 2, and 3.
And in this case, I have four of each one of these players and,
and the idea is,
we want to come up with features that we would care about in this game of backgammon.
So, so what are some features that you think might be useful?
Remember the learning lecture.
How did we come up with, like, feature templates? Yes.
Currently, still bound with the [inaudible].
So maybe like the location of the X's and O's. The number of them.
Yeah. Yeah. So one idea is you have all this knowledge about the board,
so maybe we should, like, care about the location of the X's.
Maybe we should care about like where the O's are,
how many pieces are on the board,
how many pieces are off the board.
So similar type of way that we- we've come up with features in the first few lectures.
We were basically, we would do the same thing.
So a feature template- set of feature templates could look like this, like,
number of X's or O's in column- whatever column being equal to some value or,
uh, number of X's or O's on the bar.
Maybe fraction of X's or O's that are removed, whose turn it is.
So these are all like potential features that we could use.
So for this particular board,
here are what those features would look like.
So if you look at number of O's in column zero 0 to 1, that's equal to 1.
Remember we were using these indicator functions to be more general.
So, so like here, again, we are using these indicator functions.
You might ask number of O's on the bar that's equal to 1,
fraction of O's that are removed.
So I have four pieces.
Two of them are already removed. So that's one-half.
Number of X's in column 1 equal to 1, that's 1.
Number of X's in column 3 equal to 3, that's 1.
It's O's turn. So that's equal to 1.
Okay. So, so we have a bunch of features.
These features, kind of, explain what the sport looks like or how good this board is.
And what we wanna do is we wanna figure out what,
what are the weights that we should put for each one of
these features and how much we should care about, uh, each one of these features.
So, so that is the goal of learning here.
Okay. All right. So okay.
So, so that was my model. All right.
So far, I've talked about this V S of w. I'm-
I've defined it as a linear classifier- as a linear predictor.
W's times features.
And now, the question is where do I get data?
Like where and because if I'm doing learning,
I got to get data from somewhere.
So, so one idea that we can use here is we can try to generate
data based on our current policy pi agent or pi opponent,
which is based on our current estimate of what V is.
Right. So currently, I might have some idea of what this V function is.
It might be a very bad idea of what V is, but that's okay.
I can just start with that and starting with,
with that V function that I currently have,
what I can do is I can,
I can call arg max of V over successors of s and a to get a policy for my agent.
Remember this was how we were getting policy in a mini-max setting.
Policy for the opponent is just argument of
that V function and then when I call these policies,
I get a bunch of actions.
I get a sequence of, like,
states based on, based on how we are following these policies,
and that is some data that I can actually go over and try to make my V better and better.
So, so that's kind of how we do it.
We call these policies.
We get a bunch of episodes.
We go over them to make things better and better.
So, so that's, kind of, the key idea.
Um, one question you might have at this point is, um,
is this deterministic or not, like,
do I need to do something like Epsilon-Greedy.
So in general, you would need to do something like Epsilon-Greedy.
But in this particular case,
you don't really need to do that because we have to get- we have this die that,
that you're actually rolling the dice.
And by rolling the dice,
you are getting random different- different random path that,
that we might take- so that might take us to different states.
So we, kind of, already have this,
this element of randomness here that does some of the exploration for us.
And you just mean like unexplored probability?
Yes. So my Epsilon-Greedy,
what I mean here is do I need to do extra exploration?
Am I gonna get stuck like in a particular set of states if I don't do exploration?
And in this particular case,
because we have this randomness,
we don't really need to do that.
But in general, you might imagine having
some sort of Epsilon-Greedy to take us explore a little bit more.
Okay. So then we generate episodes and then from these episodes, we want to learn.
Okay. These episodes look like
state action reward states and then they keep going until we get a full episode.
One thing to notice here is,
is the reward is going to be 0 throughout
the episode until the very end of- end of the game.
Right. Until we end the episode and we
might get some reward at that point or we might not.
Uh, but, but the reward throughout is
going to be equal to 0 because we are playing a game.
Right. Like we are not getting any rewards at the beginning.
And if you think about each one of these small pieces of experience; s, a, r,
s prime, we can try to learn something from each one of these pieces of experience.
Okay. So, so what you have is you actually go on board maybe.
What you have here is you have a piece of experience.
Let's call it s, a.
You get some reward. Maybe it is 0.
That's fine if it is 0.
And you go to some s prime through that.
So s, take an action, you get a reward.
Maybe you get a reward.
You go to some s prime from that and you have some prediction.
Right. Your prediction is your current, like,
your current, um, V function.
So your prediction is going to be this V function and add
state s parameterized with W. And this is what you already,
like, you, kind of, know right now.
This, this is your current estimate of what V is.
And this is your prediction.
I'm writing the prediction as a function of w. Right.
Because it depends on w. And then we had a target that you're trying to get to.
And my target, which is kind- kind of acts as a label,
is going to be equal to my reward,
the reward that I'm getting.
So it's kind of, the reward- so if you look at this V of s and w, well,
it's kind of close-ish to reward plus,
I'm gonna write discount factor,
Gamma V of s prime, w. All right.
So, so my target the thing that I'm trying to like get to is the reward
plus Gamma V of s prime, w, okay?
So we're playing games,
in games Gamma is usually 1.
I'm gonna keep it here for now but I'm gonna drop it at some point,
so you don't need to really worry about Gamma.
And then one other thing to notice here is,
I'm not writing target as a function of w
because target acts kind of like my label, right?
If I'm, if I'm trying to do regression here,
target is my label, it's kind of the ground truth thing that I'm trying to get to.
So I'm gonna treat my target as just like a value,
I'm not writing it as a function of w, okay? All right.
So, so what do we try to do usually,
like when you are trying to do learning?
We have prediction, we have a target,
what do I do? Minimize the- your error.
So what is error? So I can write my error as potentially a squared error.
So I'm gonna write one-half of prediction of w,
minus target squared, this is my squared error.
I want to minimize that.
So with respect to w, okay?
How do I do that? I can take the gradient.
What is the gradient equal to?
This is simple, right?
2 reduced, 2 gets canceled.
Gradient is just this guy, prediction of w,
minus target, times the gradient of this inner expression.
The gradient of this inner expression with respect to w is the gradient of
prediction with respect to w minus 0 because target is,
I'm treating it as a number, okay?
Let me move this up.
So now I have the gradient.
What algorithm should I use?
I can use gradient descent. All right.
So I'm going to update my w. How do we update it?
I'm gonna move in the negative direction of my gradient using some learning rate Eta,
uh, times my gradient.
My gradient is prediction of w minus target
times gradient of prediction of w with respect to w. All right.
So that's actually what's on this slide.
So the objective function is prediction minus target squared.
Gradient, we just took that,
it's prediction minus target times gradient of prediction.
And then the update is just this,
this particular update where we move in the negative direction of the gradient.
This is, this is what you guys have seen already, okay. All right.
So so far so good.
Um, so this is the TD learning algorithm.
This is all it does. So temporal difference learning,
what it does is it picks like these pieces of experience;
s, a, r, s prime,
and then based on that pieces of experience,
it just updates w based on this gradient descent update,
difference between prediction and target times the gradient of V, okay?
So what, what happens if I have,
if I have this, this linear function,
maybe let me write- let me write this in the case that I have a linear, linear function.
So what if my V of sw is just equal to w dot phi of s,
yeah phi of s. So what happens to my update?
Minus Eta. What is prediction?
w dot phi of s, right?
w dot phi of s. What is target?
We defined up it there, it's the reward you're getting- the immediate reward
you're getting plus Gamma times V of s prime, w,
which is w dot phi of s prime times gradient of your prediction which is what, phi of s, okay?
So I just, I just wrote up this indicates of a linear predictor. Yes.
With Q learning, what are the differences between the two?
Yeah, so this is very similar to Q learning.
There are very minor differences that you'll
talk about actually at the end of this section,
comparing it to Q learning. All right.
So, so I wanna go over an example,
it's kind of like a tedious example but I think it
helps going over that and kind of seeing why it works.
Especially in the case that the reward is just equal to 0 like throughout an episode.
So it kinda feels funny to use this algorithm and make it work but it works.
So I want to just go over like one example of this.
So I'm gonna show you one episode starting from S1 to some other state.
And, and I have an episode I start from some state,
I get some features of that state.
Again, these features are by just evaluating those han- hand coded features.
And I'm just going to start,
what w should they start with?
0, let me just initialize w to be equal to 0, okay, right?
How do I update my w?
Me- let me let me just write it in this.
So, so this is I want to write it in
a simple for- not a simpler form but just another form.
So w the way we're updating it is,
the previous w minus Eta times prediction minus target,
I'm gonna use p and t for prediction minus the target,
times phi of s. Okay,
this is the update you're doing, okay?
Uh, yeah, that's right.
Okay. So, so what is my prediction?
What is my prediction? w dot t of s?
0. What is my target?
So for my target I need to know what state I'm ending up at.
I'm gonna end up at 1, 0 in this episode and I'm gonna get a reward of 0.
So what is my target?
My target is reward, which is 0,
plus w times phi of s prime,
that is 0 because w is equal to 0.
So my target is equal to 0.
My p minus t is equal to 0.
So p minus t is equal to 0,
this whole thing is 0,
w stays the same.
So in the next kind of step, w is just 0, okay?
I'm gonna move forward.
Um, so what is prediction here?
0 times 0, prediction is 0. What is target?
I haven't done 0 because I haven't got any- anything,
any reward yet, where do I end at? I end up at 1, 2.
So yeah, so target is going to be a reward,
which is 0 plus 0 times,
whatever state of phi of s prime that I'm at,
so that's equal to 0. p minus t is equal to 0,
it's kind of boring [LAUGHTER].
So at this point,
w hasn't changed, w is equal to 0.
What is my prediction?
Prediction is equal to 0, that's great.
What is target equal to?
So I'm gonna end up in an end state where I get 1, 0 and I get a reward of 1.
So this is the first time I'm getting a reward.
What should my target be?
My target is reward 1 plus 0 times 1, 0 which is 0,
so my target is 1.
So what this tells me is,
I'm predicting 0 but my target is 1,
so I need to push my w's a little bit up to actually address the fact that this is,
this is, this is equal to 1.
So p minus t is equal to minus 1.
So I need to do an update.
Maybe I, I'll do that update here.
So how am I updating it?
So I'm doing, starting from 0, 0 minus, uh,
my Eta is 0.5, that's what I allowed it- like I put it- I defined it to be,
my prediction minus target is minus 1.
What is phi of s, phi of s is 1, 2, right?
So what should my new w be?
What is that equal to? 0.5 and then 1.
All right, so I'm just doing arithmetic here.
So my due- new w is going to become 0.5 and 1 at the end of this one episode.
So I just did one episode, one full episode,
where w is worth 0 throughout and then at the very end when I got a reward,
then I updated my w because I realized that
my prediction and target were not the same thing, okay?
So now I'm gonna, I'm gonna start
a new episode and the new episode I'm starting is going to start with this particular w,
and in the new episode even though the rewards are going to be 0 throughout,
so like we are actually going to update our w's. Yes, question?
If you use, uh, two questions.
If you use like, uh, initialize rates do not be
zeros which you update throughout instead of just to the end.
Yeah.
Okay and section two,
so S4 and S9 are the same future of activities but you said S4 is S9 [OVERLAPPING].
Uh, this is a made up example,
[LAUGHTER] so don't think about this example too much though.
Well, is it that possible to have, an end state and not end state have the same feature vector, or no? If you have the same feature vector in the same state-
It, it is possible to have, yeah,
the, the most of the states to have the same features, right.
You could have, like I said up here.
Depends on what sorts of feature, you can could,
could use like really not representative features.
Like if you really want S4 and s- S9 to,
to differentiate between them,
you should pick features that differentiates between them.
But if there were kind of the same and have the same sort of characteristics,
it's fine to have feature that gives the same value.
Like, like we have different [inaudible].
As one, uh, entry that's always isn't [inaudible] like instead of 1, 2,
we have 1, 0 leading to the,
the final weight then the weight corresponding to that.
Is going to- [OVERLAPPING] Yeah. It will never converge.
And that kind of tells you that that entry in your feature vector,
you don't care about that, or it's always,
like, it, it's always staying the same.
If it is always 0, it doesn't matter like what the weight of that entry is.
So in general, you wanna have features that are differentiating and,
and you're using it in some way.
So for the second row,
I'm not gonna write it up cause that takes time.
[LAUGHTER] So, uh, so okay,
so let's start wi- with a new episode.
We started S1 again but now I'm starting with this new W that I have.
So I can compute the prediction,
the prediction is 1.
I can compute my target it's 0.5.
And what we realize here is we overshoot it.
So before, our prediction was 0,
target was 1, we are undershooting.
We fix our Ws, but now we're overshooting.
So we need to fix that. Yes.
Uh, a little verification on the relationship between the features and the weights.
Uh, they always have to be the same dimension,
and what should we be thinking about that
would make a good feature for updating the weights specifically, like-
So, uh, okay so first off, yes,
they need to be always in the same- in dimension cause you are doing this,
um, dot-product between them.
Um, the feature selection, um,
you don't necessarily think of it as,
like how am I updating the weights,
you think of the feature selection as is it representative of how good my board is.
Is it, for example in the case of Backgammon,
or is it representative of, uh,
how good I am navigating, uh, so,
so it should be a representation of how good your state is,
and then it's- yeah, it's usually like hand designed, right.
So, so i- i- it,
it's not necessarily- you shouldn't think of it as how is it helping my weights,
you should think of it as how is it representing how good my state is.
How is that also, like, thinking of the blackjack example,
if you have a threshold of 21 and then you have a threshold of 10, uh,
if you're using the same feature extraction for both,
how does that affect the generalized ability of the model, the agent?
Yeah, so, so you might choose two,
two different features and one of them might be more like so,
so there is kind of a trade-off, right?
You might get a feature that actually differentiates between different states very well,
but then that, that makes learning longer,
that makes it not as generalizable,
and then at the end- on the other hand,
you might get a feature that's pretty generalizable but,
but then it might not do these specific things
that you would wanna do or these differentiating factors about it.
So, so picking features,
it's, it's an art, right, so.
[LAUGHTER] All right. So lemme,
lemme move forward cause we have a bunch of things coming up.
Okay, so I'll go over this real quick then.
So we have the W's, right.
So, so we now update the W based on this new value, um,
and kind of similar thing, you have a prediction,
you have a target, you're still overshooting,
so, so you still need to update it.
And then once you update it to 0.25 and 0.75 then it kind of stays there, and you are happy.
Okay. All right so,
so this was just an example of TD learning but this is
the update that you have kind of already seen, right?
And then a lot of you have pointed out that this is,
this is similar to Q-learning already, right?
This is actually pretty similar to update, um, it's,
it's very similar, like we have these gradients,
and, and the same weight that we have in Q-learning.
And, and we are looking at the difference between prediction and target,
same weight that we are looking at in Q-learning,
but there are some minor differences.
So, so the first difference here is that Q-learning operates on the Q function.
A Q function is a function over state and actions.
Here, we are operating on a value function, right?
On V. And V is only a function of state, right?
And, and part of that is,
is actually because in the setting of- in setting of a game,
you already know the rules of the game.
So we kind of already know the actions.
You don't need to worry about it as much the
same way that if you are worrying about it in Q-learning.
The second difference is,
Q-learning is an off-policy algorithm.
So, so the value is based on this estimate
of the optimal policy which is this Q opt, right?
It's based on this optimal policy.
But in the case of TD learning, it's an on-policy,
the value is based on this exploration policy which is based on a fixed Pi,
and sure you're updating the Pi,
but you're going with whatever Pi you have and, and,
and kind of running with that and keep updating it.
Okay, so that's another difference.
And then, finally like in Q-learning,
you don't need to know the MDP transitions.
So you don't need to know this transition function as transition from s, a to s-prime.
But in the case of TD learning,
um, you need to know the rules of the game.
So you need to know how the successor function of s and a works.
Okay. So, so those are some kind of minor differences,
but from like a perspective of,
like how the update works,
it is pretty similar to what Q-learning is, okay?
All right. So, so that was kind of this idea of,
I have this evaluation function,
I wanna learn it from data,
I'm going to generate data from that generated data I'm going to update my W's.
So, so that's what we've been talking about so far.
And the idea of learning- using learning to play games is,
is not a new idea actually.
So, um, so in '50s,
um, Samuel looked at a checkers game program.
So where he wa- he was using ideas from
self-play and ideas from like similar type of things we have talked about,
using really smart features,
using linear evaluation functions to try to solve the checkers program.
So a bunch of other things that he did included adding intermediate rewards.
So, so kind of throughout,
like the to, to get to the endpoint,
he added some intermediate rewards,
used alpha-beta pruning and some search heuristics.
And then, he was kind of impressive,
like what he did in '50s,
like he ended up having this game that was playing, like it was reaching,
like human ama- amateur level of play and he only used like
9K of memory which is like really impressive [LAUGHTER] if you're thinking about it.
So, so this idea of learning in games is old.
People have been using it.
In the case of Backgammon, um,
this was around '90s when Tesauro came up with,
with an algorithm to solve the game of Backgammon.
So he specifically used, uh,
this TD lambda algorithm,
which is similar to the TD learning that we have talked about.
It, it has this lambda temperature parameter
that that kinda tells us how good states are,
like as they get far from the reward.
Uh, he didn't have any, any intermediate rewards,
he used really dumb features,
but then he used neural networks which was, uh, kind of cool.
And he was able to reach human expert play, um,
and kind of gave us- and this kind of ga- gave us
some insight into how to play games and how to solve,
like these really difficult problems.
And then more recently we have been looking at the game of Go.
So in 2016, we had AlphaGo, uh,
which was using a lot of expert knowledge in addition to, um,
ideas from a Monte Carlo tree search and then, in 2017,
we had AlphaGo Zero,
which wasn't using even expert knowledge,
it was all, like, based on self-play.
Uh, it was using dumb features,
neural networks, um, and then,
basically the main idea was using Monte Carlo tree search
to try to solve this really challenging difficult problem.
So, um, I think in this section we're gonna talk a little bit about AlphaGo Zero too.
So if you're attending section I think that will be part of that story.
All right so the summary so far is,
we have been talking about parameterizing
these evaluation functions using, using features.
Um, and the idea of TD Learning is,
is to look at this error between our prediction and
our target and try to minimize that error and,
and find better W's as we go through.
So, um, all right so that was learning and, and games.
Uh, so now I wanna spend a little bit of time talking about,
uh, other variations of games.
So, so the setting where we take our games to simultaneous games from turn-based.
And then, the setting where we go from zero-sum to non-zero-sum, okay? All right.
Okay simultaneous games.
So, um, all right so,
so far we have talked about turn-based games like
chess where you play and then next player plays,
and you play, and next player plays.
And Minimax sca- strategy seemed to be
pretty okay when it comes to solving these turn-based games.
But not all games are turn-based, right?
Like an example of it is rock-paper-scissors.
You're all playing at the same time,
everyone is playing simultaneously.
The question is, how do we go about solving simultaneously, okay?
So let's start with, um,
a game that is a simplified version of rock-paper-scissors.
This is called a two-finger Morra game.
So the way it works is,
we have two players, player A, and player B.
And each player is going to show o- either one finger,
or two fingers, and,
and you're playing at the same time.
And, and the way it works is,
is if both of the players show 1 at the same time,
then player B gives two dollars to player A.
If both of you show 2 at the same time,
player B gives Player A four dollars.
And then, if, if you show different numbers like 1 or 2,
or 2 or 1,
then player A has to give o- give three dollars to, to player B.
Okay? Does that make sense?
So can you guys talk to your neighbors and play this game real quick?[BACKGROUND]
All right, so, so what was the outcome?
[LAUGHTER] How many of you are in the case where A chose 1,
then- and B chose 1?
Oh, yeah one. Okay, one pair here.
Uh, A chose 1, B chose 2?
One pair there, is it like four people played.
So A chose 2, B chose 1.
We have, okay two pairs.
And then 2 and 2?
Okay. All right.
So, so you can kind of see like a whole mix of strategies here happening.
And this is a game that you are gonna play and talk
about it a bit and think about what would be
a good strategy to use when you are solving this, this simultaneous game.
Okay. All right so, um.
All right so let's formalize this.
We have player A and player B.
We have these possible actions of showing 1 or 2.
And then, we're gonna use this,
this payoff matrix which,
which represents A's utility.
If A chooses action A and B chooses action B.
So, so before we had this,
this value function, right?
Before, we had this value function, uh,
over, um, over our state here.
Now, we have this value function that is- do we- we shall use here, I'll just use here.
That is again from the perspective of agent A.
So remember like before,
when we were thinking about value function,
we are looking at it from the perspective of the first player,
the maximizer player, the agent.
Now, I'm looking at all of these games from the perspective of a player.
So, so I'm trying to like get good things for A. Yes.
In this case it's not at the end [inaudible] ?
Uh, yeah. And then this is like a one-step game too, right?
So, so like you're just playing and then you see what you get.
So, so we're not talking about repeated games here.
So, so you're playing,
you see what happens, okay?
So, so we have this V,
which is V of a and b.
And, and this basically represent a's utility if agent A plays a and if agent B plays b.
Okay? And this is called, and,
and you can represent this with a matrix and that's why it's called a pay-off matrix.
I'm going to write that pay-off matrix here.
So pay-off matrix.
I'm gonna write A here, B here.
agent A can show 1 or can show 2.
agent B can show 1 or can show 2, right?
If both of us show 1 at the same time,
agent A gets $2.
If both of us show 2 at the same time,
agent A gets $4.
Otherwise agent A has to pay, so agent A gets minus $3.
And again the reason I only like talk about one way is
we are still in the setting of zero-sum games.
So whatever the agent A gets,
agent B gets negative of that, right?
So, so if agent A gets $4,
agent B is, is paying minus $4.
So I am just writing 1B from perspective of agent A.
And this is called the pay-off matrix, okay?
All right. So, uh,
so now we need to talk about what does a solution mean in this setting?
So, so what is a policy in the setting?
And, and then the way we refer to them in this case are as strategies.
So we have pure strategy which is almost like the same thing as,
uh, as deterministic policies.
So a pure strategy is just a single action that you decide to take.
So, so you have things like pure strategies, uh, pure strategies.
The difference between pure strategy and,
and deterministic policies, if you remember,
a deterministic policy again is a function of state, right?
So, so it's a policy as a function of state. It gives you an action.
Here we have like a one move game, right?
So it's just that one action and we call it pure strategy.
[NOISE] We have also this other thing that's called
mixed strategy which is equivalent to, to stochastic policies.
And what a mixed strategy is, is,
is a probability distribution that tells you what's the probability of you choosing A.
So, so pure strategies are just actions a's.
And then you can have things that are called
mixed strategies and they are probabilities of,
of choosing action a, okay?
All right. So here is an example.
So if, if you say, well, I'm gonna show you 1,
I'm gonna always show you 1.
Then the- if you can,
you can write that strategy as a pure strategy,
that says I'm gonna always with probability of 1 show
you 1 and with probability 0 show you 2.
So, so let's say the first column is for showing 1,
the second column is for showing 2.
So, so this is a pure strategy that says always I'm going to show you 1.
If I tell you, well, I always I'm gonna show you 2,
then I can write that strategy like this, right?
With probability 1, I'm always showing you 2`.
I could also come up with a mixed strategy.
Mixed strategy would be I'm going to flip a coin and if I get one-half,
I'm gonna give you- uh,
if I'm- if I get heads,
I'm gonna show you one,
if I get tails, I'm gonna show you two.
And then you can write that as this and this is going to be a mixed strategy.
You could only pull it out to like you're in the si- simultaneous game,
you could just bring chance in and be like half the time,
I'm gonna show you one, half the time I'm gonna show you two based on chance, okay?
Everyone happy with mixed strategies and pure-strategies?
All right. So, so how do we evaluate the value of the game.
So, so remember in, uh,
previous lecture and like in the MDP lecture even,
we were talking about evaluating.
If someone gives me the policy,
how do I evaluate how good that is?
So the way we are evaluating that is again by this value function V. And,
and we are gonna write this value function as a function of Pi A and Pi B.
Maybe I'll just write that up here.
Or I'm gonna erase this 'cause this is a repetitive.
So I'm gonna say a value of agent A following Pi A and agent B following Pi B,
what is that equal to?
Well, that is going to be the setting where, uh,
Pi A chooses action A,
Pi B chooses action B times value of choice A and B,
summing over all possible a and bs.
Okay. So, so let's look at an actual example for this.
So, so for this particular case of Two-finger Morra game,
let's say someone comes in and says I'm gonna tell you what Pi A is.
Policy of agent A is just to always show one.
And policy of agent B is this,
this mixed strategy which is half the time show one,
half the time show, show two.
And then the question is,
what is the value of,
of these two policies? How do we compute that?
[NOISE] Well, I'm gonna use my payoff matrix, right?
So, so 1 times 1 over 2 times the value that we get at 1,
1, which is equal to 2.
So it's 1 times 1,
1 over 2 times 2 plus 0 times 1 over
2 times 4 plus 1 times 1 over 2,
times minus 3, the value that I get is minus 3 plus ah,
0 times 1 over 2 times minus 3.
Okay? And, well, what is that equal to?
What is that equal to? There are two 0s here,
that's minus 1 over 2.
Okay? So I just computed that
the value of these two policies is going to be minus 1 over 2.
And again this is from the perspective of, of,
um, agent A and it kinda makes sense, right?
If agent A tells you I'm gonna always show you 1,
then probably agent- and,
and agent two is following this mixed strategy,
agent A is probably losing, and agent A is losing
minus 1 over 2 based on- based on this strategy, okay? Okay.
So I guess this doesn't seem like we only have this one statement, so it's, we only take one action, in this environment, we have one state, take one action, and that would be the end state.
If we had more than one state,
Would we have that for every single one.
So that opens up a whole set of new questions that you're not discussing in this class.
So that introduces repeated games.
Ah, so you might be interested in looking at what happens in repeated games.
In this class right now we're just talking about this, one step one play.
We're playing like zero-sum game um,
but we're playing like we'll say, rock-paper-scissors and you just play once.
Well you might say well, what happens if you play like ten times then you're building
some relationship and weird things can happen and so,
so that introduces the whole new class of games that we're not talking about here.
All right. So, so the value is equal to minus 1 over 2.
Okay? All right.
So, so that was a game value.
So, so we just evaluated it, right?
If someone tells me it's pi A and pi B, I can evaluate it.
I can know how good pi A and pi B is,
from the perspective of agent A.
Okay? So what do we wanna do like when we solve- when we want to try to solve games?
All we wanna do is from the agent A's perspective,
you wanna maximize this value.
I want to get as much money as possible and its values from my agent A perspective.
So I should be trying to maximize this, agent B should be trying to minimize this.
Right? Like, like think minimax.
So agent B should be min- minimizing this.
agent A should be maximizing this.
That's, that's what we wanna do.
But with the challenge here is we are playing simultaneously,
so we can't really use the minimax tree.
Like if you remember the minimax tree like in,
in that setting we have sequential place and and you could like wait
for agent A to play and then after
that play and that will give us a lot of information,
here we're playing simultaneously.
So what should we do?
Okay so what should we do? So I'm going to assume we can play sequentially.
So that's what I wanna do for now.
So, so I'm going to limit myself to pure strategies.
So maybe I'll, um, I'll come over here.
So right now I'm going to focus only on pure strategies.
I will just consider a setting- very limited setting and see what happens.
And I'm going to assume oh, what if,
what if we were to play sequentially, what would happen?
How bad would it be if we were to play sequentially?
So um, we have the setting where player A plays, goes first.
What do you think? Would you think like if Player A goes first,
Is that better for player A or is that worse for player A?
Worse.
Worse for player A. Okay. So, so that's probably what's gonna happen.
Try that. [LAUGHTER] Okay. So player A was trying to maximize.
Right? This V, player B was trying to minimize, right?
And then each of them have actions of either or showing 1 or showing 2.
This is player A, this is A, this is agent B.
They can show 1, show 1 or 2, right?
If we do one- if we show 1, 1,
player A gets what?
$2? Is that right?
It's 2, right? I can't see the board.
Um, otherwise player A gets minus $3
if you have 2, 2,
player A gets $4.
Right? So okay.
So, so now if,
if we have this sequential setting,
if you're playing minimax,
then player B is going second.
Player B is going to take the minimizer here.
So Player B is gonna be like
this one and in this case player B is going to be like this one.
What should player A do?
Well in both cases player A is getting minus $3.
It doesn't actually matter, player A could do any of them and
player A at the end of the day is going to get minus $3.
Right? And this is a case where player A goes first.
What if player A goes second, second?
Okay? So, so then player B is going first,
player B is minimizing and then player A is maximizing
[NOISE] and we have the same values here.
Okay? So this is,
this is player A going second,
player A going second tries to maximize.
So we'd like to pick these ones.
Player B is, is here.
Player B wants to minimize.
So Player B is going to be like, okay,
if you're going second I'd rather,
I'd rather show you 1,
because by showing you 1 I'm losing less.
If I show you 2,
I'm losing even more. All right.
So, so and then in that setting,
we are gonna get to,
so player A is going to get $2.
Okay? All right.
So that was kind of intuitive
if we have pure strategies,
it looks like if you're going second that should be better.
Okay. So, ah, so going second is no worse.
It's the same or better.
And that basically can be represented by this minimax relationship, right?
So, so agent A is trying to maximize.
So, so in the second case.
[NOISE]
In the second case, um,
we are maximizing second over our actions of V of a and b,
and Player B is going first.
So this is going to be greater than or equal to the case where Player A
is going, uh, first.
Sorry no, not min.
That makes sense. V of a and b.
So I'm gonna just write these things that you're
learning throughout on the side of the board, maybe up here.
So what did we just learn?
We learned, if we have pure strategies,
if we have pure strategies,
all right, going second is better.
That sounds intuitive and right.
[NOISE].
Okay. So far so good.
Okay? So the question that I
wanna try to think about it right now there is what if we have mixed strategies?
What's going to happen if we have mixed strategies?
Are we gonna get the same thing?
Like, if you have mixed strategies is going second better,
or is it worse, or is it the same?
So, so that's the question we're trying to answer.
Okay? So, so let's say Player A comes in,
and Player A says, "Well,
I'm gonna reveal my strategy to you.
What I'm gonna do is I'm going to flip the coin depending on what it comes.
I'm either show- going to show you 1, or I'm gonna show you 2.
That's what I'm gonna tell you,
tell you that's what I'm gonna do."
Okay. So, so what would be the value of the game under that setting?
So the value of the game, uh,
would be, maybe I'll write it here.
So the value of Pi A and
Pi B. Pi A is already this mixed strategy of one-half, one-half, right?
It's going to be equal to Pi- is this- yeah, actually.
All right. So what is that going to be equal to?
It's going to be Pi B times 1, right?
Pi- so it's going to be Pi B,
choosing 1 times one-half.
The probability one-half Agent A is also picking 1.
If it is 1, 1, we're gonna get 2, right,
plus Pi B choosing 1,
Pi A with one-half choosing 1,
and then we're gonna get minus $3 sort of choosing 2.
We're gonna get minus $3,
plus Pi B choosing 2,
times one-half Pi A choosing due- 2.
We're gonna get $4,
plus Pi B choosing 2 times Pi A choosing 1,
and that's minus $3.
So I just, like, iterated all the four options that we can get here, uh,
under the policy of Pi B choosing 1 or 2,
and then Pi A is always just half, right,
because they, they are following this mixed strategy.
So well, what is this equal to?
Uh, that's equal to minus 1 over 2 Pi B of 1,
plus 1 over 2 Pi B of 2.
Okay. So that's the value.
Okay? So, so again, the setting is someone came in,
Agent A came in,
Agent A told me, "I'm following this mixed strategy.
This is gonna be the, the thing I'm gonna do."
What should I do as an Agent B?
What should I do as an Agent B?
You always want to pick 1.
So- okay, so that was too quick.
So you always [LAUGHTER] have to do, do 1.
But why, why is that?
Well, well, if Agent A comes and tells me, "Well,
this is a thing I wanna do," I should try to minimize value of Agent A, right?
So, so what I'm really trying to do as Agent B is to minimize this,
right, because I don't want Agent A to get anything.
So if I'm minimizing this, in some sense,
I'm trying to come up with a policy that minimizes this.
Pi is the probability,
so it's like a positive number.
I've like a positive part and negative part here.
The way to minimize this is to put as much weight as
possible for this side and as little as possible for this side.
So that tells me that never show 2 and always show 1. Does everyone see that?
So, so the best thing that I can do as Agent 2 is
to follow a pure strategy that always shows 1 and never shows 2.
Okay. So this was kind of interesting, right?
Like if someone comes in and tells me, "This is the thing.
This is a mixed strategy I'm gonna follow," I'll have a solution in response to that,
and that solution is always going to be a pure strategy actually.
So, so that's kind of cool.
All right.
So, so this is actually what's happening in a more general case.
I'm gonna make a lot of generalizations in this lecture.
So I'll show you one example I generalize it,
but if you're interested in details of it,
like, we can talk about it offline.
So yeah, so, so setting is for any fixed mixed strategy Pi A.
So, so Pi A told me what their mixed strategy is.
It's a fixed mixed strate- uh, mixed strategy.
What I should do as Agent B is I should minimize that value.
I should pick Pi B in a way that minimizes that value,
and that can be attained by pure strategy.
So the second thing that I've learned here,
is if Player A plays, uh, uh,
plays a mixed strategy, mixed strategy,
Player B has an optimal pure strategy.
And that's kind of interesting.
[NOISE] Right.
Okay. So, so in this case,
also we, we haven't decided what the policies should be yet, right,
like we- we've have started- we've still,
we've still been talking about the setting where Pi A- like
Agent A comes in and tells us what their policy is,
and we know how to respond to it.
It's going to be a pure strategy.
Okay? So now we want to figure out what is this, this policy.
Like what, what should be this mixed strategy actually?
So, so I wanna think of it more generally.
So, so I wanna go back to those two diagrams and actually
modify those two diagrams in a way where we talk about it a little bit more generally.
Maybe- yeah, I'll just modify these.
Okay. So, um, so let's say that- okay,
and, and I'm gonna think about both of the settings.
So let's say it again. Player A is deciding to go first.
Player A is going to follow a mix- a mixed strategy.
So this is all we know,
but we don't know what mixed strategy.
Play- Player A is going to decide to do- to follow mixed strategy.
This is Player A. Player A is maximizing.
Player A is following a mixed strategy.
The way I'm writing that mixed strategy is more
generally saying Player A is gonna show 1 with
probability p and is going to show 2 with
probability 1 minus p. Or generally like some, some p-value.
Okay? And then after that it's Player B's turn.
We have just seen that Player B,
the best thing Player B can do is,
is to do a pure strategy.
So Player B is either 100% is going to pick 1 or 100% is going to pick 2.
Yes?
Player B could really like [inaudible] terms
with the same then like Player B following a mixed strategy.
That would be the best strategy.
You know it's just the same as any pure strategy, does that make sense?
For those terms behind on the blue on the board here right there.
Yeah. Those terms with the same blue terms,
then like Player B can follow any kind of strategy, right?
So the thing is that, that strategies are probabilities, right?
So they are values from 0-1,
and then you kinda always end up with this negative term that you're trying to make as
negative as possible and this positive term
that you are trying to get as positive as possible.
And that's kind of intuitively why you end up with a pure strategy.
And by pure strategy, what I mean is you always
end up like putting as much possible like 1,
like all your probabilities on the negative turn
and nothing on the positive turn because you are trying to minimize this.
So that's kinda like intuitively why you're getting this pure strategy.
One-half and one-half?
So, so you wouldn't get 1. So, so that's what I mean.
So like, you wouldn't ever get like one-half and one-half.
If you get one-half and one-half,
that's a, that's a mixed strategy.
That's not a pure strategy.
And I'm saying you, you wouldn't get a mixed strategy
because you would always end up in this setting that to minimize this,
you end up pushing all your probabilities to this negative term, okay.
All right. So, so, all right,
so let me go back to this.
So- all right.
So we have the setting where Player A goes first.
Player A is following a mixed strategy with p and
1 minus p. Player B is going to follow a pure strategy,
either 1 or 2.
I don't know which one, right?
So, uh, what's gonna happen is if you have 1, 1 and then,
then that is going to give me 2, value 2, right?
So it's 2 times p. I'm trying to write the value here.
Am I writing it right? Is it 2 times p plus?
Yeah. 1 minus p times 3.
Right. So with probability 1 minus p,
this guy is gonna pick 2.
If this guy picks 1, you're gonna get minus 3, minus 3.
Okay? And then for this side,
with probability 1 minus p,
A is going to show 2.
If I'm gonna show 2,
then I'm gonna get 4.
So it's 4 times 1 minus p. And with probability p,
this guy's gonna show 1.
I'm gonna show 2.
So that is minus 3p.
Okay. All right.
So what are these equal to?
So this is equal to 5p minus 3.
That is equal to minus 7p plus 4.
Okay? So, so I'm talking about this more general case.
In this more general case,
Player A comes in.
Player A is playing first, uh,
and is following a mixed strategy but doesn't know what p they should choose.
They're choosing a p and 1 minus p here.
And then Player B has to follow,
uh, a pure strategy.
That's what we decided.
And then under that case,
we either get 5p minus 3 and minus 7p plus 4, okay?.
What should Player B do here?
This is Player B and this min node.
What should Player B do? Which, which- should,
should Player B pick 1 or 2?
It should- player B should pick a thing that minimizes between these two. All right?
So Player B is going to take the minimum of 5p minus 3 and minus 7p plus 4, okay?
What should Player A do?
What should player A do? I'm thinking minimax, right?
So- so when you think about the minimax,
Player A is maxima- maximizing the value.
So Player A is going to maximize the value that comes up here.
So player is going to maximize that and also,
I'm saying Player A needs to decide what P they're picking.
So they're going to pick a P that maximizes that. Is this clear?
[inaudible]
Like these computations? Yeah, so these are the four different,
uh, things in my, uh, payoff matrix.
So I'm saying is,
with probability P, A is going to show me 1, right?
And I'm going to go down this other route where B is also choosing 1.
So if one- like both of us are showing 1,
then I'm going to get 2, right?
So I'm going to get $2.
So that's where the $2 comes from,
times probability P. With probability 1 minus p,
A is going to show me 2.
I'm going to show 1, that's minus $3,
times probability 1 minus p. So,
so that's how and and for this particular branch,
I know the pay off is going to be 5p minus 3.
That makes sense? And then for this side again,
like with probability 1 minus p,
A is going to show me 2.
If it is both of them 2,
I'm gonna get $4.
That's why it's 4 times probability of 1 minus p. With probability P,
A is going to show me 1.
So that's why I'll lose $3,
that's minus 3 times probability p. So that's minus 7p.
Okay. So and then,
and then, the second player,
what they're gonna do is,
they're going to minimize between these two values and they're going to pick 1 or 2.
They're gonna- they're deciding, "Should I pick 1 or should I pick 2?"
And the way they're deciding that is by trying to pick,
pick 1 or 2 based on which one minimizes these two values.
But I'm writing it, uh,
like using this variable p that's not decided yet.
And this variable P is the thing that Player A needs to decide.
So what, what p should Player A decide?
Uh, Player A should decide the p that maximizes this.
So I'm writing like, literally a minimax relationship here.
Okay? All right, so the interesting thing here,
is beside p minus 3,
is some line, right?
With positive slope.
This is 5p minus 3, let's say.
And this minus 7p, plus 4 is another line.
Minus 7p plus 4.
It's another line with negative slope.
What is the minimum of this?
Where is going to be the minimum of this happening?
Minimum of these two lines?
Where they meet each other, right?
This is going to be the minimum of the two.
Okay? So, so the p that I'm s- going to pick,
is going to be actually the p, where,
th- th- the value of p, where these two are equal to each
other and that turns out to be at,
I don't know what it is, 7 over 12 or something.
Actually I don't remember this- what is this value?
Yeah, so it's going to happen at 7 over 12.
And the value of it is minus 1 over 12.
Right? So okay, so let's recap. Okay, what did I do?
So I'm talking about the simultaneous game,
but I'm relaxing it and making it sequential.
I'm saying A is going to play first,
B is playing second.
The thing that's going to happen is A is playing first,
A is deciding to choose a mixed strategy.
So A is deciding to say maybe one half, one half,
but maybe he doesn't wanna say one half,
one half, he wants to come up with some other probabilities.
So the thing A is deciding is,
"Should I pick 1 with probability p and should I
pick 2 with probability 1 minus p and what should that p be?"
So, so what is the probability I should be picking 1?
So that's what A is trying to decide here.
Okay? So whatever A decides with p and 1 minus p,
ends up in two different results and based on them,
B is trying to minimize that.
When B is trying to minimize that,
B is minimizing between these two linear functions.
These two linear functions meet at one point,
that is the point that this thing is going to be minimized and that actually
corresponds to a p-value when A tries to maximize this.
This is I know a little bit- this requires a little bit of thinking,
but any clarification questions?
Any- I see a lot of lost faces, so- [LAUGHTER]
By having, um, [inaudible].
Yeah and then that the- yeah,
the interesting point is exactly right.
Yeah, so A is still by the way losing.
So even in this case,
where A is trying to come up with the best mixed strategy he could do,
the best mixed strategy A is doing is show,
show a 1 with probability 7 over 12 and show 2 with probability 5 over 12.
This comes from here. Even under that scenario, A is losing.
A is losing, minus 1 over 12.
Okay? All right.
Okay. So also, I haven't solved a simultaneous game yet, right?
Like I have talked about the setting where A plays first.
So what if B plays first?
So I'm going to swap this.
What if B plays first?
So A goes second, B plays first.
I'm gonna modify this one now.
Okay, B goes first, A is going second.
B is gone to start- is going to reveal the strategy- his strategy.
The strategy that B is going to reveal, is also again,
I'm gonna with probability p show you 1,
with probability 1 minus p,
show you, show you, uh, 2.
Then A plays, A is trying to maximize.
And A has to play a pure strategy because of that, right?
Like the best thing A can do,
is going to be a pure strategy.
So A is always going to be either showing 1 or
2 and A is deciding which one, but doesn't know yet.
And the values here are going to be exactly the same thing as there.
So they're 5, 5, 5, 5p minus 3,
minus 7p plus 4.
Okay? All right.
So what's happening here?
So, so in this case,
A is playing second.
What A likes to do is A likes to maximize between 5p minus 3 and minus 7p plus 4.
That's what A likes to do.
B is going second, uh,
sorry, B is going first,
so then B has to minimize that and pick a p that minimizes that.
Okay? So these two are exactly
the same two lines but now I'm picking the maximum of them.
The maximum of these two lines end up being exactly the same point as before,
ends up being exactly the same p as
before and giving you exactly the same value as before.
So, so this is also equal to minus 1 over 12.
So what this is telling me is,
if you are playing a mixed strategy,
even if you reveal your best mixed strategy at the beginning, it doesn't matter.
It actually doesn't matter if you're going first or second.
So like in the moral game when you're playing,
if you were playing a mixed strategy and you
would tell your opponent, "This is the thing,
I'm gonna do and this is a mixed strategy," actually and if it was the optimal thing,
like, like it didn't matter like if they know,
know it or not, like you still get the same value.
So again, you get 5p minus 3 and minus 7p plus 4.
And then now you're minimizing or a maximum of these two lines,
maximum of these two lines end up being at the same point and you pick a p that,
that kind of maximizes that and you get the same value.
So this is called the von Neumann's theorem.
So von Neum- like this whole thing that you just,
did over just one example,
there is a theorem about it that says,
for every simultaneous two-player zero-sum game,
with a finite number of actions,
the order of players doesn't matter.
So B is playing second or B is playing first,
the values are going to be the same thing.
If you're minimizing or are maxim- or maximum or min- minimum of that value,
it's going to be the same thing.
Okay? So this is kind of the third thing that we just learned,
which is von Neumann's Theorem,
which says, if- I- I'm writing a modification of a simpler, shorter version of it.
So if playing a mixed strategy,
order of play doesn't matter.
So remember, if you play mixed strategy, your opponent.
And remember, if you play mixed strategy,
your opponent is going to play pure strategy
because this is like this the first point that we had before it.
All right? If you, if you play mixed strategy,
your opponent is going to follow a pure strategy.
Either 1 or 2 with probability 1.
[NOISE] But with probability p,
like, if we're doing like ordering,
like one of the two answers might- will come out,
[inaudible] it'll be either one or two and then in that case, the second [inaudible].
So in this case, yeah.
So, uh, the thing is these two end up being equal.
So the way to- it doesn't,
it doesn't matter because the way for you to
maximize this is going to be the point where the two end up being equal.
So the two branches,
like if you actually plug in p equal to 7 over 12 here,
like these two values end up being equal.
Equal, right? [inaudible]. [OVERLAPPING]
Uh, none [inaudible] actually equal and the reason that they end up being equal
is you are trying to minimize the thing that this guy is trying to maximize.
So you are trying to pick the p that actually makes this thing equal.
So no matter what your opponent does,
like you're gonna get the best thing that you can do.
So, so yeah, like think of it like this.
Okay. So I'm player A,
I'm, I'm still- I still have a choice.
My choice is to pick a p. I want to pick
a p that I'm not gonna wi- like lose as much. What p should I pick?
I should pick a p that makes these choices the same.
Because if I pick a p that makes this one higher than this one,
of course the second player is going to make me lose and then go down a route that's,
that's be- better for the second player.
So the best thing that I can do here is make these two as equal as possible.
So then the second player whatever they choose,
choose one or two,
I guess it's gonna be the same thing, it's gonna be- does that make sense?
So sounds no in expectations,
like you're multiply by p and 1 minus p as you were saying,
like if the [inaudible]. [OVERLAPPING]
So in expectation when- you're saying when you are choosing p?
Yes, so I'm choo- I'm treating p as a variable that I'm deciding, right?
Like p is the thing I gotta be deciding.
So I'm player A, I gotta be citing a p. That's not gonna be too bad for me.
Like let say I would pick a p that doesn't make these things equal.
Let's say, I don't know, I would pick a p that makes this guy I don't
know 10 and this makes this guy 5.
The second player is of course going to make me lose and of course is going
to like pick the thing that's going to be the worst thing for me.
So the best thing I can do is I can make both of them, I don't know, 7.
So it's not gonna be as bad.
So, so that's kind of the idea. All right.
So let me move forward because there's still a bunch things happening. All right.
So, so okay.
So the kind of key idea here is revealing
your optimal mixed strategy does not hurt you which is kind of a cool idea.
The proof of that is interesting.
If you're interested in looking at the notes,
you can use linear programming here.
The reason, kind of the intuition behind it is,
is if you're playing mixed strategy,
the next person has to play pure strategy
and you have n possible options for that pure strategy.
So that creates n constraints that you are putting in for your optimization.
You end up with a single optimization with n constraints, and,
and, and you can use like linear programming duality to actually solve it.
So, so you could compute this using linear programming
and that's kind of the one that's here.
So, so let's summarize what we have talked about so far.
So, so we have talked about these simultaneous games, er, and,
and we've talked about the setting where we have pure strategies,
and we saw that if you have pure strategies,
going second is better.
Right. Going second is better if you are just telling
you what's the pure strategy you're using, right?
So that was kind of the first point up there.
And then if you're using mixed strategies,
it turns out it doesn't matter if you're going first or second.
You're telling them what your mixed- best mixed strategy
is and they're going to respond based on that.
So that's the von Neumann's minimax theorem.
Okay? All right.
So next 10 minutes,
I want to spend a little bit of time talking about non-zero-sum games.
So so far we have talked about zero-sum games,
uh, where it's either minimax, I get some reward.
You get the negative of that or vice versa.
There are also these other things called collaborative
games where we are just both maximizing something.
So, so we both get like money out of it,
and, and that's kinda like a single optimization.
It's a single maximization and you can think of it as plain search. In real life,
you're kind of somewhere in between that,
and, and I want to motivate that by an example.
So, uh, I want to do that b- by this idea of Prisoner's dilemma.
How many of you have heard of Prisoner's dilemma?
Okay. Good. Okay. So the idea of Prisoner's dilemma is you have
a prosecutor who asks A and B
individually if they will testify against each other or not, okay?
If both of them testify,
then both of them are sentenced to five years in jail.
If both of them refuse,
then both of them are sentenced to one year in jail.
If one testifies, then he or she gets out for free and,
and then the other one gets 10 years sentence.
Play with your partner real quick.
[NOISE]
All right. [LAUGHTER] Okay.
Okay, so let's look at the pay off matrix.
So I think you kind of have an idea of how the game works. Is that A or B?
So, uh, so you have two players A or B.
Each one of you have an option.
You can either testify or you can refuse to testify.
So you can- B can testify and A can refuse to testify,
and I am going to create this payoff matrix.
This payoff matrix is going to have two entries now in each one of these, these cells.
And, and why is that? Because we have a non-zero-sum game.
Before, our payoff matrix only had one entry.
Because this was for player A,
player B would just get negative of that.
But now player A and B are getting different values.
So if both of us testify,
then both of us get five years jail, right?
So A gets five years of jail,
B gets five years.
Right? If both of us refuse,
A gets one year of jail,
B gets one year of jail.
One year, one year of jail.
And then if it is a setting where one of us testifies,
the other one refuses, one of us gets 0,
the other one gets 10 years jail.
So if I refuse to testify,
then I get 10 years jail right away and then B gets 0.
And then in this case,
A gets 0 and B gets 10.
Okay? So the payoff matrix is now
going to be for every player we are gonna have a payoff matrix.
So now we have this,
this B value function which is a function of a player.
For policy A and policy B,
will be the utility for one particular player,
because you might be looking at it from perspective of different players.
Okay? So the von Neumann's minimax theorem
doesn't really apply here because we don't have the zero-sum game.
But do you actually get something a little bit weaker,
and that's the idea of Nash equilibrium.
So a Nash equilibrium is setup policies Pi star A and
Pi star B so that no player has an incentive to change their strategy.
So, so what does that mean?
So what that, that means is if you look at the,
the value function from perspective of player A,
value function from perspective of player A at the Nash equilibrium at
Pi star A and Pi star B is going to be greater than or equal to value of,
of any other policy Pi A if you fix Pi B.
Okay and at the same time the same thing is true for value of B.
So for agent B, value of B at Nash equilibrium is gonna be greater
than or equal to a value of B at any other Pi B if if,
if Pi A fixes their policy.
Okay? So, so what does that mean in this setting?
Do we have a Nash equilibrium here?
So let's say I start from here.
I start from A equal to minus 10,
B equal to 0.
Can I get this better?
Can I make this better, or did I flip them I all?
[NOISE] Okay. Flip, right?
0 minus 10, er, minus 10, 0.
Okay. So let's say I start from here.
Can I, can I get this better? Can I make this better?
I start from this cell,
A gets 0 years of jail.
That's pretty good. B gets 10 years of jail.
That's not that great.
So B has an incentive to change that.
Right? Like B has an incentive to actually move in this direction.
Right? So B has an incentive to get 5 years jail instead of 10 years.
Similar thing here.
What if we start here?
A has 1 year of jail,
B has 1 year of jail.
A has an incentive to change this now and get 0 years jail.
B has an incentive to change this and get 0 years jail.
And we end up with this cell.
Where like, we don't have any incentive to change our strategy.
So we have one Nash equilibrium here and that one Nash equilibrium here is,
is both of us are testifying and both of us are getting 5 years jail.
Just kind of interesting because there is like
a socially better choice to have here, right?
Like both of us, like if both of us would refuse,
like we would each get 1 year jail but that's not gonna be a Nash equilibrium.
Okay? All right.
So there's a theorem which is, er,
Nash's existence theorem which basically says if
any finite player game with a finite number of actions,
if you have any finite player game with a finite number of actions,
then there exists at least one Nash equilibrium.
And then this is usually one mixed strategy Nash equilibrium,
at least one mixed strategy Nash equilibrium.
In this case, it's actually a pure strategy Nash equilibrium.
Uh, but, but in general,
there is at least one Nash equilibrium if you have a game of this form.
Okay? All right.
So, uh, so let's look at a few other examples.
Two-finger Morra. What would be the Nash equilibrium for that?
So we just actually solve that using the minimax- von Neumann's minimax theorem, right?
So there would be if you're playing a mixed strategy of 7 over 12 and 5 over 12,
you might, you might kind of modify your Two-finger Morra game and make it collaborative.
So in a collaborative setting, uh,
what that means is we both get $2 or we both get $4 or we both lose $3.
So, so a collaborative Two-finger Morra game,
it's not a zero-sum game anymore and,
and you have two Nash equilibria.
So, uh, you would have a setting where
A and B both of them play 1 and the value is 2,
or A and B both of them play 2 and the value is 4.
Okay? And then Prisoner's dilemma is the case where both of them testify.
We just, we just saw that on the board. All right.
Okay. So summary so far is we have talked about simultaneous zero-sum games.
We talked about this von Neumann's minimax theorem, er,
which has like multiple minimax strategies and a single game value, right?
Like we had a single game value because it was zero-sum.
But in the case of non-zero-sum games, er,
we would have something that's slightly weaker that's
Nash's existence theorem. We would still have multiple Nash equilibria,
we could have multiple Nash equilibria.
Uh, but we have multi- we also have
multiple game values from- depending on whose perspective you are looking at.
So this kind of was just a brief like short introduction to game theory and econ.
There's a huge literature around different types of games,
uh, in game theory and economics.
If you're interested in that, take classes.
And yeah, there are other types of games still like
Security Games and or
resource allocation games that have
some characteristics that are similar to things we've talked about.
If you're interested in any of them,
maybe you can take a look at them,
would be useful for projects.
And with that, I'll see you guys next time.
 Two countries, Austria and,
and- what's the other one?
Hungary.
Hungary, right? [LAUGHTER] Because, speaks Hungarian.
Yeah. So like, Austria and Hungary,
that could be an answer.
Uh, so the way you thought about this problem was- if, if,
if you think about it, it was pretty different from,
from, like normal search problems.
You had this constraint in your head that, oh,
I have two countries that are right next to each other, that's one constraint.
And if you were thinking about, oh,
one of them needs to start with an A.
So you had a bunch of [NOISE] constraints whe- when,
when you think about, you have a bunch of constraints when you
think about a problem that- that's like this.
Uh, and that makes it pretty- like that,
that helps us to use different types of
model that could be pretty different from state-based models.
So this is more of a motivating example.
We are gonna talk about these types of models.
So, so far, we have talked about reflex based models, state-based models.
So we spent some time talking about search problems, MDPs, adversarial games,
and then what the plan is to talk about variable-based models,
specifically constraint satisfaction problems today and, and on Wednesday.
And then we're going to talk about Bayesian networks next week.
So we'll have three lectures on Bayesian networks.
Uh, so what's gonna happen is,
Reed is going to talk, uh,
talk about the CSP,
the second lecture of CSP on Wednesday,
and then Percy is going to be back next week talking about Bayesian networks,
and I will do the third lecture of Bayesian networks.
So it will be whole mix of us,
talking about variable-based models.
You'll see all views of us.
So, so that's, that's the plan.
Okay? All right.
So, um, okay, so going back to our paradigm.
So our paradigm is,
uh, starting with modeling.
So how do we model, uh,
various types of problems,
and then how do we develop inference algorithms?
I tried to answer questions we care about,
objectives we care about based on those models.
And we have been talking about learning a little bit.
So, so if you have these models and they are not full models,
how do we go about learning, learning these models?
So, so here is just a review of what we have talked about so far.
In terms of modeling,
we talked about various frameworks like search problems,
or MDPs, or games.
So these were various frameworks that we had,
and, and we had different objectives.
So we had things like minimum cost path for search problems or we
cared about other things like maximizing the value of policies for,
uh, for MDPs, or, or games.
So this was kind of some of the frameworks we talked about.
And in terms of inference,
we discussed tree-based algorithms,
and we discuss- we discussed graph-based algorithms.
So if you remember, backtracking search was
the simplest most naive thing we tried out for our search problems.
Uh, for, for games,
like we looked at minimax and expectimax,
which was also going down a tree.
And then you can have more,
uh, graph-based type algorithms where,
where you're looking at a recurrence relationship, and,
and examples of that are things like dynamic programming,
uniform cost search, A star.
In terms of MDPs in games,
we looked at value and policy it- iteration.
And then in terms of learning,
we discussed a few types of methods for each one of these frameworks.
We looked at structured perceptron, Q-learning, TD learning.
So, so these are some of the topics that we have talked about so far.
So, so if you're midway through the quarter,
these are all the cool things we have learned so far,
and these are for state-based models.
Okay? So state-based models were kind of cool.
And we had, we had a couple of takeaways from state-based models.
So let's just summarize like two main takeaways from state-based models.
One of the key way- key takeaways was,
was that when we're modeling these,
these state-based models, we had,
we had local relationships.
So, so our model would specify
these local interactions and local relationships that we had between the states.
So for example, if I wanted to go from S to A,
my neighboring state A,
then I would think about what would be the cost of going from S to A.
So I had this local relationship between them.
And the goal was to do inference and an inference
was more trying to like look at a global property.
Can I find the,
the shortest path from some state to some other state in this whole graph?
So, so the idea was,
let's actually model and specify these local relationships,
and then do inference where we find globally optimal solutions.
So, so that was kind of the whole idea of state-based models.
And the thing that they use, the thing that they,
that made them powerful was this concept of a state.
So, so let's just summarize what state was.
Well, a state is a summary of
all past actions that's sufficient to choose future actions optimally.
And, and that's how we define states,
that's how we went about states.
Okay? And, and once we had state,
when once we had the notion about state,
then our mindset was,
I'm gonna move through these states through actions.
So I have states that I can think of them as nodes here,
and I have actions which,
where I can think of them as the edges in this graph.
And the question is, how do I go through one state to another state,
and what is the sequence of actions I should take?
So, so if you think about a policy,
like we were talking about a sequence of actions and,
and the sequence actually mattered, right?
Like I, I would take this action and another action,
and the goal is, I'd say for me,
to go from here to the door and I would have a [NOISE] sequence of
actions that need to go one after each other for me to,
to achieve the task.
Okay? So the type of problems that we wanna talk about today,
um, don't have, they,
they have a little bit more structure.
They don't really care about ordering.
And that's kind of the key difference.
So, so when I asked that very first question of pick like two countries,
one of them, the name of one of them should start with A,
the other one should speak Hungarian and they're not right next to each other,
then the way you think of like all those constraints,
all the things that you need to satisfy,
you don't really need to like follow a specific order.
They're a bunch of constraints,
you need to satisfy all of them.
It really doesn't matter to start from where.
And then that's kind of the idea that a,
a variable based model is.
And, and we're going to go through this example throughout the lecture.
So the example is,
it's a map coloring example.
So, so the idea is, let's say we have a map of, uh, Australia here.
And Australia has seven provinces.
So these are all the provinces here.
And what we wanna do is,
we wanna color this map.
So, so the question is,
how can we color each of these seven provinces with three colors?
I have red, green, and blue.
So that no two neighboring provinces have the same color.
Okay. So that's, that's a task we wanna do.
Okay. And then kind of the key idea again here,
is the order of things doesn't matter, right?
I can pick any of them [NOISE] and pick a color,
and then just go from there.
It doesn't matter, like, if I'm, if I'm,
it matters in the sense of the algorithm side of the things,
but in terms of the model,
it doesn't matter to, to include that.
So, so here, for example,
this is one possible solution.
Right? Like I can have the map of Australia.
I can have these different colors like red,
green, and blue for,
for different parts of it,
and no two neighboring countries have, have the same color.
So, so this is one possible solution that we can get,
we can have other solutions, er, to get.
And, and our goal is to find these types of solutions.
Right? All right.
So, so I can think of this as a search problem.
I can, I can perfectly think of this as a search problem,
where let's say, it starts with a partial solution.
And my partial solution is,
somehow I've decided to,
to choose, I'm just gonna refer to these provinces by their first letters.
So I'm gonna choose WA, V,
and T. I'm gonna just make them red.
And now what I wanna do is,
I wanna figure out what other colors to use for the rest of the provinces.
Okay. So I can just go down like a search tree.
So, so my state here is this partial assignment,
and I can go down the search tree and I can choose Queensland as,
as my next thing.
And, and I'm gonna color that red.
So if I color that red, everything looks good.
Everything is great. So now,
I'm looking at Northern Territories, so NT.
I'm gonna pick a color,
I'm just gonna color that green, let's say.
So [NOISE] I color that green.
Then if you look at SA,
I only have one option for it, right?
Because I've already picked red and I've already picked green,
and SA is connected to all these red and greens.
So, so the only color I can pick for SA is blue.
So, so that is, that is all I have.
Then how about NSW?
Then that has to be green, right?
Because, because I've already picked blue right there,
so that has to be green.
And, and here is one solution.
So I just went down a search tree and picked a solution to this problem.
I could have picked some other solution like
that decision that I made over there to make NT green,
that was kinda random, right?
Like I can just pick blue there.
So let me just pick blue there.
And then I can just have another solution,
that's a perfectly fine solution,
and I'll have my, my map going.
How about I choose, I choose a different color for Queensland?
So, so I decided,
I decided to make it red,
maybe I want to make it, uh, blue.
So if I make it blue then NT has to be green because that's the only option I can have.
And then when I get to NSW,
I don't really have any options for it, right?
NSW, I have no colors for it that would work.
Because, because green is taken, red is taken,
blue is taken, NSW is connected to all three of these.
That's not really gonna work.
How about I choose queen to be- Queensland to be, uh,
green, same story, NT has to be blue,
SW I don't really have a solution for it.
Okay. Okay, so, so this was
just going through this example assuming that it's a search problem,
and I have these states that represent partial assignments,
and I'm going to pick actions and the actions are going to just give
a coloring to the next- to some next variable here, okay?
So, so the state is partial assignment of colors to provinces and
the action I'm going to take is assign the next uncolored province to a compatible color.
So I can perfectly think of this problem as
a state-based or like using state-based models,
using this particular state and action.
But, but the thing is there is more structure to the problem and
the structure in this particular case comes
from the fact that again ordering doesn't matter.
So, so variable ordering does not really affect correctness here.
It's just a bunch of constraints.
It doesn't matter in what order I'm satisfying those constraints.
And, and in addition to that the variables,
they're, they're kind of interdependent in a local way.
So, so for example if I just look at Tasmania like right here,
it's not connected to anything,
so I can just pick whatever color I want for that.
And it's not affecting the rest of my problem.
So I don't really need to have some order to like pick t first or pick t last, right?
I can just pick a color for t and it doesn't affect the rest of- the rest of my system.
Okay. So the idea variable-based models is, let's,
let's kind of make our models- like
let's make our models simpler than state-based models.
Let's now try to figure out what is this
the state thing that's sufficient for us to make,
er, make decisions in the future and,
and pick actions sequentially.
Let's try to have an easier language, er,
to represent the model of,
of a problem that kind of looks like this.
So, so the idea is to come up with this new framework.
And, and in this new framework,
we're going to have variables as opposed to states.
So, so we are going to call these things variables.
And we're going to have assignments to these variables.
So, so the whole job of modeling is to figure out what the variables
are and what's sort of assignment we are picking for those variables.
And this decision of, well,
what order should I color things or what value should I pick for, uh,
for, pick for each province like,
like that decision of what order of values should I pick?
What order of variables should I pick?
I can push all of that to inference.
Okay. So, so it's not going anywhere,
I'm just pushing it to inference.
So another analogy here is you can think that you have
a difficult problem and you can have like an ad hoc way of going about it and solving it,
an, an analogy in programming languages so that it would be,
I would be solving it using assembly language.
If you look at state-based models,
you come up with the idea of state.
You're doing something, something more general
and you're doing a lot of work and why are you doing that?
Because you have a higher level of abstraction,
so when you're using something like state-based models,
an analogy to that is maybe you're programming in
C. So you're moving the level of abstraction.
And when you're using things like variable-based models,
it's even moving the level of abstractions a little bit higher.
It's, it's even like programming in Python.
So, so sure you can do the exact same thing in C too,
but now you have this higher level of abstraction to think about
problems and that makes your model much simpler.
And, and the order of things that can become the problem of inference.
Okay. All right.
Everyone happy with, with why we want to do state-based or we want
to do variable based modeling? All right.
So, so I've kind of motivated this but I haven't really said what it is,
how we go about solving it.
So what I want to do for the rest of the class is,
I want to start formalizing variable-based models by this idea called factor graphs .
And then after that I want to talk a little bit about
inference in the case of state- uh, variable-based models.
So specifically, I'm going to talk about dynamic ordering and
arc consistency as ways- as heuristics that allows us to,
to solve these variable-based models.
And then towards the end, I just want to show you a couple of examples,
other examples of why variable-based models are so powerful and where they
come in and just give you some ideas of like what- some other examples to look at, okay?
All right, so, so that is the plan for today.
So, so let's, let's start with a simpler example.
So let's say that I have three people.
Maybe I can draw that here.
So I have three people,
Person 1, Person 2, Person 3,
and each of them they are going to choose a color either red or blue,
that- that's what they're gonna do,
red or blue, red or blue, okay.
And each of them have a set of constraints.
So, so the idea is maybe this guy really wants to pick blue.
So, so really wants to pick blue.
Maybe this third person prefers to pick red,
but maybe he doesn't- it's not like as bad as this guy so prefers red.
And maybe we want to make sure that they pick the same thing.
The first person and second person.
And maybe we want to ensure that the second person and the third person,
we prefer they pick the same,
they pick, they pick the same thing- the same color.
Okay. So these are some set of constraints almost that I'm putting on, on this example.
And the way we can think about these constraints that I've just laid
down on this picture is using this idea of a factor graph.
So a factor graph is going to have a set of variables.
Okay, and this is like analog of states as,
as we talked about in state-based models.
It's going to have a bunch of variables.
I'm going to have three variables because this person is going to pick something,
this person is going to pick another color,
this last person is going to pick a color.
So I'm going to have variables; X_1,
X_2 and X_3, okay?
Let me actually write down some of these.
So we're going to go over a bunch of
definitions for the first part of the class at least.
So we're going to talk about factor graphs.
Factor graphs are going to have some number of variables.
We're going to represent variables with capital letters,
so like capital X.
So in that particular example the variables that I have are X_1, X_2 and X_3.
Okay. And each one of these variables,
they're going to- they're going to live in some domain,
they're either going to get red or blue, right?
So, so each one of these X_is,
they are going to live in some domain.
So we're going to say,
X_i lives in some domain of i.
So in this particular example,
the domain is just red and blue.
So each one of these X_is are going to live in either red or blue.
And if I pick a value for it,
and if I come in and say, well,
this guy picked red and this guy picked blue and this guy picked red,
then I'm giving an assignment.
So that's called an assignment.
[NOISE]
So an assignment, I'm going to write it with small x.
And it's going to tell me well what X1 took and what small means kinda red or blue.
So capital means the actual variable.
And X2 and X3.
What were they? So maybe for this particular example maybe you're talking about red.
Blue and red, okay? All right.
So, so that was variables.
They live in a domain and then we can pick an assignment, okay?
So now I have all these constraints and I can write
those constraints as something that's called factors.
So these factors are going to be functions that
tell me how happy I would be if this X1 takes value red or value blue.
So their functions, in this case F1 is a function of X1.
So, so I'm gonna write, a factor graph needs factors.
And these factors are Fj's.
There are some number of them.
There might be a lot of them.
Fj's of, ah, some X taking some value Xi.
Some Xi taking some value Xi or
some number of, let me just write the most general form right now.
X. And these Fj's have to be greater than or equal to zero, okay?
So they are kind of telling me how happy I would be, right?
So, so here I would have F1 of X1.
Um, so if I really want this guy to
pick blue then what would be a good factor to put here?
What should I say for F1 of X1?
So I can write it as an indicator function making sure that X1 definitely takes blue.
Maybe I can write it like this, okay?
So, ah, so if it is an indicator function what does it say- what does it tell me?
If it is an indicator function,
if X1 actually takes blue,
then the value of this factor is going to be one.
If X1 takes red,
the value is going to be 0.
So I'm kind of treating 0 as this thing that I don't want and
anything above 0 as something that I actually want to get, okay?
So, so I'm going to have another constraint.
This constraint is going to be F2.
It's a function of X1 and X2 that's why it's connected to both of them.
So I'm gonna draw these squares as,
as kind of like showing where the factors are.
So, so the circles are my variables.
And then the squares are my factors.
These functions that kind of told me what are the constraints?
What are the things that I need to satisfy?
So F2 is going to somehow encode that they need to pick the same thing.
Again it can be maybe an indicator function.
Making sure this is- these two are equal to each other.
And maybe I'll have F3 of.
F3 is going to be a function of X2 and X3.
[NOISE] This ensures that they sometimes make the same,
or sometimes here kinda means that we can have an indicator function,
but maybe if they don't pick the same thing you wouldn't be too sad.
So, so maybe you don't put 0 for that.
So it would be an indicator function plus some constant.
That's one way of going about it.
And then X3 is going to be take, prefers red.
So it's going to have a factor.
That says it prefers red, okay?
All right. So let's look at the same thing on the slide.
So that's a factor graph.
So I can actually look at the values of the factors maybe F1 of X1.
Maybe what I want is I want if,
ah, for that to be equal to 1.
If X1 picks blue I want that to be equal to 0.
If X1 picks red.
For, ah, the two, they have to agree, for the case that they have to
agree that I can define it as
an indicator function but if they are not equal to each other,
if I'm gonna get 0, I'm going to be very unhappy.
If they are equal to each other I'm going to get 1.
So I would be happy.
And then for the case that X2 and X3 needs to kind of be equal to each other,
then maybe we can do something like an indicator function plus 2.
This means that if they don't pick the same thing,
oh I'll be happy.
But like if they picked exactly the same thing,
I'm going to be even happier. So I'm going to get 3.
And then for the last one similar thing I,
I preferred the last person to pick red.
So I'm gonna give it a value of 2 to that and I'm going
to give 1 for the case of blue, okay?
So, so these are my factors, question?
Does the factor value matter, or is the only thing that matters if it's equal to 0 or not.
So good question. So question is, does the factor value matter
or is it just like if it is above 0 or not.
In general it does matter like what you were picking.
For you're soon going to be talking about a specific case of concerns,
specific case of factor graphs where the
zero and one is the only thing that's, that matters.
So I'm not focusing too much on the exact value.
It's just if you get zero that's pretty bad.
If you get non-zero that's good.
So I'm treating them like that because soon we are going to talk about CSPs,
constraint satisfaction problems which are
just factor graphs where you have
0s and 1s, you don't have anything above them, okay?
All right. So let's try to actually write this up.
So, um, here's this environment that you can play with it if you want.
Um. Okay. This is visible.
Yeah. All right.
So here you can define variables.
So I have variable X1.
It can take value red or blue.
X2 and X3 similar thing.
They can take values red or blue.
I have four factors so I'm going to write up what those factors are.
Factor F1 depends on X1.
It's a function and it's going to return the result of this indicator.
And then a similar thing,
ah, I'm going to define, ah,
the second factor also as a function of X1 and X2 and it
returns a value of the indicator and has all these other factors.
And on the right you can kind of see these factors being generated.
So we're going to look at this environment even more next time
when we talk about more fancier inference algorithms,
but for now let's move to-.
Let's move to finding our factor graphs.
All right. So. Alright. So what is a factor graph?
So more formally a factor graph has a set of variables X1 through Xn and each one of
these variables each one of these Xi's lies in
some domain in this case the red or blue was our domain.
And then the factors are going to be F1 through Fn.
We have m of them in this case let's say.
And, and each of these factors is just a function over
X that is going to be greater than or equal to 0, okay?
So, so that's a factor graph.
It tells us what are the things that we really want.
So let's look at one example here.
So in this- in this particular map coloring example,
the variables or the provinces that we have. We have seven of them.
The domain is going to be red, green, and blue.
So those are the colors that we can pick.
And then the factors.
Well, the factors here are just going to be telling us that
don't pick the same color for two provinces that are neighbors.
So I'm going to have factors that are indicators ensuring that we
don't give the same value to two neighboring territories.
So we have factors that basically connect every neighboring territory.
And again this square here corresponds to each one of these functions, question?
Now isn't it the same for all the variables?
Not necessarily. So the question is is the domain always the same for all variables?
It depends on the problem. Not really.
Also we are going to talk about how to reduce the domain as we go.
So that's, that's another reason that I'm emphasizing
on the domain because when we think about the inference algorithm,
the domain is not going to stay the same throughout.
If I pick a red for example for WA,
then NT is not gonna have red in its domain anymore.
So, so the reason I keep bringing up the domain is we're going to look at how to update
the domain for the- for the inference algorithm, okay? All right.
So this is a factor graph.
Um, [NOISE] let's define a few more things just so
we have a common language to talk about things.
So, so we're going to find a scope.
So scope of a factor is a set of variables it depends on.
So, so it's really simple. So scope.
So I'm going to write scope here.
Scope. So it's just set of variables a factor depends on.
So for example, ah,
for this case if I have F2,
it depends on two variables X1 and X2.
So the scope of F2 is just X1 and X2.
Ah, so in this other case,
when we looked at the map coloring example.
If we look at F1 as a factor that tells us WA and NT should not have the same color.
The two variables that are used are WA and NT,
okay? So, so that's the scope.
Then now that we have the scope,
we can define something else called arity which is the number of variables in the scope.
So each one of these squares just
how many- how many edges is it coming out of it? That's arity.
So in this case, this particular square depends on two variables, arity is two.
I can have a setting where maybe I have
a factor that depends on three variables, then arity is three.
I can have a factor that depends on only one variable, then arity is one.
And, and if arity is two then we call it- we call the factor a binary factor.
If arity is one we call the factor a unary factor.
So just common language.
[NOISE] So we have arity and then we have-.
Which is- which is the number of variables in the scope.
And then you have unary, unary, unary factors.
When arity is one,
our binary factors and arity is two.
So it's just defining things.
So for example in this case of map coloring F1 is a binary factor.
So in the case of map coloring all our factors were binary if you- if you look at it.
All right. Let me go back to that. Here it is.
So I have a bunch of factors that just
say these two variables should not be equal to each other.
So I have a bunch of binary factors and that's pretty much the only thing I have, okay?
In this case, I have a binary factor,
I have a binary factor,
I have a unary factor, a unary factor.
All right. Okay. So, so far so good.
So- so we talked about the assignments, right?
The assignments are going to be a setting where we give actual values to these variables.
And an assignment can have a weight that tells us how good that assignment is.
So- so remember, a factor tells us how good this particular excite,
like how happy I would be if x_2 takes a value and x_3 gets a value,
a weight tells me how happy I would be for the full assignment.
So- so what it is going to be is like in this case,
we can- we- we can look at weight to just be a product of- of my factors.
So I'm gonna write- uh,
maybe I'll just write it in front of here.
So I'm going to define a weight of an assignment x.
And the way I'm writing that is I'm just gonna write it to be a product of f_j's,
uh, j from 1 through m. So I have m factors,
so it's going to be f_j's of x taking assignment x.
Okay? So for this particular example,
we looked at the tables,
and each one of these tables represents our factor.
But- but now, if I talk about a full assignment,
then I'm looking at what does it- what happens if x_1, x_2,
and x_3 take all possible values that they could be taking.
So I have eight possible options here.
And then I'm looking at a weight, eh,
as- as a product of ea- all of these factors multiplied out by each other.
So- so remember, I was saying well,
0 is the thing that I really don't want to have.
So if I have a 0 ever, like that,
that's a super like hard constraint that I'm trying to enforce,
and that makes my weight equal to 0.
So so- if x_1 ever picks red,
that was like a hard constraint.
We really wanted the first person to pick blue.
So fir- if the first person picks red,
then the weight is going to be equal to 0.
The other thing we really wanted was
the first and second person to pick exactly the same color.
If they pick different colors,
then my second factor is going to be 0,
weight of that is equal to 0.
Otherwise, I would have different- I- I would have different weights.
Maybe the thing I care about is to maximize the weight,
so I'll pick the one,
the assignment with- with the value 4.
Okay? So going back to this, um,
demo environment we were just looking at,
um, what we can do is, uh,
we can- basically, we've defined our factor graph,
and we can actually step through it,
and you can play with this, but you can basically get
these [NOISE] two different- different assignments that- that give you non-zero weights,
and you can pick your favorite.
So we're gonna talk about various types of algorithms
that allow you to compute these weights.
Okay? All right.
Okay. All right.
So weight of an assignment x is just a product of the factors of that assignment.
Okay? And then our objective is to maximize the weight of the assignments.
So I- I want- what I want to find is,
at the end of the day, what I wanna do is I want to find an assignment.
So I wanna find that small x that maximizes the weight of, er,
of that particular x.
Okay? All right.
So going back to the map coloring example.
So here, um, let's say that we defined all these indicator factors.
So if it is an indicator factor,
I'm either going to get 0 or 1,
I'm not gonna get anything other than that.
Then if I have this particular assignment which kinda looks right,
then the weight of that assignment is just going to be
a bunch of 1 multiplied out by each other,
so I'm just gonna get 1.
Okay? So- so if I find a solution to this map coloring problem,
the weight of that- that particular assignment is going to be 1.
I could have another assignment where I don't get,
uh, a good solution.
I had two of- two of these-
these neighboring territories are going to
be have the same color if they're both going to be red.
Then in that case,
two of my factors are going to be 0.
If they are going to be 0,
the weight is going to be equal to 0.
So for this particular map coloring example,
where my factors are just indicators,
the only weights I can get are 0 or 1.
I can either get 0 or I can get 1.
If I get one, I find a solution.
If I don't get 1,
I don't find a solution. Okay? All right.
So we have been talking about factor graphs, they're these more general things.
Now, we're going to start talking about CSPs, constraint satisfaction problems,
which are just factor graphs where all factors are called constraints,
and the factors are going to take value 0 or 1.
And the constraint is satisfied if the factor takes value 1.
[NOISE] So we talked about factor graphs.
We're going to talk now about constraint.
I'm just gonna write CSP,
constraint satisfaction problem, CSPs.
Okay? They also have the same variables as before.
And we're gonna pick assignments for them.
So same thing, I'm gonna- assignments.
But the factors are going to be called constraints.
[NOISE] And these factors fj's of x are either 0 or 1,
they're not anything else.
Okay? And if you find an assignment where your weight is equal to one,
then that means that you are satisfying all your factors,
and that's called the consistent assignment.
So you- we have consistency,
consistent assignment, assign- I'm gonna write assignment.
Um, that is when the weight is equal to 1.
If the weight is equal to 0,
then we have an inconsistent assignment.
So- so it's either 0 or 1.
We have consistent assignments or inconsistent assignments.
[NOISE] Okay?
So an assignment x is consistent if and only
if the weight of that particular assignment is 1.
That means, all the constraints are satisfied,
because constraints are just give me 1 and 0.
I'm multiplying 1 and 0.
If anything is not satisfied,
then the thing is 0, okay? All right.
So, so far, summary so far is we have just gone over a bunch of definitions.
Factor graph is the more general case of it.
Constraint satisfaction problems is more of an all or nothing kind of a situation.
So you have hard constraints,
everything is a hard constraint.
And then you have- so, um,
so for example, if you think of map coloring,
you can think of that as- as
a constraint satisfaction problem because everything is a hard constraint, right?
Like you- you don't want any two neighboring countries to have the same color.
So you're either going to give 1 if- if that constraint is satisfied,
or you're going to give 0 if that's not satisfied.
You still have variables.
Factors are called constraints.
Assignment weight.
If that is equal to 1,
we have consistent assignment.
Otherwise, we have an inconsistent assignment.
Can we just think of the CSP as a-
Constrained factor graph is that the idea?
It's- it's a more constrained factor graph.
Yeah, it is- factor graph is this big picture of CSP is
an instance of factor graph. All right.
So that was factor graphs and constraint satisfaction problems.
So, so let's talk about how we go about solving these.
So, so how should we find an assignment?
Our goal is to find an assignment.
So we have consistency, right?
Because, because if, if you are talking about CSPs,
we wanna get weight 1,
that means you wanna have an assignment that's consistent and makes all my factors 1.
So, so how do I pick, how do I pick that?
Okay. So, er, so let's look at an example.
Let's just like, let's just see how we would do it normally,
like if you wanted to solve this.
Like, if I was solving this I would pick one of these,
like one of these nodes, or variables,
I would pick WA, maybe I would say well,
let's just pick red, just to see how that goes with that.
And then I would go to a neighboring,
neighboring node like NT.
And I do have a constraint.
The constraint is WA and NT should not be equal to each other.
So the only thing that tells me is that NT should not be red.
So I'm just gonna pick some color, let's just pick green.
So then I'm gonna go to some other neighboring,
neighboring node, so that's SA.
I have two constraints.
The two constraints are is SA should not be equal to WA,
should not be equal to NT,
so it shouldn't be red or it shouldn't be green.
The only option I have is blue,
so I'm gonna set that equal to blue.
Then I'm gonna go to Q, the only option I have where Q is red because,
because green and blue are already taken,
then I'm gonna go to NSW,
the only option I have there is green.
When you go to V, again then the only option I have is, is, is red.
And then I can pick whatever color I want for
T because that's kind of random node out there.
Okay. So, so this is a thing that we would probably do if we were to do this, right?
We, we would go over these nodes with
some order and we would pick colors in some other order,
and I know that's important.
But, but the way we would do it is just,
just pick some order and maybe we'd have some heuristic that picks your order
and picks the values and tries to make the constraints satisfied.
So, so what we wanna do is we actually wanna spend [NOISE] a little bit of time,
uh, talking about doing that,
and go- and having actually heuristics that,
that tells us what order we should use,
we should use for the variables,
and what order of values we should pick.
So, so we're gonna talk about a few heuristics mainly this time.
So, so, so to do that, um,
we need to define one more thing, it's the last thing I'm gonna define,
and then after that talk about the algorithm.
So, so we're gonna define dependent factors.
So dependent- so the partial assignment is going to
be partially assigning values to variables in this,
in this, um, CSP, right?
So, so a partial assignment, for example here,
could be that WA needs to be red and NT needs to be green.
That's a partial assignment, okay?
Then I can define dependent factors to be a function
of partial assignments and a new variable X_i.
So let me depend- let me just write that somewhere,
maybe I'll write it, a different color because it's.
So we have, um,
we have something else called dependent factors, it's D of x and X_i,
where x is partial assignment and X_i is a new variable I'm picking.
And dependent factors is going to return a set of factors.
It- it's going to return a set of factors that,
that depend on x and X_i.
So, so for example,
in this particular case,
this we said, this is a partial assignment.
Let's say I'm asking what are the dependent factors of this partial assignment and SA?
So I'm picking a new variable,
I'm picking SA, and I'm saying,
what are, what are the dependent factors?
And then these are going to be the factors that depend on this new thing
SA and depend on the partial- partial assignment.
So it's going to be this factor and this factor, right?
I'm going to pick the factor that says WA is not equal to SA,
and I'm gonna pick a factor that tells me NT is not equal to SA, okay?
[NOISE] And that kind of like the idea of dependent factors is that it allows me to,
to think about the next thing I should- next things I should be worrying about.
So, so if you remember like tree search algorithm,
if you would look at children of, of some note.
Here we are going to look at dependent factors, because,
because those are the factors, the next factor is we should,
we should care about, that's why I'm defining these dependent factors.
Okay. All right.
So, so now this is the algorithm.
Kinda I want to write it up on the board because it would be good to have it,
[NOISE] but it is a little bit of a long pseudo-code.
So, all right.
So the algorithm we're gonna talk about right now is,
is just backtracking search.
It's not doing anything fancy.
We're gonna talk about fancier things next time.
But, um, you have backtracking search.
It does the thing that you expect it to do.
So it takes some partial assignment x,
it takes the weights that we have so far,
and it takes the domains,
domains of, of those variables that I have so far.
Okay. So if x is a complete assignment,
if you have found a complete assignment,
then we are going to update the best thing we have and we would return,
or we would do whatever you're supposed to do for the problem, right?
Like we might have different types of problems here,
like maybe the question is find one assignment.
If I find one complete assignment,
I can, I can just return.
Maybe I'm looking for another question which,
which tells me count all possible assignments that you can have.
So, so if I'm counting assignments then I'm just
going to update my counter and try to find the next assignment.
So depending on what the question is I might want to do
different things when I find my, my complete assignment.
But let's say I find my complete assignment,
then I update and I'm happy.
Okay. Then [NOISE] um,
I feel like then we're going to choose an unassigned variable,
so I, I should have written this.
So if x is complete,
then let's say we are happy.
Then we are gonna choose, um,
an unassigned variable, unassigned,
so choose a variable, chosen an unassigned variable X_i.
And well, how do I do that?
I'm gonna talk about a heuristic to do it.
Um, so, so we'll talk about that,
but let's say I have some way of figuring out what is the next variable I'm picking.
And then after you pick the variable,
you're gonna pick some value for it, right?
The map coloring. You're gonna pick a province and you're gonna say red.
So how, how do you know it's red?
Like how do you know the val- the next value you need to pick is red?
Well, that comes from another heuristic, uh,
which says order, values, and domain.
So values would be red, blue, green.
So those are my values, right?
So ordered the values that are in domain I,
um, [NOISE] I've chosen X_i.
So, so you picked up the next i,
maybe the only colors that you can use right now are red and blue.
So, so then you are going to order red and
blue using some heuristic that I haven't talked about yet.
But maybe some heuristic says,
you should use red first and then you- you'd use red first,
you'd order it in that domain- in, in that order.
And then for each of these values in this order,
so for each v in this order,
that you've decided, you're gonna update your weight,
or you're gonna have this Delta weight value.
And this Delta weight value is going to be product of your factors, okay?
[NOISE] And these factors are factors of
your partial assignment whatever you've decided so far,
maybe you, you have assigned two colors for
two territories already and you're looking at the third one.
So it's going to be the partial assignment union whatever value you are
looking at for this new X_i that you're trying to pick, maybe a color for.
Okay. And, and what are these f_j's that
you are looking at? Well, these f_j's are going to be the
f_j's that are interdependent factors of the partial assignment and your variable,
that's why we defined dependent factors.
Because these are the factors that we care about,
these are- I'm not gonna look at Tasmania if I'm not looking at that part of the graph,
I'm just gonna look at the things that depend on my current partial assignment and my,
my, my, er, my X_i.
Okay. If Delta is equal to 0 return, or continue, 0 continue.
So that means that this assignment you- continue, continue.
Er, this means that this is a
particular value that you have picked just made everything 0, it didn't work.
So, so you should try other things.
The other thing you're gonna do is if
this value works is you're gonna update your domain,
so we're gonna talk about how to do that.
That's the thing that's going to save you- save your time.
Because like you have now found out that you only need to care about colors red and blue,
and you don't need to worry about green.
So, so that- that's updating the domain,
making sure that you don't need to worry about all the colors.
And then after that, you're just going to backtrack on this new thing.
Backtrack on this new thing.
So on this new thing is X union you've picked value v for X_i.
So this is your new assignment you have extended your assignment by value
v. Your weight is going to be whatever weight you started times Delta,
that's weight Delta, and then you've updated your domain,
so you're just gonna use domain prime, okay?
Domain's prime. So this is domains of everyone's,
like domains of, uh, all the other nodes.
All right. So, so we're gonna talk about this a little bit more.
So- but this is the basic of, of the algorithm.
Okay. So gonna first talk a little bit about updating domain.
So, so how do we update domain?
So, uh, one very simple way of updating domain is,
is this thing that's called forward checking,
which says well, if you pick a color,
so let's say that you pick W to be red,
then just look at the neighbors of WA and,
and then see if you can update the domains of them.
So this is the simplest thing I can do, right?
Like I've picked WA,
I've decided WA is red.
So the thing that I'm gonna do is I'm just gonna look
at the neighbors and the neighbors are NT and SA.
They cannot be red,
so I'm gonna to just update their domains to be red- er,
to be blue and green, I just drop red.
Yeah. So, so that's like
the simplest thing while would do so maybe I'll write it in different color.
So what option is this forward checking approach for updating domain.
Okay. So let's go further.
So maybe now I'm at NT.
I'm deciding NT to be green.
If I'm deciding NT to be green,
I'm gonna look at neighbors of NT.
So I'm gonna look at SA and Q,
they cannot be green anymore.
So I'm gonna drop green.
Okay. I'm, I'm gonna look at Q for whatever reason.
And Q, I'm going to pick blue for Q,
because I want to pick blue for Q.
And, and then I'm gonna look at the neighbors,
and my neighbor SA does not have anything in its domain.
So I realized that at this point,
like this particular assignment is inconsistent.
I don't need to worry about the rest of
the nodes and when what I'm picking for the rest of the nodes,
it's kinda like equivalent to pruning,
like I don't need to worry about anything else,
because I've just found out that this- this assignment does not work.
Okay. So that's kinda the whole idea of updating the domain.
So, so forward checking is the idea of doing one step lookahead.
So after assigning a variable X_i,
you wanna eliminate inconsistent values from domains of X_i's neighbors.
So you want to reduce the,
the domains of X_i's neighbors,
uh, and if any domain becomes empty,
then, then you don't recurse on that.
And, and when you,
you unassig- something to notice is,
if you're unassigning X_i,
you have to restore the domains.
So, so because you change the domains if you're unassigning, if you're deciding, uh,
green who was not the color to go then, then,
then you got to- you got to update your domains, okay?
All right. So the other question was this heuristic.
All right, so this heuristic updating domain,
one way to go about it is forward checking, just update the neighbors.
Another, um, place that,
that we need to, uh,
pick things wisely is choosing the unassigned variable.
So which one- which,
which unassigned variable should I start off?
So, so which variable to look next?
And, and again, one heuristic to,
to look at here is to pick the variable that's the most constrained variable.
So, so choose the variable that has the fewest consistent values.
So, so you are going to pick the one that's the most constrained variable.
Why do we wanna do this?
Why would I pick the most constrained thing?
Probably because of less options.
Yeah. So you're left with less options.
And, and, and the idea is if I'm going to fail,
let me just fail early.
Like if this is not gonna work,
let me just find out that it's not gonna work early.
So, so that's the whole idea of it.
And in this case, like if you are left with this option
where we- where we choose red and green here,
and now we wanna pick what should I look at next?
I should be looking at SA because that only has one value.
So if that's not going to work, well,
nothing else is going to work, right?
So, so we want to choose, choose, uh,
a variable that has the fewest consistent values.
And again, the reason this works is,
is if we have some number of constraints in our factor graphs.
So, so these are more general for factor graphs too.
Like everything I'm saying is not just about CSPs,
it's about factor graphs.
Um, and, and the reason this works is we have some constraints, right?
We ha- we have some, some of these factors are going to
return a 0, because they are going to return a 0,
that is why I, I,
I would like to follow a heuristic like this because
that allows me to not look at everything.
So, so this, this heuristic only gives us
benefit if we have some factors that are constraints.
Okay. All right.
So, so that's one heuristic.
The second question is, okay,
so now like using most constrained variable,
I pick my variable,
what value am I going to pick for it?
And, then for value,
but it's interesting because for value you want to pick the least constrained value.
So- and, and the reason again
is [NOISE] you pick the most constrained variable because you wanted,
you wanted to know if you're going to fail,
you wanted to fail early.
But now you've committed to that variable.
Like now you're going with that variable.
So you might as well- you,
you have to like assign a value for it.
So you might as well pick the least constrained variable here,
to, to leave options for,
for the other variables around you.
So, so an example here is,
and, and how can you think about- so,
so an example here is you're going to look at, um,
[NOISE] this, this setting where,
what is it, you're picking Q, right?
And, and you want to choose what color to,
to use, what value to use for Q, right?
You can- you can color Q red.
If you color Q red,
you're gonna do this forward checking,
and if you're gonna do forward checking,
you are going to update the domains.
And when you update the domains,
you have two options here, two options here, two options here.
So that could be a measure of consistency.
So you have six consistent values.
If you decide to use blue for Q,
what's gonna happen is you are going to update NT,
and, and that's going to have one value,
SA is going to have one value,
and SW is going to have two values.
So you have 1 plus 1 plus 2 pl- and that's equal to 4 consistent values.
And, and you're gonna, you're gonna basically pick the one that,
that leaves the most options possible.
So you're going to order the values,
the colors values here refers to colors, of selected X_i by
decreasing number of consistent values of neighboring variables.
[inaudible].
Yeah. Yeah. So it's the cardinality of the domain of neighbors.
Yeah. And, and one other thing is like
these heuristics are only going to work if you are doing forward checking.
If you're not updating our domains they're not going to give us any benefits.
Okay. And, uh, also another note about this particular heuristic,
uh, which is for ordering the values,
the only like place that this is actually going to give us
some benefits is when you're working with CSPs when,
when, when we actually have everything as constraints.
Because, because if we don't,
we actually need to go through all the va- all the values
and then figure out where the value of the factor is for, for them.
So, so, so this is only going to be beneficial when we
have- when we have everything as, as, a constraint.
Just a question, so when we are doing all of this,
we are not actually copying anything, right?
Well, we're it's just,
it's just one possible what if we find something without worrying about [inaudible]
[OVERLAPPING] So it is a recurrence.
Other optimal, more optimal solutions.
Uh, yeah.
So, so depends on what we were doing, right?
So, so that's kind of this pa- this part.
So, so the question is are we finding for the optimal solution,
are we finding for S solution?
It depends on like- and that's kind of this line.
If you find S solution and you're happy with
that one solution you can just like return it here and be happy.
If you want to find the best solution and you need to like iterate this multiple times,
then maybe you have like a counter here that still like keeps iterating.
Um, for CSPs you want to find S solution because,
because, because we, we just want satisfy the constrain- constraints.
But if I have a factor graph I actually want to optimize my,
my, my weight. All right.
Yeah. So, so yeah.
So the, so the idea of this most constrained variable is we must assign every variable.
So if you're going to fail,
let's just fail early,
it's kind of similar to pruning.
And the idea of, uh, what order we are picking for,
for values is we are going to pick values for the least constrained value.
Uh, so and, and kind of the reasoning behind that is you've got to choose some value.
Like, like we have to choose values for all of these things.
So, so choosing, uh, so,
so choose a value that's the most likely to lead a solution for everything.
Okay. And this is what we just actually said.
Okay. So, so going back to this, this algorithm,
now we have a heuristic to,
to follow for all these three different red lines.
And, in doing so we're just doing backtracking,
and then we can update this and,
and just go through it, and it does- it does find a solution.
Okay. All right.
So, um,
so now I want to spend a little bit of time talking about arc consistency.
So what arc consistency is,
is it's just a fancier way of doing forward checking.
So, so we talked about a heuristic for this one, a heuristic for this one,
the only algorithm we are talking about today is this,
that's, that's the only thing.
And, uh, we said, well,
in this algorithm we gotta update the domain,
the way we have been updating the domain is just looking at
the neighbors and trying to update the domain using forward checking.
So another idea is to do something slightly better which is called arc consistency.
And arc consistency doesn't just look at the neighbors,
it goes through the whole, the whole, uh,
the whole CSP, and tries to update,
uh, the domains of even like further nodes ahead of us.
So it doesn't just look at the neighbors.
So, so that- that's what this whole section
is going to be about, how to do arc consistency.
Okay. All right.
So, so the idea of arc consistency is let's eliminate the values from domain.
So, so I have this, this giant domain,
I don't want to go over all those, uh, values.
Uh, I have a for loop here for all the values.
If I can update my domain,
I have less things to iterate over that's going to be much better.
So let's just try to reduce branching.
Okay. So, so here is an example.
So let's say that I have X_i and X_i lives in- so I'm looking at X_i and X_j,
and X_i takes val- the,
the domain of X_i is 1, 2, 3, 4,
and 5, and then the domain of X_j is 1 and 2.
Okay. So now what I wanna do is, um,
I had a constraint,
the constraint is X_i plus X_j is equal to 4.
So if this is my current domain of X_i,
I don't really need to worry about
all these values in X_i because the constraint tells me,
well, 5 never works because X_i plus X_j has to be 4, so that's not going to work.
This one is not going to work.
The only way for things to work is to have 3 plus 1,
and 2 plus 2, and that's it, right?
So, so the only variables that I actually need to worry about for domains of X_i is,
is 2 and 3, not 1, 2, 3, 4.
So, so what I wanna do is I wanna take the domain 1, 2, 3,
4, and 5, and reduce that to just looking at 2 and 3.
Because those are the only values that I should actually care about.
And this const- yeah,
because this constraint is kinda enforcing that.
Okay. So and enforcing+
our consistency basically tries to get to the, this smaller domain.
Okay. So, um, all right.
So a variable X_i so let's actually formally define this.
A variable X_i's are consistent with some variable X_j.
If each, each, uh,
value X_i in the domain of- for each value
X_i in the domain of X_i there exists some X_j the domain of X_j.
So, so the factor is equal to- is not equal to 0.
So basically it's ensuring that everything is going to be consistent.
So, so if you have inconsistencies,
remove things from the domain of X_i.
So our consistency ensures that if there are any sort of
inconsistencies between two variables X_i and X_j's,
it's let's say it starts from X_i,
and it tries to remove for- from the domains of X_i,
uh, to, to make sure that all factors are not equal to 0.
[inaudible].
So we start from 1 and- so we pick x_1,
and I will try all,
all these other variables,
xj's and values of them,
and then we keep, like, iterating.
We do iterating over all of them,
but we gotta like pick one,
and update the domains of that.
Okay. Yeah, so, so what we're
gonna do is we're gonna just write up the function, enforcing our consistency.
And it's gonna remove values from domain of i to mix- it
make xi consistent with respect to some other xj.
Okay. So, so the only thing I'm touching is domain of xi.
All right. So, so let's actually, like,
go over an example of how this works.
And then we're going to look at the pseudocode for it.
So here's our example.
I'm gonna start from WA.
I'm gonna pick red for it.
Okay. So that's my current domain for WA, is red.
If I was doing forward checking, what would I do?
I would just look at NT and SA.
I would update the domains of NT and SA.
So now what I'm gonna do is I've realized that NT and SA their domains are changed.
So I'm gonna push them to the,
to the same- to the same list of things I have,
and I'm gonna look at each of them and see the neighbors of them too.
So the arcs that come from them.
So I'm going to look at NT.
Um, well, that is right here. Actually, it's too soon.
So, so e- everything looks consistent there. Everything is great.
I can't update anything more.
I'm going to pick NT now.
Let's say I decide NT is green.
So NT is green, I'm gonna look at neighbors of NT.
So neighbors of NT are WA. WA is red.
Everything is great. SA has a green.
I need to get rid of that green because it can't be green anymore.
Q has a green, I need to get rid of that. So let's update that.
So Q and SA,
their domains are touched, right,
their domains have changed.
So I actually need to look at them,
and then see how the domains of their neighbors are going to be affected.
For example, I can look at SA,
and I can see well, SA is, is blue.
The only way for SA to be consistent with
the rest of these guys is that they don't have a blue in them.
So I'm gonna remove blue from Q and SW and V. Because,
because they cannot have blue for these two to be consistent.
Again, if SA here is kinda of my xi.
So I'm, um, sorry.
It's actually my xj.
So I am gonna pick xiQ here,
and I'm gonna update the domain of xi,
so it becomes consistent with SA.
Right? So I'm gonna like pick- change the domain of Q, get rid of blue.
I'm gonna change the domain of NSW, get rid of blue.
I'm gonna change the domain of V, get rid of blue.
Okay. So what has updated? Q is updated,
NSW is updated, V is updated.
They're gonna go to- go back,
and I'm gonna go through them again and see if,
if their neighbors need to be updated.
Okay. So going back to,
to, to Q, Q is red.
NSW's domain needs to be updated to be consistent with Q.
So I'm gonna remove red.
NSW's domain is touched.
So, so now I gotta go back to V. V is going to become red,
and then T can take any value that it wants.
So if I do like this full,
like enforcing our consistency here,
I'm gonna end up with, with something that looks like here.
So all my domains are kind of pruned,
and I have, I just like have a solution, right.
Like I don't need to actually iterate over any values.
And this is just done by,
by updating the domains.
And then doing this arc consistency approach,
rather than doing backtracking search.
So, so all of that is done in this step.
Okay. All right. Yes?
[inaudible] solution, go back and make NT blue as well.
Uh, so, so if you wanna,
if you wanna actually- so,
so this whole, like,
pruning is only, like, useful, right,
if you want to find best- like a solution in a CSP,
but if you have a factor graph and you actually- if you have a factor graph,
you need to actually try out all these values to
see what is the value you're gonna get for each,
each one of the colors.
If we did forward checking instead,
we actually would have arrived at the same conclusion here, right?
It would have just have taken more steps like filling more of these different-
If we were doing forward checking,
we had to do the, like,
we actually had to do the algorithm.
Like, like we wouldn't get to this, like,
we would get to this much later because if,
because if you are doing forward checking,
we would just look at the immediate neighbors,
we'd update the domains,
and then we'd go to the next, like,
nodes in the neighbor- neighbors and do backtracking search again.
Here, like, I'm not- I haven't,
like, called back-, like, I'm here.
I've updated by domain.
And I'm with that scenario,
and I haven't called backtracking search yet.
All right. So yeah,
so forward checking is kind of a simpler version
where we're assigning xj to be equal to xj,
and, um, and you're enforcing
arc consistency on all the neighbors of xi with respect to xj.
Arc consistency, what it does is it repeatedly- well,
there- there are different algorithms that try to do arc consistency.
The particular algorithm we were talking about in this class is called AC-3.
It's just the most useful- like,
the most, um, common way of doing arc consistency.
And what it does is it repeatedly enforces arc consistency on all the variables.
So, so it goes over everything pretty much.
So, so what it does is you're gonna add xj to your set.
Then while set is not empty,
you're gonna remove an xk from, from that set.
And for all neighbors,
let's call them xl of,
of this this xk that you have picked.
For all the neighbors,
what you're gonna do is you're gonna call enforce
arc consistency on xl with respect to x here- xk.
Okay. And then if your domain is changed,
if don- you change the domain of domain of l,
then you're gonna add that back in.
And that's kinda what we're doing in this previous example,
like, we, we kept adding the nodes back in.
So, um, yeah, so in terms of complexity,
um, of this algorithm or worst-case scenario,
it's going to be order of e times d cubed,
where e is the number of edges and d
let's say is the number- maximum number of values that you can have.
So, so, so the reason it is that is,
when you're enforcing arc consistency,
this line takes order of d squared.
Let's say you have d values.
For each of them you have d values,
you need to co- consider all that combi- all those combinations.
That's d squared. You are,
are doing- going over all the edges,
right, so, so you have all the edges.
So that's ed squared.
And another thing to notice is,
you're sometimes adding these things back in the set.
Well, why are we adding them?
Because their domains can be changed.
Their domains can be changed at most d times.
So that's that extra d. So,
so that's order of ed cubed.
If you're interested in it, you can look at the notes for it.
That's worst-case scenario.
In general, it doesn't take that long.
In general, like, I'm not gonna keep, like,
adding the same value, like,
a million times, like, back in.
Or the same xl back in my set.
Um, in general it's much faster.
In practice, it's much, much less.
Okay. All right.
So, so again, it's a heuristic.
It's not the best thing in the world.
Like, if- I, like, ideally,
you would have wanted AC-3 to not
return a solution if there doesn't exist a solution.
But, but here, for example AC-3 is not being very effective.
Here's an example. Right. So you have these three nodes,
and let's say you are left with these domains.
So blue and red.
If you're enforcing arc consistency,
the domains are not gonna change.
These domains are very consistent with each other.
But, but there is no solution that actually,
uh, you can find here, right?
Because if you choose blue and red here,
you don't really have an option for the third one.
So arc consistency is actually not going to,
uh, be able to figure out that this, this doesn't work.
And there are more complicated versions of arc consistency that consider,
um, uh, that, that go beyond these binary relationships,
uh, but they are,
they're going to take exponential time.
So, so our consistency is simple.
You run it, it's usually useful,
but it's not gonna find everything for you.
Okay, okay.
Yeah, so and I'm kind of the whole intuition of
arc consistency is we are looking at this graph in a local manner.
And locally we're trying to like update our domains to be more efficient,
but it's- it's not,
it's not gonna give us a global answer.
Of course, it's not gonna give us a global answer.
Because if you want it to have a global answer,
we had to reconsider the relationships of
all arc- all arc constraints with respect to each other.
Uh, but it's, it's basically making sure that locally at least everything looks good.
How do you figure out when you should or shouldn't use AC-3?
When you should- so in general you can use- so I would- in,
in general I would say use AC-3 because it's going to prune things usually.
Um, if, if you have a lot of dependencies between, like,
if you ha- if you have like the circular type of dependencies,
it's not gonna figure everything out but it's usually just going to be useful.
So, so running it in practice,
it doesn't take that long.
Running it is usually going to prune part- part of your domain.
So I do recommend using it,
but it's not gonna figure out everything because you have- you have connected.
Like everything is connect- everything is connected with everything.
Then you have dependencies between all your variables.
All right, okay.
So, so summary so far is, uh, well,
we've been talking about backtracking search on partial assignments,
we talked about dynamic ordering,
so how to order our variables and how to order our values.
We decided to order our variables based on the most
constrained variable because if you're failing, you wanna fail early.
And we decided to order our values, like,
if I'm picking red, blue, or green, uh,
based on the least constrained value because if you're- if you've,
if you've decided to pick a value,
you should try to succeed.
So, so that's kind of the intuition behind it.
And, and look ahead is useful,
forward checking is, uh, one way of doing it.
So it enforces arc consistency only on neighbors,
our consis- cons- consistent AC-3 enforces arc consistency,
on neighbors and their neighbors and just goes over all the arcs that,
that we have in the- in the graph, okay?
All right. So that was kind of the set of algorithms I wanted to talk about,
but next time we're going to talk about more,
more inference and learning type,
types- type of algorithms for CSP.
So now what I wanna do is,
I wanna spend a little bit of time talking about modeling.
So, uh, we've talked about two examples now, right?
The, the map, the map coloring.
And this one is also, like, picking colors.
These, these are the examples we have talked about so far.
So, so let's look at another example.
So, so let's say that we have three sculptures, A, B,
and C and they're gonna be exhibited in a museum or in an art gallery and,
and I have room 1 and 2.
So they can be either in room 1 or, er, room 2.
And I'm going to have a set of constraints,
so maybe my constraints are;
sculpture A and B cannot be in the same room,
sculpture B and C must be in the same room,
and room 2 can only hold one sculpture, okay?
So, so these are my constraints.
How would I go about this?
Well, I need to write a bunch of factors.
I need to write- I need to actually model this.
So, so let's try to do this.
[NOISE] So this was my domain.
So I have three sculptures.
So I am gonna find variable A, right?
So that's sculpture A,
it can be in room 1 or 2, okay?
Then I have three sculptures,
so I'm gonna have variable B and variable
C. Each one of them can be in room 1 or 2, okay?
So now I gotta define factors, right?
I had all these constraints.
One of the constraints was A and B cannot be in the same room,
so, so that's a factor, okay?
Let's call that f1.
It's going to depend on A and B, right?
It, it cannot- A and B cannot be in the same room.
Er, what is that factor?
It's a function, right?
Over A and B.
And that function is going to return something. What should it return?
It should return A not being equal to B, okay?
So, so that's one factor. What else do I need?
Let's just make sure that everything is okay here.
So, so far what I've done is I've defined A,
B and C. They can take values 1 and 2.
I've defined one factor that connects A and B. I'm gonna define another factor
f2 that's going to connect B and
C. And I really want a sculpture B and C to be in the same room.
So these are local variables but let's just be consistent.
So what I want is B and C to be equal to each other, okay?
So to be in the same room. So that's factors f2 that's just created here.
And what was the last thing I wanted?
Each room, one?
Yeah. So every room gets, uh, one, right?
Was it every room or was it- actually I don't remember.
Second room. [OVERLAPPING]
Second room. Okay. Second room only gets,
um, only gets one.
So that's a factor that depends of- on all three of them, right?
It depends on A, B, and C. And one way to enforce that is- what I am gonna say is,
well, if A is in 2 or if B is in 2,
or if C is in 2, right?
So I'm gonna- I'm gonna write indicator functions if A is in 2,
B is in 2, C is in 2.
And if I add those up,
well, that should be what?
That should be less than or equal to 1, right?
Because I don't want there to be more than one of them in,
in a- in one room, okay?
And this is enforcing that.
So okay, I have this third factor.
This third factor is not a binary factor anymore, right?
It depends on all three of them.
And then if I step, then,
we're going to talk about these algorithms next time.
But, um, here is the assignment that, that you're gonna find.
So we're gonna find that,
uh, sculpture A is going to be in room 2,
B is going to be in room 1,
C is going to be in room 1 and,
and, and that satisfies all the factors that we just like wrote, okay?
So if you're interested in writing up more models,
use this environment, it would be cool.
So, so that was another example of, of CSPs.
So now I want to talk about one more example,
so, uh, I think two more examples.
Uh, okay. So this is an event scheduling example.
So, so the event scheduling example is,
I have E events.
Let's say these are classes or yeah,
different courses that you're taking.
And then you have T times slots.
So you have E events and T time slots, okay?
And you wanna schedule- you wanna schedule
a time slot for an event that- that's what your plan is.
So it's a scheduling problem.
And you have one of two constraints.
So the first constraint is,
each event must be put in exactly one time slot.
So each event in exactly one time slot.
One time slot T. So that's one constraint.
Another constraint that you wanna have is,
maybe we want each time slot T to hold at
most one event because we don't want them to overlap.
So each time slot T you want that,
uh, it can have at most, uh
At most, uh, one event,
at most one event.
So this is another constraint,
at most one event.
And then maybe I have a set maybe,
maybe event E is allowed in time slots T only if
event E and time slot T are in some set that someone gave me some A set.
So, so I have another constraint that ensures that some E,
with its time slot,
is in some predefined fixed set that someone gave me.
Okay. So, so these are some of the constraints that I have.
And what I wanna do is I want to,
I want to formulate this problem as a CSP.
So, so how would I go about it?
What should be my variables?
What should be my variables?
[NOISE] Events? Events. [NOISE] So we're gonna go with events.
Okay. So let's say that- so we can actually have multiple formulations for this.
So one formulation, maybe the most natural formulation here
is to say that my variables are going to be events.
So those are going to be my X_es here.
And every event can take a time slot,
so the value that it's going to get is 1 through
T where T is the time- we have T different time slots, okay.
So then if I, if I start with this,
if I start with a setting where I'm saying every event is a variable,
then I kind of get this first constraint for free.
So each event E is,
is in exactly one time slot because I have my variables E,
they're not gonna get multiple values assigned to them,
they're gonna get one assignment.
So if they get one assignment,
I kind of already get this one for free.
The second constraint is this constraint which makes sure
that each time slot can have at most one event.
So to ensure that,
then I need to make sure that X_e is not equal to any some other X_e prime, right?
Because, because X_e is my variable,
my event variable, it's gonna get a time,
time slot value,
the two time slot values for two different events should not be equal to each other.
So the constraint that I have is X_e is not equal to X_e prime, X_e prime.
Okay. And how many of these do I have?
Well, I have like order of e squared of them,
right, because I have- let's say I have e events,
so I have- so then I have e times
e options here to make sure that they're not equal, equal to each other.
So I have e squared, uh,
binary, uh, factors, okay.
So, um, and then I have another constraint which tries to ensure that these,
these events and their time slots which is the X_e value is going to be in some set A.
You can kind of treat this as a unary factor,
so you have some number of unary factors,
you have e unary factors here.
Okay. So, uh, and, and the,
the, the number of variables that you have is,
uh- the number of variables you have is e but their domain is size T. So,
so it's, it's good to think about this.
Because if you have multiple choices,
so I'm gonna talk about the second choice in a second.
But if you have multiple choices for modeling this- it's
a good idea to think about what type of factors do you have.
How many of them do you have?
So here are like the worst-case scenarios.
I have e squared binary factors.
Okay. So I have another option, right?
So in this option what I did was I took,
um, the events as my variable.
The second way to formulate this is to say,
well, maybe my variables are just the time slots.
So, so maybe I'm gonna go a different,
different approach, take a different approach for modeling this.
I'm going to call it Y_t,
Y_t are the time slots.
So I have variables for time slots.
Each one of them can either take an event or maybe there's no event added,
that's empty, empty value.
So, so they either take an event or no event.
And if I model this, this problem using,
using the second approach,
then I get the second, the second one,
the second constraint for free because my variables are again time slots,
so I can, I can, I can satisfy the second constraint.
And then for the first constraint,
I actually need to write something for the first constraint which
says each event is in exactly one time slot.
So I'm gonna write a constraint that says Y_t,
this time slot is going to get an event for exactly one t. So,
so this particular constraint that I have,
it's how many variables does it have?
If I want it to be exactly one time slot.
Remember the sculpture example?
I wanted it to be exactly in one room.
It needed to depend on everything else.
So this is going to be a t-ary constraint, right?
So I have t variables here.
So previous formulation, everything was binary or unary.
Here, I have a t-ary constraint.
I have less of it but I have a t-ary constraint.
Okay. And then I'm going to have another constraint to just ensure this,
this last, this last constraint.
Okay. So, so one way to think about
these two different approaches is how many of these constraints do I have?
So, so we just saw that we have a t-ary constraint here,
one thing that you can actually do,
and I have the slide afterwards about that,
is if we have some,
some n-ary constraints, some t-ary constraints,
some constraint that depends on t number of variables,
I can actually change that to order of t binary constraints.
So I can actually like reduce down to binary constraints.
So, so I can make these two algorithms- not algorithms, sorry.
Make these two models to have all binary or unary constraints.
So that part is fine.
But one of them is going to have t number- like order of t number of factors,
the other one is going to have order of e type factors.
And the question is,
well, which one should we use?
And it really depends on if your e is greater than t or t is greater than e, right?
So if you have,
if you have more time slots than events,
if you've a lot of time slots and you have like five events, let's say that you want,
you want to set,
then you should use the first algorithm because that,
that was where we had order of e squared number of constraints.
But if you have it the other way around,
which is again less natural but maybe you don't want to,
you don't want to- you're okay with not assigning all events a time slot, so,
so if you, if you had it the other way around,
then, then you can use the second formulation.
So the point of it is you might have different ways of formulating a problem,
you should use the one that,
that is the most beneficial depending on
what, how many constraints you have and then so forth.
And then one last thing to,
to- before, before we head out,
so I just said if you have an n-ary constraint,
we can actually write down binary constraints that are equivalent to this.
And the reason is usually our algorithms require having binary or unary constraints.
Here, I have, I have a setting where I have this or between X_1,
X_2, X_3, and X_4.
So the way to make this- makes
the binary constraint is what we can do is we can define an auxiliary variable.
So, so I'm going to define a new variable,
and this new variable, I'm going to call these A_is.
And these A_is are going to be just the result of- the or of A_i minus 1 or X_i.
Okay. So what's happening here is,
let me just draw this real quick.
So I have a setting, ah. I'll just do one.
So I had X_1, X_2, X_3,
X_4, I can have an n-ary constraint that
connects all of these together to one factor graph.
What I can do is I can actually define new variables.
So I am defining that many new variables, A_2, A_3,
A_4, and then I'm
ensuring- and I'm defining new factors where A_1 is the result of these two ors.
So I'm going to just draw this like this.
A_2 is the result of or of these two variables.
A_3 is the result of or of these variables and so on.
This is not binary, right?
What is there right here? It's three.
So we need to do one more step, like,
after you're defining these auxiliary variables, after that,
we need to define- we need to do one more step where we define
a new variable B which kind of represents A_i and A_i minus 1.
So I'm going to replace these two with just one variable.
I'm gonna call it B_1 and I'm gonna just connect that to X_1.
I'm not going to draw it,
but you get the idea.
So, so B_i's are just going to be representing A_i minus 1 and A_i.
And that allows me to have binary,
um, binary factors here.
Okay. And, and by doing so,
I'm adding actually one more constraint.
I actually need to add a consistency constraint that makes
your B_i minus 1 of 2 is equal to B_i minus- B_i of 1.
Just ensuring that like pre and
post are staying the same as we're moving through the graph.
So that's another reinforcement.
All right. Let's chat next time about this more.
 Today, we're going to be covering CSPs,
but possibly just as importantly,
you're all halfway through the quarter.
So congratulations.
I think six more weeks to go,
so keep it up.
Uh, also happy Halloween,
um, hope you all have fun.
Uh, but today, we're going to be talking about CSPs,
so this is continuing, um,
these topics that we've been covering since Monday,
which is about, um,
this kind of setting, right?
So you have variables,
in this case we have three,
X_1, X_2, and X_3.
And each variable represents some kind of
discrete object that can take on one of several values.
And so the set of values that a single variable can take on is called its domain.
[NOISE] And, um, just to continue reviewing,
these variables have factors which are
functions that take as arguments one or more variable.
And the factors basically say, okay,
how much do I like this assignment of my variables?
So for example, factor 2 looks at X_1 and X_2 and it says,
okay, X_1 has been assigned some value,
X_2 has been assigned some value.
How much do I like that assignment? Do I really like it?
If so, you give it a high value, do I not like it?
If so, you zero it out.
Um, and so importantly,
um, we call the arguments,
so all the variables that we give to a factor,
that's the scope of that factor.
Um, so we had this example that we talked about on Monday,
and I'm just going to revisit it.
So in this case we have, um,
variables correspond to people and there's three people.
And, um, we know that first of all,
the first two people, person 1 and person 2,
they have to agree to each, each other.
And then person 2 and person 3,
they sometimes agree with each other, but not all the time.
Um, and so first, what we say,
is with our first factor,
we say that person 1 is definitely blue.
So we say this here, um,
by saying these are the values that the variable can take on, red or blue.
And this is how much we like that value.
And what this fac- first factor is saying,
is that it doesn't like red at all,
it's a 0, and it likes blue, it's a 1.
And then we have the second factor,
this encodes the fact that they must agree.
And what that's saying is that every time,
both the first variable,
which is X_1 and the second variable which is X_2,
every time they agree, where either they're both red,
or both blue, we give it a 1,
otherwise we give it a 0.
And then we have this factor which says that they tend to agree,
and we're encoding that by saying, well,
if they're- if both of its arguments are the same,
if they're both red or they're both blue,
then we give it a slightly higher number than if they differ,
but we're not going to zero it out.
And then last we say, okay, well,
the las- last person kind of prefers to go red,
but, uh, you know, it's not a hard constraint, nothing like that.
Um, and so then again,
so like we talked about last time, um, assignments,
which is like if you have
all your variables and you have all the values that you've assigned to them,
they have what's called a weight.
And a weight is basically just, um,
plugging in all the values for your variables into your factors and
then multiplying them all up to get that product. And that's the weight.
And our goal across all of these problems, in this whole unit,
is to find assignments for all of our variables that will give us the maximum weight,
when we multiply up all of our factors, Yeah?
Is there any particular reason to why you define this product as opposed to a sum?
So the question was,
why is the weight a product and not a sum?
And the answer for that is because,
um, remember these are constraint satisfaction problems.
And so if you look back at this example,
we wanted to encode the constraint that they must agree.
And we did that by putting in a 0.
And since we're multiplying it up,
if you have a single factor that gives a 0, it's like a veto.
Um, and so that veto power is actually really critical,
and we leveraged it,
um, on Monday, which I'll talk about soon.
Um, so on Monday,
we talked about an- one algorithm for solving these kinds of problems.
We called it backtracking search.
And it's, it's a kind of an exhaustive,
you could think it as like a depth-first search of all the possibilities.
So we have this example, um,
from last week where we were coloring provinces in Australia, right?
Um, so we have- I actually don't know,
the provinces of Australia,
but we have WA and we have V and we have T, I think, T,
is Tanzania, um, Tasmania.
Okay.
Um, I'll work on my geography.
In fact, uh, we color them all red, right?
And so what we decide to do is we decide that, hey,
we'll color Q, red. Let's do it.
And then we say, hey, if we color, Q, red,
let's go with, um,
green here for NT, and then,
we pick blue for SA,
and then, um, green for NSW,
and we've completed that tree.
We've gone as far as we can, we've found a coloring that works.
We say, oh, wait, what if we backtrack up to this point and try to sub in blue in for NT,
instead of green, which is what we had before?
And then we can do the same thing. We go down that branch of the tree,
or we can try different colors for Q.
So for example, what if we try Q, over here?
We take that tree down.
Oh, but that gives us something that is,
um, is not as satisfiable.
So for example, here in SA,
no matter what color we give it,
according to our factors which say like you can't have two neighboring colors,
it will be not allowed, um,
and you fill up the tree this way.
And so this, this is the algorithm we covered and, um,
it works because it always gives you a good version,
but it's not working because it's super slow. It's exponential time.
So there's N nodes and each node has a domain.
And so you're, you're- it's like for each value of,
um, here, I can draw it out.
So like for example, if we had two fact- if we had two variables,
X_1 and X_2, um,
and they both took on three values.
[NOISE]
So these are va- our,
um, variables and these are the values they can take on.
And these are how much our factors like them.
Um, then you would have to say for each value in X_1,
for each value in X_2.
So it's this, um, exponential blow up,
which is just very slow.
And so we learned some kind of, uh,
like heuristic ways to speed things up and prune off that tree.
So we did forward checking, um,
which is where like once you decide my value for one variable,
I'm going to go ahead and propagate that decision as far as I can.
Um, that shrunk it a little bit.
We looked at dynamic ordering,
which is like, okay, which variable?
And I'm going to choose to work next.
And then once I've chosen that variable,
I'm going to choose my value a little more intelligently.
I'm going to pick the thing that has the least wiggle room because maybe that'll help us,
um, all- once I can,
and that helped us a little bit.
Um, but at the end of the day, these constraints,
they helped us prune the tree,
but they can only work for constraints.
Only if a factor gives us a value of zero, does it work.
Because that's when it has veto power.
And we use the vetoes to say,
this branch of the tree will never be useful,
so we can never go down it.
So if we have factors that are going to be non-zero,
we can't use any of these things, Yeah?
Could you have a little more on the example of how you got the numbers.
So like for, for one variable there's only one,
and then I guess it's 2_N minus 1.
Yeah. So for this, I guess this example,
what- it would look like as a graph.
I guess it wasn't the best example.
But, and actually later in lecture today,
we're gonna talk about exactly how you could go about this more smartly.
But let's say you just had two variables,
and each had unary factors.
If you're already running binary like backtracking search,
we would still say were- we would try all different combinations of both variables,
um, which is a very dumb thing to do.
[inaudible]
Yes, so that's the backtracking search.
Interesting.
Yeah. So we'll discover smarter ways to get around that.
Um, yeah, so that's backtracking, slow,
but it gives you the optimal solution every time,
so maybe a mixed bag.
Um, okay, so I'm going to lead into this as
a running example that we're gonna be talking about the whole lecture, Object tracking.
So with object tracking,
you have sensors that are telling you like, oh,
my object is here, no,
it's down here. Wait, it's over here.
And what you wanna do is you want to take that noisy observation and,
and run it through your CSP to
infer a more realistic estimate of where the object actually is.
So I will also draw this on the board.
Um, [NOISE] so,
so what this looks like is if we have,
um, this is time,
so we'll call it T for time.
And this is position,
uh, so we'll call it p for position.
And what this is going to say is we have sensors that are giving us estimates,
like noisy estimates for where this thing is that every point in time.
So for example, maybe at time step 1,
um, we get an observation down here.
At time step 2, it's up here,
and at time step 3, it's up here.
Um, but in reality,
like we want maybe something more like that.
Um, and so that's going to be our goal with this running example.
Um. [NOISE]
So this is how we encode it into a factor graph.
Um, in, in this case, we have variables,
where the variable is our guess for the real location for that object.
We have two kinds of factors,
we have unary observation factors, um,
that say how close is our guess to the observation, to what our sensor said.
And then we have transition factors,
which basically tell us,
um, you can't change your guess by too much from timestep to timestep.
So on this graph,
what it would look like is if these are observations,
so this is like o_1, o_2, and o_3, um,
then maybe our first guess would be here so we have X_1 down here,
we have X_2 down here,
and then maybe like X_3 up here.
Um, and so our observation factor is going to look at this distance.
And that would- what this is going to say is, okay,
how far is our estimate from the observation?
And they want that to be close.
And then our transition factors are gonna look,
are going to look at this distance between one object- one guess and the next guess,
and say, well, our guesses shouldn't be moving too much. Uh, yeah?
What's the difference between observation and estimate?
Um, so the observation is what this sensor gives us,
and the estimate is,
is us saying thank you for the- thank you sensor.
Um, now, I think the person is actually right here,
because the sensor is noisy,
we can't trust it, yeah.
Um, okay, so that's how we're going to set up this problem.
Um, and I think so there's and,
and- there's this really cool kind like
Java applet that you can all play around with on your own time,
um, and I will briefly walk you through it.
Um, so what's going on here is,
um, basically- so there's a lot of documentation that you can read.
Um, but basically what's happening here is we're just creating these variables,
we have three variables,
and we're allowing them to go in three positions.
So in position 0, 1 or 2.
Um, and then this is a little function that's basically
encoding the fact that if things are nearby,
then we want it- then we like that.
So we have two variables, A and B,
and if they're in the same position,
then we return 2, we really like that.
Um, if they are only one away from each other,
we return 1, so it's okay, we'll take it.
And if they're further away than 1 from each other,
then it's- we zero it out.
That's a hard constraint, we don't like that.
And then we have this observed function,
which is kind of a higher-level thing, and it kind of,
um- I guess you could say it kind of like preloads our nearby function,
um, with, with a variable.
Um, we're going to create our factors,
and then this is what it looks like.
So we have, um,
we have three variables,
we have X_1, X_2, and X_3,
and then we have our observation factors are unary.
Um, remember that say, okay,
you have to be close to the sensor,
and then our transitional factors are binary.
And that says you can't move too much between time steps.
Um, and you can run it,
and there's actually a lot of output here,
and I'm gonna ignore,
um, most of this for now.
I think the thing that is important here is that we ran backtracking search,
and we found the optimal assignment.
So in this case it's 1 for X_1,
2 for X_2, and 3 for X_3,
which gives a final weight of 8.
So on our little drawing,
um, basically what that's saying is,
is it saying something like this is optimal where we,
we put- we say,
thank you sensor there for these estimates,
but we think the person is actually here at time step 1,
here at 2, and here at 3.
That's the solution that backtracking gave us.
[NOISE] Uh, yeah?
[inaudible] so those are the weights or the timesteps because you were saying,
it's like one then two.
So X_1 1 this is a timestep 1,
and X_2 is timestep 2,
but X_3 is also timestep 2.
Yes, so the question was,
what do you mean by here at 3?
And what I mean is basically, so X_i,
X_i is an- is our,
um, estimate for the position of this object.
And so at timestep 3,
our estimate for the position is at position 2. And.
So is your timestep [inaudible]?
Uh, yeah, so X_2 and X_3.
So that's why there's X' at two for both time- this is like timestep 1,
2, and 3. Yeah. Yeah?
So I- I understand like what you're talking about with the scope of the factors,
but how exactly is their constraint being [inaudible] by the problem?
Yeah, so the constraint here is,
um- so the fact that if we look at our nearby function,
and if A and B are farther than 1 away from each other, then it returns 0.
And that's a constraint because when we're calculating the weight of a factor graph,
we multiplied together all the factors.
And so if there's a 0 in there,
then the whole weight goes to 0. Um.
The constraint is only for the transition factor but not for the [inaudible].
Yep, yeah, you can say that, yeah.
Great, um, okay so that's our setup,
and we're gonna be returning to that a few times.
Um, awesome.
Uh, okay, so moving on backtracking search,
very slow, let's try to speed it up.
Beam search, faster.
Yay, uh, beam search,
so backtracking search, if you- we have that tree analogy, right?
And backtracking search exhaustively searches the entire tree,
gets us the best solution, but it's very slow.
Um, and so one way to exha- avoid this kind of exhaustive search is greedy search,
which is where you greedily- it's like, um,
for each variable, you greedily select the value that gives it the highest weight.
Uh, so it's right here,
you look at the values it can take on,
and you just choose whatever variable- whatever value is best.
And you never look back, you just keep on running through it.
Um, and you go through the whole tree this way until you end up at a complete solution.
Uh, so the benefits is its very fast, right? It's linear.
But the con is that it's a very narrow window,
like you don't see a lot of the state space,
you don't explore a lot,
and so you can often miss the global max.
Um, so for people who,
who prefer this kind of notation,
what we are doing is we basically say,
we loop through all the variables, and, um,
we try out every value,
and we just take whatever value has the highest weight.
Um, yeah, so beam search,
um, is kind of like an in-between backtracking and greedy.
Beam search is very cool, so one way to think. Yeah?
Excuse me. Come up again, [inaudible].
Yeah, so the question was explain that greedy again,
and so with greedy,
what we're doing is,
is we say, so we have- we have a partial assignment, right?
And we pick, we want,
we want to extend our partial assignment.
So we pick a new variable,
and we try out every single value that that variable can take on,
and then we take the value that,
that, that gives it the highest weight.
So all the factors touching that variable are the most happy with that value.
And we pick that, and then we never look back,
and then we pick a new variable.
[inaudible] the new value.
Yeah, so that's a good observation,
we can end up at inconsistent solutions, and that's totally true.
Um, so you can actually,
you can- and during our greedy search,
you can actually kinda like find your way in kind of a hole where it's like,
oh damn, you know what?
[LAUGHTER] I can't go any further.
Um, and it's a big problem with greedy search.
Yeah, so with backtracking, um,
beam search is kinda like a heuristic way to maybe get around that.
So with beam search,
so remember again, in, in ,
um, so for greedy search,
we had one partial assignment, right?
And we were choosing one variable,
and choosing the extension of that one variable.
With beam search what you do is instead of one partial assignment,
you maintain a list of k partial assignments.
In this case, k is 4.
And then what you do is on every step is for each of your partial assignments,
you pick a new variable,
you try all the ones.
And then what you do is you- so you have your k partial assignments,
and you try to extend every partial assignment,
you test out all the values for every partial assignment,
and then you sort those partial assignments based on their weight,
and then you just take the top k. So it's like you have your partial assignments,
you extend them into all the possible successors,
you sort them based on weight,
and then you take the top k. So in this case,
if we have, um, four partial assignments,
then we try extending them all in the two directions they can go,
and then we sorted them and then we took the four,
you can see there's four things that are filled in that have the highest weight.
And we continue this procedure.
So we say, um,
so what we would do in this case is we would say, okay,
for each of these four solid things,
we're going to try out,
we're going to extend each of those partial assignments,
and then we sort all the extensions and select the top k. Um.
So yeah.
It won't start if it is not K partial sums yet. You just sort of [inaudible].
Yeah, exactly. So the question was up to the part K, right?
And the que- and the answer is yes.
Yeah. So for example here, for example,
1 is less than K so we extend to 2.
2 is still less than K, so we can extend completely.
Um, so in- in notation, um, uh, yeah,
so we say for each- for each variable we- we try to extend each of
our partial assignments and then we prune out everything but the K largest,
um, like best K weights.
Um, so beam search is also not guaranteed to find the best weight.
Um, um, but what's cool about it is that
it- it gives you kinda like a knob that you can
control between being greedy and exploring a lot.
So if K is very wide,
then you explore more and more of the tree,
um, and if K is actually infinity- I think this is on a slide soon.
Um, yeah, so if K is infinity,
then that's actually like I can't do a breadth-first search of the entire tree. Yeah.
So on that graph or on the picture where you have like the solid and the shaded out ones-
Yeah.
For that shaded out grey ones we would actually never explore those.
Yeah. So- so this picture,
I don't think it's the best picture because, um,
so for example, okay,
so I'm looking like- I'm looking right here,
and in reality you would never extend this.
Because it was never selected, um-
But you're still like initially explore it and then find that it's not-
Yeah, so up here, so at this point,
we do consider it, because we extend down,
but then we decided not to select it.
Yeah. Awesome. Um, runtime.
So for- okay, so for beam search you're selecting a single node,
and then for, um,
for each partial assignment,
you're trying out every value in the domain.
So if b is the size of your domains, uh,
then you're trying out b things for every partial assignment,
and you have K partial assignments, right?
So tho- those are the number of extensions you have as Kb,
and then to- you sort them,
and to sort it take- if you have a list of length n, sorting is n log n,
and so you sort your list of Kb,
so that becomes Kb log Kb,
and then there's n nodes that you need to- the height of this tree
is n. So you do this n times.
Uh, so like I said,
beam search of the K gives you this really cool knob,
between do you wanna explore everything,
or do you want to focus in on um,
on, you know, being fast and greedy?
Um, okay. So, uh,
everything until now what we've covered is- is extending partial assignments.
So we have- we're giving like a blank slate,
picture of Australia with no colors,
and we say, "Color me Australia."
Like build up this house from the foundations,
and now what we're gonna talk about is, okay,
given like a map of Australia that's already filled with covers- colors,
how do we make changes to it in order to improve it?
And that's local search.
Uh, so the first algorithm is called Iterated Conditional Modes,
ICM, um, and what ICM is doing is it says, okay,
we pick one variable, and then we ask,
how- what is a new setting that we could choose
for this variable that would improve the overall weight?
Um, so in this case,
we have one variable, it's x_2,
and we try out all the different values it can take on,
which is 0, 1 or 2.
Um, and then for each of those values we go through and recompute the weight,
and then we pick whatever value is best.
So we start with 0, 0, 1,
and here it looks like 1 is a better choice.
So we go with it, and- and from this,
from now on we would say x_2 is equal to 1.
Um, so something cool about ICM is
that when you're evaluating a new value for a variable,
you only need to consider factors that touch that variable.
That's all you really need to recompute because
everything else is constant with respect to it,
and so that gives you big, big, big time savings in practice.
Uh, one last thing is so the name Iterated Conditional Mode.
So iterated comes to the fact that you could solve the whole CSP this way.
If you just iterate over which variable you're selecting.
Um, conditional means that once you select a variable,
you're clamping down the values of everything else, and then modes are saying,
once you select your variable,
you- you try out every single value for that variable.
Excuse me.
Yeah.
It just like kept bringing us every variable would you eventually like arrive at
the optimal solution or could you like kinda like end up in like-
Yeah. So the question is if you kept- so if you- if you have
your three variables and you keep on going through them and- and choosing one,
clamping the others, and then choosing the best value for it,
would you arrive at an optimal solution?
And, uh, the answer is no.
So, and we'll see- and we'll see that in practice, yeah.
Um, yeah.
So again, this does- this is just
to give you an illusion that we iterate through these variables [NOISE],
and for each variable,
um, we pick a weight that improves it.
Um, [NOISE] so we have this in the demo.
So in this case what's going on here,
is we're saying, okay,
right now we've selected,
we're looking at x_1.
Um, we start with a random initialization.
We start- we're looking at x_1 and-
and these are the different values that x_1 could take on,
and then we go through, um,
and calculate the weight,
and we say, okay, 1 is the best weight for x_1 right here.
So we choose 1 to be x_1,
and we step again, um,
we're looking at x_2 now, and oh,
it looks like actually a value of 1 is better for x_2,
and so from now on, we choose the value of 1 for x_2.
And we iterate again,
now we're looking at x_3,
and it looks like we choose the value of 1,
and you just keep on iterating through this until you hit some kind of local optimum.
And why I'm saying this is important that it's a local optimum because right now, um,
it's converged, so I can keep on pressing step and it's
not gonna change, but the weight is 4.
And if you remember during the other thing when we
were in back-tracking we actually found an assignment with a weight of 8.
So it does- it can fall into these local optima.
Um, one way around this is a second algorithm called Gibbs sampling.
Um, with Gibbs sampling what we do is we injected
some randomness into the process to try to like bump us out,
um, bump us out of those local optima,
into something that can maybe get us into a better area.
Um, so basically Gibbs sampling is super similar to ICM.
The only difference is that instead of,
so you- you try all the values,
and instead of selecting the value that gives you the biggest weight,
you sample the value,
um, according with probability that's proportional to its weight.
So for like this example,
we say, se- uh,
setting of 0 would give us 1,
value of 1 would give us weight of 2.
Value of 2 would give us a weight of 2,
and then to get the probabilities we take
the weight and we sum by- we divide by the total,
which would in this case will be 5.
We sum all up and divide by that.
That gives us the probability of 0.2.
So here, we say it's 2 divided by 5,
which is 1 plus 2 plus 2,
and then we use that probability distribution to sample
a new value for x_2 instead of just choosing and say, oh, you're the best.
Um, so this is the demo.
So in this case we're looking at x_1,
and we're trying out different probabilities for it,
and we have weights,
and then that gives us a probability distribution,
and we sample from this probability distribution.
So in this case, it looks like we sampled and we chose 0 for x_1,
and then we keep a record of how many times we've,
um, ended up with cer- a certain assignment.
So if we step again,
now we're looking at x_2.
Um, it looks like x_1,
um, a value of 1 is the only thing that works.
So we choose that value,
and we add it to the counts.
Um, and you can just keep on running this process, um,
and over time [NOISE] you build up this kinda like probability distribution.
Um, so this is actually an unlikely sample.
Um, so I had a point- I had 20% chance of choosing 1,
and 80% chance of choosing 2,
but it still chose 1, and that's a way
that it can kinda like break out of these local optimums.
Um, but in any case,
if you look at this table,
then what you find is that over time,
if you run this thousands, millions of times,
then, um, in practice, uh,
settings with very high weights will occur very often.
Um, and actually when we get into probabilistic graphical models,
which Chris will talk about soon, um,
you could actually say that, um,
the global optimum will be the most frequent,
in Gibbs sampling in the limit,
which is, I think pretty cool.
Um, yeah.
Okay, so just to- just to show an example of- of what can go wrong with this algorithm,
um, it's still flawed.
So if we have x_1, and we have x_2,
and um, let's say,
um, let's say it's two people and they're trying to decide where to go to dinner.
So we could have, um,
let's say they're deciding between vegetarian and going to a steakhouse.
So we have V for vegetarian and S for steakhouse,
and they really both want to go to vegetarian, um,
and they want to eat together, um,
and they'll eat steak but they're not super crazy about it.
So if you're in this state,
then even if you're doing Gibbs sampling,
it's really hard to bounce over here because you have
these two kinda like transitionary settings in between,
and in order to make it to both vegetarian, um,
they're gonna have- one is gonna have to
make the decision to go to a different restaurant,
and so since this is so low priority,
it's- it's gonna be very difficult for these two people to go over to vegetarian.
They're both kinda like, oh,
I wanna do what you wanna do.
You wanna do steakhouse, right?
I wanna eat together, you wanna eat together, let's do steakhouse then,
and they- they both don't really know
that they will be much happier overall if they both get vegetarian.
Um, and then this- this would really be even worse if- if you had, so for example,
if you had like zeros here,
then there's actually no way for them to get there.
Because there's no probability there. Yeah.
So this is like it's basically very initialization,
like centric, but whatever you pick is your initialization is gonna be very important.
Because if you initialize to the state where they're both vegetarian,
then you'd never want to leave and you'd be happy to never leave it.
But if you initialize to both steak then you're in trouble.
If you initialize [OVERLAPPING]-
So in general, in optimization, um,
what- in any kind of optimization area where- where you can
have this problem of falling into some kind of
local optimum that's worse than the global optimum,
it's very dependent on your initialization.
Yeah, because if you initialize over here then you'd fall somewhere lower,
just by chance. Yeah.
But what if you like initialize that like SV or VS,
where you initialize to a state where it was zero,
would you have an equal chance of [NOISE] one?
Um, so the question was, what if you
initialized into a state that had zero probability? Uh.
So what you would do is you would say,
is you would select a variable,
you would select S1, and then you would try out different values for this.
You would say, "Okay, I'm going to either choose S or V holding X2 constant."
In that case, transitioning to V would have a probability of 1.
So you would do that almost deterministically.
[inaudible] or rewards?
Um, so for Gibbs sampling, these are probabilities,
you turn them into probabilities.
Yeah. Yeah.
So just to clarify, um,
Gibbs sampling is not guaranteed to find the best assignment?
It is not guaranteed, yeah.
But you mentioned something like,
in the limit it is?
Um, yes.
But that's- so that's not like- that's kind of
a theoretical point, it's not really practical.
Yeah. it's- so in the limit doesn't mean like,
I guarantee, I guess,
um, yeah, I guess you could interpret it that way.
Um, yeah.
So I guess,
this is directly about your question.
So if you were to compare these questions, uh,
which ones are guaranteed to give you the maximum weight assignment?
And the only one is backtracking search,
because greedy is too narrow,
beam search is maybe too narrow, um,
ICM is too myopic,
and Gibbs sampling is you- it's not a guarantee,
it's likely, but it's not guaranteed. Yeah.
[inaudible].
Sorry. Can you say it again.
For this example, even if the limit,
it wont converge to the optimum [inaudible]?
Yeah. So the question was, In this example,
even in the limit, you wouldn't converge, and that's true.
So if you- if you initialized right here,
there's no way for you to get over there.
Yeah. So yeah.
I think it's just in certain conditions.
Yeah. So that I did- I wasn't- my intent wasn't to confuse you guys.
So it- let say that it was a theoretical point for like,
some subsets, some CSPs yeah, yeah.
Like you said, if we suppose initialized X steak state it-
there's no point that we can go to like, vegetable, vegetable, right?
Because in the- the middle transition part equal 0.
So, uh, is it like safe to say that, uh,
when you are modeling a problem,
uh, with Gibbs sampling, you should never read,
even if you encounter probability zero just give them
some epsilon probability so that there might exist some chance to,
uh, go to the optimum?
Yeah, so the question was, uh,
is it worthwhile to add some like plus 0.0001 to these just to give it some probability?
And I- my intuition says that's it sounds like a pretty good idea, uh,
it sounds kinda like adding tiny little epsilons to avoid division by 0,
but, um, I think it would depend on the problem that you're solving.
Though, you can imagine some cases where you would really want those zeros.
Um, okay.
So just to summarize so far, um,
we've learned two ways of extending partial assignments in these graphs.
So one is backtracking search we learned last time,
which is like a full search of that tree,
gives you the exact solution every time super slow.
Beam search is approximate,
and it gives you this cool little knob to trade off, um,
speed and I guess,
you could say like success, um, or exactness.
And then second, we learned ways to given a assignment to improve it, to modify it.
One was ICM, which was approximate,
and then, um, the other is Gibbs sampling,
which is also approximate,
but it uses some probability,
um, and some randomization.
And now what we're going to look at, we're gonna look at two ways to solve these kinds of
problems by actually changing the structure of the graph itself.
Um, so our motivation comes from Australia,
um, Tasmania, [LAUGHTER] That is- that is the motivation.
So Tasmania, if you remember,
was completely disconnected, right?
So I think we called it red in the previous example,
but it doesn't really matter what color we gave her, right?
We give it anything we wanted.
And so what we want is we want to kind of leverage this property.
And more than leverage it, we want to- we want to kinda
like inject this property into graphs that don't exhibit it.
We want to- we want to like build this probability,
and this property out of graphs.
Um, so first we were like, what is this?
It's called independence, and it can speed things up.
So just like I said before,
um, so is it still there?
My old- Oh, it's partially erased.
[LAUGHTER] Um, so if we write this down again.
Um, so this is the same examples before.
We have two variables each with a unitary factor on it.
Um, with backtracking search,
I told you what happened, right?
You do something really dumb,
which is you do,
it's- it's like two for loops, right?
You try out every combination of them, which is exponential.
Whereas what you could really do is just say,
"Okay, for each thing,
I'm gonna choose the value that maximizes,
that makes my factor most happy." And that's linear time.
And so that would give us a more efficient algorithm.
Um, oh, no.
Okay. And we call that property independence.
So A, um, so in this case,
X1 and X2 are independent,
and the reason that they're independent is that there is no factor connecting them,
there's no paths between them,
and there's no edges between them.
And so we call that independence.
And that's the same thing with Tasmania and the rest of Australia.
Um, so I don't know how independent it is, like culturally or politically,
but it certainly is in terms of like map coloring, graph theoryness.
Uh, in symbols, so we use this,
we use this kind of cool-looking pipe thing,
um, and that denotes independence.
So yeah, so like we said, Tasmania is independent.
Um, what about cases like this?
So it's not quite independent,
but it's almost independent, you know,
only if X1 didn't exist,
then the rest of them would be independent.
Um, and so that- this is where we introduced this idea of conditioning.
And, um, conditioning is a way to rip nodes out of a graph.
So- and we do that by- by saying,
okay, in- in this example, let me draw it up.
Um, so it's X1 and X2,
and then it's 1, 7, 3, 2. 1, 7, 3, 2, and red.
So 1, 7, 3, 2, red, red, red,
blue, blue, red, blue, blue, 2, 3, 7, 1.
Okay. So we're saying X_1 and X_2 are connected to each other with the factor right?
[NOISE] But what if I say X_2 is definitely blue? Yeah.
So we're talking about constraint satisfaction problems,
but also like graph problems will do well,
like all constraint satisfaction problems can be all be written as like graph problems?
Yes. So the question was, can, uh,
constraint satisfaction problems be written as graphs and as all of them?
Um, uh, yes, yeah,
because fac- because variables are nodes and factors are edges, yeah.
So it's actually really elegant way to think
about it because then you can bring in all this graph theory stuff.
Um, can you also see the board?
No.
Okay, I'll try to write big.
[LAUGHTER] Um, but, um, okay.
So if- if I say X_2 is definitely blue,
like, trust me it's blue. Then what does that let me do?
Um, it let's me cross out all the rows where X_2 is not blue.
So here, X_2 is red,
so I can be like,
"You are never gonna happen.
I know you're not the case, you are not true."
And for you, same thing,
you are not gonna happen.
And now once I fix X_2,
I don't- all of X_2s values are the same.
So this doesn't really add information to this table,
and I can just drop that data from the table.
And what that gives me, is it gives me a reduced table,
which is just a unary factor of X_1,
and that it can take on red or blue,
and then I have a value of 7 if I choose red,
and 2 if I choose blue.
And graphically, what that looks like is, I'm deleting X_2,
and I just have one factor now,
X_1, and it has a unitary factor.
Um, now the price to pay for that was I had to assume that X_2 was blue.
I had to condition on X_2 being blue.
Um, so notationally or maybe like programmatically,
uh, I think a good way to think about it,
is it's kinda like you're taking this- the factor that touches X_2,
and you're kinda like preloading it with a value for one of the arguments,
and then the rest of it is untouched.
So you choose a value for a variable,
you remove the value,
the variable from the graph,
and then you- you stick in that value,
you preload the associated factors with that value.
Um, so for exep- example,
if we were to condition on these,
and I'm saying SA is green and Q is blue- as red, trust me.
And then what that does is it rips those out of the graph,
and everything touching, um,
everything touching those conditioned variables turns into some kind of a stump,
and you preload the value for those variables in.
Um, the only edge that gets removed here is the one that connects SA and Q.
And that's because you have a function that takes two arguments,
but you've preloaded both of those arguments already,
so the- it's done, there's nothing to do there.
Might as well not exist. Um, just an example of the new factors that go in.
So NT, for example, um,
that would- this factor,
which you used to say NT and Q can't have the same color.
It- it turns into this little thing,
which just says NT can't be red.
And you know that because you are assuming that Q-
you're conditioning on the fact that Q is red. Yeah.
[inaudible] independence, is it like two variables have
no edge between them or no path that can connect them in the ground?
Um, yes, so the question was,
is independence no edge?
Er, does independence mean is there no edge between them or is there no path between them?
And, uh, we will talk about that.
Um, so for independence, um, uh,
you'd need both true, um,
but then we're gonna cover another form of independence called conditional independence,
uh, which is just the latter, yeah.
How is this different from the idea of extending partial assignments,
because actually like starting with partial assignment in the graph,
and you're kind of exploring or optimizing
the rest of the nodes based on a partial assignment?
Yes. So the question was, how is this different from extending partial assignments?
And, um, it's not so- that's actually a good point.
I think- so conditioning might actually be a case of building up partial assignments.
Um, this- so what- I
think the main thing to think about is now we've kind of like moved on.
So partial assign- extending
partial assignments and modifying existing partial assignments,
that was kind of like our old world, which is where, like,
this graph structure is very perfect and we can't touch it or change it.
Now we're in a new world where we're allowed to change the structure of the graph.
Um, and so we kind of like left that way of thinking. Yeah, yeah.
What drives the decision-making process, so
it'd like setting it to green or setting it to red?
Um, what drives the decision process of choosing these values?
Um, we will get into that. Yeah, yeah.
And how do you know exactly to condition those two,
um, and that you end in a re- independent result afterwards?
Yeah, so the question was, why did we choose to condition these?
So in this example it's arbitrary,
but soon we're gonna cover ways of choosing them. Yeah.
How is it different from forward checking?
What?
How is it different from forward checking?
How is this different from for checking?
So in forward checking,
what we were doing is- is we were propagating the decision
forward and to reduce the domain of existing factors.
Whereas, in this one,
we are- we are changing the structure of the graph itself,
and we're- we're literally removing variables and inserting new factors.
Um, okay, so I'll move on.
Um, graphically, in general,
what this looks like is,
if you have a variable that you want to condition, um,
you rip it out of the graph,
and then everything that touches it turns into some kind of a stub,
um, with the value preloaded in there.
Uh, this is just a picture of the same idea.
Um, yeah, so I guess you could say
this is the main- this is the big difference is that whereas in forward propagation,
um, we were keeping the existing graph and we're
just propagating our decisions to reduce domains.
Whereas now, we're- we're literally removing this variable and the factors with it,
and then we're adding new factors,
um, that are preloaded.
Uh, okay, so we are going to- now go to what you're asking about,
which is conditional independence.
Um, so if you have three- if you have three variables; A, B,
and C. So let's say we have A, um,
C, and B, um,
then you would say that A and B are conditionally independent,
because once you condition on C,
once you pick a value for C, remember,
these turn into stubs or stumps or whatever you wanna call them, Gs.
Um, and so these are now independent.
They used to not be independent because there
is- there is a- they could access each other through C,
but now they're independent.
There is no edge connecting them,
and there's no way to reach each other.
Um, and so this- this is just a way of formalizing that,
um, this is how we write it.
So we condition on C,
and then A and B become independent. Um.
Yeah. So commonly you say every path from A to B
goes through C. And then that means if you remove C,
then there's no path connecting them.
Um, so in this example, um,
you have SA and Q, if you condition on them,
then you rip them out of the graph,
and then it looks like Western and Eastern
Australia are now independent of each other because they- they've turned into islands.
Um, and that's writing it mathematically.
So there's another notion of the Markov blanket,
which is basically saying, "Okay,
I want to make- I've chosen some subset of the graph,
in this case it's V, and I want to make it independent.
What is- what- what are the variables I have to
destroy in order to make my variable an island?"
Um, so in this case it would be SA and NSW.
If you condition on these,
then V becomes an island and it's independent.
If you wanted to make this subset independent,
then you would condition on Q and SA.
And the set of nodes, um,
that you have to condition on is called
the Markov blanket of the set of nodes that you want to be independent.
Um, so it's like if you have
some part of the graph A and you wanna make it independent of a part B,
then C is kind of like the Markov blanket of A if,
um, when you delete it,
A becomes independent of C. Um,
this thing- this is kinda like a set notation thing,
it's a set difference.
Um, and this is just a [NOISE] way of writing it more mathematically.
[NOISE] Um, so we can use these ideas to,
um, to like create independent structures now of our data.
So we had this example before where it was almost independent, but not quite.
Now what we can do, is we can condition on it.
So we condition this to be red,
and then we can find the maximum weight assignment of the rest of it,
which we showed was easy before, right?
With- with this example,
is just linear, like what's the best for you? What's the best for you?
So once we condition on this thing that's making it non-independent,
then it be- the problem becomes very easy.
So what we do to solve this,
um, is we just condition repeatedly.
We say, "Okay, I've picked my node,
now for red, green,
and blue, solve it."
Condition and solve, condition and solve, condition and solve.
Um, and this becomes very quick,
and then you can read off the maximum weight very easy.
[NOISE] You just say, "Oh, it's green because that was
the maximum weight I found once after conditioning." Yeah.
So that the weight there, so the weight that's-
some of the weights are holding together, just like one.
Um, so the product of the weight. So one of weight.
The question was when we talk about weights, what is that mean?
And, um, this is- its kinda loosey-goosey,
it's not very formally defined.
But in this example,
I would say it's the weight of the whole graph.
So you condition on this and then you find the best values for all of these variables,
and then you take the products of all of those unary factors,
and that gives you, um, the weight.
Is this just a example or is this the actually what
the numbers would be, like [inaudible]
This is just an example.
Okay.
Yeah, their numbers are arbitrary.
Okay.
Yeah, yeah.
So if you are going through R, G and B,
isn't that the same as doing like all possible combinations, anyway or computationally?
Uh, yeah, it is.
So- so it is, um,
so you [NOISE] cover every solution that backtracking would give you,
this would give you two.
Yeah.
Um, this is much faster,
because what you're doing is you're taking
a very complicated problem and you're breaking it into easily solvable pieces.
You- you're taking an exponential problem and you're breaking it
into a linear number of linearly solvable pieces.
And in practice it's much faster.
It's more just like imitational distinction rather than like space search distinction?
Uh, yes.
Yeah, sort of, yeah. Yeah.
So adding on to that is essentially that
instead of doing all possible assignments for all of the variables,
you're just choosing a subset of them to like, exhaustively search?
So the question was, instead of doing- instead of exhaustively searching all variables,
you're exhaustively searching just a couple of variables.
And, um, um, yeah,
that's an interesting way of- of thinking about it.
I think in both- in both ways,
if you think about it, you've- it's like,
even if you're conditioning for every single variable,
you're eventually going to consider all of the values it can take on.
But you'd- you're- you're reordering things in a way that's much smarter,
and that lets you take care of, like,
take advantage of this independent structure,
um, in order to do it better.
Yeah. [NOISE] Any more questions on this?
Do you think it's faster than-
Yeah, so- so it is faster than backtracking, and you can see it here.
So for example, if we did backtracking here,
um, it would be, what?
There's three colors and there's seven nodes.
So it- it would be this huge exponential blowup.
Whereas, if we condition here,
then it- it becomes much smaller.
Um, okay. So just to summarize independence is when we
have A and B and they can't- there's no way to get from A to B.
Conditioning is when we- we take,
um, a var- a val- we take a variable,
plug-in its value, rip it out of the graph,
um, and then like pre-load all the factors that touched it. Yeah.
I have a question on the last slide.
Okay. Let- I'll finish this first.
Okay.
Um, conditional independence is when you have, um,
two blocks in your graph,
and if you condition on one part of your graph,
then if you rip that out,
then these two become separated.
And then a Markov blanket is saying,
"Okay, I wanna make my variable an island.
What nodes do I have to destroy to make it an island?"
Yeah. What was your question?
Um, I guess, I'm like unsure why it's computationally cheaper
because if you try- if you condition on X_1 range of the colors,
you see I have to- like- like, you're basically,
you still have to come up with three sets of
unary factors for each of the flow variables [OVERLAPPING] and you
iterate over each other domains [OVERLAPPING] it also seems exponential?
So- so in this case- so doing backtracking.
So if we did backtracking- [LAUGHTER] so if we did backtracking that will be what?
7_3, right? Is that true?
Yes.
3_7? Yeah. [LAUGHTER] And then if we did, um, conditioning,
[NOISE] so that'll be 3 for the first condition, right?
And then, every time we condition,
we do, um, uh,
7 times 3, right?
6 times 3.
All right. 6 times 3. Yeah, you're right. 6 times 3.
So this is, uh, 6 squared, right?
So what is this like?
What?
Is it still 6 times 3 or 3 times 3?
6 times, [OVERLAPPING] um, it depends.
I don't- I don't- I don't know exactly what these factors are saying.
Um, this is kind of an arbitrary example.
But I think- I think the point- the point here is that,
um, this is smaller, right?
So this is- this is 3_4 or whatever.
Um, so it's faster. Yeah. Yeah.
Can you explain how you're getting
that value of that computation referring to that condition.
Down here?
Yeah.
Yeah. So this was saying- this is saying that there's three colors for X_1.
And then every time I choose a color for X_1,
I'm saying that there's, um,
three options for each of the six other things.
So once I- once I've,
once I've set X_1,
it turns into this situation.
Where now I'm saying, what's the best value for X-1?
Okay. So I look at three things and choose the best.
Now, what's the best value for X_2,
and I look at three things and choose the best.
And so there's- now there's six of these,
and each one, for each item,
I have to consider three different possibilities.
Yeah. I don't, ah, you know,
it would probably be different depending on what these-
what the actual factors are and what they're calculating,
but, um, it's just an arbitrary example.
[NOISE] Um, okay. So now,
we're gonna do elimination.
Um, elimination is very cool, I think.
So conditioning is saying,
I'm gonna rip- I'm gonna rip my variable out of the graph.
And then I'm going to plug in
whatever value I conditioned on into all the neighboring vectors.
Well, elimination says is,
is- okay, I'm gonna rip my variable on the graph.
But instead of- instead of plugging in a single value across every single factor,
I'm going to plug in a different value per
factor in order to individually optimize each decision.
Um, so I think,
I think it's best shown through example.
So again, um, this is the thing,
we have two variables and they're connected by a factor and they have these weights.
Um, if we condition, then, um,
we- I think this is- this example, right?
So if we condition, we get what we got before.
And then if we eliminate,
um, here, I'll do the elimination now.
So [NOISE] I'm going to redraw this table.
So we have X_1 and X_2.
We have red, red, red,
blue, blue, red, and blue, blue.
Um, and they have weights of 1,
7, 3, and 2.
Okay. So what elimination is gonna
do is- is we- we condition,
um, it like internally optimizes the value that we
want to condition on based on the other arguments in that factor.
So we would say,
okay, we're trying- we're trying to eliminate X_2 right now.
And we do that by looking at each value X_1 could take on.
And then for each value,
we dynamically choose the best value for X_2.
So first, we look at red.
So X_1 is red.
What's the best value for X_2 in this case? It would be blue.
So we cross out this row.
We say, if X_1 is blue,
then X_2 is gonna be red because that gives us the biggest.
And then, um, just like before, now, for,
for any value of X_1,
the value of X_2 is already set,
it's fixed, it's decided.
And so that means that we can drop this variable out of the graph,
because we've already like, internally optimized
into this function what its value would be.
And so this gives us, again,
a new table where we have X_1,
we have the values it could take on, red and blue.
And then we have the weights associated with that,
which is now 7 and 3.
Um, so in math,
what that looks like is,
is where we used to have this binary factor,
we've now ripped X_2 out of the graph, and we,
we have this new factor that's a unary factor now,
which internally optimizes over X_2.
It says, give me my X_1,
give me an X_1, and then as soon as you get my X_1,
I'm going to spin through all my values of X_2
and give you the best one that would be the best match for it.
So um, what this would look like in pictorially,
I guess, is every time you remove of,
of- a factor, um,
you take all the factors that touch it and you rope them all together,
and you merge them,
and- into one big vector.
And then internally, what that factor is doing is it's optimizing over,
um, whatever variable you just removed.
So for example, um,
if we have- um, so for example,
we have this kind of coloring problem, um,
let's say, we want X_1 to be red and X_4 to be red.
What's the best value of X_3 that we could give?
So over here, we say X_1 is red,
X_3 is red, that gives us a value of 4.
And then 3-4, red,
red gives us value 1.
So we have a 1 in here.
The other value that X_3 can take on would be blue.
So now, we go from red to blue,
which gives us a value of one,
and then blue to red, which gives us a,
um, weight of two.
And so we multiply those together and we 1 times 2 which is 2.
And then internally, what this factor is gonna do is it's gonna maximize over those
and choose the value for the deleted variable that maximizes,
that internally optimizes it for this local problem, which in this case is red.
So just another example.
Um, if we did red blue.
So X_1 is red, X_4 is blue.
And there's two options for X_3, first, it would be red.
So if we go from red to red, that's 4.
And then from red to blue is 2,
so we got 4 times 2 here.
And then, um, the next thing would be,
if X_3 is blue.
So we go from,
um, red to blue,
which is 1, and then blue to blue,
which is 1, and that's 1 times 1.
And then we would say, oh, well, in this case,
the best value of X_3 would be red.
So it's kinda like internally optimizing.
Um, in general, what this looks like is you rip the node out of the graph,
you take all the factors that used to touch
it and you tie them all together into one big factor.
[NOISE] Um, and again,
that's kind of the mathematical notation.
Um, there is another way of interpreting this, um,
that might be helpful to some people, um, uh, basically,
it's like you- it's like I pick
my value and then I'm going to look at
all of the- all of the variables in its Markov blanket,
and I'm going to repeatedly condition for
every- on every assignment that blanket can take on.
And then, for each of those conditionings,
what's the best value of my selected variable?
Um, that's kind of what's going on behind the scenes.
Yeah?
So if ripping out the variables and constructing new factors leads to like, faster execution times, why don't we
just rip out all variables except for one,
and then it becomes a unary factor and a unary random variable.
Yeah. So the question was, if,
if it's faster to rip out variables,
why don't we just do it for the whole graph,
and, um, we totally will.
Yeah, you'll see- it's in another couple slides, that's the algorithm.
So like we have this strategy for ripping out variables,
and then the next step is to use it to solve the CSP, which is what we're about to do.
Yeah. Um, okay.
So there's this question,
which I think is cool.
Um, so if we have some kind of a star-shaped graph,
so if we have a setup,
[NOISE] let's say we have a setup, um,
[NOISE] like this, with- which is a bunch of factors going into our hub.
Um, and we have- we have this variable,
we'll call it S. Um,
do we wanna run elimination or a conditioning on S?
I hear some whispers on that, oh, yeah.
Conditioning.
Yeah, conditioning. Um, and the reason for that is that if we- if we condition,
then all these turn into unary factors.
And if we eliminate,
then it turns into one giant factor,
um, which, which is harder to solve.
[NOISE] Um, okay. So like you were saying,
this is- this is the algorithm that you got out of it.
So basically, what you do is you,
you loop through all your variables and you eliminate them all in turn.
And then at the end of the day,
you're going to have,
um, one variable to rule them all,
and that variable will just hold the best answer. Yeah.
[inaudible] to the, the degree [inaudible]
can we end up in to a case where if we don't have,
uh, like a smart way to choosing [inaudible] conditioning on.
Can we end up in a case where we've conditioned over this one,
and that one and it turn out that there is no more of possibility for-
Yeah.
[inaudible] the graph or whatever?
Yeah. So the question was, um,
wait a second, doesn't ordering still matter,
like can't we still end up somewhere that's not good? Um, and that's totally true.
So towards the end of this lecture is a discussion on exactly that topic,
um, variable ordering does matter, and it's actually hard,
it's an NP-Complete problem is to decide the best area ordering.
Um, but so I think, um,
I'm gonna do one more example of- I'm going to run elimination on a whole graph,
um, which I think would be helpful.
[NOISE] So our graph,
so we have three variables.
We have X, we have Y,
and we have Z, and they're all connected through a single factor.
Um, I'm gonna write out the whole table for this.
So we have X, we have Y, and we have Z.
And let's say, um,
they have two values,
they could be A or B.
So we have A, A, A, um,
A, A, B, I'm gonna run out of space.
Um, A B, B.
Uh, A, B, A.
Uh, B, A,
A. Um, B, A, B.
Uh, B, B, A,
and B, B, B.
Is that everything? 1, 2,
3, 4, 5, 6, 7, 8, yeah.
Okay. So, um, hopefully,
this is readable, um,
but and they have weights, right?
So we can just arbitrarily say they're,
1, 2, 3, 4,
5, 6, 7, 8.
[NOISE]
Can people make this out?
Seems like. Maybe? So I will go with it.
Okay, so what do we do?
Um, let's say we wanna choose Z first, we wanna eliminate Z.
So what we do in elimination is we say,
for all the variables in Z's Markov blanket,
we're going to repeatedly condition on the values that they can take
on and then we're going to dynamically choose the value of Z that would best match that.
So, um, X and Y could either be A and A.
So this is A, A.
And in this case,
the best value of Z would be B,
right? So we cross this out.
Uh, next, X and Y could be A,
B, in which case the best value of Z is A. So we cross this out.
Um, it could be B, A, um,
which would be B, or it could be B, B.
Um, and the best value of Z in this case would again be B.
So again, like we've seen a couple of times before,
now for any value of X and Y,
the value of Z is already decided,
it's set, it's precomputed.
So we can just drop this variable from our table,
which is equivalent to dropping it from our graph.
Okay. So now, we're gonna work on Y.
Again, we do the same thing.
We say for each- for each value in Y's Markov blanket, which is X,
we repeatedly condition on all the possible values it can take on,
um, and then dynamically choose the best va- value based on that.
So if X is A- um, so now,
we're comparing- so now,
we're comparing these two.
So if X is A, what's the best value of Y?
It's B. So we can cross this out.
Now, if X is B,
what's the best value of Y?
Um, it's again B. So we can cross this out.
And now again, for every value of X,
we've decided B and so we can drop this from the graph.
And now what we have is,
now we have one variable in our graph with a unary factor.
Um, so we have X,
and X can be A or it can be B.
And if X is A, then it has a value of 4.
And if X is B,
then it has a value of 8.
And now what we can do is we can just say,
okay, we're gonna choose X to be 8.
And then, um, in kind of implementation,
all you'd need to do is- is kinda like look back,
you can store some kind of backpointers or store your own tables,
and you can recover your solution from the whole graph,
um, from this end point.
[NOISE] So that is-
that is variable elimination. Um, yeah.
For the second point,
why did we go in the opposite order from n to one instead of one to n?
Oh, that's- so that- that's basically- that was what I- that's was,
um- that kind of is a mathematical way of saying what I just said about the backpointers.
So it's like, now we've gone forward and we've eliminated,
and now we can go backward and say,
okay, X is B.
Now that we've decided X to be B,
like we look at our old table and be like,
what was the best value of Y that gave us that decision?
So it's kinda like, once you go forward to eliminate,
you get your solution and then you go back and like read off the values as you go.
Yeah.
Um. Okay. [NOISE] So in terms of runtime,
um, basically what, what you can say is,
um, so for each variable, um,
it's going to have domain to the- so every variable is gonna
have domain to the arity the-
the factor touching- so the factors touching,
um- like the factor you create from eliminating your variable,
is gonna have this many rows in it,
um, and then there's n variables that you have to eliminate.
Um, and with the- why- the reason why it's max
arity is because if you have- let's say you had some kind of graph, um,
that looks like this, um,
thif- this factor would be D squared,
the domain squared, and then this factor would be D_3.
And so like as you're going- as you're eliminating going through this graph,
like this is just gonna dominate D_2.
Um, so it's, as you're going through and eliminating in the graph,
the arity of the biggest factor you've created is gonna bound your performance.
Um, and that's where variable ordering gets in.
So what- was it- yeah.
So what is plus one?
Yeah. So the plus 1, um,
I'm not actually sure, I- I thought about, I looked,
I looked around and I- I wasn't able to explain it, so, um, yeah.
Um. Okay. So but as you're going,
order matters, um, because if you looked at this,
there's two ways to do it, right?
So if you went from the leaves and eliminated each leaf variable first,
um, then all the factors you create would have arity 1. It'd be super easy.
But if you eliminated X_1 first,
then you get this huge giant factor that it gives you
this big table of this arity 6, which would be very slow.
So the order really matters.
Um, in general, it makes sense to eliminate variables with the fewest neighbors first.
It's a pretty like sensible heuristic that in practice works well.
Um, and we define this term called treewidth,
um, which is saying,
if you had a factor graph and you're going through doing elimination, um,
the maximum arity of a factor that you create along the way,
using the best ordering is the treewidth of that graph.
Um, so in general, a treewidth is, is very,
very hard to compute, um,
but for a few cases,
you can kind of reason about it.
So for example, if you had a chain- um,
let's say we had a chain.
And if I were going through eliminating this,
the biggest factor I would create is just a unary,
just that little guy, right? So it's a treewidth of 1.
Um, if I had a tree,
so it's just like our example before.
So if you go from the leaves again,
you can create these little factors- um,
these little unary factors and so you can get away with 1.
Um, if you had a cycle,
I'll make my cycle a little bigger.
But if you had a cycle,
the maximum arity is gonna be 2, um,
because even if- even if you kinda work away from the sides,
you're eventually gonna end
up as something that just connects the last things before you delete them.
Um, so it's like you have your cycle and you chew away at
both ends and you're gonna end up with
something- with just those two nodes relating- remaining.
Um, and then if you had a, uh, n by n grid,
um- if you had an n by n grid,
um, so let's say this just goes on,
it's the smaller of the two.
And the reason for that is because if you,
let's say, you go through and you're eliminating these things,
do like that and like that,
where you- you gonna end up with something,
um, some situation like this.
And once you delete this,
you're gonna get a factor that ropes them all together.
Um, yeah, so that's treewidth,
super important, hard to compute. Yeah.
You end up with two factors,
uh, for [NOISE] in the last two in open cycle.
Yes.
So when you have a feedback is that a cycle?
Yeah.
And you're going through elimination?
Yeah.
You end up [OVERLAPPING]
Oh, yeah. You're right.
So it's not the first two,
it's the- it's not the last two, it's the first two.
So when you are- let's say this is my cycle,
the first factor I create after I delete that is gonna be a binary factor.
Yeah. So thank you for that.
Um, yeah.
So just to summarize,
we learn different ways of solving CSP today.
Beam search, which is kind of like a souped up version of backtracking,
um, which doesn't always give you the answer, but it's faster.
Local search, you get entire things and then you improve them a little bit at a time.
Conditioning, um, break up the graph into smaller pieces that are easier to solve.
And then elimination, which is where it's kind of like in, you know, conditioning,
except instead of choosing one value for all your factors, um,
you kinda like dynamically choose the value based on the Markov blanket.
And that's CSPs. So we will see you on Wednesday.
 All right. Let's get started.
Long time no see. I'm excited to be back and tell you guys about Bayesian networks.
Um, so before we dive in,
I wanted to do a few announcements first.
Um, there's four things that should be on your radar.
So the scheduling homework is due tomorrow.
Hopefully, you guys are well aware of that.
Um, the car assignment is, uh,
released today, and it'll be due next Tuesday.
So there's some, um,
conceptual challenges here, especially if you're not to- up to speed on your probability.
Uh, the section, uh, Thursday will,
uh, really help you go over that. So please come to that.
Um, then there is a final project,
ah, you guys have- hopefully,
have all received your feedback, uh,
for your proposal and are actively making changes.
So just to make sure that you guys are making progress,
there's a progress report that is due, um, next Tuesday.
And for this one, the,
the guidelines are all on the,
the website but just to kind of reemphasize, um,
especially if you didn't, uh,
manage to get a baseline or Oracle,
we- we really expect that you to have that now.
And also, we expect you to have some sort of preliminary results with, you know,
some sort of implementation of your actual,
um, [NOISE] procedure or algorithm or model.
And definitely some description of what that is,
and be as concrete as possible, um, as you can.
Um, and finally, the exam is in about, uh, two weeks.
Um, I would start, uh, looking at that.
Um, and the- actually the best way I think to prepare for
the exam is to look at the old exam problems because there is a certain style,
um, that you have to kind of get used to when taking the exam.
So I know this is a busy time,
there's a lot of things going on.
But hopefully, um, you guys will manage. Yeah?
Progress is due Tuesday or Thursday?
It's Tuesday, I believe. But I could be wrong.
[inaudible] Uh, Tuesday but [inaudible]
Yeah. Let's say it's Tuesday.
Whatever the website says.
It's whatever the website says.
Oh, that says Thursday?
Okay. Well, then we'll defer to the website on that.
Okay. Um, okay so the next,
uh, agenda item is the Pac-Man competition.
So many of you, uh,
worked hard to submit,
um, various entries into this competition.
In the end, only three could,
uh, make it to, uh, the top three.
So, um, here are the winners of the Pac-Man
competition- because out of town but if- in the audience,
uh, maybe you guys could come down.
Let's give them a round of applause.
[APPLAUSE] And we have these, uh, um,
prizes [NOISE] which are Pac-Man themed, uh,
Cups [LAUGHTER], um, filled with candy in case you didn't get enough for Halloween.
So there you go.
[NOISE] Congratulations.
Thank you.
Do you guys want to say a little bit about what was your secret sauce?
Sure, called Pacman and um, actually, the fourth one is the stupidest of all.
And the third one is like, um, super messy.
[NOISE] I had like all of these [inaudible] extracted from like the food, the capsule,
the hunting ghost, and the scaredy ghost [inaudible] and all that stuff.
But it actually turned out to be not as useful as a very simple [NOISE] method
that- which is similar to how everybody plays Pac-Man
if there's a hunting- uh if there's a scared ghost, then go chase it.
[NOISE] If there isn't then go for the capsule [NOISE] or else
look for the food and dodge the hunt- uh, the hunting ghost.
And, um, also, um,
I changed the- I think the distance to, um,
[inaudible] algorithm because the Manhattan distance is different from,
um, the [inaudible] [NOISE] algorithm.
So lesson is, keep it simple.
Okay. [inaudible].
I recently experienced.
There was a lot of variations that I went through.
And I know mine was best due to ending up being
a very simple model [inaudible] the policy, you want to keep the ghost
Every once in awhile, the scared ghost is in play.
And after, uh, eating all the capsules get included
as good as possible for you to get [inaudible] for tracking
down at the speed of DFS search, which I use dynamic programming so that
everybody [inaudible] transitions and always catch that and have that robot-
Great.
- [inaudible]. Yeah.
Okay. Well, great. Well, congrats again.
[inaudible] [LAUGHTER] [APPLAUSE] Okay. All right.
So keep it simple I guess is,
uh, is a good, uh, lesson.
Okay So back to our regular programming.
Um, last week, we started talking about factor graphs.
Just a quick review of what factor graphs are.
Factor graphs consist a set of variables.
These variables could denote colors of provinces of Australia or locations of,
uh, objects at different time steps.
Factor graphs also include a set of factors which depend on certain sets of variables,
and these factors are meant to specify, uh,
preferences or constraints on what values are good for these variables to take on.
And the weight of an
assignment is the- simply the product of all of the factors, [NOISE] right?
So there's this theme that comes up in this class which is- I call it,
uh, specify locally and optimize globally, right?
So it's very easy to think about how two variables might interact and
how you want something local, um, to happen.
And these are defined in terms of the factors.
But what you care about is some globally optimal solution.
So the weight is a global function of all the var- assignment to all the variables.
And last time, we talked about various different types of algorithms
for finding the maximum weight assignment including backtracking search,
beam search, Gibbs sampling,
and so forth and so forth.
Okay. So, um, one example we looked at was object tracking.
And in this example,
we have a set of variables corresponding to the location of,
an unobserved object at time step i.
Um, and we looked at two types of factors that captured where this object might be.
There's transition factors which capture
the intuition that across two successive time steps,
the object shouldn't move if you can't teleport,
that has to remain close.
And tran- observation factors then incorporate the information from the sensors.
At each position, there's going to be some factor that kind of, uh,
encourages the position to be similar to what the sensor reading was.
So sensor readings are noisy so it's not
a hard constraint but it's a- but a soft constraint.
Um, and last time,
we saw, uh, this, uh,
demo where you can define the factor graph and you clicked Run.
And you see all the factors which are represented in these, uh, tables.
And when you multiply everything together, you get, um,
for every joint assignment to all the variables
some number that corresponds to how good that w- uh assignment was.
And if you look at the maximum weight assignment,
that's what the answer you would, uh, return is.
Okay? So so far so good,
and you can- with this framework,
you can do a lot with it already.
You can define a bunch of factors,
you can run all the algorithms that we looked at last week.
But, you know, what is- what do these factors mean and how do you come up with them?
Intuitively, you can define these factors just,
you know, hack on a 2 if you like it, 1 if you don't like it.
But, you know, philosophically,
maybe you should be a little bit bothered by this because,
um, these factors are kind of just arbitrary in some sense.
So the goal of this,
uh, lecture and, uh, um,
next two will be to, um,
give more meaning to the factors,
and we're gonna talk about Bayesian networks.
There's a way to do that. So in one sentence,
Bayesian networks are factor graphs plus probability.
Um, just take it, taking a step back.
Where have we been in this course?
This course has been a lot about designing new modeling frameworks.
So we, uh, looked at state-based models which
result in search problems and MDPs in games.
And this was, uh,
useful tools for solving a lot of, uh, problems already.
Um, but then we looked at,
uh, starting last week,
cases where maybe the order of actions doesn't matter so much,
and it's more natural to think about a set of variables that
you want to find some assignment in any order,
uh, uh, is, you know, permitted.
Um, and you can think about that as going maybe stepping up in abstraction,
kind of going from assembly to maybe C++.
And in this lecture,
we're gonna talk about Bayesian networks.
You can think about loosely analog, analogizing going from C++ to Python.
It gives you, uh, a kind of a more high level language to think about modeling,
um, it's just another tool in your, you know, toolkit.
Okay. So let's start with, uh, the basics.
So just a quick review of probability.
Usually, we see probability sort of with outcome spaces.
I'm gonna jump directly to random variables,
assuming that you have, uh, basic,
um, um, CS109 knowledge.
So random variables are things- in this example,
are sunshine and rain.
So they're variables whose values are unknown.
And furthermore, there is a probability distribution over
all the random variables that captures how they might interact.
And, um, so this is called a joint distribution.
Um, so we write P,
uh, this blackboard P of,
uh, the two random variables,
S and R. And this is
this entire table which specify for every possible assignment to all the variables,
a single number which is its probability.
So the probability that it's sunny and it's not rainy is 0.7, for example.
Now, so I want to distinguish,
uh, um, two things.
One is that we're gonna use uppercase letters to denote random variables,
and lower case letters to denote the values that the random variables can take.
In addition, I wanna point out that when I write P,
S equals S R equals R,
that quantity expression represents
a single number which is a probability, for example, 0.7.
Whereas, if I write P of S and R,
that expression denotes a whole distribution which is the table.
And I know these are kind of minor, uh,
notation differences but I think it will,
uh, avoid a lot of confusion if you kind of pay attention to this.
So from the do- joint distribution,
you can use the laws of probability to derive, uh, several quantities.
One quantity is called the marginal distribution.
And in marginal distribution,
you pick a subset of the variables that you care about;
those are called the query variables,
and you induce a distribution over them.
So in this case,
I've picked S. And what I'm saying is I only care about the probability of S. Um,
I don't care about R. But R still has kind of influence on S.
So I need to take R in to account somehow.
And the way I do this is I look at all possible values that S can take on,
so look at 0.
And then I look over to the joint distribution [NOISE] and
look at all the rows that match that particular,
uh, S. So here,
I'm looking at S equals 0.
So that's the first two rows.
And then I look at those probabilities and I summed them up.
So 0.2 plus 0.08 is 0.28.
And similarly, for S equals 1,
I look at all the rows that matched S equals 1 which is the last two rows,
and that gives me 0.72.
Okay. So what I'm doing here is called
marginally- marginalizing out R. Because I don't care about R,
I'm interested in the marginal distribution over S.
So another concept which is gonna be really important is,
uh, the conditional distribution.
And the conditional distribution arises when
your interests- when you have, um, some evidence.
So assume- let's say I observe that it's raining.
So R equals 1.
So I write P of S given R equals 1,
to say this is the- I'm interested in distribution over S,
given that it's, uh, raining.
And to compute this, um,
I look at this condition R equals 1,
and I simply select all the rows which match that.
So the second and the fourth rows.
So now these are numbers, now probabilities.
They don't sum to 1, right?
Because it's only a subset of the rows.
But what I'm gonna do is make them sum to 1 by normalizing.
So normalizing means taking, uh,
the relevant numbers 0.08,
0.02, adding them up,
and dividing by that number.
Okay? So I'm dividing by 0.1,
which gives me the normalized distribution 0.8 and 0.2.
Okay? So these two concepts are gonna be really important,
and if you remember from last week, uh,
there- we talked about marginalization as conditioning,
later in this lecture I'll connect these, uh, two concepts.
Okay. Any questions about,
uh, basic probability so far?
Hopefully this is all uh, review.
Okay. Let's move on.
So suppose I have a joint distribution over some set of variables.
So then in this example, it's, um, sunny,
it's raining, whether there's traffic,
and whether it's the autumn season.
Um, the way to think about this is a p- as a probabilistic, uh, database.
Um, for every possible assignment,
I have a number that is either,
uh, is- is between 0 and 1.
Um, so I can think of it as an oracle.
This is like a source of, you know, truth.
I don't know what any of these variables is,
but I know how they behave and how they operate,
just like I know- I don't know what the outcome of a coin flip is gonna be,
but I know that it's half and half, heads and tails.
So the main thing that we're gonna do with
a joint distribution is called perform probabilistic inference.
Okay? So this is an important thing to,
you know, understand, um,
because we're gonna spend the whole time during probabilistic inference,
so it's good to know what it is.
So, um, probabilistic inference,
the way to think about it is that,
um, you observe some evidence.
You wake up and you see,
uh, okay, it's- it's autumn,
and, um, and it's a bay area so there's traffic outside.
So, uh, you're conditioning on some evidence,
T equals 1 and A equals 1.
Okay. That's what you know.
And what you like to find out, um,
querying this oracle is,
you know, whether it's raining.
So you're interested in some set of query variables.
Okay. So the general form of a probabilistic inference, um,
a problem or task is probability of some set of query variables conditioned on some set of,
you know, conditioning variables which are set to particular values.
And notice that there are some variables which are not mentioned in this query,
such as S, and those variables are the ones that are marginalized out.
So you can think about this query as combining
both the marginalization and the conditioning from the previous slide.
Okay? So this, without loss of generality,
just captures everything that we seek to do with,
uh, distribution for the purposes of this class.
Okay. So at this point,
you can actually just do probabilistic inference, right?
If I give you a joint distribution,
um, which is this huge table with all the,
uh, probabilities for all the assignments,
you can go and um,
you can compute anything you want.
So now, there's a, kind of,
a slight problem here which is that,
if you have N variables,
and just suppose each variable takes on two values.
How many possible- how many rows in a table are there?
Anyone? 2 to the N. Right?
So that's exponential, that's a lot.
So if N is 100, then that's, I don't know, a lot.
Um, so- so clearly,
we can't do this naively, right?
So the first challenge is,
how do you even write down this joint distribution compactly, right?
I don't want to write down 2 to the N numbers.
So Bayesian networks is going to allow us to
define joint distribution using the language of factor graphs.
So this is really cool because now I have a very compact way of specifying what is,
um, implicitly, something that's very, very large.
The second challenge is algorithmic.
How do you do inference, right?
We wanna do- perform a probabilistic inference answering queries like this.
How do we do this efficiently?
Again, you don't want to have to, uh,
go through to 2 to the N possibilities,
because that would be really really slow.
Um, and we'll see that variable elimination,
Gibbs sampling, particle filtering,
which is the probabilistic analog of beam search,
all these algorithms that we, uh,
talked about last week are actually going to come into play.
And we're just gonna talk about the probabilistic analog of these,
as opposed to finding the maximum weight assignment.
Okay. All right.
So now let's try to motivate why we need,
uh, Bayesian networks with this following example.
So, um, here's a setting.
So earthqua- earthquakes and burglaries are things in the world, they are bad things.
Um, but suppose that they're independent,
right? That kinda makes sense.
Um, but in your house you've ins- installed an alarm system,
which is going to detect either,
uh, both earthquakes and alarms.
Okay. So one day you wake up,
and you hear an alarm go off.
Okay. So you should be alarmed.
Um, but, uh, but then you turn on the radio and you hear that,
uh, there's actually an earthquake.
Um, so how does that affect your beliefs about whether there was a burglary or not?
Okay. So okay.
There's three options, does it increase the probability of a burglary?
Does it decrease the probability of burglary or it does not change anything at all?
Okay. So how many of you think that hearing, uh,
the news about the earthquake on the radio increases the probability of a burglary?
So a few say it increases,
how many of you say it decreases?
So many of you say it decreases.
How many are saying it doesn't change?
Okay. Almost as many say it doesn't change.
Okay. That's interesting.
So we'll answer this question,
but you know, keep on thinking about that in your back of your head.
And one thing I'll say is that, you know,
I shouldn't- you shouldn't expect to necessarily find
the right answer here just by kind of intuiting things.
And one of the points of making things codified in
a Bayesian network is that you don't leave anything up to a, kind of, vagueness.
It's- you- it's- there's actually a correct answer that we can derive.
Okay. So, um, let me talk about how to go about,
uh, modeling this as a Bayesian network.
So with this core example,
so there's four steps.
Um, the first step is defining what the variables are.
Okay. Variables. Um, so
what are the variables here? Yeah.
Burglary.
Okay. So there is a burglary-
Earthquake.
Earthquake.
And alarm.
And alarm. Okay, great.
So these are the three things that we don't know about that are mentioned.
Okay. So the second step is,
um, you draw some edges.
Okay. So these are gonna be directed edges,
that correspond to notions of influence.
Um, and if you- if you want cause- causality.
But causality is a very, uh,
more philosophical thing which we don't really need for this class.
Um, so, but I'll- but I'll use it anyway.
So what causes what?
So this alarm cause burg- burglaries, no.
Okay. I think it's the other way around, right?
So burglaries cause alarm,
and similarly earthquakes cause alarm.
Um, and these two aren't,
uh, or I said they're independent,
so let's just leave that out.
Okay. Okay. So now I have a direct a- acyclic graph
that shows how all the variables are related in somehow.
Okay. So the third step is to define local conditional distributions.
So now I'm going to go one step further and say, um,
how these, uh, what the probabilities of these, uh, variables are.
Because in the end, and remember I wanted to
define a joint distribution of all the variables.
Okay. So um, I'm going to define
a local conditional distribution for each of these variables.
So here I have P of B,
P of E, and um, P of, uh,
A given B and E. So in general,
a local conditional distribution is P of whatever that variable is, given its parents.
So the parents are the variables that directly point into it.
So the parents of A are B and E. E has no parents,
and A, uh, B has no parents.
Okay. So in particular,
what I'm going to do is now- let me flesh this out a little bit more.
So what is P of B?
P of B is a table that specifies only what's going on in this region of the space.
So I have B, and have P of B,
and I just fill out this.
What are the possible values of B?
0, 1.
0, 1. So let's say,
uh, 0- 1 and 0.
So let's say that probably of burglary is Epsilon.
Um, Epsilon generally denotes a small number,
which you hope to be the case, um, here.
Um, so this must be 1 minus Epsilon,
because it has to sum to 1.
Um, and for simplicity,
let's say that probability of earthquake is also Epsilon and 1 minus Epsilon,
just for simplicity.
And then, okay.
So this one's a little bit more complicated.
So I'm gonna write the parents B,
E, and the variable itself, A.
And I'm going to look at probability of A given B and E.
And now I'm gonna list out all the eight possible, uh, combinations here.
So it's 0 0 0, 0 0 1,
uh, 0 1 0,
0 1 1, 1 0 0,
1 0 1, 1 1 0, 1 1 1.
Okay? Okay. So for each of these I need to specify the probability.
So 0 0 0.
Um, and I should say that this alarm system you bought was,
uh, is really good- really good.
So it's, um, it detects earthquakes and burglaries, uh, perfectly.
Okay. So if there is no burglary and no earthquake,
then the probability of the alarm not going off should be 1. Right? It's perfect.
And this is, uh,
the failure case which is 0,
because, um, if there is a burglary- no burglary and no earthquake,
the alarm shouldn't be going off.
And, um, this is- I'm not gonna bother you with the details,
you can, uh, you can just fill in the rest of this.
So there is a burglary and earthquake that should be,
uh, maybe someone should check them during this, right?
Um, this should be a 1,
this should be a 0, and this should be a 1.
Something like that?
Okay? So now I've defined
the local conditional distributions so remember I'm
not defining the joint distribution yet.
I'm just defining in from zooming in on a particular variable.
How does it relate given its parents, right?
And you can think about it like, you have a million nodes.
I'm only, each local distribution might be only touching like a very small part.
Okay, so finally the fourth step is to define the joint distribution.
Okay, this is the thing we're all after, right?
Which is, what is the joint distribution over
all three variables here
and the joint distribution is going to be written with a black pen.
P is, um, B equals,
uh, b, um, E equals e,
A equals a so random variables equals a particular possible value,
and this is defined to be the product of all the,
uh, local conditional distributions.
Okay? So P of b,
p of e and p of a given b and e.
[NOISE] Okay?
So let me reveal the slide which hopefully should have the same content on this.
Um, one thing I'll,
I'll point out is that, um,
there is a difference between the small p's and the big P's.
So the small p's are local conditional distributions.
Um, these are things that you just define, right?
There's no right or wrong there.
You just define them.
They're just true.
Um, and then there's this big P which is, um,
the joint distribution which is
again defined to be just the product and then from this joint distribution,
you're going to read out things like marginals and conditionals, um,
which might look like some of these local distributions but they're, uh,
right now I think about them as distinct objects. Yeah, question?
Can we find [inaudible]
So the question is are we assuming b and e are, um, independent here?
Um, so let's see how do I answer that?
So yes in this one b and e are, um, independent.
Um, and, uh, I'll show you a little bit further how we can kind of see them more clearly.
Yeah. Okay so these are Bayesian networks.
So what's the connection between this and factor graphs?
Well, if you, um,
squint a little bit,
you see that the right hand side here is a product of
things and the left-hand side is this kind of joint,
uh, global thing and so what does this look like?
Looks like weight equals product of vectors, right?
So let's go with that analogy and it's actually much deeper than just an analogy,
um, and let's draw this as
an equivalent factor graph.
Okay? So for every Bayesian network,
we can actually draw it as a factor graph.
So here we have b, um,
e and a and- okay so now it's, um,
you know, it's really important to note that how did the factors, uh, arise.
Through there's a local conditional distribution
remember for every variable and that is a factor.
So for every variable,
there is a factor right.
It's tempting to look at these edges and draw factors on them but that's, that's wrong.
Okay? Remember, one factor per variable.
Okay? So this variable has a factor.
That is P of B.
This variable has a factor,
that's P of E and this variable has a factor and,
uh, this-what does this depend on,
what is its, uh, scope?
B, and E, and A, right?
Okay. Now again, common mistake is
to just put two factors here because it's really tempting.
But one way to think about it is that if you think about your,
your parents they- they're married and connected.
So that's why these are your parents are connected.
Actually the- um, I'm not making this up but there's, um,
some people call, uh, this process,
um moralization.
Um, yeah.
Can you guys use this system to compute the probability of
alarm given just an earthquake or a probability of alarm given just a burglary.
Yeah so the question is,
can you use this to compute probability of alarm given earthquake alone or burglary alone?
And the answer is you compute whatever you want and we'll- I'll show you how to do that.
Okay. So single factor connects all the parents,
one factor per variable, okay? Got it?
All right, so, um,
the joint distribution over all the variables,
remember is the product of
all the local conditional distributions and just for reference,
this is what it is. Um,
and now you can go and answer questions about this.
So this is kind of the fun part and I'm not going to go through
the details of how this is done but I'm just gonna show you kind of the interface,
um, what you would expect.
So again, this is, um,
the definition of an alarm network, um,
using the same machinery as a factor graph because it is a factor graph,
um, and first we're gonna ask what is the probability of B?
So what is that? That says in the absence of any information,
is there a burglary or not?
Okay? So what do you think that should be? And epsilon here is 0.05.
So I think I heard it 0.05.
Someone said that, okay?
So in D, the probability of a burglary is 0.05.
Um, should be intuitive, um,
and now suppose I- uh,
the alarm went off.
Okay? So now what's the probability of burglary?
So what is P of B given A equals 1?
Does it go up or down? Should go up if your alarm is working.
Um, and indeed we see that probability of burglary given alarm equals 1 is 0.51.
Okay? And now the moment of truth,
what happens if we condition on the fact that there's also an earthquake?
So let's do this and you get 0.05.
So many of you are correct, um
when you said that the probability of,
uh, earthquake goes down.
And intuitively, you can think of, uh,
it makes sense from this phenomenon called explaining away.
So explaining away happens when you have structures that look like this,
and you have- suppose you have two causes, positive influencing effect?
So by positive influence I mean that if you flip B equals from 0 to 1,
then the probability of A goes up.
And, um- so explaining away says I conditioned on the fact,
conditioning on one cause reduces the probability of the other one.
Okay? So at some level this makes sense because, you know,
this A is either B or, uh, uh,
is either driven by B or E and I don't
know which one it is if I just heard an alarm go off.
But each of these is very small proba- has very small probability.
So the moment I can kind of, uh,
see that one of them explained this cause, see that one of them is true then I,
I can revert back to the my prior belief on the, you know, other one.
Okay? So humans do this all the time when you're reasoning.
When you're thinking about like what,
what the cause is and you find one,
one cause and you discount all the other ones.
So um, now the thing that's kind of interesting here is that I did
say that B and E are independent which is also true.
Right? So this might have led people to
think like well, it shouldn't change because they are independent.
So why should the probability change?
But the key thing is that when you condition on A,
you actually changed, uh,
the independent structure of the model.
So this is why writing things down really precisely
is helpful to kind of reconcile these seemingly,
um, contradictory intuitions that you might get. [NOISE]
Okay, any questions about this?
[NOISE]
All right, let's move on.
So we've talked about the alarm network.
This is your first example of a small Bayesian network.
Hopefully, you have an idea of
the intuition behind this and now I'm going to generalize it.
And the generalization shouldn't be surprising.
So in general I have n random variables usually denoted X_1 through X_n.
And the Bayesian networks is a directed acyclic graph over
these variables and it defines
a joint distribution over all the variables like this, X_1 through X_n.
And this is defined as a product of local conditional distributions,
one for each node.
Okay, so this is a product of all n,
X_i given X parents of i.
And this notation just means the values assigned to the parents of i.
Okay, so this is a very general framework.
Um, and, uh, just like factor graphs are a very, you know, general framework.
But the key difference from factor graphs
is the fact that these factors aren't arbitrary,
right, there are local conditional distributions.
And what does that mean? That means all factors satisfy this property.
So if you pick up a factor for the i-th node,
p of X_i given X parents is equal to
1 if you sum over all the possible values that X_i can take on.
Okay, that's what it means to be a, uh, probability distribution.
And this is true for every setting of experiments.
So this property has two implications which I'll
discuss consistency of sub-Bayesian networks and consistency conditional distributions.
Um, and these properties are going to allow us to really,
uh, take advantage of the probabilistic structure when we're doing inference.
Okay, so the first thing is the question
is suppose I have this Bayesian network, this alarm network.
And, um, I'm going to suppose I'm interested in
the marginal distribution of only B and E. Okay, I don't care about A.
So remember this is, um,
the joint distribution and by laws of probability,
I can derive the, um, marginal distribution.
Now, the question is what does this marginal distribution
have to do with the- the Bayesian network, the graph here?
Okay, so let's go through some algebra to find out.
So this is a sum over all A and by definition
this is just the product of all the local conditional distributions as we just discussed.
And now I notice that P of B and P of E don't depend on A which means that I
can pull this out and push the summation in. That's just, uh, algebraic manipulation.
And then what is this value?
This value is just 1 because of the previous slide.
So I can just drop it.
And now I have p of b times p of e. And lo and behold what is- what
is this? This is if you had just gone and defined
a sub- miniature of Bayesian network over
B and E. This was exactly what you've written down.
Okay, so that's kind of cool.
So the general idea here is that when you're marginalizing out, uh,
a leaf node that yields a Bayesian network just without that node.
So marginalization produces this, um,
this Bayesian network where you've just erased,
um, the very- the leaf node along with its incoming edges.
All right, so in other words,
I've turned basically what was, would have been
a algebraic operation into a graphical one.
And generally those are good moves
because it's much easier to kinda think graphically and,
uh, make large operations then go through tons of algebra. Yeah.
Definition equals, it seems like it's from like the probability [inaudible].
Yeah, so the question is what about this first definition
equals? What I mean here is by the laws of probability.
Um, so it's not technically a definition,
it follows from the axioms of probability.
Yeah, thanks.
Okay. So notice that in this world,
P- B and E are independent.
So this is one way you can kind of, uh,
see that actually when you define the joint distribution,
in that joint distribution, um,
two variables, B and E are independent.
So one thing to note is that if we looked at the factor graph,
um, which is this thing.
And remember last time we talked about marginalization in fact- factor graphs.
And what does that look like?
If you- what happens if you did marginalization in this factor graph?
Okay.
Yeah, you just
remove A but this factor is, does it disappear?
No, it's- it doesn't, right, because factor graphs,
remember that factor graphs don't know anything about this factor.
Other than that it,
uh, returns non-negative numbers.
So you would have to keep, hold onto this factor.
Right, so the moral of the story here is that if you're using factor graph,
if you convert the factor graphs too early,
then you might lose out on opportunities that really simplify it.
Whereas, if you look at this- the factor graph of this one,
there is no P of a given B and E. Right.
I mean just to go back here factor graphs will create a factor which is summation of A,
P of A given B and E and call that a factor.
And we know because these are local conditional distributions that's just one,
so you can just drop it.
Okay, so- so that's the first property.
Just summarize, if you marginalize out leaf nodes, uh,
you get Bayesian networks by just dropping them from the graph.
So the second property is its consistency of local conditionals.
As I alluded to before,
if you have P probability of D given A and B,
there's two versions of this that you might be thinking about.
One is the local conditional distribution,
which has again you just define it as such.
And then there is the corresponding quantity
that comes about from probabilistic inference.
So this quantity is derived from taking the definitions,
forming the joint distribution,
and then using the laws of probability to derive this particular quantity.
And this property says that don't worry about it, the two are equal.
So, you know, it means that you can kind of
intuitively think about there just can
be one notion of probability in your head.
But I wanna make this explicit but that this is- that doesn't come necessarily for free,
you have to kind of verify that this is true.
I'm not gonna go through the verification step, it's in the,
uh, notes in the slides,
but I'll just state it as such.
Okay. So let's do another example just
to familiari- familiarize ourselves with Bayesian networks a little bit more.
Um, so the question here is that suppose you have, um,
you wake up and you are coughing and you have itchy eyes
and you're wondering, do I have a cold or do l have allergies?
Okay, so let's follow this four-step procedure to define this Bayesian network.
Okay, so step 1, what are the variables here?
There's, um, coughing, let's denote that as H,
and itchy eyes, and then cold, and allergies.
Okay, so four random variables.
Um, how should I connect these things up?
Yeah, so H and I should be connected to C. So if you have a cold,
you probably have, um,
uh, a cough and you probably have itchy eyes.
And here you tap into your medical knowledge and,
um, what was that?
Yeah, so generally, uh,
I'm no doctor but let's just assume for now that allergies don't really cause the
coughing, cause the itchy eyes.
It's probably not true but let's just pretend it is.
Um, okay, so just to make the network a little bit more interesting.
Okay, so those are the edges,
and now I have to specify local conditional distributions over all these.
So what are the local conditional distributions?
So I have P of C,
P of A, remember one for every node, um,
and P of H given C and here over here is P of i given C and n, right.
So probability of a node given its parents.
And then finally I have the joint distribution which is probability of C, A, H, I.
And this is by definition just a product of everything.
Um, for this example I'm not going to go through and
define the actual tables because that's gonna take too much time.
But I'm gonna do it in this demo here.
Okay, so this is a Bayesian network that I just drew on the board
and this is its- its associated factor graph.
Remember one factor per node. Yeah.
The PowerPoint switches the allergies and cold.
C, A, oh, yeah, you're right. Um.
I guess that makes sense.
Which one makes sense?
[inaudible]
Yeah. Okay, I got,
I got a little bit, uh, confused [NOISE] Okay.
So it should be like this and then I have to adjust things, um, okay.
So we're fixing this.
I given a, um,
and c and a, okay?
Just for the record,
I'll just make this h given c and a and i given a, okay.
That wasn't too bad. Okay, thanks for catching that.
Okay. So this is the factor graph, um,
and let me show you, uh, this demo.
So you can click on this and you can see, uh,
this Bayesian network and this factor graph.
Um, and to answer this question, what was the question?
The question was if I have, uh,
if you are coughing and have itchy eyes or do you have cold or allergies.
So I conditioned on cough equals 1, uh,
itchy eye equals 1,
and I am asking for,
uh, the probability of, um, the cold.
Okay. And if you work it out,
you see that the probability of a cold is 0.13.
Um, and, you know,
so why does this- so- okay,
I guess I didn't really tell you enough about the actual prior probability.
So the probability of a cold is, you know,
0.1, um, let's say and the probability of allergies is, you know, 0.2.
And then there's a, kind of,
a noisy or where if you're, uh,
if you have, um,
a cold or allergies then you,
you end up coughing.
And, um, the- if you have, uh,
itch, allergies and you have itchy eyes with probability 0.9.
Um, and what happened here is that,
um, if you- oops.
Um, if you condition on, uh,
your coughing and you have itchy eyes,
um, there's this, kind of,
interesting explaining way happening here.
Um, where, you know,
even though you didn't observe A,
you observe evidence of A,
and that's enough to, kind of, uh,
lower the probability that you have a cold.
So this is- example show something a little bit more subtle how information can
kinda propagate along the Bayesian network in ways
that if you try to do it just kind of intuitively,
you will probably, um, not be able to.
Okay. So let me summarize so far what we've done.
So we've introduced Bayesian networks,
where we have random variables that capture the state of the world.
And we have edges between those variables that represent
dependencies between, um, those variables.
And, um, based on those dependencies,
we go and define local conditional distributions,
you multiply all those local conditional distributions, you get a joint distribution.
Now, with that joint distribution,
by laws of probability you can go and ask
probabilistic inference queries and ask questions about the world, um, given evidence.
And we saw that this captures interesting reasoning patterns such as explaining a way.
And finally, all of this can be, uh,
brought under the umbrella of
the factor graph interpretation which we will see is very useful for,
um, actually doing probabilistic inference in general, in a bit.
Okay. So any questions before I move on to the next section?
Okay. So now, I'm gonna talk about probabilistic programs.
So this is going to be, um, kind of,
a little bit of a whirlwind tour and hopefully give you different perspectives,
um, and open your eyes to, kind of,
the possibilities of Bayesian networks.
Um, so let's look at this alarm network again.
I can write it as on the board, um,
just a product of all the local conditional probabilities,
basically use math or I can think about this as a probabilistic program.
Okay. So what I'm gonna write down is a program that it's a very simple program,
um, it has three lines,
one for every, uh, variable.
And the first line is B is,
uh, drawn from Bernoulli Epsilon.
So this notation just means B is set to, uh,
a random value that has, uh,
distribution Bernoulli Epsilon, and same with, uh, earthquake.
And then finally I set A equals B or E. Okay.
Um, and, uh, so the idea here is that a probabilistic program is just
simply a program with randomness in it that when you run, sets the random variables.
So this is I, I think a really useful way to think about, um, Bayesian networks.
And just to be very concrete about this,
so you can think about Bernoulli of Epsilon as just a Python program that just returns,
uh, true with a probability Epsilon.
So here, random less than Epsilon,
the random is a number between 0 and 1,
has a probability of Epsilon being less than Epsilon.
Okay. Any questions about the- what this is doing? Yeah.
Why does the randomness help rather than [inaudible]
So the question is why does randomness help?
Um, the, the reason is that I want
this program to be, put a distribution over possible assignments.
Every time I run the program,
it's gonna produce a different assignment.
And the distribution over that assignment is the distribution that I'm defining.
So, so, so this is a kind of an interesting philosophical point.
So normally you run programs and wr- write
programs with the intention of running them and do, do something useful.
But here the re- programs are just a kind of artif- artifact to define a distribution.
Uh, hopefully this will become a little bit clearer as I go through more examples. Yeah.
Uh, if you want to define some distribution,
can you just, uh, can you find like hard-code the table instead of doing this.
I mean you can just hard-code Epsilon into your table instead of like
maybe like writing this program [inaudible] Epsilon?
Yeah. So the question is why don't you just, uh,
hard-code- define a table directly,
um, instead of running this program?
So the intention here again is not to run this program
because it's not an efficient way to do a probabilistic inference,
but it's more of a, a metaphor or a tool to help you get more intuition about,
um, probabilistic, uh, programs in Bayesian networks.
So hopefully, we, we can, uh,
come back to this question after I go through a few more examples.
So here's a more interesting probabilistic program.
So suppose you're doing object, um,
tracking and you define a program which starts with X_0 equals 0, 0.
So the initial location is at the origin,
and then for every time-step, um,
so I'm writing the program in kind of pseudocode here.
Um, with probability Alpha,
i set X_i equals X_i minus 1 plus 1, 0,
so I'm going to the right, and with probability 1 minus Alpha I'm going down.
Okay. So, um, now this program, you know, that I just described, um,
it induces a particular Bayesian network structure
where each x_i is only connected to x_i minus 1.
Okay. So what I'm trying to ge- get you to think
about is there's multiple ways of thinking about the same object.
And I think when you get- when you can kind of internalize all these things,
you kind of get a deeper understanding of what you're dealing with, right?
We have the probabilistic, uh, view, uh, viewpoint.
You can look at the tables, you have, you know, your equations,
you have this graph and now I'm giving you an additional tool, uh, the programs.
Okay. So just for fun,
um, you can actually run this program.
Again, this is not what you would do normally but,
um, I can run the program in any case.
So every time I hit Enter,
um, this gives you a different trajectory.
So this is a way to visualize the distribution over proba- uh,
X_1 through X, um,
whatever how many of- many, uh, red squares are.
Okay. And if I change Alpha,
that gives me distributions which are either skewed to one side or the other side.
Um, so that's the distribution over,
uh, um, programs, oh sorry, distribution over assignments.
Okay. So what does probabilistic inference look like in this setting?
So remember, what is probabilistic inference?
I'm conditioning on some piece of evidence and I'm
asking for the distribution over some other set of variables.
So in case, in this case I'm conditioning on the fact that I
spotted X of the object at 8 ,2 at time step x_10, that's it.
And I'm interested where it could have been before that.
So, um, here what I'm gonna do is I'm gonna run
the former program and I'm only gonna keep
those trajectories and show it if X_10 equals 8,2.
So if I do that, I'm gonna- so this is 8 ,2.
Um, I'm seeing that the set of,
uh, possible trajectories look like this.
So this is the distribution over,
um, trajectories given X_10 equals 8, 2.
Okay. So it's important- what I'm trying
to get you to think about is that Bayesian network or probabilistic program as,
what is the distribution?
You can visualize the distributions by looking at samples from that distribution.
It's another way to think about it.
Right, because distributions are, um, think about like, ah,
suppose you have a dish- I tell you I have a distribution over images,
and how do you actually get a hold of that or understand that?
Well, probably the easier- easiest way is to draw samples
from it and look at kind of the types of images that you get. Question?
Is this the way to specify a distribution,
are they like- is this a way to specify distribution [inaudible].
Uh, so question is,
is this way a way of specifying a joint distribution?
By this I mean- I guess you mean the- ah,
so probabilistic programming in general.
It is so- so for every probabilistic pro-, um,
program, it specifies a joint distribution
over the random variables that you set in that program.
And vice versa. If I have a Bayesian network,
I can write down a probabilistic program.
Um, one thing as you'll hopefully become clear is that,
the reason to think about in terms of programs is that you
can inherit all the nice properties of programs like,
the ability to define functions,
or even have recursion,
or, you know, you could do a lot more,
um, fancy stuff with programs that you can't do what- I mean which will be hard to do.
You can think about Bayesian networks as another way to think about is like,
okay you're basically writing assembly code right,
for every, ah, um,
variable you specify its value,
but if you have a million value- variables,
sometimes it's useful to be able to structure,
um, your- your code in some way.
We- we'll see that over the next few examples.
Okay. So this is going to be a march of I think around seven possible,
um or so possible examples,
and I just wanna give you a flavor of types of
probabilistic programs that we're talking about here.
So the first one is called, ah,
just a Markov and- by- whenever I say probabilistic program think Bayesian networks or,
um, generalizations of that.
So Markov model, um,
so this has a lot of applications in,
um, you know, modeling language or time series.
And, ah, the program works as follows,
for every position i through n,
I'm gonna generate a particular, ah,
word X_i, given the previous word.
Okay. So this is also happens to be the same type of program as for the object tracking.
Okay? So this is this Bayesian network structure.
Um, so here's another one.
This is called the Hidden Markov model which, um,
is, ah, was a very popular, ah,
model that was, um, used, ah,
for all sorts of things like speech recognition notably before,
um, the rise of deep learning.
Ah, so the idea here is that for every time step T equals 1 to
T. I'm gonna generate an object with location HT given the previous HT minus 1.
So this part is, just looks like a Markov model.
Okay. But the- the reason
why it's called a Hidden Markov Model is that I'm not actually gonna observe HT,
I'm gonna observe sensor readings ET at each time step T given,
ah, the hidden location.
Okay. So this is what a hidden Markov model looks like.
Sequence of object locations [NOISE] which I don't
observe and sensor readings which I do observe,
which depend respectively on the given object, ah, locations.
And just as a convention,
whenever I shade a variable,
that means I, you know,
observe it and if it's not shaded,
that means I don't observe it.
Okay. So this program defines a joint distribution over all of these variables.
And now you can ask a particular question,
you can do probabilistic inference.
And the most, um,
common thing that people do here is,
given the sensor readings, where is this object?
Which is something we've already been exposed to through the lens of
factor graphs but this is again a way to think about it,
um, through the lens of, um, vision algorithms.
So now, ah, with this kind of programming metaphor,
you can actually do, ah,
kinda more complicated things in a very kind of succinct way.
So to describe, uh, multiple object tracking,
you can think about, ah,
there being two objects A and B,
and each position, um,
at each time step and every object I'm gonna generate a location for the object,
and this is going to be two independent, um,
Markov chains which are running,
but the thing is that, at each time step,
I only observe one sensor reading and that sensor reading
is going to be some combination-
some function of the actual locations of objects at that particular time step.
Okay. So now hopefully you can see a little bit of the vantage of thinking in
terms of a program because I can write this kind of very simple four-line program that,
um, very precisely nails down what the actual, you know, model is.
Um, and in particular this factorial HM,
as it's called, is something that you're gonna be exploring in your- the car assignment.
Um, here's another example,
so this is, ah,
for- usually used for cla- classification.
It's called naive Bayes.
Some of you might have heard of it.
Um, and the program looks like this.
You first generate a label Y. Um,
let's suppose you generate travel.
And now you're gonna- for every word in your, ah,
document, you're gonna generate a word,
um, given that label.
So if you generate travel,
you may generate words like beach in Paris.
Um, so now the- that
again specifies a distribution over all the variables, what are you typically interested in?
If you're interested in classification,
you're given the words and now you wanna go back and,
uh, figure out what the,
the-, um, the class is.
You're given a text document,
what is, ah, the label?
Um, here's a fancier, ah,
model of documents called Latent Dirichlet allocation.
Um, so here instead of how to generate a single topic,
I'm gonna generate a distribution over topics.
This is getting a little bit meta because
this random variable in itself is actually a distribution but,
you know, let's not worry too much about that.
So this- this is a distribution, um,
and for every position I'm going to first generate a topic like travel or Europe.
And then for that, ah,
topic I'm gonna generate a word given that topic.
Okay. So this allows you to model documents which
talk about multiple things, for example, traveling Europe.
Okay. So this is also a very popular model that can be used to,
if you're given a collection of documents trying to understand,
understand the, ah, latent structure inside it.
Um, [NOISE] so here's one that's kinda a generalization of the, uh,
the medical diagnostics, um,
um, uh, example on the board.
So in general let's say you have a bunch of diseases.
Um, you generate the activity of a particular dis- disease in a patient,
um, according to some, you know, prior distribution.
And now you- for every symptom that, um,
you might obs- observe or any sort of lab test you
have the probability of some outcome of that symptom given the diseases.
And of course, the probabilistic inference question here is,
if the patient has particular symptoms,
what kind of diseases, ah,
does or problems does he or she have?
Okay? So I think this is f- maybe the final example.
Um, here is a social network analysis example where,
um, you have, um,
a set of people,
each person has ah,
you know, qua- a type,
maybe a politician or a scientist.
Um, and these- for every pair of people,
ah, they can either interact or not interact.
They might be connected or not connected,
let's say in a social network.
And so in the end what you're given is a social network of, ah,
connectivity and you're asked,
what kind of types of people are there?
So generally, you, you observe maybe some graph,
and you want to understand, um, you know,
what kind of features or, ah, you know,
what is- what is a concrete way of summarizing the types of people there are.
And there's- this called a stochastic block model but
there's other kinda fancier models that are based on a similar idea.
So that was a very quick, um, you know,
overview of different types of probabilistic programs or Bayesian networks.
And there- the point is that there are
many many different types of models that can be written down in the literature.
Many things, generative models can be just written
down in a probabilistic program or equivalently a Bayesian network.
Um, and all of them kind of have this kinda basic structure.
If you observe carefully,
all of them kind of look like that whereas there's some set of variables H, um,
which you don't observe,
and that generates or causes,
um, a set of variables E,
which you do observe.
So the mindset when you're designing Bayesian networks is,
you're coming up with stories of how the data which you- what you
observed was generated through the quantities of interest, the output.
So this is probably kind of maybe counter-intuitive and for
those of you who are really used to thinking
about just normal classification where you- it's the opposite.
You start with the input and you think about,
what are things to do to the input that it can, ah, you know,
what kind of things can I do to get it to a point where I can,
you know, classify the input precisely?
But Bayesian networks kind of go the opposite.
Um, it starts with the output or the structure is interested in which
are presumably kinda more- kinda a platonic idea or something cleaner,
and then you're trying to describe how that
clean data gets- gives rise to this kinda messy-sorry,
the clean structure give rise to the messy data that you observe. Question?
Can you explain again why it's called the output?
Right. So- why is,
uh, this called the output?
Um, so I'm using input output here in-
to borrow terminology for when we talked about classification,
where you're going from input to output.
Input is what you are given and output is what you're outputting, I guess, producing.
Right? And in the, the Bayesian network,
you first define the model,
kind of going from output to input.
So kind of the opposite of what you would normally do.
And now, now there's a second stage,
where you do probablistic inference, which reverses that.
And you go from the observations,
which are the input to the output which is right.
[NOISE] Okay?
Any other questions about this?
[NOISE] All right.
So now let's talk about inference.
Um, this is also gonna be the topic of next lecture but I'm just gonna start,
[NOISE] um, [NOISE] playing around with this a little bit.
So remember what is probabilistic inference?
We're given a Bayesian network,
define some joint [NOISE] distribution.
We're also given some setting of the variables,
which are the evidence,
for example I saw that the alarm went off,
um, and [NOISE] I'm interested in a subset of the variables.
[NOISE] Okay.
So what I'm trying to ch- produce is,
a probability of some query variables conditioned on evidence and what this really means,
is I want this for all values [NOISE] of,
um, the query variables.
Okay. So for example,
if I have coughing, I have itchy eyes or I have a cold.
It's an example of a probabilistic inference query.
[NOISE] Okay.
So let's start with this simple example.
Suppose I have this Markov model and I asked this query,
what is the probability of X_3,
[NOISE] given X_2 equals 5?
So condition X equals 5,
I'm interested in X_3.
Um, so at this point,
you already have the tools to do this, um,
and I'm gonna show you how you can just, uh,
go through the calculations and then I'm gonna show you an easier way to do this.
So if you were just shown this right now,
this is probably what you would, um,
do which might be a little bit tedious,
uh, so by laws of probability, this,
uh, this conditioning is equal to the joint over this, um, marginal case.
[NOISE] This is just by definition [NOISE] of conditional probability.
Um, and, uh, one thing I'm gonna do here is,
um, notice that I'm only interested in distributions of X_3.
So from that perspective,
this blo- denominator is just a constant.
[NOISE] It doesn't depend on X_3.
So what I'm gonna [NOISE] write is this proportional to, which means that,
the actual value here is this thing on the right-hand side times some constant which,
um, I don't care about.
And the reason I can do this and I don't care about is because I know that, um,
the left-hand side is the distribution,
so whatever I get on the right-hand [NOISE] side,
if it sums to 6 or something,
then I just divide by 6 and I get a distribution.
Okay. So this is gonna save you a lot of work [NOISE] if you use a proportional to sign.
But you have to use it carefully,
otherwise you can get wrong answers.
[NOISE] Okay.
So let's expand this.
So this is a marginal distribution of X_2 and X_3.
Um, I can write it in terms of the joint,
where I sum over the variables that I don't care about.
So there's again, laws of probability [NOISE] um,
and then the definition of the Bayesian network here,
is a joint distribution is equal to the product or local conditional distributions.
So right now, I have a lowercase p now because there're local distributions.
Um, now I'm gonna do some algebraic manipulation.
So notice that, um,
this stuff doesn't depend on X_4.
So I can push the summation of X_4 over here and then these two first two terms,
um, [NOISE] uh, only these first two terms depend on X_1.
So I can group this and to [NOISE] have the sum [NOISE] over X_1 apply here and then,
I can look over here and use, what does this sum to?
One. So I can drop it and then, what is this?
Does this depend on X_3?
Nope. So I can also drop that and I get a p of X_3 given X_2 equals 5.
Right? So this hopefully shouldn't be surprising
to anyone because remember that slide where I said, uh,
consistency of local conditional distributions,
this is- should be equal to this and this is just one way of verifying that.
That's actually the case for this example.
Okay? Um, so, you know this was- you can do this.
I mean for this one, it's actually not that bad.
Especially when you already know the answer.
Um, but I promise you there are gonna be situations where you definitely don't want to
grind through all the math because you can fill up 10 pages of equations.
Um, I'm gonna show you kind of a faster way to do this.
[NOISE] Um, and [NOISE] so let's start.
[NOISE] Okay.
So this is going to be a five-step, [NOISE] uh, procedure.
But in many [NOISE] cases,
not all the steps are necessary.
Um, okay. So let me erase this.
And the key idea is going to be [NOISE] to use the structure of the Bayesian network,
um, and factor graphs,
to simplify some of these operations.
Okay? So, um, let's start with- okay,
so you have X_1,
um, X_2, [NOISE] X_3, X_4.
Um, we just have four.
Okay? All right.
So- and I'm, um, [NOISE] conditioning on, um, X_2, right?
Okay. So X_2, uh,
that takes on value 5.
[NOISE] Okay.
So [NOISE] um,
the f- and I'm interested in this, uh, query variable.
So the first thing I want to do is,
[NOISE] I want to remove as many variables as I can.
Um, I just- because that's gonna simplify [NOISE] my life.
So I'm going to remove or marginalize, um,
non-ancestors [NOISE] of, uh,
the query and the variable I'm conditioning.
[NOISE] So by non-ancestors,
I mean, um, anything that's upstream,
I am gonna keep for now.
Anything that's downstream, I can let go.
[NOISE] Okay.
So what can I remove here?
[NOISE]
X_4.
X_4, right? So I can,
um, let me show this.
So I can graphically just remove X_4.
And that corresponds to on the slide,
basically the fact that this thing sums to 1.
But I've done this again graphically,
[NOISE] which hopefully should be, uh, more intuitive.
Okay. So the second step is,
I'm going to [NOISE] convert to a factor graph [NOISE] um,
because, uh, one already
takes care of- basically I'm exploiting the properties of Bayesian networks.
But after I've done one, um,
[NOISE] I don't- it's simpler to think about as a factor graph,
where I want to think about [NOISE] the factors more explicitly as
just arbitrary functions and not worry about
which way the conditioning is going because [NOISE] it's really easy to get confused by,
um, Bayesian networks, where you're wondering like,
oh this is conditioning over here,
wha- what's a marginal distribution and, um,
factor graphs I think,
by actually [NOISE] removing the directionality and some semantics,
actually make things a little bit easier.
Okay. So I'm gonna convert this into a factor graph,
which means I have,
um, let me actually just draw it down here again.
[NOISE] So here's a factor graph.
Um, remember I have, uh,
probability of X_1, um,
[NOISE] probability of X_2 given X_1, um. [NOISE]
So this might look like more, ah,
work [NOISE] right now,
uh, because I'm making things explicit.
Um, but you can actually do a lot of these things in your head
if you, um, get the hang of it.
Okay? So remember, every variable has, uh,
is associated with a factor, um, okay.
So now, I want to, um, you know,
condition [NOISE] on, on the, you know, evidence.
[NOISE] So I'm conditioning on x_2 equals 5.
So remember what the conditioning does,
remember from last week's lecture.
Conditioning just removes this,
and changes the factors to be set to the value that,
that um, variable takes on. Yeah.
Can [inaudible] x_4 in this?
Sorry. Yeah. We shouldn't have x_4, good point. Okay.
We still have factor on that other side or, uh-
This factor should be there.
So x_4 is- this is the factor graph corresponding to that.
Yeah. Um, okay.
So I'm conditioning on x_2,
so I wipe x_2 from the face of the earth,
and I'm going to set this- change
this factor to be a partial evaluation of where I put our x equals to 5,
and this factor is x_2 equals 5 given x_1.
Okay? Um, so this connection is good. So now.
I, um, can marginalize out the disconnected components.
Um, and these are the components that I will- remember,
I care about x_3.
So this stuff is disconnected so I don't care about it.
So I'm just going to, um,
let's say you just cross it out.
And that operation corresponds to the fact that,
you know, this thing over here.
I just can drop because it's, uh,
not related to x_3, it's just a constant.
Okay? So finally, um,
the fifth step is actually,
uh, [NOISE] do work.
Okay? So what does that mean?
Um, you might not be so likely to be left with just,
you know, a single variable whether factor where,
where that's just the answer.
Um, in that case,
you actually have to, um,
actually compute, do the marginalization operations that we saw last week.
In this case, we are fortunate that, um,
[NOISE] this factor, this actually represents a distribution of x_3,
so that is just the answer to, you know, the problem.
Okay. But I'll go through some other examples where it's not as obvious.
Okay? So this is just a general strategy that I outlined on the board here.
And again, I think once you get kind of good at this,
you can basically, the steps, um,
1 and 4 should be kind of very,
um, kind of visual because you can just see,
uh, well, all everything downstream just to be clear, it doesn't matter.
And when you see these,
um, [NOISE] these are, you know,
conditioning things, you can kind of automatically just not
ignore things and just jump directly to 5.
So that's the idea. I am just doing things that are more
explicitly on the board so you can kind of see where things are coming from.
Okay. So, um, I'm gonna do another example.
Um, this is the alarm.
[NOISE] So, uh, here,
I have this vision network,
and I- let's suppose I'm interested in probability of B.
Okay. So this should be an easy one.
So s- start with one,
marginalize out non-answers.
So which are the non-answers of B?
So A and E, right?
So I just removed them from the face of the earth,
and I'm just left with the single, uh, variable B.
And obviously, it has a factor of p of B, and then I'm done.
Okay? Okay. So this one's maybe a little bit more,
um, you know, complicated.
So this is the probability of earth,
uh, sorry, burglary, we've given A equals 1.
Um, so let's go through this example.
[NOISE] Um, I'll try to do it quickly. All right.
[NOISE] So I have, um, B,
E, and A. Um, okay.
So marginalize out non-ancestors.
So what am I interested in?
I'm interested in the probability of,
ah, B given A equals 1.
Okay? So I have A and B that I care about.
So what are the [NOISE] non-ancestor of these variables?
There's none. [NOISE] Right.
So this is the non ancestor of A, so I can't remove it.
So I can't do anything there, too bad.
Convert to your factor graph.
We've done this before.
Probability of B, um, moralized the parents.
[NOISE] So this is probability of A given B and E,
and then this is probability of E. Okay.
Um, condition on the evidence now.
So I conditioned on A equals, uh, 1.
So I'm gonna remove this and change this factor to A equals 1 [NOISE] given, uh,
B and E. Uh, fourth step is marginalize out anything that's [NOISE] disconnected.
Uh, nothing's disconnected. So I can't do anything.
And last, I have to do actual work.
Okay. So what does actual work mean here?
I'm interested in the probability of B.
So I need a marginalize out E. Now,
I have to do this kind of a hard way, um,
based on last time, uh, last lecture.
So, um, what I'm gonna do here is,
you know, what happens when I marginalize out E?
I create a new factor.
Let, let me actually replicate this down here,
so it doesn't get too confusing.
Um, so I'll create a new factor,
and this new factor that's called f of b,
um, which is the Markov [NOISE] negative E,
there's only one other variable B.
And this is going to be the product of
all the factors here that touched this variable that I'm marginalizing out.
And the only difference between this and what we are
doing last time is before we had a max,
because we're doing maximum weight assignments, [NOISE] and here,
I'm going to have a sum because we're doing probabilities in marginalizing.
So this is going to be, uh,
a summation over E. Okay.
And then the final query is going to be,
uh, just the product of those two things.
Okay. Um, I'm not gonna have time to actually drill down into expanding these values.
Um, but if you actually,
uh, plug-in Epsilons into these, um,
then you'll find that the probability of B equals 1, given, um,
A equals 1 is 1 over 2 minus Epsilon which is,
um, remember, 0.51 is for Epsilon equals 0.05.
Okay. But this calculation, um, you know,
you can look into the slides to see how this is actually done but it is just algebra.
Okay? Um, so the- there's another example which I'm gonna defer to section to talk about.
Um, I think in all of this,
you just need to do some practice [NOISE] and get
kind of comfortable doing these operations.
[NOISE] Um, to summarize,
to find Bayesian networks,
uh, there's this way of,
uh, defining models that,
um, allow you to specify locally and optimize globally.
Once you have a Bayesian network you can do
probabilistic inference where you condition on evidence and query variables of interest.
And next time, we're going to focus on number 5,
and hopefully not do things completely
manually but do things more automatically. Okay. That's it.
 All right, let's get started.
So we're gonna continue talking about Bayesian networks which we started on Monday.
Um, and just a kind of quick recap, uh,
we've been talking about Bayesian networks which is a new paradigm for defining models.
Um, and what is a Bayesian network?
Uh, you have a set of variables which are nodes in a graph.
For example, uh, whether you have a cold,
whether you have allergies, whether you're coughing,
whether you have itchy eyes.
These nodes are related by a set of directed edges which capture various dependencies.
For example, itchy eyes is caused by allergies but not cold or by cough.
[NOISE] And then formally for every variable in the Bayesian network,
you have a local conditional distribution which specifies
the distribution over that variable given the parents.
So the parents of cough are cold and allergies.
So what, you would have a local conditional distribution of p of h given c and a.
You do that for all the variables,
and finally you take all the factors or
local conditional distributions and you multiply them together and you get
one glorious joint distribution over all the possible variables in your distribution.
Okay? So in other words,
to sum it up you can think about Bayesian networks as factor graphs plus probability.
They allow you to define ginormous joint distributions over lots of random variables,
uh, using factor graphs which allow you to specify things very compactly.
And moreover, we saw glimpses of how we can
use the structure factor graphs to permit efficient inference.
So probabilistic inference in Bayesian networks
is the task of- given a Bayesian network, which is, you know,
this oracle about, uh,
what you know about the world, um,
and you look at some evidence that you've found.
So it's, you know,
it's raining or not raining,
or, [NOISE] or you have itchy eyes or so on.
And you condition on that evidence,
and you also have a set of query variables that you're interested in asking about.
And the goal is to compute
the probability of the query variables conditioned on the evidence that you see,
big E equals little e. Remember lower- upper cases,
random variables, lowercase, um, is actual values.
Um, and so for example, um,
in the coughing case there's, um,
the probability of a cold given the fact that
you're coughing but don't have itchy eyes, okay?
And this probabi- this probability is
defined by just the laws of probability which we went over,
uh, the first, uh, slide of last lecture.
Um, and the challenge is how to do this efficiently, okay?
And that's going to be the topic of,
uh, this, this class.
So any questions about the basic setup of what Bayesian networks are and,
um, how do you- what does it mean to do probabilistic inference?
Okay. One maybe kind of a high-level note about Bayesian networks is that I,
I think they are really, um,
powerful as a way to describe kind of knowledge.
I think a lot of, uh,
AI today is focused on particular tasks where you define some outpu- inputs,
and you define some outputs, and you train their classifier.
And the classifier that you train can only do this one thing, input, output.
But the paradigm behind Bayesian networks and, you know,
databases in general is that you have to develop a kind
of a knowledge source which can be probabilistic,
it's captured by this joint distribution.
And once you have it,
you use those tools of probability to allow you to answer arbitrary questions about it.
So you can give me any pieces of evidence
and any query and it's clear what I meant to do,
I'm supposed to compute these values.
So this is- it's kind of a more flexible and powerful, uh,
paradigm than just, you know,
converting inputs and outputs.
And so that's why I think it's so interesting.
Okay. So uh, today we're gonna focus on how to
compute these arbitrary inference queries efficiently.
I'm gonna start with forward-backward and particle filtering,
these are going to specialize to the specific, uh,
Bayesian networks, uh, called HMMs or Hidden Markov Models.
And then we're gonna look at Gibbs sampling which is a much,
uh, you know, more general way of doing things.
Okay. So um, a Hidden Markov Model which I talked about, uh, last time, um,
which we're going to go in more detail at this time is,
uh, a Bayesian network where there exists,
uh, a sequence of hidden variables and a corresponding sequence of observed variables.
So as a kind of motivating example,
imagine you're tracking some sort of object,
um, in your homework you'll be tracking cars.
So uh, Hi is going to be the location of the object or car at a particular time step i.
And Ei is going to be some sort of
sensor reading that you get at that particular time step.
It could be the location plus some noise,
or some sort of distance to that, um,
the true object, um, and so on.
Okay, so these are the variables and, uh,
it goes well as saying that
the hidden variables are hidden and the observed variables are observed.
Um, so the distributions over, uh,
this- all the variables are specified by three types of local conditional distributions.
The first one is just the starting distribution.
What is the probability of H_1?
Um, this could be uniform over all possible locations just as an example.
Um, and then we have the transition distributions which
specify what is the distribution over a particular hidden variable,
the location of the true location of object H_i given H_i minus 1.
So this captures dynamics of how this object or car might move over time.
Um, for example, it could just be uniform over adjacent locations, right?
So cars can't teleport,
they can only move to adjacent locations over uh, one timestep.
And finally, we have emission distributions which, uh,
govern how the sensor reading is,
uh, computed as a function of the location.
Okay? Um, so this, again,
could be something as simple as uniform over adjacent locations,
um, if you expect to see some noise in your sensor.
The sensor doesn't tell you where exactly the car is,
but it tells you approximately where the car is.
And the joint distribution over all these random variables is
going to be given by simply the product of everything you see on the board.
I'm just gonna write this up on the board just for, you know, reference.
Um, so we have the probability of
H equals h. So this- when I write H equals h that means,
uh, the H_1 through H_n.
So all the random variables, uh,
all the hidden random variables,
all the observed random variables,
and this is by definition equal to,
um, the, the start distribution.
Um, let's see just to make sure I have- okay, good notation here.
So start distribution over,
um, h_1, and then I have the transitions,
i equals, um, 1 to- uh,
I guess, 2 to n,
just to make sure, okay.
So this is the probability of,
um, h_i given h_i minus 1.
And then finally I have for every time step I through 1 through n. I
have the probability of an observation given, um, h_i.
Okay? So multiply all these factors together,
that gives me a single number that is, uh,
the probability of all the observed and all the hidden variables.
Okay? Any questions about the definition of a Hidden Markov Model?
[NOISE] Okay.
So given we have one of these models,
remember with a Bayesian network I can answer any sort of queries.
I can ask what is the probability of, um,
H_3 given H_2 and E_5,
and it can do, do all sorts of crazy things.
And all of these things are possible and efficient to, um, you know,
compute but we're going to focus on two main types of questions.
Uh, motivated by the,
uh, let's say object tracking example.
The first question is filtering.
Filtering says, "You're at a particular time step,
let's say time step 3.
What, what do I know about the true object location
H_3 given all the evidence I've seen up until, uh, now?"
So this is kinda real-time object tracking,
at each point in time you look at all the evidence and you want to know where,
um, the object is.
Uh, a similar question is smoothing and you're still looking at a particular time step,
um, um, three, let's say.
Um, but you're conditioning on all the evidence.
So you're looking at all the observations,
and you're looking at- you're kind of thinking about,
uh, this more retrospectively.
Where was object- where was the object at time step 3?
So think about if you're trying to reconstruct the trajectory or something.
Okay? So this is called filtering and smoothing.
Um, so let's now try to develop, um,
an algorithm for answering these type of queries, and,
um, without loss of generality,
I'm going to focus on answering smoothing queries.
Um, so my- why is it the case that if I tell you I can solve all smoothing questions,
I can also solve all filtering questions?
[NOISE] [inaudible]
So it is true so that this- in filtering this is- the evidence is a subset.
But the answers are going to be different depending on what evidence you compute on.
So you can't literally just use one as the answer for the other. Yeah.
You marginalize over the things that you,
uh, like E_45 to get to-
Yeah, yeah, so you marginalize.
That's- that's the key idea.
Is that suppose I had a smoother and I wanted to answer this filtering query, right?
So this is H_3 given E_1 to E_2, E_3, right?
Remember last time we talked about how you can take leaves of
Bayesian networks which are not observed and just essentially wiped them away.
So if you don't observe E_4, E_5, H_4,
H_5, you can just pretend those things, you know, don't exist.
Right. And now you're back
to a smoothing query where you're conditioning on all the evidence.
[NOISE].
Okay, so we're gonna focus on smoothing and to make progress on this problem,
I'm going to introduce a representation that's going to help
us think about the possible assignments, right?
And just to be- be clear, right.
There's- the reason why this is not completely trivial is that
there are for- if you have N hidden variables,
there's 2 to the N or exponential N number of possible assignments.
And you can't just enumerate all of them.
So you're going to have to come up with some algorithm
that can compute it more efficiently.
Okay, so what we're gonna do is introduce this lattice representation
which is gonna give us a compact way of representing those assignments.
And then we can see how we can operate on that representation, okay?
So this is gonna smell a lot like a state-based model.
So we're kind of going backwards,
uh, but hopefully it'll make sense.
So the idea behind a lattice representation is that I'm going to have,
um, a set of rows and columns.
So each column is going to correspond to a particular variable.
So the first column is going to correspond to H_1.
And each row is going to correspond to some setting of that variable.
So there's two possible things I can do.
I can either set H_1 equals one or I can set H_1 equal to 2.
I'm- the version I'm drawing on the board is going to be
a simplification of what I have on the slides just,
uh, in the interest of space.
And the second column is going to be either H_2 equals 1 or H_2 equals 2.
So by going through these, uh,
lattice nodes which are drawn as boxes,
I'm kind of assigning random variables to a particular value.
Okay, so I'm going to connect these up.
So from this state I can either set H_2 equals 1 or 2.
Here I can also go from to 1 or 2,
and finally let's just do H_3 equals 1,
H_3 equals 2, and similarly I can choose either one of them from no matter where I am.
And finally I have an end state, okay?
So first notice that the size of the lattice is reasonably well controlled.
It's simply the number of timesteps times the number of
values that a variable can take on.
So let's suppose that there's
N timesteps and K possible let's say locations,
so values of H_I.
So how many nodes are here?
K times N.
K times N, right?
Okay, so that means we can- essentially,
this doesn't blow up exponentially.
Okay, so now let us interpret a path from start to end.
What does a path from start to end tell us?
So let's take- let's take this path.
What does this tell us? Yeah.
It's like a particular assignment of arranged variables.
Yeah, it's a particular assignment of the variables.
So this one says, set H_1 to 1,
H_2 to 1, H_3 to 1.
This path says set H_1 to 2,
H_1 to 1, and H_3 to 2, and so on.
Okay, so every path from start to end is
an assignment to all the unobserved or hidden variables.
Okay, so now remember each assignment comes with some sort of probability.
So we're going to try to represent those probabilities,
um, juxtaposed on this graph.
Okay. So I'm gonna go through each of these edges.
So remember these are the probabilities.
So every assignment has- is a product of the factors.
And I'm going to basically take these factors and just sprinkle them on
the edges at the points where I can compute the factors,
and I'll explain more what I mean by this.
Okay, uh, so- so maybe one- one kind of preliminary thing I should mention is
suppose for this example we have- we are conditioning on
E_1 equals 1, E_2 equals 2.
Let's say E_1 equals- sorry E_3 equals 1, okay?
So I'm conditioning on these things.
Notice I'm not drawing them in here because these are observed variables.
I don't have to reason about what values they take on.
I'm only going to consider the hidden variables which I don't know.
But this is just going to be some sort of reference.
Okay. So let's start with- start H_1 equals 1, right?
So if you remember, uh,
backtracking in CSPs, right?
We basically took factors,
and then whenever we could evaluate the factor of, we just put it
down on that edge in the backtracking tree.
So here what- what can we do we have the probability of H_1 equals 1, okay?
And then we also have the probability
of the- the first emission probability I can compute, right?
So that's the probability of E_1 equals the evidence I saw which is 1,
given H_1 equals 1,
which is the value that I've committed to here.
So this is a number that is essentially the weight,
or cost, or score,
or whatever you wanna call it that I,
um, am going to incur when I traverse that edge.
Okay, so what about this one?
So I have the transition from P- let's say,
uh, H_2 equals 1,
given H_1 equals 1,
and times the probability of E_2 equals whatever I observe which is 2,
given H_2 equals 1.
And similarly over here I have probability of H_3 equals 1 given
H_2 equals 1 times the probability of E_3 equals 1,
which is whatever I observed,
given H_3 equals 1.
And then over here there's no more factors, uh, left,
so I'm just going to put 1 there, okay?
And you can check that when I traverse this path and I
multiply all these probabilities together,
that's exactly this expression for H_1 equals 1,
H_2 equals 1, H_3 equals 1.
E_1 equals 1, E_2 equals 2, and E_3 equals 1.
And for each of these edges,
I have an analogous quantity depending on the values that I'm dealing with, okay?
Any questions about this basic idea?
So in the slides, this is basically what I just said.
Okay. So- so now, um,
now what I am trying to do now is to- let's say I'm interested in,
um, you know, smoothing.
So I'm interested in what is the probability of H_3 equals 2, right?
Actually, let's- let's do the example on the board just- just,
uh, because that's the one I'll actually do.
So suppose I'm interested in the probability of H_2 equals,
uh, let's say 2 given the evidence.
E_2 equals 2, E_3 equals 1.
Okay, so this is the query I'm interested in, you know, computing.
Okay. So how can I interpret this quantity in terms of this lattice, right?
So this is- there is this H_2 equals 2 here, right?
That's somehow privileged and then asking,
you know what is the probability of this given the evidence? Yeah.
Sum over all the probabilities [inaudible].
Yeah, so sum over all the probabilities of the paths, right?
So remember every path through from start to end is an assignment.
Some of those paths go through this node which means that H_2 equals 2,
and some of them don't,
which means that it's not true, right?
So if you look at all those paths and you sum up
their weights of this node and divide by the sum over all paths,
then you get the probability of H_2 equals to 2, given the evidence.
Okay. So let me just write this.
This is going to be, uh,
sum over, um, colloquially,
sum over paths through, um,
each two equals 2 divided by sum of over all paths.
Okay. So now, the problem is to compute the sum over all paths going through,
um, h_2 or not going through h_2.
Okay, again we don't want to sum over all the paths literally
because that's going to be, um, exponential time.
So how can we do this? Yeah.
When you say sum, do you mean sum of the weights or some of the counts like how many [inaudible]?
Right. So what I mean by sum,
I mean sum of the weights.
So every path has a weight which is the product of
the weights on the edges and you sum of the- those weights, yeah.
Okay, so what's an idea that we can use to compute the,
the sum efficiently?
Sum, key word.
Dynamic programming.
Yeah, dynamic programming.
Okay, so this,
um, we're gonna do this kind of recursively.
Um, let me just show this slide.
So, er, it's gonna be a little bit different from the diamond programming that we saw,
er, before it's gonna be, um,
more general because we're not computing and let's say, ah,
one particular query but I'm going to compute a bunch of quantities
are gonna allow us to compute all queries essentially.
Um, okay, so let's- there's going to be three quantities I'm gonna look at,
um, and hopefully I can [NOISE] kind of out of colors but,
um, let's not use green for this then.
So there is going to be, um,
you know, forward messages, um,
F which I'll explain a bit.
There's going to be backward, um, messages B.
Okay, so what I want to do is,
ah, for every node I'm gonna compute two numbers. Yeah, question?
No.
Um, I have- I think I'm okay.
What color is that? Um, sure I'll take a blue marker, yeah.
Great. Thanks. Okay great.
So, um, let's call this S, okay.
Um, okay so, um,
so for every node I'm gonna compute two numbers.
One number we'll call the forward number is- or the forward message is going to
be the sum over all the weights of the partial paths going into that node.
And the orange number is going to
be the sum of all the partial paths from that node to the end.
Okay. So let's, let's,
um, so these are all meant to be probability.
So the number should be less than 1 but just to keep things simple,
I'm gonna put actual numbers on these just two, um,
uh, just two integers on them, so they don't have to carry around decimal points.
Okay, so this is one,
let's say, one, two, one,
um, one, two, one,
uh, one, two and one.
Okay, so remember every edge has a weight associated with it.
And so now let me compute,
um, the forward probability.
So what is the sum of all paths going into that node?
It's just 1, right?
Okay, so, ah, sorry forward is green.
So one and this is two,
okay just copying this.
And now recursively what is the p- um,
sum of all the weights going into this?
Okay, So, um, I could have come from here or I could have come from here.
Right, so if I come from here it's one times,
um, whatever the sum was there.
So that's 1 times 1, that's 1,
ah, plus 2 times 2.
Okay, so I'm gonna get a 5.
1 plus 4.
And here, I'm going to have, ah,
1 times 1 so that's 1,
1 times 2, that's 2.
So that's going to be 3.
Hopefully, you guys are checking my math here.
Um, so what about this node?
So now recursively this could have-
the paths going in here could have come from this node or that node.
So that's 2 times 5, so that's 10.
1 times 3 and that's,
ah, 3. So that's 13.
And this is, uh,
1 times 5 plus, uh, 3 times 2.
So that's, um, 11.
Right, okay.
So 13 represents the sum over all paths going into H_3 equals 1, okay?
So I can do the backward direction [NOISE].
So, um, so this is going to be, ah, in orange,
the backward messages which are,
um, paths going to the n. So this is gonna be 1.
So here, I'm going to have,
ah- so 2 times 1 plus 1 times 1.
So that's a 3.
This is 1 times 1, 2 times 1.
So that's a 3 as well.
Ah, this is 1 times 3,
ah, 1 times 3.
So that's a 6.
This is 2 times,
that's a 6 and a 3.
So that's a 9, ah,
and then I'm, you know, done.
Okay. Okay. Does that make all sense?
So these are kind of compact representations over the essentially the flow in and out of,
ah, these lattice nodes.
Okay, so the, the kind of the s- kind of magic happens,
um, when I have, um, these axes.
So now for every node,
I'm going to also just multiply them together, okay?
So that's gonna be 6,
uh, 18, um, 18,
as 9, 13, 11 and that's it, okay?
So what happens when I multiply them together?
Let's take another look at this node, right?
So what does 9 represent?
9 represents the sum over all the paths going through here,
right because I can take whatever paths I have
coming in and I can take whatever paths I have
going out and any sort of combination of them will be a valid end to end path.
Okay, and so this total weight is,
you know, 9 there. Yeah?
Why instead of sum we multiply for this case?
So why do we multiply instead of sum here?
Um, because we're multiplying, the weight of a path is the product, okay?
Mathematically, what's going on is, um, exactly factoring.
Right, so I suppose I had numbers,
let's say a, um,
b and c and d and I could choose a and b and c and
d. So what are the possible- so then I can do a plus b times c plus d, right?
Which is the sum over all possible paths and, uh,
you can- thus paths are either ac,
uh, ad, bc and bd.
Right, so I'm basically doing this th-
computing it in a factorized way rather than expanding out.
That's mathematically what's going on when
I multiply the forward and the backward messages.
And why are these called messages?
So the idea of messages, uh,
comes from the fact that you can intuitively think about
the forward messages as being kind of sent across the graph, right?
Because the message here depends only on the neighbors here.
And once I get these messages,
I can compute the f- the- my messages are next time-step based on that.
So it's kind of a summary of what's going on and I ca- I
can send the messages forward and same in the backward direction.
Okay, so now once I have these values, uh,
how do I go back and compute my query?
Sum over all paths through h_2 equals 2. What is that?
9, right. And over the sum of all our paths,
what's the sum over all paths?
Sorry, this should be 15.
I was wondering, did I screw anything else up? I think that's right.
I was checking because you know,
when if you sum these two numbers you get 24 which is all of the sum of all paths
going through here and that better be the same number
here and also here would be the same number there, right?
Okay. Someone that should have caught that.
Okay, um, all right,
so these are all the paths going through nine,
er- oh, sorry, going through this node.
And, um, if you look at all paths,
that's going to be 15 plus 9,
and that's going to be 24.
Okay, so final answer is a probability of h_2 equals 2 given these made-up,
uh, weights is going to be 9 over 24.
Okay, any questions about that?
[NOISE]
Okay, so le- let me just quickly go over
the slides which is gonna be a more mathematical treatment of what I did on the board.
Hopefully, one of the ways will resonate with you.
So define the forward messages for every node is going to be a sum over all of
the values at the previous time steps of
the forward message at our previous timesteps times the weight on the edge from,
uh, the previous value to the, the current value?
Um, the backward is gonna be defined similarly for every node.
Sum over all the values assigned to the- at the next time step,
all outgoing edges, um,
of the backward message at the next time step times the weight into that next time step.
And then define S as simply just the product of F and B.
Okay, so that's what I did on the board.
And then finally, if you normalize the sum at each point in time,
you can get the, uh,
distribution over the hidden variable or given all the evidence.
And to summarize the algorithm, the forward-backward algorithm,
this is actually a very old algorithm, um,
developed actually for, ah,
for speech recognition, a while back.
I think it's, you know,
probably in the '60s or so.
Um, so you sweep forward,
you compute all the forward messages,
and then you sweep backwards and compute all the backward messages,
and then for every position,
you compute S_i for each i and you normalize.
So the output of this algorithm is not just the answer to one query,
but all the smoothing queries you want.
Because at every position,
you have the distribution over the,
um, the hidden variable h_i.
And the running time is n times,
ah, k squared, ah,
because there's n time steps and every time step you have to compute the sum.
So for a k possible values here,
you look at k possible values there.
So that's a k squared,
and it's n times k squared.
Interestingly if you ask, okay,
what's the cost of computing a single query?
It would also be n times k squared.
So it's kind of cool that you compute
all the queries in the same time that it takes to compute
a single query. Okay. Question?
[inaudible].
So, uh, question is does this only work for Hidden Markov Models or is it more general?
There's certainly adaptations of this which work,
uh, very naturally for other types of networks.
And one immediate generalization is if you have not just a chain structure,
but you have a tree structure then the idea of passing messages along that tree, um,
it's called belief propagation, um,
is, uh, just works pretty much out of the box.
For arbitrary Bayesian networks this won't work because, ah,
once you have cycles,
then you can't represent it as a lattice anymore.
Any other questions? Okay. So to summarize, this lattice representation,
uh, allows us to think of paths as assignments,
which is a familiar idea if we're thinking about state-based models.
Um, we can use the idea of dynamic programming to compute
the sums efficiently but we're doing
this extra thing where we're computing all of the sums,
um, for all the queries at once.
And the forward-backward algorithm, uh,
allows you to share intermediate computation across the different queries. Yeah.
[inaudible]
So the output of this algorithm is, uh,
basically the probability of h_i given all the evidence, for every i.
[inaudible]. [NOISE].
Oh, so how would you actually use this, do you sample from it?
Um, depends on what you want to do with it.
So the output of this you can think about it as a distribution at each time step.
So it's like n by k matrix of probabilities, right?
From that you can sample if you want, uh,
you're gonna- you might be only interested in only,
uh, various points in time.
Um, it's yeah.
Okay. So, uh,
let's move on to the second algorithm which is called particle filtering.
Um, so we're interested still in Hidden Markov Models,
um, or the particle filtering again is something actually much more general than that.
Um, and we're going to only focus on query- filtering questions.
So we're doing our filtering,
we're at a particular time step,
we're only interested in the probability of
the hidden variable at that timestep computation on the past.
And why might, um,
we not be satisfied with,
um, Hidden Markov or the forward-backward algorithm?
So here's the motivating picture.
So imagine, um, you're doing, uh,
the car assignment, um, let's say,
and you're, so you're tracking cars.
Okay? So cars let's say live on a huge grid, um,
so at each position h_i,
um, the value of h_i is some point on this grid.
But you don't know where it is, you want to track it.
Okay. So if this is like 100 by 100, you know,
that's 10,000, um, if this were a thing
where it continuously would be even, you know, worse.
Uh, so this, um,
this k squared where k is the number of values
could be like 10,000 squared and that's a large number.
Right? So, um, even though hidden Markov model with backward,
forward-backward is not exponential time,
even the quadratic can be pretty expensive.
And in the further the,
motivation is, you know,
you- and you really shouldn't have to pay that much, right?
Because let's say your sensor tells you that,
oh the car is up here somewhere.
And you know cars can't move all the way across here.
So then, you know,
but the algorithm is going to consider each of
these possibilities and most of
all these probabilities are gonna have pretty much 0 probability.
So that's really wasteful to consider all of them.
So can we somehow focus our energies on the region that,
um, have actual high probability? Yeah, question?
Is that way of like saying do you think of backwards like for the later timesteps you
can't have 0 in one of those positions in the original table?
The question is can you go backwards.
Like if you can, like if you're continuing to do it one way and
you say like it's very unlikely that I'm gonna go back to the starting position,
do- do each of those variables happen in the same domain, have to say?
Oh so each of these variables,
they don't have to be from the same, um, domain.
For this presentation, they're in the same domain just for simplicity.
Um, but I think what you're asking is you know that maybe,
um, a car only moves let's say forward or something.
Then there is some restriction on the domain.
Um, it's not gonna be that significant because you still don't know where the car is.
So, uh, it doesn't really cut out that many possibilities.
Yeah, maybe by a factor of 2 or something but that's not, um, that significant.
Yeah. Okay, so how do we go about making this a little bit more efficient?
Um, so let's look at beam search.
So our final algorithm is not gonna be beam search,
it's gonna be particle filtering but beam search is gonna give us some inspiration.
So remember in beam search we keep a set of k candidates of partial assignments,
um, and algorithm as follows.
You start with a, a single empty assignment,
and then for every, um,
position time step, um,
I'm going to consider all the candidates which are
assignments to the fi- first i minus 1 variables.
I'm gonna extend it.
There's possible ways of extending in our setting h_i to
v from any v in the domain of i. Um,
so now I'm going to amass this,
uh, set of, um,
extended assignments, um, now have k times as many
because each previous assignment got expanded by k. So I'm gonna prune down.
I'm just going to take all of them,
sort them by weight and take the- the top k. Okay?
So visually, um, remember from last time it looks like this.
So here is, uh,
this object tracking, um,
where we have five variables and you start with beam search which is, um,
assigning X_1 to 0, um,
or 1 and then,
um, you extend it.
So you extend the assignments, you prune down,
you extend assignments, you prune down,
you extend the assignments and prune down, and so on.
And at the end of the day,
you get k candidates.
Each candidate is, uh,
full assignment to all the variables and it has a particular weight,
which is its actual weight.
And at each intermediate time,
it's a partial assignment to only the prefix of,
uh, i random variables.
Okay? And remember that beam search doesn't have any guarantees.
It's just a heuristic, but it,
uh, often it works well in practice.
And the pict- picture you should have in your head is that you have
the exponentially sized tree of
all possible assignments and beam search is kind of this pruned,
uh, breadth-first search along this tree which
only looks at promising directions and continues,
um, so you don't have to keep track of all of them.
Okay. So at the end,
um, you can use beam search,
you get a set of candidates which are full assignments to
all the random variables and you can compute,
uh, any quantities you want.
Um, uh so the problem with this is that it's slow.
Um, for the same reasons as I described, uh,
before it requires considering every possible value of, um, H_i.
So it's a little bit better than forward-backward, right?
So for forward-backward, um,
you have to have the domain size times the domain size and now for beam search,
um, it's the size of the beam times the domain size,
you know, which is better.
But- but still I think we can do a little bit better than that.
Um, and finally there's this kind of more subtle point is that, um,
as we'll see later really taking the best k might not
be the best thing to do because you want some- maintain some diversity.
Right. Just a kind of a, uh, quick visual.
So suppose you, um,
your beam consists of only cars over here.
It's kind of a little bit redundant but you might want,
uh, kind of a broader representation.
Okay. So the idea with particle filtering is to just tweak beam search a little bit,
um, and this is going to be expanded into three steps which I'll, um, talk about.
Okay, so let me, um.
Does anyone need this on the board?
Can I erase it? Okay. We're good.
[NOISE] Anyway.
You can look at video if you ah, don't remember.
[NOISE] Alright.
So there's three steps here.
Okay. And we're gonna try to do this ah, pictorially over here.
Um, [NOISE] so, so the idea behind particle f- filtering is,
I'm going to maintain a set of particles that kind
of represent where I think the object, ah is.
So imagine [NOISE] um,
[NOISE] you know the object starts over here somewhere.
So you have a set of particles,
and I'm gonna iteratively go through these three steps.
So propose, [NOISE] um,
w- w- w- weight,
[NOISE] and um, ah re-sample [NOISE].
So this is meant to be kind of a replacement of the extend-prune uh,
strategy for Beam search.
Okay, so the first,
step is to propose.
So at any point and time particle filtering maintains a set of partial assignments,
known as particles that kinda tries to mimic a particular distribution.
So um, to kind of jumping to the second time step.
We can think about this,
set of particles as representing the probability of
H1 and H2 given the evidence, you know, so far.
Okay. Um on the board,
I'm only gonna draw the,
ah the particle representing the value H2,
um because it- it's hard to draw trajectories but you can think about um,
ah really particle filtering maintains this lineage as well.
Okay, so the key idea of the proposal distribution is that,
okay, we want to advance time, now.
So um, we're interested in H3 but we only have H2,
so how do we figure out where H3 is?
So we, propose possible ah,
values of H3 based on H2.
So this is idea of proposal.
We just simply draw H3 from this transition distribution.
So this- remember this distribution is- comes from an HMM.
This is d- you're given HMM, so you can do this.
Um, and this gives us a set of new particles which are now extended by 1.
And this represents the ah,
distribution H1, H2, H3,
given the same evidence.
Okay? So pictorially, wh- ah,
you should think about propose as um,
[NOISE] So propose is kind of taking
each of these particles and sampling according to th- the transition.
So think about the particles as um,
you know ah, just moving in some direction.
It's almost like simulating where,
you know cars are, are, are going.
And this is done in kind of stochastically and randomly.
Okay. Um, step two is to wait.
So, so far the new locations really don't represent ah, reality.
Right? Because we also see E3.
At timestep three we get a new observation that
hasn't been incorporated somehow, into this.
We're just kind of s- simulating what ah, might happen um,
and so the idea here behind a weighting is for each of those particles,
we're gonna assign a weight now,
which is equal to,
the mission distribution over E3 given, you know H3.
Again, this is emission distribution which is given by our
HMM so we can just evaluate ah,
it whenever we feel like it.
And the set of new particles w- which are weighted,
can be thought of representing this distribution where now we
have condition on um, E3 equals 1.
Okay, so now each of these particles has some, you know weight.
So on, on this picture [NOISE] it kind of looks like this, um,
so maybe let's say the ah,
emission distribution is kind of let's say um,
ah, let's say a Gaussian distribution around th- the observation.
So suppose, the observation tells you,
well, it's over here somewhere, um,
which means that these particles are
gonna ge- get higher weight and these particles are gonna get lower weight.
Um, and if they're far away enough then maybe they get like almost zero weight.
So um, I, I'm going to kind of softly [NOISE] x [NOISE] these out.
So think about these [NOISE] as um, okay.
Mayb- maybe I'll do this.
So I'll upweight these, um,
[NOISE] and kind of start downweighting them.
[NOISE] So nothing really gets ah, zeroed out.
But you can think about these as downweighting and these as
upweighting provided [NOISE] you have let's say some evidence um,
E3 that tells you,
your- you're going in that direction.
Okay. Okay so the final step is,
is really about a resource distribution question.
Um, so now we have weighted particles,
we need to somehow get back to unweighted particles.
Okay, so which one to choose?
Um, [NOISE] so imagine ah,
you have this situation where you have particles which are kind of distributed,
the weights of the particles are fairly uniform.
Um, then you could imagine let's take just the particles with the highest weight.
Right. This is very similar to what Beam Search would do.
You just take all the particles with high weight and ah,
just keep those and nothing else.
Um, so this is not a crazy thing to do but um,
it, it might give you an impression that you are more confident than you actually are.
Right, because imagine the,
the weights are fairly uniform.
So maybe this one is like, you know,
0.5 and this one is like 0.48.
Um, so you're kind of just b- breaking ties in a very biased way.
Um, so the idea is that if you sample it and instead sample from this distribution,
you're gonna get something a lot more representative rath- rather
than just taking um, kind of the best.
Okay, so how do you sample from this distribution?
Um, so the general idea is,
if you have- if I- this is just kind of a useful module to have here having.
If I give you a distribution, um,
over n possible values,
then I'm gonna draw,
ah, let's say K samples from this.
So if I have the distribution over four possible locations with
these probabilities or you know, weights, then um,
I might draw off ah, four samples,
and I might pick a1 ah,
and then a2, and then I may pick a1 and a1.
So some of these things are not gonna be chosen ah,
if they have sufficiently low probability.
Okay. So going back to the particle filtering setting,
uh, we have these old particles remembering which are weighted.
Uh, and first, I'm going to normalize these weights to get a distribution.
So add these numbers up and divide by that.
And then, I'm going to sample
according to the distri- distribution given weights on the previous slide.
So I might draw in this case 0, 1, 1 once,
and I might draw it again and might not,
uh, even keep this particle, right?
Um, and the idea here is that suppose a particle has really,
really low weight, it has like 0.00001.
Then, um, I shouldn't, kind of, keep,
um, it around and because it continues to occupy memory and I have to keep track of it.
It's basically, you know, gone right.
So this re-sampling, kind of,
regenerates the pool by focusing their efforts,
um, on the higher weight particles.
It might even have re-sample a hi- hi- higher weight particle multiple times.
Um, um, and then not sample the low weight particles, uh, zero times.
Okay. So in this,
in this picture here,
so re-sampling, uh, might- let's see,
uh, how do I, how do I draw this.
So maybe, now I have, um,
maybe I sample this twice,
maybe I sample this twice,
and maybe these don't get sampled.
Maybe I sample this once,
this once, uh, right.
So, so the blue represent the particles after this one round of particle filtering.
Where I've, kinda, moved the particles over here a little bit and lots of weight from there.
So that's why it's kinda called particle tracking because you can think about
the swarm of particles representing where the object might be.
And over time as,
as I follow the transition dynamics and,
um, hit it with the observations,
I can, uh, move this swarm over time.
So that's the picture you should have in your head.
Okay. So let's go through the formal algorithm.
It's gonna be very similar to beam search.
So you start with empty assignment,
and then you propose s- where you
take your partial assignments to the previous i minus 1 variables.
And then I'm going to consider for each one of them just sampling
once from this transition distribution and augmenting that assignment.
So unlike beam search,
where the size of C prime was K times larger than C,
the size of C prime is equal to the size of C in this case.
Second, I relate so looking at the evidence,
and applying the evidence- probability of evidence given the particle H_i,
um, gives me a weight for every particle.
Um, and then I'm going to normalize this distribution sample
K elements independently from that distribution and that redistributes,
um, the, the particles to where I think they're more promising.
Okay. So let's go through this, um, quick demo.
Um, so the same problem, uh,
as before, uh, I'm gonna set the number of particles to a 100.
So I start with all the particles, um,
[NOISE] assigning X1, you know,
to 0 and there is 100 copies of them.
Um, I extend and notice that some of the particles go to 0,
and some of the particles go to 1 with, uh,
approximate probability proportional to whatever the transitions are.
Uh, and then going to redistribute which changes the,
uh, balance a little bit.
And gonna extend prune, uh, extend.
Uh, by prune, I really mean,
uh, ris- re-weight and re-sample.
Um, and notice that, uh,
the particles kind of get more diver- This is more diverse than, um.
Well, it's more diverse than beam search because I'm using K
equals 100 rather than like 3.
But, uh, but you can see that,
um, some of these particles might, like, my, like,
this one has 0 weight,
so that when I re-sample and they just go away. Do you have a question?
Yeah. Why don't we aggregate
all the ones into a single category and all the zeros into single category?
So just to show the branching pattern,
or is it actually relevant?
Yeah. So that's a good question.
So notice that, um,
all of these are,
all of these zero for the purposes of,
uh, X4 are just pretty the same.
So if you only care about the marginals
then you can collapse them. You're absolutely right.
In this demo, it's- I'm maintaining the entire history so that,
yeah, you can show the- see the branching.
Okay. So this is that point [LAUGHTER].
If you only care about the last, uh,
position and not about the possible trajectories, then you can
actually collapse all the particles with the same H_i into, uh, one.
And then furthermore, if there's repeats then
you can just keep track of the count, right.
And this is actually what you would do in your assignment.
I am giving you kind of the more general picture in case.
Because particle filtering is more general than just, uh,
looking at the last timestep,
but most of the time you're just interested in the last timestep.
Okay. So just a quick, kinda quick,
um, visual illustration of this.
Um, let's define this factor graph where,
oh, you have, um,
transitions that are basically 1 or 0
depending on whether h_y- i and H_i 1- minus 1 are close to each other.
And O_i is some sensor reading.
Um, one thing I've been a little bit, uh,
sliding under the rug is sometimes I've talked about local conditional distributions,
sometimes I've been talking about factors.
Remember from the point of view of inference,
um, it really doesn't matter.
They're all just, you know, factors, right?
So in particular, if I give you
a factor graph which- right remember which is not necessarily,
uh, uh, a Bayesian network.
I can nonetheless still define a distribution by simply just, uh, normalizing.
Pick all the weights a mulpti- uh,
and normalize and divide by that.
And these objects are actually called, uh,
Markov networks or Markov random fields,
which is another object of study that we're not gonna talk about in this class, but, um,
this is actually a more, you know,
general way to think about the relationship between factor graphs and distributions.
Um, we're only mostly focusing on,
on Bayesian networks in this class.
But some examples would be more general than that.
Okay. So you have this distribution and,
um, so you can play with this demo in the slides.
If you click then, this yellow dot shows you,
uh, the observation at a particular point in time.
And the noise, uh,
the observation is related to the true position
of some particle by based on the noise that you define here.
So here I've defined box noise which means that it's gonna
be a uniform distribution over a box of,
uh, 3 by, uh, 3 by 3.
Or I guess a 6 by 6, uh, box.
Um, and so if I increase the number of particles to,
uh, let's say 10,000,
then what I'm gonna show you is a, a red blob,
um, that looks like it's trying to eat the [LAUGHTER] uh,
uh, the yellow dot.
Uh, and this red blob shows you, uh,
the set of particles where the intensity is
the count of that number of particles in a particular cell.
So this swarm, kind of,
corresponds to on the board,
it's the set of particles.
Uh, but since this is discretized, you can, kind of,
see the pile of particles,
piling up on each other.
Uh, and just to see how well this is doing,
show true position, er,
you can see the blue dot is the actual object position,
the yellow dot is the noisy observation,
and it's trying to do its best to track where the blue is.
It's not perfect, uh, because this is kind of an approximate algorithm,
but it kind of gets most of the way there.
Okay. Any questions about particle filtering?
So to summarize, you can do forward backward if you
can swallow computing on a number of domain values times number of domain values.
If you have large domains,
but you really think that none- most of them don't matter,
then particle filtering is,
is a good tool because it allows you to
focus your energies on the relevant part of the space.
Okay. So now, let's revisit Gibbs sampling from a probabilistic inference point of view.
So remember, Gibbs sampling we talked about, uh,
last week as a way to compute the maximum weight assignment in an arbitrary factor graph,
where the main purpose is to get out,
uh, of local minimum.
Uh, so remember how Gibbs sampling works,
you have a weight which is defined for, uh, complete assignments.
So unlike particle filtering or beam search,
we're starting with complete assignments and trying to modify
the complete assignments rather trying to extend partial assignments.
Uh, so you loop and you compute.
You pick up a variable X_i and then you consider all the possible, uh, um,
possible values you can take on,
and you choose, uh,
the value with probability proportional to its weight.
Okay. Let me show you this, um,
example now, that uh, we saw last week.
Uh, so, so same graph here.
Uh, so we start with this complete assignment.
Uh, and then we're gonna examine X1.
X1 can take on two possible values.
For each of these values, I'm gonna compute its weight.
And remember in Gibbs sampling,
I only need to consider the Markov blanket of that variable.
Uh, the factors here are o1 and t1 because that's only thing that changes.
Everything else is a constant.
Uh, and then I'll normalize and sample from that.
Okay. So then, uh, I go onto the next variable and so on.
So I sweep across all the variables, and, um,
eventually the weight hopefully goes up,
but not always up because sometimes I might sample a value that has lower probability.
Okay. And at the same time,
I can do various things like, um,
computing the, the marginal distribution over a, a particular variable.
So, or if two variables.
So I can, um,
basically count the number of times I see particular patterns
and I can normalize that to get a distribution over that particular pattern.
[NOISE] Okay.
So now let's try to interpret Gibbs sampling from a probabilistic, ah, point of view.
So instead of just thinking about, ah,
a weight as just a function,
we can actually think about the probability distribution
induced by that factor graph by
again summing all the weights over all possible assignments.
Normalizing, there you have a distribution.
Um, so the way to think about Gibbs sampling is
now more succinctly and more actually traditionally written as the following,
which is you loop through all the variables,
and for every variable you're going to look at
the probability of that variable taking
on a particular value condition on everything else.
So now I can give like, write down this probability which is, you know,
a nice way to think about what Gibbs sampling is actually, you know, doing.
Um, and the, the guarantee with Gibbs sampling,
um, under, you know, some conditions,
which I won't get into is that,
as you run this for long enough, you, ah,
the sample that you get is actually a true sample from this distribution,
as if you had sampled from this distribution.
And if you did a multiple times,
now you can actually compute any sort of marginal,
ah, distribution, you know, you like.
So now, while there's, that guarantee sounds really nice,
there are situations where Gibbs sampling could take
exponential time to get there, so caveats.
Okay. So let's look at a possible application of Gibbs sampling, image denoising.
So suppose you have some sort of noisy image and you want to clean it up.
So how can this be, um, helpful?
So we can model this image denoising problem as,
um, this factor graph where, um,
you have a grid of, ah, pixel values,
um, and, ah, they're connected,
um, in this kind of grid like way.
So every value of X_i is,
where i is a location, um, two numbers,
is going to be either,
ah, 0 or 1,
um, and we're gonna assume that some subset of the pixels are observed.
Um, and in case, ah,
we observe it then we're actually just going to have, ah,
a factor that is actually a constraint that
says that value of X_i has to be whatever we observed.
And these- we have these, um,
transition potentials that say
neighboring pixels are more likely to be the same than different.
So it assigns value 2 to pixels which are the same and 1 to pixels which are different.
Okay. So is the model clear?
So now let's try to do Gibbs sampling in this, in this model [NOISE].
Just to give you, um, a concrete idea of what this looks like.
Um, so we now look at- I'm not gonna draw the entire, ah,
[NOISE] the grid but I'm gonna center around
a particular node that we're interested in, um, sampling.
So there's more stuff over here [NOISE].
Okay. So, um, and remember in,
in Gibbs sampling, um,
at any point in time,
all the variables have some sort of preliminary as- assignments.
So this might be 1, um, ah,
1, 1, and 0 and this might be 1.
Okay. Um, so now I sweep through and I'm gonna pick up this variable,
and you're gonna say, [NOISE] shall I try to change its value?
First of all, you ignore the old value because it doesn't,
[NOISE] ah, factor into the algorithm.
And now you're going to consider, um,
ah, let's say, this is X_i.
So X_i, there's two possible,
um, ah, values here.
So 0 and 1, right?
So, um, and I'm gonna look at the weight.
So if it's 0,
then I'm gonna evaluate each of these,
ah, factors based on that.
So remember, the transition potential is,
um, um, well, I'm not gonna write it down.
But if- let's consider 0 here.
So these are different.
That means I'm gonna get a 1,
ah, these are different,
that's gonna be a 1.
These are different, that's gonna be a 1,
um, these are the same and I'm gonna get a 2.
Okay. Now I try 1.
Um, these are the same.
These are the same.
These are the same,
and, ah, these two are different.
Um, so for every assignment I have this weight, so this is 2,
this is 8x, um,
and then I, ah, normalize.
So this is gonna be 0.8 and this is gonna be 0.2,
[NOISE] and I draw, um,
ah, flip a coin with heads,
ah, 0.8 and whatever I get I put down.
So I might have with point a probability I put that 1 back, and so on.
Okay. And here's another example which I'm not gonna go through.
Okay. So Gibbs sampling is gonna do that.
So now let's look at this, ah, um, concrete.
Oops. Okay. So. All right.
Okay. So what you're looking at here is this grid of pixels.
Um, white means that the pixel is unobserved.
Black or red means that it's observed to be whatever color,
ah, you see on the screen.
And, ah, so this is some- somewhat of a noisy image and the goal
is to fill in the white pixels so the picture makes sense.
And visually, you guys can probably look at this and see the hidden, ah, text.
No? [NOISE] Okay. [LAUGHTER] Okay.
So your denoising system is pretty good.
Um, okay. So I'm gonna run Gibbs sampling, so we click.
And what you're seeing here, each iteration,
I'm gonna go through every pixel and apply
exactly the algorithm of whatever I did on the board.
Okay? So you can see that, um,
these are just samples from,
ah, the set of unobserved random variables.
Okay. So to turn this into something more, um, useful,
you can instead of looking at a particular sample,
you look at, um,
the marginal, which is for every location,
what is the average pixel value that I've seen so far?
Ah, and if you do that,
then you can see a little bit clearer picture of, um, 221,
and it's not gonna be perfect because the model that we have is fairly simplistic.
I always says is similar pixels- neighboring pixels have or tend to have the same,
um, ah, color and it has no notion of,
you know, letters or something.
Okay. But you can see kind of a simple example of,
ah, you know, Gibbs sampling our work.
Um, there's a bunch of parameters you can play with.
Um, you can- here's, ah,
another picture of a cat and you can try to-, um,
you can play around with the coherence,
which is how sticky the, the, ah,
transition constraints are, you could try to use ICM,
which won't work, um,
at all, and so on.
Okay. Any questions?
I think we might actually end early today.
Okay. So, um, just to kind of sum up,
we've, last week, Mondays we define new models.
So we defined a Bayesian network or a factor graph.
And today, we're focusing on the question of how do we do probabilistic inference,
and for some set of models I've shown you how to do this.
Um, there's a number of algorithms here, um,
and a, a forward backward, which, ah,
work for HMMs and are exact,
particle filtering which works for HMMs although they can be generalized,
or which is approximate, and Gibbs sampling which works for
general factor graphs which is also approximate.
Each of these algorithms we've seen kind of these ideas in previous incarnations.
So forward backward is very similar to variable elimination.
Um, because it- variable elimination also,
ah, computes things exactly.
Um, particle filtering is like Beam Search,
um, to compute things approximately,
and Gibbs sampling is like, um,
iterative conditional modes or Gibbs sampling,
um, which, ah, we saw from last week.
Okay. So, ah, next Monday,
we're gonna look at how we do learning.
So up until now, the Bayesian network,
all the probabilities are fixed and now we're going to actually start to do learning.
Um, I should say that maybe this learning sounds a bit scary
because there's already a lot of machinery behind
inference and factors and all this stuff but you'll be
pleasantly surprised that learning is actually much simpler than inference.
So stay tuned.
 Okay. Let's get started again.
Um, so before I, uh,
proceed to Bayesian networks,
I'm gonna talk about a few announcements.
So as you probably know,
car is due tomorrow, um, p-progress is, uh,
due Thursday, um, and then finally the,
the kind of the big one, uh,
is the exam which is next Tuesday.
So everyone, if you're in the room or if you're watching from home,
please remember to go to the exam.
Um, um, the exam will cover all the material from the beginning
of the class up until and including today's lecture.
So all Bayesian networks, no logic.
Um, it doesn't mean you shouldn't use logic on the exam.
Uh, reviews are going to be in section,
uh, this Thursday and Saturday.
So you've got not one,
but two review sessions.
The first one we'll talk about, uh,
reflex and state-based models and the second we'll talk about variable risk models.
Um, at this point, all the alternative exams have been, uh, scheduled.
Um, if you can't make the exam for some reason,
then, uh, really please come talk to us right now.
Um, and just a final note is that the exam is,
uh, the problems are a little bit different than the homework problems.
They require more kind of problem-solving,
and the best way to prepare for the exam is to
really go through the old exams and get a sense for what kind of,
uh, uh, problems and questions you're gonna be asked. Question?
Where is the Saturday session?
Where is the Saturday session, does anyone know?
We don't know yet.
We don't know yet. It will be posted on Piazza.
Any other questions about anything?
I know this is, uh, a lot of stuff, um,
but this is probably going to be the busiest week,
and then you get Thanksgiving break. So that will be great.
Okay. So let's jump in.
So two lectures ago,
we started introducing Bayesian networks.
So Bayesian networks is a modeling paradigm where you define a set of variables,
which capture the state of the world,
and you specify dependencies depicted as directed edges between these variables,
and given the graph structure,
then you proceed to define distribution.
So first, you define
a co- local conditional distribution for every variable given the parents.
So for example, H given C and A and I given A,
and then you slam all these local conditional distributions, aka factors together,
and it defines a glorious joint distribution over all the random variables, okay?
And you think about the joint distribution as the source of all truth.
It is like a probabilistic database,
a guru or oracle that can be used to answer queries.
So this- in the last lecture,
we talked about algorithms for doing
probabilistic inference where you're given a Bayesian network,
which defines this glorious joint distribution,
and you're asked a number of questions,
and questions looked like this: Condition on some evidence,
which is a subset of the variables that you have observed,
what is the distribution over
some other subset of the variables which you didn't observe?
Then we look at a number of different algorithms in the last lecture.
There is the forward-backward algorithm,
which was useful for doing filtering and smoothing queries in HMMs,
this was an exact algorithm,
then we looked at particle filtering which happens, uh,
to be useful in a case where your state space,
the number of varia- uh, the values that a variable can take on,
it can be very large.
Um, and this is an approximate,
but in practice it tends to be a good approximation.
Um, and then finally, particle filtering which- sorry,
and Gibbs sampling, which is this much more general, uh,
framework for doing probabilistic inference in, um,
arbitrary, uh, factor graphs,
and this was again approximate, okay?
So so far, we've, uh, bit off two pieces of this,
uh, modeling inference learning triangle.
We've talked about models,
we've talked about inference algorithms,
and finally today we're gonna talk about learning.
And so the question in learning,
as we've seen repeatedly throughout this, uh,
course, is: Where do all the parameters come from?
So so far, we have assumed that someone just hands you this,
these local conditional distributions which,
uh, have numbers filled out.
But in the real world, where do you get these?
And we're gonna consider a setting where all of these parameters are
unknown and we have to figure out what they are from data, okay?
So the roadmap, we're gonna start- first start with supervised learning,
which is going to be the kind of the easiest, easiest case,
and then we're gonna move to unsupervised learning where some of the,
uh, variables are gonna be unobserved, okay?
Any questions before we dive in?
[NOISE] Okay.
Let's do it. So the problem of learning is as follows.
We're given training data and we want to turn this into parameters, okay?
So for the specific instance of, uh, Bayesian networks,
training data looks like an example is an assignment to all the variables, okay?
This will become clear in an example.
And the parameters are all those local conditional probabilities that
we now assume we don't know, okay?
So here's a question: Which is more computationally more expensive for Bayesian networks?
Um, is it probabilistic inference given,
uh, the parameters or learning the parameters given data?
Um, so how many of you- just use your intuition,
how many of you think it's the former,
probabilistic inference is more expensive?
Okay. One maybe.
How about how many of you think learning is more expensive?
Yeah. That's probably what you would think, right?
Because learning seems like there's just more unknowns,
um, and it can't- how can possibly it be, um, easier.
It turns out that it's actually the opposite. Yeah. So good job.
[LAUGHTER] Um, and, this will hopefully be a relief to many of you,
um, because, uh, I know probabilistic inference gets a little bit, uh,
quite intense sometimes and learning will actually be, um,
maybe not quite a walk in the park, but, uh,
it will be, um, uh,
a brisk stroll in the park.
Um, and then when we come back to unsupervised learning, it's gonna get hard again.
So, um, at least in the fully supervised setting,
it should be easy and intuitive.
So what I'm gonna do now is going to build up to
the general algorithm by a series of examples of increasingly complex Bayesian networks.
So here's the world's simplest Bayesian network.
It has one variable in it.
[NOISE] Let's assume that variable represents the rating of a movie.
So it takes on values 1 through 5.
And to specify the local conditional distribution,
you just have to specify the probability of 1,
the probability of 2, the probability of 3,
probability of 4, and probability of 5, okay?
So these five numbers represents the parameters of the Bayesian network, okay?
By fiddling these, I can get different distributions.
Okay. So suppose someone hands you this training data.
So it's fully observed, which means I
observed- one time I observed the value of R to be 1,
um, the second day I observed the value to be 3,
third day I observed it to be 4,
and so on, okay?
So this is your training data, okay?
So now the question is: How do you go from the training data
here to the parameters, okay?
Any ideas? Just use your gut, what does your gut say?
Count the number of [NOISE] and then divide them.
Yeah, count and then divide or normalize, okay?
So this seems a very natural intuition.
Um, later, I'll justify why this is a sensible thing to do.
But for now, let's just use your gut and see how far we can get.
Okay. So the intuition is that, um,
the probability of some value of r should be
proportional to the number of times it occurs in the training data.
So in this particular example,
I look at all the possible values of r 1 through 5 and I just see 1 shows up once,
2 shows up 0 times,
4 shows up 5 times, and so on.
So these are the counts of the particular values of r,
and then I simply normalize, okay?
So normalization means adding the counts together,
which gives you 10,
and dividing by 10,
which gives you actual probabilities, okay?
It's pretty easy, huh? Yeah? Good. Okay. So let's go to two variables.
So now you improve your model of, uh, ratings.
So now you take into account the genre.
So genre is a variable G which can take on two values,
drama or comedy, and the rating is the same variable as before.
And now let's draw the simple Bayesian network which has two variables, um,
and the local conditional distributions are P of G and P of R given G, okay?
So again, I'm gonna give you some training data which specif-
each training example specifies a full assignment to all the variables.
So in this case, it's d, 4,
d- and this one it's d,
4 again, uh, this one is c, 1 and so on.
And the parameters here are, uh,
the local conditional distributions,
which is P of G and P of R given G, okay?
Okay. So let's proceed to do this,
um, and here following our nose again.
Uh, we're going to estimate each local conditional distribution separately, okay?
So this is may not be obvious why separate- doing it separately is the right thing to do.
But trust me, it is the right thing to do and it certainly is the easy thing to do.
So let's, uh, just do that for now.
Okay. So again, loo- if we look at P of G,
so we have to look at only this data restricted
to G and we see that d shows up three times,
c shows up twice so I get these counts and I normalize,
and that's my estimate of the probability of G, okay?
And now let's look at the conditional distribution R given G. Again,
I'm going to count up the number of times that these variables,
uh, G and R show up.
So d, 4 shows up twice; d,
5 shows up once and so on;
count and normalize. Question?
So if, if instead of R given G had-
The probability R of r,
would that cause any differences in the results,
like, if the order [NOISE] were changing?
Yeah. That's a good question. So the question is,
what happens if we define our model in
the opposite direction where we have P of R and P of,
uh, uh, G given R?
So then you would be estimating different probabilities.
You would be estimating P of R,
which contains 1 through 5,
and then P of, uh,
G given R, which would be,
you know, a different table.
Do you get the same results?
Uh, so the question is,
If you do inference,
do you get the same results?
Um, in this case,
um, you will get the same results.
In general, you're not.
Uh, you're- depending on the,
the model you define,
you're actually gonna get a different, uh,
distribution, um, which will lead to different inference results.
Yeah. This happens when you have two variables and
you couple them together and you do things like this way.
You're effectively estimating from the space of
all joint distributions G and R. But if you have, uh,
conditional independence and you have, for example, HMM,
the order, how you write the model definitely affects what inferences can return.
Okay. All right.
So, so far, so good.
So now let's add another variable, okay?
So, uh, so now we have a variable A,
which represents whether a movie won an award or not.
Um, we're going to draw this Bayesian network,
um, and the joint distribution is P_G,
P_A, P_R given A and G, okay?
So this type of structure is called a V structure, uh,
for obvious reasons, and this remember was the thing that was tricky.
This is the thing that leads to explaining away and,
um, all sorts of tricky things in Bayesian networks.
It turns out that when you're doing learning in them,
um, it's actually the- all other trickery goes away.
Okay. So, um, let's just do,
uh, the same thing as before.
So we get this data.
Each data point, full assignment to all the variables.
So this is d, 0,
3 corresponds to G equals d and, um,
A equals 0, and R equals 3,
and the parameters are all again all the local conditional distributions.
Um, and now I'm gonna count and normalize.
So this part was the same as before, counts in, uh,
the genres, d shows up three times,
c shows up twice, normalize.
Um, A is treated very much the same way.
So, um, three movies have won no awards,
two movies have won one award.
So the probability of a movie winning award is 2 out of 5.
And then finally, um,
this is the local conditional distribution of R given G and A,
and here we have to specify for all combinations
of all the variables mentioned in that local conditional distribution.
I'm gonna count the number of times that configuration shows up.
Um, so d, 0, 1 shows up once right here; d,
0, 3 shows up once,
right here and so on, okay?
And now when you normalize,
you just have to be careful that you're normalizing only over the variable,
uh, that you're, uh,
the local distribution is on; in this case,
R. So for every possible unique setting of G and A,
I have a different distribution, okay?
So d, 0, that's a distribution over 1 and 3.
And if I normalize, I get half and a half and each of these other ones
are completely separate because they have different values of G and A.
Okay. Any questions?
All good? All right.
So that wasn't too bad.
Um, so now let's invert the V and look at this structure,
where now we have genre,
there's no award variable,
but instead we have two people rating a movie and the Bayesian network looks like this.
The genre and we have R_1 given G and R_2 given G. And for now,
I'm going to assume that these are different,
uh, local conditional distributions.
So we'll have PR_1 and P of R_2.
So notice that in this lecture,
I'm being very explicit about the local conditional distribution.
Here, I'm instead of just writing P of G,
which can be ambiguous,
I'm writing P of G to refer to the fact that this is the
local to conditional distribution for variable G. Um,
this one's the one for R_1,
this one for R_2, and those are different.
And you'll see later why this, uh, this matters.
Okay. So for now,
we're gonna go through the same motions.
Hopefully, this should be, um,
fairly, um, intuitive by now.
You simply count, normalize,
um, for the P of G and then for R_1,
I'm just going to look at the first, uh,
or the second element here which corresponds to R_1 and ignore R_2, right?
So G R_1. So d,
4 shows up twice.
So I have d, 4, uh, and d,
4 that shows up twice;
d, 5 shows up once,
that's the d, 5; c,
1 shows up once;
and c, 5 shows up once, okay?
And then normalize, get
my distribution, and then same for R_2.
Now ignoring R_1, I'm gonna look at how many times did G equals d and R_2 equals 3, okay?
So you can think about each of these as a kind of
a pattern that you sweep over the training set.
So, um, you have d,
5 so that's a 1 here; and d,
4 that's a 1 here; and d,
3 that's a 1 here and so on and so forth, okay?
And then you normalize, okay?
How many of you are following this?
Okay. Cool. All right.
So now things- um,
I'm gonna make things a little bit more interesting.
[NOISE] So here I've defined
different local conditional distributions for each rating variable R_1 and R_2, right?
But in general, maybe I have R_3, and R_4,
and R_5, and I have maybe a thousand people who are rating this movie.
I don't really want to have a separate var- distribution for each person.
So in this model,
I'm going to define a single distribution over rating conditioned on genre called PR,
and this is where subscripting with the actual identity of
the local conditional distribution becomes useful and this
allows us to distinguish PR from this case,
which is PR_1 and PR_2, okay?
Notice that the, the structure of the graph remains the same.
So you can't tell from just looking at the Bayesian network.
Um, you have to look at carefully at the parameterization.
Okay. So if I just have one PR,
what I'm going to do is,
um, what do you think the right thing should
to do with this if you're just following your nose?
[inaudible].
I'm sorry?
[inaudible].
Yeah, so count both of them, I think is what you're saying.
So you combine them, right?
So, um, so P of G is the same,
and now I'm going- I only have one distribution I need to estimate, uh,
here r given g. Um,
and I'm gonna count the number of times in the data where I'm using- uh,
I have a particular value of g and a particular value of r. Okay?
And I'm going to look at both R_1 and R_2 now.
Okay? So D_3 shows up once here.
So that's, uh, R_2,
D_4 shows up three times.
So once here with R_1,
once here with R_1,
and once here with R_2, and so on.
I'm not gonna go through all the details,
and then you count and normalize.
Um, another way you can see this is that if I take the counts from,
um, um, these two tables,
I'm just kind of merging them,
adding them together, um, and then,
um, normalizing. Question? [BACKGROUND].
If the Rs are IID, are they drawn from the same conclusion, or?
Yeah, so is this assuming something about independence here?
Um, so here when I am doing- I am assuming the data points are independent first of all,
and moreover, I'm also assuming
the conditional independent structure of the Bayesian network is true.
So conditioned on g, R_1 and R_2 are independent.
[BACKGROUND]
Yes, so here I'm also assuming that R_1 given g has the same distribution as R_2 given g,
and this is part of the-when I define the model that way I'm,
you know, making that assumption.
[NOISE] Okay, so this is a general idea which I want to call out,
which is called, uh, parameter sharing.
Um, and parameter sharing is when
the local conditional distribution of different variables use the same parameters.
So the way to think about, uh,
parameter sharing is in terms of, uh, powering.
So you have this Bayesian network, it's hanging out here.
And behind the scenes,
the parameters are kind of driving it, right?
And you can think about the,
the parameters as these little tables sitting out there,
and you connect a table up to a variable if
you say this table is going to power, uh, this node.
Okay? So, uh, what parameter sharing in this particular example was saying,
this distribution of G powers this node and these two variables,
R_1 and R_2, are going to be powered by the same co-local conditional distribution.
Okay?
So, um, okay,
I'm gonna go to two [NOISE] more examples.
Maybe I'll draw them on the board to make this a little bit more clear.
So remember the naive based model,
uh, from, uh, two lectures ago.
So this is a model that has a variable that represents- adapted to,
uh, movie genre setting.
We have a genre variable which takes on var-values comedy and drama,
and then I have a movie review which represents
a document with L words in it and each word is,
um, drawn independently given, uh, y.
So I have said the joint probability is therefore going to
be probability of genre y times the probability of,
uh, w_j given y for all, uh,
words j from 1 to L. Okay?
So, so the way to think about this, um,
graph the Bayesian networks is,
um, you have a variable Y here,
and so I'm just gonna draw W_1, W_2, W_3,
and, um, I'm going to have
a local conditional distribution here which is y P genre of y.
So that's some table that's powering this node,
and then I have a separate one single other of variable, um, sorry,
local conditional distribution of, uh,
w, uh, y and w, and, um,
probability of what I call it word, word, um,
W given y, and
this distribution powers, um, these variables.
Okay? So notice that, um,
here there's two, uh, local conditional distributions,
we have genre and we have word,
even though there are l plus 1, uh,
variables in the Bayesian network. Yeah.
[BACKGROUND]
Yeah, so the input,
what are- so the question is what is the input to, uh,
P word W given y, uh,
so when you apply this to a particular variable, um,
in some sense you bind Y to Y and you bind W to a particular Wi at hand.
[BACKGROUND] [inaudible].
Exactly. And you can see this kind of mathematically,
where you hear we have,
um, you pass into the P word,
W j given y and j ranges from 1 to l. [BACKGROUND].
Okay. Um, just to kind of solidify understanding,
uh, let me ask the following question.
If, um, y can take on two values as in here and each word can take on,
um, d values, how many parameters are there?
In other words, how many numbers are in these,
uh, two tables on the board?
So shout out the answer [BACKGROUND]
to the [inaudible] right.
Okay, okay. So 2D plus 2,
so there's two here, right, so there's C_B.
So there's two numbers,
and then there are for every C,
there's d possible values, for d there're d possible values.
So there's 2 plus, uh,
so there's two here,
and then there's, uh,
d plus d here.
Okay? Now if you really want to
be fancy and count the number of parameters you really need,
it should be less and less because if I specify the probability of C, then you can, uh,
1 minus that probability is probability of D,
but let's not worry about that.
[BACKGROUND]
Okay, let's do the same thing for HMMs, um,
just to make sure we're on the same page here.
Since we all love HMMs, um, okay.
So in HMM, remember there is a sequence of hidden variables H_1, H_2, H_3.
Um, and for each hidden variable,
we have E_1, um,
E_2, and E_3 which are the observed variables.
And, um, there are three types of,
uh, distributions for HMMS.
There is, um, the probability of, um,
the- I'm gonna call it Pstart of h,
which is gonna specify with the initial distribution over H_1.
I'm gonna have the transition probabilities,
which I'm gonna denote,
um, h-, um, it's called h prime.
Probability of h prime.
Oh, I should write down Ptrans here.
H prime given h. So this is going to be
another distribution which powers each, uh, transition.
Uh, each non-initial hidden variable, okay.
Remember, the- these are pointing to variables, not edges or anything else.
And finally, I'm going to have, um, a distribution,
the emission distribution of e given h. Um,
that table is going to power, um,
each of the, uh,
observed variables e, okay.
And here, [NOISE] again, there are three types of distributions: start,
transition, and emission, even though there could be any number of,
uh, variables in a natural Bayesian network.
Okay. And just to be very clear about this,
when I apply this table to this node, H binds to,
um, H_1 and H_2- h prime binds to H_2.
And then when I apply it to this node,
H_2 binds to h and h prime binds to H_3.
And again, you can see this from, you know,
formulas where I'm passing in to Ptrans,
hi given hi minus 1 as i sweeps from 2 to,
um, n. Okay, so you can think about this as like a little function with local variables,
the argument to that,
um, that function r, h, and h prime.
But when I actually called this function so to speak,
when defining the joint distribution,
I'm passing in the actual variables of the Bayesian network.
Okay, any questions about this?
Okay, maybe just to summarize some concepts first.
Okay. Um, so we talked about learning
as the generically the problem of how we go from data to parameters.
And data in this case is full assignments to all the random variables.
In HMM case, it's, a data point is,
an assignment to all the hidden variables and the observed variables.
And parameters usually denoted Theta is all the local conditional distributions,
which are these, uh,
three tables in the case of HMM.
Okay. Um, the key, um,
intuition is count and normalize, um,
which is intuitive and later I'll justify why this
is an appropriate way to do parameter estimation.
Um, and then finally parameter sharing is this idea which allows you to define huge, uh,
Bayesian networks but, um,
not have to blow up the number of parameters because you can share
the parameters between the different variables, okay.
So now, let's talk about the general case.
Um, so hopefully, you kind of understand the basic intuition.
So this is just going to be, um,
some abstract notation that's gonna kind of sum it up, um.
So in a general Bayesian network,
we have variables x_1 through xn.
And we're gonna say that parameters are a collection of distributions.
So Theta equals P_d, okay.
And so big D is going to be,
um, the set of types of distributions.
So in the HMM case,
it's going to be three types: start, trans, and emit.
So basically, that's the number of the- these little boxes that I have on the board there.
Um, and the joint distribution when you define a Bayesian network is
going to be the product of P of X_i given x parents of i.
So this is the same as we had before.
But now notice the crucial difference which I've out-
outlined in red here is that I'm subscripting this p with,
um, a d_i, okay.
So what d_i for the i'th variable says is
which of these distributions is powering that variable, okay?
So d of, uh,
this variable is emit,
d of this variable is, uh,
transition, and d of this variable is start, okay.
So this looks maybe a little bit abstract, um, um, notationally,
but the idea is just to multiply in the probability is,
uh, that, uh, was used to generate that variable or power that variable.
Okay. And parameter sharing just means that the d_i could be the
same for multiple i's. Yep?
In a Markov model case where the emission probabilities are all the same for all variables.
Like why do we need multiple emission distributions,
like, wouldn't be the same as just drawing a emission distribution different?
Yes, so the question is if we only have one emission, uh,
distribution, why do we need so many of these replica copies?
Um, and the reason is that,
these variables represent, um,
the objects' locations at a particular time.
So the value of this is gonna be different based on what time step you're at.
But the mechanism for generating that variable is the same.
Just like if I flipped,
a coin, you know, 10 times,
I only have one distribution that represents the probability of heads,
but I have 10 realizations of that random variable.
Um, another analogy that might be helpful is think of probability of these as,
like, a parameter- of, like,
functions in your program that you can call, right.
This is like a sorting function.
And sorting is just used in a whole bunch of different places,
but it's kind of the same- kind of local function that, uh,
powers a bunch of, uh, different, um,
use cases which are specific to the context.
Yeah. Okay, so in this general notation,
what does learning look like?
So the training data is, um,
a set of full assignments and I want to output the parameters.
So here's the, the basic form,
it's count and normalize.
So in counting, um,
there's just a bunch of for loops.
So for each, um, training example,
which is x is a full assignment,
I'm gonna look at every, um, variable.
I'm going to just increment a counter which,
uh, of the di'th, um, uh,
distribution of this particular configuration, uh,
x parents i and xi, okay.
And then in the normalization step,
I'm going to consider all the different types of, um, distributions.
And then I'm gonna consider all the possible local assignments to the parents,
and I'm going to normalize,
um, that distribution. Yeah?
[inaudible] of d_i the right table we're looking, correct?
Yeah, so the d_i, uh, refers for the i'th variable,
which red table I'm looking at.
[NOISE].
Okay. So, um, I've given you already a bunch of examples.
So hopefully this, um,
the notation might be a little bit abstruse,
but hopefully you guys already have the, you know, intuition.
Um, any questions? We're moving on.
The main point of this slide is just to say that this is actually very general and I
just didn't do it for
hidden Markov models and naive Bayesian doesn't work for anything else.
Okay. So now let me come back to the question of,
you know, why does count and normalize make sense, right?
So count and normalize is just like,
you know, some made up procedure.
So why is it a reasonable thing to do?
And it turns out this is actually based on very firm kind of foundational principles,
this idea of maximum likelihood,
which is an idea from statistics,
um, that says if I see some data and I have,
uh, a model over that data,
I want to tweak the parameters to make
the probability of that data as high as possible, okay?
Uh, so in symbols,
what this looks like is I want to find the parameters Theta.
So remember parameters are probabilities in the,
uh, red tables on the board, um,
and then I'm going look at- make the product of all
over all the training examples the probability of that assignment.
So for every possible setting of the parameters,
I can, that assigns particular probabilities to, um,
my training examples and I want to make that number as high as possible, okay?
And the point is that the algorithm on the previous slide exactly
computes this maximum likelihood through,
uh, parameters in closed form,
so which is really nice.
If you think about when we talk about machine learning,
we define a loss function and you cannot
compute anything in closed form except for maybe linear regression,
and you have to use stochastic gradient descent to optimize it.
Here, it's actually much simpler because of the way that the model is setup.
You just count and normalize,
and that is actually,
uh, the optimal answer.
So just because you write down a max,
it doesn't always mean you have to use gradient descent.
Is the, is the lesson here. Um, okay.
So let me- I'm not gonna prove this in generality,
but I want to give you some intuition why, uh,
this is true and hopefully connect the,
the kind of the,
the abstract principle with, uh,
on the ground, um,
algorithm that you- of count and normalize.
So suppose we have, um, uh,
two variable Bayesian network with genre and rating.
So I have three data points here and I have this maximum likelihood principle,
which I'm, you know,
going to follow and let's do some algebra here.
So I'm gonna expand the joint distribution,
and remember joint distribution is just the product
of all the local conditional distributions,
um, and I've also expanded this,
you know, the product over these three instances.
So I have the probability of d,
probably of 4 given d, um,
probability of d here,
probably of 5 given d, um,
probably of c, and probably of 5 given c, okay?
And what I'm maximizing over here right now
is a distribution over genre and I have a distribution of
rating conditioned on genres c and the distribution of rating conditions
genre equals d. Okay.
So, um, I've color-coded these in a certain way to emphasize, um,
that the- all the red touches is,
is the local conditional distribution correspond to genre,
the blue is corresponds to, um,
probability of rating given genre equals d and green
is probability of rating given genre equals c, okay?
So now I can shuffle things around, okay?
And I notice, um,
that these factors don't actually depend on probability of g at all.
So they can just hang out over here.
And I've essent- and the- and this likewise, um,
if I'm thinking about the maximum over,
uh, you know, argument c,
these other factors are just constants.
So they don't really matter either.
So I've basically reduced this as a problem of three independent maximization problems, okay?
And this is why I could take each local conditional distribution in turn
and do count and normalize on each one separately. Um, okay.
And then the rest is actually, um, you know,
to do- to do do it actually properly,
you have to use Lagrange multipliers,
um, to, to solve it.
But, um, intuitively, uh,
hopefully you can either do that or just believe that if I ask you,
um, what is the probability,
best way to set probabilities so that this quantity is maximized,
I'm going to set, um,
probability of d equals two-thirds and probability of c equals one-third.
[NOISE] This is similar to one of the questions on,
uh, the foundations homework if you, uh,
remember, but only for probability of,
um, uh, a coin flip essentially.
Okay. So hopefully some of you can now,
uh, rest, uh, you know,
sleep at night thinking that if you do count and normalize,
you're actually obeying some,
um, high-minded principle of maximum likelihood.
Okay. Um, so let's talk about Laplace smoothing.
So here's a scenario.
If I have given you a coin and I said I don't know what's the probability of heads,
but you flip it 100 times and 23 times it's heads and 77 times it's tails.
What is the maximum likelihood,
uh, estimate going to be?
Yeah. So it's going to be probability of heads is,
you know, count and normalize, uh,
it's 23 over 100, which is 0.23,
probability of tails is 0.77, okay?
So it seems reasonable, right?
Um, so what about this?
So you flip a coin once and you get heads,
what's the probability of heads?
So the maximum likelihood says 1 and 0.
So, you know, some of you are probably thinking, you know,
smiling and you're probably thinking,
"This is a very, uh,
closed-minded thing to do, right?"
Just because you saw one heads, it's like, "Oh, okay.
The probability of heads must be 1.
Tails is impossible because I never saw it."
So it seems pretty, um,
you know, foolish, um,
and intuitively you feel like, "Well,
tails might happen, you know, sometimes.
Um, so it shouldn't be as stark as 1, 0."
Okay. And this is an example of overfitting,
which we talked about in the machine learning lecture,
where maximum likelihood will tend to just maximize the probability,
and in here it does maximize the probability
because the probability of data is now 1 and you can't do better than that.
But this is definitely overfitting.
So we want, uh,
a more reasonable estimate.
So this is where Laplace smoothing comes in,
and again I'm gonna introduce Laplace smoothing, uh,
from kind of a follow your nose kind of framework and then,
um, I'll talk about why it might be a good idea.
Okay. So here's maximum likelihood, um,
just the number of times heads occurs over the total number of trials you have.
So Laplace smoothing is, um,
just adding some numbers.
Um, so, uh, La- this is Laplace named after the famous French mathematician who,
um, did a lot more than add numbers, um,
like the Laplace transform and Laplace distribution.
But we're only going to use or talk about his, um,
adding numbers, um, invention I guess.
Um, so, so here in red,
I'm shown that for this probab- estimate,
no matter what the data is,
I'm just gonna add a 1 here.
I don't care what the data looks.
I'm just gonna add a 1 and we're gonna divide by the total number of values,
uh, which are possible, which is 2, heads or tails.
And for tails, I'm also gonna add a 1,
I don't care what the data says, um,
and I'm gonna divide by 2, okay?
So now I get two-thirds and one-third,
which should be a more,
you know, sensib- intuitively sensible
estimate if you're gonna come up with any sort of estimate.
It says, "Well, I saw heads,
so probably more than 50% is gonna be heads.
But, um, it's probably not, you know, 100%."
Um, okay.
So let's look at it in a slightly,
uh, more complicated setting.
So here I have two variables and, um,
Laplace smoothing is driven by a parameter Lambda, um,
which by default is going to be 1,
but it can be any number,
um, uh, non-negative number.
Um, and what Laplace smoothing amounts to is
saying start out with your tables and instead of filling them up with 0,
fill them in with Lambda, okay?
So here's my table. Before I even look at the data,
I'm just gonna put 1s down.
And then when I look at my data,
I'm just going to add to that counter.
So d shows up twice,
c shows up once,
and then count and normalize,
okay? Same over here.
Before I look at any data,
I'm just gonna populate with ones, which is Lambda,
and then I get my three data points and I add the three counters,
uh, which are shown here,
and I count and normalize.
So by construction, no,
uh, events should have probability of 0 unless Lambda is 0.
Because I already start with 1,
and 1 divided by some positive number is not 0, okay?
So the general idea is, uh,
for every distribution and partial assignment, um,
I'm going to add Lambda to that count, um, you know,
and then I'm just going to normalize,
uh, to get the probability estimates, okay?
So an interpretation of Laplace smoothing is you're essentially, you know, um,
pretending that you saw Lambda occurrences of
every local assignment even though you didn't see in your data.
You're hallucinating this, this data,
sometimes it's called pseudocounts or virtual counts.
Um, and the more, uh,
higher Lambda is, the more hallucination or more smoothing you're doing,
and that will draw the probabilities closer to the uniform, you know,
distribution and the other extreme Lambda equals 0 is simply maximum likelihood.
But in the end, you know,
data, uh, wins out.
So if you had, um,
Laplace smoothing and you saw one head and,
uh, now you're saying estimated the probability of two-thirds.
But suppose you keep on flipping this coin and it keeps on coming up heads 998 times,
then by doing the same Laplace smoothing,
you're going to eventually update your probability to 0.999.
You're never gonna reach 1 exactly because no probabilities are going to be ever 0 or 1.
But increasingly, you're gonna be much
more confident that this is a very, very rich coin.
Okay. Any questions about Laplace smoothing?
[NOISE]
So Laplace smoothing is [NOISE] just,
um, add Lambda to everything essentially.
And oh, so, so I forgot the principle behind Laplace
smoothing is if you think in terms of,
um, this is, kind of,
beyond the scope of this course but you can think about
a prior distribution over your parameters,
um, which is, um, some uniform distribution.
And, um, instead of doing maximum likelihood,
you're doing, um, map or a mac- maximum a posteriori estimation.
So there is another principle but you don't have to worry about it for this class. Yeah?
[inaudible] make Lambda?
The question is how big you make Lambda?
Um, it's a good question so one is- Lambda should be small.
It probably shouldn't be like 100.
Um, probably should be more like 1 or even- you can make it less than 1,
maybe it should be like 0.01 or something.
It depends on, um, you know,
how, I guess, uncertain, you know, you are.
So in general, that these priors are meant to capture what you know about the,
um, the, kind of, the distribution at hand.
Okay. All right.
So now we get to the fun part, this is going to- in some sense,
combined learning which we'd done with probabilistic inference,
uh, which we did before.
And the motivation here is that,
what happens if you don't observe some of the variables?
So far, learning has assumed that we
observe complete assignments to all the random variables,
but in practice, this is just not true, right?
The whole reason people call them Hidden Markov models is that
you don't actually observe the, the hidden states.
So what happens if we only get the observations?
Or the simpler example,
what happens if the data looks like this where we
observe the ratings but we don't observe the genres?
So what can you do?
So obviously, the counter normalized thing doesn't work
anymore because you can't count with question marks.
So, um, there is,
again, two ways to think about what to do here.
The high-minded principle is appeal to
maximum likelihood and make it work for, uh, unobserved variables.
Um, and the other way to think about it which I'll come to later
is simply guess what the latent variables are and then do counter normalize.
Okay? And those- these two are gonna be equivalent.
So let's be high-minded for now and think about,
um, what maximum marginal likelihood.
Okay? So in general,
we're going to think about H as the hidden variables and E as observed variables.
In this case, G is hidden,
and R1 and R2 are observed.
And suppose I see some evidence E,
so I see R_1 equals 1 and R_2 equals 2 but I don't see the value of G. And again,
the parameters are all the same, the same as before.
I just have less data or information to estimate the parameters.
Okay. So if you're following maximum likelihood,
what does maximum likelihood say?
It says tweak the parameters so that the likelihood of your data is as large as possible.
So what is the likelihood of the data here?
It's simply instead of H and E,
I have probability of E. Okay?
So this seems like a, kind of,
a very natural sound extension of maximum likelihood,
um, and it's called, um, maximum marginal likelihood.
[NOISE] Um, because, um,
this [NOISE] quantity is a marginal likelihood, right?
It's not a joint likelihood or a joint distribution, it's
a marginal distribution over only a subset of the variables.
Okay. So now, to unpack this a bit,
um, [NOISE] what is this marginal distribution?
It's actually, uh, by the axioms of probability,
it's the summation over
all possible values of the hidden variables of the joint distribution.
So in other words, you can think about maximum marginal likelihood as saying I
wanna change the parameters in such a way that such that, um,
the probability of, um,
what I see as high as possible,
but what that really requires me to do is think about all the possible values that,
um, H could take on.
I don't see it, so I have to consider what if's,
what if it were C?
What if it were D and so on?
Okay? So in other words, fundamentally,
if you don't observe a variable,
you have to consider possible values on it.
All right. So now, let's, uh,
skip to the other side of the,
kind of, scrappy way and think about what is a reasonable algorithm that makes sense.
And I'm not gonna have time, uh,
in this course to, kind of,
show the formal connection,
but if you take, you know, CS 228 or a graphical models class,
um, you'll go into this in much more detail.
Um, so the intuition here is, um,
the same as what we had for k-means.
So remember in k-means,
you tried to do clustering,
you don't know the,
the centroids, and you also don't know the assignments.
So it's a chicken and egg problem.
So you go back and forth between, uh,
figuring out what the centroids are and figuring out what the, the assignments are.
And the centroids are going to be an analog of parameters here,
and the assignments are going to be the analog of the hidden variables.
Okay? So here's, here's the algorithm.
Um, it's called Expectation-Maximization.
It was, you know, formalized, um,
in its generality in the- in the 70's,
and you start out with some parameters,
um, maybe initializing to uniform or uniform plus a little bit of, uh, noise.
And then it's gonna alter- we're gonna alternate
between two steps: the E-step and the M-step.
Um, and if you- it's useful to think about
the k-means in your head while you're going through this algorithm.
So, um, in the E-step,
we're going to guess what
the hidden variables are or what the values of the hidden variables take on.
So we're going to define this q of h which is going to be,
uh, or represent our guess.
And since we're in probability land,
we're going to consider the distribution over, uh,
possible values of h. And this guess is going to be given
by our current setting of parameters,
and the, the evidence that we've observed in our fixing.
So we're asking the question what is the probability
of the hidden variables taking on particular values of h,
given my data and given my parameters?
So this should- this should look, kind of,
familiar to you, right?
What is this? This is just probabilistic inference, right?
Which we've been doing for,
um, the last lecture.
Which means that, you can use any probabilistic inference algorithm here,
you can do forward backward,
you can do Gibbs sampling,
um, and that's, kind of,
some module that you need to do, um, EM.
Okay? Okay so now what happens if we have our,
uh, setting of- or guess over the possible values H?
Now, we, um, make up data.
Um, so we create weighted points, um,
of- with particular values of H and E, and, um,
each of them gets some weight, uh,
q of h. Um, and then finally,
once we're given these weighted points,
now we're just going to use,
uh, do counter normalize and do maximum likelihood.
[NOISE] Okay?
So I'm gonna walk through an example,
um, to make this a little bit more grounded out.
Um, so I'm gonna do this on a board because it's [NOISE] gonna get a little bit,
um, maybe a little bit hairy.
Um, okay.
Let's, let's do this.
So here, we have a three variable, uh, Bayesian network.
So we have- um,
let's draw it over here.
Actually, I might need a space.
So we have G, we have R_1 and [NOISE] R_2.
Okay? And our data that we see is, ah,
we get [NOISE] data which is,
um, 2, 2 and 1, 2.
Okay? So I observed, um, 2, 2,
that's one data point and,
uh, 1, 2, that's another data point.
Okay? Um, so initially,
what I'm going to do is,
um, start with some setting of parameters.
Okay? So I'm going to start with my parameters Theta,
[NOISE] which specify a distribution over,
uh, g. And for,
[NOISE] um, lack of information [NOISE] I'm going to consider [NOISE] just half and half.
Let me write 0.5 to be consistent.
[NOISE] So I'm initializing it with a uniform distribution,
and my other table here is going to be, um,
g and r. So [NOISE] probability of, um,
[NOISE] uh, r given g. [NOISE] And here,
I have c, [NOISE] uh,
the possible values of g and the possible [NOISE] values of r
which for simplicity I'm gonna assume to just be 1 and 2.
Um, and then for this,
I have- [NOISE] for these numbers, 0.4,
0.6, um, [NOISE] and 0.6, 0.4.
So I'm- [NOISE] I'm not setting them to uniform,
um, but adding a little bit of noise.
So hopefully, people can check that,
um, the numbers on the board are the same as the numbers on the slides.
Okay. So- [NOISE] so that's my initial parameter setting.
So now, in the E step,
[NOISE] I'm gonna use these parameters and to guess what g is.
[NOISE] Okay? So what does this look like?
Um, so [NOISE] for [NOISE] each possible datapoint.
So for every example,
so I have 2, 2, that's one datapoint.
I'm gonna try to guess what the value of g is.
So it could be c, [NOISE] um,
it could be d. And for the other datapoint: 1,
2, [NOISE] it also could be c or it could be d. Okay?
[NOISE] And now, I'm gonna try to,
um, compute a weight for each of these datapoints [NOISE] based on the model.
Okay? So the weight is- not now,
look- look at this. This is cool, right?
Because I have a fu- a complete
assignment and you know what to do with complete assignments.
I can just evaluate the probability of that assignment.
Okay? So now, um,
ju- just to make things concrete,
I have probability of g times [NOISE] probability of r_1 given
g [NOISE] probability of r_2 given g. [NOISE] Um,
this is by definition the probability of g [NOISE],
um, 2 equals, uh [NOISE].
Sorry, that's a little bit messy but-
[NOISE] So this is joint distribution by definition of this Bayesian network, it's this.
Um, okay?
So how do I compute the probability of this configuration?
Well, it's a probability of g equals c. So I look at this table.
That can be a 0.5 [NOISE] times the probability of r_1 given g, that's,
uh, 2 and c. So if we look over here, that's, ah, 0.6.
[NOISE] And then another r_2 given g,
that's also a 2 and a c. So [NOISE] that's,
ah, 0.6 because of parameter sharing.
[NOISE] And that's, ah, 0.18, right?
[NOISE] Um, let me give you guys a slide so you guys can check my work. Um, Okay.
So now, I look at this table.
So probability of g equals d is 0.5.
[NOISE] Um, and then the probability of r_1 equals 2 given g
equals d. Or if we look in this table [NOISE], that's 0.4.
[NOISE] And then I have another 0.4 from the other two,
and that is [NOISE] 0.08.
Um, and then I can normalize this. Question?
[NOISE] [inaudible] versus 0.6 for c2?
So why is this 0.4 versus, uh, 0.6 here?
Um, this is because I initialize the parameters so
that the probability of r equals 2 givens g equals d is 0.4.
So I- I guess I'm wondering why did you use that initialization?
Ah, why did I use this initialization?
Um, just for fun. I just made it up.
[LAUGHTER] Just so that if I had put 0.5's here,
then you couldn't tell what was going on.
And also, um, as a side mark it wouldn't,
um, you know, work, ah, yeah. [NOISE].
So initialization is always random?
Initialization is generally going to be
random but as close to uniform as, as possible. Yeah.
Thank you.
Okay. So this is- this column is going to be the probability of, um,
basically G equals [NOISE] g,
uh, r- R_1 equals R_1,
R_2 equals R_2 [NOISE].
Okay? [NOISE] And then the q, um,
is going to just be [NOISE],
um, is just going to be the weight of this.
And normalizing these two,
which means that you add them up and you divide,
[NOISE] and then you get, uh,
was it 0.69 and 0.31?
[NOISE] Sorry, the numbers are a little bit kind of,
um, awkward but that's what you get there.
Ah, I'm not gonna do the second row but it's,
it's basically the same type of calculation. Question?
[inaudible] kind of like particle filtering in that like in [NOISE] each step,
you do like the proposal and then then waiting,
and then the end step is like you're res- resampling, like finding the maximum?
Um, so the question [NOISE] is,
is you use, um,
Expectation-Maximization is [NOISE] kind of like
particle filtering because you have this proposal and you're reweighting.
Um, structurally, it's quite different.
I mean, there is a sense like there's some,
um, you're proposing different options.
But certainly, the two algorithms are meant to solve very different tasks.
One is for learning and one is for, um, probabilistic inference.
Yeah. Okay. So let's look at the M-step now.
Okay. So the M-step.
Uh, I'll let you guys fill this in.
So the M-step- [NOISE] um,
I actually wanna keep this up.
[NOISE] Let me do the M-step over here.
[NOISE] Is going to- now,
just take, ah, these examples.
[NOISE] So these weights are 0.5.
So it's, it's like someone handed you fully labeled data.
Complete observations, four points but each of observation has a weight.
So now, the only difference when counter normalized.
Instead of just adding 1 whenever you see a particular configuration,
you are gonna add the weight.
Okay. So [NOISE] for each of the parameters,
I'm going to look at,
um, so this is gonna be [NOISE], uh,
g probability of g. [NOISE] So this can be either c or d. Um,
[NOISE] and to get this I have- let me first do the count I guess.
Well, okay, let me just add it here.
So I look at my data.
[NOISE] So let me mark this.
So this is now my kind of, um,
I'm gonna put data in quotes because I didn't actually observe it,
I just hallucinated It.
And these are the weights [NOISE] which are associated with the data points.
So now, I'm going to look at g- uh,
look at g equals c. So I see c here,
c here, and I have,
um, the weights 0.69,
um, [NOISE] 0.69 plus
0.5 probability of- sorry, I just count.
[NOISE] And then for d. I see a d here and I see a d here.
That's 0.31 [NOISE] plus 0.5.
[NOISE] And then I normalize this and I get my estimate.
Okay. So on the slide,
this is exactly what I did here.
I count and normalize.
And I'm gonna- not gonna do this table, um, but you,
hopefully get the idea. Yeah.
[inaudible] [NOISE] estimate for all possible assignments of h?
Yeah. So in each step,
you have to estimate for all possible assignments to h. So in this case,
h is only one variable.
Um, in the next example,
h is going to be in a hidden Markov model,
all the variables, and we're gonna see how we can deal with that. Yeah?
Can you explain briefly how we get the values in the second table?
[NOISE] The second table with this one?
[inaudible].
This one?
Yes.
Yeah. Sure. Just, uh,
briefly- so you look at these datapoints,
um, and you see, ah, c1.
Okay. So where does c1 show up?
So g equals c here,
r_1 equals 1 here,
so that's a 0.5 here.
And then what about c2?
Where does c2 show up?
c2 shows up twice here with r1 and r2.
So that's why there's two of, ah, 0.69's here.
And, uh, c2 shows up once here with a weight of 0.5.
So everything in the [inaudible].
Yeah. Everything here- these counts are based on the table from,
ah, the a step there. [NOISE].
Okay? All right.
So let's do something a little bit fun now.
[NOISE] Um, so the Copiale cipher is
this 105 page encrypted volume that
was discovered and people date it back from the 1730s.
So it looks like this.
So it's unreadable because it's actually a cipher,
it's not meant to be just read.
Um, and for decades,
people were trying to figure out, you know,
what is the actual, uh,
message that was inside this text. It's a lot of text.
Um, and, and finally in 2011, um, some researchers,
um, actually cracked this code [NOISE] and they actually used,
um, EM to help do this.
So I'm gonna give you a kind of a toy version of using EM to do decipherment.
And it turns out that this code is, um, uh,
basically some book from a secret society,
which you can go read about on Wikipedia if you want.
Um, so substitution ciphers.
A substitution cipher consists of a substitution table which specifies for every letter,
assume we have only 26 right now, um,
a cipher letter, okay?
And the way you apply a substitution table is you take
a plaintext which generally is unknown,
if you're trying to decipher like, you know,
let's say hello world and you look up each letter
in the substitution table and you write down the corresponding thing.
So h maps to n. So you write n,
e maps to m,
so you write down m and so on.
And so someone did this and then they obviously didn't give you the plaintext.
They give you the ciphertext.
And so now all you have is a ciphertext.
And obviously, he didn't give you the substitution table either.
So you- all you have is the ciphertext and you're trying to figure out both the cipher,
a, a substitution table,
and also the plaintext, okay?
So hopefully this, uh, pattern matches something.
And let's try to model this as a Hidden Markov Model, okay?
So here, the hidden variables are going to be the characters in the plaintext.
This is the actual sequence that the hello world.
And then observations are the characters of the ciphertext.
So familiar equation, the joint distribution
of the Hidden Markov Model is given by this equation,
and we want to estimate the parameters which include p_start, p_trans, and p_emit.
Okay. So how do we go about, um, doing this?
So we're gonna approach this in a, um,
in a slightly different way,
I guess, than, uh,
simply just running the algorithm because we have,
um, additional structure here, right?
So the probability of start,
I've no idea, So we just set it to uniform.
The, the probability of transition.
So this is interesting.
So normally if you're doing EM,
you don't know the probability of transition.
But because we know- let's say we know that,
uh, the underlying text was English,
we can actually estimate the probability of a wo- of
a character given a previous English character from just English text lying around.
So this is cool because it allows us to hopefully simplify the problem.
Um, I, I should maybe comment that in general,
unsupervised learning is really, really hard.
And just because you can write down a bunch of equations doesn't mean that will work.
And, and, and it's very hard to get it to work.
So you want to get as much, um,
kind of supervision or,
uh, information as you can.
Okay. So finally, e- p_emit
is the substitution table which we're gonna derive from EM, okay?
So now the, the- this might not type check in your head because a substitution
specifies for every letter, one other letter,
the cipher letter, but in order to fit into this framework,
we're going to think about,
uh, a distribution over possible cipher,
uh, letters given a plaintext letter, okay?
With the intention that well,
you know, if it's doing its job, it can always, uh,
put a probability 1 on the actual cipher letter, okay?
So just, you know,
stepping back away from the formulas, um,
why might you think this could work?
And in general, this is not obvious that it should work.
But, but what, what you have here is, um, a,
a language model p_transition,
which tells you kind of what plain text looks like, right?
If you gen- generated a cipher and you say, ah,
this is my cipher and you get some garbage,
then it's probably not right.
So we have some information about what we're looking for.
Like, so like when you solve a puzzle,
you know when you kind of solved it.
Um, and then finally we also have this emission that tells us that each,
uh, letter that E has to go to kind of be substituted in the same way.
So you can't have E going to, you know,
different- completely different things at different, uh, points at a time.
Okay. So, um, so for doing estimation in HMM,
we're going to use the EM algorithm.
And remember, in the E-step,
we're just doing probabilistic inference.
And we saw that forward backward gives us probabilistic inference.
In particular, it gives you- for every, um, position,
I'm going to give you, uh,
my guess which is a distribution of,
uh, possible hidden variables.
So remember at each position,
I observed a cipher letter,
and I'm going to guess a distribution over the plain text letter, okay?
And then the M-step,
I'm just going to count and normalize as we did before.
So once I've guessed the cipher letters,
I can just, um,
compute the probability of some cipher letter given a plain text letter, okay?
So I'm actually going to code this up, um,
and see, um, it in- so you can see it in action.
Okay. I only have five minutes here.
So- to make this quick.
Um, okay. So here we have ciphertext, okay?
Looks pretty good. Um, we're gonna decrypt this or, or decipher it.
[NOISE] Um, and then we also have our, uh,
text, uh, which is just, uh,
some English text that we, uh, found.
Um, and then I'm going to- whoops,
I wanna decipher and not encipher.
Okay. Um, so there's some utilities which are gonna be useful.
So things for reading text,
converting them to things- into integers, um,
normalization of, uh, weight,
and then most importantly the s- implementation of
a forward-backward algorithm which we're gonna use.
Um, [NOISE] okay, because I'm not gonna try to do that in five minutes.
[NOISE] Um, okay.
So let's initialize the HMM.
And then later, we're gonna run EM on it, okay?
So the HMM parameters are- there's gonna be start,
um, probability which is p in our notation, probability of start,
and this is going to be one over the number of possible,
uh, letters here, [NOISE] Um,
for this is just of- a uniform distribution,
uh, 1 over k. And I should say k is 26 plus 1.
So this is, uh, lowercase letters plus space.
[NOISE] Okay.
So now we have our, um, transition probabilities.
Um, and I'm going to- this is gonna define a distribution over, uh,
um, hidden variable, [NOISE] uh,
our next hidden variable given previous hidden variable.
And notice that the order is reversed here because I wanna first condition on each one.
And then this the- thing is gonna be a distribution.
[NOISE] Um, so to do this,
I'm going to first, um, remember my strategies.
I'm gonna use the language model data to, to count.
So I'm going to have,
um, again, set things to, uh,
0 for h_2 and range k for h_1 and range k. So this basically gives me a,
a matrix of k by k matrix of all 0s.
[NOISE] Um, and now I'm going to get my raw text, um,
and I'm gonna read it from,
um, lm.train which, remember, looks like this.
Um, and I'm gonna convert this into a sequence of integers where,
um, they're gonna be 0 to k minus 1.
Um, and then I'm going to- how do I estimate the transition?
Uh, again, counter normalize.
So I'm going to go through this sequence and, um,
[NOISE] and I'm going to count every successive transition from one character to another.
And I'm going to,
um, so I'm going to- at position I,
I have a character which is rawText of i at position i plus 1,
I have another character h_2 [NOISE] and I'm just going to increment this count.
[NOISE]
Okay? And finally I'm gonna normalize this distribution.
So, um, transitionProbs equals,
um, so for every,
um, h1, um,
I have- I have a transition counts of h1,
and I can call
the helpful normalize function which takes this distribution and normalizes it.
Okay? So this is just doing fully observed
maximum likelihood count normalized on just the,
um, the plain text.
Okay? [NOISE] Um, all right.
So now emissionProbs, um,
this is going to be probability of emits of e given h,
and I'm going to just initialize these to uniform because I don't know any better.
Um, so 1 over K
for e in range K for range K. So it just so happens that both the,
um, hidden variables and observed variables have the same domain,
um, that's not generally the case.
Okay? So now, I'm ready to run EM.
So, um, gonna do 200 iterations just- just put in a number.
Um, so have the E-step and the M-step.
So in E-step, remember I'm gonna do probabilistic inference.
I'm gonna call forward backward.
And remember this is, um,
this is going to be basically in
our notation at the position I- what is my distribution over hidden states?
So this is actually forward backward,
um, oh, I need a, um,
read some observations and read my ciphertext, uh,
ciphertext, and I'm going to convert this again into an integer array.
Um, and then I have observations,
and then I pass in the parameters of my,
um, hidden Markov model,
um, and then, I have my guess.
So let me print out what that guess is.
Okay? So what I'm gonna do here is for every position,
I'm going to get the most, uh,
likely outcome of h. So util of argmax of q i,
um, for i in range,
um, length of observations.
Okay? So that gives me an array of guesses for each position, um,
and then I'm gonna convert that into a string so it's easier
to look at, and put them in line.
[NOISE] So this is printing on the best guess.
Okay. So finally for the M-step, um,
I'm go- I have my,
uh, q which gives me counts.
So now I pretend I have a lot of,
you know, data which are weighted by q.
So I'm gonna have emission,
um, counts equals, uh, same as before.
I'm just gonna get a matrix of 0s, um,
e in range K for h in range K,
um, and then I'm gonna go through the sequence,
from i equals range of through the length of the observation sequence,
um, and for each, uh,
position in the sequence,
I'm gonna consider all the possible,
um, possible plaintext hidden values,
[NOISE] and I'm going to increment, um,
h, observations of i,
so this is the observation that I actually saw.
Um, and this should be qih which is the weight of,
uh, eight h at position i.
Okay? And then finally I'm just going to normalize.
So, um, like what I did,
uh, well, okay I'll just normalize it here.
So emission probabilities is util dot, um,
normalize of the accounts, um,
for h in range K. Okay?
And I think that's, uh, pretty much it.
Um, lemme me see if this- okay.
Um, this should be [NOISE]
for for e but- right?
Okay. Okay. Good. Um, okay.
So let's ru- run this and then I'll go back to
the- lemme just go over that code just one more time.
Okay. So, [LAUGHTER] uh,
we initialize the, the probabilities, to uniform.
For transition probabilities we can use our observations,
uh, or the- just a plaintext.
We just count and normalize.
In emissions, um, we initialize with uniform,
and then while running EM,
we read the ciphertext,
and then in the E-step,
we're going to use the current prob- parameters to guess where the ciphertext is.
And then we're going to update our estimate of what
the parameters are given our guess of what the ciphertext is.
Okay. So here's the final moment of truth.
Uh, the ciphertext- and so
remember each iteration it's going to print out the best guess.
So hope, it'll look like gibberish for a little bit.
It's not gonna be perfect, um,
but this is- it starts to look somewhat like English.
Um, there's an and, and in my in there,
um, anyone can read this?
[BACKGROUND] Alone without- okay,
that looks like English.
Can be this like anyone that I could.
Anyone to guess what this text is?
So here's the plain text.
So I've lived my life alone without anyone that I could
really talk to until I had an accident with my plane and the,
um, does with my- whatever something.
[LAUGHTER] But so, again,
unsupervised learning doesn't- it's not magic.
It doesn't always work, but here you at least
see some signal, and in the actual application,
they got partway there and then,
you can iterate on this man- kind of in a manual way.
[BACKGROUND] Okay.
[BACKGROUND] Oops.
All right. So that's for it for Bayesian networks.
On Wednesday we'll do [NOISE] logic.
 All right. So let's get started.
[NOISE] So today's lecture is going to be on logic.
Um, to motivate things,
I wanna start with, uh, hopefully an easy question.
So if X_1 plus X_2 is 10,
and X_1 minus X_2 is 4.
What is X_1?
Someone shout out the answer once you figure it out.
7. So how did you come to get 7?
Yeah. You do the algebra thing that you learned a while ago, right?
[NOISE] Um, so what's the point of this?
Um, so notice that this is, uh,
a factor graph where we have two variables,
they're connected by two constraints or factors.
And you could in principle go and use backtracking search to try
different values of X_1 and X_2 until you eventually arrive at the right answer.
But clearly, this is not really an efficient way to do it.
And somehow in this problem,
there's extra structure that we can leverage
to arrive at the answer in a much, much easier way.
And this is kind of the- going to be the poster child of what we're
gonna explore today and on next Monday's lecture,
how you can do logical inference to
arrive at answers much faster than you might have otherwise.
[NOISE] So we've arrived at the end of the- of the class, um,
and I wanna just, uh,
[NOISE] reflect a little bit on what we've learned,
and maybe this will be also a good review for the exam.
Um, so in this class,
we've boast- bolstered everything on the modeling,
um, inference learning paradigm.
And the picture you should have in your head is this.
Abstractly, we take some data,
we perform some learning on it,
and we produce a model.
And using that model, we can produce a perform inference
which looks like taking in a question and returning an answer.
So what does this look like for
all the different types of instantiations we've looked at?
So for search problems,
the model is a, is,
is a search problem and the inference asked the question;
what is the minimum cost path?
Um, in MDPs and games,
we asked the question what is the maximum value policy?
In CSPs, we asked the question [NOISE] what is the maximum weight assignment?
And in Bayesian networks,
we can answer probabilistic inference queries of the form,
what is the probability of some query variables conditioned on some evidence variables?
And for each of these case,
we looked at the modeling,
we look at the, the inference algorithms,
and then we looked at different types of, uh,
learning procedures going backwards, maximum likelihood.
Uh, we looked at,
uh, various reinforcement learning algorithms.
We looked at structured perceptron and so on.
And hopefully, this, this kind of sums up, um,
the kind of the worldview that CS221 is trying to impart is
that there are these different co- you know, components.
Um, and depending on what kind of modeling you choose,
you have different types of algorithms,
um, and learning algorithms.
Inference algorithms and learning algorithms that emerge.
Okay? So we looked at several modeling paradigms,
um, roughly broken into three categories.
The first is state based models,
search problems, MDPs, and games.
And here, um, the,
the way you think about modeling is in terms of states,
and as nodes in a graph and actions that take you between different states, um,
which incur either a cost or give you some sort of reward,
and your goal is just to find paths,
or contingent paths, or policies,
um, in these graphs.
Then we shifted gears to talk about variable based models.
Where instead we think about variables, um,
and factors that constrain or,
um, these variables to,
uh, take on certain types of values.
Um, so in today's lecture,
I'm gonna talk about logical based models.
So we're gonna look at propositional logic and first-order logic which are
two different types of logical languages or, um, models.
And, um, we're gonna instead think about logical formulas and
inference rules which is going to be another way of kind
of thinking about, uh, modeling the world.
Historically, logic was actually the dominant paradigm in AI,
um, before the 1990s.
So it might be hard to kind of believe now.
But just imagine the amount of excitement that is going into deep learning today.
This equal amount, uh,
of excitement was going into logical based methods in,
in AI in, uh,
um, in the '80s and before that too.
Um, but there was kind of two problems with logic.
One is that, um,
logic was deterministic so it didn't handle uncertainty very well,
and [NOISE] that's why
probabilistic inference and other methods were developed to address this.
And it was also rule-based which allow- didn't allow you
to naturally ingest a large amount of data to,
you know, uh, guide behavior,
and the emergence of machine learning has addressed this.
Um, but one strength that kind of has been left on the table is the expressiveness.
And I kind of emphasize that logic as you will see gives you,
um, the ability to express very complicated things in a very,
you know, succinct way.
And that is kind of the main point of, uh,
logic which I really want everyone to, um,
kind of appreciate, and hopefully this will become clear through examples.
Um, as I motivated on the first day of class,
the reason- uh, one,
one good way to think about why we might want logic is, imagine,
you wanna lie on a beach, um,
and [NOISE] you want your assistant to be able to do things for you but, um,
hopefully it's more like,
um, Data from Star Trek rather than Siri.
Um, you wanna take an assistant and you want to be able to at
least tell information and ask your questions,
and have, um, these questions actually be answered in response- eh,
to, to reflect the information that you've, uh, [NOISE] told them.
Um, so just kind of a brief refresher on the first day of class.
I showed you this demo where you can talk to the system,
[NOISE] uh, um, and say things and ask a question.
So a small example is, um,
let's say all students like CS221.
It's great and teaches them important things.
Um, and, uh, Alice does not like CS221.
And then you can ask,
um, you know, ''Is Alice a student?''
And the answer should be,
''No'', because it can kind of reason about this one.
[NOISE] Um, and just to,
uh, dive under the hood a little bit, um,
inside, it has some sort of knowledge-based stack contains the information at,
um, it has, we'll come back to this in a second.
Okay? Um, so this, this, uh,
system needs to be able to digest
heterogeneous information in the form of natural language,
you know, uh, utterances,
and it has to reason deeply with that information.
So it can't just do, you know,
superficial pattern matching.
So I've kind of suggested natural language as an interface to this.
Um, and natural language is very powerful because, um,
I can send up here and use natural language to give
a lecture and hopefully you guys can understand at least some of it.
Um, and- but, you know,
let's, let's go with natural language for now.
So here- here's an example of how you can draw inferences using natural language.
Okay? So a dime is better than a nickel.
Um, a nickel is better than a penny.
So therefore, a dime is better than a penny, okay?
So this seems like pretty sound reasoning.
Um, so what about this example,
a penny is better than nothing,
um, nothing is better than world peace.
[inaudible].
Therefore, a penny is better than world peace, right?
Okay. So something clearly went, uh, wrong here.
And this is because,
languages- natural language is kind of slippery.
It's not very precise,
which makes it very easy to kind of make these, um, these mistakes.
Um, but if we step back and think about what is the role of natural language,
it's really- language itself is an ex- mechanism for expression.
So there are many types of languages.
There's natural languages, um,
there's programming languages, which all of you are, you know, familiar with.
Um, but we're going to talk about,
a different type of language called logical languages.
Um, like programming languages,
so they're gonna be formal.
So we're gonna be absolutely clear what we mean by
when we have a statement in a logical language.
Um, but, and like natural language,
it's going to be, um, declarative.
Um, and this is maybe a little bit harder to appreciate right now,
but it means that,
uh, there's kind of a more of a,
um, one to one isomorphism between logical languages, and natural languages,
as they're compared to, um,
programming languages and natural language.
Okay. Um, so in a logical language,
we want to have two, uh, properties.
First, a logical language should be, uh,
rich enough to represent knowledge about the world.
Um, and secondly, it's not
sufficient just to represent the knowledge, because you know, uh,
a hard drive can represent the knowledge,
but you have to be able to use that knowledge in a- in a way to reason with it.
Um, a logic contains three, uh,
ingredients, um, which I'll,
um, go through in a- in subsequent slides.
There is a syntax, which defines, um,
what kind of expressions are valid or grammatical in this language.
Um, there's semantics which is for each,
um, expression or formula.
What does it mean?
And mean means is- is actually,
means something very precise which I'll,
you know, come back to.
And then inference rules allow you to take various f- uh,
formulas and, um, do kind of operations on them.
Just like in the beginning,
we- when we have the, um,
algebra pro- problem, you can add equations,
you can move things to different sides.
Um, you can perform with these rules,
which are syntactic manipulations on these formulas or expressions.
That preserves some sort of, um, semantics.
Okay, so just to,
uh, talk about syntax versus semantics a little bit,
because, I think this might be a s- slightly subtle point which,
um, hopefully will be clear with this example.
So syntax refers to,
what are the valid expressions in this language,
um, and semantics is about what these ex- expressions mean.
So here is an example of two expressions,
which have different syntax.
2 plus 3 is not the same thing as 3 plus 2,
but they have the same semantics.
Both of them mean, the number, you know, 5.
Um, here's a case where,
we have two expressions with the same syntax,
3 divided by 2,
but they have different semantics,
betw- depending on which language you're in.
Okay? So in order to define a language precisely,
you not only have to specify the syntax,
but also the, um, semantics.
Because just by looking at the syntax you don't actually know what its, um, meaning is.
Unless I tell you. Um, there's a bunch of different logics.
Um, the ones highlighted in bold are
the ones I'm gonna actually talk about in this class.
So today's lecture is going to be,um, on propositional logic.
Um, and then, uh,
in the next lecture, I'm gonna look at first-order logic.
Um, as with most models in general,
there's going to be a trade off between,
the expressivity and the computational efficiency.
So as I go down this list,
to first-order logic and beyond, um,
I'm going to be able to express more and more things, using the language.
But it's gonna be harder to do, uh,
computation in that language.
Okay. So this is the- the kind of a key diagram, um,
to have in your head,
while I go through syntax semantics and inference rules.
So for every- I'm gonna do this for propositional logic,
and then in, uh,
Monday's lecture I'm gonna do it for, uh, first-order logic.
Um, so just to get them on the board, um,
we have syntax, and we have,
um, semantics, and then we have inference rules.
Um, let's just write it here.
Um, so this lecture is going to have a lot of definitions and concepts, um, in them.
Just to giv- give you a warning.
There's a lot of kind of ideas here.
Um, they're all very kind of, uh, um,
simple by themselves and they kinda piece together,
but there's just gonna kind- kinda be a barrage of, uh,
terms and I'll try to write them on the board,
so that you can kind of remember them.
Um, so in order to find, uh,
a logic called language, um,
I need to specify what are the formulas.
Um, um, so one maybe other comment about logic is that,
some of you have probably taken, um,
CS 103 or an equivalent class where you have been exposed to propositional logic.
Um, what I'm gonna do here,
is kind of a much more methodological and- and rigorous treatment of it.
Um, the- I wanted to distinguish the difference between,
um, being able to do logic yourself.
Like if I give you some, uh,
logical expression you can manipulate it.
That's different than, um,
talking about a general set of algorithms,
that can operate on logic itself.
Right. So remember in AI,
we're not interested in you guys doing logic,
because that's just I. That's intelligence.
Um, [LAUGHTER] but we're interested in developing general principles,
or general algorithms that can actually do the wo- work, uh, for you.
Okay? Just like in, uh,
in the Bayesian networks, it's very fine and
well that you can- you guys can, uh,
manipulate and condition, uh,
calculate conditional and marginal probabilities yourself.
But the whole point is we devise algorithms like Gibbs sampling,
and variable eliminish- elimination that can work on any, uh, Bayesian network.
Just wanna get that out there.
Okay. So let's, uh, begin.
Um, this is gonna be building from the ground up.
So first of all, there are in propositional logic,
there are a set of propositional symbols.
These are typically going to be uppercase letters or even, um,
words, Um, and these are the atomic formulas.
Um, these are formulas that can't be any smaller.
There is going to be logical connectives such as, uh,
not and or, um,
implication and bidirectional implication.
And then, the set of formulas are built up recursively.
So if F and G are formulas,
then I can- these are also formulas.
I can have not F. I can have F and G, F or G,
F implies G and F, um,
bidirectional implication G or equivalent to G. Okay?
So key, um,you know ideas here are, we have, um,
propositional symbols, um, we're gonna mo- move this down, because we're gonna run out of space.
Um, so this- these are things like A,
there's- that gives rise to formulas in general,
um, which is gonna be denoted, um,
F. Um, and so here are some examples.
So A is a formula.
Okay? It's in particular, it's an atomic formula which is a propositional symbol,
not A is a formula,
not B implies C as a formula.
This is a formula.
Um, this is the formula.
Double negation is fine.
This is not a formula,
because there's no connective between A and not B.
Um, this is also not a formula,
because what the heck is plus?
It's not a connective. So- so I think in- in thinking about logic,
you really have to divorce yourself from the common sense,
that you all come with in interpreting these symbols.
Right? Not is just a symbol or is just a symbol,
and they don't have any semantics.
In- in fact, I can go and define some semantics,
which would be completely different from what you would imagine.
It would be a lo- valid logical, um, system.
These are just symbols,
and all I'm here at defining is what symbols are valid
and what symbols are not valid, slash grammatical.
Okay? Any questions about the syntax of propositional logic?
So the syntax gives you the set of formulas or basically statements you can make.
So you can- think about as- as this is our language.
If we could only speak in propositional logic, I could say,
A or not B or the or,
um, A implies C. And that's all I would be able to say.
Um, and of course now I have to tell you what do these things mean.
Okay? And this is the realm of semantics.
So semantics, there's gonna be a number of definitions.
So first, is a model.
So this is really unfortunate and confusing terminology.
But this is standard in the logical literature. So I'm just gonna use it.
So a model, which is different from our general notion of a model.
Um, for example, um,
Hidden Markov Model for example.
It- a model here, in propositional logic,
just refers to an assignment of,
um, truth values to propositional symbols.
Okay? So if you have three propositional symbols,
that they are 8 possible models.
Um, A is, uh, 1,
B is 0, C is 0 for example.
So these are just complete assignments that we saw from factor graphs.
But now, um, in this new kind of language.
Okay. So that's the first constant.
And in first-order logic models are going to be more complicated.
Um, but for now,
you think about them as a complete sentence.
And I'm using W, because sometimes you also call them, um, worlds.
Um, because a complete assignments slash a model,
is both to represent the state of the world at any one particular point in time. Yeah.
[inaudible]
Yeah. So the question is,
can each propositional symbol either be true or false?
And in logic, as I'm presenting it, yes.
Only true or false or 0 or 1.
Okay? So these are models.
Um, and next is a key thing that actually defines the semantics,
which is the interpretation function.
So the interpretation function, um,
takes a formula and a model and returns true if that formula is,
uh, is true in this model and false,
um, you know, otherwise.
Okay? So I can make the interpretation function whatever I want,
um, and that just gives me the semantics.
So when I talk about what are the semantics,
it's the interpretation function.
Function i of f,
w. [NOISE] So the way to think about this is,
um, I'm gonna represent formulas as these, uh, horizontal bars.
Okay? So this is- think about this as,
uh, a thing you'd say.
It sits outside though,
uh, reality in so- in some sense.
And then this box, I'm gonna draw on the space of all possible models.
So think about this as a space of situations, uh,
that we could be in the world,
and a point here corresponds to a particular model.
So an interpretation function takes one, a formula, takes,
um, a model and says,
"Is this statement true if the world looks like this?"
Okay? So just to ground this out, um,
a little bit more, um,
I'm gonna define this for propositional logic, again recursively.
Um, so for propositional symbols, um, p,
I'm just gonna interpret that propositional symbol as a lookup in,
uh, the, the model, right?
So if I'm asking, "Hey, is A true?"
Well, I go to my, uh,
model and I see well,
does it say A is true or false.
Okay? That's a base case.
So recursively, I can define
the interpretation of any formula in terms of its sub formulas.
And the way I do this is suppose I have two formulas f and g
and they're interpreted in some way.
Okay. And now, I took a formula let's say f and g. Okay?
So what is the interpretation of f and g, um, in w?
Well, it's given by this truth table.
So if f is 0 and g is, uh,
interpreted to be 0,
then f and g is also interpreted to be 0.
And, um, 0,1 maps to 0,
1, 0 maps to 0 and 1, 1 maps to 1.
So you can verify that this is kind of, um,
your intuitive notion of what and should be, right?
Um, or is, um, 1 if,
um, at least one of f and g are 1,
um, implication is 1.
If f is 0 or g is,
uh, is, is 1.
Um, if bidirectional implication just means that if f and g evaluate to the same thing,
uh, not f is, you know,
clearly just, you know,
negation of whatever the interpretation of f is.
Okay. So this slide gives you the full semantics of propositional logic.
There's nothing more to propositional logic
and at least the definition of what it is, um, aside from this.
Um, let me go through example and then I'll maybe take questions.
So, so let's look at this formula,
not A and B, bidirectional implication C. Um,
in this model A is 1,
B is 1, C is 0.
Um, how do I interpret this formula against this model?
Well, I look at the,
the tree, um, which breaks down the formula.
So if I look at the leaves,
um, let's start bottom-up.
So the interpretation of A against w is just 1.
Because for a proposition symbols,
I just look up what A is and A is 1 here.
Um, the interpretation of not A is 0 because if I look back at this table.
If this evaluates to 1,
then this evaluates to 0.
I'm just looking, um,
based on the table.
Um, B is 1 just by table lookup, and then, um,
not A and B is 0 because I just take these two values and I add them together.
Um, C is 0 by table look up and then,
uh, bidirectional implication, um,
is interpreted as, uh,
1 here because 0 is equal to 0 here.
Yeah. Yeah, question?
Interpretation function is user defined in this case not like learning
how to interpret [inaudible]
Yeah, so the question is,
is the interpretation function user defined?
It is just written down.
Um, this is it,
there's no learning that's just these are- this is what you get.
Um, it's not user-defined in the sense that not everyone's gonna go define their own,
[LAUGHTER] you know, truth tables.
Um, some logicians came up with this,
and that's what it is.
Um, but you could define your own logics,
and it's kind of a fun thing you could try doing.
Okay? Any other questions about interpretation functions and models?
So now, we're kind of connecting syntax and semantics, right?
So an interpretation function binds what our formulas,
which are in syntax land to,
um, a notion of models which are, uh, in semantics.
So a lot of logic is very, um,
it might seem a little bit pedantic but it's just
because we're trying to be very rigorous in a way
that doesn't need to appeal to your common sense
intuitions about of what these formulas mean.
Okay? Any questions? All right.
So, so while we have the interpretation function that defines everything,
it's really going to be useful to think about formulas in a slightly, uh, different way.
So we're gonna think about a formula, um,
as representing, um, the set of all models for which interpretation is, you know, true.
Okay? So M of F,
which is the- is the set of models,
um, that f is,
uh, true on that model.
Okay? So pictorially, this is,
ah, f that you say out loud.
And what you mean by this is, um,
simply this subset of models,
um, which, uh, this f is true.
Okay? So if I make a statement here,
what I'm really saying is that I think we're in
one of these models and not in one of these other models.
So that's a kind of an important,
uh, I think intuition to have,
the meaning of a- a formula is carving
out a space of possible situations that you can be.
And if I say there's a water bottle on the table,
what I'm really saying is that,
I'm ruling out all the possible worlds that we could be in where there is
no water table- bottle on the table. Okay?
So models, um, M of f is gonna be a subset of all the possible models in the world.
Okay? So here's an example.
So if I say it's either raining or wet- rain or wet,
then the set of models can be represented,
um, by this, um,
subset of this two-by-two.
Okay? So over here,
I have rain, over here I have wet.
So this corresponds to no rain but it's wet outside,
[NOISE] um, this corresponds to it's raining but it's not wet outside.
And the set of models f is this red region which are these three possible models.
Okay? So I'm gonna use this kind of pictorial depiction throughout this, uh, lecture.
So hopefully, this makes sense.
So, so one key idea here- remember,
I said that logic allows you to express
very complicated and large things by using very small means.
So here, I have a very small formula that's, um,
able to represent a set of models,
and that set of models could be exponentially large.
And much of the power of logic allow- it comes from the ability to,
um, do stuff like that.
Okay. So, um, here's yet another definition.
This one's not somehow, um,
such a new definition- um, sorry,
a new concept but it's kind of just
trying to give us a little bit more intuition for what these,
um, formulas and, um,
uh, models are doing.
So knowledge base is just a set of formulas.
Um, and, and think about this as the set of facts you know about the world.
So this is what you have your head.
And in general, it's going to be a- just a set of
formulas and now the- the key thing as I need to connect us with semantics.
So I'm gonna define the set of models denoted by a knowledge base
to be the intersection of all of the models denoted by the formulas.
So in this case, if I've rain or snow being this, uh,
green ellipse and traffic being this red ellipse,
then the model is denoted by the knowledge base is just the intersection.
Okay? So you can think about knowledge as how fine-grain,
uh, we've kind of zoomed in on where we are in the world, right?
So initially, if you don't know anything,
we say anything is possible all 2 to the n,
you know, models are possible.
And as you add formulas into your knowledge base,
the set of, um,
possible worlds that you think might exist, uh,
are possible is going to shrink, um,
and, you know, we'll see that in a sec- in a second.
Okay. So here's an example of a knowledge base.
If so have rain that corresponds to this set of models,
um, rain implies wet corresponds to the set of models.
And if I look at the models of this knowledge base,
it's just going to be the intersection which is this,
uh, red square down here.
Okay? Any questions about knowledge bases, models, interpretation functions so far?
All right. So as I've alluded to earlier,
knowledge base is the thing you have in your head.
And as you go through life,
you're going to add more formulas to your knowledge base.
You're gonna learn more things.
Um, so your knowledge base just gets, uh,
union with whatever formula you- you have.
And over time, the set of models is going
to shrink because you're just taking your intersection.
Um, and one question is,
you know, how much is this sh- uh, shrinking?
Okay? So here, there's a bunch of different cases to contemplate.
Um, the first case is, um, entailment.
So suppose this is your knowledge base so far,
and then someone tells you, um,
the formula that corresponds to this, ah, set here.
Okay. So in this case, um, intuitively,
f doesn't add any information or new constraints that was known before.
And in particular, if you take the intersection of these two,
you end up with exactly the same set of
models you had before so you didn't learn anything.
Um, and this is called entailment.
So, um, so there's kind of three notions here,
there's entailment which is written this way with two horizontal, uh, bars,
um, which means that the set of models, um,
of f is at least as large as the set of models in,
uh, KB or in another words, it's a superset.
Okay? Um, so for example, rain and snow.
If you already knew it was raining or snowing and someone tells you, "Ah, it's snowy."
Then you say, "Well, um,
duh, I didn't learn anything."
Um, a second case is contradiction.
[NOISE] So if you believe the world to be
in here or somewhere and someone told you it's actually out here,
then, um, your brain explodes, right?
Um, so this is where the set of models of
the knowledge base on the set of models denoted by the formula is empty.
So this- it doesn't make sense.
Okay? So that's a contradiction.
Um.
Okay, so if you knew it was rainy and snowing
and someone says it's actually not snowing,
then you, you- right,
know that, that can't- that can't be right.
Okay. So the third case is,
um, basically everything else.
It's contingency where, um,
f adds a non-trivial amount of information to a knowledge base.
So the, the new set of models- the intersection here is neither
empty nor is it the original knowledge base. Okay?
One thing to kind of uh- not get confused by is,
if the set of models were actually strictly inside and the knowledge base,
that would also be a contingency.
Right, because when you intersect it,
it's neither empty nor the original.
Okay. So if you knew it was rainy, it's,
um- raining and someone said,
"Oh, it's also snowing too."
Then you're like, "Oh cool, I learned something."
Okay, so there- there's a relationship between contradiction and entailment.
So contradiction says that the knowledge base Mf, um,
are- have zero intersection, and entailment, um,
this is equivalent to, um,
the knowledge base entailing not f. Okay so just,
there's a simple proposition that says,
um, KB contradicts f if KB- if and if KB entails not f. Okay so,
um, there's the picture you should have in
here is like not f is all of the models which are not in this.
And if you think about kind of,
wrapping that around, it kind of looks like this.
Okay, [BACKGROUND] all right.
So with these three,
um, notions: entailment, contradiction,
and contingency which are relationships between a knowledge base and a new formula,
we can now go back to our kind of
virtual assistant example and think about how to implement, um, these operations.
So if I have a knowledge base and I tell, um,
the- the virtual assistant of a particular, um,
formula f, there are
three possible abilities which correspond to different appropriate responses.
So if I say it's raining, then,
if it's in entailment, then I say,
"I already knew that because I didn't learn anything new."
If it's a contradiction, then I should say,
"I don't believe that because it's not consistent with my knowledge so far."
And otherwise, you learn something new.
Okay, um, there's also the ask operation which if you're asking a question,
again, the same three, uh,
entailment, contradiction, and contingency can hold.
Uh, but now, the responses are- should be answers to this question.
So if it's entailment,
then I say yes.
And this is a strong yes and it's not like,
uh, probably yes, this is a definitely yes.
If it's a contradiction,
then I say no.
It's- again, it's a strong no, it's impossible.
And then in the other case it's just contingent which you say I don't know, okay.
So the answer to a yes or no question,
there's three responses, not two, okay.
Okay, any questions about this?
Okay, how many of you are following along just fine?
Okay, good. All right.
So this is a little bit of a digression and it's going to connect to Bayesian networks.
So you might be thinking in your head,
well, we kind of did something like this already, right?
In Bayesian networks, we had these complete assignments
and we actually defined joint distributions over complete assignments.
And- and now, what we're talking about
is not distributions but sets of assignments or models.
And so we can actually think about the relation between a knowledge base and
an f also having an analog in a Bayesian network land given by this formula.
So, um, remember, a knowledge base is a set of models or possible worlds.
So in probabilistic terms, this is an event.
Um, and that event has some probability.
So that's- that's a denominator here.
And when you look at f and KB and you intersect them,
you get some other,
uh, event which is a subset of that and you can ask for the probability mass of that,
uh, intersecting event, that's the numerator here.
And if you divide those,
that actually just gives you the probability of a formula,
uh, given the knowledge base.
Okay, so this is actually a kind of pretty nice and direct, um, um,
probabilistic generalization of propositional logic. Uh, yeah.
It's only once if you had like all the var- all the variables required for that,
for me they're already exists a set of networks like in this scenario, there's A, B, C,
if we were asking something about D,
would still be in I don't know,
because we don't have that information.
Yeah so the question is this only works when restricted to the set of predefined,
uh, propositional symbols and you are asking about D,
then yeah, you would say I don't know.
And it's- in fact, when you define propositional logic,
you have to pre-specify the set of symbols that you are dealing with,
um, in general, yeah.
[inaudible] given the example that we did earlier,
like reading wasn't given a set of examples are things
that our agent knew about before we started like training.
So is that something we'll get to later?
Um, yes so the question is in the- in the- in practice,
you could imagine giving you an agent like it is raining or it's snowing or it's sleeting,
and having novel concepts.
Um, it is true that you can devise,
build systems and the system I'm showing you has that capability.
Um, um, and this is, uh,
it'll be clear how we do that when we talk about
inference rules because that allows you to operate directly on the syntax.
Um, here, I'm talking about semantics where you essentially just, just for convenience.
I mean you can be smarter but- but we're just defining the- the world.
Yeah. Yeah, so in this formula,
why is this union not intersection?
So I'm unioning the KB with a formula,
which is equivalent to intersecting the models of the KB with the models of the formula.
Okay. So this is a number between 0 and 1.
And this reduces actually to,
uh, the logical case if,
if this probability of 0, um,
that means there's a [NOISE] contradiction, right?
Because this intersection is- it's gonna be pros- probability 0.
And if it's 1, that means in its entailment.
Um, and the cool thing is that instead of just saying I don't know,
you're gonna actually give a probabilistic estimate of like,
well, I don't know, but it's probably like 90%.
Um, so, you know, we're not gonna talk,
uh, this is all I'm gonna say about probabilistic extensions to logic,
but there are a bunch of other things, um,
that you can do that kind of marry the, um,
the expressive power of logic with some of
the more advanced capability of handling uncertainty of probabilities. Yeah?
[inaudible].
Assuming that, I mean, the probability distribution?
Yeah. To do this, you are assuming that we actually have the joint distribution at hand.
And a separate problem is of course, learning this.
So for logic, I'm only talking about inference,
I'm not gonna talk about learning although there are
ways to actually in- infer logical expressions too.
Okay. So back from the digression now, no probabilities anymore.
We're just gonna talk about logic.
Um, there's another concept which is,
um, really useful called, uh, satisfiability.
[NOISE] Um, and this is going to allow us to implement,
um, entailment, contradiction, contingency using kind of one, um, primitive.
[NOISE] And the definition is that knowledge base is satisfiable if,
um, the set of models is non-empty, okay?
So it's not self-contradictory in other words.
So now, we can reduce ask and tell to satisfiability, okay?
Um, remember, ask and tell have three possible outcomes.
If I ask a satisfiable question,
how many possible outcomes are there?
Two? So how am I gonna make this work?
I have to probably called satisfiable twice, okay?
So let's start with asking if knowledge-based union,
um, not f is satisfied or not, okay?
So if the answer is,
um, no, what can I conclude?
So remember, the answer is no.
So it's not satisfiable,
which means that KB contradicts not f,
and what is that equivalent to saying?
[inaudible].
Sorry?
Like it's- like it's [inaudible].
Um, yeah so it's not f.
So it's- which one of these should have been entailment, contradiction, or contingency?
[inaudible].
Yeah, so, um, I'm interested in the relationship between KB and f.
[NOISE] I'm asking the question about KB Union not f. Yeah?
[inaudible].
Yeah, yeah. So exactly- so this should be an entailment,
relation between KB and f. Remember,
KB entails f is equivalent to KB contradicting not f, okay?
Um, okay, so what about, um,
if it's yes, then I can ask another question,
is KB Union f satisfiable or not?
So the answer is no,
then what should I say?
[inaudible].
It should be a contradiction because, I mean,
this literally says KB contr- contradicts f. And then finally,
if it's yes, then it's contingency, okay?
So this is a way in which you can def- reduce answering ask and tell,
which is basically about assessing entailment, contradiction,
or contingency to just- uh,
to- at most two satisfiability calls.
So why are we reducing things to satisfiability?
For propositional logics, uh,
checking satisfiability is just the classical SAT problem
and it's actually a special case of solving constraint satisfaction problems.
Um, and the mapping here is,
we just call propositional symbols variables,
um, mo- formulas constraints,
and we get an assignment here and we call that a model.
So in this case,
if we have a knowledge base, um,
like this, then there are three variables; A,
B and C and we define this CSP and then we can, um,
if we find a satisfying assignment,
then, um, then we return satisfiable.
If we can't find one, then we
return unsat, okay?
Um, so this is called model checking.
Um, it's called model checking because we're checking whether a model,
eh, exists or is, uh, um, is true.
So you- in model checking takes
a knowledge base and outputs whether there is a satisfying model.
Um, there are a bunch of algorithms here which are,
you know, very popular, there's something called DPLL named after,
uh, four, four people.
Um, and this is essentially backtracking search plus, uh,
pruning that takes into account the,
the structure of your CSPs,
um, which are propositional logic formulas.
Um, and, uh, there's something called WalkSat which is,
you can think about the closest analog that we've seen as
Gibbs sampling which is a- a randomized local search.
Um, okay?
So at this point,
you really can't have all the ingredients you, uh,
you need to do, um,
inference and propositional logic.
So to define what propositional logic symbols are,
um, or formulas are,
I define the semantics, um,
and I've told you even how to solve,
uh, entailment and, um,
contradiction and contingency queries by reducing
the satisfiability which is actually something we've already,
you know, seen coincidentally,
um, so that should be it, okay?
Um, but now, coming back to the,
you know, original motivation of X_1 plus X_2 equals 10,
and how we were able to perform their logical query much faster,
we can, uh, ask the question now,
can we exploit that, that the factors are,
are formulas rather than arbitrary, you know,
functions, and this is where inference rules is gonna come into play, okay?
So, um, so I'll try to explain this, uh,
figure a little bit since it looks probably pretty
mysterious, um, from the beginning.
So I have a bunch of formulas.
This is my knowledge base over time, I accrue formulas.
And these formulas carve out, um,
a set of models in semantics land.
And, and this formula here,
if it's, um, a superset,
that means it's entailed by these formulas, right?
So I know that this is true given my knowledge,
which means that this is kind of a logical consequence of what I know, okay?
So, so far, what we've talked about is taking
formulas and doing everything over in semantics land.
What I'm gonna talk about now is inference rules that are gonna allow us to
directly operate on the syntax and hopefully get,
um, some results that way.
Okay. So here is an example of making an inference.
Um, so if I say it is raining,
and I tell you if it's raining,
it's wet, um, rain implies wet,
then what should you be able to conclude?
It's wet.
It's wet, right?
So I'm gonna write this inference rule this way with this,
um, kind of fraction looking like thing where there's a set of
premises which is a set of formulas which I know to be true.
And if those things were true,
then I can derive a conclusion which is another formula.
This is, um, an instance of a general rule called Modus ponens, um,
that says for any propositional symbols p and q,
if I have p and p implies q in my knowledge base,
then I can- that entails, uh, q, okay?
So let's talk about inference rules, [NOISE] um, inference.
Actually, let me do it over here,
since we're gonna run out of space otherwise.
[NOISE] Okay.
So Modus ponens is a first thing we're gonna talk about.
[NOISE]
So notice here, that if I could do these type of inferences, it's much less work, right?
Because I- it's very localized.
All I have to do is look at these three formulas.
I don't have to care about all the other formulas or propositional symbols that exist,
and going back to this question over here about- or how
do I- what happens if I have new concepts that occur?
Well, you can just treat everything as if it's just a new symbol.
There's not necessarily a fixed set of symbols
that you're working with at any given time.
Okay, so this is the example of inference rule.
In general, the idea of the inference rule is that you have rules that say,
''If I see f 1 through f k which are formulas,
then I can add g.'' Um,
and the key ideas I mentioned before is that in
these inference rules operate directly on the syntax,
and not on the semantics.
So given a bunch of inference rules I have this kind of meta algorithm,
that can do a logical inference as follows.
So I have a set of inference rules,
and I'm just going to repeat until there's no changes to the knowledge base.
I choose a set of formulas from the knowledge base.
Um, if I find a matching rule,
inside rules, that exists,
then I simply add g to the knowledge base.
Okay? Um, so what the- other definition I'm going to make is,
this idea of derives and proved.
So, um, so inference rule, um, derives,
proves, um, so I'm gonna write KB,
and now with a single horizontal line.
Um, to mean that from this knowledge base given
a set of inference rules I can produce f via the rules.
Okay? This is in contrast to entailment which is defined by the relationship between
the models of KB and the models of f. Now this is
just a function of mechanically applying a set of rules.
Okay, so that's a very, very important distinction.
And if you think about it,
why is it called proof? So whenever you do
a mathematical proof or some sort of logical argument,
you are in sense, in some sense,
just doing logical inference: where you have
some premises and then you can apply some rule.
For example you can add,
um, you know, multiply both sides of the equation by two.
That's a rule. You can apply it.
And you get some other equation which you can- um,
which you hope is true as well.
Okay. So here's an example.
Um, maybe I'll just for fun I'll do it over here.
Um, oops.
So I can say it is raining and if I dumped that,
it gives me my knowledge base that has rain.
Um, if it is raining it is wet.
Um, so if I dumped then I have- this is the same as,
um, rain implies wet.
Okay. Just- just just, uh,
in case you're rusty on your propositional logic.
If I have P implies Q that's the same as not p or q.
Okay? And notice that I have also wet appear in
my knowledge base because this in the background
it's basically running forward inference to,
um, try to derive as many conclusions as we can.
Okay. Um, and if I say if it is wet,
it is, uh, slippery.
Um, Again.
Now I have. Um, I have,
uh, wet implies slippery.
Um, and now I also derived slippery.
Um, I also derived rain, um,
implies slippery which is actually as you have seen not derivable from modus ponens,
so it behind the scenes is actually a much more,
uh, fancy inference algorithm.
But, um, but- but the idea here is that you have your knowledge base.
Um, you can pick up rain and rain implies wet and then you could add wet.
And you pick up rai- wet- wet implies slippery and then you can add slippery here.
And with modus ponens,
um, you can't actually derive somethings.
You can't derive not wet, um,
which is probably good because it's not true.
And you also can't derive rain implies slippery which
actually is true but a modus ponens is not powerful enough to derive it.
Okay. So- so the burning question you should have in your head is- okay.
I talked about there's two relations between a knowledge base
KB and a formula f. There is entailment relation.
And this is really what you want because this is
semantics you all- you care about meaning.
Um, and you also have this: KB derives f which is a syntactic relationship.
So what's a connection here?
In general there's no connection.
Um, but there's the kind of these concepts that will help us think about that connection.
So semantics these are things which are- Um,
when you look at semantics you should think about the models implied by the, um,
formulas, and syntax is just some set of rules that someone made up.
Okay. So how do these relate?
Okay. So to, um, understand this,
imagine you have a glass and this glass is um-um,
what's inside the glass is formulas.
And in particular it's the formulas which are true.
Okay. So this glass is all formulas such
that the- this formula is entailed by the knowledge base.
So, um, soundness is a property of a set of rules.
And it says if I apply these rules until the end of time,
do I stay within the glass? Am I always going to generate formulas which
are inside the glass which are semantically valid or entailed?
Okay. So soundness is good.
Um, completeness says that the kinda, um,
other direction which says that I am going to
generate all the formulas which are true for entailed.
I might generate extra stuff.
But at least I'll cover everything.
That's what it means to be complete.
Okay. So the model you should have in your head is you want the truth,
the whole truth and nothing but the truth.
Soundness is really about nothing but
the truth and completeness is about the whole truth.
Ideally you would want both.
Sometimes you can't have both so you're going to have to pick your battles.
Uh, but generally, you want soundness.
Um, you can maybe live without completeness, um,
but if you're unsound,
that means you are just going to generate erroneous conclusions,
which is, uh, bad.
Whereas, if you're incomplete,
then maybe you just can't, uh,
infer certain f- notions,
but at least you- the things that you do infer,
you know, are actually true.
Okay. So how do we check, um, soundness?
[NOISE] So is modus ponens sound?
Um, so remember, uh,
there's, kind of, a rigorous way to do this.
And the rigorous way is to look at two formulas.
Rain, rain, uh, implies wet,
uh, and then look at their models.
Okay. So rain corresponds to these set of models here.
Um, rain implies wet and corresponds to this set.
Um, and when I intersect them, that's the,
the set of models which are conveyed by the knowledge base,
which is this corner here.
Um, and I have to check whether that is a s- a subset of the models of wet.
And wet is over here.
So this one-one corner is a subset of one-one and zero-one.
So this rule is sound.
[NOISE]
Remember this, why is this subset of the thing I wanna check?
Because that's just the definition of entailment.
Right? Okay. So let's do another example.
So if someone said it was wet,
and you know that rain implies wet,
can you infer rain?
[NOISE] Well, let's, let's,
uh, let's just double-check this.
Okay. So again, what are the models of wet? They're here.
What is the models of rain implies wet? They're here.
And I intersect them,
I get this, um,
this- these two over here in dark red,
and then is that a subset of models of rain?
Nope. So this is unsound.
Okay. So in general, soundness is actually a fairly,
uh, easy condition to check,
especially in propositional logic.
But in higher-order logics,
it's, you know, not as bad.
So now completeness is a,
kind of, a different story,
which I'm not gonna have time to really do full justice in this class.
But, um, but here's a,
kind of, a, a example showing modus ponens as,
um, incomplete, so, um,
uh, for propositional logic.
[NOISE] Uh, so here we have the knowledge base,
some rain, rain or snow implies wet.
Um, and is this entailed, wet?
So it's raining, and if I know it's raining or snowing,
then that should be wet.
How many of you say yes?
Yeah. It should be entailed, right?
Okay. Well, what does modus ponens do?
Well, all the rules look like this.
Um, so [NOISE], um,
clearly you can actually arrive at this with modus ponens because
modus ponens can't reason about or, or disjunction. Yeah.
Is it possible for it to be right about the rain or snow? Or is
it saying that it's not possible for it to be not wet given rain?
Uh, is it [NOISE] not?
Yeah. Because you're- you already [NOISE] know that it's raining.
All right.
So you should say that it's wet.
Yeah. Okay. So this is incomplete.
Um, so we can be, um,
you know, sad about this.
Uh, there are two ways you can go to fix this.
The first way is,
um, we say, okay, okay,
propositional logic was, um,
too fancy. Uh, question?
Um, just going back to the [inaudible] -
Yeah.
-of the notation, when it says KB equals rain, then comma,
rain or snow implies wet,
is in implying in any type of assignment to rain there?
Like, is it saying that it is raining or is it just saying that we have a variable rain?
Yeah. So the question is what does this mean?
Um, this- so the knowledge base is a set of, uh, formulas.
So in this particular formula is rain.
And remember, the models of a knowledge base is where are the formulas are true.
So yes, in this case it does commit to rain being 1.
The- then models of KB are- only include the,
um, the models where rain is 1.
Otherwise, this formula would be false.
Thank you.
Yeah. Yeah?
[inaudible] the probability of the model, um,
as in way back before the [inaudible]?
Oh, it was, how can we have a probability over a model?
Um, so remember that a model is- um, where did it go?
[NOISE] Okay.
So remember a model here is just an assignment to a set of,
uh, proposition symbols or variables, right?
So when we talk about Bayesian networks, um,
we're defining a distribution over assignments to all the variables.
So here, while I'm saying is that assume there is some distribution over,
um, complete assignments to random variables,
and I can use that to compute, um,
probabilistic queries of the form- of formula given knowledge base.
Am I answering your question?
Yeah. [inaudible] [NOISE] -shouldn't you- if you have two models that contradict,
they can't be in the same knowledge base, right?
[NOISE] Um, if you have two models that [NOISE], uh,
or [NOISE] formulas that contradict,
then this intersection is going to be, uh, 0 [NOISE].
So there is- exists a set of models.
So let me do it, um, you know, this way.
So imagine you have these two [NOISE] variables, rain and wet.
Um, [NOISE] a Bayesian network might assign a probability 0.1,
point- um, I should make these sums of one.
Um, point, what is this?
Five? Um, so some distribution over these states, right?
And, um, [NOISE] and if I have [NOISE] rain,
[NOISE] that corresponds to these models.
So I can write the probability of rain [NOISE] is 0.2 plus 0.5, [NOISE] so 0.7.
Okay? And if I have the probability of wet [NOISE],
um, given rain, um,
this is going to be the probability of the conjunction of these,
which is going to be wet and rain, which is here.
This is going to be 0.5 divided by the probability of rain, which is 0.7.
[NOISE] Does that help?
[NOISE] Okay.
[NOISE]
Whoops.
Um, okay. So- okay.
So modus ponens is sound,
but it's not complete.
So there's two things we can do about this.
We can either say propositional logic is too fancy.
Let's just restrict it so that modus ponens becomes complete with,
with respect to a restricted set of formulas.
Or we can use more powerful inference rules.
So today we're going to restrict propositional logic,
um, to make it complete.
Um, and then next time we're gonna show how a resolution which is, uh,
even more powerful inference rule can be used to make,
um, any arbitrary inferences.
And this is what's, uh,
powering the, the system that I showed you.
[NOISE] Okay.
So um,
a few more definitions.
[NOISE]
So we're gonna define,um, a propositional logic with, with horn clauses.
Okay. So, so a definite clause is,
um, [NOISE] a propositional formula of the following form.
So you have some propositional symbols, all conjoined together,
conjoined just means it's added, um,
implies some other propositional symbol q.
And the intuition of this, uh,
formula says if p_1 through p_k hold, then q also holds.
So here's some examples.
Rain and snow implies traffic.
Um, traffic is can be- is possible.
Um, this is a non-example.
Um, this is a valid propositional, uh,
logic formula, but it's not a valid definite clause.
Um, here is also another example.
Um, rain and snow implies peace- uh,
traffic or peaceful, okay?
So this is not allowed because the only thing allowed on the right side- hand side of,
um, the implication is a single propositional symbol,
and there's two things over here.
Okay? So a horn clause is, um,
a definite clause or a goal clause,
um, which might seem a little bit mysterious.
But, um, it's defined as a,
[NOISE] um, something that,
um, p_1 through p_k implies false.
And the way to think about this is the negation of a conjunction of things, right?
Because remember, um, p implies q is not p or q.
So this would be not p or true,
which is not p in this case.
Okay? So, so now we have these horn clauses.
Um, now, remember the inference rule of modus ponens.
Um, we are gonna slightly generalize this to include,
um, not just p implies q,
but p_1 through p_k implies q.
So you get to match on, um,
premises which include formulas which are atomic propositional symbols and a rule that,
um, looks like this, and you can, uh,
derive or prove, uh, q from that.
Okay? So as an example, wet and weekday.
If you see wet, weekday,
wet and weekday implies traffic, those three formulas.
Then you can- you're able to add traffic.
[NOISE] Okay.
So, um, so here's the claim.
So modus ponens is complete with respect to horn clauses for propositional logic.
So in other words,
what this means is that suppose that
the knowledge base contains only horn clauses and that,
um, p is, some entailed,
uh, propositional, uh, symbol.
By the entailed propositional symbol, I mean,
like, KB actually entails p semantically.
Then applying modus ponens will derive p. Means that the two relations are equivalent,
and you can celebrate because you have both, uh,
soundness and, uh, completeness.
Okay. So just a quick example of this.
Um, so here imagine is your knowledge base, um,
and you're asking the question,
uh, is there traffic?
And remember, because, um,
this is a set of only horn clauses,
and we're using modus ponens which is complete, that means, um,
entailment is the same as I'm being able to
derive it using these rules- this particular rule.
Um, and you would do it in the following way.
So rain, rain implies wet, gives you wet.
Wet, weekday, wet and weekday implies traffic, gives you traffic,
and then you're done. Yeah?
You are saying wet implies to, uh,
like rain and weekday,
why are those horn clauses?
[NOISE] The question is why are rain and weekday horn clauses?
Yes.
So if you look at the definition of horn clauses, they're definite clauses.
If you look at the definite- definition of definite clauses,
they look like this.
And, um, k can be 0 here.
Which means that, um, there's,
um, there's kind of like nothing there.
[NOISE] That makes sense?
It's a little bit of like- I'm using this notation kind of, um,
to exploit this corner case that if you have the n of zero things,
then that's just, um, you know, true.
Or you can just say by definition,
definite clauses contain propositional symbols. Um, that will do too.
Okay. So let me try to give you some intuition why modus ponens and horn clauses.
So the way you can think about, um,
modus ponens is that,
it only works with positive,
uh, information in some sense.
There's no branching, either or.
It's like, every time you see this,
you just definitively declare q to be true.
So in your knowledge base,
you're just gonna build up all these propositional symbols
that you know are gonna be true.
And the only way you can add a new propositional symbol ever is by, um,
matching a set of other things which you can definitely know to be true
and some rule that tells you q should be true and then you make q true,
um, add q to your knowledge base as well.
The problem with propositional sym- uh,
the more general clauses is,
if you look at this,
rain and snow implies traffic or peaceful.
You can't just write down traffic or,
or peaceful, or both of them.
You have to reason about,
well, it could be either one.
And that, um, is outside the scope of what modus,
you know, ponens can do. Yeah?
[inaudible] and peaceful, that way,
you can say like [inaudible].
Yeah, yeah, good, good question.
So what happens if it were traffic and peaceful?
Um, so this is an interesting case where technically,
it's not a definite clause,
but it essentially is.
Um, [LAUGHTER] so- I don't- there's a few subtleties here.
So if you have a implies b and c,
you can rewrite that.
And this is exactly the same as having two formulas
a implies b and a implies c. And these are definite clauses.
Um, just like, you know,
technically if I gave you, "Hey,
what about a not a or b?"
It's not a definite clause by the definition,
but you can rewrite that as,
um, a implies b.
So there is, um,
slight [NOISE] kind of- you can extend this to say not only definite clauses,
but all things which are morally,
um, horn clauses, right?
Where you can do a little bit of rewriting, um,
and then you get a horn clause and then you can do your, you know, inference.
Okay? Um, so resolution is this inference rule
that we'll look at next time that allows us to deal with these disjunctions.
Um, okay.
So to wrap up. So today,
we talked about logic.
So logic has three pieces.
We introduced the syntax for propositional logic.
There are propositional symbols which you string together into formulas.
Um, and over in syntax land, uh,
these are given meaning by,
um, talking about, you know, semantics.
And we introduced an idea of a model
which is a particular configuration of world that you can be in.
A formula denotes a set of models which are true under that formula,
this is given by the interpretation function.
Then we looked at, um,
entailment contradiction [NOISE] and contingency,
which are relations between a knowledge base and a new formula that you might pick up.
You could either do satisfiability check or model checking,
which tests for satisfiability to solve that,
or you can actually do things in, um,
syntax land by adjust, uh,
operating directly on, um, inference rules.
Um, so that's all I have for, uh, today.
And I'll see you next Monday.
 Okay, let's get started.
So before I get into Logic, a few announcements.
The exam is tomorrow. Remember that.
Next week is Thanksgiving break,
so we won't have, uh, any classes.
There's no more sections.
And after you guys come back from Thanksgiving break on the Monday there's going
to be a poster session from 2:30 to 5:30.
So there's more details on the website and we'll post more details on Piazza as well.
And then finally the day after there is a logic homework due.
So that's pretty much it aside from
the final report of things that you should keep track of,
um, in this class.
Um, I want to take a few minutes to talk about CodaLab worksheets.
So this is a platform that, uh,
we've been developing in our group to help,
um, help people do research in a more efficient and reproducible way.
And the thing that's relevant for 221
is that, um, you will get, uh,
an opportunity to get extra credit by, um,
using CodaLab worksheets and also it provides
additional compute if you're running low on that as well.
I want to give a quick demo to give you an idea of how this works.
So if you go over two worksheets that codalab.org you can register for an account.
Um, I'm going to demo a kind of
a newer interface that you're actually going to see on a website,
um, just because that's what's going to be rolled out soon.
Um, so let's create a- a worksheet, um, cs221-demo.
Um, a worksheet is like a Jupyter Notebook if you are familiar with that.
Um, and you can do things like,
um, write up, write text.
So I'm going to run some sentiment classification.
Um, let me try to at least spell this correctly.
So let's suppose this, the title is CS221 Final Project.
Okay and then you can upload code or data.
So I'm going to go ahead and upload,
um, the sentiment dataset.
Hopefully this sounds familiar to some of you.
Um, and then I'm also gonna upload this textclass.py which is a source code.
So each of these, uh,
resources, data, or code is called a bundle in
CodaLab and you can look at the contents of
this bond or you can download it and- and so on.
Um, it has a unique ID which specifies forever the precise version of this asset.
Um, and now the- the interesting thing you can
do with this right now is you can run commands.
So CodaLab is pretty flexible.
You can run basically any command you want.
Um, you specify the dependencies,
um, that this command will dep- need to rely on.
And then you can type in whatever textclass.py,
train polarity.train, test polarity.test.
Um, and then you can confirm.
You can also see over here you can specify how much resources you want whether you
want GPUs or you need to access the network, um, and so on.
So this goes and creates, um,
a Docker image that's actually running, or a Docker container
that's actually running this command and you can, uh,
visualize the standard output in kind of real time as the command is- is running.
Um, and you can see the files that are generated.
Um, so for example one of these files is just a JSON file that has the test error in it.
So suppose you wanted to, um,
visualize your experiments a little bit better because this is
kind of just the default information and how much, how big the bundle is and so on.
You can, um, go- this is a
little bit more advanced but I want to show you how this works.
You can define custom, um, schemas.
So if you define a schema which is called run, you add,
um, just some fields, you can actually specify test error as a custom field.
And you say go to stats.json, read it out.
And- and then now you use that table to,
um, the schema to define this table.
Um, you can see this is the test error.
Let me make this a little bit nicer and format it to three decimal places.
Okay and then you can go and,
um, you can modify this command.
Um, and as you say rerun maybe I wanted to try some other parameters.
This eta is, um, the step size.
Let's try some more.
You can rerun this 0.2 and so on.
So you can fire a bunch of jobs,
um, and you can kind of monitor them.
So this one's running, this one's created and you can
monitor kind of various statistics that you want.
So this is generally a good way to, um,
just launch jobs and kind of you know forget about
it and keep- keep track of all these things.
Um, so then you can say, um,
larger step sizes are- are hurt accuracy or something.
So the idea behind a worksheet like
in Jupyter is that you document your experiments as you go along.
And so every asset, data, code and
bundles a- and the experiments are all kind of treated the same
way so that you can go in here and six months later and you know
exactly what command you ran to get this result and the exact dependencies,
so there's kind of no question.
So you should think about this as kind of a Git for, um, experiments.
And if you go to the main side,
uh, you can actually fire up
some jobs with GPUs in them and then there are
depending on how many people are using or there might be a queue, or might not.
Um, so if you want some extra compute that's a good way to go as well. Question.
How much memory can you typically get?
How much memory can you typically get.
So there's um- so one thing that if you want to,
um, find out, uh,
so it varies depending on what kind of- of resources are available.
But if you type, uh,
any sort of command like free you
can actually see the exact environment that your job is running.
Um, so I think, um,
you can get like maybe let's say 10 or 16 gigs of- of memory.
Okay.
Yeah.
Thank you.
Any other questions about this?
So there's, um, documentation here.
And if there's any issues that you run into,
file a GitHub request or email me or something, Piazza, um,
won't have the highest of you know, you can post on Piazza too but, um,
it'll probably faster if you, um,
um, submit a GitHub issue because I'll go directly
to the team that's working on this. Yeah.
Does this only work with, uh, Python?
Ah, does this work only with Python.
You can run any command you want.
So you can C++,
Java. Um, it's- it's-
[inaudible]
Yeah you can run it on Julia.
So the thing when you do a run, um,
you specify that Docker image which is, basically contains your environment.
So if you have, uh Julia probably has Docker images available.
We have a default one that has,
um, I don't- I'm not sure if it has Julia but it,
um, but it has kind of the standard Python TensorFlow PyTorch libraries. Yeah.
[inaudible]
Yeah. So if you want to install some dependencies,
um, there's two things you can do.
You can build your own Docker image which takes a little bit
of work but it's not too hard or you can,
um, if you want to be lazy you can just do pip install here in the command.
And for that you have to make sure you turn on network
access so you can actually download from PyPy.
[inaudible].
Yeah. Yeah you can have the requirements file.
Yeah.
Does this support pop up windows? For example if you want to [inaudible] .
Does this support pop-up windows?
No. This is more like a batch run.
So the way there's, um,
there's several ways you can do this, there's,
uh, you can actually expose, um,
like a port so you can connect if you're using TensorPort or
something you can actually connect to your job on the fly,
or you can- actually,
there's a way to mount the contents of your
[inaudible] running to your local disk and you can run whatever scripts you want.
Maybe I'll hold off further questions,
you come talk to me afterwards if you're- if you're interested and want to know more.
Okay. Just wanted to make that clear that that thing is available,
uh, go check it out, um.
Okay. So back to, uh,
the topic that we've been discussing.
Uh, so on- last Wednesday we introduced logic, uh,
and remember there's three ingredients of a logic, uh,
there is the syntax which defines a set of valid formulas,
for example in propositional logic,
it's rain and wet as a particular formula.
So syntax is- formulas are just, uh, symbols, uh.
They have no intrinsic meaning to themselves.
The way you define meaning is by specifying the semantics.
So we talked about the interpretation function which takes a formula and
a model which represents state of the world and returns either true, or false.
And the way you should think more generally about a formula is that it carves out
a set of models which are configurations of the world where the formula is true.
So in this case,
there are four possible models, uh,
and rain and wet corresponds to this set of models,
which are in red here where it's raining and wet.
And finally, we talked about inference rules where,
if you have a knowledge base which is a set of formulas,
what new formulas can be, you know, derived?
So one important thing to remember is that
these formulas are not meant to kind of replace the knowledge base.
These are things which are derived which could be very simple things as, you know,
you might- you have a lot of knowledge about the world but you might want on
any given context you might know that it's- it's raining which is.
So F is much- generally much smaller than the knowledge base in terms of complexity.
So for rain and wet, you can derive rain.
Okay. So, uh, in general, we run inference.
What does it mean to do logical inference?
You have a knowledge base and then you have a set of inference rules that you
keep on turning and turning and then you see if you produce W, oh, oh sorry, ah,
F. So an example,
what we saw last time was modus ponens, uh,
which says if you have wet in weekday,
and wet and weekday implies traffic then you can derive traffic.
So the things on the top are called premises and
the things on the bottom are called- is the conclusion.
And more generally, you have this,
a modus ponens inference rule.
Uh, so now the question is,
what does this inference rule have to do with semantics?
Because th- this is just symbol manipulation.
You just- you saw the symbols,
you produce some other symbols.
And in order to anchor this in semantics,
we talked about soundness and completeness.
So entailment is a property between, uh,
a relationship between a knowledge base and a formula,
which is given by the models, right?
So the models of F have to be a super set of models of KB,
that's a definition of entailment, and separately,
we have the notion of derivation,
which is symbol manipulation.
You can derive F given a set of inference rules from KB.
And, uh, soundness means that the set of formulas that you derive are always entailed,
and completeness means that you can derive all of entailed formulas.
So remember this, uh, water glass analogy where this set of things in the glass are, uh,
true, uh, entailed formulas and you want
to you know, stay within the glass but you don't want to spill over.
So, so far we've looked at propositional, uh, logic, uh,
which is any legal combination of symbols,
propositional symbols, and their connectives.
Uh, we also looked at a subset of that called
propositional logic with horn clauses where all the formulas look like this.
You have and, of a bunch of propositional symbols implies,um,
some other propositional symbol.
And so there's a trade off here.
So we saw that propositional logic, um,
is not- if you use a modus ponens in propositional logic,
you're- you're gonna be sound,
but you're not gonna- going to be complete.
There are certain types of formulas whic- which you won't be able to derive.
Um, so we could either restrict a propositional logic to only horn clauses,
and we showed last time that this indeed is complete,
um, or we can say we really want propositional logic,
the full expressive power.
And instead, we're gonna do is this thing called resolution,
which we're gonna talk about in this lecture.
Okay. So this lecture has two parts,
we're gonna talk about resolution for propositional logic
and then move on to first-order logic. Yeah.
[inaudible] is complete,
does it mean that anything we could represent in the propositional logic is resolution,
we can still represent it with horn clauses.
Uh, so you were talking, asking about this last statement, or-
The last two together, are they effectively equivalent?
Is there anything I could do with the last one,
something I can do with the second last one?
Um, so is- the question is,
is there anything I can do with the last one?
Anything I can do with the previous second to the last one?
Uh, it depends on what you mean by do?
So these are different statements about expressive power and inference rules.
Uh, propositional logic subsumes propositional logic with only horn clauses, uh.
So you could just say,
I only care about propositional logic.
But it's turned out this is going to be
exponential time and this is going to be linear time.
So there's a trade-off there. Yeah.
May I quickly ask for one second? Are we like saying like
complete- what kind of level of completeness are we in?
So what is completeness?
I'm using a very precise way to talk about,
uh, of a completeness of a logical system.
Uh, a set of inference rules means that anything that is entailed by
the semantics of propositional logic is derivable via a set of rules.
And a particular set of rules here is modus ponens,
for this case and then resolution for this case, yeah.
So the completeness is really a property of resolution- of
the inference rule with respect to a particular logic, [BACKGROUND] yeah.
Any other questions? [NOISE]
Okay. So let's dive into resolution now.
So let's revisit horn clauses,
and try to grow them a little bit.
Um, to do that,
we're going to take this example,
horn clause A implies C,
and we're going to write it with disjunction
for reasons that will become clear in, in a second.
Um, I'm gonna write, uh,
some of these identities on,
you know, the- on the board.
Um, so these are things,
which are, um, hopefully,
you, uh, you know.
Um, I also wrote this last time.
This is just the,
um, just true, I guess.
Um, because I wanna say definition,
but it's not really a definition because the definition is the,
um, the, the interpretation function.
But you can check the two-by-two truth table,
and this is, you know, true.
Intuitively, um, P implies Q,
really just, is the same as saying either P is false or Q is true.
If P is false, then the kind of the hypothesis is false,
so it's irrelevant what Q is,
and if Q is true,
then, um, then the whole statement is true, okay?
Um, so what about this? A and,
and B implies C. So I can write it as not A
or not B or C. So this invokes another, um, identity.
So which is that if I have not of P,
um, and Q, that's the same as not P or not Q.
Okay. So- and there's also another version,
which is P or Q negated is the same as P,
uh, not P and not Q.
So what I'm doing intuitively is pushing this negation, um,
pass the, the connective into the propositional symbols.
And when I push it passed- on negation pass and it flips to an or,
and then when I push a pass on or, it flips to an and.
Okay? And hopefully, you guys should be comfortable with
this because when you're doing programming and you're writing if statements,
um, you should know about that. Yeah?
[inaudible]?
Yeah. So a good question.
So the- what is the order of operations.
It's here, it's A and B parentheses implies C. Okay?
So if you apply the second identity on the board here,
you have A and B, is not A or not B.
And then you apply the,
the first identity and that thing, um, or C
is this, this is the same thing over there.
Okay. So now, I'm going to introduce some terminology.
Um, first is a literal.
So this is going to be either a propositional symbol or its negation.
Um, there's a notion of a, uh,
clause, which is just a,
you know, disjunction of literals.
So disjunction means or.
So these things are all clauses.
Um, and finally, there is, um,
a particular type of clause called a Horn clause,
um, which I introduced last time.
But here, I'm defining a kind of a different light here which
is clauses that have at most one positive literal.
Okay. So, um, in these clauses,
there is indeed only one, uh, positive literal.
So these are Horn, Horn clauses.
And if you remember from last time,
if you have snow or traffic all appearing on the right-hand side,
then that has two positive literals which is- which means it's not a Horn clause.
So now, I can write modus ponens the following way.
So A, and A implies C,
which can be written as a disjunction, um,
allows me to derive C. And here is
another intuition which is that I'm kind of effectively canceling out A and not A,
and I'm taking the, you know,
the resulting things and putting them on, on the bottom.
Okay. All right. So now,
let's, uh, introduce a resolution rule.
So general clauses could have any number of literals.
So this is not a horn clause, but it is a clause.
And, um, the resolution rule for a particular- this particular example looks like this,
so rain or snow.
And if you have not, um,
snow or traffic, allows you to derive rain or traffic.
Okay. So this is not a Horn clause, right?
Because I have two positive literals.
Um, and how do we intuitively understand what's going on?
So you could say, okay,
it's either raining or snowing.
And snow implies traffic,
which means that it was, it was- it's
snowing that I can get traffic. There was not snowing.
I still have rain here,
so I can, um,
I can conclude it's either rainy or trafficking.
So in general, the resolution rule looks like this.
So you have a clause up here,
um, with some P, a positional symbol,
and then you have a second clause with
not P. And what you can do is you can cancel out P and not P,
and then you can take everything else,
and then hook them up as a big,
um, you know, clause.
Okay. So this is a rule.
I've kind of sketchily argued that it's a reasonable thing to do.
Um, but to really formally verify that,
you have to check the soundness.
And the way you do soundness,
remember how do you check soundness?
You go back to the semantics and- of propositional logic,
and you verify that that's consistent with what resolution is trying to do.
So in this rule,
you have rain or snow.
The set of models of rain or snow is,
um, everything that's not white here.
Um, the set of models have not snow or traffic,
is everything that's not white over here.
And when you, um,
intersect them, you get the dark red.
And that, that represents your, um,
where you think the save the world is if you only have the, the premises.
Um, and if you look at the models of the conclusion rain or traffic,
it's this green area.
And you just have to check that, um,
what you derived is a superset of what you know.
And again, this might be a little bit counterintuitive,
but you should think about knowledge as restriction.
Knowledge means that you actually have pinpointed the state of the world to be smaller.
So the fewer color boxes you have,
the more knowledge you have.
[NOISE]
Okay? So this is sound.
Um, completeness is, um,
another, uh, much harder thing to check. Yeah, question?
So you mentioned that we wanted to have
a superset at the end but not a subset but there's the
two top most [inaudible] for snow- allow for snow removing.
Yeah.
That are not there. Is that because we've eliminated snow?
This is, uh, so why are these there?
This is- so this,
um, this square is only true in rain or snow.
Um, this is only true in,
uh, not snow or traffics.
But remember, the- the way to think about a knowledge base is that
semantics is the intersection of all the four- models of all the formulas.
So when I have intersected the models of everything up here,
I'm only left with the dark red, here.
Um, there's just one square in our final green cell,
that's not a part of the intersection [inaudible] [NOISE]
Uh, there's, well, there's two, these two.
Yes.
Yeah.
Uh, are we allowing for those because of the fact that,
we're- that it's- our conclusion is rain or traffic.
But I'm just sort of wondering when you're mentioning the super-set versus subset, um,
[NOISE] why then the other two squares up on the first row not included?
Um, so let's see.
Why are the- the ones up here not included?
Because they're not part of the intersection.
So is your question why are the squares not part of the intersection?
[inaudible]
Um, so they're not- le- let me clarify.
So if you only look at the premises up here,
the set of models is this square,
this square, this square, and this square.
Then you look at the premises or, sorry, the conclusion,
and you look at the models, independently,
of the premises and you get these six squares.
I see. So those six squares are not related to the two that we have beforehand?
Yeah. So this is the green is just derived from the, the green here.
All right, okay.
Yeah. Okay. Good. All right.
So, um, it turns out that resolution is also complete and this is you know,
kind of the, the big result from the '60s that, um,
demonstrated, I even, kind of a,
single rule can kind of rule all of propositional logic,
um, but you might say, wait a minute,
wait a minute, um, there's clearly things that this resolution,
uh, rule doesn't work on because it only works on clauses.
So what you have- what if you have formulas that aren't clauses at all?
Um, so there's a kind of this trick that we're going to, um,
do is that we are going to reduce all formulas to clauses, okay?
So another definition that is important here is,
um, CNF, so it stands for conjunctive normal form.
So a CNF formula is just a conjunction of clauses, okay?
So here's an example of CNF formula.
Um, here's a clause,
here's a clause and you can join them.
So it's important to remember that,
um, so j- just to refresh,
this is a CNF formula.
It's a conjunction of clauses,
each clause is a disjunction of literals,
and each literal is either a
propositional symbol or its negation, okay?
So or is on the inside, um,
and is on the outside,
and the one way to kind of,
make sure you remember that is,
a knowledge-base remember is, um,
a set of formulas but really it represents the conjunction of
all those formulas because you know all the facts in your, um, knowledge base.
And, uh, so you can think about a CNF formula is just,
um, of knowledge-base where each formula as a clause.
Okay. So we can actually,
take any formula in
propositional logic and we can convert it into an equivalent CNF formula,
which I'll show in the next slide.
And once we've done that, then, you know,
we can use resolution, um, and life is good.
Okay. So the conversion, um,
is going to be just a six step procedure, um,
and, uh, that- I mean,
it's a little bit grungy but,
um, but I just want to kind of highlight the- the general, you know, intuition.
So we have this formula.
So this is not a CNF formula,
but we're gonna make it one.
Okay, so the first thing w- we wanna do is we
want to remove all the symbols that aren't, um,
ands or ors or negation because those definitely,
don't show up in this, uh, in a clause,
um, or a CNF formula.
So we can use the identity,
the first identity on the board to, uh,
convert implication into, um,
[NOISE] um, on a not and a or, um,
you do that for the inner, guy here, um,
and now, you only have symbols that you're supposed to have.
Um, the second thing is, that remember,
the order in which these connectives,
uh, is important for CNF.
So negation is on the very inside,
negation is only allowed to touch a propositional symbol.
Then you have, um,
or disjunction, um, and then you have and.
So we want to change the order so that- that is- is true.
So first, we want the push the negation,
all the way inside, um,
and this is using the De Morgan's laws,
so the first, uh, the second and third identities on the board,
um, and so we pushed this inside,
um, so that now,
all the negation is on the- the on the inside, um,
we can remove double negation, um, you can check,
v- very easy to check that- that's, uh, valid.
Um, and finally, so this is not a C- a CNF formula,
it might look like one but it's not, um,
if you turn your head upside down,
it actually looks like a CNF formula.
Um, but the reason is that, um,
and is on the inside but it really should be on the outside,
and to fix that, you can actually,
distribute or over and which allows you to say this is summer or
biza- bizarre and not snow or bizarre, okay?
So now, this is a CNF formula, and then you're done.
Um, this is a general set of rules, just to recap,
you eliminate bidirectional implication implication to get the symbol inventory, right?
and then you move negation all the way to the inside,
um, and you're eliminating a spurious negation that you don't need,
and then you move any or from the outside to inside the,
um, the, the and, okay?
So long story short,
take any propositional logical formula,
you can make it a CNF formula.
So without laws of generality,
we're just going to assume we have CNF formulas.
[NOISE] Okay?
Um, another place that CNF- or you might have seen CNF formulas, uh,
come up is when you're talking about,
um, in theoretical computer science when you're talking about, uh, 3SAT.
3SAT is, uh, a problem where you're given
a CNF formula where every clause has three, um, uh,
s- symbols and, you know,
three literals and you're trying to ts- determine if it's satisfiable,
and we know that to be, uh,
a very hard problem.
Okay. So- so now let's,
uh, talk about, um,
the resolution algorithm, um, remember,
there is a relationship between entailment and contradiction.
So knowledge-based entails f is the same as
saying knowledge base is incompatible with not f. Like,
f really, really must hold.
It's- it's impossible that not f,
you know, holds, okay?
So suppose we wanted to prove that,
um, f is derived from the knowledge base,
um, what we're gonna do is,
do this proof by contradiction strategy,
where we're going to say insert not f into the knowledge base,
and see if we can derive a contradiction, okay?
So you add not f into the knowledge base,
convert all the formulas into CNF,
and then you keep on re-applying the resolution rules and you, uh,
return entailment if you can derive false, okay?
So here's an example of what this looks like.
So here's the knowledge base,
and here's a particular formula,
and now we want to know whether KB entails,
um, f or not, okay?
So you add it, um,
add not f into knowledge base,
so that's not C, and,
um, I'm going to convert this into a CNF.
So that only affects the first formula here,
um, and then I'm going to repeatedly,
apply the resolution rule.
So I can take this, uh, clause.
Resolution says allows me to cancel not A with A,
I get B or C, and then I take B and not B cancel it out,
C and I cancel out C or I mean,
when you see C and not C, um,
that's clearly a contradiction and you can derive false.
Which means that the knowledge base entails f,
in this particular example.
[BACKGROUND]
Okay. This also maybe gives you a little bit intuition of the mysteries of
defining the goal clause and horn clauses as
deriving of blah blah blah and implies false,
um, because you can add, um,
something that you're trying to prove and you
can use modus ponens to see if you can derive false.
And if you do derive false then it's, uh,
it's a contradiction. All right.
So as I alluded to before, um,
there is a time complexity difference between modus ponens and, uh, resolution.
So for modus ponens,
each rule application adds only- adds a clause with one propositional symbol.
So imagine you have n propositional symbols,
you can really only apply modus ponens n times.
So that's a linear number of applications there.
Whereas the thing with resolution is that,
you can add, uh,
each row application can add a clause with many propositional symbols.
And in the worst case you can imagine any subset of
the propositional symbols getting
added and this results in an exponential time algorithm.
This should not be surprising because we know that 3-SAT is,
you know, NP complete.
So, um, unless there was some magic here there's,
there's no way to kind of circumvent that. Yeah.
[inaudible] preferred?
So the question is why is resolution preferred?
Um, so you could just, uh,
convert everything to CNF and check,
uh, do backtracking search or whatever on CNF.
Resolution, turns out that all have generalizations, um,
to first-order logic which,
um, model checking doesn't.
Right. So- so remember there's two ways you can go about, you can, um,
do basically reduce things to CSPs and then you can solve it,
or you can try to use inference rules.
So this inference rule doesn't,
um, as far as I know,
people don't really reuse resolution in propositional logic,
but in, uh, first-order logic you kind of have no choice.
So, um, I'm thinking that,
when you see modus ponens inference rules it's kind of like everything's
going to be still down to n to relationships.
Yeah.
Like sort of NAND and NOR are the universal gates.
And so I'm thinking that resolution is like a more production,
and, um, more [inaudible].
So of the two can you prefer one to the other?
Um, the the question is whether the two are,
um, resolution looks like kind of like NAND.
Um, there's quite a bit of difference there.
Maybe you could talk about it offline.
Um, okay, so to summarize,
there's two routes here.
You can say, I am gonna use propositional logic
with horn clauses and be using modus ponens.
This is fast but it's less expressive,
or I can embrace the full complexity of a propositional logic and use resolution.
And this is exponential time,
it's slow but it's more expressive. Yeah.
[inaudible].
Right. What I mean by expressive?
I mean the- the latter which is that there's simply some things you can't write down,
um, in- in- with proofs using horn clauses,
like you can't write down rain or snow at all.
Any sort of branching or disjunction you can't do in horn clauses.
So in some applications horn clauses actually turns out to be,
um, you know, quite en- enough.
Um, so these type of horn clauses show up in- in programming languages where you're just,
uh, you know, you see some premises and you're trying to,
um, derive some other quantity.
So unlike in program analysis this is actually quite useful and efficient, um.
Okay, so let's move to first-order logic.
So what's wrong with propositional logic?
I mean, it's already exponential time so,
um, you know, you better be pretty good.
Um, so remember the point of logic is to,
in general, from an AI perspective is to be
able to represent and reason with knowledge in the world.
So there's a lot of things that,
um, we want to represent but might be awkward in propositional logic.
So here's, uh, examples.
So Alice and Bob both know arithmetic.
So how would you do this in propositional logic?
Well, propositional logic is about propositions.
So this has two propositions,
um, which are statements,
uh, which are either true or false.
AliceKnowsArithmetic and BobKnowsArithmetic, okay?
Fine. So what about all students know arithmetic?
How would you represent that?
Well, um, you probably do something like this, where you say,
okay if Alice is student than AliceKnowsArithmetic,
and Bob is student then BobKnowsArithmetic, and so on.
Um, because all propositional logic can do is from reason about statements.
So what about this?
There's Goldbach's conjecture.
Every even integer greater than two,
is a sum of two primes.
Um, so good luck with that.
Um, you might have to write down all the integers which there are a lot of them.
So propositional logic is clunky at best and not expressive,
um, and worse, what's missing?
Um, when we have knowledge in the wor- in the world,
it's often more natural to think about there as being
objects and predicates on these objects,
um, rather than just, um, opaque propositions.
So AliceKnowsArithmetic, actually, has more internal structure.
It's not just a single proposition that has nothing to do with anything else.
It has notions of Alice,
and knows, and arithmetic in them.
And finally, once you can decompose a proposition into parts,
you can do fancy things with them.
You can use quantifiers and variables.
For example, all is a quantifier that applies to each person and
we want to do that inference without enumerating over all the people,
or all of the integers.
Okay, so I'm going to talk about first-order logic,
going through our plan of first talking about the syntax,
then the semantics, and then inference rules.
So I want to warm up with just, um, some examples.
I'm not gonna do as rigorous of a treatment
of first-order logic as propositional logic because,
um, it gets more complicated and I just wanna give you an idea of how it works.
So Alice and Bob both know arithmetic.
This is going to be represented as, um,
Knows alice, arithmetic and Knows bob, arithmetic, okay?
So this is, er,
there are some familiar symbols like and,
and now the proposit- uh,
the propositional symbols have been replaced with these more structured objects.
And all students who know arithmetic gets mapped to this where now
have this quantifier for all x student of x implies knows x arithmetic.
Okay, so a bit more formally,
so there's a bunch of definitions I'm gonna talk about.
So first-order logic.
So I mean, in first-order logic there's two types of things.
There's terms, and then there's formulas.
In propositional logic, there only formulas.
So terms are, uh,
expressions that refer to objects.
So it could be a constant symbol, um,
it could be a variable,
or it could be a function applied to some other terms.
So for example, arithmetic is a- is just a constant,
it's, um, let's think about it as a name.
Um, there are variables like x,
um, which I'll explain later,
um, and there's functions of terms.
So 3 plus, uh,
x would be represented as sum of 3 of x, okay?
Remember these are just symbols.
Um, and, uh, formulas refer to truth values,
so there's atomic formulas or atoms.
Uh, so this, uh,
atomic formula is a predicate applied to, um, terms.
So knows, x is a term,
arithmetic is a term, therefore,
a pre- and knows is a predicate,
so knows, x, arithmetic,
is an atomic formula.
Um, so atoms are supposed to be indivisible but here there's a substructure here.
So maybe you can think about these subatomic particles of that,
if that is useful.
Um, there's connectives as before.
So what we're doing right now is,
you're taking these atomic formulas,
atoms, and they behave like propositional symbols.
So given these atoms are generalizations of
propositional symbols we can string them together using any number of connectives,
as we've done in propositional logic.
And then finally, we have quantifiers applied to formulas.
Which means that, if you have a formula with a variable in it, um,
we can stick a quantifier over these variables to,
uh, specify how the variable is meant to be interpreted.
Okay, so there's connectives,
um, and, um, quantifiers.
All right. So let's talk about quantifiers.
Quantifiers are in some sense,
the heart of why first-order logic is, you know, useful.
And there's two types of quantifiers;
universal quantifiers and existential quantifiers.
So universal quantifiers, you should think about as just glorified conjunction.
So when I have for all x P of x,
that's really like saying P of A and P of B and P of C and for all the constant symbols.
And existential quantifiers are a glorified disjunction when I
say there exists x such that p of x holds,
that's like saying P of A or P of B or and so on so on.
So I'm cheating a little bit because I'm only- I'm still talking about the syntax of
first order logic but I can't resist but give you
a little bit of intuition about what the syntax means.
I'm not formally defining the,
the interpretation function here but I'm just trying to give
you an idea of what the symbols, um, correspond to.
So here are some properties.
Um, so if I push a negation through a universal quantification,
then that goes on the inside and the- for all becomes and exists.
Does this sound familiar to people?
Wh- what is the name for this kind of thing?
Yeah, it's just the Morgan's law about
applying to first-order logic as opposed to propositional logic.
And it's really important to remember that,
um, the order of quantifiers matters. All right.
So for all exist is very different from exists for all.
Okay. So, um, one more comment about quantifiers.
It will be useful to be able to convert natural language sentences into uh,
you know, first-order logic.
Um, and on the assignment you're gonna do a bunch of this.
But so this is kind of- there's an important distinction I want to make.
So in natural language,
you talk, have, um,
quantifiers in natural language are words like every, or some, or a.
And so how do these get represented in,
um, in uh, formal logic?
Uh, every student knows arithmetic.
Um, every generally refers to for all.
So you might write something like this, but this is wrong.
So what's wrong about this?
[inaudible]
Sorry say again.
Not every [inaudible]
Yeah. So the problem is that what does this say?
This one says everyone's a student.
For all X, X is a student and for all X,
um, X knows arithmetic.
So it's basically saying everyone's a student and
everyone knows arithmetic which is different.
So what it really should be is implication. All right.
So for anyone that's not a student I don't,
I don't care in terms of this assessing the validity of this formula.
And only if someone's a student then I'm going to
check whether that student knows arithmetic.
Okay. So what about existential quantification?
Some knows student knows arithmetic.
This is student of X and knows X arithmetic.
So those are different connectives.
And a general rule of thumb is that whenever you have universal quantification,
it should be implication,
and whenever you have existential quantification,
it should be, um, an and.
So of course there's exceptions but this is a ge- this is a general rule.
Okay. So let me give you a few examples just to get you used to thinking about quantifiers.
So imagine you want to say there is some course that every student has taken.
So what- how is that?
So there is, there is some course,
so there should be exist Y or Y is a course that every student has taken.
So every is a for all X and,
um, here I want student implies takes XY.
Okay. Remember, uh, exist has usually has and,
and for all has implies.
Okay. What about um, Goldbach's conjecture?
Every integer is greater than, greater than 2 is the sum of two primes.
This is every even integer, so
every even integer greater than 2 implies that what about these?
This is a sum of two primes.
So notice that there are no maybe explicit hints that you need to use X essential.
But the fact that these two primes are kind of under-specified means that,
um, there should be exist.
So there exists Y and Z such that both of them are prime and the sum of Y and Z is X.
Uh, and finally, here's a statement.
If a student takes a course and the course covers
a concept then the student knows that concept.
Uh, whether that's true or not uh,
is a different matter but this is a valid formula,
and it's- can be represented as follows.
So one other, you know,
piece of advice is that if you see the word if,
that generally suggests that there is a bunch of universal quantifications.
Because if is kind of like saying there's a general rule and
universal quantification says like in general something, you know something happens.
Um, so this is for all X, all Y, all Z. Um,
if you have a student and takes some course and that course covers um,
some uh, concept Z,
then uh, that student knows that uh, concept.
Um, I guess technically,
there should be uh, also and concept of Z in there.
But let's run into getting complicated.
Okay. Any questions about first-order logic,
what the syntax is and any of these intuitions that we're having for it? Yeah.
[inaudible] why you don't use
equal sign instead of just equals or is that just to find [inaudible]
So the question is: Why don't we just use the equal sign?
So I'm being a little bit uh,
I guess cautious and, you know,
following the strict syntax where you have functions that just take or- it gives you,
it shows you the structure of the logical uh, expressions more.
So now, in, in certain cases, you,
you can use syntactic sugar and you can write equals if you want.
But remember the point of logic is not to be able to write
these things down manually and reason with them, um,
but to have a very kind of primitively built system
of formulas that you have general rules like resolution that can operate on them.
Okay. So let's talk about the semantics of first-order logic.
So in propositional logic,
um, a model with something that maps propositional symbols to truth values.
In other words, it's a complete assignment of truth values to propositional symbols.
So what is this in first-order logic?
So still we're going to maintain the intuition that a model
is supposed to represent a possible situation in the world.
Um, so I'm gonna give you of- kind of some gra- graphical intuition.
So imagine you only have unary and binary predicates.
So these are, um,
predicates that only take one or two arguments.
Then we can think about a model as being represented as a graph.
So imagine you have three nodes,
these represent the objects in the world.
So objects are kind of first-class citizens in first-order logic.
And these are labeled with constant symbols.
So you have Alice,
you have Bob and Robert and you have arithmetic here.
And then the directed edges are going to represent binary predicates.
Um, and, and these are going to be labeled with a predicate symbols.
Um, so here I have a knows predicate that applies to 01, 03.
Another knows predicate that applies to 02 or 03,
and a unary predicate here that applies to only 01.
Okay. So more formally,
a model in first-order logic is a mapping that takes
any- every constant symbol to, um, an object.
So Alice goes to 01,
Bob goes to 02, arithmetic goes to 03.
And it maps predicate symbols to tuples of objects.
So knows is a set of pairs such
that the first element of the pair knows the second element of the pair.
Um, I'm skipping function symbols just for
simplicity but you would define them analogously as well.
Okay. So that is our model.
It's a little bit more complicated than propositional logic
because you have to define something for both,
um, the term, the,
the constant symbols and the predicate symbols.
So now to make our lives a little bit easier,
I'm going to introduce a restriction on model,
as motivated in the following example.
So if I say John and Bob are students, um,
then in your head you might imagine,
well, there's two people John and Bob and they're both students.
But there could be technically only one person whose name
is both John and Bob or someone who's anonymous and doesn't have a name.
And there's two simplifications that'll rule out,
um, W2 and W3.
So unique names assumption says that an object has
most- each object has at most one constant symbol.
And domain closure says that each symbol has at least one constant symbol.
So the point of this restriction means that constant symbols and objects are,
um, in a one-to-one relationship.
And once you do that,
then we can do something called propositionalization.
And in this case, a first-order logic is actually
just a syntactic sugar for our propositional logic.
Um, so if you have this knowledge base in first-order logic,
um, student Alice and Bob- student of Bob for all,
all students are people and there's some creative student, um,
then you can actually convert very simply into propositional logic by kind of unrolling,
it's like unrolling your loops in some sense.
So we just, um,
have student Alice implies person Alice.
Student Bob implies person Bob.
And because there is a finite set of pro- of
constant symbols it's not going to be like an infinite set of formulas.
There might be a lot of formulas but, um,
it's not going to be an infinite set.
Okay. So the point of doing this is now you can
use any inference algorithm for propositional logic for first-order logic.
Okay. So if you're willing to make this restriction,
unique names and domain closure,
that means you kind of have direct access to all the objects in,
in the world via, via your
um, constant symbols in which case
you've- you're just propositional- you just have propositional logic.
Okay. So why might you want to do this?
Um, so first-order logic as,
as a syntactic sugar still might be convenient.
You might still want to write down your expressions in first-order logic, um,
and have the benefits of actually having, um, you know,
propositional logic where the inferences in some sense are much more developed.
Um, but later we'll see that,
um, there are some cases where you won't be able to do this.
Okay. So that's all I'm gonna say about the semantics of, of first-order logic.
Um, so now let's talk about inference rules.
Okay. So I'm gonna start by talking about first order logic with horn clauses.
And we're gonna use some generalization and modus ponens and then we're going to move
to a full on first-order logic and talk about the,
um, generalization of resolution.
Okay. So, um, let's begin by defining definite clauses for first-order logic.
So remember a definite clause in propositional logic was,
uh, conjunction of propositional symbols implies some other propositional symbol.
And now the propositional symbols are now these atoms, atomic formulas.
And furthermore, we have might have variables so we're going to have,
uh, universal quantifiers on outside.
So intuitively you should think about this, uh,
as a single template that gets real if you were to propositionalize, it would be, uh,
a whole set of definite formulas in propositional logic.
So thi- another way to think about this is that
this single statement is a very compact way
of writing down what would be very kind of cumbersome in,
uh, propositional logic because you would have to
instantiate all of the possible symbols.
Okay. So here's a formal definition.
So a definite clause has the following form.
You start by a se- having a set of variables,
which are all universally quantified and then you have atomic formulas,
which are all conjoined implies,
um, another atomic formula.
And these atomic formulas can contain any of these variables.
Okay. So now let's do modus ponens.
So here is a straightforward generalization of modus ponens.
You have some atomic formulas a_1 through a_k that you pick up and then you have
a_1 through a_k implies b and then you use that to derive b.
Okay. So it says the first attempt, so you might,
uh, see my catch on the fact that this actually won't work.
So why doesn't it work?
So imagine you have P of Alice.
And then you have for all x, P of x implies Q of x.
Um, so the problem is that you can't actually infer Q of Alice at all.
Because P of x here and P of Alice just don't match.
This is supposed to be a1.
This is supposed to be a1 and P of x and P of Alice are not the same a1.
So this is kind of important lesson
because remember these inference rules don't know anything.
They have no kind of intrinsic semantics.
There's just pattern matching, right?
So if you don't write your patterns right,
then it's just not going to work.
But we can fix this.
And the solution involves two ideas substitution and unification.
So substitution is taking a formula applying,
uh, find and replace to generate another formula.
So if I want to replace x with Alice,
apply to P of x, I get P of Alice.
I can do two find and replaces, x with Alice and y with z.
And I am going to replace x with Alice and y with z. Um, and so in general,
a substitution Theta is some mapping from variables to terms and substitution Theta of f
returns the result of just performing that substitution on f. So it generates
another formula with these variables replaced with these terms.
So a pretty simple idea.
Okay. Unification takes two formulas and tries to make them the same.
And to make them same you have to do some substitution,
so it returns what substitution it needed to do that.
Okay. So here's an example.
Knows Alice, arithmetic knows, x arithmetic.
These expressions are not syntactically identical.
But if I replace x with Alice,
then they are identical.
So that's what unification does.
So what about this example, how do I make these two identical,
I replace x with Alice and y with z.
And what about this one,
I can't do anything because I can't- I can only
remember substitution only can replace variables with other things.
It can't replace constant symbols.
So it can't replace Alice with Bob, so that just fails.
Um, and then things can get a little
bit more complicated when you have functional symbols.
So here to make these the same I need to replace x with Alice
and then y with F of x but x has already been replaced with Alice.
So I need to make this y goes to F of Alice.
Okay. So to summary- summarize our unification takes two formulas f and g and
returns a substitution which maps variables to terms,
um, and this is the most general unifier.
Which means that if I unify x and x,
I could also replace x with Alice and that'd be fine,
but that's not the most general thing.
I want to substitute as little as possible to make two things, um, equal.
Um, so unify returns a substitution such that. And here's an important property.
If I apply that substitution to f,
I get identically the same expression as if I apply Theta
to g. And if I can't do it, then I just fail.
Okay. So now yeah, question
Can we say that F of x,
like what should we say to F of x is it a variable or is it a formula?
So is the question is f,
f of x, is this a variable or, uh, a formula?
So f of x, f is a function, uh, symbol.
So it takes a term and returns a term.
So the technical term f of x is a term,
uh, which represents an object in the world.
Um, and you can check that,
um, knows is a,
is a predicate, so it needs to take, uh, terms.
So f of x is a term.
Okay. So now with substitution and unification,
we can now revise our modus ponens to make our work.
So, um, I'm going to have a1 prime through
ak prime which are distinct syntactically from ath, a1 through ak.
And what are we going to do is try to unify
the primes that are not primes into some substitution,
and once I have the substitution,
I can apply this to b, uh,
and derive b prime,
and that's what I'm going to write down.
Okay. So let me do go through this example now.
So suppose Alice has taken 221,
and 221 covers MDPs =,
and I have this general rule that says
if a student takes a course and a course covers topics,
then that student knows that topic.
So I need to unify this, uh,
takes Alice 221, covers 221 MDP with this abstract version.
And when I unify, I get, um,
the substitution to be x,
needs to be replaced with Alice,
y with 221 and z with mdp,
and, um, then I can derive,
uh, I'll- and then I take this, uh, Theta,
and I apply that substitution to Knows x,
z, and I get,
um, Knows Alice, mdp.
So intuitively, you can think about a1 prime and- to ak prime.
These are concrete- this is concrete knowledge. You have about the world.
This is a general rule.
So what the substitution does is it specifies how
the general variables here are
to be grounded in the concrete things that you're dealing with.
And now, um, this final substitution, uh,
grounds it out, rounds this part into,
uh, the concrete symbols,
in this case alice 221, mdp.
Okay. So what's the complexity of this?
Um, so each application of modus ponens produces an atomic formula,
just one, not multiple ones.
So that- that's the good news.
And if you don't have any functions, uh, symbols, uh,
the number of the atomics formulas is,
uh, most the number of constant symbols to the maximum-predicate-arity.
So in this case, if you have like 100 possible values of x,
100 possible values of y,
100 possible values of z,
that will be the number of possible,
um, formulas that you might produce is 100 to the 3rd.
So, um, you know.
That, that could ima- you could imagine this being,
um, a very, very large number,
so its exponential in the arity,
but if arity is, you know,
let's say 2, then you know,
this is not too bad.
It's not exponential.
Um, so that's, that's the good news.
The bad news from a complexity point of view is,
if there are function symbols,
then actually, um, that's infinite.
I guess not just exponential time,
it's like infinite, um, infinite time.
Because the number of possible formulas that you could produce is, uh, kind of unbounded.
And when you might have something like this, well,
if you remember one of the functions could be sum.
So you could have like sum 1 and sum of 1,
and sum of 1,
and, and so on.
So you can kind of essentially encode arithmetic using this, uh, first-order logic.
Okay. So, so here's what we know.
So modus ponens is complete for first order logic with only Horn clauses.
Right. So what is completeness mean?
It means that anything that's actually true, that's entailed.
There exists a derivation,
a way of apply modus ponens to get there.
But the bad news is that it's semi-decidable.
Um, this means- so first-order logic,
even when you restrict it to Horn clauses is semi-decidable. This means what?
If f is entailed,
forward inference, um, using,
um, uh, the complete inference rules,
in this case a modus ponens,
will eventually prove or derive f in finite time,
because it's complete, so eventually you'll get it.
But if, if it's not entailed, we don't know.
We don't know when to stop because it could go and just keep on going on and on,
and actually no algorithm can,
uh, show this in finite time.
So there's a complexity throughout the result that says,
um, it's not just exponential time,
but it's actually, there's no algorithm.
It's like the- if you're familiar with the halting problem,
that's- this is very related to that.
Okay. So that's a bummer.
Um, but you know, it's,
it's not the end of the world because you can still actually, uh,
just run, um, of impro- inference,
and get a partial result.
So you might succeed in which you know for sure because it's sound that,
um, it's, uh, the f is entailed.
And after a while,
well, you just, uh,
run out of CPU time and you stop,
and then you say I don't know.
Okay. So now let's talk about resolution.
So we've f- finished talking about first-order logic with, uh,
restricted to Horn clauses,
and we saw that modus ponens is complete.
Um, there's a small wrinkle that you can
actually compute everything that you hope for, but that's life.
Um, and now we're going to,
um, go to resolution.
Uh, so remember that first-order logic includes a lot more clauses.
So here's an example.
So this is all students know something.
Um, and the fact that this exists here.
Remember existential quantification is like glorified disjunction.
So this is like our example of snow- is snow or traffic.
Um, so what do we do with this?
So we're going to follow the same strategy as what we did for propositional logic.
We're going to convert everything just CNF,
and then we're going to repeatedly apply the resolution rule.
And the main thing that's going to be different is now we have to handle
variables and quantifiers and use substitution and unification,
but the structure is going to be the same.
So the conversion to CNF is, um,
a bit, um, messy and gross and slightly non-intuitive.
Um, but I just want to present it so you know what it looks like.
Um, so here is a example of,
um, not a CNF formula.
Um, so what does this say,
just you know the practice, um,
that says for all x, um,
so if anyone who loves all animals,
um, is loved by someone. Okay?
And what we want to produce is the final output is this CNF formula, which again,
CNF means a conjunction of disjuncts,
and each disjunct is,
um, uh, atomic formula or atomic formula that's been indicated.
Um, and here we see some, uh,
functions that have emerged called Skolem functions which I'll explain later.
And that's- that's basically it.
So we have to handle variables and we're going to have to handle somehow.
And the way we do this is we- remember there's no quantifiers that show up here.
And by default, everything is going to be universally quantified.
Which means that the existential quantifiers have to go away
and the existential quantifiers get converted into these functions.
Okay. All right.
So part one.
So there's again, the six,
or I can't remember,
six to eight step procedure.
We start with this input.
What is the first thing what I wanna do?
We wanna remove all the symbols that don't- shouldn't show up.
Get our symbol inventory correct.
So we eliminate implication.
This is the same as, you know, before.
So here is this thing implies this thing,
and we replace that with not the first thing or not the second thing.
So now, the expressions are more gross but it's really
the same rule that we- identity that we were invoking before.
It would do that for the inner expression.
We push the negation inwards so when it
touches the atomic formulas it eliminates double negation.
So this is all old news.
And something new here is we're going to standardize the variables.
So this step is technically not necessary.
By standardizing variables, I just mean that, you know,
this Y- this Y are actually different.
It's like having two local variables in two different functions.
They have nothing to do with each other.
Because we're gonna remove quantification later,
I'm just gonna make them separate.
So this y gets replaced with a z.
Okay. So now I have this.
I'm going to replace existentially quantified variables with Skolem functions.
Okay. So this requires a little bit of explanation.
So I have exist z loves z of x.
Okay. And this existential is on the inside here.
So of this universal quantifier.
So in a way,
z depends on x,
for every x I might have a different z.
So to capture this dependency,
I can't just drop exist z.
What I'm gonna do is I'm going to capture the dependency by turning z into a function,
and the same thing happens over here.
I have exist y and I replace this lowercase y with a big Y that
depends on the variables that are universally quantified outside the scope here. Yeah?
[inaudible]
So loves all animals is on the- in the I guess the first part.
So everyone who likes all animals is loved by someone.
So this is the someone part.
[inaudible]
Because here I push the negation inside.
Yeah. Yeah. So remember,
when I push negation past for all,
it becomes a exists.
Okay. So now, I can distribute or over and to change the order
of the- of these connectives so that because in
CNF I want a conjunction of disjuncts not disjunction of conjuncts.
And finally, I just ditch all the universal quantifiers.
Okay. Okay. So I don't expect you
to follow all that in complete detail but this is just giving you a basic idea.
Okay. So now we're ready to state the resolution rule.
And this should look very familiar.
It's the same resolution rule as before.
But now all of these things are not propositional symbols but atomic formula.
And now, this is not p and not p,
but p and not q.
And I- because these in general might be different and I need to unify them.
And then I will take this substitution return by
unification and I'm going to apply it on the result.
The same way we did for modus ponens.
So here's an example of this of animal or loves,
and over here I have not loves or feeds.
And what do I do?
I try to unify this loves with this not loves and I get this substitution.
So u has to be replaced with z of x,
and v with x.
And that allows me to cancel these now.
Now I've made them equal.
And now I take the remaining parts and I apply the substitution.
So this feeds u of v becomes feeds z of x and x.
Okay. So there's a bit more intuition I can provide but this
does become a little bit abstract and you just kind of have to
trust that resolution is doing its job.
I personally find it kind of difficult to look at intermediate stages of
logical inference and really get any intuition about the individual pieces.
But- but that's why you define the principle,
is to prove that they're right and then you
trust that logical inference does the right thing.
Okay. To summarize, we've talked about propositional logic and first-order logic.
So for inference in propositional logic,
you could just do model checking which means that convert it to a CSP and solve it.
In first-order logic, there's no way
to enumerate all the possible infinite models. So you can't do that.
But in certain cases you can propositionalize
and you can reduce first- order logic to propositional logic, in certain cases.
Or you can stick with inference rules.
And if you stick with inference rules,
you can use modus ponens on
the Horn clauses or you can- if you don't want to restrict your horn clauses,
you can use resolution.
And the only thing that's different about
first-order logic here is the plus plus which means
that you have to use unification and substitution.
Okay. Final takeaway is, you know,
there's a lot of kind of symbol manipulation on
the details here but I wanted to kind of stress the importance
of logic as expressive language to represent knowledge and reason with it.
And the key idea in first-order logic is the use of variables.
So these are very not the same notion of variables as in,
in CSPs those variables are
propositional symbols which are like the simplest thing and logic.
So in logic, first-order logic,
we've kind of gone up of a kind of a layer in the expressive hierarchy.
And variables here allow you to, um, you know,
give compact representations to a very, you know, rich thing.
So again, that kind of- if you don't remember anything,
just remember the takeaway that logic allows you to express
very complicated and big things using kind of small formulas.
Okay. So that's it. On Wednesday,
I'll be giving a lecture on deep learning
and there is one and then we have the poster session after Thanksgiving.
And then the final lecture that I will give,
that will sum everything up.
So, okay.
I will see you at the poster session, and good luck on the exam.
 Okay, so let's begin.
Um, first of all,
um, I want to say congratulations,
you all survived the exam.
Uh, well you don't have your grades back but you, you completed it.
Um, so yeah, I just want to say,
so we're gonna have the grades back as soon as we can.
Um, the CAs are all busy grading.
And we're actually gonna cancel office hours today so we can
focus on getting those grades back to you quickly.
Um, after this, the course definitely goes downhill.
So you guys can [LAUGHTER] kind of like take a breath.
Um, so after the exam there's pretty much just two things left.
So there's the project, um,
so the final presentation, uh,
the poster session for the project is going to be I believe the Tuesday after vacation.
Um, it's like a big auditorium hall,
there's gonna be a lot of people from industry and academia.
Um, it's really exciting to have like, you know,
so many smart people showing off their hard work.
Um, and then you have the last p-set which is logic. Yeah?
It's on Monday.
Oh Monday, okay, yeah.
Um, so, so right after you're back from vacation is that poster session.
Um, and then on Thursday is the last piece of it is due, logic.
Um, so logic is, um, uh,
so this is not like my official opinion but lo- I think
logic when I took the class was easier than the others,
uh, it doesn't take as much time so
you guys are definitely past the hardest point in this class.
Yeah, I think, I think that's the general opinion, yeah.
Um, but that being said,
I wouldn't wait until the last minute,
so I'd still start early and-
Personally, I didn't get the [inaudible].
[LAUGHTER] Then, um, yeah so Piazza and the office hours will be your best friend, yeah.
Um, okay.
So but today though we're talking about
this fun advanced-ish topic which is deep learning.
Um, I say ish because I think a lot of you are probably working,
um, on deep learning or have heard of it already.
A lot of you have it in your projects, um, and today,
um, we just kinda do a very,
very high level broad passive,
a lot of different subjects within deep learning.
Hopefully get you excited, um,
give you kind of like a shallow understanding of a lot of
different topics so that if you wanna take, um,
follow-up classes like 224N or, um,
229 even, uh, then you'll be armed with some kind of background knowledge.
Um, okay, so first we're gonna talk about the history.
So deep learning, you've probably heard of it,
it's really big especially in the last five,
ten years, but it's actually been around for a long time.
Um, so even back to the '40s,
there's this era where people are trying to build more computational neuroscience models.
They noticed that they knew back then that, you know,
there's neurons in the brain and they're arranged in these networks,
and they know that intelligence arises from these small parts.
And so they really wanted to model that.
Uh, the first people to really do this were McCulloch and Pitts.
Uh, so Pitts was actually a logician,
and it was, um,
they were concerned with making these kind of like
logical circuits out of a network like topology.
Um, like what kind of logical expressions can we implement with the network?
Um, back then this was,
this was all just a mathematical model.
Like there was no backpropagation,
there were no parameters,
uh, there was no inference.
It was just trying to, uh, write about fru,
I guess like theorems and proofs about what kind of problems these structures can solve.
Um, and then Hebb came along about 10 years later and started
moving things in the direction of I guess like training these networks.
Uh, he noticed that if two cells are firing a lot together,
then they should have some kind of connection,
um, that is strong.
Uh, this is- was inspired by observation.
So there's actually no formal math theory backing this.
There was, a lot of it was just, uh,
very smart people making, um, conjecture.
And then it wasn't until the '60s, um, that,
so neural networks was I guess you could say
maybe in the mainstream like a lot of people were thinking about it and excited about it,
until 1969 when Minsky and Papert they released this, uh,
very famous book called Perceptrons, uh,
which was this like big fat book of proofs.
And they were basically talking about the,
they approved a bunch of theorems that were about the limits of,
uh, very shallow neural networks.
Um, so for example,
[NOISE] um, early I think very,
very early in this class we talked about the XOR example where if you have, um,
two classes and they're arranged in this, um,
configuration then there's no,
there's no linear classification boundary
that you can use to separate them and classify them correctly.
And so th- Minsky and Papert in
their book Perceptrons they came up with a lot of these, um,
I guess you could say like counterexamples, um,
like that a lot of theorems that really proved
that these thin neural networks couldn't really do a lot.
Um, and at the time it,
it was, it was a little bit of a killing blow to neural network research.
Uh, so mainstream AI became much more
logical and neural networks were pushed very much into I guess a minority group.
Uh, so there's all these people thinking about and working on it.
But the mainstream AI went definitely towards kind of the symbolic logic based,
um, methods that Percy has been talking about the last couple of weeks.
Um, but like I said,
there's still these people in the background working on it.
So, um, for example in 1974, um,
Werbos came up with this idea that back-propagation that we learned about using
the chain rule to automatically update weights in order to improve predictions,
um, and then later on, um, so Hinton,
and Rumelhart, and Williams,
they kind of I guess you could say they, um,
popularized this, so they,
they definitely I guess you could say rediscovered, um,
Werbos's findings and they really said,
"Oh, hey everybody, you can use backpropagation."
Um, and it's a
mathematically, well kinda like well-founded
way of training these like deep neural networks.
Um, and then in the '80s, uh,
so today we're gonna talk about two types of neural networks;
convolutional neural networks and recurrent neural networks.
And the convolutional networks trace back to the '80s.
So there's this neocognitron that was invented by a Japanese, uh, Fukushima,
[NOISE] and it kind of laid out the architecture for a CNN,
but there was no way of training it.
And in the actual paper,
they used hand-tuned weights.
They're like oh, hey,
there's this architecture you can use and basically
we just like by trial and error came up with these numbers to plug in and,
and look at how it works.
Uh, now it just seems like insane,
but back then that was this, you know,
there were no ways of training these things.
Um, until LeCun came about 10 years later, and so,
um, he applied those ideas of backpropagation to CNN's.
And LeCun actually came up with a,
so there's the LeCun Net which was a very famous check reading system,
um, and it was one of the first like
industrial large-scale applications of deep learning.
Uh, so whenever you write a check in and have your bank read it, um,
almost all the time there's a machine-learning model that
reads that check for you and, um,
those check reading systems are some of
the oldest machine-learning models that have been like used at scale.
And then later, so recurrent neural networks came in the '90s,
so Elman kinda proposed it and
then there's this problem with training it that we'll talk about later,
um, called expect- exploding or vanishing gradients.
And then, um, Hochreiter and Schmidhuber,
about 10 years later came out with I guess you could say maybe a,
it solved to some extent those issues with a long short-term memory network, an LSTM.
And we'll talk about that later.
Um.
And then- but I guess you- you could still say that,
um, neural networks were kind of in the minority.
So in the '80s,
you used a lot of rule-based AI, um,
in the '90s, people were all about, uh,
support vector machines and inventing new kernels.
Um, if you remember support vector machine is
basically just like a it's- it's a linear classifier with the hinge loss,
and a kernel is a way of projecting, um,
data into kinda like a non-linear subspace.
Um, but it was- the 2000s,
people finally started making progress.
Um, so Hinton had this cool idea of hey,
we can train these deep networks one layer at a time.
So we'll pre-train one layer,
and then we'll pre-train a second layer and stack that on,
third layer stack that on,
and you can build up these successive representations, um.
And then deep learning kinda became a thing.
Er, so this looks like maybe,
uh, three-four years ago where they started taking off.
And ever since then, it's really been in the mainstream,
and you can as kind of proof evidence towards its mainstreamness.
Uh, you can look at all of these applications.
So speech recognition.
Um, for about almost a decade,
this performance in speech recognition, um,
state-of-the-art recognizers were using a hidden Markov model based um,
like that was- that was the heart of these algorithms.
And for 10 years,
performance just stagnated and then
all a sudden neural networks came around and dropped that performance.
And what's new and surprising is that all of the big comps so IBM, Google, Microsoft,
they all switched over from
these classical speech recognizers
into fully end-to-end neural network-based recognizers, er,
very quickly in a matter of years,
and when these- these large companies are operating at scale and they've, you know,
dozens maybe hundreds of people have tuned
these systems very intricately and for them to so
quickly and so radically shift
the core technology behind this product really speaks to its power.
Um, same thing with object recognition.
So there's this er ImageNet competition er which
goes on every year that says basically like how well can you say
what's in a picture and the first and so for years people use these handcrafted features,
um, and all of a sudden AlexNet was proposed
and it almost got half the error of the next best submission for this competition,
and then ever since then people have been using neural networks.
And now if you want to do computer vision, um,
you kind of have to use these CNN's, it's just the default,
if you walk into a conference,
every single poster is going to have a CNN in it.
Um, same thing with Go.
So, um, um Google DeepMind had a,
had a CNN based um algorithm,
they trained with reinforcement learning and it beat
the world champion in this very difficult game,
and then in 2017 it did even better,
it didn't even need a like real data just did self play um, and machine translation.
So Google Translate for almost a decade had been working on building a very,
very advanced and a very well performing classical machine translation system
and then all of a sudden, um,
the first machine translation system was proposed in 2014-2015,
and then about a year later they threw away 10, you know,
almost a decade of work on this system and
transferred entirely to a completely new algorithm,
um, which again speaks to its power.
Er, so but what
is I guess deep learning like what why
is this thing so powerful and why is it so good and,
um, I think, um,
so broadly speaking it's a way of learning, um,
of taking data you can slurp up any kind of data you want like I sequence, a picture, um,
even vectors um, or even like a game like Go and you can turn it into a vector,
and this vector is going to be a dense representation
of whatever information is captured by that data.
And this is very powerful because
these vectors are compositional and you can use these components,
these modules of your deep learning system kind of like Lego blocks,
you can, you know,
concatenate vectors and add them together and use this to
modify you and just the compositionality makes it very flexible.
Um, okay. So today we're going to talk about feedforward neural networks,
convolutional networks, which work on images,
or I guess just anything with repeated kind of structural information in it,
recurrent neural networks which operate over sequences,
and then if we have time we'll get to some,
um, unsupervised learning topics.
Okay, so first for feedforward networks,
um, so in the very beginning of this class we talked about linear predictors.
Linear predictor, um, if you remember is basically you define like
a vector w that's your weights and then you hit it with some input,
and you dot them together and that just gives you output.
Um, and neural networks we defined very similarly.
So you can think of each of these hidden units
as the result of a linear predictor in a way.
Um, so working backwards you- so you have the,
you define a vector w and you hit it with some activation function- with some activation,
um, like inputs, some hidden inputs,
and you dot that with your hidden and you get your output,
and then you arrive with what you are hitting
by defining a vector and hitting it with inputs.
Er, so you use your inputs to compute
hidden numbers and then you use your hidden numbers to compute your final output.
Um, so in a way,
you're kind of like I guess stacking linear predictors,
like each, each number.
So h1, h2, and f of beta are all the product,
I guess you could say they're all the result of like
a little mini linear predictor and they're all kind of like roped together.
Um, so just to visualize this,
if we want to go deeper you just rinse and repeat.
So this is- you can say this is a one layer neural network,
it's what we were talking about before with linear predictor.
You just- you have your vector weights and you apply it to your inputs.
For a two layer, you apply,
instead of a vector, to your inputs,
you apply a matrix to your inputs which gives you a new vector,
and then you dot this intermediate vector,
this hidden vector with another set of weights and that gives you your final output,
and then you can just rinse and repeat.
So you pass through a vector,
you pass through a matrix to get a new vector.
You pass that through another matrix to get a new vector,
and then you finally at the very end dot it with a vector to get a single number.
Um, so just a word about depth,
that's one of the reasons why these things are really powerful.
Um, so there's a lot of interpretations for why depth
is helpful and why kind of like stacking these matrices works well.
One way to think about it is that it learns
representations of the input which are hierarchical.
So h is going to be some kind of representation of
x. H prime is going to be a slightly higher-level representation of x.
So for example in a lot of image processing systems,
h maybe represents, um,
h could represent like the edges in a picture.
H prime would represent, um, like corners.
H double prime could represent er like small like fingers or something.
H triple prime would be the whole hand.
Er, so it's successfully,
I guess you could say like
higher-level representations of what's in the data you're giving it.
Um, another way to think about it is each layer is kind of like
a step in processing and, um,
you can think of it maybe like a for-loop where it's- it's like the more, the more, er,
the more iterations you have,
the more steps you, have the more depth you have,
um, the more processing you're able to perform on the input.
And then last, um,
the deeper the network is, um,
the more kinds of functions it can represent,
and so the, um- yeah so there's flexibility in that as well.
Um, but in general,
there isn't really a good formal understanding of why
depth is helpful and I think a lot of
deep learning is- there's definitely a gap between the theory and the practice, um.
So yeah, so this I guess just goes to show why depth is helpful,
so if you input pixels,
maybe your first layer is giving you edge detection and your second layer is giving you
little eyes or noses or ears and
then your third layer and above is giving you whole objects.
Um, yeah.
So just a summarize;
so we have these deep neural networks and
they learn hierarchical representations of the data successfully,
um, I guess you could say it's like gaining altitude in its perspective, um.
You can train them the same way that we learned,
er- you can train them the same way that we learned how
to train our linear classifiers just with gradient descent, um,
so you have your loss function,
you take the derivative with respect to your loss and then you
propagate the gradients to step in a direction that you think would be helpful.
Um, and this optimization problem is difficult,
um, so it's non-linear, and non-convex, um,
but in general if- we found that if you throw like a lot of data at it,
a lot of compute at it then somehow you manage, um.
Okay. So it seems like the slides are a little out of order,
but basically just to review how you train these things.
Um, in general, it's the same as a linear predictor.
You define a loss function.
So for example, this is squared loss, where you'd say,
I'm going to take the difference between my true output and my predicted output,
and square that, and then the idea is to minimize this.
Um, and the way you do that,
is you sample data points from your training data,
and you take the derivative of your parameters with respect to this,
um, with respect to your loss function,
and then you move in the opposite direction of that gradient,
which would hopefully move you down on the area surface.
Um, so the problem is a non-convex optimization problem.
So, er, for example, linear classifier,
because it's linear, will have co- it'll- it'll just look like a bowl,
um, whereas, these things,
you have these non-linear activation functions,
and you end up with a very messy looking area surface.
Um, and before the 2000s,
that was the big- that was the number one thing that was holding back neural networks.
Is that they are difficult to get working or hard to train.
Um, and so basically the thing that's changed is,
one, way faster computers.
We have GPUs which can parallelize operations,
especially those big matrix multiplications.
And then there's a lot more data.
Um, that's not entirely true.
So there's also a lot of other tricks that we found out recently.
So for example, if you have lots of hidden units,
then that can be helpful because it gives more- it gives more flexibility,
you could say, in the optimization.
Like if you have- if you over-provision,
if your model has more capacity than it needs,
then you can be more flexible with the kind of functions that you can learn.
Um, so we have better optimizers.
So whereas SGD will make- it'll take-
It'll step in the same direction by the same amount every time.
We have these newer optimizers like AdaGrad and ADAM,
that decide how far to move in a direction,
once you've decided the direction.
We have dropout, which is where you noise the outputs of each hidden unit,
and that makes the model more robust to its own errors,
and it guards against overfitting.
Um, there's better initialization strategies.
So there's things like Xavier initialization,
and there's things like pre-training the model on
a related data set before moving on to the data you actually care about,
and then there's tricks like batch norm, uh,
which is where you ensure that the inputs to your neural network units have,
uh- are normally distributed.
They have mean zero, standard deviation one,
and what that does is it allows you to basically take bigger step sizes.
Um, yeah, and the takeaway here is that- but i- in general
the optimization problem and the model architecture you define are very tightly coupled,
and um, it's kind of a black magic to get that right balance that you need,
and we're still not very good at it.
Um, okay. So we're gonna talk about convolutional neural networks now,
and so these operate over images.
Um, the motivation is that- okay,
so we have a picture here right?
And we want to do some kind of machine learning processing on it.
Um, we have all the tools that we need to do that.
You could say, Okay,
each picture, each pixel,
is an element in a big long vector,
and then I'm just gonna throw that out of matrix.
Um, but the thing is- is that- that
doesn't really take advantage of the fact that there's spatial structure in this picture.
So this pixel, is going to be more similar to this pixel,
than this pixel down here.
But if you pass this entire thing through a matrix,
then every pixel is gonna be treated uniquely and differently,
and so we wanna leverage that spatial structure.
And the idea to- the core idea is with um,
convolutions- so convolutions you have this thing called a filter,
which is some collection of parameters,
and what you do is you run your filter over the input,
um, in order to produce each output element.
So for example, this filter when applied to this upper left corner, um,
produces this upper left corner of the output,
and an application of a filter works kind of like a dot-product.
Where you multiply- you multiply all the numbers,
and then you add them all up,
and so how you produce these outputs,
is you take your filter,
and you basically just slide it around in the input,
um, in order to get your output at the next layer.
Um, so- yeah, so this- this example is a little more concrete.
So here- so whereas this was a two dimensional convolution,
because we had a two-dimensional filter,
and we were sliding around in both dimensions.
This is one-dimensional.
We have a one-dimensional filter,
and we slide it horizontally across.
So for example, at- at the very left, we apply it.
So 1 times 0 is 0,
0 times 1 is 1,
and negative 1 times 2 is negative 2.
So negative 2 goes in the output,
and then we do the similar thing here.
So we would dot-product this filter with um,
these three numbers in order to arrive at two.
Um, one of the advantages of this, is that,
whereas a- so if you
had- so if you had- let's say you had um,
four inputs, you- so this- this is your hidden layer.
So what h_1, h_2, h_3, and h_4,
and then you had four inputs: X_1,
X_2, X_3, and X_4.
If you did a regular fully-connected matrix layer,
then every one of these is going to be connected to every one of these,
and your parameters, you're gonna- you're gonna end up with a four by four matrix,
W_11, W_12, W_13, W_14,
and W, W, W,
W. This is what your matrix is gonna look like if this is your
W. Cause you need a new- you need a way for every one of those connections.
Whereas if you're doing convolutions,
it's much more efficient.
Because there's this idea of local connectivity.
So you have your h_1, h_2, h_3,
h_4, X_1, X_2, X_3, X_4.
Each hidden layer is only connected to,
um, what's called its receptive field.
Which is the inputs that the filter would be applied to,
and in this case,
we will only have three weights.
Because we just have this sliding window,
and you apply it at each step.
Um, so A, gives you local connectivity.
Um, B, it's much more efficient in terms of parameters.
You- you're sharing the same parameters at different places in the input.
Um, and it gives you this cool intuition of sliding around in the input.
So it's like, I have my filter of three things,
and it gives you this good intuition of- if-
if a- let's say this is- let say this is negative 1,
this is 100, this is 1, and this is 3.
Then with this, you can- you can interpret this as
my filter really likes whatever pattern is going on in these three inputs.
Um, and it doesn't like so much all the other patterns that it's picking up on.
Um, and so you- yeah,
you have this nice interpretation for the filters.
Um, in general what this looks like,
is- so in practice,
instead of one-dimensional two-dimensional,
they're very high-dimensional volumes.
Um, and so your filter is going to be a cube in the input space,
and you're sliding it around,
and applying it at every place it can fit in this input, and then,
the reason why the output is also a volume,
is because, um, you have multiple filters.
So over here for example,
this blue filter is- when you slide it around an input,
it's gonna give you this, uh,
like plane of outputs.
But then you have a second filter, this green filter,
that you can also slide around the input,
and that's gonna give you a second dimension to your hidden states.
Um, so Andrej Karpathy has this nice demo,
where basically we have- so we have a three-dimensional input,
and we have two filters which are like,
you can think of as little cubes,
and it's sliding these cubes around the input,
and every application gives you one output in this,
um, like three-dimensional output volume.
So this is, uh,
the same picture as before where you're sliding around cubes in order to fill in um,
you could say like layers of the output.
Sliding around cubes to fill in these layers of the output.
Another thing people do is max pooling.
So remember that interpretation of a filter as a,
as like a pattern detector.
Um, what this is saying is you take a region in your input.
So you, you run your filters of the input.
Get your, uh, like preliminary output,
and then you look at regions in the output and take
the maximum activation and carry that on to successive layers.
An intuition there is that instead of,
um, that you're looking- you're searching for a pattern in a region of the input.
And it's also helpful because remember at the end
of the day we want to do classification or regression or something.
We wanna get this thing down to like a very small number of, um, numbers,
and if we have this huge high dimensional volume,
then any way we can reduce its size is good.
Um, so this is an example of how these things work is,
is- it's they're pretty straightforward basically so you,
you just have your convolutional layers,
you stack them all up,
every once in awhile you have some pooling and you go
down and down in dimensionality until you eventually get down to,
um, er, like a distribution over possible labels.
And this ties into what I was saying before about that Lego block analogy because this,
this entire network is built up of one, two, three,
four different Lego blocks in a way,
and it's basically just stacking them on top of each other and
composing them up in order to get a image classifier.
So I'm going to talk about three case studies of CNN architectures.
So the first one, um, is AlexNet.
So this was that one that did really well in
the ImageNet competition and really brought CNN's to the mainstream for computer vision.
Um, basically it was just a really big neural network.
Um, um, one trick they did was they used ReLU's instead of, um.
So the sigmoid that we've learned about I think we've.
The sigmoid that we've learned about, um,
this is an activation function and it's gonna look something like this.
And what they did was instead they use the ReLU,
um, which looks a little more like that,
and in practice it turns out to be a little easier to train and use.
Um, the next one is VGGNet,
um, which did on ImageNet a couple years later.
Basically, it's, um, it's very similar,
it's just a CNN.
I think the thing to note about this one is that it's very uniform, um,
so it was 16 layers and there's nothing fancy in it.
It was just a bunch of these Lego blocks stacked up.
Um, the entire network is pretty much,
uh, like you- just by looking at this picture
you could probably re-implement their network.
Um, something else to note about it is it started this trend of deeper,
of, of kind of like tall and skinny networks.
So you'll notice that there's a lot of layers,
but each layer is, uh, very thin.
And residual networks or ResNets,
um, kinda takes that to the nth degree.
So the idea with a ResNet is, um,
so most of the time you take your input,
you pass it through a matrix state and output.
Um, if you add in your input again,
then that is very helpful because it makes
it easy for the model to learn the identity function,
and so you can give the model the capacity for like 100 layers.
But if you add in these residual connections
which is what you call it when you basically just like add in that,
add in x, um,
then allows the model to skip a layer if it decides that that's what's best for itself.
You just set W to 0,
that's what you would do.
Um, so it also helps with training.
Um, back-propagation, if you take the derivative of the loss with respect to your input,
uh, that derivative is, is just going to be 1,
um, for this part of the sum.
And so it gives- in a way you could think of it as it,
as it gives the- it gives the error signal kinda like a highway through the network,
um, and it allows the gradients to propagate
much deeper into these large neural networks.
Um, and so ResNet got a 3.6 % error on ImageNet.
If you remember the AlexNet would-
it blew everyone off the water and it got like 15%.
Uh, I think this is much better than human performance and, um,
it will come up later when we talk about- this idea of like
residual connections will come up later when we talk about recurrent neural networks.
So just to summarize, uh,
convolutional neural networks are often applied in image classification.
Um, the key idea is that there is- you have
these filters which you are sliding around the input and that lets you one,
um, have- it does,
uh, kind of this like idea of local connectivity.
So as a space in the ou- in the output only
depends on a small patch of the input instead of the entire input.
And then second, um,
it's- the parameters are shared.
Um, depth has turned out to really matter
for these networks and I think to this day it's like people,
it's like every day there's just a deeper network that's coming out and
people haven't really found a bound to depth I guess, yeah.
What's the best way to design one of these networks?
Is it trial and error, um,
but effectively you're trying to get out of some results or
is there any intuition as to how many layers,
what the layer should be and so forth?
Yeah. So the question was how to design
these things since there's- they seem so arbitrary.
[LAUGHTER]. And yeah it is,
it is really arbitrary.
I think, um, I think- so there's a few different ways.
So first, um, you start with something.
Okay. So first, you would start with something that sounds reasonable,
um, and then you would do some kind of like a grid search or you would do, um,
now there's a literature on
meta-learning which is where like you have a model, decide what
your model looks like or- but in most cases you just kind of like hand-tune it,
you're like oh if I add a layer does it go up or down?
Um, second, you look at the literature and say okay,
someone else solved a similar problem to me and they used network X,
Y and Z and so I'm gonna start with that and then start fiddling from there.
Um, and then third is to literally take
that network that's been pre-trained on an- a task and then apply it to yours.
Uh, we'll talk about it later but pre-training networks and
applying that to your task has shown to be very helpful.
Okay. So now we're gonna talk about recurrent neural networks.
The idea here is that you're modeling sequences of input.
Um, this could be things like text or sentences.
It could also be things like time series data or financial data.
Um, and the recurrent neural network is something where the input,
um, it feeds its past inputs into itself,
so it has time dependencies.
So for example, we have this very simple recurrent neural network here.
Um, it is a function with one matrix and it
takes as arguments a past hidden state and a current input,
and then it predicts the next hidden state.
So this, this is what it looks like if you were to write it in code or something.
This is what the actual network looks like.
So there's an input and you feed that into your function as well as your current state,
and it just kind of loops on itself.
Most of the time, people talk about this third perspective which is, uh,
taking kind of this network and I- and kind of like unraveling it across time.
I guess you can say unfolding it across time, um,
where every time-step you have an input and you have a state,
and then you have your function which carries you to the next state. Yeah.
I'm just curious, how does it differ from having
your original weight and
updating that weight because it sounds like that- that's a similar analogy.
[inaudible] we have the previous state.
Yeah.
But how is it compared to our machine [inaudible] classifier,
so we had the previous weights and we're just updating the previous weights.
Oh I see. So the question was,
what's the difference between this and the setting before when we had- when we were,
like, stochastic gradient descent where we were updating our weights sequentially.
Yeah. So, um, that is- that is an interesting question.
So the inf- difference is that before,
for SGD, that was for,
um, it was sequential in the training whereas this is sequential in the inference.
So each- so you do- you feed in ten inputs,
let's say, ten timesteps as inputs.
And then after all that time,
then you back-propagate once for all those time steps.
So to make that more clear.
So for SGD, it's like you have x_1,
y_1, x_2, y_2, and you use this to update w, right?
So you update w, and then you update w. And,
yeah, that's- that's an interesting observation
that there's this kinda like time-dependency.
Um, but there's no time within the data itself.
For, for the recurrent setting,
it's, it's more like this.
It's like, um, if,
uh, which marker is better? So it's more like this.
It's more like you have x_1_1, x_1_2, x_1_3,
and y, and then x_2_1,
x_2_2, and x_2_3 and y.
And then you use this to update w. And in this setting,
when we talk about time or temporal,
like, when we talk about a sequence,
we're talking about a sequence here in the data,
not necessarily in the learning.
Yeah. Okay. So to make- this a more concrete example.
We're gonna talk about a neural network language model.
So this is a model that is in charge of sucking
in a sentence and predicting
what is the most likely word that would come next in the sentence.
So we're- so each input,
we call x, and our hidden states, we call hs.
And the way this works is we have some function that
takes x_1 and it encodes it into our hidden state.
And then we have a second function that takes
the hidden state and decodes it into the next input.
We continue by taking both the next both x_2, our next input,
and h_1, our previous hidden state,
and then we use that to create a new hidden encoding.
And then, we take that new hidden encoding and decode it into our next input.
And we just rinse and repeat, each time we take the current input and
the previous hidden state to first create
an encoding and then second predict our next input.
So there's those two steps.
The cool thing about this to note is that this now, we're building up vectors,
these h_is, and that's exactly what we're looking for.
It's a vector that, in some way,
captures the meaning or a summary of
all the x- all the inputs that we fed up until that time step.
So now, we have a vector which compresses all those inputs into one vector.
So to make this very concrete,
one way you could build this thing is by basically,
each of these arrows,
you stick a matrix in there.
So our encode function would take the input, the x_t,
and multiply it by a matrix in order to get a vector and then it would take
the previous hidden state h_t minus
1 and multiply that by different a matrix to get a new vector,
and you add those vectors,
and that gives you a vector which is your new hidden state.
And decode is the same thing,
you take your hidden state,
pass it through a matrix to a vector,
and then, um, send that through a Softmax to turn
the vector of logits into a distribution of probabilities.
In general though, there's this problem with recurrent neural networks.
So if there is a short dependency which mean the input and the output,
if an output depends on a recent input,
then the path through this network is very short, right?
So it's easy for the gradients to
reach where it needs to reach in order to train the network properly.
But if there's a very long dependency,
then the gradients have a- have- they have difficulty getting all the way through.
You- so, uh, if you remember,
we talked about, um,
gradient descent as a credit-assignment problem where the gradient is in some ways,
like saying, how much, um,
okay so if I change the in- if I change
this input by a small- if I perturbed by a small amount,
how much will the output change?
That's- in what's sense,
that what the gradient is saying.
But if the input and output are super far away from each other,
then it's very difficult to
compute how small perturbations the input would affect your output.
And the reason for that,
we won't get into it so much,
but basically if you want to compute the gradient,
then, what you have to do is you have to trace
the entire path of that dependency
and you'd look at all the partial derivatives along that path,
and you multiply them all up.
And so the problem is is that if the path is very long,
then you're multiplying a lot of numbers,
and so if your numbers are less than 1,
then the- then the product,
the overall product is gonna get really small and really fast, right?
And if the numbers are bigger than 1,
then that product is going to blow up really quickly.
And so that is a problem because it means your gradients are
going to be tiny and no learning signal,
or they're going to be way too big and you're going to just like
shoot into some crazy direction that,
in practice, will like blow up your experiments and nothing will work.
[LAUGHTER]. So it's a problem.
Um, So for the,
the good thing is that for
the exploding gradient problem that's not so bad, there's a quick fix.
What people do is, is,
what they do is what's called clipping gradients.
So you'll specify some norm.
You'll be like, any gradient with a norm bigger than 2,
I'm going to clamp off at 2.
So if your gradients explode and they go to 10 million,
you're gonna say, "Okay, that's bigger than two.
So it was- it wasn't a million,
it was actually two," and you go from there [LAUGHTER].
But for the vanishing gradient problem,
here's this cool idea.
The long short-term memory cell, uh,
which is similar to a recurrent neural network,
um, but it has two hidden states.
And, um, so this is kind of a wall of equations,
but I think the important thing to note is that you're doing this,
this is basically like your input in a way,
and this is kinda like your previous hidden state.
And so what's going on here is you're doing an additive combination,
you're taking your input and you're adding in your previous hidden state,
very similarly to those residual connections in the ResNet.
And so this- because you're adding in your previous state,
it's kinda like adding in your previous input, I guess you could say,
and it allow- it gives the gradients, uh,
kind of like of a highway to very easily go back in time.
Um, there's another perspective on this.
So this picture that in- the, uh,
notation is different but, um,
I think the thing to note here is that this- so those,
those are, are you could say,
are hidden states in this network.
Or I guess- so in LSTM- so you co- I guess,
you could say that there's two hidden states. That's what people say.
Um, so you have one hidden state
which is your HT and that's the state that you expose to the world.
If you say, "My LSTM is gonna have the same API as my RNN,
then this would be like the equivalent of that hidden state that we have for RNN.
But then, you also have this internal hidden state- the C state,
that you never expose to the world.
And so in this picture, um,
the- sorry the notation is a little confusing but
the o in this picture corresponds to the h in the previous picture.
So this is the hidden state that you are exposing to the world.
And then the s corresponds to c,
so this is your internal hidden state.
And the thing to note about this picture is that s
is just zipping around on what's called the constant error carousel.
And it's always internal and it's zipping around this thing in a loop.
And so, um, what it ends up doing in practice is it ends up lear- like that- it's
a vector and it contains
very long-term information that's useful for the network over many many time steps.
So if you, if you poke around at individual dimensions of that state,
then you an- then you can find these long term things being learned.
So for example, Andrej Karpathy has a great blog post.
Um, you find networks that- you find units that track the length of the sentence,
you find units that track syntactic cues like quotes or brackets.
But in general, you find a lot of things that are just not easily interpretable.
Uh.
So one last cool.
I guess, idea that people have used with these recurrent neural networks
is sequence to sequence models, like machine translation.
Um, which is where you have two sequences.
You have an input sequence,
and an output sequence.
And you want to suck in your input sequence and then spit out your output sequence.
And you do this with what's called the encoder decoder paradigm.
You encode your sequence,
um, by giving it to your RNN,
and that gives you a one vector which is encoding or compression of that input.
And then you decode your sequence by spitting out,
um, your outputs just like we were talking about before with the language model.
And more recently, there's
these attention-based models which are very
helpful in the case where there's long sequences.
So if you look back here, x_1, x_1, x_2,
x_3 are all getting compressed into a single vector.
Um, well if you have a really long paragraph,
maybe it's hard to shove that into your,
you know, 200 dimensional vector.
It's hard to capture the depth of all that language with just a bunch of numbers.
Um, and so the idea behind attention is to,
is to look back.
Um, so th- the way attention works is,
um, at a very high level is.
So if you have your inputs,
we have x_1, x_2, and x_3.
And we've run our RNN over these inputs.
And so we have three hidden state vectors,
h_1, h_2, and h_3.
And now we're decoding.
And so we have,
we have our RNN decoder that has some hidden state.
We'll call it s_1.
Um, what happens is you compare
your current hidden state with all of the states in your encoder,
and you compute a number that says how much do I like this state.
So maybe, maybe it like really,
really likes this number,
and it's not too happy about this num- this vector and it doesn't like this vector.
What it does is it, is it uses these scores to turn them into a distribution.
A probability distribution, that again says how much,
how much do I as s_1 like each of these vectors.
And then what you do, is you compute a weighted average of
these hidden vectors [NOISE] where the weights come from this distribution.
And what this does is it serves two purposes.
So first, um, and there,
there is another way of kind of writing this down on the slides, um.
But I think this serves two purposes.
So first of all,
it gives you some interpretation.
So every time step you can see what parts of the input is it focusing on,
what parts of the inputs have a lot of probability mass on them.
And then second, what it lets you do,
is it lets you, ah,
it kind of releases the model from the pressure of
having to put the entire input sequence into a single vector.
Now, what it can do is it can go-
dynamically go back and retrieve the information it needs.
Um, and then more recently,
uh, there's what's called transformer models,
um, which are, which do away entirely with the RNN aspect, it's just attention.
And with a transformer,
you have your hidden states,
and instead of, instead of having some kind of
decoder hidden state that you're comparing to the others.
What you're doing is you just- you select each hidden state and you
compare h_1 to all the other h's including itself to get a number of how much,
um, h_1 likes those other h's.
And then you compute your weighted average of all of these hidden states,
and that becomes [NOISE] your next layer.
Um, so I, I would recommend taking 224,
and if you're interested in this topic, um,
transformers are very cool and they've recently become,
I guess you could say like the new LSTM.
Um, so from these attention distributions you get
cool interpretable maps, like in translation.
So this is an attention distribution and it points at words that are correspondent.
So uh, like economic, um,
corresponds to economic and,
and you can see that in the distribution.
Um, they also do this in computer vision.
Um, you can highlight areas of a picture.
Yeah, so just to summarize.
So recurrent neural networks, um,
you can throw a sequence at them,
and they'll give you a vector.
Um, there's this intuition, uh,
that they are processing inputs sequentially kind of like a for loop.
But they have a problem with training where the gradients
either blow up or they shrink very small.
So LSTMs are one way of mitigating this problem.
Um, but they're not perfect,
they still have to shove information into one vector.
And so the way people get around this is with attention based models,
where you dynamically go back into your input
and retrieve the information you need as you need it.
[NOISE] Um,
so now we're gonna talk about, um, unsupervised learning.
Um, so like I said before,
neural networks, we got them to work well recently,
and a lot of that is just because they need a lot of data.
Um, but if you're a smaller lab or if you don't have
enough money to basically pay for a data set or
even if it's a hard problem that just there isn't a lot of data for it.
Um, there's a lot of cases where there isn't enough data to train these very,
very large models with millions, billions of parameters.
Um, but on the other hand,
there's tons of unlabeled data laying around.
And you can download the whole Internet if you want.
Um, and there's kind of this real inspiration from us, as human beings.
Like, we are never given labeled data sets of,
of what foods are edible and what foods are not edible.
Right. You just kind of, you absorb experiences from the world and then you
use that to inform your future experiences,
and you're able to kind of like reason about it and make decisions.
Um, so, uh, I'm gonna turn off my, okay.
So uh, yeah, so the first,
I guess thing that we're gonna get into is auto-encoders.
So the idea behind auto-encoders is that if you
have some information and you try to learn
a compressed representation of that information that allows you to reconstruct it,
then presumably, um, you've done something useful.
So in neural network speak the way that works is you,
ah, give it some kind of a vector,
and use pass that through an encoder,
um, which gives you a hidden vector.
And then you pass that hidden vector through
a decoder which you use to reconstruct your input.
And the implicit loss in most cases is you wanna take the difference.
Basically, you want your reconstructed input and the original input to be very similar.
Um, so just to motivate this, um,
this isn't deep learning but principal component analysis is,
could be viewed as one of these encoder, decoders.
And the idea behind principal component analysis is you
wanna come up with a matrix U, um,
which can be used to both encode and decode a vector.
So you multiply x by U to give you a hidden vector or a new representation of your data.
But then if you transpose U and multiply it by your hidden vector
then that should give you something as close as possible to your original data.
Um, so but there's a problem.
So as we- if we have a hidden vector with a bunch of units,
then it's not gonna learn anything, right.
It's just gonna learn how to copy inputs into outputs.
Um, and so a lot of research on auto-encoding
and this kind of unsupervised learning is about how to control the complexity,
and, and make the model robust
enough to generate useful representations instead of just copying.
Um, so the first pass at that,
can be using lawnmower transformations.
Um, so you would do something like the lo- logistic or the Sigmoid loss.
And that means that the problem can't be solved anymore by just copying into the output.
And so you're gonna have to actually learn something useful.
[NOISE] Um, another way of doing it is by corrupting the input.
So you have your input and you noise it.
Maybe you drop out some numbers from it.
Maybe you perturb some numbers of it.
Maybe you add in, maybe draw from like a Gaussian and add that to your input,
and then you pass that through.
Um, so yeah, so you could drop.
So if your vector is 1, 2, 3, 4,
you could drop out 1 and 4,
and just set them to 0,
or you could slightly perturb these numbers so that they're close to their original but,
um, different, not the exact same.
And then the idea is that after you pass this encoded input through
both your- after you pass this corrupted input through both your encoder and decoder,
then the output, the eventual x-hat should
be very close to your original uncorrupted input.
[NOISE] Um.
Yeah.
So another is a variational encoder which has a,
um, cool comp probabilistic interpretation.
Um, so you could think of it as kind of a Bayesian network.
Um, I- I think maybe this is more useful to look at.
So you have an encoder and a decoder and they are both modeling,
um, I guess probability distributions.
So what this is saying is I want to encode x into a distribution over h's.
And you learn a function which is in charge of doing that.
And then you want to specify some conditions.
First you say, "Okay,
I wanna make x recoverable from my h distribution."
And then second this is a term that kind of,
um, it prevents h from being degenerate.
So maybe a good way of thinking about this
is instead of- so our traditional autoencoder would take my input and it would map it.
It would send it through some kind of encoder and map it into a hidden vector.
Send that through a decoder and that would reconstruct my input.
Whereas a variational autoencoder is gonna take
my input and it's gonna map that into a distribution over possible h's.
And then what I'm gonna do is I'm going to sample from this distribution,
pass that through my decoder and produce my reconstructed input.
Um, and the nice thing about this is
that since this is a distribution instead of a vector,
you've imposed some structure on the space.
So points that are close together in this space should map to similar x-hats.
And then similarly as you move through
this space you shou- you should be able to gradually
transition from one I guess reconstructed input to a second.
So for example, there's these cool experiments in computer vision where they'll say,
up here, this is gonna give me like a chair or something.
Um, and then down here it's gonna give me like a table.
And then if- if I move from one to the other,
then- and you constantly decode then it'll like gradually
morph into the table. Um, it's really cool.
Um, okay, and then- so then the last method of,
uh, of unsupervised learning that we're going to talk about is motivated by this task.
So there's this dataset called SQuAD, um,
which is about 100,000 examples where each example consists of a paragraph,
and then a bunch of questions, uh,
like multiple choice questions based on that paragraph.
Um, so the problem here is that there's only 100,000 examples.
And really the intelligence that
this task is trying to get at is just can you read a text and understand it,
um, which is more general and is- is captured by more data than just these 100,000.
In particular, it's captured by just all the texts that you could possibly read.
So there's billions of words on Wikipedia, on Google.
You can just crawl the web and download it.
And if somehow you could leverage that
maybe that would be helpful for your reading comprehension.
Um, and that is
just a perfect case of this- of this setting where we have tons of unlabeled data,
very small amount of data.
Um, so recently the NLP community has come up with this idea called BERT.
Um, well, there's actually not just BERT,
but there's a lot of people who are doing
similar things but BERT is the example we're talking about.
With BERT what you do is you take a sentence and then
you mask out some of the tokens in the input.
And then you train a model to fill in those tokens.
And they actually train the model on a bunch of things.
So they trained it on token filling.
And then they also would glue two sentences
together and- and ask the model are these sentences,
um, like would they be adjacent in the texts or not?
Like do they make sense together or not?
Um, but the idea is basically like have- give a bunch of unlabeled text to
a model which is just going to kinda like manipulate
that data in order to learn structure from it,
um, without any explicit purpose other than just learning the structure.
And they trained it on a bunch of data for a long time.
And then what you can do is once- so BERT is actually
a big- so we talked about transformers before and BERT is like a big transformer.
So they trained this thing on a ton of unstructured texts,
on just this word filling task for a long time.
And then what they did was they took their pre-trained BERT
and they took the- and they started feeding it questions from SQuAD.
So they took questions and then they would glue on to it the paragraph,
the context that they used to answer the question and
then they would take whatever vectors are coming out of BERT,
and they would pass that through a single matrix which
basically predicts the answer for that SQuAD question.
Um, and it did really well.
So this picture is right when BERT was released.
And these are all of the state of the art, um,
models for SQuAD and BERT,
not only beat all the other models by a large margin,
but also beat human performance.
And, um, I guess the- the intuition- the intuition behind BERT was
that by doing these seemingly trivial tasks
like word filling and next sentence prediction,
what you end up learning is you end up learning the vectors that are
coming out of this are vectors that say like,
what is the meaning of a word?
Um, what is the meaning of a word in this context?
What is the meaning of this sentence?
And that meaning isn't like operationalized towards solving any task in particular,
um, it's just like, in a very general sense,
like what is- I'm going to imbue this model with
an understanding of language and then once I have an understanding of language,
I'm going to then apply it to my very targeted downstream task.
Um, and that is- that is kind of the principle behind unsupervised learning.
So you kinda like make up these almost trivial prediction tasks, uh,
just to manipulate data and learn structure from it,
understand language, understand what a picture is.
And then what you do is then
you fine tune it on the very small amount of label data that you have.
Um, and that's kinda what the current state of the art
is in a lot of fields is basically just
doing more and more unsupervised pre-training with bigger,
bigger models and bigger, bigger data.
Um, and the field really hasn't found like a limit to this yet.
And it'll be interesting to see how far it goes.
Um, so I'm going to skip those slides but just to kind of wrap things up, um,
recently I guess the biggest things
that people have gotten to get neural networks working is;
one, better optimization algorithms.
So we have these adaptive algorithms that are not as,
um, I would say like obtuse as SGD.
It doesn't have to move in this- by the same amount every time.
You have a lot of tricks,
like, uh, you know, fine tuning,
unsupervised learning, clipping the gradients, batch norm.
Um, we have better hardware.
We have better data and that allows us to experiment more and- and,
um, train larger models faster.
Yeah. We're waiting a long time.
But I think- I think one of- maybe one of the problems
with the field is that the theory is- is in a lot of ways lacking.
Um, and we don't know exactly why neural networks work well and
why they're able to learn good functions despite having a very difficult,
um, like optimization surface.
Um, yeah. So just to summarize.
We- we talked about a lot of different building blocks.
We talked about how to leverage spatial structure with convolutional neural networks.
We talked about how to feed sequences into
recurrent neural networks and transformers, and LSTMs.
We talked about, um, you know,
the sequence to sequence paradigm for machine translation and
unsupervised learning methods that help you kinda
like jumpstart your downstream applications.
Um, and I think the big takeaway here is that in-
in some ways the big advantage of a neural network is that they are compositional.
So you- it's like a- it's like Legos.
You take- they take an input and they turn it into a vector.
And then once you have a vector,
you can start combining these things in very flexible ways.
And so in a lot of ways designing these things is a lot like putting together a Lego set.
You have your building blocks in LSTM, attention and coding,
and you can decide how to- it's like,
"Oh, I wanna run this LSTM here and this LSTM here.
And then I want this one to attend over this one.
And then I'm going to concatenate the result with the output of this CNN."
Um, and because of- because of I guess you could say like the magic about propagation,
you can combine these things.
Um, and I think even more generally,
it allows you as a programmer to instead of make a program for solving a problem,
it allows you to make this scaffolding, um,
that allows a computer to teach itself how to solve the problem.
So, um, instead of defining the function,
you want the software to learn.
You define a very broad family of functions that the software is allowed to learn.
And then you let it go and run off and find the best match within that.
Uh, yeah.
So those are all the things we're talking about today.
Uh, but, um, I hope you all have a good Thanksgiving break.
 Welcome to the last lecture.
It's a smaller group of people today.
So, um, yeah, so just a quick announcement.
The project reports are due next Friday,
so just make sure that you return those.
And yeah, Poster session was awesome.
So I showed up for a little bit of it,
but the, the posters that I saw was really amazing,
and I really enjoyed like talking to the groups I
talked to and good job on all the projects, it was great.
Uh, we're going to have a Best Poster Award thing too.
And we're going to announce that on Piazza later.
So just an announcement on that. Okay. All right.
Cool. So, uh, let's,
let's conclude the lecture, so CS221.
So the plan for today is we're going to do a quick summary of what we have talked about.
So general summary of the class,
like all the things that we have learned.
And then I wanna talk a little bit about some
of the next courses that you might wanna take,
uh, after taking 221.
Uh, if you were in 229,
I know in the morning they,
they went over a bunch of next courses, uh,
from the perspective of 229,
all the AI courses one would wanna take.
We're kind of doing a similar thing here,
but from the view of 221,
what would be some of the next courses,
that would be good to take.
And then after that,
I wanna talk a little bit about the history of AI.
We did this in the first lecture.
So it would be a little bit of review of that.
But then, I wanna talk a little bit also about
some of the next directions that might be interesting to
think about and some of the research that is done
currently in various topics of- subtopics of AI,
and what are some of the problems that people are struggling with.
So it would be fun to think about that and,
and if you're interested in any of that,
you can go do research in that area or take classes in those particular areas.
So that's kind of the plan for, for today's lecture.
It is gonna be shorter than usual.
So I think it's going to be probably an hour-ish.
Okay. All right.
So, so let's talk about the summary of the class.
So we started the class talking about this paradigm
of modeling inference and learning, right?
So, so we started thinking about how there exists a real-world problem,
you're gonna pick up a real-world problem and we're going to do modeling.
So modeling would be an abstraction of that real-world problem.
And in general, we are interested in reasoning about that real-world problem like finding
the shortest path or solving so- some sort of
optimization about that problem and we call that inference, right?
So we had a model of the world and then we would do inference reasoning on that model.
And the idea of the learning,
the learning part of the lectures was that,
well, our models are not gonna be perfect, right?
You're not gonna be perfectly modeling everything in the world, instead, uh,
we might have a partial model, and in addition to that,
you might have some data around
the world and around the things that are happening in the world.
So we would like to use that data to, uh,
to learn about the model and kind of complete our model.
So, so this was kind of the common paradigm through the class.
And, and this was,
uh, the topics that we covered.
We started with machine learning and we treated
machine learning as a tool that allows us to,
to better learn these models,
parameters of these models.
Uh, and then we, we talked about various levels of intelligence in the course, right?
So, so we started with reflex based models.
Then we increased the level of intelligence a little bit,
talk about state-based models,
variable-based models, and, and finally logic.
So let me briefly, I'll just remind you of some of these,
some of these topics.
So in machine learning,
kind of the common thing that we started looking at was,
uh, this idea of loss, uh, minimization.
So, so we have some data,
we have some training data,
inputs and outputs, x and y's.
And then we, we define some sort of loss.
We looked at different types of loss functions and properties of these loss functions.
And then the idea was we would want to minimize
this training loss for the hope of generalizing to a new scenario, right?
For the hope of if I get some sort of new input,
I would be able to give the best output possible, uh, with respect to that.
So, so I would- in,
in general I would like to minimize my tests- test error but one way to go
about that is to minimize this training loss
based on some set of variables that we have in our model.
And, and the approach that we followed for that
was using techniques like a stochastic gradient descent.
So this would be the most common thing one would want to do.
So we have this loss function.
How do I go about optimizing it?
I take the gradient and move in the negative direction of the gradient.
So, so stochastic gradient descent was commonly used,
uh, when we're doing loss minimization.
So, so these two things like,
like writing out the loss,
minimizing it, and doing something similar to a stochastic gradient descent,
this was kind of common across a lot of different machine learning algorithms that,
that we used throughout the course and we apply that to a wide range of models, right?
So, so we would like to apply this to all sorts of, uh,
models that we had in reflex-based models or state-space models,
it was kind of the same framework throughout.
So and, and in the first, uh,
set of lectures that we had,
we started talking about reflex-based models.
The simplest form of which was these linear models.
If you remember regression,
like we would have this linear class, er, regression classification.
We had these linear models,
linear classifiers, and we just wanted to learn the parameters of that.
And, and a more complex version of that are things like neural networks or
deep neural networks or even like
nearest neighbors would be an example of these reflex-based models.
And what was inference? Well, inference was pretty easy when,
when it came to these things, right?
Inference was just a feet-forward run of your model and that would give you the output.
So we weren't doing that much hard work when,
when it would come to inference.
And in terms of learning, well,
we looked at stochastic gradient descent,
we looked at other things like alternating
minimization as a way of learning these models.
Okay. So, so that was reflex-based models.
Then increasing the level of complexity,
we started talking about state-based models.
And, and the key idea that I want you guys to remember from,
from the state-based models is,
is the idea of a sta- like what is a state?
A state is a summary of all past actions
that is sufficient to choose the future actions optimally.
And then we spent a good amount of time thinking about how to define a state like how,
how to pick a good state and how to, how to do this,
this modeling when it comes to the state-based models.
And, and we looked at things like search problems or we have deterministic systems.
We looked at things like MDPs when,
when we're playing against nature,
we have a little bit of a stochasticity.
And then we looked at things like games where
you're playing against some other intelligent agents.
So there are some other intelligent agent that's coming in and playing against us.
And, and in terms of inference,
we looked at a couple of really cool algorithms, right?
We, we talked about uniform cost search and A-star.
We talked about dynamic programming, value iteration, minimax.
So we covered a set of number of different,
different ways of intelligently looking at these models and doing inference.
And, and when it came to learning,
we looked at structured Perceptron,
Q-learning, TD learning, reinforcement learning in general.
Like those were some of the learning algorithms that we
applied when it came to state-based models, okay?
So, so that was state-based models.
Then, then we moved the level of complexity and intelligence
a little bit higher and we looked at things like variable based models.
And the idea of variable-based models was that
the ordering of these states doesn't really matter.
It's just the relationship between them that matters.
And we define things like factor graphs.
So if you remember the map coloring example,
we define a factor graph around it and,
and the idea was there's this graph that- and that graph structure here
captures the conditional independence between different variables that we have.
So different variables in this case was these different provinces and,
and you would want to color them differently.
And the relationship between them is going to be represented by a factor.
The two types of models we discussed in this setting were
constraint satisfaction problems and Bayesian networks.
Uh, Bayesian networks but in the case where we have probabilistic relationships,
uh, and, and we talked about inference,
specifically backtracking, er, forward-backward, beam search,
Gibbs sampling, as various ways of doing inference on these algorithms.
And then when it came to learning,
we looked at things like maximum likelihood and EM to,
to try to do learning on these,
on these types of systems, okay?
And then finally, the last few lectures we've been talking about logic.
So pushing the level of complexity just a little bit higher and, uh,
thinking about formulas like actual like
logical formulas that represent intelligent things about your system.
So, so the key idea of logic is we're gonna have
these powerful formulas that are going to represent
powerful like meaningful things about your system.
And we've talked about things like prepositional logic or first- and first order logic.
And we talked about model checking which is commonly used in satisfiability.
We talked about modus ponens,
resolution as various types of
inference algorithms that could be used when we have logic.
And we didn't really talk about learning when it comes to logic.
And I would say this is kinda like an open question still, like, how,
how do you combine ideas from learning and
ideas from logic and get the best of the both worlds?
So there are ways of combining them,
but how do you ensure that you're getting the best of both worlds like from
data-driven ways of looking at things and a model-based look- way of looking at things?
So, so what did CS221 gave us really?
The CS221 is this,
is this class where it gives us a set of tools to look at the world and,
and think about difficult problems in the world and pick the right models.
Pick, pick the right, like way of formulating
that problem and the right inference algorithm to go about solving them.
So, so I would say it's pretty much like we've covered a lot of material.
So, so we covered, uh,
breadth like pretty broad set of, set of topics.
And the idea of it is to know that you have
all these tools and you can pull out these tools and you can go deep into any of them.
But, but the, but the goal of CS221 was to just give a broad view
of what is artificial intelligence and what
are some of these tools that, that we would have.
So- but if you're interested in going a little bit deeper in any of these topics that,
that we've discussed in the class,
there are a good number of classes that, that you can take.
And I want to just briefly mention some of them,
an overview of some of these courses.
So I would categorize the classes- the next classes that you can take into,
into two main categories.
You can take foundational classes that you go
deeper in some of the foundational things you are talking about or you can take
application classes where you go deeper in like
the specific applications like natural language processing, vision, robotics.
The specific applications that we kind of
briefly covered in this class but we didn't go that deep in.
So, so if you're interested in foundational classes,
some of these other AI-based classes or things
like Probabilistic Graphical Models, CS228.
If your interested in Machine Learning,
there's 229 and 229T.
And, uh, there is the Deep Learning class.
If you're more interested in the optimization side of things,
there is Convex Optimization,
Decision Making Under Uncertainty.
And also if you're interested in logic side of things then,
uh, there's this Logic and AI course.
And also there's the, there's the Big Data class
too that if, if you're interested in that.
So I'm gonna go a little bit deeper in some of these courses but this
is just more of an overview of some of the foundations,
uh, if, uh, next courses that you might be interested in taking.
And all of these are also posted on the AI website.
So ai.stanford.edu/courses.
So, so that's foundations.
But if you're interested more on the application side of things,
there's a good number of courses around Natural Language Processing.
Um, I'm gonna go again a little bit deeper in some of these courses.
And then there's a good number of courses around vision,
good number of courses around robotics.
There's also this other course around General Game Playing,
which would be fun to take if you're interested in that side of things. All right.
So, so let me just briefly mention like one slide on
some of these courses that I think would be good courses to take after this class.
So, so one of these is CS228.
So CS228 is a probabilistic graphical models course.
If you remember the variable,
variable-based models part of,
part of the lecture, um,
this would be kind of a next course that goes deeper in that.
So, so we talked about algorithms like forward-backward, variable elimination.
But, uh, if, if you take 228,
you'll be talking about more general type algorithms like belief propagation,
variational inference, Markov Chain Monte Carlo, and so on.
And another thing that 228 is going to cover is
invariable based models like the way we treated things was,
the model was given, the structure was given to us, right?
We would say, well this is an HMM,
and given that it's an HMM I'm gonna do these extra things on it.
But in 228, you'd actually be thinking about learning the structure,
learning the right structure to put in and,
and how you think about these different variables and the relationships between them.
So if you want to go deeper in that,
that would be the course to take.
Another interesting course to take and some of you might have
already taken it is the Machine Learning course.
So in this class, the way we treated machine learning was just as a tool, right?
Like we, we had a few lectures on it and we
just learned about machine learning just enough for us to,
to do some of the things we wanna do in this AI course,
but, but it's definitely broader than what we have discussed in the class.
And some of the ways that it is,
it is broader and more general than what we have discussed in the class is,
first off in this class we talked about discrete, actions,
and undiscrete, undiscrete time,
discrete action and state type systems.
229 is going to cover a more broader set of, set of,
set of models where you're actually thinking about continuity a little bit.
We talked about linear models.
221 we'll talk about Kernel methods,
decision trees, uh, boosting, bagging, feature selection,
like all these sorts of different types of algorithms and models that are go,
go- that are gonna go beyond what we have discussed in this class.
Uh, we talked about k-means.
Then we're going to talk about more broader set of clustering algorithms,
like mixture of Gaussians, PCA,
ICA, all these sorts of things.
So a really useful good class to take if you,
if you want to just learn more on the machine learning side of things,
more from practical perspective.
If you're more interested in a theoretical proce- eh,
theoretical side of machine learning,
there is this other course called to 229T.
So this is Statistical Learning Theory,
and this is going to actually think about the mathematical principles behind learning.
So, so it doesn't necessarily cover the particular algorithm but it,
it- it's going to cover, uh,
like properties- mathematical properties around that algorithms.
Say things like uniform convergence.
Let's say you have a predictor and you want to make sure that
that predictor with high probability is going to have some bounded error.
How are you going to bound the error of that?
How are you going to bound the test error with respect to
the training error and some properties of, of your predictor.
Or, or how would you formalize things like regret,
uh, of your learning algorithm.
So, so thinking about, uh, complexity,
thinking about putting bounce, convergence,
regret, these are going to be the topics that will be discussed in,
in the Statistical Learning Theory, CS229T.
So if you're more theoretically minded,
I think this would be a good course to take.
So now thinking more on the application side of things.
Um, so a couple of good applications of AI
after this course are things around vision, NLP, and robotics.
I would say those would be kind of, uh,
three main applications that you might want to consider
going deeper and if you're interested in any of these areas.
If you're interested in vision,
there are a good number of classes around vision.
There are a lot of interesting tasks around vision,
some of them are more solved and some of them are
more researchy things around like object recognition,
detection, segmentation, but also things like activity recognition.
Like if you, if you have frames of- different frames of a video of,
let's say a soccer game,
how would you predict where the ball goes?
Ho- how would you predict where the person goes in the next few frames?
Like that's actually a pretty difficult problem.
Like doing activity recognition from just frames of images.
So, um, if you're interested in some of these problems from the vision perspective,
um, I, I would recommend taking 231 type classes, okay?
So robotics would be another interesting,
uh, application to look at.
So in robotics in general,
we are interested in problems around manipulation and navigation and, and grasping.
So, um, the main applications that you might
think about are things around self-driving cars,
medical robotics, assistive robotics.
Um, and, and the interesting thing that robotics brings
is that there is a physical system sitting there so you're putting
your AI algorithm, the things you have developed in this class and some
stuff beyond it on this physical system, physical robot.
And you need to deal with things like continuity.
You need to deal with things like uncertainty and,
and you need to deal with physical models that could come from kinematics and control.
So there are a lot of interesting robotics classes if you're, uh,
so I think Intro to Robotics is offered next quarter with, Oussama Khatib is teaching it.
But there's also a new robotic series course,
uh, that just came out.
So this is the Robot Autonomy 1 and 2.
Um, so advertisement for myself,
I'm teaching this next quarter with Marco Pavone and Jeanette Bohg.
So this is the Robot Autonomy 2.
Robot Autonomy 1 was already offered in the fall.
And, and the idea of Robot Autonomy 1 is to just
cover the dif- different layers of the robotic stack.
And, and at the end of the day,
they actually have this project,
this really cool project where you have,
you have a robot platform and you have a lighter on top of
it and you want this robot to just move around in a fake city.
So if you're interested in s- I think the project,
the project presentations is not already done.
So, so if you're interested you can show up to,
to do a round and see how,
how these robots are moving around.
But they basically have this fake city where this robot just
navigates around in this fake city and does autonomous driving.
So you can see a picture of a bicycle in the back where the robot is to detect it and
the [LAUGHTER] bicycle it's actually like moving so [LAUGHTER]
the robot needs to detect it and do obstacle avoidance,
do coordination and with,
with other agents around it in this particular environment.
So that- that's Robot Autonomy 1.
In Robot Autonomy 2,
which is, which is offered in winter,
what we wanna do is you want to put a manipulator on top of
the robot so we are looking at mobile manipulation where we
actually have an arm and we have this arm
pick-up objects and blocks and put them on top of each other and,
and do interactions so,
so the class is- two,
two big chunks of the class out of four is focused on
interaction with the world- with the physical world and interaction with other agents.
So there are interesting multi-agent like
game theoretic questions that could come up and you have,
uh, multiple robots trying to interact with each other.
So ideas from games could come up there like naturally. All right.
So, uh, so that was robotics.
And then another interesting application is natural language processing.
Um, and natural language processing is
particularly very interesting because if you think about it,
the world is continuous,
but the words that you're using are,
are discrete and these discrete words have continuous meaning.
So there is a lot of mismatch between the fact that we have
discrete words in a continuous world and, and,
and we need to use these words to describe,
uh, the, this continuous world and,
and there are very interesting questions and challenges that arise in
NLP around like compositionality and, and grounding.
And if you're interested in these types of tasks,
I think there are a lot of again interesting tasks that are more
solved versus less solved and more researchy around NLP.
So if you're interested in any of them I would recommend taking classes like 224.
So, so some of these tasks are around like machine translation,
let's say text summarization,
dialog, some of them are much harder, uh,
like text summarization, dialog, those sort of things.
So, um, and we had a couple of,
uh, homeworks around this too.
So, so if you're interested in going deeper in that,
I do recommend taking NLP classes.
So those are some of the foundation and
more application-y courses that I would recommend taking.
I want to briefly mention two other courses too that are,
that are not necessarily directly in AI but they're in the neighboring fields that,
that would be still interesting to look at.
One is looking at cognitive science.
So, uh, so cognitive science in general looks at how human minds work.
And it's one of those fields that,
that kind of grew together with AI, right?
Like the cognitive science and AI,
they kind of started together and they went their ways but they,
they still tend to inform each other.
And there's a group of cognitive scientists
who are looking at computational cognitive science.
And they use ideas like Bayesian modeling and
probabilistic programs when they look at cognitive science.
So there's this course, uh,
PSYCH204 which is cross-listed as CS428.
I think Noah Goodman teaches this usually.
And, and I think th- this would be a great course to take if you're
interested in the cognitive science side of things,
uh, and you would have ideas and topics from,
uh, other pe- other cognitive scientists like Josh Tenenbaum, and Tom Griffiths,
and Noah who are working in this particular area of,
uh, computational cognitive science.
So, so that's one.
But if you think about cognitive science as kind of the software here,
neuroscience on the other hand is,
is the hardware of, of the problem.
So, so another neighboring field that you might be interested in
looking in a little bit deeper is, is neuroscience.
And if you think about neural networks,
like back in the day when they first start,
like when people first started looking at neural networks,
they were kind of thinking of them as computational models of brain.
But today's neural networks- modern neural networks
aren't really like biologically plausible in any ways, right?
So, so they're not really models of,
of the brain but still they're, they're interesting,
there are interesting connections and insights that could be used in
neuroscience from the perspective of AI or from AI to neuroscience.
So I do recommend taking kind of cross-section neuroscience courses with AI,
I think Dan Yamins would be someone who works in this area if
you're interested in looking deeper in courses around neuroscience.
All right. So that was kind of a quick summary
of what we have discussed so far in the class,
what are some next courses you are interested in taking,
think deeper around them if you're interested in learning more and just come,
come chat with me, I'm,
I'm around, like, if you want to talk about them.
But for the rest of the class what I wanna do is,
I want to just do a quick like history of AI again,
and then after that let's just talk
about what are some of the problems, like what is left,
like we've talked about all these cool algorithms toolsets,
but what is difficult, what is not solved?
So, so I want to spend a little bit of time on that.
So let's talk about the history of AI.
All right. So birth of AI.
So we talked about this during the overview lecture.
This was the first lecture we co- came in and were like,
okay, how did AI happen?
So the birth of AI really like people refer- referred to
it as this workshop that happened in 1956.
This was a summer school in Dartmouth, um,
and basically everyone famous in the field showed up to this workshop,
including people like John McCarthy, Marvin Minsky,
Claude Shannon, all of these people showed up to this- to the Summer School,
and the reason for this summer school was to kind of
understand the general principles of intelligence.
So what they really wanted to do was they wanted
to figure out all the features of intelligence,
and if they could formalize that,
they could go ahead and like simulate that and have simulate intelligence.
That is what they really wanted to do.
And the workshop was really useful because after that,
these people went back their ways,
and then started doing really cool stuff,
and this was the first rise of AI, and,
and we started seeing things like problem-solving type, type,
type systems, things like Samuel's checkers program,
which was able to kind of beat a strong amateur level pla- pla- player.
We started seeing other types of problem-solving programs like theorem provers,
and then people, like,
started really using logic as a way
of thinking about AI and thinking about problem-solving.
So that was really exciting, because people, like,
at a time were thinking they have just solved everything, right,
like it was exciting, the logic was there,
like people had all these cool programs,
they were super excited about the,
the potential that, that these systems can bring.
Uh, here are some of the quotes from people around that time.
Things like, machines will be capable within 20 years of doing any work a man can do.
So this is what Herbert Simon said.
Marvin Minsky said, within 10 years,
the problems of artificial intelligence will be substantially solved.
Claude Shannon said, I visualize a time
when we will be to robots what dogs are to humans,
and I'm rooting for the machines.
[LAUGHTER] So none of these really happened,
but one thing to notice is these are not random people on the street, like,
these are like fathers of the field,
like, these are people who,
who were like in- in it,
like, they were looking at,
they had a lot of insight like in terms of what is
ca- what we can do and what we can't do.
And it's kind of interesting that even like them,
like they had all this like overwhelming optimism,
and this did not pan out, right.
So there was a lot of optimism here, and,
and we started getting really underwhelming results.
So lots of optimism, government came in,
government was like here's my money,
take my money, go do stuff.
And, uh, basically the,
the problem that government was really excited about was machine translation, right?
So they wanted to take Russian texts and translate that.
And, and the outcome of that was something like this,
so the translation is the vodka is good,
but the meat is rotten, so that's not really,
like, a good translation of that text.
And people started feeling like these types of
problem-solving algorithms are not going to do it.
So, so on and at point, then the government cut funding,
and this was the first winter of AI.
So we had the rise of AI with problem-solving with
that summer school, lots of excitement,
and then it didn't really work when it came to machine translation,
and then we had, like,
the first winter of AI.
One thing that I wanna kind of like point out is,
is we are at a good place for AI right now, right, like I,
I would say like AI right now is also pretty overhyped, right?
And then I wanted to put this quote here.
So this is from Roy Amara, who says,
"We tend to overestimate the effect of a technology in a short run,
but actually underestimate the effects of it in a long run."
Like if you think about any system,
any technology that we have developed,
it's always like oh within two years it's going to solve everything,
and it's not going to solve everything within two years,
but if you look at what it has achieved in 20 years,
it's actually achieved a lot of things,
and we usually underestimate that.
And I think it's the same thing with AI,
like, we are going to think, well,
we're gonna have autonomous cars tomorrow, or by 2020.
Actually, autonomous car companies were saying we're going to have autonomous cars by
2020 when I first started working on that,
that's next month [LAUGHTER],
I'm don't think we're going to have autonomous cars by 2020,
but we are going to see a lot of advances,
like, we are, we are seeing a lot of improvements in terms
of the algorithms and the systems that we are developing.
So, so I think in general,
we should be aware of that,
and we should be, we should be smart about it.
Like, like surely AI is overhyped,
but what can we do to actually address some of these problems?
And going back to this first era of AI,
this problem-solving e- era,
well, why didn't it work?
Well, the reason it didn't work was we had limited computation,
we had limited information.
This is the thing that we actually,
like, started this class off.
He said, well, a lot of AI algorithms,
they haven't changed that much, right?
But the with thing that has changed over
the years is we have lots and lots of computation,
we have lots of lots of data,
and that's the thing that has really made the bigger difference here.
And that's kinda, like, one of the reasons that it didn't work.
But even though, like,
we had these problems,
and we had this first winter of AI,
there were lots of interesting contributions from that era.
The LISP programming language came out around that time,
garbage collection came out that time, time-sharing,
like, really interesting foundational ideas of
computer science emerged during this period.
And, and also the key paradigm that you are using in
this class, thinking about separating modeling and inference,
that actually came out around the same era too.
Like, like the fact that we shouldn't have this declarative model thing,
and at the same time this procedural inference algorithm kind of
separated out from each other and think of them as separate things,
like, i- is, is a huge contribution that came out around that time.
All right. So, so that was kind of rise up and down of first up and down of AI.
The second rise of AI was around '70s and '80s.
This was when the knowledge-based systems came out,
the expert systems, and,
and I would argue that the reason that we had the second rise was people,
people started thinking about AI differently.
Like, originally people wanted to build
artificial intelligence because they were interested in intelligence.
They were interested in understanding intelligence,
that- that's kinda what the summer school was about.
But at this point, people were not interested inteli- in intelligence,
what they were interested in was just building really useful systems that can do things,
like they didn't care about intelligence at this point.
And then that's why we had this rise of expert systems.
So we think about logic,
and we think about using domain knowledge to,
to have things like if,
if-then-else type statements, like, if we have a premise,
then we have some sort of conclusion,
and, and building these experts,
expert systems allows us to do a lot of cool tasks in a real world.
So, like, we had,
we had actually impact around this time on things like inferring molecular structures,
like diagnosis- diagnosis of blood infections,
things like converting customer orders into parts and specification.
So actual applications in the world,
people started taking each of them and thinking
about the expert knowledge that you have in that particular application,
and formulating that in these expert-based systems,
and then putting an AI on top of it,
that, that does actual, actual work.
So, so that was really cool.
So the contributions of,
of this era is that, first off,
we had real applications that actually impacted industry,
and, and this domain knowledge,
the idea of I'm going to pick the domain knowledge,
and this knowledge is actually going to help me make exponential growth,
was the thing that, that was really powerful at this time.
But why didn't it work? It didn't work, right?
This, this was the, the second rise,
and we had another winter,
the second winter of AI.
So the second winter of AI came because there was a bunch of problems.
One of the problems was knowledge in general,
it's not deterministic, right?
Like we have a lot of uncertainty when we think about knowledge,
and these systems were not able to,
like, encode uncertainty the way you want it to be.
And in addition to that,
there was a lot of information, right?
Like, if you think about any of these expert-based systems that requires a lot of
many- manual effort to write down these rules and all of
these relationships between all these different subparts of the system.
So an example of that is SHRDLU,
so SHRDLU is one of the first natural language understanding programs, computer programs.
Uh, this was written by Terry Winograd,
who is at Stanford now.
I think this was when he was at MIT.
And, and he created this system, this computer program,
where you have this block world environment in it, and,
and you can actually have a person that interacts with this computer program, uh,
and maybe the person says,
pick up a big red block,
and the computer says okay because the computer understands, like,
the relationships between big and small,
and red, and different colors,
and where the blocks are placed,
and what can be picked, and what cannot be picked, right?
So, so this had a who- a whole bunch of relationships and,
and rules around- around it, and,
and you could actually converse something with this computer program,
and that was really powerful.
But even, even Terry himself,
like a couple of years after, uh,
had a statement talking about his worries about, how,
how SHRDLU- programs like SHRDLU are not going to solve the problem,
they're not going to go all the way.
Like, like, he was saying, this is kind of a dead end in AI, and,
and thinking about these complex interactions,
there's just a lot of them, and,
and it just doesn't seem feasible to write down
all these rules that you would have between each one of your subparts with,
with other par- sub-parts,
and there is no easy footholds.
So, so at this point people were thinking this complexity barrier is not
gonna really allow these AI systems to do cool big things.
So then we had the second winter of AI, and then finally,
there is this third rise of AI that we are still on,
and God knows where it's going to [LAUGHTER] come down again.
Is, is this modern vie- modern view of AI that started around 1990s, and, and- I,
I would argue that this, this modern AI,
the reason that we had this,
this new rise is due to two main things.
One is the idea of using probability in AI,
this is- this was not a thing that existed from early on,
this is actually due to efforts of Judea Pearl, who,
who was very adamant about using Bayesian networks in AI to model uncertainty.
So, so finally people were able to,
to use probability to bring ideas from probability to model uncertainty.
Because if you remember like expert-based systems,
they were super deterministic, like,
we didn't really have a way of talking about uncertainty.
But Judea Pearl, Pearl's idea was let's bring probability into this.
Let's actually talk about uncertainty,
and let's have our models and make predictions.
And then the second reason is machine learning, right?
So, so people started inventing support vector machines to tune parameters,
and then from that point on,
we started seeing the rise of neural networks and the fact that you have lots
of lots of data now and that can actually help us build better models.
So, so given that we have these two,
two big new viewpoints in,
in AI, we have started seeing all these new advances.
And then one point that I just want to make at the end of this is,
is that AI is really a melting pot of a bunch of ideas from different fields, right?
Like, not all of these are from pure AI, right?
Like if you think about Bayes rules,
it comes from pro- probabilities,
least squares come from astronomy, er,
first-order logic, or from logic,
maximum likelihood from statistics,
we have ideas from neuroscience, econ, optimization,
algorithmic- algorithms theory, control theory, like,
like we can think about value iteration that came from
Bowman, from the field of control theory.
So it's really, like,
if you think about artificial intelligence,
it just brings all these different ideas from these different fields
together to solve this AI problem, and in general,
I think it's a good idea to be mindful of that,
and to be open to that because,
um, the new insights and ideas really come from,
like, having this broader view of things,
and kind of the boundaries you put between different fields
are really superficial and don't really need to be there.
All right. So, so that was the history of AI, right?
Like rise of AI, downfall, rise, downfall.
And then we're on this,
this last rise now.
So, so let's just think a little bit about what have we achieved,
what are the cool things we've had in the past couple of years and then
what can go wrong and what should be- what should we worry about in the meantime.
Okay. All right.
So, so I think I've argued enough that AI is everywhere,
right, like, AI is being used in consumer service,
in advertising, in healthcare,
transportation, manufacturing, and, and AI is going to make decisions now, right,
because it has shown all these advances and because of that,
like, we are using AI these days to make decisions for us.
To make decisions for our education,
to make decisions for credit,
employment, advertising, healthcare, all of these different applications.
So, so if AI is making decisions then we should actually be
really careful about how AI is making decisions.
And the fact that we should,
we should think about, like,
all the possible things that could go wrong or
could not go wrong and understand the system fully,
uh, before making it make decisions for us.
So, so what are some of the advances? So one of the huge advances we have seen in
recent years is this Google Neural Machine Translation.
So, so the idea is this was
kind of a huge advancement when it comes to machine translation.
The idea is you could have a bunch of
different languages and you can have a way of translating let's say
from Korean to English and English to Korean and you
can have a lot of data on that, and that would be great.
And then maybe you can have Japanese to
English and English to Japanese and train on that.
And that's a lot of data and that's all great.
But then even, like, if,
if you put all of that data in the same,
like in a melting pot,
then what you can do is you can actually go from Korean to,
to Japanese without having any data that just goes
directly from Korean to Japanese and that's kind of exciting,
right, like, because you had,
you had like no data for that and if you're putting all of these together then you can
actually make a lot of advances in terms of language translation.
So this came out around 2016,
lots of excitement, language translation just became so much better after that.
There are still problems though.
Here is one of them. So here is the problem of bias, okay,
so let's say that
you're starting from a language,
like Hungarian that doesn't have gender.
And then you start from this language and then you go
to English, a language that has gender.
And, and this is a translation that you're gonna get.
You're gonna get she's a nurse and he is a scientist and,
and, and she- she's a baker,
and then of course he's a CEO, right.
So, so you're gonna get like all these, like,
gendered, ah, proving us here that there's,
there's no reason for- well, just,
just by looking at it and assuming that the algorithm is neutral,
there is no reason for it to pick up,
like, the- these particular genders.
But the reason that it's picking up these genders is this algorithm is trained on data,
our data is biased.
If our data is biased,
the algorithm has learned to pick up,
pick up patterns, so it's going to pick up this bias and sometimes even reinforce it.
So, so we're going to see all sorts of these weird behaviors.
I wouldn't say it's weird, it's biased behavior but we should actually
be aware of this if you are building these types of systems.
And even in addition to bias,
bias I can explain it,
you might get weird behaviors that are even harder to, to explain.
So you might have a text that looks like this like,
dog, dog, dog, dog, dog.
And then that could be just,
just translated to,
to, like, something else that is kind of crazy.
So, so, like, under- like understanding what goes on.
Um, a lot of times with these kind of closed form black-box systems are,
are a little challenging.
And I think there's a lot of research around trying to better understand and give
transparency to some of these systems and understand what goes on.
So that was, that was language, right, that was translation.
Another example is image classification.
So image classification has just become so much better over the years.
Around 2015, it just hit human performance.
So we have image classifiers
that are just much better than humans.
And, and that is amazing, right, like,
that is really exciting because,
because perception is a difficult problem.
If I can do image classification,
then I can use these systems on real s- real world like systems,
like my phone, or my autonomous car and that's really exciting,
right, but there are again a lot of issues around this.
One of the issues we actually discussed this in the first lecture
is the idea of having adversarial examples,
right, so I can have AlexNet,
a system that does image classification.
And an AlexNet is going to classify these images on the left perfectly fine.
That's a school bus, that's a temple, like,
it's gets going to classify them correctly.
But then what I can do is I can add some sort of noise to them.
And when you add this noise to this picture,
you're gonna get this third picture.
And that kind of looks like a school bus to me like I don't- I,
I can't tell the difference between th- the first and third picture.
But what's going to happen is that AlexNet is going to predict
ostrich for all of the pictures on,
on, on that side on the right.
So, so that's not great, right,
that having these adversarial examples it's
not that great because the system is not really robust, right, your,
your AI-based, uh, im- image classifier is not
very robust when it comes to just adding the- these sort of adversaries.
And then- and, and after this,
this work came out basically,
people started writing all sorts of
papers about how to create adversarial examples and how to be
robust towards that particular adversarial example and then
breaking that again and creating more robustness and a lot of back and forth.
One of my favorite papers actually around this area came out this year,
so this is from, eh, Shamir and others.
And what they have shown is for a specific type of neural network when you have ReLus,
what they've shown is, um,
what you can do is you can always make the system classify,
uh, classify the number in this case as something also.
So, so let me give a concrete example.
So this is a MNIST dataset,
I have numbers in it from 0, 1, 2, 3,
4, I have 10 numbers here, right, and,
and what Shamir and others have shown is you can pick this
seven and you have 10 classes so you,
you need at most 11 pixels.
So pick 11 pixels that they,
they pick the 11 pixels carefully.
So it's not any 11 pixels but pick
11 pixels and change it as much as their algorithm tell- tells you.
And then the seven is going to be classified as zero.
So you can make this seven be classified as any of these numbers,
0, 1, 2, 3, 4, 5,
and 6, and 7, and 8,
and 9 by just picking
the right 11 pixels to modify and they tell you like how much you modify, which is, like,
crazy because like gi- give me anything,
I'll create this adversarial example for you to, to,
to just mis-classify it as something else and you'd
only need 11 pixels because there were 10 classes here.
Um, so there are a bunch of assumptions that
I haven't really discussed here, like one of the assumptions
is the way you are modifying these pixels is unbounded in this picture.
So the, the greens and reds are just very high and very low.
So, so it is not actually between 0-255.
It's, it's numbers greater than 255 and less than 0.
But they've actually shown that if you're allowed to have more than 11 pixels,
let's say you have 20 pixels,
you could actually fit it between 0-255 to make it like a realistic, realistic figure.
Anyway, so lots of work around this, ah,
lots of exciting theory work and,
and practical work thinking about adversarial examples when it comes to images.
But, yeah, what are the implications of this?
Why are we so scared of this?
Because, well, these systems are going to run on,
on our phones doing image recognition on our cars,
doing recognition of other vehicles and they can easily be,
like, they can easily be attacked.
Um, a group at,
uh, Berkeley, Dawn Song's group,
what they did was they had these stop signs and they put stickers on stop signs.
Again, the stickers are at the right place like the place they wanted it to be.
But then the stop signs are,
are now classified as like a speed limit sign,
um, which is not what you want your autonomous car to detect.
Or, or here's another example, another work where you
have these pictures and you put thesefunny glasses on them.
And when you put the glasses on to pictures,
then they are classified as a celebrity's pictures.
So, so you can easily attack these systems,
not easily, but you can attack- systematically [LAUGHTER] attack these systems.
Ah, and that can actually affect the security of your,
your vehicle or your image recognition system. All right.
Another example that's pretty challenging is,
is around reading comprehension.
So, so what is reading comprehension?
So if you remember your SAT or a GRE like type of- type exams,
you have a text, you have a lot of text and you have
to read that text and you have to answer questions.
So you'd have a question like this.
Ah, so the number of new Huguenot colonists declined after what year?
So this is the thing you've got to answer.
So, so Google put out this system BERT, which is actually really amazing, it can,
can do this, this reading comprehension.
And, and BERT can answer this question perfectly,
it's gonna say 1700 and then that's great.
Um, but, but what people have shown is you could actually just add
an extra sentence at the end of
this text that has nothing to do with the rest of the text like,
like, it something to do, it has the word year in it
but it doesn't have to do anything with this particular question that's asked.
And now BERT is going to respond 1675.
So, so you can again easily trick these systems and the way that they are
tricked is just not the same way that humans are tricked.
And, and that is I guess weird to people.
And that's kind of expected but, um,
but that is something that we are dealing with these days.
All right. So another example,
so I'm basically gonna talk about a bunch of
examples throughout the lecture and the rest of the lecture.
So another example I wanted to briefly talk about is,
this idea of optimizing for clicks.
So, so is that a good thing?
Is that a thing we should be doing?
Right. So-so sometimes the objectives that we
have, the reward function we are writing for our system.
We know what it is.
We wanna do machine translation.
We know exactly what we want to do and it's very clear.
But sometimes it's actually not clear what we should be optimizing.
Right. Like Facebook let's say wants to make money,
like should they optimize for clicks?
Is that an ethical like rewards function to put in,
and what could be some of the effects of optimizing for clicks?
Let's say that I have a reinforcement learning algorithm.
I'm making this up. Let's say I have
a reinforcement learning algorithm that wants to optimize for clicks and,
um, I have my own Facebook account and it's optimizing clicks from Dorsa, right.
So this reinforcement learning algorithm what it can do is,
it can learn that, well,
maybe if I show outrageous articles to Dorsa,
Dorsa is more likely to click on
these outrageous articles and I'm gonna get
more rewards because I'm optimizing for clicks.
So that's all good, right. That's expected.
But another thing that the reinforcement learning algorithm by itself can figure out,
is that if I show outrageous articles to Dorsa,
Dorsa is going to become more and more outrageous,
and then I'm gonna get more clicks because then I'm going to show more articles,
and it'll be great.
And then, that's kind of amazing because
these systems are not interacting in a closed loop world.
They're interacting with other systems like humans, we're also changing,
we're also adapting, and this system through this RL algorithm by
itself could figure out how to change me to like more outrageous things.
And then we would end up in a situation where we are right now with very bipolar views, right?
Because- because you're optimizing for clicks.
So- so it's quite interesting to think about,
what are the objects we should be optimizing and what world are we dealing with?
We're not always in a Pacman role where we can control everything, right?
Usually these systems are running in a real society where there are people
being affected by them and their responses are going to change.
And there is, the changes in the responses is going to affect things even more.
So- so it's interesting to think about these feedback loops.
And speaking of humans, I think,
another thing- another question that comes up usually when it comes to robotics,
or when it comes to AI, is, well,
what is it that humans want?
Like in general, if I- even- even in the case of robotics it's a big problem.
I have a robot arm and I want my robot arm to pick up- pick up this object.
That's all I want, right.
This is the thing that me as a human wants, right?
I want a robot arm to pick up- the robot arm to pick up this mobile phone.
So back in the day,
this was called good engineering, right?
Good engineering was good engineers would write down the correct reward function,
the correct objective, and the robot arm would go and
pick up the object and everything would be great.
The problem is that doesn't always work, right?
It's really hard to write the correct reward function and get the robot to do that.
And because of that,
people these days are more interested in trying to do things that are around
imitation learning or things around preference based learning,
where you just try to learn from how humans do it.
Like how a human would do this as opposed to just a human sitting down and saying, well,
this is the object that I want you to pick up-pick up
the robot arm because- because the robot might end up doing very weird things.
Like an example of that that commonly comes up is,
this vacuum cleaner example.
Let's say- let's say you have a vacuum cleaner.
You have a robot vacuum cleaner that wants to clean your house.
And your objective for the vacuum cleaner is to suck up dirt.
That- that's all it needs to do, okay.
So you write your objective. Everything is great.
And one way that the vacuum cleaner could suck up
dirt is it could just go to a place, suck up dirt,
put it out, suck up dirt, put it out,
suck up dirt, put it out,
and just keep doing that, right.
Obviously, you didn't want your vacuum cleaner.
You don't- you don't want that vacuum cleaner because
you didn't want your vacuum cleaner to do that, right?
That wasn't the thing you were thinking.
But the objective of go suck up dirt,
could end up in that behavior.
Another behavior it could end up with is you could have your vacuum cleaner and
your vacuum cleaner by itself could just break its own like sensors,
so now it doesn't sense dirt.
Now you're good because there are no dirt around us because we can't see them.
I'm gonna close my eyes so I can't see the dirt.
So I'm not going to suck up anything.
So- so all of these are things around reward hacking.
Like if you- if you just write the reward function
that you think the robot should optimize,
it's not necessarily going to work.
And thinking about what are
some good objectives that you should optimize is actually a really difficult problem.
And this is something that I'm very interested in in my group,
we focus on that a lot.
Actually, another work that has recently came out on this is this work by,
this- this new book by Stuart J. Russell on- on Human Compatible.
And- and basically, what Stuart is kind of arguing
is, is the fact that there is a mismatch between what humans actually want,
what is the reward function that's in their head,
and- and what is it that the AI system or the robot thinks the human wants.
And- and those two are not always
the same thing and that could cause a lot of issues around it.
So interesting book, take a look.
All right, what else can go wrong?
So, um, generating fake content.
That was the thing that came out a couple years back.
So- so you could create like videos that just- or
images, uh, that- that look exactly like, uh, Obama in this case.
And- and you can just put fake content on that.
And- and that, that again raises an ethical question, right.
Just because you can build it,
should you build it or not.
Like- like we can build that.
We have the- we have the system to create ca- fake content.
It sounds fun, but- but should we do it just because we can't do it.
Another place that this question comes up,
and- and I do encourage you guys in general to think about that in
your future like when you can build something,
but should you build it? And- and yeah.
Another place this comes up is in autonomous weapons systems.
So, um, having,
like thinking about military and thinking about having autonomous weapons, right.
Like we would have- we could pote- we can have autonomous weapons these days, right?
We can have systems that automatically detect an enemy and-
and automatically just- just do- like just do the job, right.
Yeah, you- do the task.
So, um, should we do it?
Should we have autonomous weapons systems or does there need to be a person in the loop?
And if- so- so just like thinking about it,
like, let's say that,
yeah, we do not- we never want to have
autonomous weapons systems and we always want to have a person in the loop.
Well, why? Like- like what is it about the person that we want to be in the loop?
Like- like that kind of tells us that there is something about the person.
Maybe it's empathy, maybe it is something about what- what people know,
or what people have,
that the-the autonomous system doesn't have yet.
And just like understanding that,
I think, by itself is a very interesting problem.
And- and there's a whole debate around us like
of- of aut- autonomous weapon systems, should we have them?
If we don't have them, what if other countries have them?
Like how do we go about it?
Uh, should we put a moratorium on it,
and- and lots of debates around these types of systems.
So- so in general I do encourage you to think about some of
these ethical aspects of building AI systems.
All right, next up, fairness.
So, um, so fairness is a big problem. [NOISE]
I think a lot of you know this already, right.
So- so we might have a classifier that prob- like on your majority of dataset,
perfectly separates your majority of datasets, um,
such as the- the picture in the left,
and then you might have some data points from minority group.
And- and the classifier just does exactly the opposite thing for the minority group.
So- so if you- if you put all these datas together,
then you're probably going to get data- a classifier that looks like the first one,
and it's just not gonna work on the minority dataset.
And- and that is kind of, uh, that's a big problem,
especially when it comes to applications like let's say healthcare.
Like you might have different populations and a drug
might just act very differently in different populations.
And the question is, how should we address these fairness questions?
And one way to go about it is- is to think about our errors.
So- so, uh, you might have
two classifiers and both of them might give you 5% error.
Uh, but one of them could give you
5% random error and the other one could give you 5% systematic error.
And- and I think it's pretty important to think about if you're
getting systematic error or random error and what type of error
on what population are you getting an- and- and
that could address some of these questions around fairness.
There's a lot of work actually around fairness these days.
There's a- there's a conference around it,
uh, around fairness, accountability, and transparency.
This is worked by Moritz Hardt.
So if you're interested in this, uh,
take a look at some of the- some of the work from Moritz group.
Um, another example of fairness, I think,
we did talk about this in the overview lecture,
uh, is around this criminal risk assessment.
So, um, so Northpointe is a company that put out the system called COMPAS.
And what COMPAS does is- is it
predicts if- if a criminal is going to re-offending or not.
The risk of a criminal re-offending or not.
And it's going to give a score of 1-10.
So- so that's what the system does.
And- and they put out this system,
this system was actually being used.
And what happened was ProPublica which is a non-profit,
came out and did a study and ProPublica showed
that given that an individual did not re-offend,
African Americans were twice likely to be wrongly classified five or above, okay.
So- so that just seemed not fair.
So- so ProPublica put us- puts out this article being like,
well, the system is not fair.
Why are we using this?
Like- like it doesn't satisfy this fairness criteria.
And then Northpointe actually did further studies.
Northpointe did further studies and they showed that, well, they said, no,
our system is fair because we are looking at this definition of fairness.
Our definition of fairness is that given a risk score of seven,
60% of whites reoffended and 60% of blacks reoffended,
so we wanna make sure that we get the same percentage to be
fair and- and that's our fairness- fairness property.
We do satisfy that. And this kind of, uh, thes-
these two fairness definitions, um, kinda made a group of, uh, researchers, um, from,
actually, Stanford, Cornell, a bunch of different places to
start thinking about definitions of fairness.
And what they've actually shown is that these two definitions of fairness,
um, they are not going to be satisfied at the same time.
They're always going to go against each other.
You can't have both of them at the same time.
So- so then if that is the case,
then what is the right definition of fairness that- that we should use?
Right. If we can't have both of those at the same time,
then- then how- how do we make sure that we can use this system,
or should we ever use these systems again?
So, um, lots of interesting questions about formalizing fairness.
Omer Reingold, uh, here in the CS department,
works a lot around ide- ideas of fairness from the algorithmic side of things.
So if you're interested in that,
you can take Omer's classes,
learn- learn from- about that.
And- and kind of going back to this idea of are algorithms neutral.
Like when you talk to people who haven't taken
necessarily algorithm classes or AI classes,
they usually think, well, yeah right?
Algorithm's gotta be neutral,
like they're doing math, they gotta be neutral.
But as you have seen already,
they're not really neutral because by
design we really want our algorithms to pick up patterns.
That's what they're good at.
They're good at picking up patterns and- and
biases and all sorts of weird things that we see,
uh, in our data. They- they're just in our data.
There are patterns in our data,
and these algorithms just will pick them up and- and even reinforce them at times.
And- and that's why we see bias in our algorithms and all of these issues
around fairness and- and security and all these other things, uh, in our data.
And another problem that comes up is
this feedback loop that I was talking about earlier, right.
So- so if- if algorithms are picking up patterns,
well, they're putting out, you know,
those patterns, if they have biases,
they're putting out those biases in a world where there are humans,
and those humans are observing those biases and can
get even more biased and give more biased data.
And- and this could be like this negative feedback loop that could go forever.
So again, we gotta be really careful about what we were putting out
and- and what it is- like how it is affecting the bigger society.
Next stop is privacy.
I guess I have like a couple of more things around
these and- and after that I'll- I'll wrap up.
Um, another- another thing- another issue in general is- is privacy, right?
So we're using a lot of data and in- in a lot of our algorithms,
and- and in general, uh,
some of them could be- could be sensitive data and we
don't want to- we don't want to actually reveal that sensitive data.
So- so be- so- so to address that- one way to address that is, instead of putting
out the actual data,
putting out the right statistics that gives us the right information.
So for example, you might want to com- uh,
compute the average statistics.
And like if- if you're asking if someone has cancer or not,
instead of getting the yes-no answer,
you could just- you might just need
the average statistics and that would just be enough for you.
So- so- so in general when you're collecting data,
you shou- you should- you could randomize your data or you could change your data
so- so you can get the average statistics as one way of protecting privacy.
Another way of protecting privacy is in general randomized responses.
So- so you might have a question of, do you have a sibling?
So- so that is a question you can ask a user.
And the user might not wanna reveal exactly if they have a sibling or not.
So- so one way of responding that,
is the user could flip two coins.
And then if both of them can come- come heads,
then they can say answer yes-no randomly.
Otherwise, they can answer yes-no truthfully.
So- so based on the answer that you get from a particular user,
you wouldn't be able to tell if
that particular user is- is going to have- it has a sibling or not.
But you could actually compute the- the true probability of that.
Because now you have observed this probability,
three-fourths of the time their- by true probability,
they're telling you the truth, one-fourth of the time they- they're telling you randomly.
So- so then you have this observed probability and then
from that you can recover the true probability,
and that is probably enough for like
the type of data that you- you- you need to deal with.
So- so randomized responses in general could be
one way of going about some of these privacy issues.
Um, another issue that comes up is- is causality.
So, um, this a little bit and- and a variable based models right,
so- so you might wanna look at the effects of something.
Let's say you're- you want to look at the effects of a treatment on survival.
And this is your data.
So you have, for untreated patients,
80% of them survived,
and for treated patients,
30% survived. This is your data.
So the question is,
does treatment help or not?
How many of you think treatment helps?
Treatment helps. Think carefully.
[LAUGHTER] So- so the answer is actually- who knows?
[LAUGHTER] Because, well, if you think about it,
the- there sick people are probably more likely to take treatment, right?
Like i- i- if sick- sick people are more likely to undergo treatment,
then- then you can't really like takes
this data like at it- at its face and- and- and say,
well, treatment helped or didn't help.
Beca- because your- your data actually there's this-
this extra causality that you didn't really consider the fact that, well,
those people who took treatment they were sick,
so you have to actually consider that,
how the sickness is it going to affect and the- the rate of survival or not?
And then, finally the last- I think this was the last thing I want to briefly talk
about is- is this idea of interpretability versus accuracy, right?
So- so you've seen kind of this rise of neural networks in a lot of
applications and most of them are not safety critical applications.
We haven't really seen like things like neural networks and safety critical applications.
I guess you've seen it in cars and we- and you've started seeing in autonomous cars.
But let's say airplanes or- or like
other types of safety critical systems, health care systems.
And- and one question that always comes up is,
should we use these systems in safety critical settings?
Because as we're using them,
we're gonna lose interpretability.
So- so there's this work by Michael Cook in the first group, where, uh,
they're basically looking at air traffic control and- and- and
they're looking at the- the system that runs on aircraft.
And then previously, it was basically
a bunch of rules that the system need- needed to follow,
but it was interpretable.
Like- like they could actually interpret it and understand what it does.
Uh, and- and the systems- aircraft systems would use that.
But Michael has been working with this new system
called ACAS and ACAS-Xu where,
uh, they are basically trying to replace that with just,
let's say, a POMDP,
a partially observable Markov decision process that- that does
the same job but it's not necessarily
do- it doesn't have necessarily the same level of interpretability.
But it's pretty accurate and you can prove that it's even accurate.
It's not even a neural network, right?
I- it is a thing that you can actually like enumerate.
And- and the question is,
what are we willing to put in on our safety critical systems?
If you lose transparency,
if you lose interpretability,
are we still willing to like put these systems
that we think theyr'e statistically more accurate.
Um, and in general,
how can we increase interpretability and
transparency of- of some of these systems that we are building?
Because that is useful when we come- we think about these systems.
So- so AI is important, [NOISE] I think.
I think I've convinced you guys that AI is important.
And then, um, all these different governments also think that AI is important.
In 2016, uh, the White House put out, uh,
an article about some of the directions that we should invest money in,
and a lot of them were just around AI.
So making long-term investments in AI research,
thinking about human AI collaboration,
thinking about ethical, and legal,
and societal implications of AI,
safety and security of AI systems.
So all of these things that we have been discussing so
far are really challenging problems and then
everyone's excited about them and everyone wants to
put in- put in a lot of energy and time and money in it.
And- and in this document, uh, well, this document said,
big data analytics have the potential to eclipse
long-standing civil rights protections in how
personal information is used in all sorts of applications like,
housing, credit, employment, and so on.
And Americans relationships with data should expand not diminish their opportunities.
And- and some of the things that we have discussed so far,
right, like biases, fairness, safety,
all of these issues are not necessarily satisfying this last sentence, right,
like if- if you're building these- these systems,
we should actually be really careful about some of these implications.
And- and as I was saying earlier,
like around this there is a new conference.
Uh, there's this FAT ML conference around
fairness, accountability, and transparency.
And kind of the guidelines of- of- of this-
this new community that- that's being built around AI,
is that we gotta think about the fact that there's
always a human that's behind these algorithms.
So there's always a human ultimately responsible behind what is going to happen.
And then you can't just say, well,
the algorithm did it, right?
Like in- in general, that's just like
the wrong way of going is because there was a human designer,
one of you guys, one of us, right,
that's going to write these algorithms.
And- and I do really want you guys to think
about some of these principles as you go further in
your- in your career and- and you think about building these sort of AI algorithms.
And just to end on a more positive note.
Um, there's enormous potential for
actually positive impact fo- for AI systems and- and please just use it responsibly.
With that, I wanna thank you all guys for
this exciting quarter and please fill out the surveys,
uh, on Access.
Thanks.