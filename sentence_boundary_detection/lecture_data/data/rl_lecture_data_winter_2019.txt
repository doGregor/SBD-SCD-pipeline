 Hi everybody, um, I'm Emma Brunskill.
Um, I'm an assistant professor in Computer Science and welcome to CS234, um,
which is a reinforcement learning class, um,
which was designed to be sort of an entry-level masters or
PhD student in an introduction to reinforcement learning.
So, what we're gonna do today is I'm gonna start with
just a really brief overview of what is reinforcement learning.
Um, and then we're gonna go through
course logistics and when I go through course logistics,
I'll also pause and ask for any questions about logistics.
Um, the website is now live and so that's
also the best source of information about the class.
That and Piazza will be the best source of information.
Um, uh, so I'll stop there when we get to
that part to ask if there's anything that I don't go over that
you have questions about and if you have questions about
the wait-list or any particular things relating to your own circumstance,
feel free to come up to me at the end.
Um, and then the third part of the class is gonna be where we start
to get into the technical content that we're thinking about,
uh, an introduction to sequential decision making under uncertainty.
Um, just so I have a sense before we get started,
who here has taken a machine learning class?
All right. Who here has taken AI?
Okay. So, a little bit less but most people. All right.
Great. So, probably everybody here has seen a little bit about reinforcement learning.
Um, varies a little bit depending on where you've been at.
We will be covering stuff starting from
the beginning as if you don't know any reinforcement learning, um,
but then we'll rapidly be getting to other content, um,
that's beyond anything that's covered in at least other Stanford related classes.
So, reinforcement learning is concerned with this really foundational issue of
how can an intelligent agent learn to make a good sequence of decisions?
Um, and that's sort of a single sentence that summarizes what reinforcement learning is.
Do we know what we'll be covering during this class?
But it actually encodes a lot of really important ideas.
Um, so the first thing is that we're really concerned now with sequences of decisions.
So, in contrast to a lot of what is covered in,
uh, machine learning, we're gonna be thinking about agents,
intelligent agents or an intelligent agent in
general that might or might not be human or biological.
Um, and how it can make not just one decision but a whole sequence of decisions.
We're gonna be concerned with goodness.
In other words, we're gonna be interested in- the,
the second thing is how do we learn to make good decisions,
um, and what we mean by good here is some notion of optimality.
We have some utility measure over the decisions that are being made.
Um, and the final critical aspect of reinforcement learning is the learning, but, um,
that the agent doesn't know in advance how its decisions are gonna affect
the world or what decisions might necessarily be associated with good outcomes,
and instead it has to acquire that information through experience.
So, when we think about this.
This is really something that we do all the time.
We've done it since we were babies.
We try to figure out,
how do you, um,
sort of achieve high reward in the world and there's a lot of
really exciting work that's going on in neuroscience and psychology,
um, that's trying to think about this same fundamental issue
from the perspective of human intelligent agents.
And so I think that if we wanna be able to solve AI,
um, or make significant progress,
we have to be able to make significant progress
in allowing us to create agents that do reinforcement learning.
So, where does this come up?
There's this, um, nice example from Yael Niv who's,
uh, an amazing sort of psychologist and neuroscience researcher over at Princeton.
Um, where she gives us an example of this sort of primitive creature,
um, which evolves as following during its lifetime.
So, when it's a baby,
it has a primitive brain and one eye and it swims around and it attaches to a rock.
And then when it's an adult, it digests its brain and it sits there.
And so maybe this is some indication that the point of intelligence or
the point of having a brain in at least in part is helping to guide decisions,
and so that once all the decisions and the agent's life
has been completed maybe we no longer need a brain.
So, I think this is, you know,
this is one example of a biological creature but I think it's
a useful reminder to think about why would an agent
need to be intelligent and is it somehow
fundamentally related to the fact that it has to make decisions?
Now of course, um,
there's been a sort of really a paradigm shift in reinforcement learning.
Um, around 2015, um,
in the Neurex Conference which is one of the main machine learning conferences,
David Silver came and went to a workshop and presented
these incredible results of using reinforcement learning to directly control Atari games.
Now, these are important whether you like video games or not.
Um, video games are a really interesting example of
sort of complex tasks that take human players a while often to learn.
We don't know how to do them in advance.
It takes us at least a little bit of experience.
And what the really incredible thing about this example was, this is, uh,
Breakout, is that the agent learns to play directly from pixel input.
So, from the agent's perspective,
they're just seeing sort of these colored pixels
coming in and it's having to learn what's
the right decisions to make in order to learn to play
the game well and in fact even better than people.
So, this was really incredible that this was possible.
Um, when I first started doing reinforcement learning,
a lot of the work was really focused on very artificial toy problems.
Um, a lot of the foundations were there but these sort of
larger scale applications we're really lacking.
And so I think in the last five years,
we've seen really a huge improvement, um,
in the types of techniques that are going on in
reinforcement learning and in the scale of the problems that can be tackled.
Now, it's not just in video game, um, playing.
Uh, it's also in things like robotics, um,
and particularly some of my colleagues up at University of,
um, California Berkeley, um, uh,
had been doing some really incredible work on robotics
and using reinforcement learning in these types of scenarios,
um, to try to have the agents do grasping,
fold clothes, things like that.
Now, those are some of examples if you guys have, um,
looked at reinforcement learning before,
are probably the ones you've heard about.
You probably heard about things like video games or robotics.
Um, but one of the things that I think is really exciting is that, uh,
reinforcement learning is actually applicable to a huge number of domains,
um, which is both an opportunity and a responsibility.
So, in particular, um,
I direct the AI for human Impact Lab here at Stanford and one of the things that we're
really interested in is how do we use
artificial intelligence to help amplify human potential?
So, one way you could imagine doing that is through something like educational games.
Where the goal is to figure out, um,
how to quickly and effectively teach people how to learn material such as fractions.
Another really important application area is health care.
Um, this is sort of a cutout, um,
of looking at seizures that some work that's been done by
Joel Pineau up at McGill University
and I think there's also a lot of excitement right now thinking
about how can we use AI in a particular reinforcement learning,
um, to do things like to interact with things like
electronic medical records systems and use them to inform patient treatment.
There's also a lot of recent excitement and thinking about how we
can use reinforcement learning and lots of other applications
kind of as an optimization technique for
when it's really hard to solve optimization problems.
And so this is arising in things like
natural language processing in vision and a number of other areas.
So, I think if we have to think about what are the key aspects of reinforcement learning,
they probably boil down to the following four,
and these are things that are gonna distinguish it
from other aspects of AI and machine learning.
So, reinforcement learning from my sentence
about that we're learning to make good decisions under uncertainty,
fundamentally involves optimization, delayed
consequences, exploration and generalization.
So, optimization naturally comes up because we're interested in good decisions.
There's some notion of relative different types of decisions that we can make,
um, and we want to be able to get decisions that are good.
The second situation is delayed consequences.
So, this is the challenge that the decisions that are made now.
You might not realize whether or not they're a good decision until much later.
So, you eat the chocolate Sunday now and you don't realize until an hour later that
that was a bad idea to eat all two courts of ice cream or, um,
you in the case of things like video games like Montezuma's Revenge,
you have to pick up a key and then much later you realize that's helpful
or you study really hard now and
Friday night and then three weeks you do well on the midterm.
So, one of the challenges to doing this is that
because you don't necessarily receive immediate outcome feedback,
it can be hard to do what is known as
the credit assignment problem which is how do you figure
out the causal relationship between the decisions
you made in the past and the outcomes in the future?
And that's a really different problem than we tend to see in most of machine learning.
So, one of the things that comes up when we start to
think about this is how do we do exploration?
So, the agent is fundamentally trying to figure
out how the world works through experience in much of
reinforcement learning and so we think about the agent as
really kinda being the scientist of trying things out in the world
like having an agent that tries to ride a bicycle and then learning about how
physics and riding a balanced bike works by falling.
And one of the really big challenges here is that data is
censored and what we mean by
censoring in this case is that you only get to learn about what you try to do.
So, all of you guys are here at Stanford clearly that was the optimal choice.
Um, but you don't actually get to
figure out what it would have been like if you'd went to
MIT it's possible that would've been a good choice as well,
but you can't- you,
can't experience that because you only get to live one life and so you
only get to see that particular choice you made at this particular time.
So, one question you might wonder about is, um,
you know, policy, what we're gonna,
we're gonna talk a lot about policies.
Policies, decision policies is gonna be some mapping from experiences to a decision.
And you might answer why,
we, this needs to be learned.
So, if we think about something like Deep Mind,
um, Atari playing game.
What it was learning from here,
is it was learning from pixels.
So, it was essentially learning from the space of images what to do next.
And if you wanted to write that down as a program,
a series of if then statements,
it would be absolutely enormous.
This is not tractable.
So, this is why we need some form of generalization and
why it may be much better for us to learn from data directly,
as well as to have some high level representation of the task.
So, that even if we then run into
a particular configuration of pixels we've never seen before,
our agent can still know what to do.
So, these are sort of the four things that really make up reinforcement learning,
at least online reinforcement learning and why are they
different than some other types of AI and machine learning.
So, another thing that comes up a lot in artificial intelligence is planning.
So, for example, the Go game,
um, is, can be part of as a planning problem.
So, what does planning involve?
Involves optimization, often generalization and delayed consequences.
You might take a move and go early and it might not be immediately obvious if that was
a good move until many steps later but it doesn't involve exploration.
The idea and planning is that you're given a model of how the world works.
So, your given the rules of the game,
for example, and you know what the reward is.
Um, and the hard part is computing what you should do given the model of the world.
So, it doesn't require exploration.
And supervised machine learning versus reinforcement learning.
It often involves optimization and generalization but frequently it doesn't invo-,
involve either exploration or delayed consequences.
So, it doesn't tend to involve exploration because
typically in supervised learning you're given a data set.
So, your agent isn't collecting its experience or data about the world instead it's given
experience and that it has to use that to say in for whether an image is a face or not.
Similarly, um, it's typically making
essentially one decision like whether this image is a face or not
instead of having to think about making decisions
now and then only learning whether or not those were the right decisions later.
Unsupervised machine learning awful,
also involves optimization and generalization but generally does not involve
exploration or delayed consequences and typically you have no labels about the world.
So, in supervised learning,
you often get the exact label for the world like this image really is,
has a, contains a face or not.
Um, in unsupervised learning you normally get no labels about the world
and an RL you typically get something kind of halfway in between those which you get a,
a utility of the label you put.
So, for example, you might decide that there's
a face in here and it might say, ''Okay, yeah,
we'll give you partial credit for that,''
because maybe there's something that looks sort of like a face.
But you don't get the true label of the world or maybe you decide to go to Stanford,
um, and then you don't know.
And you're like okay that was a really great experience but I
don't know if it was, ''the right experience.''
Imitation learning which is something that we'll probably touch on
briefly in this class and is becoming very important,
um, is similar, um, but a little bit different.
So, in, uh, it involves optimization, generalization,
and often delayed consequences but
the idea is that we're going to be learning from experience of others.
So, instead of our intelligent agent getting to ex-,
take experiences, um, from the world and make its own decisions,
it might watch another intelligent agent which might be a person,
make decisions, observe outcomes and then
use that experience to figure out how it wants to act.
There'll be a lot of benefits to doing this but it's a little bit
different because it doesn't have to directly think about the exploration problem.
Imitation learning and I just want to spend a little bit more time on
that one because it's become increasingly important.
So, to my knowledge,
it was first really sort of popularized by Andrew Ng,
um, who's a former professor here, um,
through some of his helicopter stuff
where he was looking at expert flights together with Pieter Abbeel,
whose a professor over at Berkeley, um,
to see how you could imitate very quickly,
um, experts flying toy helicopters.
And that was one of sort of the first kind of
major application successes of invitation learning.
It can be very effective.
There can be some challenges to it because essentially,
if you get to observe one trajectory,
let's imagine it's a circle of a helicopter flying and
your agent learns something that isn't exactly the same as what the expert was doing,
that you can essentially start to go off that path and ven-,
venture into territory where you really don't know what the right thing is to do.
So, there's been a lot of extensive work on imitation learning that's sort of combining
between imitation learning and reinforcement learning that ends up being very promising.
So, in terms of how we think about trying to do reinforcement learning,
we can build on a lot of these different types of techniques.
Um, and then also think about some of the challenges that are unique to
reinforcement learning which involves all four of these challenges.
And so these RL agents really need to explore
the world and then use that exploration to guide their future decisions.
So, we'll talk more about this throughout the course.
Um, a really important question that comes up is where do these rewards come from,
where is this information that the agents are using to
try to guide whether or not their decisions are good,
um, and who is providing those and what happens if they're wrong?
And we'll talk a lot more about that.
Um, we won't talk very much about
multi agent reinforcement learning systems but that's also a really important case,
as well as thinking about game theoretic aspects, right.
So, that's just a really short overview about some of the aspects of
reinforcement learning and why it's different
than some of the other classes that you might have taken.
Um, and now we're gonna go briefly through course logistics and then start sort of
more of the content and I'll pause after course logistics to answer any questions.
In terms of prerequisites, um,
we expect that everybody here has either taken an AI class or
a machine-learning class either here at
Stanford or the equivalent to another institution.
And if you're not sure whether or not you have the right background for the class,
feel free to reach out to us on Piazza and we will respond.
Um, if you've done extensive work in sort of related stuff,
it will probably be sufficient.
In general, we expect that you have basic Python proficiency,
um, and that you're familiar with probability,
statistics, and multi-variable calculus.
Um, things like gradient descent,
loss derivatives, um, those should all be very familiar to you.
Um, and I expect that most people have probably heard of MDPs,
um, before, but it's not totally critical.
So, this is a long list [LAUGHTER] but
I'll go through it slowly because I think it's pretty important.
So, this is what are the goals for the class,
what are the learning objectives?
So, these are the things that we expect
that you guys should be able to do by the time you
finish this class and that it's our role to help
you be able to understand how to do these things.
So, the first thing is that it's important to be able to define the key features of
reinforcement learning that distinguish it from other types of AI and machine learning,
um, frames of problems.
So, that's what I was doing a little bit of so far in this class to figure out,
how does this distinguish this.
How does RL distinguish itself from other types of pro-, problems.
So, related to that,
um, for most of you,
you'll probably not end up being academics,
um, and most of you will go into industry.
And so, one of the big challenges when you do that is that when you're faced with
a particular problem from your boss or when you're giving a problem to one of your, um,
supervisees is for them to think about whether or
not it should be framed as a reinforcement learning problem,
um, and what things are applicable to it.
So, I think it's very important that by the end of this class,
that you have a sense of if you're given a real-world problem like
web advertising or patient treatment or robotics problem, um,
that you have a sense whether or not it is useful to formulate it as
a reinforcement learning problem and how to write
it down in that framework and what algorithms are relevant.
Um, during the class, uh, we'll also be
introducing you to a number of reinforcement learning algorithms,
um, and you will have the chance to implement those in code
including deep reinforcement learning cla-, uh, problems.
Another really important aspect is if you're trying
to decide what tools to use for a particular,
say robotics problem or health care problem, um,
is to understand which of the algorithms is likely to be beneficial one and why.
And so, in addition to things like empirical performance,
I think it's really important to understand,
generally, how do we evaluate algorithms.
Um, and can we use things like theoretical tools like regret sample complexity, um,
as well as things like computational complexity to
decide which algorithms are suitable for particular tasks.
And then the final thing is
that one really important aspect of
reinforcement learning is exploration versus exploitation.
This issue that arises when the agents have to figure out what decisions
they wanna make and what they're gonna learn
about the environment by making those decisions.
And so, by the end of the class,
you should also be able to compare different types of techniques for doing
exploration versus exploitation and what are the strengths and limitations of these.
Does anyone have any questions about what these learning objectives are.
Okay. So, we'll have three main assignments for the class,
um, will also have a midterm.
Um, we'll have a quiz at the end of the class,
um, as well as a final project.
The quiz is a little bit unusual.
Um, so, I just want to spend a little bit of time talking about it right now.
The quiz is done on both individually and in groups.
Um, the reason that we do this is because we want
a low stakes way to sort of have
people practice with the material that they learn in the second half of the course.
Um, in a way that's sort of fun engaging and really tries to get
you to think about it and also learn from your peers.
Um, and so, we did it last year and I think a number of people who are
a little bit nervous about how it would go before and then ended up really enjoying it.
So, the way that the quiz works is it's a multiple choice quiz.
At the beginning and everybody does it by
themselves and then after everybody has submitted their answers,
then we do it again in groups that are pre-assigned by us.
And the goal is that you have to get everyone to decide on what
the right answer is before you scratch off and see what the correct answer is.
And then we grade it according to, um,
whether you scratched off the right answer,
correctly first or not.
You can't do worse than your individual grade.
So, doing it in a group can only help you.
Um, and for SCPD students,
they don't do it in groups.
So, they just write down justifications for their answers.
Again, um, it's a pretty lightweight way to do assessment, um,
the goal is that you sort of have to be able to articulate why you believe that
answers are the way they are and discuss them in small groups and they use that informa-,
um, use that to figure out what the correct answer is.
Um, the final project is paired pretty
similar to other projects that you guys have done in other classes.
Um, it's an open-ended project.
It's a chance to, uh, reason about, um, and,
and think about reinforcement learning, uh, stuff in more depth.
We will also be offering a default project that will be announced
over the next couple of weeks before the first milestone is due.
If you choose to do the default project, your breakdown,
because you will not need to do a proposal or milestone,
will be based on the project presentation in your assignment, uh, write up.
Since we believe that, um,
you guys are all of each other's best resource,
um, we use Piazza, um,
that should be used for pretty much all class communication
unless it's something that's sort of
a private or sensitive manner in which case of
course please feel free to reach out to the course staff directly,
ah, and for things like lectures and
homework and project questions pretty much all of that should go through Piazza.
For late day policy,
we have six late days, ah,
for details you can see the webpage and for
collaboration please see the webpage for some of the details about that.
So before we go on to the next part,
do I have any questions about logistics for the class?
Okay, let's get started.
Um, so, we're not going to do
an introduction to sequential decision-making under uncertainty,
a number of you guys who have seen some of this content before,
um, we will be going into this in prime,
more depth than you've seen for some of
this stuff including some theory not theory today but in other lectures,
and then we'll also be moving on to content that will
be new to all of you later in the class.
So, sequential decision-making under uncertainty.
Um, the fundamental that we- thing that we think about in
these settings is sort of an interactive closed-loop process,
where we have some agent,
an intelligent agent hopefully that is taking actions that are
affecting the state of the world and then it's giving back an observation and a reward.
The key goal is that the agent is trying to maximize the total expected future reward.
Now, this expected aspect, um,
is going to be important because sometimes the world itself will be
stochastic and so the agent is going to be maximizing things in expectation,
this may not always be the right criteria, um,
this has been what has been focused on for the majority of reinforcement learning
but there's now some interest in thinking about distribution honorable,
RL and some other aspects.
One of the key challenges here is that it can require balancing between
immediate and long-term rewards and that it might
require strategic behavior in order to achieve those high rewards,
indicating that you might have to sacrifice
initial higher rewards in order to achieve a better awards over the long-term.
So as an example,
something like web advertising might be that you have
an agent that is running the website and it
has to choose which web ad to give to a customer,
the customer gives you back an observation such as how long they spent on the web page,
and also you get some information about whether or not they click on an ad,
and the goal is to say how people click on ads the most.
So you have to pick which ad to show people so that they're going to click on ads.
Another example is a robot that's unloading a dishwasher,
so in this case the action space of the agent might be joint movements.
The information that agent might get backwards are camera image of
the kitchen and it might get a plus one reward if there are no dishes on the counter.
So in this case it would generally be a delayed reward,
for a long time there're going to be dishes on the counter, er,
unless it can just sweep all of them off and have them crash onto the floor,
which may or may not be the intended goal of the person who's writing the system.
Um, and so, it may have to make a sequence of
decisions where it can't get any reward for a long time.
Another example is something like blood pressure control, um,
where the actions might be things like prescribed exercise or
prescribed medication and we get
an observation back of what is the blood pressure of the individual.
Um, then the reward might be plus one if it's in the- if
the blood pressures in a healthy range maybe
a small negative reward if medication is
prescribed due to side effects and maybe zero reward otherwise.
[NOISE] So, let's think about another case,
like some of the cases that I think about in my lab like having an artificial tutor.
So now what you could have is you could have a teaching agent,
and what it gets to do is pick an activity,
so pick a teaching activity.
Let's say it only has two different types of teaching activities to give, um,
it's going to either give an addition activity or
a subtraction activity and it gives this to a student.
Then the student either gets the problem right, right or wrong.
And let's say the student initially does not no addition or subtraction.
So, it's a kindergartner that student doesn't know anything about
math and we're trying to figure out how to teach the student math,
and that the reward structure for the teaching agent is they get a plus
one every time a student gets something
right and they get a minus one if the student gets it wrong.
So, I'd like you to just take a minute turn to somebody
nearby and describe what you think an agent that's trying to learn,
to maximize its expected rewards would do in this type of case,
what type of problems it would give to
the student and whether or not that is doing the right thing.
[NOISE].
Let me just- let me just clarify here,
and let me just clarify here [NOISE].
Let me just clarify here is that let's assume that for
most students addition is easier than subtraction, so that,
like what it says here that
the problem even though the student doesn't know either of these things
that the skill of learning addition is
simpler for a new student to learn than subtraction.
So what would, what might happen under those cases?
Is there maybe we want to, raise their hand and tell me what they and somebody
nearby them was thinking might happen for an agent in this scenario?
[NOISE].
The agent would give them really easy addition problems, that's correct.
That's exactly actually what happened.
There's a nice paper from approximately 2,000 with Bev Wolf,
which is one of the earliest ones but I know
where they're using reinforcement learning to
create an intelligent tutoring system and the reward was for the agent to,
to give problems to the student in order to get them correct.
Because, you know, if the students getting things correct them they've learned them.
But the problem here is with that reward specification
what the agent learns to do is to give really easy problems,
and then maybe the student doesn't know how to do those initially but
then they quickly learn how and then there's no incentive to give hard problems.
So this is just sort of a small example of what is known as reward hacking,
[LAUGHTER] which is that your agent is gonna
learn to do exactly what it is that you tell him to
do in terms of the rewards function that you specify and yet in reinforcement learning,
often we spend very little of our time
thinking very carefully about what that reward function is.
So, whenever you get out and test for
the real world this is the really really critical part.
But normally, it is the designer that gets to pick what the reward function is,
the agent is not having intrinsic internal reward and so depending on how you specify it,
the agent will learn to do different things. Yeah, was there question in the back?
In this case, it seems like the student will also be
RL agent and that like in real life the student,
so what we asked for her questions
so techniques to approach or is it okay that we ignore that part?
So, the question was to say well, you know,
we also think that people are probably
reinforcement learning agents as well and that's exactly correct,
and maybe they would start to say, "Hey,
I need to get harder questions,
or be interactive in this process."
For most of this class we're going to ignore the fact that the world
that we interact with itself might also be an RL agent,
in reality it's really critical, um,
sometimes this is often considered in an adversarial way like for game theory,
I think one of the most exciting things to me is when we
think about it in a cooperative way?
Um, so, who here has heard about the sub-discipline of machine teaching?
Nobody yet, so, er,
it's a really interesting new area that's been around for maybe 5-10 years,
some a little bit beyond that.
One of the ideas there is,
what happens if you have two intelligent agents that are
interacting with each other where they know that each other's trying to help them?
Er, so there's a really nice classic example
from sorry for those of you that aren't so familiar with machine learning but,
imagine that you're trying to learn a classifier to decide where
along this line things are either positive or negative.
So in general you're going to need some amount of samples,
samples if you, uh,
wear that sort of the number of points on
the line where you have to get positive or negative labels.
Um, if you're in an active learning setting,
generally I think you can reduce that to roughly log n
by being strategic about asking people to label particularly points in a line,
one of the really cool things for machine teaching is that,
if I know you are trying to teach me where to divide this line,
you'll only need one point or at most two points essentially constant, right?
Because, if I'm trying to teach you,
there's no way I'm just going to randomly label things.
I'm just gonna label you a single plus and
a minus and that's gonna tell you exactly where the line goes.
So that's one of the reasons why if
the agent knows that the other agent is trying to teach them something,
it can actually be enormously more efficient than what we normally think of for learning.
And so, I think there's a lot of potential for machine teaching to be really effective.
But all that said, we're going to ignore most of that for the course,
if it's something you want to explore in your project, you're very welcome to.
There's a lot of connections with reinforcement learning.
Okay. So, if we think about this process in general um,
if we think of sort of a sequential decision making process, we have this agent.
We're going to think about almost always about there being discreet timer.
So, agent is gonna make a decision,
it's gonna affect the world in some way,
it's gonna see the world, it's gonna give some new observation and a reward.
The agent receives those and uses it to make another decision.
So, in this case when we think about a history,
what we mean by history is simply the sequence of previous actions that the agent took,
and the observations and rewards it received.
Then the second thing that's really important is to define a state-space.
Again, often when this was first discussed,
this is sort of thought about is some immutable thing.
But whenever you're in a real application,
this is exactly what you have to define,
is how to write down the representation of the world.
Um, what we're going to assume in
this class is that the state is a function of the history.
So, there might be other aspects of- there might be
other sensory information that the agent would like
to have access to in order to make its decision.
But it's going to be constrained to the observations is received so far,
the actions is taken, and the rewards is observed.
Now, there's also gonna be some real-world state. So, that's the real world.
The agent doesn't necessarily have access to the real world.
They may have access only to a small subset of the real world.
So, for example as a human,
right now, I have eyes that allow me to look forward.
You know, roughly 180 degrees.
Um, but I can't see behind my head.
But behind my head is still part of the world state.
So, the world state is the real world,
and then the agent has its own state space it uses to try to make decisions.
So, in general, we're gonna assume that it has some function of the history.
Now, one assumption that we're gonna use a lot in this class
which you guys have probably seen before is the Markov assumption.
The Markov assumption simply says that we're going
to assume that the state used by the agent uh,
is a sufficient statistic of the history,
and that in order to predict the future,
you only need to know the current state of the environment.
So, it's simply basically indicates that
the future is independent of the past given the present,
if in the present, you have the right aggregate statistic.
[NOISE] So, as a couple of examples of this,
yeah? Question name and-.
Would you just explain maybe with
an example the difference again between the state and the history?
Like I'm having trouble to differentiate.
Yeah. So, the state, um, uh,
if we think about something like uh, um, a robot.
Um, so let's say you have a robot that is walking down a long corridor.
Okay. Let's say there's two long corridors.
Okay. So, your robot starts here.
This is where your robot starts,
and it tries to go right,
right, and then it goes down, down, down.
Okay. Let's say its sensors are just that it can observe whether in front of it um,
uh, um, whether there is a wall on any of its sides.
So, it can- the observation space of the robot is simply is there
a wall on any side-on each of its four sides?
I'm sorry, it's probably a little bit small on the back.
But the agent basically has, you know,
some sort of local amount via laser range finder or something like that.
So, it knows whether or not there's a wall immediately around it,
that has been immediately around it square, and nothing else.
So, in this case, what the agent would see is that initially the wall looks like this,
and then like this, and then like this, and then like this.
The history would include all of this.
But it's local state is just this.
So, local state could just be the current observation.
That starts to be important when you're going down
here because there are many places that looked like that.
So, if you keep track of the whole history,
the agent can figure out where it is.
But if it only keeps track of where it is locally,
then a lot of partial aliasing can occur.
So, I put up a couple of examples here.
So, in something like hypertension control,
you can imagine the state is just the current blood pressure,
um and your action is whether to take medication or not.
So, current blood pressure meaning like you know,
every second for example what is your blood pressure?
So, do you think this sort of system is Markov?
I see some people shaking their heads. Almost definitely not.
Almost definitely there are other features that have to do with, you know,
maybe whether or not you're exercising,
whether or not you just ate a meal,
whether it's hot outside.
What the- if you just got an the airplane.
All these other features probably affect whether or not
your next blood pressure is going to be high or
low and particularly in response to some medication.
Um, similarly in something like website shopping, um,
you can imagine the state is just sort of
what is the product you're looking at right now?
So, like I open up A- Amazon,
I'm looking at some um, you know, computer,
and um that's up on my webpage right now,
and the action is what other products to recommend.
Do you think that system is Markov?
Systems is not Markov?
Do you mean the system generally?
But if the assumption is Markov and if it doesn't fit?
Question is whether or not the system generally is Markov and
the assumption just doesn't fit or make- just some more details. I'll think about this.
What I mean here is that
this particular choice re-representing the system is that Markov.
Um, and so, there's the real-world going on,
and then there's sort of the model of the world that the agent can use.
What I'm arguing here is that these particular models of the world are not Markov.
There might be other models of the world that are.
Um, but if we choose
this particular observation say just the current blood pressure as our state,
that is probably not really a Markov state.
Now it doesn't mean that we can't use algorithms that treat it as if it is.
It is just that we should be aware that we might be
violating some of those assumptions. Yeah?
Um, I'm wondering so if you include um,
enough history into a state,
can you make them part of the Markov?
Okay. It's a great question.
So, why is it so popular?
Can you know-can you always make something Markov?
Um, generally yes.
If you include all the- the history,
then you can always make the system Markov.
Um, in practice often you can get away with just using
the most recent observation or
maybe the last four observations as a reasonably sufficient statistic.
It depends a lot on the domain.
There's certainly domain, maybe like
the navigation world I put up there where it's really important to model.
Either use the whole history as the- as the state um,
or think about the partial observability um,
and other cases where you know,
maybe the current- the most recent observation is completely sufficient.
Now, one of the challenges here is you might not want to use
the whole history because that's a lot of information.
[LAUGHTER] and you have to keep track of it over time.
And so, it's much nicer to have sort of a sufficient statistic.
Um, of course, some of these things are changing
a little bit with LSTMs and other things like that.
So, um, some of our prior assumptions about how
things scale with the size of the state-space
are changing a little bit right now with deep learning.
Um, but historically certainly,
there's been advantages to having a- a smaller state-space.
And um, again historically,
there's been a lot of implications for things like computational complexity,
the data required, and
the resulting performance depending on the size of the state space.
So, just to give some intuition for why that might be, um,
if you made your state everything that's ever happened to you in your life,
um, that would give you a really, really rich representation.
You'd only have one data point for every state.
There would be no repeating.
So, it's really hard to learn because um,
they're- all states are different.
Um, and in general if we wanna learn how to do something,
we're gonna either need some form a generalization or some form of
clustering or aggregation so that we can compare experiences,
so that we can learn from prior similar experience in order to what to do.
So, if we think about assuming that your observation is your state,
so the most recent observations that the agent gets,
we're gonna treat that as the state.
Then we- the agent is modelling the world is that Markov decision process.
So, it is thinking of taking an action,
getting observation and reward,
and it's setting the state,
the world state- that the environment state it's using to be the observation.
If the world- if it is treating the world as partially observable um,
then it says the agent state is not the same, um,
and it sort of uses things like the history or beliefs about the world state
to aggregate the sequence of previous actions taken and observations received,
um, and uses that to make its decisions.
For example, in something like poker,
um, you get to see your own cards.
Other people have cards that are clearly affecting the course of the game.
Um, but you don't actually know what those are.
You can see which cards are are discarded
And so that's somewhere where it's naturally partially observable.
And so you can maintain a belief state over what the other cards or at the other players.
And you can use that information or to make your decisions.
And similarly often in health care there's a whole bunch of
really complicated physiological processes that are going
on but you could monitor parts of them for things
like you know blood pressure or temperature et cetera.
Uh, and then use that in order to make decisions.
So, in terms of types of sequential decision making processes,
um, one of them is Bandits.
We'll talk more about this later the term.
Um, Bandits is sort of a really simple version of a markup decision process in
the sense that the ideas that the actions
that are taken have no influence over the next observation.
So, when might this be reasonable?
So, let's imagine that you have a series of customers coming to
your website and you show each of them an ad.
So, and then they either click on it or not and
then you get another customer login into your website.
So, in this case the ad that you show to customer one,
generally doesn't affect who cuts- which customer two comes along.
Now it could maybe in really complicated ways maybe customer one
goes to Facebook and says I really really loved this ad, you should go watch it.
Um, but most of the time whatever ad you showed a customer
one does not at all affect who next logs into your website.
And so the decisions you make only affect the local, um,
the first customer and then the customer two is totally independent.
Bandits have been really really important,
um, for at least 50 years.
Um, people thought about them for things like clinical trials,
how to allocate people to clinical trials.
You will think of them for websites and a whole bunch of other applications.
MDPs and POMDPs say no wait the actions that you take can affect the state of the world,
they affect often the next observation you get,
um, as well as the reward.
And you have to think about this closed loop system
of the actions that you're taking changing the state of the world.
So, the product that I recommend to my customer might affect what
the customer's opinion is on the next time-step. In fact, you hope it will.
Um, and so in these cases we think about,
um, the actions actually affecting the state of the world.
So, another important question is how the world changes?
Um, one idea is that it changes deterministically.
So, when you take an action in a particular state,
you go to a different state but the state you go to it's deterministic. There's only one.
And this is often a pretty common assumption in a lot of
robotics and controls. All right.
Remember, um, Tomás Lozano-Pérez who's a professor over
at MIT ones suggesting to me that if you flip a coin,
it's actually deterministic process.
We're just modeling it as stochastic.
We don't have good enough models.
Um, so, there are many processes that if you could sort of write down,
um, a sufficient perfect model of the world it would actually look deterministic.
Um, but in many cases even though it maybe hard to write down those models.
And so we're going to approximate them as stochastic.
And the idea is that then when we take an action there are many possible outcomes.
So, you couldn't show an ad to someone and they may or may not click on it.
And we may just want to represent that with a stochastic, stochastic model.
So, think about a particular example.
So, if we think about something like Mars Rover, um,
ah when we deploy rovers or robots on really far-off,
um, planets, it's hard to do communication back and forth.
So, be nice to be able to make these sort of robots more autonomous.
Let's imagine that we have a very simple Mars rover that's,
um, thinking about a seven state system.
So, it's just landed.
Um, it's got a particular location and it can
either try to go left or try to go the right.
I write down try left or try right meaning that that's
what it's going to try to do but maybe you'll succeed or fail.
Let's imagine that there's different sorts of scientific information to be discovered,
and so over in S1 there's a little bit of
useful scientific information but actually over at
S7 there's an incredibly rich place where there might be water.
And then there's zero in all other states.
So, we'll go through that is a little bit of an example.
As I start to talk about different common components of an oral agent.
So, one often common component is a model.
So, a model is simply going to be a representation of the agent has
for what happens in the world as it takes its actions and what rewards it might get.
So, in the case of the markup decision process it's simply
a model that says if I start in this state and I take this action A,
what is the distribution over next states I might reach
and it also is going to have a reward model that predicts the expected reward of taking,
um, an action in a certain state.
So, in this case, ah,
let's imagine that the reward of the agent is that
it thinks that there's zero reward everywhere.
Um, and let's imagine that it thinks its motor control is very bad.
And so it estimates that whenever it tries to move with 50% probability it
stays in the same place and 50% probability it actually moves.
Now the model can be wrong.
So, if you remember what I put up here the actual reward is
that in state S1 you get plus one and in state
S7 you get S you get 10 and everything else you get zero.
And the reward I just wrote down here is that it's zero everywhere.
So, this is a totally reasonable reward model the agent could have.
It just happens to be wrong.
And in many cases the model will be wrong, um,
but often can still be used by the agent in useful ways.
So, the next important component that is always needed by an oral agent is a policy.
Um, and a policy or decision policy is simply how we make decisions.
Now, because we're thinking about Markov decision processes here,
we're going to think about them as being mappings from states to actions.
And a deterministic policy simply means there's one action prostate.
And the stochastic means you can have a distribution over actions you might take.
So, maybe every time you drive to the airport,
you flip a coin to decide whether you're going to take
the back roads or whether you're going to take the highway.
So, as a quick check imagine that in every single state we do the action try right
is this the deterministic policy or stochastic policy? Deterministic great.
We'll talk more about why deterministic policies are
useful and when stochastic policies are useful shortly.
Now, the value function, um,
is the expected discounted sum of future rewards under
a particular policy. So, it's a waiting.
It's saying how much reward do I think I'm going to get both now and in the future
weighted by how much I care about immediate versus long-term rewards.
The discount factor gamma is going to be between zero and one.
And so the value function that allows us to say
sort of how good or bad different states are.
So, again in the case of the Mars rovers let's imagine that our discount factor is zero.
Our policy is to try to go right.
And in this case say this is our value function.
It says that the value of being in state one is plus one everything
else is zero and the value of being in S7 is 10.
Again, this might or might not be the correct value function.
Depends also on the true dynamics model,
but this is a value function that the agent could have for this policy.
Simply tells us what is the expected discounted sum of
rewards you'd get if you follow this policy starting in
this state where you weigh
each reward by gamma to the number of time steps at which you reach it.
So, when we think about, yeah.
So, if we wanted to extend the discount fac- factor to this example, um,
would there be, like, ah,
an increasing value or decreasing value to reward depending on how far it went?
Yes. Question was if,
if the Gamma was not 0 here.
Um, so gamma is being 0 here
indicates that essentially we just care about immediate rewards,
whether or not we'd start to,
sort of, if I understood correctly,
you start to see like rewards slew into other states, and the answer is yes.
So, we'll see more of that next time,
but if the discount factor is non-zero,
then it basically says you care about not just the immediate reward you get,
[NOISE] you're not just myopic,
you care about their reward you're gonna get in the future.
So, in terms of common types of reinforcement learning agents,
um, some of them are model-based,
which means they maintain in their representation a direct model of how the world works,
like a transition model and a reward model.
Um, and they may or may not have a policy or a value function.
They always have to compute a policy.
They have to figure out what to do.
But they may or may not have an explicit representation
for what they would do in any state.
Um, model free approaches have an explicit value function and
a policy function and no model. Yeah.
Going back with [NOISE] the- the earlier slide,
I'm confusing when the value function is
evaluated ice with the- with well the setting yes.
So, why is it not [NOISE] S_6 that has value
of 10 because if you try right at S_6 you get to S_7.
You were saying well how do I- when do we think of the rewards happening.
Um, we'll talk more about that next time.
When really, uh, there's many different ways people think of where the rewards happening.
Some people think of it as the reward happening for the current state you're in.
Some people think of it as it's the reward you're in [NOISE] and the action you take.
And some people- some- another common definition is r- SAS prime,
meaning that you don't see what reward you get until you transition.
And this particular definition that I'm using
here we're assuming that rewards happened in one year in that state.
All of them are, um,
basically isomorphic, um, but we'll
try to be careful about which one we're using [NOISE].
The most common one we'll use in the class is s,a which says that when you're in a state,
and you choose a particular action,
then you get a reward, and then you transition to your next state.
Great question. Okay. So, when we think about reinforcement learning agents,
and whether or not they're maintaining these models and these values and these policies,
um, we get a lot of intersection.
So, I really like this figure from David Silver, um,
I- where he thinks about, sort of,
RL algorithms or agents mostly falling into these three different classes.
They even have a model or explicit policy or explicit value function.
And then there's a whole bunch of algorithms that are,
sort of, in the intersection of these.
So, things like actor critic often have an explicit.
And what do I mean by explicit?
I mean like often they have a way so that if you give it
a state you could tell- I could tell you what the value is,
if I give you a state you could tell me immediately what
the policy is, without additional computation.
So, actor-critic combines value functions and policies.
Um, there's a lot of algorithms that are also
in the intersection of all of these different ones.
And often in practice it's just very hopeful to maintain.
Many of these and they have different strengths and weaknesses.
For those of you that are interested in the theoretical aspects of learning theory,
there's some really cool recent work, um,
that explicitly looks at what is the formal foundational differences
between model-based and model-free RL that just came out of MSR,
Microsoft Research [NOISE] ,
um, in New York, which indicates that there may be
a fundamental gap between model-based and model-free methods,
um, which on the deep learning side has been very unclear.
So, feel free to come ask me about that.
So, what are the challenges in learning to make good decisions,
um, in this, sort of, framework?
Um, one, is this issue of planning that we talked about a little bit before,
which is even once I've got a model of how the world works,
I have to use it to figure out what decisions I should make,
in a way that I think it's going to allow me to achieve high reward.
Um, [NOISE] and in this case if you're given
a model you couldn't do this planning without any interaction in the real world.
So, if someone says,
here's your transition model,
and here's your reward model, you can go off and do a bunch of computations,
on your computer or by paper,
and decide what the optimal action is to do,
and then go back to the real world and take that action.
It doesn't require any additional experience to compute that.
But in reinforcement learning,
we have this at other additional issue that we might want to think about not
just what I think is the best thing for me to do given the information I have so far,
but what is the way I should act so that I can get
the information I need in order to make good decisions in the future.
So, [NOISE] it's, like, you know, you go to a brand new restaurant, and, ah,
let's say- let's say you move to a new town,
you go to- there's only one restaurant,
you go there the first day, and they have five different dishes.
You're gonna be there for a long time,
and you wanna optimizing at the best dish.
And so maybe the first day you try dish one,
and the second day you tr- try dish two,
and then the third day three,
and then et cetera so that you can try everything,
and then use that to figure out which one is best so that over
the long term you pick something that is really delicious.
So, in this case the agent has to think explicitly about what decision it should take
so it can get the information it needs so that in the future it can make good decisions.
So, in the case of planning,
and the fact that this is already a hard problem,
um, you think about something like solitaire,
um, you could already know the rules of the game,
this is also true for things like go or chess or many other scenarios.
Um, and you could know if you take an action
what would be the probability distribution of the next [NOISE] state,
and you can use this to compute a potential score.
And so using things like tree search or dynamic programming,
and we'll talk a lot more [NOISE] about these, um,
ah, particularly the dynamic programming aspect you can use
that to decide given a model of the world what is the right decision to make.
But sol- the reinforcement learning itself
is a little bit more like solitary without a rule book.
We're here just playing things and you're observing what is happening,
and you're trying to get larger reward.
And you might use your experience to explicitly compute
a model and then plan in that model,
or you might not and you might directly compute a policy or a value function.
Now, I just wanna reemphasize here this issue of exploration and exploitation.
So, in the case of the Mars rover it's only going to
learn about how the world works for the actions it tries.
So, in state S2 if it tries to go left it can see what happens there.
And then from there it can decide the right next action.
Now, this is obvious but it can lead to a dilemma because it has to
be able to balance between things that seem like they might be good,
based on your prior experience,
and things that might be good in the future,
um, that perhaps you've got unlucky before.
So, in exploration we're often interested in trying things that we've never tried before,
or trying things that so far might have looked bad,
but we think in the future might be good.
But an exploitation we're trying things that are
expected to be good given the past experience.
So, here's three examples of this.
In the case of movies, um,
exploitation is like watching your favorite movie,
and exploration is watching a new movie,
that might be good or it might be awful.
Advertising is showing the ad that sealed the most highest click-through rate so far.
Um, exploration is showing a different ad.
And driving exploitation is trying the fastest route given your prior experience and
exploration is driving a different route.
[inaudible].
Great question, which is,
what's the imagine for that example that I gave?
I am that you're only going to be in town for five days.
Um, and with the policy that you would
compute in that case if you're in a finite horizon setting,
be the same or different as one where you know you're going
to live in this for all of infinite time.
Um, we'll talk a little bit more about this next,
ah, next time but very different.
Um, and in particular, um,
the normally the policy if you only have
a finite horizon is non-stationary which means that,
um, the decision you will make depends on the time step as well as the state.
In the infinite horizon case the assumption is that, um,
the optimal policy and the mark off setting is stationary,
which means that if you're in the same state whether you're there on time
step three or time step 3,000 you will always do the same thing.
Um, but in the finite horizon case that's not true.
And as a critical example of that.
So, why do we explore?
We explore in order to learn information that we can use in the future.
So, if you're in a finite horizon setting and it's the last day is
your last day in Hollywood and you know you're trying to decide what to do, um,
you're not going to explore because there's no benefit from
exploration for future because you're not making any more decisions,
so in that case you will always exploit,
its always optimal to exploit.
So, in the finite horizon case, um,
the decisions you make have to depend on the value of
the information you gain to change your decisions and the remaining horizon.
And there's this often comes up in real cases. Yeah.
How much, um, how much more complicated
is if there's a finite horizon but you don't know where is this?
Uh, is just something I remember from game theory this tends to be very complicated.
How this [inaudible]?
Question is what about what I would call it indefinite horizon problems where there
is a finite horizon but you don't know what it is that can get very tricky.
One way to model it is as an infinite horizon problem with termination states.
So, there are some states which are essentially stink
states once you get there the process ends.
There's often happens in games, um,
you don't know when the game will end but it's going to be finite.
Um, and that answer that's one way to put it into the formalism,
um, but it is tricky.
In those cases we tend to model it has
infinite horizon and look at the probability of reaching different termination states.
[inaudible] you miss exploitation,
exploration essentially subproblems, I guess particulary for driving.
It seems like it would be better to kind of
exploit has you know are really good and maybe
explore on some [inaudible] don't know her as good rather than trying like completely brand new route.
In about how this mix happens of exploration,
exploitation and maybe in the cases of cars, maybe you would,
um, sort of, er,
not try things totally randomly.
You might need some evidence that they might be good,
um, it's a great question,
um, there's generally it is better to intermix exploration exploitation.
In some cases it is optimal to do all your exploration early or at least equivalent, um,
and then it came from all of that information for later,
but it depends on the decision process.
Um, and we'll spend a significant chunk of
the course after the midterm thinking about exploration,
exploitation, it's definitely a really critical part of reinforcement learning,
um, particularly in high stakes domains.
What do I mean by high-stakes domains?
I mean domains that affect people.
So, whether it's customers or patients or students, um,
that's where the decisions we make actually affect real people and so we want
to try to learn as quickly as possible and make good decisions as quick as we can.
Any other questions about this?
If you're in- you're in sort of state that you haven't seen before,
do you have any other better option and just take a random action to get out of there?
Or you can use your previous experience even though you're not never been there before?
The question is great.
It's the same if you're in a new state you've never been in before, what do you do?
Can you do anything better than random?
Or can you somehow use your prior experience?
Um, one of the really great things about
doing generalization means that we're going to use
state features either learned by deep learning or
some other representation to try to share information.
So, that even though [NOISE] the state might not be one you've ever exactly visited
before you can share prior information to
try to inform what might be a good action to do.
Of course if you share in the wrong direction,
um, you can make the wrong decision.
So, if you overshoot-overgeneralize you could
overfit your prior experience and in fact that there's
a better action to do in the new scenario. Any questions for this?
Okay. So, one of the things we're going to be talking about over
the next few lectures is this trend two
really fundamental problems which is evaluation and control.
So, evaluation is the problem as saying if someone gives you a policy,
if they're like hey this is what you should do or this is what your agent should do,
this is how your robot should act in the world to evaluate how good it is.
So, we want to be able to figure out you know your manager says Oh
I think this is the right way we should show ads to customers,
um, can you tell me how good it is?
What's the quick [inaudible]?
Um, so one really important question is evaluation,
um, and you know you might not have a model of the world.
So, you might have to go out and gather data to try to
evaluate this policy be useful to know how good it is,
you're not trying to make a new policy
with not yet you're just trying to see how good this current one is.
And then the control problem is optimization.
It's saying let's try to find a really good policy.
This typically involves as
a sub-component evaluation because often we're going to need to know what does best mean?
Best means a really good policy.
How do we know how good the policies?
We need to do evaluation.
Now one of the really cool aspects of reinforcement learning, um,
is that often we can do this evaluation off policy.
Which means we can use data gathered from other policies to
evaluate the counterfactual of what different policies might do.
This is really helpful with because it means we
don't have to try out all policies exhaustively.
So, um, in terms of what these questions look like,
if we go back to our Mars Rover example for
policy evaluation it would be if someone says your policy is this,
in all of your states the action you should take as try right.
This is the discount factor I care about, um,
please compute for me or evaluate for me what is the value of this policy?
In the control case,
they would say I don't know what the policy should be.
I just want you to give me whatever ever policy has
the highest expected discounted sum of rewards,
um, and there's actually sort of a key question here is.
Okay. Expected discounted sum of rewards from what?
So, they might care about a particular starting state,
they might say I want you to figure out the best policy assuming I'm starting from S4.
They might say I want you to compute the best policy from all starting states,
um, or sort of some average.
So, in terms of the rest of the course where we get- yeah.
I was just wondering if it's possible to learned
the optimal policy and the reward function simultaneously?
Through example if I has some belief of what the reward review that
included or for some sort of action there
will be a state and that turned out to be wrong,
ah, we have to start over and trained to find
optimal policy or could I use what I've learned so far.
In addition assumption organization of data with a belief of the rewards that [inaudible]?
Fake question, which is. Okay. Let's say I have a policy to start with I'm evaluating it,
um, and I don't know what the reward function is and I don't know what
the optimal policy is and it turns out this [inaudible] isn't very good.
Do I need to sort of restart or can I use
that prior experience to sort of inform what's the next policy I try?
Ah, perhaps a whole suite of different policies?
In general you can use the prior experience in order to
inform what the next policy is that you try our next suite of policies.
Um, there's a little bit of a caveat there which is,
uh, you need to have some stochasticity in the actions you take.
So, if you only take the same you know one action in a state,
you can't really learn about any other,
um, actions you would take.
So, you need to assume some sort of generalization or some sort of stochasticity in
your policy in order for that information to be useful to try to evaluate other policies.
This is a really important issue.
This is the issue of sort of counterfactual reasoning and how do we
use our old data to figure out how we should act in the future,
um, if the old policies may not be the optimal ones.
So, in general we can, um,
and we'll talk a lot about that it's a really important issue.
So, we're first going to start off talking about sort of Markov
decision processes and planning and
talking about how do we sort of do this evaluation both whom we know how the world works,
meaning that we are given a transition model and reward model and when we're not,
then we're also going to talk about
model-free policy evaluation and then model-free control.
We're going to then spend some time on
deep-deep reinforcement learning and
reinforcement learning in general with function approximation,
which is a hugely growing area right now.
Um, I thought about making a plot of
how many papers are going on in this area right now it's pretty incredible.
Um, and then we're going to talk a lot about
policy search which I think in practice particularly in robotics
is one of the most influential methods right now and we're
going to spend quite a lot of time on exploration as well as have,
um, a few advanced topics.
So, just to summarize what we've done
today is talk a little bit about reinforcement learning,
how it differs compared to other aspects of AI machine learning.
We went through course logistics started to talk
about sequential decision making under uncertainty.
Just as a quick note for next time, um,
we will try to post the lecture slides, um,
two days in advance or by the end of you know the evening of two days in advance,
so that you can print them out if you want to, um, in class.
And I'll see you guys on Wednesday.
 All right. So, last time we were starting to talk about the sort
of the general overview of what reinforcement learning involves,
um, and, we introduced the notion [NOISE] of a model,
a value, and a policy.
[NOISE] Um, so it's good to just refresh your brain right now,
about what, what those three things are.
Can anybody remember off the top of their head what a value, a model,
or a policy was in the context of reinforcement learning?
[NOISE]
Um, so policy is a set of actions,
that, uh, the agent should take [NOISE] in a work. [NOISE]
Exactly right. So, the definition of a policy is a mapping from
the state you're in to what is the action, um, to take.
And it might be a good policy or a bad policy.
And the way we evaluate that,
is in terms of its [NOISE] expected discounted sum of rewards.
Does anybody remember what a model was? Yeah?
A model is like, uh,
representation of the world and how that changes in response to agent's accident. [NOISE]
Yeah. So right, so normally we think of a model of incorporating either reward model,
or a decision, uh, or, or dynamics model,
[NOISE] which specifies in response to the current state and, uh,
an action how the world might change,
could be a stochastic model or a deterministic model.
[NOISE] Um, and the reward model specifies,
what is the expected reward, um,
that the agent receives from taking a state in a particular action.
[NOISE] So what we're gonna talk about today is, um,
thinking about, if you know a model of the world,
so, you know, um,
what happens if you take an action in a particular state,
or what the distribution of next states might be if you [NOISE] take an action,
[NOISE] um, how we should make decisions.
So, how do we do the planning problem?
So, we're not gonna talk about learning today.
We're just gonna talk about the problem of figuring out what is the right thing to do,
[NOISE] when your actions may have delayed consequences,
which means that you may have to sacrifice
immediate reward in order to maximize long-term reward.
[NOISE] So as we just stated, um,
the model generally we're gonna think about is statistical or mathematical models,
of the dynamics and the reward function.
Um, a policy is a function that maps the students each,
uh, uh these agents states to actions,
and the value function as the expected discounted sum of rewards, um,
from being in a state, um,
and/or an action, [NOISE] and then following a particular policy.
[NOISE] So what we're gonna do today is,
sort of, um, build up for Markov Processes,
um, up to Markov Decision Processes.
And this build, I think,
is sort of a nice one because it sort of allows one to
think about what happens in the cases where you
might not have control over the world but the world might still be evolving in some way.
[NOISE] Um, and think about what the reward might be in those, sort of,
processes, for an agent that is sort of passively experiencing the world.
Um, and then we can start to think about the control problem of how the agent should be
choosing to act in the world in order to maximize its expected discounted sum of rewards.
[NOISE] So, what we're gonna focus about on today and, er,
and most of the rest of the classes is this Markov Decision Process,
um, where we think about an agent interacting with the world.
So the agent gets to take actions,
typically denoted by a,
[NOISE] those affect the state of the world in some way, um,
and then the agent receives back a state and a reward.
So last time we talked about the fact that this could in
fact be an observation, instead of a state.
But then, when we think about the world being Markov,
we're going to [NOISE] think of an agent,
just focusing on the current state, um,
so the most recent observation, like, you know,
whether or not the robots laser range finders saying,
that there are walls, to the left or right of it,
as opposed to thinking of the full sequence of prior history
of the sequences of actions taken and the observations received.
[NOISE] Um, as we talked about last time
but you can always incorporate [NOISE] the full history to make something Markov,
um, [NOISE] but most of the time today,
we'll be thinking about, sort of, immediate sensors.
If it's not clear, feel free to reach out.
[NOISE] So, what did the Markov Process mean?
The Markov process is to say that
the state that the agent is using to make their decisions,
is the sufficient [NOISE] statistic of the history.
[NOISE] Which means that in order to predict the future distribution of states,
on the next time step.
Here we're using t to denote time step.
[NOISE] That given our current state s_t,
and the action that is taken a_t,
[NOISE] this is again the action,
[NOISE] um, that this is equivalent to,
if we'd actually remember the entire history,
where the history recall was gonna be the sequence
of all the previous actions and rewards.
And next states that we have seen up until the current time point.
[NOISE] And so essentially, it allows us to say that,
the future is independent of the past given
some current aggregate statistic about the present.
[NOISE] So when we think about a Markov Process or a Markov Chain,
we don't think of there being any control yet.
There's no actions.
Um, but the idea is that,
you might have a stochastic process that's evolving over time.
[NOISE] Um, so whether or not I invest in the stock market,
the stock market is changing over time.
And you could think of that as a Markov Process,
[NOISE] um, so I could just, sort of be,
passively observing how the stock market for a particular,
th- the stock value for a particular stock,
is changing over time.
[NOISE] Um, and a Markov Chain is,
is sort of just the sequence of random states,
where the transition dynamics satisfies this Markov property.
So formally, the definition of a Markov Process is that,
you have, um, a finite or potentially infinite set of states.
And you have a dynamics model which
specifies the probability for the next state given the previous state.
[NOISE] There's no rewards,
there's no actions yet.
Um, and if you have a finite set of states,
you can just write this down as a matrix.
Just a transition matrix that says,
you're starting at some state.
What's the probability distribution over next states that you could reach?
[NOISE] So if we go back to the Mars Rover example that we talked about last time.
[NOISE] Um, In this little Mars Rover example,
we thought of a Mars Rover landing on Mars
and there might be different sorts of landing sites,
um, so maybe our Mars Rovers starts off here.
And then, it can go to the left or right, um, er,
under different actions or we could just think of those actions as being a_1 or a_2,
where it's trying to act in the world.
[NOISE] Um, and in this case,
uh, the transition dynamics, it doesn't,
we don't actually have actions yet,
and we just think of it as, sort of,
maybe it already has some way,
it's moving in the world, the motors are just working.
[NOISE] And so in this case,
the transition dynamics looks like this,
which says that, for example,
the way you could read this, is you could say, well,
the probability that I start in a particular state s_1, um,
and then, I can transition to the next state on the next time step is 0.4.
[NOISE] There is a 0.6 chance that I stay in the same state on the next time step. Yeah?
[NOISE] Um, which dimension represents the start state?
Um, so, this is a great question.
Which dimension, which, which state is the start state?
[NOISE] I'm not specifying that here.
Um, uh, In general when we think about Markov chains,
we think about looking at their steady-state distribution.
So they're stationary distribution will [NOISE]
converge to some distribution over states,
[NOISE] that is independent of the start state,
if you run it for long enough.
Oh, sorry, I meant to ask,
like, on that matrix,
which dimension represents the initial state of-
Oh, you mean, like, where you are now right now?
Yeah. So in this particular case,
you could have it as, um,
the transition of saying,
if you start in state,
[NOISE] uh, let me make sure that I get it right.
In this case, [NOISE] answer there, there,
so if you start in state here,
um, so this is yours initial start at a state s_1
and then you take the dot product of that with,
I may have mo- let me see if I get it right in terms of mixing it up.
It's either on one side or the other side,
and then, I may have transitioned it.
Um, I think you'll have to do it for the [NOISE] other side here.
Yep, it'll be flipped.
So, you would have your initial state.
So 1, 0, 0,
0, 0, 0, 1, 2, 3, 4,
5, 6, and then times P,
and that would give you your next state
distribution s'. Yeah? [NOISE]
Um, um, so what are the probabilities computed of,
like the rewards, I guess, the probability,
based on the reward of going from state 1 to 2 [NOISE] or?
Great question, so was, you know,
one of this transition probabilities looking
at [NOISE] this relate to their word, in this case,
we're just thinking of Markov Chains,
so there's no reward yet, and there's no actions.
[NOISE] Um, and this is just specifying that there's some state of the,
uh, of the process.
So it's as if you're,
let's say your agent, um,
had some configuration of its motors.
[NOISE] You don't know what that is,
that was set down on Mars, and then it just starts moving about.
And what this would say is,
this is the transition probabilities of if
that agent starts in state, I can write it this way.
So, if it starts in state,
[NOISE] s_1, then the probability that it stays in state s_1 is 0.6.
So, the probability that you're starting in this particular state here,
[NOISE] on the next time step that you're still there,
is 0.6 because of whatever configuration of the motors were for that robot.
[inaudible] world works.
This is specifying that, this is how, yeah,
this is how the world works. So that's a great question.
So we're assuming right now, this is, um, the,
this Markov process is a state of the world that you were,
there is some the,
the environment you're in is just described as a Markov Process,
and this describes the dynamics of that process.
We're not talking about how you would estimate those.
This is really as if, this is how that world works.
This is, like, this is the,
this is the world of the fake little Mars Rover.
[NOISE] We have any questions about that? Yeah?
Uh, [NOISE] the serum one cloud action needs to be
transposed [NOISE] when you multiplied by P and all [NOISE] we can see is [inaudible]
Yes. Yeah. [inaudible] [NOISE] Let me just write down and correct vector notation.
Would be like this.
One, one two, three, four, five, six.
That would be, that would be a sample starting state you could be in for example.
So, this could be your initial state.
Initial state and that would mean that your agent is initially in state S one.
Okay. And then if you want to know where it might be on the next state,
you would multiply that by the transition model P
depending on the notation and whether you take the transpose of this transition model,
it will be on the left or the right.
It should always be obvious from context but if it's not clear,
feel free to ask us. And so what would that say?
That would say if you took the, uh,
the matrix multiplication of this vector which just
says you're starting in state s_1, what would that look like?
Afterwards it would say that you are in state s_1 still with probability 0.6,
you're in state s_2 with probably to 0.4.
And this would be your new (state distribution).
And I think that should be transposed.
But it's just a one it which specify
the distribution over next states that you would be in.
You may have any questions about that?
Okay. All right.
So, this is just specifying that the transition model
over how the world works over time and it's just I,
I've written it in matrix notation there to be compact.
But if it's easier to think about it,
it's fine to just think about it in terms of
these probability of next states given the previous state.
And so you can just enumerate those,
you can write it in a matrix form if the,
if the number of states happens to be finite.
So, what would this look like if you wanted to think of what might happen to
the agent over time in this case or what the process might look like,
you could just sample episodes.
So, let's say that your initial starting state is S four,
and then you could say, well,
I can write that as a one-hot vector.
I multiply it by my probability.
And that gives me some probability distribution over the next states that
I might be in and the world will sample one of those.
So, your agent can't be in multiple states at the same time.
So, for example, if we were looking at state s_1,
it has a 0.6 chance to abstain in s_1 or 0.4 chance of transitioning.
So, the world will sample one of those two outcomes for you and it might be state s_1.
So in this case, we have similar dynamics from s_4.
From s_4, it has a probability of 0.4 going to state s_3.
Probability of 0.4 going to state s_4
or a probability of 0.2 of staying in the same place.
So, if we were going to sample an episode of what might happen to the agent over time,
you can start with s_4 then maybe it will transition to s_5.
Maybe they'll go to s_6,
s_7, s_7, s_7.
So, you're just sampling from this transition matrix to generate a particular trajectory.
So, it's like the world you know what the dynamics,
the dynamics is of the world and then nature is gonna pick one of those outcomes.
It's like sampling from sort of a probability distribution.
Anyone having questions about that?
Okay. So, that just gives you a particular episode.
And we're going to be interested in episodes because later we're gonna be
thinking about rewards over those episodes and
how do we compare the rewards we might achieve over those episodes but for right now,
this is just a process.
This is just giving you a sequence of states.
So, next we're gonna add in rewards.
So, that was just a Markov chain.
And so now what is a Markov reward process?
Again, we don't have actions yet just like before.
But now we also have a reward function.
So, we still have a dynamics model like before.
And now we have a reward function that says,
if you're in a particular state,
what is the expected reward you get from being in that state?
We can also now have a discount factor which allows us to trade off
between or allows us to think about how
much we weight immediate rewards versus feature rewards.
So, again just like before,
if we have a finite number of states in
this case R can be represented in matrix notation which
is just a vector because it's just the expected reward we get for being in each state.
So, if we look at the Mars Rover MRP,
then we could say that the reward for being an s_1 is equal to 1.
The reward for being an s_7 is equal to
10 and everything else that reward is zero. Yeah.
Are the words always just tied to the state you're in?
I think last time you talked about it also having an option.
So, why are we not consider that here?
Great question. I'm saying that I mentioned last time that
rewards for the Markov Decision Process can either be a function of the state,
the state in action, or state action next state.
Right now we're still in Markov Reward Processes so there's no action.
So, in this case,
the ways you could define rewards would either
be over the immediate state or state and next state.
So, once we start to think about there being rewards,
we can start to think about there being returns and expected returns.
So, first of all let's define what a horizon is.
A horizon is just the number of time steps in an episode.
So, it's sort of like how long the agent is acting for or how long it,
how long this process is going on for it and it could be infinite.
So, if it's not infinite,
then we call it a finite Markov Decision Process.
We talked about those briefly last time.
Um, but it often we think about the case where,
um, an agent might be acting forever or this process might be going on forever.
There's no termination of it.
The stock market is up today.
It'll be up tomorrow. We expect it to be up for a long time.
We're not necessarily tried to think about evaluating it over a short time period.
One might wanna think about evaluating it over
a very long time period. So, we've done this.
The definition of a return is just the discounted sum of rewards you get
from the current time step to a horizon and that horizon could be infinite.
So, a return just says,
if I start off in time step T,
what is the immediate reward I get and then I transition maybe to
a new state and then I weigh that return reward by Gamma.
And then I transitioned again and I weigh that one by Gamma squared, et cetera.
And then the definition of a value function is just the expected return.
If the process is deterministic,
these two things will be identical.
But in general if the process is stochastic, they will be different.
So, what I mean by deterministic is that if you always go to the same next state,
no matter which if you start at a state
if there's only a single next state you can go to,
uh, then the expectation is equivalent to a single return.
But in the general case, we are gonna be interested in
these stochastic decision processes which
means averages will be different than particularly runs.
So, for an example of that well,
let me first just talk about discount factor and then I'll give an example.
Discount factors are a little bit tricky.
They're both sort of somewhat motivated and somewhat used for mathematical convenience.
So, we'll see later one of the benefits of mathematic, uh,
benefits of discount factors mathematically is that we can
be sure that the value function sort of expected
discounted sum of returns is bounded as long as here reward function is bounded.
Uh, people empirically often act as if there is a discount factor.
We weigh future rewards lower than,
than immediate rewards typically.
Businesses often do the same.
If Gamma is equal to 0,
you only care about immediate reward.
So, you're the agent is acting myopically.
It's not thinking about the future of what could happen later on.
And if Gamma is equal to one,
then that means that your future rewards are
exactly as beneficial to you as the immediate rewards.
Now, one thing just to note,
if you're only using discount factors for mathematical convenience, um,
if your horizon is always guaranteed to be finite,
it's fine to use gamma equal to one in terms
of from a perspective mathematical convenience.
Someone having any questions about discount factors? Yeah.
My question is, does the discount factor of Gamma always have to progress
in a geometric fashion or like is there a reason why we do that?
It's a great question. You know,
the- what we're defining here is that using a Gamma
that progresses through this exponential geometric fashion is that necessary.
It's one nice choice that ends up having very nice mathematical properties.
There, one could try using other participant is certainly
the most common one and we'll see
later why it has some really nice mathematical properties.
Any other questions? Okay.
So, what would be some examples of this?
Um, if we go back to our Mars Rover here and we now have this definition of reward,
um, what would be a sample return?
So, let's imagine that we start off in state s_4 and then we transitioned to s_5,
s_6, s_7 and we only have four-step returns.
So, what that means here is that our, um,
our process only continues for four time steps and then it maybe it resets.
So, why might something like that be reasonable?
Well, particularly when we start to get into decision-making, um you know,
maybe customers interact with the website for on average two or three times steps.
Um, there's often a bounded number of time you know bounded
length of course in many many cases that the horizon is naturally bounded.
So, in this case you know what might happen in this scenario we start off in s_4.
s_4, s_5, s_6 all have zero rewards by definition.
Um, and then on time-step s_7 we get a reward of 10.
But that has to be weighed down by the discount factor which here is 1/2.
So, it's 1/2 to the power of 3.
And so the sample return for this particular episode is just 1.25.
[NOISE] And of course we could define this for any particular, um,
episode and these episodes generally might go through different states even
if they're starting in the same initial state
because we have a stochastic transition model.
So, in this case maybe the agent just stays in s_4,
s_4, s_5, s_4 and it doesn't get any reward.
And in other cases,
um, it might go all the way to the left.
So, if we then think about what the expected value function would be,
it would involve averaging over a lot of these.
And as we average over all of these, um,
then we can start to get different rewards for different time steps.
So, how would we compute this?
Um, now one thing you could do which is sort of
motivated by what I would just showing before,
is that you could estimate it by simulation.
So, you could, um,
just take for say an initial starting state distribution, um,
which could be just a single starting state or
many starting states and you could just roll out your process.
So, right now we're assuming that we have
a transition model transition matrix and a reward model.
Um, and you could just roll this out just like what
we're showing on the previous couple of time-steps.
And you could just do this many many many times. And then average.
And that would asymptotically
converge to what the value function is cause the value function is just,
um, the expected return.
So, one thing you could do with simulation, um,
and there are mathematical bounds you can
use to say how many simulations would you need to
do in order for your empirical average to be close to the true expected value.
The accuracy roughly goes down on the order of one
over square root of N where N is the number of roll-outs you've done.
So, it just tells you that, you know,
if you want to figure out what the value is of your Markov Reward Process,
um, you could just do simulations and that would give you an estimate of the value.
The nice thing about doing this,
is this requires no assumption of the Markov structure.
Not actually using the fact that it's a Markov Reward Process at all.
It's just a way to estimate sums of returns- sums up rewards.
So, that's both nice in the sense that, um,
if you're using this in a process that you had estimated from
some data or you're making the assumption that things are, er, um,
you know this is the dynamics model but that's also
estimated from data and it might be wrong, um,
then this can give you sort of, um,
if you can really roll out in the world then you can get these sort
of nice estimates of really how the process is working.
But it doesn't leverage anything about the fact that if the world really is Markov,
um, there's additional structure we could do in order to get better estimates.
So, what do I mean by better estimates here?
I mean if we want to, um,
get sort of better meaning sort of computationally cheaper,
um, ways of estimating what the value is a process.
So, what the Markov structure allows us to do,
with the fact that the present that
the future is independent of the past given the present,
is it allows us to decompose the value function.
So, the value function of a mark forward process is simply
the immediate reward the agent gets from
the current state it's in plus the discounted sum of
future rewards weighed by
the discount factor times
the- and where we express that discounted sum of future words is we
can just express it with V, V(s').
So, we sort of say well whatever state you're in right now,
you're going to get your immediate word and then you're going to
transition to some state s'.
Um, and then you're going to get the value of
whatever state s' you ended up in discounted by our discount factor.
So, if we're in a finite state MRP we can express this using matrix notation.
So, we can say that the value function which is a vector is equal to
the reward plus gamma times the transition model times
V. Again note that in this case because of the way we're defining the transition model,
um, then the value functions here
the transition model is defined as the next [NOISE] state given
the previous state and multiplying that by the value function there.
So, in this case we can express it just using a matrix notation.
Um, and the nice thing is that once we've done that
we can just analytically solve for the value function.
So, remember all of this is known.
So, this is known. And this is known.
And what we're trying to do is to compute what V(S) is.
So, what we can do in this case is we just move this over to the other side.
So, you can do V minus gamma PV is equal to R or we can
say the identity matrix minus the discount factor times P. These are all matrices.
So, this is the identity matrix times
V is equal to R which means V is just equal to the inverse of this matrix times R.
Um. So, if one of the transitions can be back to itself,
um wouldn't it be become a circular to try to express V(s) in terms of V(s)?
Um, the question was was if it's possible to have self-loops?
Um, could it be that this is sort of circulator defined [NOISE] in this case.
Um, I in this case because we're thinking about processes that are infinite horizon,
the value function is stationary, um,
and it's fine if you have include self loops.
So, it's fine if some of the states that you
might transition back to the same state there's no problem.
You do need that this matrix is well-defined.
That you can take that you can take the inverse of it.
Um, but for most processes that is.
Um, so, if we wanna solve this directly, um,
this is nice it's analytic, um,
but it requires taking a matrix inverse.
And if you have N states so let's say you have N states there's generally
on the order of somewhere between N squared and N cubed
depending on which matrix inversion you're using. Yeah.
Is it ever actually possible for, uh,
that matrix not to have an inverse or does like the property
that like column sum to one or something make it not possible?
Question was is it ever possible for this not to have an inverse?
Um, it's a it's a good question.
Um, I think it's basically never possible for this not to have an inverse.
I'm trying to think whether or not that can be violated in some cases.
Um, if yeah sorry go ahead.
Okay. [NOISE] Yeah.
So, I think there's a couple,
um, if there's a- if this ends up being the zero matrix,
um depending on how things are defined.
Um, but I'll double-check then send a note on a Piazza. Yeah.
Well, actually I think the biggest side about the transition matrix [inaudible]
Let me just double check so I don't say anything that's
incorrect and then I'll just send a note on- on Piazza. It's a good question.
So, that's the analytic way for computing this.
The other way is to use dynamic programming.
So, in this case,
it's an iterative algorithm instead of a one shot.
So, the idea in this scenario is that you initialize the value function to be
zero everywhere and in fact you can initialize it to anything and it doesn't matter.
If you're doing this until convergence.
And so then what we're gonna do is we're going to
do what's going to be close to
something we're going to see later which is a bellman backup.
So, the idea in this case is because of the Markov property,
we've said that the value of a state is exactly equal to
the immediate reward we get plus the discounted sum of future rewards.
And in this case,
we can simply use that to derive an iterative equation where we use the previous value of
the state in order to bootstrap
and compute the next value of the state and we do that for all states.
And the computational complexity of this is a little bit lower because it's only
|S| squared because you're doing this for each
of the states and then you're summing over all the possible next states.
When I say we do this total convergence
generally what we do in this case is we define a norm.
So, generally we would do something like this,
V_k minus V_k-1.
I need to do this until it's lower than some epsilon.
So, the advantage of this is that each of the iteration updates are
cheaper and they'd also will be some benefits later when we start to think about actions.
The other thing does not apply as easily when we
start to have actions but we'll see also where it can be relevant.
So, here are two different ways to try to compute the value of
Markov Reward Process or three really one is simulation,
the second is analytically.
The analytic one requires us a step
a finite set of states and the third one is dynamic programming.
We're also right now defining only all of these for when the state space is finite,
but we'll talk about when the state space is infinite later on.
So, now we can finally get onto Markov Decision Processes.
Markov Decision Processes are the same as
the Markov Reward Process except for now we have actions.
So, we still have the dynamics model but now
we have a dynamics model that is specified for
each action separately and we also have a reward function.
And as was asked before by Camilla I think,
the reward can either be a function of the immediate state,
the state and action to the state action and next state for most of the rest
of today we'll be using that it's the function of both the state and action.
So, the agent is in a state they take an action,
they get immediate reward,
and then they transition to the next state.
So, if you think about serve an observation you'd see something like
this s, a, r, and then transition to state s'.
And so a Markov Decision Process is typically
described as a tuple which is just the set of states,
actions, rewards, dynamics, model, and discount factor.
Because of the way you've defined that dynamic model,
is the case that if you take a specific action that is
intended for you to move to your state s',
you won't fully successful move to that state?
Like I guess I'm curious about why there's a- why there is a probability at all?
Like if you're deep in a state in K action,
why is it deterministic what the next state is?
Question is same like well why is this- why are there stochastic processes I think.
Um, there are a lot of cases where we don't have perfect models of the environment.
May be if we had better models then things would be deterministic.
And so, we're going to approximate our uncertainty over those models with stochasticity.
So, maybe you have a robot that's a little bit faulty and so
sometimes it gets stuck on carpet and then sometimes it goes forward.
And we can write that down as a stochastic transition matrix
where sometimes it stays in the same place and sometimes it advances to the next state.
Or maybe you're on sand or things like that.
Maybe when you're trying to drive to SFO sometimes you hit traffic, sometimes you don't.
You can imagine putting a lot more variables into your state-space to
try to make that a deterministic outcome or you could just say,
"Hey sometimes when I try to go to work, you know,
like I hit these number of red lights and so I'm late and other times,
you know, I don't hit those red lights and so I'm fine."
So, if we think about our Mars Rover MDP.
Now, let's just define there being two actions A1 and A2.
You can think about these things as the agent trying to move
left or right but it's also perhaps
easier just to think about in general them as sort of
these deterministic actions for this particular example.
So, we can write down what the transition matrix would be in each of
these two cases that shows us
exactly where the next state would be given the previous action.
So, what's happening in this case is if the agent tries to do
a_1 in state s_1 then it stays in that state.
Otherwise, it will generally move to the next state over.
If it's trying to do action a_1 and for action
a_2 it'll move to the right unless it hits s_7 and then it'll stay there.
So, like we said at the beginning of class,
a Markov Decision Process policy specifies what action to take in each state.
And the policies themselves can be deterministic or stochastic,
meaning that you could either have a distribution over in the next action you might
take given the state you're in or you could have a deterministic mapping.
It says whenever I'm in this state I always,
you know, do action a_1.
Now- and a lot of this class we'll be thinking about
deterministic policies but later on when we get into
policy search we'll talk a lot more about stochastic policies.
So, if you have an MDP plus a policy
then that immediately specifies a Markov Reward Process.
Because once you have specified the policy then you can think of that as
inducing a Markov Reward Process because you're only
ever taking you've specified your distribution over actions for
your state and so then you can think of sort of what is the reward,
the expected reward you get under that policy for any state and similarly you can define
your transition model for Markov Reward Process by
averaging across your transition models
according to the weight at which you would take those different actions.
So, the reason why it's useful to think about these connections between
Markov Decision Processes and Markov Reward Processes is it implies that if
you have a fixed policy you could just use all the techniques that
we just described for Markov Reward Processes mainly simulation,
analytic, analytic solution or dynamic
programming in order to compute what is the value of a policy.
So, if we go back to the iterative algorithm then it's exactly the same as before,
exactly the same as the Markov Reward Process except
for now we're indexing our reward by the policy.
So, in order to learn what is the value of a particular policy we
instantiate the reward function by always picking the action that the policy would take.
So, in this case, I'm doing it for simplicity for
deterministic policy and then
similarly just indexing which transition model
to look up based on the action that we would take in that state.
And this is also known as a bellman backup for a particular policy.
So, it allows us to state what is the value of the state under
this policy well it's just the immediate reward I would get by
following the policy in the current state plus
the expected discounted sum of rewards I get by following this policy.
And then for whatever state I end up by next continuing to follow this policy.
So that's what the V^pi_k-1 specifies.
What would happen if the expected discounted sum of rewards we get by
continuing to follow policy from whatever state we just transitioned to.
So, if we go to
the Markov- the Markov chain
or the Ma- now the Markov Decision Process for the Mars Rover,
then let's look at the case now where we have these two actions.
The reward function is still that you either have for any action if you're in state
one you get plus one and in any state any action for state s_7 you get plus 10.
Everything else is zero.
So, imagine your policy is always to do action a_1 and your discount factor is zero.
So, in this case,
what is the value of the policy
and this is just to remind you of what like the iterative way of computing it would be.
Yeah in the back.
Um, and I think that will be zero for everything
except s_1 and s_7 where it's +1 and +10.
That's exactly right. So this is a little bit of a trick question
because I didn't show you again what the transition model is.
Said is exactly correct.
The- it doesn't matter what the transition model is here,
um, because gamma is equal to zero.
So that means that all of this goes away,
um, and so you just have the immediate reward.
So if your discount factor is zero then you just care about immediate reward.
And so the immediate reward for this policy
because the reward for all actions and state one is always +1.
And the reward for all actions and all other states is zero except
for in state s_7 where it's always 10 no matter which action you take.
So this is just equal to one.
That's the value function address.
Okay. So let's, um, look at another one.
So now we've got exactly the same process.
Um, I've written down a particular choice of the dynamics model for ah, state s_6.
So let's imagine that when you're in
state s_6 which is almost all the way to the right, um,
you have a 50% probability of staying there under action A1
or a 50% probability of going to state s_7.
That's what this top line says.
And then there's a whole bunch of other dynamics models that we're
not going to need to worry about to do this computation.
And then the reward is still +1 for state s_1,
+10 in state s_7,
zero for all the states in the middle.
And then let's imagine that, um,
we're still trying to evaluate the policy where you're always taking action a_1.
Um, and we've just said that V_k is equal to 1,0,0,0,10,
um, and now what we wanna do is do one more backup essentially.
So we want to move from V_k=1 and now compute V_k=2.
So how [NOISE] about everybody take a second and figure [NOISE] out what would be
the value under this particular policy, okay, for s_6.
So you can use this equation, um,
to figure out given that I know what
my previous value function is because I've specified it there it's 1,0,0,0,10.
Um, and now I'm going to be doing one backup,
and I'm only asking you to do it for one state,
you could do it for others if you want.
Um, what would be the new value of s_6 if you use this equation to compute it?
And it just requires plugging in what is the value of the reward.
The value is and- and the particular numbers for the dynamics and the old value function.
And the reason that I bring this up as an example is to show sort of
essentially how could have information flows as you do this computation.
So you start off in the very initial.
Let me just go over here first.
So when you start off, you're going to initialize
the value function to be zero everywhere.
The first backup you do basically initializes
the value function to be the immediate reward everywhere.
And then after that you're going to continue to
do these backups and essentially you're trying to
compute its expected discounted sum of
future rewards for each of the states under this policy.
So if you think about looking at this,
that's with information of the fact that state s_7 is good,
is going to kinda flow backwards to the other states because they're saying "Okay well,
I've been in state s_4 I don't have any reward right now but at a couple of timesteps
under this process I might because I might reach that really great +10 state."
So as we do these iterations of policy evaluation,
we start to propagate the information about future rewards back to earlier states.
And so what I'm asking you to do here is to just do that for one, one more step.
Just say for state s_6,
what would its new value be?
Its previous value was zero.
Now we're going to do one backup and what's this new value.
So what if you just uh,
let's ask a question then we can all take a second to uh.
I'm just wondering, er, if repeating the same process to find the value function.
I guess if you don't necessarily know the value function of s,
you could just like reversibly follow it down.
Question was can you- if you don't know what
the value function is. I guess I'm not totally sure.
This is a way to compute the value,
wait your question is asking because this is a way to compute the value function.
So what we've done here is we've said,
we've initialized the value function to be zero everywhere.
That is not the real value function,
that just sort of an initialization.
And what this process is allowing us to do is we keep
updating the values of every single state until they stop changing.
And then that gives us the expected discounted sum of rewards.
Now you might ask, okay well they- are they ever guaranteed to stop changing?
And we'll get to that part later.
We'll get to the fact that this whole process is guaranteed
to be a contraction so it's not going to go on forever.
So the distance between the value functions is going to be shrinking.
And that's one of the benefits of the discount factor.
So if people don't have any more immediate questions,
I suggest we all take a minute and then just compare with
your neighbor of what number you get when you do this computation.
Just to quickly check that the Bellman equation make sense.
[NOISE] All right. So, um,
wherever you got to, um,
hope we got a chance to sort of compare check
any understanding with anybody else that was next to you.
Um, before we go on I just want to, um,
answer a question that was asked before about whether or
not the analytics solution is always possible,
um, to invert. Let's go back to that.
So in this case, um,
because p is a stochastic matrix,
its eigenvalues are always going to be less than or equal to one.
If your discount factor is less than one,
then I which is the identity matrix minus gamma times P is always going to be invertible.
That's the answer to that question.
So this matrix is always invertible as long as gamma is less than one. All right.
So let's go back to this one, um,
which we're going to require any way for some of the other important properties we want.
So in this case what is that?
So the immediate reward of this is
zero plus gamma times [NOISE] 0.5 probability that we stay in
that state times the previous V of s_6 plus 0.5 probability that we go to V of s_7.
And this is going to be equal to zero plus 0.5 times zero plus 0.5 times 10.
So that's just an example of, um,
how you would compute one Bellman backup.
And that's back to my original question which is you seem to be using
V_k without the superscript pi to evaluate it.
Oh, sorry this should, yes.
This should have been pi.
That's just a typo. And that's that was correct in there.
Question was just whether or not that was supposed to be pi up there.
Yes it was, thanks for catching.
All right, so now we can start to talk about Markov Decision Process control.
Now just to note there. So I led us through or we just went through policy evaluation
in an iterative way you could have also
done it analytically or you could have done it with simulation.
But as a particularly nice analogy now that we're going to start to think about control.
So again what do I mean by control?
Control here is going to be the fact that ultimately
we don't care about just evaluating policies,
typically we want our agent actually be learning policies.
And so in this case we're not going to talk about learning policies,
we're just going to be talking about computing optimal policies.
So the important thing is that there exists a unique optimal value function.
So- um, and the optimal policy for
an MDP and an infinite horizon finite state MDP is deterministic.
So that's one really good reason why it's
sufficient for us to just focus on deterministic policies,
with a finite state MDPs,
um, in infinite horizons.
Okay. So how do we compute it?
Well first before we do this let's think about how many policies there might be.
So there are seven discrete states.
In this case it's the locations that the robot.
There are two actions.
I won't call them left and right,
I'm just going to call them a_1 and a_2.
Because left and right kind of implies that you will definitely achieve that.
We can also just think of these as generally being stochastic scenarios.
So let's just call them a_1 and a_2.
Then the question is how many deterministic policies are
there and is the optimal policy for MDP always unique?
So kind of right we just take like
one minute or say one or two minutes feel free to talk to a neighbor
about how [NOISE] many deterministic policies there are for
this particular case and then if that's- um,
once you've answered that it's fine to think about in general if you
have |S| states and |A| actions,
and this is the cardinality of those sets.
How many possible deterministic policies are there?
Um, and then the second question which is whether or not these are always unique.
[NOISE] Can anyone I'd
take a guess at how many deterministic policies that are in this case?
[NOISE].
It's a mapping from states to actions so it's gonna be 2 to the 7th.
That's exactly right. That is it's a mapping.
Er, if we remember back to our definition of what a policy is,
a mapping is going to be a map from states to actions.
So what that means in this case is that there are
two choices for every state and there are seven states.
And more generally that the [NOISE] number of policies is |A| to the |S|. So we can be large,
its exponential and the state-space but it's finite.
So it's bounded. Um, any
one want to take a guess of whether or not the optimal policy is always unique?
I told you the value function is unique.
Is the policy unique?
Yeah.
I think there might be cases where it's not.
Exactly right, um. It's not always unique.
The value function is unique but if there may be cases where you get ties.
And so there might be that there are two actions that,
um, are or two policies that have the same value.
So no. Depends on the process.
You mean like unique optimal value function?
Ah, yes.
So the question is can I explain what I mean by there's a unique optimal value function.
I mean that the optimal value of the state.
So the expected discounted sum of returns, um,
there is- there may be more than one optimal policy but there
exists at least one optimal policy which leads to the maximum value for that state.
Um, and there's a single value of that.
We'll talk about- probably a little bit clearer
to when we talk about contraction properties later.
Um, that there's- so for each state it's just a scalar value.
It says exactly what is the expected discounted sum of returns and this
is the maximum expected discounted sum of returns under the optimal policy.
Yeah.
And on the [inaudible] policies in our-
When we first define policies I thought I was describing the- um,
the entire hash table with sort of
one action per state rather than saying all possible combinations.
It's a little surprised that is 2 to the 7th rather than being
just the number of states with each one of the maps because of action.
For me to sort of better clarify, you know,
what this- what this how many policies there are and whether
maybe- there maybe it looked like it
was going to be linear and it's actually exponential.
Um, the way that we're defining a decision policy here,
um, a deterministic decision policy is a mapping from a state to an action.
And so that means for each state we get to choose an action and so
just as an illustration of why this ends up being exponential.
Um, so, in this case let's imagine instead of having
seven states we just have six or two states.
Now we have s_1 and s_2.
[NOISE] So, you could either have action a_1-a_1,
you could have action a_1-a_2,
you could have action a_2-a_1 or action a_2-a_2.
And you have to and all of those are distinct policies.
So, that's why the space ends up being exponential. Sure.
When you have like A to the power S. I'm assuming that A refers to
legal actions per state assuming like
you could have different actions depending on the state.
The question is whether or not you might be able to have
different constraints on the action space for state, absolutely.
So, in this case, today for simplicity,
we're going to assume that all actions are applicable in all states.
Um, in reality that's often not true.
Um, in many real-world cases,
um, some of the actions might be specific to the state.
Ah, for totally, there's a huge space of medical interventions.
Um, er for many of them,
they might not be at all even reasonable to ever consider,
um, for certain states are applicable.
Um, so, in general,
you can have different actions sub-spaces per
state and then you would take the product over the actions,
the cardinality of the action set that is relevant for each of the states.
But for right now, I think it's simple as just to think of it as there's
one uniform action space and then they can be applied in any state.
Okay. So, um, the optimal policy for an MDP and a
finite horizon problem where the agent acts forever.
Um, it's deterministic.
It's stationary which means it doesn't depend on the time-step.
We started talking about that a little bit last time.
Um, so, it means that if I'm in this state- if I'm in state s_7,
there is an optimal policy for being in
state s_7 whether I encountered that at time-step one,
time-step 37, time-step 242 stationary.
Um, er one of the intuitions for this is that if you get to act
forever there's always like
an infinite number of future time steps no matter when you're at.
So, if you would always do action a_1 from state s_7 now,
um then if you encounter it again in 50 time-steps you still have
an infinite amount of time to go from there and so you'd still
take the same action if that was the optimal thing to do.
As we were just discussing, it's not the optimal policy is not necessarily unique,
um because you might have ah more than one policy with the same value function.
So, how would we compute this?
One option is policy search uh and we'll talk a lot more about this in
a few weeks when we're talking about
function approximation and having really really large state spaces.
Um, but even in tabular cases,
er we can just think of searching.
So, the number of deterministic policies we just discussed is A to the S,
um and policy iteration is a technique that is generally better than enumeration.
So, what do I mean by enumeration in this context?
I mean there's a finite number of policies.
You could just evaluate each of them separately and then pick the max.
So, if you have a lot of compute,
you might just want to and this might be better if you really care
about wall clock and you have many many many processors.
You could just do this exhaustively.
You could just try all of your policies,
evaluate all of them either analytically or iteratively or
whatever scheme you want to use and then take the max over all of them.
But if you don't have kind of infinite compute,
it's generally more computationally efficient if you have to do
this serially to do policy iteration and so we'll talk about what that is.
So, in policy iteration what we do is we basically
keep track of a guess of what the optimal policy might be.
We evaluate its value and then we try to improve it.
If we can't improve it any more,
um then we can- then we can halt.
So, the idea is that we start by initializing randomly.
Here now you can think of the subscript is indexing which policy we're at.
So, initially we start off with some random policy and
then π_i is always going to index
sort of our current guess of what the optimal policy might be.
So, what we do is we initialize our policy randomly and while it's not changing
and we'll talk about whether or not it can change or go back to the same one in a second,
we do value function policy.
We evaluate the policy using the same sorts of techniques we just
discussed because it's a fixed policy
which means we are now basically in a Markov Reward Process.
And then we do policy improvement.
So, the really the new thing compared to what we were
doing before now is policy improvement.
So, in order to define how we could improve a policy,
we're going to define something new which is the state action value.
So, before we were just talking about state values,
state values are denoted by V. We're talking about
like V^pi(s) which says if you start in state s and you
follow policy pi what is the expected discounted sum of rewards.
A state action value says well,
I'm going to follow this policy pi but not right away.
I'm going to first take an action a,
which might be different than what my policy is telling me to
do and then later on the next time-step I'm going to follow policy pi.
So, it just says I'm going to get my immediate reward from taking this action a
that I'm choosing and then I'm going to transition to a new state.
Again, that depends on my current state and the action I just
took and from then on I'm going to take policy pi.
So, that defines the Q function
and what policy improvement does is it says okay you've got a policy,
you just did policy evaluation and you got a value of it.
So, policy evaluation just allowed you to compute what was the value of
that policy [NOISE] and now I want to see if I can improve it.
Now, remember right now we're in the case where we know
the dynamics model and we know the reward model.
So, what we can do then is we can do this with
Q computation where we say okay well I've got
that previous value function by policy and now
I compute Q^pi which says if I take a different action,
it could be the same and we do this for all A and for all S. So,
for all A and all S we compute this and then we're
going to compute a new policy and this is the improvement step which maximizes this Q.
So, we just do this computation and then we take the max.
Now, by definition this has to be greater than or equal to Q^πi(s, pi_i(a)),
right, because either a is equal to pi_i(a),
sorry pi_i(s). So,
either you the arg max is going to be the same as that
your previous policy π_i or it's going to be different and the only time you're going
to pick it differently as if the Q function of that alternative action is better.
So, by definition this Q^π that max over A of Q^π_i(s,a),
has to be greater than or equal to Q^π_i(s, π_i(s)). Question at the back.
Is this going to be susceptible?
Is this going to be like finding a local maximum goal then
its kind of gets stuck there and [inaudible] for actions.
Okay. So, this is going to allow us to maybe do some local monotonic improvement maybe,
um but are we going to be susceptible to gain stuck.
Um, in fact, ah for any of you that have played
around with reinforcement learning and and policy gradient and
stuff that is exactly one of the problems that can happen when we start doing
gradient based approaches nicely in this case this does not occur.
So, we're guaranteed to converge to the global optima and we'll see why for a second.
Okay. All right. So this is how it works.
You do this policy evaluation and then you compute the Q function and then
you compute the new policy that takes an arg max of the Q function.
So, that's how policy improvement works.
The next critical question is Iris was bringing up
is okay why do we do this and is this a good idea.
So, when we look at this,
um let's look through this stuff a little bit more.
What we're going to get is we're going to get,
um this sort of interesting type of
policy improvements step and it's kind of involving a few different things.
So, I just want to highlight the subtlety of it.
So, what is happening here is that we compute this Q function and then we've got this.
We've got max over A of Q^π_i(s,a) has to be greater than equal to R(s, π(a)).
The previous policy that we were using before.
[NOISE].
So, what I've done there is I've said, okay,
the max action over the Q has to be
at least as good as following your old policy by definition,
because otherwise you could always pick the same policy as
before or else you're gonna pick a better action.
And this reward function here is
just exactly the definition of the value of your old policy.
So, that means that you're- the max over your Q function
has to be at least as good as the old value you had.
So, that's encouraging. But here's the weird part.
So, when we do this,
if we instead take arg max we're gonna get our new policy.
So, what is this doing? It's saying,
I'm computing this new Q function.
What does this Q function represent?
It represents, if I take an action and then I follow my old policy from then onwards.
And then I'm picking whatever action is maximizing that quantity for each state.
Okay. So, I'm gonna do this process for each state.
But then- so that's going to just define a new policy, right?
Like I thought that might be the same or it could be a,
a different policy than the one you've had before.
Here's the weird thing.
So, this is saying that if you were to follow
that arg max A and then follow your old policy from then onwards,
you will be guaranteed to be doing better than you were before.
But the strange thing is that we're not gonna follow the old policy from then onwards.
We are going to follow this new policy for all time.
So, remember what we're doing is we're completely changing
our policy and then we're going to evaluate that new policy for all time steps,
not just for the first time step and then follow the old policy from then on.
So, it should be at least a little unclear that this is a good thing to do [LAUGHTER].
Should be like, okay, so you're,
you're saying that if I were to take
this one different action and then follow my old policy,
then I know that my value would be better than before.
But what you really want is that this new policy is just better overall.
And so the cool thing is that you can show that by doing
this policy improvement it is monotonically better than the old policy.
So, this is just saying this on a words, we're saying,
you know, if we took the new policy for one action,
then follow pi_i forever then we're guaranteed to be
at least as good as we were before in terms of our value function,
but our new proposed policy is just to always follow this new policy.
Okay. So, why did we get a monotonic improvement in
the policy value by doing this say in the policy value?
So, what- first of all what do I mean by a monotonic improvement?
Um, what I mean is that the value, uh,
something that is monotonic if, um,
the new policy is greater than equal to the old policy for all states.
So, it has to either have the same value or be better.
And my proposition is that the new policy is greater than or equal to
the old policy in all states with strict inequality if the old policy was suboptimal.
So, why does this work? So, it works for the following reasons.
Let's go ahead and just like walk through the proof briefly.
Okay. So, this is- what we've said here is that,
um, V^pi_i(s),
that's our old value of our policy.
So, this is like our old policy value.
Has to be less than or equal to max a of Q^pi_i(s, a).
And this is just by definition. Uh, let me write it like this.
Is equal to R(s, pi_i+1(s)). Because remember the way that we
define pi_i+1(s) is just equal
to the policy that match- maximizes the Q^pi_i.
Okay. So, this is gonna be by definition.
So, I've gotten rid of the max there.
Okay. So, this is going to be less than or equal to R the same thing at
the beginning times max over a of our Q^pi_i.
Again by definition, because we've said
that the first thing there that we know that the pie i of
s prime would also be less than or equal to max over a of Q^pi_i(s', a').
Okay. So, we just made that substitution.
And then we can re-expand this part using r reward.
So, this is gonna be the max over a' R(s',a') plus dot-dot-dot,
basically making that substitution from that line into there.
So, I'm nesting it. I'm re-expanding what the definition is of Q^pi.
And if you keep doing this forever,
essentially we just keep pushing in as if we get to continue to
take pi_i+1 on all future time steps.
And what- the key thing to notice here is that this is a greater than or equal to.
So, if you nest this in completely what you get is that this is the value pi_i+1.
So, there's kind of two key tricks in here.
The, the first thing is to say,
notice that the V^pi_i is always lower- is the lower bound to max a over Q^pi.
And then to re-express this using the definition of pi_i+1.
And then to re-upper bound that V by Q^pi and just keep re-expanding it.
And so you can do this out and then that allows you to
redefine to- when you substituted it in for all actions using pi_i+1,
then you've now defined what the value is of pi_i+1.
So, this is what it allows us to know that the new pi_i+1 value is by definition at least as good as the previous value function.
So, I'll just put that in there [inaudible]. All right.
So, the next questions that might come up is so we
know we're gonna get this monotonic improvement,
um, so the questions would be if the policy doesn't change, can it ever change again?
And is there a maximum number of iterations of policy iteration?
So, what do I mean by iterations?
Here iterations is i.
It's a kind of how many policies could we step through?
So, why don't we take like a minute and just think about this maybe talk to somebody
around you that you haven't met before
and just see what they think of these two questions.
So policy is monotonically improving
and is there a maximum number of iterations as we've read before?
[NOISE] Just in the interest
of time for today- just in the interest of
time for today because I want us to try to get through value iteration as well,
um, why doesn't- does somebody wanna give me, um,
a guess of whether or not the policy can ever- if the policy stops changing,
whether it can ever change again?
So, what I mean by that is that if the policy at pi,
so the question here was to say,
if pi of i+1 is equal to pi i for all states,
could it ever change again?
Somebody wanna share a guess of whether or not that is true.
Once it has stopped changing it can never change again.
So, no. And the second question is, um,
is there a maximum number of policy iterations? Yeah.
There's no- you can't have more iterations than there are policies.
That's right. There- We know that there is at most a to the s policies.
You cannot repeat a policy ever,
um, because of this monotonic improvement.
And so, there- there's a maximum number of iterations.
Okay? Great. And this just- um,
I'll skip through this now just so we can go through a bit of value iteration,
but this just steps through to show a little bit
more of how once your policy stopped changing,
essentially your Q^pi will be identical.
And so you can't- uh,
there's no policy improvements to be, yeah, to change.
After it's sort of converged, you're gonna stay there forever.
Okay, so policy iteration computes,
um, the optimal value in a policy in one way.
The idea in policy iteration is you always have a policy,
um, that is- that you know the value of it for the infinite horizon.
And then you incrementally try to improve it.
Value iteration is an alternative approach.
Value iteration in itself says we're gonna think of computing
the optimal value if you get to act for a finite number of steps.
The beginning just one step and then two steps and then three steps et cetera.
Um, and you just keep iterating to longer and longer.
So that's different, right? Because policy says you
always have a policy and you know what its value is.
It just might not be very good.
Value iteration says you always know what the optimal value in policy is,
but only if you're gonna get to act for say k time steps.
So they're just- they're computing different things,
um, and they both will converge to the same thing eventually.
So when we start to talk about value iteration,
it's useful to think about Bellman.
Um, so the Bellman equation and
Bellman backup operators are things that are often talked about in,
um, Markov Decision Processes and reinforcement learning.
So this constraint here that we've seen before,
which says that the value of a policy is
its immediate reward plus its discounted sum of future rewards,
um, is known as the Bellman equation.
The constraint for a Markov process, er,
Markov Decision Process say that it as to satisfy that.
And we can alternatively,
like what we were just seeing before,
think of this is as, um,
as a backup operator,
which means that we can apply it to
an old value function and transform it to a new value function.
So just like what we were doing in some of the, um, ah,
evaluation of a policy,
we can also just sort of do these operators.
In this case, the difference compared to what we've seen with
evaluation before is we're taking a max there.
We're taking this max a over th-
the best immediate already credit plus the discounted sum of future rewards.
So sometimes we'll use the notation of BV to mean a Bellman operator,
which means you take your old V and then you'd
plug it into here and you do this operation.
So how does value iteration work?
The algorithm can be summarized as follows.
You start off, you can initialize your value function to zero for all states.
And then you loop until you converge, um,
or if you're doing a finite horizon,
which we might not have time to get to today, but,
um, I- then you'd go to that horizon.
And basically, for each state,
you do this Bellman backup operator.
So you'd say, my value at k plus one time steps for that state is if I get to pick
the best immediate action plus the discounted sum of
future rewards using that old value function I had from the previous time step.
And that Vk said what is the optimal thing my optimal value for
that state s prime given that I got to act for k more time steps.
So that's why initializing it to zero is a good thing to do because in this case,
or a certainly reasonable thing to do if you want the result to
be the optimal as if you had that many time steps to go.
If you have no more time steps to act, your value is zero.
The first backup you do will basically say what is
the optimal immediate action you should take if you only get to take one action.
And then after that you start backing up,
um, and continuing to say well,
what if I got to act for two time steps?
What if I got to act for three time steps?
What's the best sequence of decisions you could do in each of those cases?
Um, again just in terms of
Bellman operations if we think back to sort of what policy iteration is doing,
you can instantiate this Bellman operator by fixing what the policy is.
And so, if you see sort of a B with, um, ah,
pi on top and saying, well,
instead of taking that max over actions,
you're specifying what is the action you get to take.
So policy evaluation you can think of as basically just
computing a fixed point of repeatedly applying
this Bellman backup until V stops converging and stops changing.
So, um, in terms of policy iteration,
this is very similar to what we saw before you can think of it
in these Bellman operators and doing this argmax.
Wanna see if we can get to a little bit on sort of the contraction operator.
So this is what, um, value iteration does.
It's a very similar policy iteration and evaluation.
Um, let me talk a little bit about the contraction aspect.
So, for any operator, um,
let's let O be an operator and x denote a norm of x.
So x could be a vector like a value function and then we could look at
like an L2 norm or an L1 norm or L infinity norm.
So, if you wanna- if an operator is
a contraction it means that if you apply it to two different things,
you can think of these as value functions, um,
then the distance between them shrinks after,
um, or at least is no bigger after you
apply the operator compared to their distance before.
So just to, um- actually,
I'll, I'll save examples for later.
Feel free to come up to me after class if you wanna see
an example of this, um, or I can do it on Piazza.
But this is the formal definition of what it means to be a contraction.
Is that the distance between,
in this case we're gonna think about it as two vectors, um,
doesn't get bigger and can shrink after you apply this operator.
So, the key question of whether or not value iteration will
converge is because the Bellman backup is a contraction operator.
And it's a contraction operator as long as gamma is less than one.
Which means that if you do- if let's say have two different Bell- er,
two different value functions and then you did the Bellman backup on both of them.
Then the distance between them would shrink.
So how do we prove this?
Um, we prove it- for interest of time I'll show you the proof.
Again, I'm happy to go through it,
um, I- or we can go through it in office hours et cetera.
Let me just show it kind of briefly.
So the idea to, to prove that the Bellman backup is a contraction operator,
is we consider there being two different value functions, k and j.
They don't have to be- This has- doesn't have to be anything to do with value iteration.
These are just two different value functions.
One could be, you know, 1,3,7,2 and the other one could be 5,6,9,8.
Okay. So we just have two different vectors of value functions and then we
re-express what they are after we apply the Bellman backup operator.
So there's that max a,
the immediate reward plus the discounted sum of
future rewards where we've plugged in our two different value functions.
And then what we say there is, well,
if you get to pick that max a separately for those two,
the distance between those is lower
bounded than if you kind of try to
maximize that difference there by putting that max a in.
And then you can cancel the rewards.
So that's what happens in the third line.
And then the next thing we can do is we can bound and say
the difference between these two value functions is diff- is,
um, bounded by the maximum of the distance between those two.
So you can pick the places at which those value functions most differ.
And then you can move it out of the sum.
And now you're summing over a probability distribution that has to sum to one.
And that gives you this. And so that means that
the Bellman backup as long as this is less than one has to be a contraction operator.
The distance between the two value functions can't be
larger after you apply the Bellman operator than it was before.
So, I think a good exercise to do, um,
is to then say given that it's a contraction operator,
um, that means it has to converge to a fixed point.
There has to be a unique solution.
So if you apply the Bellman operator repeatedly you- there is
a single fixed point that you will go to which is a single,
um, vector value fun- uh, values.
It's also good to think about whether the initialization and values impacts
anything if you only care about the result after it's converged.
All right. So, um, I think we can halt there.
Class is basically over.
There's a little bit more in the slides to talk about, um,
the finite horizon case, um,
and feel free to reach out to us on Piazza with any questions. Thanks. [NOISE]
 So, what we're gonna do today is we're gonna start to
talk about Model-Free Policy Evaluation.
Um, so, what we were discussing last time
is we started formally defining Markov processes,
Markov reward processes and Markov decision processes,
and we're looking at the relationship between
these different forms of processes which are
ways for us to model sequential decision-making under uncertainty problems.
So, what we're thinking about last week was,
well what if someone gives us a model of how the world works?
So, we know what the reward model is,
we know what the dynamics model is.
It still might be hard to figure out what's the right thing to do.
So, how do we take actions or how do we find a policy
that can maximize our expected discounted sum of rewards?
Um, if- even if we're given a model,
then we still need to do some computation to try to identify that policy.
So, what we're gonna get to very shortly is how do we do all of
that when we don't get a model of the world in advance.
But, let's just first a recap,
um, sort of this general problem of policy evaluation.
So, we heard a little bit about policy evaluation last time when we talked
about policy evaluation as being one step inside a policy,
um, iteration which alternated between policy evaluation and policy improvement.
So, the idea in policy evaluation is somebody gives you
a way to act and then you want to figure out how good that policy is.
So, what is the expected discounted sum of rewards for that particular policy?
And what we're gonna be talking about today is dynamic programming,
Monte Carlo policy evaluation, and TD learning.
As well as some of the ways that we should think
about trying to compare between these algorithms.
So, just as a brief recall, um,
remember that last time we defined what a return is for Markov reward process.
And a return for a Markov reward process that we defined by G_t was
the discounted sum of rewards we get from that particular time point t onwards.
So, we're gonna get an immediate reward of Rt and then after that,
we're gonna get Gamma,
where Gamma was our discount factor.
And remember we're gonna assume that's gonna be somewhere between zero and one.
And so, we're sort of weighing future awards generally less than the immediate rewards.
The definition of a state value function was the expected return.
And in general, the expected return is gonna be different from
a particular return if the domain is stochastic,
because the [NOISE] reward you might get when you try to drive to the airport
today is likely gonna be different than
the reward you get when you drive to the airport tomorrow,
because traffic will be slightly different,
and it's stochastic, varies over time.
And so, you can compare whether, you know,
on a particular day if it took you two hours to get to the airport versus on average,
it might take you only an hour.
We also defined the state action value function which was the expected reward,
um, if we're following a particular policy Pi but we start off by taking an action a.
[NOISE] So, we're saying if you're in a state s,
you take an action a, and from then onwards,
you follow this policy Pi that someone's given you.
What is the expected discounted sum of rewards?
And we saw that Q functions were useful because we
can use them for things like policy improvement,
because they allowed us to think about, well,
if we wanna follow a policy later but we do something slightly different to start,
can we see how that would help us improve in terms of the amount of reward we'd obtain?
So, we talked about this somewhat but as a recap, um,
we talked about doing dynamic programming for policy evaluation.
So, dynamic programming was something we could apply when we know how the world works.
So, this is when we're given the dynamics,
and I'll use the word dynamics or transition model interchangeably in this course.
So, if you're given the dynamics or the transition model p and the reward model,
then you can do dynamic programming to evaluate how good a policy is.
And so, the way we talked about doing this is that you initialize your value function,
which you could think of generally as a vector.
Right now, we're thinking about there being a finite set of states and actions.
So, you can initialize your value function for this particular policy to be zero,
um, and then you iterate until convergence.
Where we say the value of a state is exactly equal to
the immediate reward we get from following that policy in that state plus
the discounted sum of future rewards we get [NOISE]
using our transition model and the value that we've computed from a previous iteration.
And we talked about defining convergence here.
Convergence generally we're gonna use some sort of norm to compare
the difference between our value functions on one iteration and next.
So, we do things like this,
V_Pi_k minus V_Pi at k minus one [NOISE].
And wait for this to be smaller than some Epsilon.
Okay. So, just as a reminder to what is this quantity that we're computing representing?
Well, we can think of this quantity that we're computing, um,
as being an exact value of the k horizon value of state s under that policy.
So, on any particular iteration,
it's as if we know exactly what value we would get if we could
only act for a finite number of time steps like k time steps.
Says, you know, how good would it be if you followed
this particular policy for the next k time steps?
Equivalently, you can think of it as an approximation
of what the value would be if you acted forever.
So, if k is really large,
k is 20 billion,
then it's probably gonna be a pretty good approximation
to the value you'd get if you'd act forever.
And if k is one,
that's probably gonna be a pretty bad estimate.
This will converge over time.
So, I think it's useful to think about some of these things graphically as well.
So, let's think about this as you're in a state s,
which I'm denoting with that white circle at the top and then you can take an action.
So, what dynamic programming is doing is it's computing
an estimate of the V_Pi here at the top by saying,
"What is the expectation,
expectation over Pi of RT plus Gamma V_k minus one.
And what's that expectation over it's gonna be the probability of s prime given s,
Pi of s. Okay.
So, how do we think about this graphically?
Well, we started in this state,
we take an action and then we think about the next states that we could reach.
We're kind of again assuming that we're in a stochastic process.
So, maybe, you know, sometimes the red light is on and sometimes the red light is off.
So, depending on that, we are gonna be at a different next state,
we're trying to drive to the airport.
And then we can think about after we reach that state,
then we can take some other actions.
And in particular, we can take one action in this case because
we're assuming we're fixing what the policy is.
And then from those, that,
those actions would lead us to other possible states.
So, we can think of sort of drawing the tree of trajectories that we might
reach if we started in a state and start following our policy,
where whenever we get to make a choice,
there's a single action we take because we're doing policy evaluation.
And whenever there's sort of nature's choice,
then there's like a distribution over next states that we might reach.
So, you can think of these as the S-prime and
the S double-primes kind of time is going down like this.
So, this is sort of you know the,
the potential futures that your agent could arise in.
And I think it's useful to think about this graphically because
then we can think about how those potential futures,
um, how we can use those to compute what is the value,
a difference of this policy.
So, um, in what dynamic program what we're doing and
in general when we're trying to compute the value of a policy is,
we're gonna take an expectation over next states.
So, the value is the expected discounted sum of future rewards if we follow this policy,
and the expectation is exactly over these distributions of futures.
So, whenever we see
an action and then we think about all the next possible nodes we could get to,
we want to take an expectation over
those features and expectation over all the rewards we could get.
So, that's what dynamic programming is
or that's what we can think of this graph is doing.
And when we think about what dynamic programming is doing,
is it estimates this expectation over
all those possible futures by
bootstrapping and computing a one timestep expectation exactly.
So, what does it do? Again, it says,
"My V_Pi of s is exactly equal to r of s, Pi of s,
my immediate reward plus Gamma sum over probability of s prime given
s a V_Pi k minus one, the best part.
So, it bootstraps, and we're using the word bootstraps there because it's not
actually summing up all of these lower down potential rewards.
It's saying, "I don't need to do that."
Previously, I computed what it would be like if I started say
in this state and continued on for the future.
And so, I, now I already know what the value is at that state,
and I'm gonna bootstrap and use that as
a substitute for actually doing all that roll-out.
And also here, because I know what the expected discounted or I know what the,
um, sorry, the model is,
that it can also just take a direct expectation over s prime.
So, my question is, is there an implicit assumption here that
the reward at a given state and
thus the value function of evaluated states doesn't change over time.
So, like because you're using it from the prior iteration?
So, I think that question is saying, um,
is there an explicit assumption here that the value doesn't change over time?
Yes. The idea in this case is that the value that we're
computing is for the infinite horizon case and therefore that it's stationary.
It doesn't depend on the time step.
From that way we're not gonna talk very much about the finite horizon case today,
in that case it's different.
In this situation, we're saying at all time
steps you always have an infinite number more time steps to go.
So, the value function itself is a stationary quantity.
So, why is this an okay thing to do like we're bootstrapping?
Um, the reason that this is okay is because we actually have
an exact representation of this V_k minus one.
You're not getting any approximation error of putting that in
instead of sort of explicitly summing over lots of different histories.
Sorry, lots of different future rewards.
So, when we're doing dynamic programming
the things to sort of think about here is if we know the model,
then know dynamic model and know the reward model,
that we can compute the immediate reward exactly.
We can compute our expected sum over future states exactly,
and then we substitute in instead of thinking about,
we, instead of thinking about expanding this out as being a sum over rewards,
we can just bootstrap and use our current estimate of V_k minus one.
And the reason that I'm emphasizing this a lot is that when we start to
look at these other methods like Monte Carlo methods and TD methods,
they're not gonna do this anymore.
They're gonna do other forms of approximation of trying to compute this tree.
So, ultimately to compute the value of a policy,
what we're essentially doing is we're thinking about
all possible futures and what is the return we'd get under each of those futures.
And we're trying to make it tractable to compute that particularly when we don't know how
the world works and we don't have access to the dynamics model or the reward model.
Okay. So, just to summarize dynamic programming,
we should talk a little- a little bit about last time,
but we didn't really talk about the bootstrapping aspect.
Dynamic programming says the value of a policy is approximately equal to
the expected next- the expectation over pi of
immediate reward plus gamma times the previous value you computed requires a model,
it bootstraps the future return using an estimate,
using your V_k minus 1.
And it requires the Markov assumption.
And what- what I mean by that there is that, um,
you're not thinking about all the past you got to reach a certain state.
You're saying no matter how I got to that previous state,
my value of that state is identical,
um, and I can sort of assume that,
and I can compute that singly based on the current observation.
So, may I have any questions about this.
So, right now we're mostly recap of last time, um,
but sort of slightly pointing out some things that I didn't point out before.
Okay. So, those things are useful now that we're gonna be
talking about policy evaluation without a model.
So, what we're going to talk about now is Monte Carlo policy evaluation
which is something that we can apply when we don't know what the models are of the world,
and we're gonna talk a little bit about how we can start to think
about comparing these different forms of estimators,
estimators of the value of a policy.
So, in Monte Carlo policy evaluation,
um, we can again think about the return.
So, the returning and G_t are discounted sum of future rewards under a policy,
and the value of a policy we can represent now is just,
let's think about all the possible trajectories we could get,
um, under our policy and what's average all their returns.
So, we can again think about that tree we just constructed.
Each of those different sort of branches would have had a particular reward,
um, and then we're just going to get the average over all of them.
So, it's a pretty simple idea.
The idea is that the value is just equal to your expected return.
And if all your trajectories are finite,
you just can take a whole bunch of these and you average.
So, the nice thing about Monte Carlo policy evaluation is it doesn't require you to
have a sp- a specific model of the dynamics or reward.
It just requires you to be able to sample from the environment.
So, I don't need to know a particular like parametric model of how traffic works.
All I have to do is drive from here to the airport, you know,
hundreds of times, and then average how long it takes me.
And if I'm always driving with the same policy,
let's say I always take the highway,
um, then if I do that,
you know, 100 times, then I have
a pretty good estimate of what is
my expected time to get to the airport if I drive on the highway.
Well that is my policy.
So, it doesn't do bootstrapping,
it doesn't try to maintain at this root V_k minus 1.
Um, it's simply sums up all the rewards from each of
your trajectories and then averages across those.
It doesn't assume the state is Markov.
Just averaging doesn't- there's
no notion of the next state and whether or not that sufficient to,
um, to summarize the future returns.
An important thing is that it can only be applied to what are known as episodic MDPs.
You act forever if there's no notion
of- if this is sort of like averaging over your life this doesn't work,
[LAUGHTER] because, you only get one.
So, you need to have a process where you can
repeatedly do this many times and the process will end each time.
So, like driving to the airport might be really long,
but you'll get there eventually and then you can try again tomorrow.
So, this doesn't work for all processes like if
you have a robot that's just going to be acting forever,
can't do Monte Carlo policy evaluation.
Okay. So, we also often do this in
an incremental fashion which means that after we maintain a running estimate,
after each episode, we update our current estimate as V_pi.
And our hope is that as we get more and more data,
this estimate will converge to the true value.
So, let's look at, um, what the algorithm for this would be.
So, one algorithm which is known as
the First-Visit Monte Carlo on policy evaluation algorithm,
as we start off and we assume that we haven't- N here
is essentially the number of times we visited a state.
So, we start off and this is zero.
Also the return- the- or average return from starting in any state is also zero.
So, we initialize say right now or we think that
you know we get no reward from a state and we haven't visited any state.
And then what we do is we loop.
And for each loop we sample an episode which is we start in
the starting state and we act until our process terminates.
I start off at my house and I drive until I can get to the airport.
And then I compute my return.
So, I say okay, well maybe that took me two hours to get there.
So, now my G-i is two hours.
Um, but you've just compute your return and you compute it for
every time step t inside of the episode.
So, G_i,t here is defined from the t time step in that episode,
what is the remaining reward you got from that time step onwards,
and we'll instantiate this in our Mars Rover example in a second.
And then for every state that you visited in that particular episode,
for the first time you encountered a state,
you look- you increment the counter and you update your total return.
And you use, then you just take an average of those estimates to
compute your current estimate of the value for the state.
Now why you might be in the same state for more than one time step in an episode.
Well let's say I get to the red light, let's say I've discredited my time steps.
So, I look at my state every one minute.
Well, I got to a red light and there was a traffic accident.
So, on time step one I'm at the red light,
time step two I'm on the red light,
time step three I'm on the red light.
And so you can be in the same state for multiple time steps during the episode.
And what this is saying is that you only use the first time step you saw that state.
And then you sum up the rewards you get til the end of that episode. Okay.
We saw the state but in,
I guess like different time steps and the same episode,
we'd still be incremented twice because it's not- there's gonna be a gap between them?
The question is, what happens if we,
um, see the same state in the same episode?
In first visit, you only use the first occurrence.
So, you drop all other ones.
So, the first time I got to my red light then I would
sum up the future rewards till the end of the episode.
If I happen to get to the same red light during the same episode, I ignore that data.
We'll see a different way of doing that in a second.
Okay. So, how do we estimate whether or not this is a good thing to do.
How do we evaluate whether or not this particular- this is an estimate.
It's likely wrong at least at the beginning where we don't have much data.
So, how do we understand whether or not
this estimate is good and how are we going to compare
all of the estimators and these algorithms that we're going to be talking about today.
So, um, actually just raise your hand because I'm curious.
Um, who here has sort of formally seen definitions of bias and variance in other classes.
Okay. Most people but not quite everybody.
So, just as a quick recap, um,
let's think about sort of having a statistical model that is parameterized by theta,
um, and that we also have some distribution over some observed data p of x given theta.
So, we want to have a statistic theta hat which is a function.
So, theta hat is a function of the observed data and it provides an estimate of theta.
So, in our case, we're going to have this value,
this estimate of the value we're computing.
This is a function of our episodes and this is an estimate of the
true discounted expected rewards of following this policy.
So, the definition of a bias of an estimator is to compare what is
the expected value of our statistic versus the true value,
for any set of data.
So, this would say,
if I compute, you know,
the expected amount of time for me to get to
the airport based on trying to drive there three times.
Does the algorithm that I just showed you is that unbiased?
On average is that the same is the true expected time for me to get to the airport.
The definition of a variance of an estimator compares
my statistic to its expected value squared.
Expected over the, er, the, um,
the type of data I could get under
the true parameter and the mean squared error combines these two.
Mean squared error is normally what we care about.
Normally, we ultimately care about sort of how far away is
our estimate of the quantity we care about versus the true quantity?
And that's the sum of its bias and its variance.
And generally, different algorithms and
different estimators will have different trade offs between bias and variance.
Okay. So, if we go back to our First-Visit Monte Carlo
algorithm the V_pi estimator that we use there is
an unbiased estimator of the true expected discounted sum of rewards from our policy.
It's just a simple average, um, and it's unbiased.
And by the law of large numbers,
as you get more and more data,
it converges to the true value.
So, it's also what is known as as consistent.
Consistent means that it converges to the true value as the- as data goes to infinity.
So, this is reasonable, um,
but it might not be very efficient.
So, ah, as we just talked about,
you might be in the same state, you might be at
the same stoplight for many, many time steps.
Um, and you're only going to use the first state in an episode to update.
So, every visit at Monte Carlo,
simply says well every time you visit a state during the episode,
look at how much reward you got from that state
till the end and average over all of those.
So, essentially every time you reach a state,
you always look at the sum of discounted rewards
from there to the end of the episode and you average all of that.
Which is generally going to be more data efficient.
Bias definition, I guess I'm just a little confused how we would get biased,
even if we don't actually know theta.
How we compute bias. [NOISE]
Yeah, given that we don't know theta.
It's a great- the question is how do you compute bias?
Yes, if you, uh,
if you can compute bias exactly that normally means you know what theta is,
in which case why are you doing an estimator?
Generally, we do not know what bias is,
um we can often bound it.
So, often using things like concentration inequalities we can,
um, well concentration qualities are more for variance.
Often, um, we don't know exactly what the bias is,
unless you know what the ground truth is.
And there are different ways for us to get estimates of bias in practice.
So, as you compare across different forms of parametric models,
um, sometimes you can do is structural risk, ah, ah,
structural risk maximization and things like that to try to get
sort of a quantity of how you compare your estimator and your model class.
I'm not going to go very much
into that here but I'm happy to talk about it in office hours.
So, in every visit Monte Carlo,
we're just gonna update it every single time.
And that's gonna give us another estimator.
And note that that's gonna give us generally a lot more counts.
Because every time you see a state,
you can update the counts. But it's biased.
So, you can show that this is a biased estimator of
V_pi. May have intuition of why it might be biased.
So, in the first case for those of you that have seen this
before or not necessarily this particularly but seen this sort of analysis.
First visit Monte Carlo,
is you're getting IID estimates of a state,
of a state's return right?
Because you only take that, um,
each episode is, is
IID because you're starting at a certain state and you're estimating from there.
Ah, and you only use the return for the first time you saw that state.
If you see a state multiple times in the same episode,
are their returns correlated or uncorrelated?
Correlated. Okay. So, your data is no longer IID.
So, that's sort of the intuition for why when you mod- move to every visit Monte Carlo,
your estimator can be biased 'cause you're not averaging over IID variables anymore.
Is it biased for an obvious reasons to inspectors paradox? [inaudible]
I don't know. That's a good question.
I'm happy to look at it and return.
However, the nice thing about this is that it is a consistent estimator.
So, as you get more and more data,
it will converge to the true estimate.
And empirically, it often has way lower variance.
And intuitively, it should have way lower variance.
We're averaging over a lot more data points,
uh, typically in the same.
Now, you know, if you only visit one-
if you- if you're very unlikely to repeatedly visit the same state,
these two estimators are generally very close to the same thing in an episode.
Because you're not gonna have multiple visits to the same state.
But in some cases you're gonna visit the same state a
lot of times and you get a lot more data
and these estimators will generally be much better if you use every visit,
but it's biased. So, there's this trade-off.
Empirically, this is often much better.
Now, of course in practice often instead
of the- often you may wanna do this incrementally.
You may just want to kind of keep track of a running mean and then
you keep track of your running mean and update your counts sort of incrementally.
And you can do that if also as you visit you don't
have to wait until the end lessons- oh, that's wrong.
You do have to wait till the end because you always have to wait till you
get the full return before you can update. Yeah, in the back.
So, a question on that, if you could like- if you condition on the fact that you have
the same number of estimates approximately in each of the states,
would then the two be more or less equivalent but the other one would be less biased.
For example, if you did I guess there is no way you could
have for example a same number of episodes,
ah, the same number of count in each state with the first visit approximation.
But if you did have that,
would you imagine that the episode would be lower in that case?
I would- expressions about if you have
the same number of counts to a state across the two algorithms.
And in terms of the episodes,
you couldn't have that be the case
unless- so they'd need to be identical if you only visit one state,
um, once in an episode and then they'd be totally identical.
If it's not the case, if you visit, um,
a state multiple times in,
in one episode, then, uh,
by the time you get to the same counts,
the one for the single visit would be
better 'cause it's unbiased and it would have basically the same variance.
Any other questions about that?
Cool. Um, so, incremental Monte Carlo, um,
on policy evaluation is essentially the same as before except where you can
just sort of slowly move your running average for each of the states.
And the important thing about this is that,
um, as you slowly move your estimator,
if you set your alpha to be 1 over Ns,
it's identical to every visit Monte Carlo.
Essentially, you're just exactly computing the average.
Um, but you don't have to do that.
So, you can skew it so that you're running average is more weighted towards recent data.
And the reason why you might wanna do that is
because if your real domain is non-stationary.
We have a guess of where,
where domains might be non-stationary.
It's kind of an advanced topic.
We're not gonna really talk about non-stationary domains for most of
this class though in reality, they're incredibly important.
Um, I don't know if your mechanical parts are breaking down or something's off.
Example of like if you're in a manufacturing process and
your parts are changing- are breaking down over time.
So, your dynamics model is actually changing over time.
Then you don't want to reuse your old data because
you're- actually your MDP has changed over time.
So, this is one of the reasons often empirically like when
people train recommender systems and things like that,
you know, the, the news all these things are non-stationary.
And so people often retrain them a lot to deal with this non-stationarity process.
Do I see a question on the back?
Okay. Yeah. So, empirically that's often really helpful for non-stationary domains,
but if it's non-stationary there's all- there's a bunch of different concerns.
So, we're going to mostly ignore that for now.
Okay. So, let's just check our understanding for a second.
For Monte Carlo, for on policy evaluation.
Let's go back to our Mars rover domain.
So, in our Mars rover,
we had these seven states. Our rover dropped down.
It was gonna explore,
a reward is in state S_1,
one and state S_7 it's plus 10 everywhere else at zero.
And our policy is gonna be A_1 for all states.
And now imagine we don't know what the dynamics model is.
So, we're just gonna observe trajectories.
And if you get to either state one or state seven,
the next action you take terminates the reward.
I don't know. Maybe it falls off a cliff or something like that.
But whenever you get to S_7 or S_1,
then the next action you take so you get whatever reward.
You either get the one or you get the 10 and then your process terminates.
So, let's imagine a trajectory under this policy would be you start in S_3.
You go to action- take action A_1,
you get a reward of zero. This is for reward.
Then you transition to state S_2,
you take an action of A_1, you get a zero.
You stay in the same state.
So, you stay in S_2 again.
Take action A_1, you get another reward of zero and then you reach state S_1,
take an action A_1,
you get a 1 and then it terminates.
So, it's one experience of your Mars rover's life.
So, in this case,
how about we just take a minute or two,
feel free to talk to a neighbor and compute what
is the first visit Monte Carlo estimate of
the value of each state and what is the every visit Monte Carlo estimate of state S_2?
Then I put the algorithm for both first visit at every visit above just depends on
whether you update the state only once for
this episode or whether you can potentially update it multiple times.
[NOISE] You may ask the question too if we have not seen it yet what is the value we use.
So, the value you can also say that you initialize V_Pi of S
equal to zero for all S if you haven't seen it yet.
[NOISE] All right.
Raise your hand if you'd like a little bit more time otherwise we'll go ahead.
Okay. So what- someone wanna to share what they and maybe somebody nearby them
thought was the first visit Monte Carlo estimate of V for every state.
I think the first- this estimates is
really one for every single state except for the last one that's sent.
Which states? We've only updated a few of them so far.
Which why don't you give me the full vector.
Like okay we'll just start here.
So, V of S_1 is what?
Is one.
Okay. And V of S_2?
Is also one.
And V of S_3?
Is also one.
And V of S_4?
Also one. [NOISE].
Anybody disagree.
Zero.
Zero. Okay and V of S_5? Zero. And V of S_6?
Zero.
And V of S_7? [OVERLAPPING] Yeah.
So, we only get to update in this one that the states we've actually visited.
Okay. So, here it's one,
one, one. Zero, zero, zero, zero.
Now what about for every visit the Monte Carlo estimate of just S_2.
So, I picked only S_2 'cause that's the only state we visit twice.
What's its, what's its estimate?
Well, we increment. Yeah.
Is it still gonna be one?
Yeah, yes it is and why?
Because incremental also we have Ns is two at the end of it but Gs is also two.
So, the increment both twice.
Exactly. So, the return from both times when you started in
S_2 and got an added up till the end of the episode was one in both cases.
So, it was one twice and then you average over that so it's still one. Yeah.
Is the reason that they're all one because gamma's one?
'Cause like shouldn't there be some gamma terms in there.
Oh, good question. So, here we've assumed gamma equals one,
otherwise there would be- there'd be a gamma multiplied into some of those two.
Yeah, good question.
I chosen gamma equal to one just to make the math a little bit easier.
Otherwise, it'd be a gamma factor tpo. Okay great.
So, you know, the, the second question is a little bit of
a red herring because in this case it's exactly the same.
But if the return had been different from S_2, um,
like let's say there was a penalty for being in a state,
then they could have had different returns
and then we would have gotten something different there.
Okay. So, Monte Carlo in this case updated- we had to wait till the end of the episode,
but when we updated it till the end of the episode,
we updated S_3, S_2, and S_1.
So, what is Monte Carlo doing when we think about
how we're averaging over possible futures.
So, what Monte Carlo is doing, um,
I've put this sort of incremental version here which you could
use for non-stationary cases but you can think of it in the other way too.
Um, so, and remember if you want this just to be equal to every visit,
you're just plugging in 1 over N of S here for alpha.
So, this is what Monte Carlo Evaluation is doing
is it's just averaging over these returns.
So, what we're doing is if we think about sort of what our tree is doing,
in our case our tree is gonna be finite.
We're gonna assume that each of these sort of branches eventually terminate.
They have to because we can only evaluate a return once we reach it.
So, at some point like here when we got to state S_1 or
S_7 in our Mars example, the process terminated.
And so what does Monte Carlo policy evaluation do?
It approximates averaging over all possible futures by summing up one,
uh, trajectory through the tree.
So, it samples the return all the way down till it gets to a terminal state.
It adds up all of the rewards along the way.
So, like reward, reward, reward.
Well, I'll be more careful than that.
Reward, reward.
Here you get a reward for each state action pair.
So, you sum up all the rewards in this case.
Um, and that is its sample, um, of the value.
So, notice it's not doing any, um, er,
the way it's gonna get into the expectation over states,
is by averaging and across trajectories.
It's not explicitly looking at the probability of
next state given S and A and it's not bootstrapping.
It is only able to update,
when you get all the way out and see the full return.
So, so, this is it samples.
It doesn't use an explicit representation of a dynamics model,
and it does not bootstrap because there's no notion of VK minus 1 here.
It's only summing up a- all of the returns. Questions? Scotty.
[inaudible] policy evaluation like this would do a very poor job in rare occurrences?
Well, it's interesting. Question is,
is it fair to say that this would do a really bad job in very rare occurrences?
It's intriguing. They're very high variance estimators.
So if you're- Monte Carlo,
in general, you essentially just like rolling out futures, right?
And often you need a lot of possible futures until you can get a good expectation.
On the other hand, for things like AlphaGo which is one of
the algorithms that was used to solve the board game Go, they use Monte Carlo.
So, you know, I think, um,
you wanna be careful in how you're doing some of
this roll out when you start to get into control.
And when you start to- because then you get to pick the actions, um,
and you often kind of want to play between,
but it- it's not horrible even if there's rare events.
Um, er, but if you have other information you can use, it's often good.
It depends w-what your other options are.
So, generally this is a pretty high variance estimator.
You can require a lot of data,
and it requires an episodic setting because you
can't do this if you're acting forever because there is no way to terminate.
So, you have to be able to tell processes to terminate.
So, in the DP Policy Evaluation we had the gamma factors,
because we wanted to take care of the cases
where state were seen in-between that started with a probability equals to one.
But in this case, um,
if we had such a case that would never terminate,
right, because the episode would never end.
So, technically, do we still need a gamma factor to evaluate policy equation,
uh, policy evaluation on?
The question was about,
do we still need a gamma factor in these cases,
and what about cases where you could have self-loops or small loops in your process?
So, um, this G in general can,
you know, can use a gamma factor.
So, this can include a gamma when you compute those.
You're right, that if the process is known to terminate,
you don't have to have a gamma less than one because
your reward can't be infinity because your process will always terminate.
Um, this could not handle cases where there's some probability it will terminate.
So, if there is a self-loop inside of- or a small loop inside of your process,
such that you could go round it forever and never terminate,
you can't do Monte Carlo, and having a good discount there won't help.
There are physical reasons why you might have a gamma models like that, which is great,
say you model the fuel cost or something,
or something would interact, would that be reasonable?
The question is whether or not there might be a physical reason for
gamma like fuel costs or things like that.
I mean, I think normally I would put that into the reward function.
Good. So, if you have something like- you can have it.
So, I keep thinking about cases where
basically you want to get to a goal as quickly as possible,
um, and you want to sort of do a stochastic shortest paths type problem.
Um, I think generally there I would probably rather pick making it a terminal
state and then having like
a negative one cost if you really have a notion of how much fuel costs.
Um, but you can also use it as a proxy to try to encourage quick progress towards a goal.
The challenge is that how you set it is often pretty subtle because if you set it
too high you can get weird behavior where
your agent has sort of effectively like too scared to do anything,
it will stay at really safe areas.
Um, and if it's too high in some cases,
if it's possible to get sort of trivial reward,
your agent can be misled by that.
So, it's often a little bit tricky to set in real-world cases.
Okay. So, they're high variance estimators that require these episodic settings,
um, and, um, there's no bootstrapping.
And generally, they converge to the true value under some,
uh, generally mild assumptions.
We're gonna talk about important sampling at the end of class if we have time.
Otherwise, we'll probably end up pushing that towards later.
That's for what how we do this if you have off policy data,
data that's collected from another policy.
Okay. Now let's talk about temporal difference learning.
So, if you look at Sutton and Barto, um,
and if you talk to Rich Sutton or, ah,
number of, uh, and a number of other people that are very influential in the field,
they would probably argue that these central, um,
contribution to reinforcement learning or contribution to reinforcement learning
that makes it different perhaps than some other ways of thinking about adaptive control,
is the notion of temporal difference learning.
And essentially, it's going to just combine between
Monte Carlo estimates and dynamic programming methods.
And it's model-free.
We're not going to explicitly compute a dynamics model or reward model or
an estimator of that from data and it both bootstraps and samples.
So, remember, dynamic programming as we've defined it so far,
um, it bootstraps, er,
and the way we have thought about it so far you actually have access
to the real dynamics model and the real reward model,
but it bootstraps by using that VK minus one.
Monte Carlo estimators do not bootstrap.
They go all the way out to the end of the trajectory and sum up the rewards,
but they sample to approximate the expectation.
So, bootstrapping is used to approximate the future discounted sum of rewards.
Sampling is often done to approximate your expectation over states.
The nice thing about temporal difference learning is you can do it
in episodic processes or continual processes.
And the other nice aspect about it is that you don't have to wait till the end of the,
uh, the episode to update.
So as soon as you get a new observation, taking, ah,
starting in a state taking an action and going to a next state and getting some reward,
um, you can immediately update your value.
And this can be really useful because you can
kind of immediately start to use that knowledge.
Okay. So, what are we gonna do in temporal difference learning?
Again, our aim is to compute our estimate of v pi.
And we still have the same definition of return,
um, and we're gonna look at remind ourselves of the Bellman operator.
So, if we know our MDP models,
our Bellman operator said we're gonna get
our immediate reward plus our discounted sum of future rewards.
And in incremental every visit Monte Carlo,
what we're doing is we're updating our estimate using one sample of the return.
So, this is where we said our va-
our new value estimate of the value is equal to our old estimate
plus alpha times the return we just saw minus
V. But this is where we had to wait till the end of the episode to do that update.
What the inside of temporal difference learning is, well,
why don't we just use our old estimator of
v pi for that state and then you don't have to wait till the end of the episode.
So, instead of using GI there you use the reward you just saw plus
gamma times the value of your next state. So, you bootstrap.
Say I'm not going to wait till I get only an episode,
started my state, I got a reward,
I went to some next state.
What is the value of that next state? I don't know.
I'll go look it up in my estimator and I'll plug that in and I'll treat that as,
uh, as an estimate of the return.
So, the simplest TD learning algorithm is exactly that,
where you just take your immediate reward plus your
discounted expected future value
where you plug that in for the state that you actually reached.
Now, notice that this is sampling.
There is no- normally we would have like that nice sum.
The Bellman operator we would normally have a sum over
S prime probability of S prime given s a of v pi of S prime.
We don't have that here.
We're only giving you a single next state.
And we're plugging that in as our estimator.
So we're still going to be doing sampling to approximate that expectation.
But just like dynamic programming we're going to bootstrap
because we're gonna using our previous estimate of v pi.
We also write this as like a sub a and sub k minus one to show like the iterations.
Yeah. I might have down there if you want to see. No, I don't in this case.
You could also write this with- um,
question is if we want just to be clear about what is happening in terms of iterations.
You can also think of this as p of k plus one and this is V of k,
for example, you're updating this over time.
The thing is is that you're doing this for
every single state compared to dynamic programming,
where you do this in ways where for all states-
so you have sort of a consistent VK and then you're updating.
Here we can think of there as just being a value function and you're just sort of
updating one entry of that value function depending on which state you just reached.
So there's not kind of this nice notion of
the whole previous value function of any value function.
I'll keep that there just for that reason.
Now, people often talk about the TD error,
the temporal difference error.
What that is is it's comparing what is your estimate here.
So, your new estimate,
which is your immediate reward plus gamma times your value of the state you actually
reached minus your current estimate of your value.
Now, notice this one should have been sort of essentially
approximating the expectation over S prime.
Because for that one you're going to be averaging.
And so this looks at the temporal difference.
So this is saying how different is
your immediate reward plus gamma times your value of your next state,
versus your sort of current estimate of the value of your current state.
Now note that that doesn't have to go to zero because
that first thing is always ever just a sample, it's one future.
The only time this would be defined to go to zero is if this is deterministic,
so there's only one next state.
So, you know, if half the time when I try to drive to
the airport I hit traffic and half the time I don't,
then that's sort of two different next states
that I could go to for my current start state,
either hit traffic or don't hit traffic.
Um, and so I'm either going to be getting that v
pi of hitting traffic or v pi of not hitting traffic.
So this TD error will not necessarily go to zero even with infinite data because one is
an expected thing from the current state and the
other is which actual next state did you reach.
So, the nice thing is that you can immediately update this
value estimate after your state action
reward s prime tuple and you don't need episodic settings. Yeah, Scotty?
Does that affect convergence if you keep alpha constant?
Yes, good question. Does this affect convergence if you keep alpha constant?
Yes, and you normally have to have some mild assumptions on decaying alpha.
So, things like one over T is normally
sufficient to ensure these estimates convert. Yeah, question?
Um, can you say anything about the bias of this estimator?
Yeah. The question was whether- question was a good one,
what can you say anything about the bias of this estimator?
Am I having a sense of whether this is going to be a biased estimator?
What of your previous or we have a sense of whether it's going to be biased?
Well think back to dynamic programming,
was V_k minus one.
Um, an unbiased estimator of infinite horizon.
Like, let's say, k is equal to two if we want the infinite horizon value.
Is that- no matter how you've done those updates, it's not going to be cool.
Generally, when you bootstrap, um,
it's going to be a biased estimator because you're
relying on your previous estimator which is generally wrong.
[LAUGHTER]. So, that's going to be biasing you in one particular direction.
So, it's a definitely a biased estimator.
Um, it also can have fairly high variance.
[LAUGHTER]. So, it can both be high-variance and be biased.
But on the other hand, you can update it really, really quickly.
Um, you don't have to wait till the end of the episode and you can use a lot of information.
So, it's generally much less high-variance than, um, im- um,
Monte Carlo estimates because you're
bootstrapping and that sort of helping average over a lot of your of variability.
[inaudible]
Now, this question is whether or not it's a function of the initialization. It's not.
It's a, it's a function of the different properties of
the estimators you could initialize differently.
Um, the, the bootstrapping is because you're using a- by bootstrapping and
using this V_Pi as a proxy for your real expected discounted sum of returns,
um, unless this is the true value,
it's just going to bias you.
Note that this, um, this doesn't- you don't get biased in
dynamic programming when you know the models because that V_Pi,
when you bootstrap it's actually V_Pi.
This is actually the real value.
So, the problem is the- here is that it's
an approximation of the real value and that's why it's biasing you.
So bootstrapping is fine if you know the real dynamics model.
The real reward functions,
you need computed the Pi of k minus one exactly,
um, but it's not okay here because we're introducing bias.
So, how does TD zero learning work?
Um, I do zero here because there's sort of some interesting, um,
in-between between TD learning and Monte-Carlo learning where instead of doing
an immediate reward plus the discounted sum of
future rewards versus summing all of the rewards,
you can imagine continuums between these two where you
may be- some up the first two rewards and then Bootstrap.
[NOISE]. So, there's, um,
there's a continuum of models,
there's a continuum of algorithms between just taking
your immediate reward and then bootstrapping versus never bootstrapping.
Um, but we're just gonna talk right now about
taking your immediate reward and then bootstrapping.
So TD learning works as follows: You have to pick a particular alpha,
um, which can be a function of the time-step.
Um, you initialize your value function,
you sample a state action reward, next state.
Now in this case,
because we're doing policy evaluation,
let me- this will be equal to Pi of st,
and then you update your value.
Okay. So let's look, um,
again at that example we had before.
So we said that for first visit Monte Carlo,
you will get 1110000,
for every visit it would be one.
What is the TD estimate of all states at the end of this episode?
So, notice what we're doing here.
We loop, we sample a tuple,
we update the value of the state we just reached.
We get another tuple, we sample it.
So, what would that look like in this case?
We would start off and we'd have S3,
we'd have S3, A1, zero, S2.
You'd have S2, A1, zero, S2, S2,
A1, zero, S1, S1,
A1 plus one, terminate.
So, why don't you spend a minute and,
and think about what the value would be under TD learning,
and what implications this might have too.
[NOISE].
Does anybody wanna say what the value is,
that you get? [NOISE]. Yeah.
Uh, one followed by all zeros.
That's right. Okay. One followed by all zeros.
So, we only updated the final state in this case.
I also just wanted to- yeah, question.
Um, explain why that happens.
Yeah, because, um, what we are doing in
this case is that we get a data point so what- we're in a state,
we take an action, we get a reward, we get next state.
We update the value only for that state.
So what we did here is we got S3,
we update it, we did action A1,
we got a zero S2.
So our new value for S3 was also equal to zero.
Then we went to S2, we took action A1,
we got a zero, we went to S2,
we got- so we updated S2,
it was also zero. We did that again.
We finally got to state S1 and we got a one.
So, the thing about this that can be beneficial
or not beneficial is you throw away your data in the most naive format.
You have a SAR S-prime tuple and then it goes away again. You don't keep it.
So when you finally see that reward,
you don't back up,
you don't propagate that information backwards.
So what Monte Carlo did is,
it waited until he got all the way there and then it computed the return for every state
along the episode which meant that that's why we got 1111.
But here you don't do that.
By the time you get to, um,
[NOISE] S1, you've thrown away the fact that you were ever in S3
or S2 and then you, you don't update those states.
I mean total reward is proportional to
the number of samples you need to get a good estimate of value function?
Say that again.
Ah, I'm assuming that like the longer it
takes for you to get a rewards, the more samples,
you'd need to like properly estimate,
uh, value of the function.
Question out [inaudible] is sort of, you know,
how long does it take you to get a reward and
how many samples do you need to get a good estimate of the value function?
Um, you mean for all states?
It's a little nuanced.
Um, it depends on the transition dynamics as well.
Um, you couldn't- say for a particular,
like how, how many, um,
samples you need for a particular state to get a good estimate of its reward?
Let's say your rewards are stochastic.
But in terms of how long it takes you to propagate this information back,
it depends on the dynamics.
Um, so in this case, you know,
if you had exactly the same trajectory and you did it again,
then you'd end up updating that S2 and then if you got that same trajectory again,
then you would propagate that information back again to
S2 and then one more time and then you get it back to S3.
I should S3 and there's the third one.
So, you can slowly this- propagate this information back, um,
but you don't get to kind of immediately like what Monte Carlo would do. Question.
I was wondering if you could highlight the differences between this
and the Q learning that we talked about last time?
Because they seem like kind of similar ideas.
That's great. So, exactly right.
In fact, TD learning and Q learning are really close analogs.
Q learning is, um, when you're gonna do control.
So, we're going to look at actions.
TD learning is basically Q learning where you're fixing in the policy,
Yeah. Next question back there.
Like you're actually like implement this so you would
you would keep looping right,
and updating or you just run through, uh, rewards?
It depends. So, um, it depends who asked you.
So if you're really, really compare- concerned about memory,
um, you just drop data,
so then you're on [inaudible].
If, um, in a lot of the existing sort of deep learning methods,
you maintain a sort of a,
a episodic replay buffer and then you would re-sample
samples from that and then you would do this update for the samples from there.
So you could revisit sort of past stuff and use it to update your value function.
Um, you could also- it can,
it can matter the order in which you do that.
So in this case, you could do a pass through your data
and then do it- another pass or maybe go backwards from the end.
[inaudible] it will end up propagating.
Some alpha back to S_2 there.
Yeah.
So, you just go into like convergence or-
We'll talk about that very shortly. Yes. That's a great question.
Like so what happens is you do this for
convergence and we'll talk about that in a second. Yeah.
So, just so I make sure I understand.
So, when we talk about sampling of tuple,
what's really happening is you're going to
a trajectory and you're iterating through the SAR,
the SAR tuples in that trajectory in order.
Right. But we're thinking of this really as acting as- to repeat the question.
The question is like we're going through
this trajectory we're updating in terms of tuples.
Yes, but we're really thinking about this as like your agent being in
some state taking an action getting reward again and getting to a next state.
So, there doesn't exist a full trajectory.
It just like I'm driving my car,
what's gonna happen to me in the next like two minutes?
So, I don't have the full trajectory and that I'm iterating through it.
It's like this is after every single time step inside of that trajectory, I update.
So, I don't have to wait till I have the full trajectory.
Right and, and I guess I'll just the order in which those tuples are chosen.
I- I'm guessing it matters or with the values that you're getting and estimates.
Yes. So, the question is like, you know,
the order in which you receive tuples,
that absolutely affects your value.
Um, so in, uh,
if you're getting this in terms of how you experience this in the world,
it's just the order you experience these in the world.
So, this S_t plus prime- T plus one prime becomes your ST on the next time-step.
So, these aren't being sampled from a trajectory.
It's like that's just wherever you are now.
Um, if you have access to batch data,
then you can choose which ones to pick and it absolutely affects your convergence.
The problem is you don't have to know which ones to pick in advance.
Questions. The other thing I just want to mention there is it's a little bit subtle, um,
that if you set alpha equal to, like, you know,
1 over T or things like that,
you can be guaranteed to,
um, for these things to converge.
Uh, sometimes if alpha is really small, um,
also these are going to be guaranteed to converge under minor conditions.
Um, but if you said something like alpha equals one,
it can definitely oscillate.
Alpha equals one means that essentially,
you're ignoring your previous estimate, right?
So, if you set alpha equal to one then you're just using your TD target.
All right. Okay. So, what is temporal policy difference policy evaluation
doing if we think about it in terms of this diagram and
thinking about us as taking an expectation over futures.
So, it's, um, this is the equation for it up there.
And what it does is it updates its value estimate by using a sample of S_t plus 1
to approximate that expected next state distribution or next future distribution.
And then it bootstraps because it plugs in
your previous estimate of V_pi for this plus 1.
So, that's why it's a hybrid between
dynamic programming because it bootstraps and Monte Carlo
because it doesn't do an explicit expectation over all the next states, just samples one.
Okay. So, now why don't we think about some of these things that, like,
allow us to compare between these different algorithms and
their strengths and weaknesses and it sometimes depends on the application.
Um, uh, you've had to pick which one is most popular,
probably TD learning is the most popular but it depends on the domain.
It depends on, um, whether you're constrained by data or,
um, you know, computation or memory et cetera. All right.
So, um, why don't we spend a few minutes on this briefly.
So, let us spend a minute and think about which
of these properties from what you remember so far apply to these three algorithms.
So, whether they're usable when you have no models of the current domain, um,
whether they handle continuing non-episodic domains,
they can handle non-Markovian domains.
They converge to the true value in the limit.
We're assuming everything's tabular right now,
we're not in function approximation land.
And whether or not you think they give you an unbiased estimate of the value.
So, if at any time point if you were to take your estimator if it's unbiased.
So, why don't you would just spend a minute see if you can fill in this table.
Feel free to talk to someone next to you and then we'll step through it.
[NOISE] All right which of
these are usable when you have no models of the current domain?
[NOISE] Does dynamic programming need a model of the current domain?
Yes.
Yes. Okay. What about Monte Carlo?
Usable.
Usable. What about TD?
Usable.
Usable. Yeah. Do either of those,
TD is known as what?
As a model free algorithm,
doesn't need an explicit notion.
It relies on sampling of the next state from the real world.
[NOISE] Which of these can be used for continuing non-episodic domains?
So, like, your process might not terminate, ever.
Okay. Well, can TD learning be used?
Yes.
Yes. Can Monte Carlo be used?
No.
No. Can DP be used?
Yes.
Yes. Okay. Which of these,
um, does DP require Markovian?
Yes.
It does. Which- does Monte Carlo require Markovian?
No. Does TD require Markovian?
Yeah, it does. So, um, uh,
temporal difference and dynamic programming rely on the fact that
your value of the current state does not depend on the history.
So, however you got to the current state,
it ignores that, um,
and then it uses that when it bootstraps too,
it assumes that doesn't- so,
Monte Carlo just adds up your return from
wherever you are at now till the end of the episode.
And note that depending on when you got to that particular state,
your return may be different and it might depend on the history.
So, Monte Carlo doesn't rely on the world being Markov.
Um, you can use it in partially observable environments.
TD assumes that the world is Markovian,
so does dynamic programming in the ways we've defined it so far.
So, you bootstrap you say, um,
for this current state my prediction of
the future value only depends on this current state.
So, I can say I get my immediate reward plus whatever state I transition to.
But that's sort of a sufficient statistic of
the history and I can plug-in my bootstrap estimator.
So, it relies on the Markovian assumption.
What about non-Markovian domain where do we apply it?
Um, the question is well,
what do you mean by non-Markovian?
Like, these are algorithms you could apply them.
Um, so yeah. You can apply these algorithms to anything.
The question is whether or not they're guaranteed
to converge in the limit to the right value.
And they're not, if the world is not Markovian and they don't.
Like [LAUGHTER] we've seen in some of our work on intelligent tutoring systems,
earlier on we were using some data, um,
from a fractions tutor and we're applying Markovian techniques and they don't converge.
I mean, they converge to something that's just totally
wrong and it doesn't matter how much data you have
because you're- you're using methods that rely on assumption that is incorrect.
So, you need to be able to evaluate whether they're not
Markovian or try to bound the bias or do something.
Um, otherwise your estimators of what the value is of
a policy can just be wrong even in the limit of infinite data.
Um, what about converging to the true value in the limit?
Let's assume we're in the Markovian case again.
So, for Markovian domains, does,
um, DP converge to the true value in the limit?
Yes.
What about Monte Carlo?
Yes.
Yes. What about TD?
Yes.
Yes. They certainly do.
The world is really Markovian, um, everything converges.
Asymptotically no under minor assumptions,
all of these require minor assumptions.
Um, uh, so under minor assumptions it will converge to the true value of the limit,
depends on, like, the alpha value.
Um, uh, what about being an unbiased estimate of the value,
is Monte Carlo an unbiased estimator?
Yes.
Yes. Okay. TD is not.
DP is a little bit weird.
It's a little bit not quite fair question there.
DP is always giving you the exact VK minus one value for that policy.
So, that is perfect,
that's the exact value.
If you have K1- K minus 1 more time steps to act,
that is not going to be the same as the infinite horizon value function. Yeah.
Can you explain how the last two lines are different.
Like I don't understand the difference between unbiased estimator of
value and something that converges to the true value of order.
Your question's great. So, the question is what's the difference
between something being unbiased and consistent?
Um, so when we say converges to the true value in limit,
that's also known as formally being a consistent estimator.
So, the unbiased estimate of the value means,
if you have a finite amount of data and you
compute your statistic, in this case the value,
and you compare it to the true value, then on average,
that difference will be zero and that is not true for things like TD.
But, um, and that can be- that's being evaluated for finite amounts of data.
What consistency says if you have an infinite amount of data,
will you get to the true value?
So, what that implies is that,
say for TD, that asymptotically the bias has to go to zero.
If you have infinite amounts of data,
eventually its bias will be zero.
But for small amount, you know,
for finite amounts of data and really,
you know, you don't know what that N is.
Uh, it could be a biased estimator but as the amount of
data you have that goes to infinity then it has to converge to the true value.
So, you can have things that are biased estimators that are consistent. Yeah.
For Monte Carlo, I thought you said that the implementation has an impact on
whether or not it's biased is- I thought you said if
it's every visit then it is unbiased [OVERLAPPING]
Good question. So, I, um, the question is good.
So, this is, um, it's an unbiased estimate for the, um, first visit.
And for every visit,
it's biased. Great. Question?
Um, this might be a dumb, uh,
a dumb question but are there any,
uh, you know, model free policy evaluations models that aren't actually convergent?
Yes. Question was, are there
any model free policy evaluation methods that are not convergent?
Yes, and we will see them a lot when we get into function approximation.
When you start- so right now we're in the tabular case which means we can
write down as a table or as a vector what a value is.
We move up to infinite state spaces.
Um, a lot of the methods are not even guaranteed to converge to
anything [LAUGHTER] Not even- we're not
even talking about whether they converge to the right value,
they're not even guaranteed to stop oscillating.
And they can just keep changing.
Okay. Yeah. Question.
So, is there any specific explanation why TD is not unbiased?
Is- is what?
Why TD is not unbiased?
Why it's not unbiased? Yeah. Great question.
So, the question was to say, you know,
why is TD biased.
TD is biased because you're plugging in
this estimator of the value of the next state, that is wrong.
And that's generally going to leave to- lead to some bias.
You're plugging in an estimator that is not
the true V pi for S prime. It's going to lead to a bit of bias.
So, it's really the-the bootstrapping part that's the problem.
The Monte Carlo was also sampling the expectation and it's unbiased,
at least in the first visit case.
Problem here is that, um,
you're plugging in unexpected discounted sum of rewards that is wrong.
All right. So, um,
that just summarizes those there.
I think the important properties to think- to compare between them.
Um, how would you pick between these two algorithms.
So, I think thinking about bias and variance characteristics is important.
Um, data efficiency is also important as well as computational efficiency,
and there's going to be trade offs between these.
Um, so if we think
about sort of the general bias-variance of these different forms of algorithms,
Um, Monte Carlo is unbiased,
generally high-variance, um, and it's consistent.
TD has some bias,
much lower variance than Monte Carlo.
TD zero converges to the true value with tabular representations,
and as I was saying it does not always converge once
we get into function approximation and um,
we'll see more about that shortly.
I think with the last few minutes,
we won't have a chance to get through,
um, little bit about how these methods are
related to each other when we start to think about the batch setting.
So as we saw in this particular case for the Mars Rover just again contrast them.
Um, Monte Carlo estimate waited till the end
of the episode and then updated every state that was visited in that episode.
TD only used each data point once,
and so it only ended up changing the value of the final state in this case.
So what if- happens if we want to go over our data more than once.
So if they we're willing to spend a little bit more computation,
so we can actually get better estimates and be more sample efficient.
Meaning that we want to use our data more
efficiently so that we can get a better estimate.
So often we call this batch or offline, um,
mal- policy evaluation where we've got some data and we're willing to go through it as
much as we can in order to try to get an estimate
of the policy that was used to gather that data.
So let's imagine that we have a set of k episodes,
and we can repeatedly sample an episode.
um, and then we either apply Monte Carlo or TD to the whole episode.
What happens in this case?
Um, so there's this nice example from Sutton and Barto.
Um, let's say there's just two states.
So there is states A and B,
and Gamma is equal to one, and you have eight episodes of experience.
So you either the first episode you saw A, 0, B, 0.
So this is the reward.
In B, you saw,
in the sorry- in the, and then another set of episodes you just started in B,
and you got one, and you observe that six times,
and then in the eighth episode you started in B and you got a zero.
So first of all,
can we compute what V of B is in this case?
So the, the model of the world is going to look something like this.
A to B and the B sometimes goes to one,
and B sometimes goes to zero and then we always terminate.
So in all eight episodes we saw B.
In six of those we got a one,
in two of them we got a zero.
So, if we were doing Monte Carlo,
what would be our estimate of B, value of B.
So we do a Monte Carlo estimate
using these eight episodes and we can go over them as many times as we want.
We don't just have to experience each episode once.
This is the batch data set.
Someone already collected this for us,
can do Monte Carlo updates of this data set as much as you want.
What will be the estimate of V of B in this case?
[NOISE] which is just equal to six divided by eight.
In the Monte Carlo estimate or do TD,
what will be the TD estimate of B?
Remember what TD does is they get this S-A-R-S prime and you bootstrap.
You do Alpha times R plus Gamma V of S prime,
and then do one minus Alpha of your previous estimate.
What is the [inaudible].
Um, here in this case you can make
Alpha anything small you're gonna do it in infinite number of times.
So this is the batch data settings so for TD you're just going to
run over your data like millions and millions of times.
Until convergence basically.
Somebody have any guesses of what V of B would be for TD.
It's also three quarters.
It's also three quarters.
Okay, so for, for TD it's the same because whenever you're in B you always terminate.
So it's really like just a one step problem from B,
and so for TD it'll also say V of B
is equal to six divided by eight which is equal to three quarters.
So the two methods agree in the batch setting for this.
If you can go for your data an infinite amount of time,
V of B is equal to 3- 6 8ths since so is um,
ah, under both methods.
Um, does anybody know what V of A would be under Monte Carlo? Okay, yeah.
V of A under Monte Carlo is going to be 0.
Why? Yeah good.
Because the only [NOISE] only trajectory where you have an A and I will be [inaudible] from here.
okay there's only one trajectory we have an A and it got a zero reward.
What do you think might happen with TD?
Is it going to be 0 or non-zero?
Its non-zero.
Non-zero, why?
Because you bootstrap the value from A.
Could you bootstrap right so,
so yes there's only time you're in A you happen to get zero in that trajectory,
but this is- in TD you would say, well,
you got immediate reward of zero plus Gamma times V of B,
and V of B is three quarters.
So here Gamma is equal to one.
So your estimate of this under a TD would be three quarters.
We don't converge to the same thing in this case.
So why does this um, ah,
so this is what we just went through and we can
think about it in terms of these probabilities.
Um, So what is- what's happening here?
Monte-Carlo in the batch settings converges to the minimum mean squared error estimate.
So it minimizes loss with respect to the observed returns.
Um, and in this example V of A is equal to zero.
TD zero converges to the dynamic programming policy
with a maximum likelihood estimates for the dynamics and the reward model.
So it's equivalent to
if you essentially just- just through counting you estimated P hat of S prime given S a.
So for this would say the probability of going B given A is equal to 1.
[inaudible] Because the only time you've been on A you've went to B and then
the reward for B is equal to three quarters and the reward for A is equal to 0,
and then you would do dynamic programming with that,
and you would get out, get out the value.
So, TD is converging to this sort of maximum likelihood MDP value estimation,
and Monte Carlo is just converging to the mean squared error.
It's ignoring- well it doesn't assume Markovian.
So it's not using them this Markov structure. Question.
Just to confirm on the previous slide um,if I'm going over data
many times because for TD learning on the first iteration V of A would be zero, right?
Because V of A has [inaudible] just assuming.
But after a while V of B has converged to three quarters? [OVERLAPPING].
So, In, in, in the online setting, um,
if you just saw this once, ah,
then this- then V of A would be zero for that particular update.
It just that if you did it many, many,
many times then it would converge to this other thing.
So you know which one is better?
Well if your world is not Markovian you don't
want to converge to something as if it's Markovian so Monte Carlo was better.
But if you're world really is Markov,
um, then you're getting a big benefit from TD here.
Because it can leverage the Markov structure,
and so even though you never got a reward from A,
you can leverage the fact that you got lots of data
about B and use that to get better estimates.
Um, I encourage you to think about how you would
compute these sort of models like it's called certainty equivalence.
Where what you can do is take your data,
compute your estimated dynamics and reward model,
and then do dynamic programming with that,
and that often can be much more data efficient than these other methods.
Um, next time we'll start to talk a little bit about control. Thanks.
 All right. We're going to go ahead and get started.
Um, uh, before I get into the technical stuff,
we'll do a little bit of logistics.
Um, so, we are starting these things called sessions.
Um, we announced them on Piazza.
If you didn't, uh,
if you're not getting our Piazza stuff,
definitely make sure you've signed up for Piazza, or send us a note.
Um, the sessions are designed to go into the material a little bit deeper,
also to discuss something about the homework.
Um, these are structured session as opposed to an office hour,
where you can ask one-on-one questions about the homework .
the sessions are designed to go a little bit deeper into the material,
and they were prompted both based on some colleagues of mine,
feedback of how much students have liked them in their other classes,
as well as some request from last year for opportunities to go deeper into the material.
So, we've announced these on Piazza.
The idea is that you will sign up for a session.
They are optional, you don't have to do them.
We will be giving one percent extra credit for
attending them if you attend a sufficient number of them.
Um, the details for that has also been announced,
um, and I, I think that's true.
If I've got that right, just email me.
I think that's been announced also on Piazza.
Um, so, if you go to Piazza,
there's a number of different sessions you can sign up for.
The point of signing up for them is to make sure that we have room capacity,
but I'm pretty sure we'll be able to accommodate almost any session you want to go to.
The last session will be done via Zoom,
and it's particularly targeted at SCPD students,
but anyone is welcome to do it.
Um, the way that we'll be keeping
track of whether or not people are going to sessions or not is,
we will have a code that's, uh,
mentioned inside of the,
the material, and so, then,
you will just write in that code to indicate your attendance. Um, we'll record the last session so that if, for some reason,
your schedule is such that you can't attend
any of these but you want to participate in session,
you can go through the material later and then
record that you attended it by using that code.
And we'll be relying on the Stanford Honor Code,
that only people that are doing this will upload the codes.
Somebody had any question about sessions and what those involve?
Again, they're optional, they're way to go deeper into the material.
Um, some other people have really liked these sort of things.
You can see what you think, it's an experiment.
All right.
Any questions about anything else outside of sessions?
So, homework's been released,
office hour is happening as usual this week.
Feel free to come talk to us or use Piazza for any questions that you have.
All right. We're gonna go ahead and get started now then.
Um, as usual, I really appreciate
it if you use your name whenever you're asking a [NOISE] question or making a comment.
So, today, we're gonna finally start to get into
making decisions where we don't have a model of the world,
and in particular, we are going to be focusing on model-free control.
[NOISE] So, the things that we're going be
covering today is really focusing on how can an agent start
to be making good decisions when it doesn't know how the world
works and it's not going to be explicitly constructing a model?
Um, and remember, a model, in this case,
is going to be a reward and/or a dynamics model of the environment.
So, today, we're gonna be looking at methods that do not involve constructing a verbal,
um, a dynamics or a reward model,
but it's just going to be directly learning from experience.
[NOISE] So, um, [NOISE] before- we were mostly talking last time about,
well, maybe we don't know how the world works,
we don't have these explicit dynamics and reward models,
but we're going to be trying to evaluate a policy that was provided to us.
And now, we're going to be thinking about
the real problems that often comes up in reinforcement learning, which is,
how should an agent make decisions when they don't know how the world works,
and they still want to maximize [NOISE] their expected discounted sum of rewards.
So, when we think about sort of how good is the policy,
as soon as we have information about how good a policy is,
then we can start to think about how do we learn a good policy instead.
And, in fact, when we started off at the very start of the class,
we'd talked about how you would learn to make good decisions or how
you would compute good decisions if you were given a model of the world.
And so, that's what we're gonna be going back to now.
So, in particular, now,
we can think of starting to get at this issue of this optimization and exploration.
We're still not going to get into generalization yet.
Um, this will be happening soon.
Um, we've already seen this a little bit it came up with planning,
but now,
we're going to start to think about how do we explore and how do we do optimization.
So, when we think about- well,
I, I think I'm just gonna go through more of these as we start to,
to go into this area.
So, um, again,
we're going to be thinking about how do we identify a policy
that has a high expected discounted sum of rewards.
There's going to be delayed consequences, which means is,
our agent takes actions that may not see the result
of whether or not those actions were good or bad for a while,
and we're going to start to think about this exploration aspect.
Okay. So, let's start with, um, you know,
where these types of problems come up and where people model things when
we're thinking about Markov decision processes and maybe not building a model.
So, I think probably one of the first really big examples of success for
doing reinforcement learning and doing it in
this sort of model free way was for Backgammon,
which was roughly 1994.
They trained an agent to play Backgammon,
the board game, um,
I- actually using the- a neural network.
Um, neural networks went sort of out of fashion for probably around 10, 15 years,
and then came back, but in the early '90s,
people were using neural networks.
Um, and, uh, Gerald Tesauro used it for Backgammon and got some very nice results,
and that was sort of one of the first demonstrations of reinforcement learning
in kind of a larger setting that you could solve these sort of complicated games.
Um, many other g- problems can also be modeled in MDP,
whether games or robots or customer ad selection or invasive species management.
Um, and in many of these cases,
we don't know the models in, i- in advance.
So, what we're going to be thinking about today is, um,
situations in particular, mostly here,
where we think about the model being unknown,
but if we can sample it.
But there are occasionally cases too [NOISE] where you do know the model,
but it's [NOISE] really, really expensive.
So, for something like computational sustainability or climate modeling,
you might be able to write down a good model of the world, but it's really,
really expensive to run because actually simulating the climate is really,
really compete- really, really hard,
and even then, your model will probably be about.
But I just- I raise this second point in
the sense that when we mostly think about sort of
learning from the world, we think of a robot,
like, running around in the world, and,
and that being an expensive thing to do because
robots are taking real time to, to do this.
But you can also think about
agents that are learning to sort of interact with a simulator,
where that's also really costly.
All right.
So, what we're going to be thinking about mostly
today is what is known as on-policy learning,
where we get direct experience about the world, [NOISE] and then,
we try to use it to estimate and evaluate a policy from that experience.
But we're also going to stock- start to talk more about off-policy learning,
where we get data about the world and we use it
to [NOISE] estimate alternative ways of doing things.
So, we can kind of combine experience from trying out different things,
to try to learn about something we didn't do by itself.
And, um, this three thing's really important. So, I'm- all right.
The second thing is really important,
so I'm just gonna talk about it briefly up here.
So, imagine you have a case where,
say, there's only a single state for now,
but it's like, you have a state S1, you do A1,
you stay at S1, then you do A1.
Or you're, you're in S1, you do A2,
so you're in S1, and then, you do A2.
[NOISE] So, you'd like to be able to kind of
combine between these experiences so you could learn about doing this.
[NOISE] even though you've never done that in the world.
You've never experienced that full trajectory,
but you'd like to be able to sort of extrapolate from that, that prior experience.
So, [NOISE] this sort of policy would be an off pol- [NOISE] uh,
an off-policy learning because it's different than the previous policies we've tried.
We'll go into that more when we think about Q-learning.
All right. So, let's start with generalized policy iteration.
Okay. So, if we go back to policy iteration,
we talked about that a couple lectures ago,
and policy iteration, we originally saw it when we knew the model of the world.
So, it's a way for us to compute what was the right thing to do
given- right thing meaning the policy that
maximizes our expected discounted sum of rewards.
So, how do we do this when we know how the world works?
We're given our dynamics and reward model.
In that case, we initialize some policy, probably randomly.
So, initializing, again, would mean that we'd said,
pi of S equal to some A for all S,
and this is generally probably going to be chosen at random.
[NOISE] Um, and then,
we did this policy evaluation procedure,
where we first computed the current value of the policy,
and then, we updated the policy.
So, we took whatever we had, and then,
we did this sort of one more thing,
which you can think of as kinda doing one more Bellman backup,
where we said, okay,
we're taking that V pi,
we're plugging it in over to here,
we're using the fact that we know the dynamics model and we know the reward model,
and we're computing this one-step updated pi prime.
And we talked about the fact that when we do this,
we actually get monotonic policy improvement,
[NOISE] which is sometimes referred to as a policy improvement theorem.
So, this procedure, when we're doing it with sort of,
um, in this dy, uh,
in this case where we knew the dynamics model and we knew the reward model,
um, would guarantee to always give us
a policy that was at least as good as the previous policy or better.
Um, and, eventually, it was guaranteed to converge at least in
the case where we have finite states and
actions because there's only a finite number of policies.
So, in this case, there were only A,
to the S possible policies.
So, we only need to do this whole procedure,
at most A to the S times.
Each time, we're either picking a better policy, or we stayed in the same.
And once you find the same policy, you're, you're done.
So, now, we want to do all of this, um,
but we don't have access to the dynamics or reward model.
So, does anybody have any ideas of how we might be able to do
the same thing now that we don't know the dynamics or reward?
We can maintain another- [NOISE] a matrix transition probabilities,
uh, that you calculate [NOISE] it as you experience the world.
Yeah. The better suggestion is, well,
what if we try to, uh, if I interpret correctly,
wha- what if we try to basically estimate
the dynamics model and a reward model from the world, and then,
we could use this to- you could still compute your,
your value function maybe using some of the methods we saw last time, um, and then,
you could do this update as policy improvement using
your estimated dynamics and reward model of the world.
That's a completely reasonable thing to do, um,
and may have any other idea of what we could do. Yeah, name and-
Uh, I think instead of having to,
uh, uh, a compute a model,
can we do away with model and directly try to estimate what is
the value of a particular state or state-action pair? Doing away with models
So estimate the value of a particular state.
state and action?
Yes.
Yes and with actually?
Yes [OVERLAPPING].
What she actually said is exactly the path that we're going to look
about today which is we're going to focus on model-free control.
So we're not going to directly estimate a model today.
I'm actually personally very partial to models that
can be a very simple efficient but for today we're not gonna
look at that and we're gonna do exactly what
was just proposed which is we're gonna compute
a Q and if we compute
a Q function which just to remember Q is always a function of state and action.
We're gonna estimate the Q function directly and after we have
that then we can do policy improvement directly using that Q function.
So how would we do that?
So this is Monte Carlo for on policy Q evaluation and it's gonna look
very similar to Monte Carlo for on policy value evaluation.
Um but we have to make a couple modifications.
So before, if we were doing this for V,
I'm just gonna write it to kind of contrast.
So for V we just had a count of the number of states.
Now we have a count of the number of state action pairs.
Um before we could just keep track of G here um which can
be the sum of previous rewards we've
seen across all episodes for G of S. Now we're gonna do that for S,
A and then now we're gonna end up having a value function.
We're gonna have a Q pi. So essentially
almost everywhere where we'd just had S before now we have S,
A and then it's gonna look very similar.
So we're gonna assume that we're still provided a policy,
we can sample an episode and then we compute
GIT for every single time step and that remember now is gonna,
I mean it was before but we're gonna to think about the fact that it was
also associated with a particular state and a particular action for
that time step and then for every state action pair instead of just state visited in the
episode either for every first time we saw
that state action pair or every time we saw that state action pair,
we can always, just like before we can either have first visit or every visit.
There we're just gonna update our counts update our Sum
of Total Rewards and then estimate our Q function.
It's basically exactly the same as before
except that now we're doing everything over state action pairs.
Now once we have that now policy improvement is even simpler than before.
So we're given this estimate of
the Q function and now we can just directly take an arg max over it.
So we define our new policy to simply be arg max of the previous one.
Alright so did anybody see any problems with
doing this so far for the type of policies we've been thinking about in the class?
So far we've been thinking about mostly policies but are all
deterministic which means that per,
there are mapping from states to actions and we've been thinking
about cases where this is um a deterministic mapping.
So we always pick a particular action for a particular state.
Yeah in the back and name first please. Oh what's your name?
Oh yeah but what's the problem the problem is we're never
exploring which is correct but what's the problem with not exploring?
We only sample one path over and over again and we never
actually learned anything about the rest of the world that we don't see.
So we don't know whether there's a better policy.
So what he is saying that maybe we're only going to sample one path.
I think what he means more than that is so you can
still sample different paths because your state space
can be stochastic but you are only gonna ever try one action from one state.
So you're never gonna learn about what it would be like if you took
A2 instead of A1 in that state which means that
when you're doing this for any particular state you'll only see one corresponding action.
So the time whenever you see state S1 the only
time the only action you'll see will be A1 or whatever your policy says to do there.
Which means that you're not gonna have any information
about doing anything else there which means
your policy improvements is gonna be pretty boring because you're
not gonna get any other information about things you should be doing instead.
So we're going to have to do some form of exploration essentially now,
we are gonna have to start to have some sort of
stochasticity in our policy or there needs to be
changing over time but we can actually try
different things even from the same state and know what to do.Yes name first.
My name is . Do we know the whole action space beforehand?
Great question. question is do we know the whole action space beforehand?
Yeah we're gonna assume that we do for at least all this lecture and in general yeah.
Since you've made the action initially have high values so
then after it's computed is probably computed took it
low so the next time you see it you would try to other actions?
has made a very nice suggestion
so that relates to how we're initializing the Qs.
So one thing you can do which is what he just
suggested is you can initialize your Q function really
high everywhere to basically do
what's known as optimistic initialization um and that actually can be a really useful strategy for
exploration and if you initialize it in a particular way then you can
have a provable guarantee on
how much data you're going to need in order to converge to the optimal policy.
So optimistic initialization is often
a really good thing to do to be a little bit careful of
how you initialize things like what
those value should be but generally empirically it's really good.
And formally it can be very good to.
We're not gonna talk about Optimistic initialization today but
we will later in the class or talk about optimization.
So doesn't really on the Markov assumption to be able to estimate Q right.
Yes.
But my question is whenever we're defining the policy,
we only define it in terms of the state,
and if the reward that you get from the state depends on officially you have,
then that brings the Markov assumption in like even sorry,
even though the reward is not Markov
then your policy will act you we're defining a policy as if it were.
Yeah so your real world may or may not be
Markov all the policies we're talking about right now is assuming a world as Markov.
The policies are only mappings from current state to action.
They are not a function of history.
So those may or may not work well because your real-world may or may not be
an MDP and if it's not then you're considering essentially restricted policy class.
Considering only mappings from the immediate state to the action and if you,
what you should do really depends on the whole history then you might not
make good decisions. Good point.
Okay so this is sort of how
the basic way you would extend Monte Carlo to be able to start to
estimate Q and once you have that you could do
policy improvement but now it's clear that we need to do
something in terms of how we should get- gather experience so we
can actually improve when we tried to do this policy improvement um.
Because now we don't know how the real dynamics of the world work.
So we need to do some with some sort of interleaving of policy evaluation and
improvement and we also need to think about how we're doing this exploration aspect.
So in general it might seem a little bit subtle.
So we've already got one nice suggestion from like maybe you could
initialize everything optimistically and maybe that would help you explore.
It does, but in
general it might seem like it's a little bit
hard of how are we going to get this good estimate
of Q pi because what Q pi does is it says um if you wanted
a really good estimate of Q pi of S,A for all S and all A it would
say you kinda need to get to every different state take every possible action
and then follow pi from then onwards and so how do I make sure
that I visit all of those things
and what we're gonna talk about today is a very simple strategy to make
sure that you visit things which works
generally under some mild conditions about the underlying process.
So the really simple idea is to balance
exploration and exploitation by being random some of the time.
So let's imagine that there's a finite number of actions
we're gonna call that cardinality A, um,
here then e-greedy policy with respect to a state action value is as follows.
With probability one minus Epsilon,
you're going to take the best action according to
your current state action value function and
else then you're gonna take
an action A with probability Epsilon divided by A.
So with probability one minus Epsilon you take what you
currently think is best according to your group or your estimate of the
Q function and with probability Epsilon you select one of the other actions.
So it's a pretty simple strategy and the nice thing is that, it's still sufficient.
But before we do that why don't we just do
a brief example to make sure that we're on the same page.
So let's think about how we would do Monte Carlo for
on policy Q evaluation for our little Mars rover.
So now our Mars Rover is gonna have two things that can
do instead of we're gonna be reasoning more about that.
So I've written down the reward function here.
I'm saying that if you take action A1 you get
the same rewards we've been talking about before which is 1,
0, 0, 0, 0, 0 plus 10.
And now I'm changing it I'm saying well you're
action does -- your rewards do depend on your state and
the action you take and so the action for
A2 is now going to be 0 everywhere and then you get a plus five at the end
and gamma is one and let's assume that
our current greedy policy is you take
action A1 everywhere and that we're using an Epsilon of 0.5.
And we sample a trajectory from an e-greedy policy.
And again what an e-greedy policy means here is I set Epsilon
equal to 0.5 which means that half the time we're
gonna take our current greedy policy of
action A1 and the other half the time we're going to either take A1 or A2.
So what that would yield as an example would be a trajectory such
as state three action A1 0, state two.
And now this is a case where we're sampling randomly.
So we flipped a coin.
We said oh this time I'm gonna be random.
Then I have to flip a coin again to see whether I'm
taking an action A1 or A2 and I took A2 there.
I got a reward of 0 and then the rest of trajectory as
follows and my question to you and feel free to talk to a neighbor of
course is what is now the Q estimate for all states for
both action A1 and action A2 at the end of this trajectory using Monte Carlo estimates?
So we're doing first visit in this case.
Yeah.
Uh, [NOISE] I have a question about the action we choose on the Epsilon table.
Yeah.
Uh, is it important
when- what would the actions on policies ,
or should we pass in that action
question is whether or not when you hit,
uh, now do something random,
whether you should include the action that you'll
be taking normally if you're being greedy.
Um, you could.
In some ways, that's like just picking a different Epsilon.
Yeah. I hear less talking than normal,
so that they may have any clarification questions about this or,
or [NOISE] or are there questions? Good. [LAUGHTER] Sorry.
I have an idea.
Okay. Yeah. So, um, uh,
if everybody's ready to ask yourselves.So , what, what did you guys think?
Uh, so, you will have in this case S3,
well ,
so everything that you did not,
every state of action pair you did not see will remain zero.
Yeah.
And at a particular, uh Q of S3, A1 will be zero,
cause you saw that once and reduce that to zero,
Q of S2h will also be zero,
Q of S3A1 will be zero, and then,
the only one that will be non-zero will be Q of S1, A1,
which in this case will be 1.
Because you saw it once,
and the reward that you got when you saw it was 1.
That's one answer.
Anybody with a different answer?
so,
uh, all of the state action pairs that we've seen will be one,
and all other state action pairs will be zero.
That's another answer. So what,
uh, was say would be right for the TD case.
Okay.
What you were saying would have been right for last week,
uh, or if yesterday, or, or Monday.
Any else who may have a third answer?
Could you repeat what the second- the second choice was?
The first choice is that we only update, um, S1, A1.
The second choice is that everything that we saw will now be 1,
and maybe I misunderstood over there.
So we're gonna have two different um,
we have two vectors now.
So we have Q of A1,
and we have Q of A2,
and they're not gonna look identical.
So, sometimes we take action A1,
and sometimes we take action A2,
and we can only update what we saw the returns for the action we took.
So what actions do we take for S3?
A1.
Just A1, right? So that means for those ones,
for S3 it's gonna be 1,
and that for Q of S3, A1,
so I'll fill in all the ones that are zero,
one, two, three, four.
Um, do we ever take A2 in S3?
No. So that also has to be zero,
cause we didn't ever start there,
take action A2, and get a return.
Uh, what about for,
what action do we take from S2?
Right. So for that one, we get a 1.
So we basically, uh, distributing your experience.
So now if you were going to take a max over those,
then you would get the same thing that we saw last time for Monte Carlo,
which would be 11100000 to the end,
um, but here we- we're subdividing our samples.
So, you only get to get an experience for the action that you actually took in the state.
And because we're in the Monte Carlo case,
we'll see the TD case or,
Q-learning we'll call it later,
um, then we get to add up all the rewards to the end of the episode.
So G here is gonna be the sum of all these steps,
and I didn't speci- oh, I did.
Good. And we're keeping Gamma equal to 1 here just
to make all the math. Just adding. Yeah?
Should we just [OVERLAPPING].
Sorry.
Can just be one half for Q S1A1 or in Q S3A1?
Uh, is talking about whether or not if we did every visit,
if anything would change here. [NOISE] Excuse me.
It would not change in this case, because, um,
both times when you visited S3,
the sum of rewards to the end of the episode was 1.
So you'd have two counts of 1,
and then we divide by 2.
It da- it can actually be different,
but it's mostly different if you got like a different sum of
rewards from then to the end of the episode. Yes?
So is [OVERLAPPING].
Remind me [OVERLAPPING].
Yeah. Isn't that?
Maybe I misunderstood.
Yeah.
So, I thought we were supposed to say that everything was, and I missed that.
Did, did you say that that was different for the two actions?
That was one for in the projectory, um, zero.
I understand. Sorry about that.
Okay.
Okay. So now we're gonna show formally that this does the right thing.
So, um, we're gonna show provably
that like what we did before when we were doing policy improvement,
we're showing that if you pick a policy,
um, pi i, that was, uh,
generated by being greedy with respect to your Q function,
then that was guaranteed to yield monotonic improvement,
and the same thing is gonna be true here too,
when you do e-greedy.
Um, so if you use sort of er,
an e-greedy policy, then you can gather data such that, uh,
the new policy- the new value you get,
if you're optimistic with respect to that- oh, sorry,
if you're greedy with respect to that,
that means you're gonna get any better policy.
Okay. So let's say that, um,
we have an e-greedy policy,
Pi i, and then we're gonna call an e-greedy policy with respect to Q Pi i,
which is gonna be Pi i plus 1,
so we had a greedy- e-greedy policy Pi i that was
doing some amount of exploration and some amount of greediness in the past,
we use that to gather data,
we then evaluated that policy and we got this Q Pi i,
and now we're gonna extract a new policy.
We're going to do policy improvements.
I'm gonna show that that's a monotonic improvement.
Okay? Does anyone have any questions about the, what we are showing?
Okay. So, what does this mean?
So right now what we're gonna be trying to show is that this, this Q function,
the Pi of s Pi i plus 1,
so, is gonna be better than our previous value.
At least as good or better than our previous value of our old policy Pi i.
So the way we define this is now, um,
the Q function here is going to be a sum over,
our policy is stochastic.
So it's Pi i plus 1,
of the probability we take an action in a certain state,
times Q Pi i of SA,
and then we're gonna expand that out,
and we're going to redefine it in terms of what it,
what it means to be an e-greedy policy.
So with, remember in a e-greedy policy we either take something randomly,
and that's with probability S1,
and we split our probability mass across all the actions.
So that's how we get this equation.
So this says, this is the- this is the random part.
So with probability, with probability epsilon,
we take one action,
one of the actions, and then we would follow that from then always.
So that's just Q Pi i of SA,
and then with probability 1 minus Epsilon, we're greedy.
And we follow the best action according to our current Q Pi i.
So, now what we're gonna do is we're going to rewrite that.
The first term isn't gonna change and I'm gonna expand the second.
[NOISE]
I haven't done anything here.
I just multiplied the last term by 1,
but I expressed the 1 as 1 over Epsilon divided by 1 over Epsilon,
and now I'm gonna re-express that part.
So, and I'm gonna rewrite the first term,
plus 1 minus Epsilon,
max over a, and what I'm gonna rewrite this as-
It's gonna use the fact that whenever we define our e-greedy policy,
if you sum over all actions in a certain state,
those are all probabilities of us taking those actions in that state,
so it has to sum to one.
So I just first divide it,
I just multiply by one,
and we're expressing as 1 minus Epsilon divide by 1 minus Epsilon,
then I re-expressed the 1,
because it has to equal to 1,
cause we have to take some action in a particular state.
A policy always has to,
the probability of us taking any action state has to be equal to 1,
and then I'm gonna do the that expression because we're,
here is where we'll take the best action.
So by definition, the best action has to
be at least as good as taking any of the other actions.
So we're gonna do the following;
we're gonna push that Q inside.
[NOISE]
So that has to be smaller than what we saw before,
because basically we just push the Q inside,
and we're no longer taking a max.
And the Q values- all the Q values at best have to be equal to the max,
and in other cases they'll generally be worse.
Okay? But then once we have that,
we can cancel that 1 over Epsilon minus 1 over Epsilon,
and what do we have?
We have two different terms here that look very similar.
We have one.
Let's see. We need one was taking that apart.
And we'll keep this up.
Yeah. There is an Epsilon over a right there.
Okay. So now I'm going to pull that out.
[NOISE]
If I split those terms up,
the first term and the third term are identical,
and one is subtracted and one is added. Make sure that's clear.
So, this just ends up becoming the middle term.
[NOISE]
And that was
just the previous value.
Yeah?
first line, where we changed it to,
instead of the sum, over all,
A of Pi I a given s minus Epsilon to, [NOISE]
minus Epsilon over the cardinality of A in this case?
Yes.
Um, is that s- that,
is that [NOISE] still one minus Epsilon,
[NOISE] I mean, that, that looks all-
That's all . Got it.
Does that answer your question?
I think so.
Yeah. So, the, what we did from the one minus Epsilon, to the next one.
[NOISE] So, we had a one minus Epsilon divided by one mi- minus Epsilon,
[NOISE] and, I re-express that as the sum over A [NOISE] ,
Pi I of A given S minus Epsilon divided by sum over A, and then,
if you sum over A that's second term,
just this Epsilon, and the first term is one.
Okay? Yes.
Isn't that [OVERLAPPING]
Can you remind me your name?
[OVERLAPPING] the Pi, the, what?
Name?
Oh. Pi I is the Pi,
like Pi I plus one,
negative Pi, and then Pi I.
Pi I plus one. [NOISE]
Which line are you thinking about?
[NOISE]
.
Which- [OVERLAPPING]
The means, I'm sorry. The second line .
Yeah.
You wrote E Pi I plus [NOISE] one,
negative five [NOISE], five Pi times five.
I'm just not understanding your question
with- so you're on that second line is that right or-?
In the .
Okay. Pi plus one is- sorry. What is the question?
[NOISE] .
Yeah. The?
Yeah.
Yeah. [NOISE]
[NOISE] .
Yeah, so good question.
The greater than or equal happen because we push that Q Pi we had a max over
A Q Pi I of s, a we pushed it inside of the sum.
And so that sum now no longer includes a max.
And so, now that the max is always greater than or equal to any of the other elements.
So, that's where you got the,
greater than or equal to. Yeah?
So, I just wondering if you could explain like
intuitively you go random or optimal actions and then you end up with monotonic improvement.
[NOISE] Yeah. Can we get some intuition this is the algebraic derivation.
And I think intuitively the idea is that by doing
some e greedy exploration and you're gonna get evidence about some other state action pairs.
Um, and then you can use this to estimate your Q function and that
when you do that then that's also gonna give you uh,
then that can improve your policy and you can have evidence that there is something
better you could do then the current one, the current thing you're doing.
If you don't do any exploration your Q function wouldn't change from before.
But now because you're doing exploration then you can learn about
other stuff and then if it's better you'll see that in your Q function. function like your exploration
is not as good then you just take the old one?
Yeah. Um, yeah.
So, this is now um,
this is saying that you'll get
this monotonic improvement if you're computing this exactly.
So that's an important part.
So this show- so what this shows here is
that if you get a Q function and it looks like there's
some improvement from some other actions that you're not taking right now you're
gonna shift your policy over towards focusing on those actions.
This is assuming right now in terms of
the monotonic improvement that Q Pi I's have been computed exactly.
So that's what we thought when we were doing planning where we knew what
the dynamics model was in the Reward Model and we're
using that [NOISE] to compute a value function.
Um, so if we we're doing in that case we have
the guaranteed monotonic improvement because we had
the exact value of Pi and similarly here if we have the exact value of Q Pi I,
then when you do this improvement then you're guaranteed to
be monotonically improving if you didn't,
like if you have just an approximation of Q Pi I,
then it may not be monotonic.
like let's say you tried another action once in that state.
You may have a bad estimate of how good things are from that point.
So, this, this is an important aspect.
And this is going to be really important when we start to think about
function approximation because we almost never will have computed Q Pi I, exactly.
But if you do like that say,
you can just iterate through this a ton of times
like you're learning that's still a tabular environment.
You've converged you know your Q Pi I, is perfect.
Then when you- then do
policy improvement you can get a benefit even you can improve- though
there's going to be this interesting question of how often do you improve
your policy versus how much time do you spend evaluating your current policy .
Yeah?
Uh, yes.
Yeah.
So does this mean that it definitely converges to like an optimal Q that was Q function?
The overall- Perfect.
Yeah, we'll talk about that question is great too.
So, this is just saying like one step monotonic improvement what,
what's gonna happen in terms of total convergence we'll talk about that in just a second.
Yes? Remind me name, please.
[NOISE] question and answers When I think of V Pi, I think of it as being a function of a state? But action given a state?
Uh. So, to sort of re- refresh all over my
what is a Pi and how we define the function.
Now we're thinking of it as a mapping from states
to actions but it can be a stochastic function.
So, it can be a probability distribution over actions.
So, I can select action A1 with 50 percent probability.
or action A2 with 50 percent probability.
For example from the things- [OVERLAPPING] ? .
Okay.
I mean depends how you want to implement it like that concerned to be a bit.
Essentially I think of it as you're in a state and then you have
some probability distribution over actions you have to
sample from that to decide what action you take.
The policy the- so what we're doing here when we
expanded this as we said what is the policy for an action given a state
we said with one minus Epsilon probability we will be taking this max action.
So that one. And with Epsilon probability we would be taking one of the actions.
And so, then we summed over each of the actions we could take.
So, what we did there is we split this sum up
into the probability of taking one action and what'll be
the Q function of that action and the probability of
taking each of the other actions and what would be the value of those.
So, it's like our expected value.
Yeah at the back. Yeah . So when we talked about the Bellman operator, we said that if you got to the same value function - [NOISE]
You can stop iterating.
Here, would you have to have tried every ah,
action to know that you are done?
That's a great question before in policy improvement if you got to the same policy you you,
you are done you don't have to do any more improvement.
The question is, in this case is it true or there's some other additional conditions?
Um, this is very related to question too.
So why don't we go onto the next part that is saying, you know,
under what conditions are these going to converge and converge to optimal?
Um, do- do you have question before?
Yeah, in this this also say that the only time we get strict equalities is when Epsilon is 1. So you just act purely randomly?
Uh, the question is whether or not there's,
um,so if if policy is random,
would you get such a quality here?
Um, yeah, you should get.
I mean if you can get such a quality whenever you've converged to,
like if your Q function is converged your policy is optimal.
Are you guaranteed such a quality against what interest are?
No, I don't think so. Because if you're acting totally randomly in fact
that's normally often how you start off and then you want to improve from there.
Could you review I mean if you're, if you're if you're worried
if it's uniform some things are going to look better than others.
So even if acting randomly,
some actions are going to have higher rewards than
others and that can be reflected in your Q function.
Any other questions before we get on to convergence in the back .
One outside this should um,
yeah . Yeah, another question.
Um, um, do you exclude um, argmax when you explore?
Do we what?
Do we exclude the argmax action?
Like, you know by exploration, um, and e greedy part.
And what is your name ?
Pardon.
Um, I- no you don't exclude it, don't exclude.
You don't exclude the argmax action when you explore. You pick all of them.
Um, if you wanted to that would be equivalent to sort of defining. You could do that.
But in the simplest version, including in this proof here,
we assume that when you're um,
acting randomly you just sample from any of the states.
It's often easier from implementation, too.
Okay, great questions.
Let's, um,
write that up here as well.
Er, okay, so this other really great question that's coming up from several people here.
Um, er, I have, okay,
what does this mean over time?
Um, I have call it monotonic improvement and what guarantees do we have?
So, the guarantees that we have is, um,
if you assemble all state action pairs an infinite number of times,
and your behavior policy converges to the greedy policy.
So what do I mean by that?
Um, so the behavior policy here is sort of what policy you- you're
using versus what policy is greedy with respect to your current Q.
So, if you have the case that as the limit as I goes to infinity,
have Pi a given s
goes to argmax Q,
s, a with probability one.
Which means that in the limit you converge to
always taking the greedy action with respect to your Q function .
Then, um, then you are greedy in the limit of infinity exploration.
That's called GLIE often.
So that means you visit all these state action pairs
an infinite number of times but you are also
converging [NOISE] in the limit to be greedy with respect to your Q function.
Um, and there's different ways to do this.The simple way to do it is to sort of,
decay your um, your Epsilon or your or your E greedy Policy over time.
Um, so you can reduce your Epsilon towards zero at a rate of like one over I,
for example, that's sufficient. It's not necessarily
This is, this is separate than what you wanna do empirically.
This is just to sort of show under these conditions.
Then, um, then we're generally going to be able to show that we are going to converge to
the optimal policy and optimal value for Monte Carlo and TD methods.
So, generally when think will talk about this again as we talk about
some of the other algorithms generally when you're GLIE, um,
and you have some conditions over how you're learning the Q functions,
um, then you will be guaranteed to converge to optimal policy.
Yeah.
Um, do you realize like,
like we've seen Epsilon and.
Yeah. So question is is
this the only way to guarantee it, um,
there's sort of interesting different things that are happening here.
Um, you could be guaranteed that you're converging to
the optimal Q function without converging to the optimal policy.
So, you could keep Epsilon really high, um,
and you could get a lot of information you will be
learning about what the optimal Q function is,
but you might not be following that policy.
And we'll talk more about that in a, in a minute.
All right, so let's talk a little bit more about Monte Carlo Control.
In that given this precursor.
So, if we wanted to do Monte Carlo Online Control,
instead of just this evaluation we talked about before,
we can kind of combine these ideas of learning the Q function and doing this er,
improvement at the same time.
So we can initialize our Q functions and
our counts in the same way we were talking about before.
Um, and then what we could do is we can construct an E greedy policy.
So E greedy policy in this case is always going to be
that with probability one minus Epsilon.
We pick the argmax with respect to Q with probability Epsilon we select an action and,
let me write it this way: probability Epsilon over a we select action a.
So we're just mixing up between this random um,
or being greedy. Yeah.
If I heard that so,
actually like the optimal actions in this case you are selecting with
probability one minus Epsilon plus Epsilon over the cardinality of A right?
Yeah.
Okay.
Several people would ask about this.
So essentially, you're being greedy with
probability one minus Epsilon plus Epsilon over a.
And then the remaining part of your probability is going to be an exploration.
Because when you're being random you could also select what's currently the best action.
So, um, it looks pretty similar to what we saw before.
We're going to sample an episode,
after we finished the episode then in this case I'm defining as first visit,
no, you could make this every visit.
I could do every visit.
The same, um, benefits and restrictions apply here.
So what we had before in the sense that you could either be getting
a slightly more biased estimator if you're doing
every visit but generally going to be able to use more data.
It's going to be sort of less noisy.
Um, so in this case what we're doing is we're
just maintaining counts over state action pairs and we're updating our Q function.
And then after we finished that episode then we can update, um,
our k and our Epsilon,
in this case we're just using Epsilon equal to one over k,
and then we redefine our new E greedy policy with respect to Q,
and then we get another episode.
So that's just sort of Monte Carlo Online Control.
So why don't we go back to that Mars Rover example?
So, in the Mars Rover example what we had is for this is what our 2 Q functions look like.
So, at this point what would
be just spend a minute and say what would be our new policy, um,
if we're at the end of this episode and- and its fine just write down tie if there,
if there are two Q functions that have exactly the same, um,
value for the, for the same state for two different actions and it's just a tie.
Then you can choose how to break, the break the tie.
Um, and then also write down what the new E greedy policy is.
I'll just take a minute to do that.
Okay, what's our greedy policy?
What is the greedy policy for S1?
A1.
What is the greedy policy for S2?
Two.
And then what's our greedy policy for S3?
One.
And then what is it for everything else?
Tie. Okay. And depending on your implementation you could
either always be you could either sort of define
your greedy policy or you would just like
break ties randomly and keep track of that.
Could constantly be breaking ties randomly.
That would probably be better empirically like,
instead of predefining one greedy policy,
you can probably just always be Q,
er, querying what argmax is of Q.
And if you're getting ties just break them randomly to get more exploration.
Um, so then if we then define
an E greedy policy where K is three and our Epsilon is one over k,
with what probability do we follow?
Random. So k is three,
Epsilon is equal to one over three.
So that would mean that with one-third probability,
we select something random and with two-thirds probability,
we select the pi greedy policy.
And then that would be the update for that particular episode.
So, if you do this,
if you do- if you have greedy in the limit of infinite exploration Monte Carlo,
then you're gonna converge to the optimal state-action value.
[NOISE] So, now, we're gonna start to talk about TD methods.
So, similar to what we were seeing, um,
for Q, uh, Monte Carlo,
there is gonna be sort of this simple analogy that moves us over to TD.
So, remember, for TD what we had before is,
we have our V pi of S. It was equal to our previous V pi of
S plus one minus Alpha times- oops,
let me rewrite that- plus Alpha times R plus Gamma V pi of
S prime minus V pi of S. [NOISE] And this was where we were
sampling an expectation [NOISE] because we're only getting one sample of S prime,
and we were bootstrapping because we're using our previous estimate of V pi.
So, that was kinda the kwo- two key aspects
of TD learning that we're both bootstrapping and sampling.
In Monte Carlo, we were sampling, but not bootstrapping.
Um, and one of the nice aspects of TD learning is that then we could
update it after every tuple instead of waiting till the end of the episode.
So, just as, like, what we do with Monte Carlo,
we're kinda replacing all of our Vs with Qs,
we're gonna do exactly the same thing here.
[NOISE] So, now, we're gonna think about
this sort of what's often known as temporal difference methods for control.
[NOISE] So, what we're gonna do now is,
we can do- we can estimate the Q pi function using temporal difference updating with,
like, a e-greedy policy,
um, and then, we could do Monte Carlo improvement
by setting pi to an e-greedy version of Q pi.
That would be one thing we can do.
[NOISE] There's an algorithm called SARSA,
which stands for state-action-reward- next state-next action, so SARSA.
Um, how does SARSA work?
So, what we do is, we initialize our e-greedy policy randomly.
For example, uh, we take an action,
we observe reward and next state, and then,
we take another action,
and we observe another reward and next state,
and then, we update our Q as follows: We say our previous va- um,
our value of Q for [NOISE] ST,
AT is gonna be whatever our previous value was.
Actually, I'm gonna be careful with this.
We're not going to index them with pi
anymore because we sort of have this running estimate,
and our policy is gonna be changing, too.
So, the, the Q function that we get here is now not just for one policy,
but we're going to be averaging it over different samples,
and we can be changing how we're acting over time.
So, it's ST, AT,
it's gonna be equal to Q of ST,
AT plus one minus Alpha RT plus
Gamma Q of ST plus
one AT plus one minus Q of ST, AT.
The important thing about this equation is
that I am plugging in the actual action that was taken next.
So, you see- you're in a state, you do an action,
you get a reward, you go to a next state,
and then, you do another action.
And so, once you know what the next action is that you've done,
then you can do this update in
SARSA as you're actually plugging in the action that was taken.
And then, once you have that, you can do policy improvement in the normal way.
So, you can have ST is equal
to arg, max Q,
[NOISE] so, like, the E-greedy wrapper for that.
Now- so, this is a little bit different than Monte Carlo for two reasons.
Um, it's sort of,
uh, we're doing these tuple updates,
we see the state, action, reward, next state,
next action tuples, and then,
once we do those, we can update our Q function.
Um, we can do those along the way,
we don't have to [NOISE] wait till the end of the episode,
and similarly, we don't have to wait till the end of
the episode to change how we're acting in the world.
So, like, in the, um,
trajectory that we saw before,
we saw some states multiple times.
In this case, we could actually be changing our policy
for how we act in those states during the same episode.
So, if your episodes are really long, this can be really helpful.
[NOISE] So, in general,
um, I think it's often extremely helpful to,
um, update the policy a lot. Yeah Is there
a reason [NOISE] ?
Oh, yeah. So, they're both the same,
it's just either you could write it where you put the V in the next part or not.
So, you can either have it as one minus Alpha times
your old value plus reward plus Gamma of your next thing,
or you could have it as V plus Alpha times, or that,
that should be still an Alpha here plus reward minus V. So that's either.
They- they're the same. If you know that I've made a typo,
just let me know. Yeah.
Uh, and is there a reason we use, like,
the next state action pair that we choose, uh,
uh, A plus one rather than the max state action?
question's about why do we use
the next state action pair you choose instead of the max.
Q-learning is going to be the max,
we'll see that in about a slide.
Um, SARSA's basically updating on policy,
um, that can have- generally,
you want to do Q-learning,
which is going to be doing the max.
Sometimes, there's some benefits,
particularly in cases where, um,
[NOISE] you could have lot of negative outcomes,
that the optimism of being max can end up sort of causing your agent to make a lot of
bad decisions early on because it's really optimistic about what it's-
what it's- could do instead of what it's actually doing.
Um, there's a nice cliff walk example inside Sutton and Barto
where they show that SARSA actually is doing better in sort of early,
early stages, early samples compared to Q-learning,
because SARSA is realistic about what happens
if you take certain actions next to- as opposed to optimistic.
And if you're doing a lot of randomness, um,
that means that SARSA can be more realistic in the early stages.
But empirically, generally, you want to do Q-learning,
and both will convert to the same thing. Yeah.
Um, so, [NOISE] ,
should be, um, Q ST one A be plus one be ST plus one.
Yeah. Thank you. Yes
This might be question but you're talking about
how its getting the information from the future action,
but you have to have already done that action.
So, why is it called, um, er,
state action or -or next state actions,
when it's really the past one that you're updating from what I'm understanding.
Because you- you're doing this one and you're using the information you learn,
that take the one in the back.
So, why is it- why are we talking about it like
it's a future action? What's the purpose of that?
Um, all right. I- I don't think isn't about the particular terms used to define SARSA ,
I don't think it- I mean, I guess,
it's really just that you have to wait till you get,
um,f that- that last A is important.
So that instead of saying that- but before we
thought with TD learning if you were in a state action reward
next state and then you could update your Q-function
now we're just saying you have to wait till you've
actually decided what to do in that next state.
Okay.
Because that's how you're choosing how to do update your Q-function here,
and that's what you're plugging in for your target.
So, in terms of the convergent properties,
um, it requires a couple of different things,
uh, so, if we are,
um, we need sort of two things.
We're gonna need the fact that we're- we're updating
our Q-function and it's gonna be updating incrementally,
and so, like what we talked about before,
we're gonna need some conditions over the Alphas.
Um, if alpha is equal to 1, uh,
generally your Q-function is not gonna converge,
because it means you're not remembering anything about the past.
Um, if alpha is 0,
then you're not updating anymore.
So, generally, you need something in terms of the step sizes,
which allows you to sort of slowly be incrementing but still be converging.
So these are one sufficient set of conditions, um,
so if you have stuff like Alpha T,
is equal to 1 over T. Now
empirically often you're going to want to pick very different forms of learning rates.
So Alpha T, is often referred to as,
like, the learning rate parameter,
and empirically you are often not gonna wanna use this [NOISE],
generally not gonna use this.
This is gonna, um, uh,
you- you're often gonna wanna use different things empirically,
you could end up using sometimes small constants,
or slowly decaying constants.
Often that depends on the domain,
but this is from a theoretical side what is sufficient to ensure convergence.
And then the other aspect is that the way that your,
uh, that your policy itself has to satisfy the condition of GLIE,
which means that you are, sort of,
slowly getting more greedy over the time but you're doing so in a way that
you're still sampling all state action pairs an infinite number of times.
Now- now, just note for a second that that's not always possible,
like, so if you have a domain, um,
where, uh, things are not reachable after a point it's not argotic,
you can't get back to certain states after you get there.
Let's say you're flying a helicopter and you break the helicopter.
So you can't get back up there, um,
then you're not gonna be able to satisfy GLIE
because at some point you broke your helicopter and
then like you have no idea what it would have been like
if you continued to fly your helicopter in the air.
So, there can be some domains for which it is very hard to satisfy GLIE, um,
but we generally are going to ignore those even
though there are some really interesting work on,
so, how do we deal with those cases as well.
In those cases, somebody might assume that it's more of an episodic problem,
so maybe you have like a 100 helicopters and so when you
crash one that's considered a termination condition and then you get out your next one.
Um, so you may or may not be able to be greedy in the limit of information,
in the limit of infinite exploration there but you can,
sort of, have a bounded amounts of exploration.
And we're going to talk a lot more later about, sort of,
how to do this exploration in a much more smart manner and in a way that can give us
finite example guarantees on how much data we need to learn a good policy.
So, this is just what I said before which is,
you know, we generally are not gonna use the step type.
where you have
Q plus alpha because 1 is alpha, times the-the .
Yeah. Okay, yeah. So this is the- this is for SARSA.
So this is the condition for SARSA assuming that-
that particular update of how we're updating our Q-functions.
Okay? So, yeah.
Uh, so in the Monte Carlo case,
we have sufficient condition on- with the pie
that has been GLIE with the Epsilon going down to 1 over t.
Do we have anything similar in general?
Um, great question. Uh, question is about if,
for the Monte Carlo, do we have a sufficient, uh, a similar condition.
If you're just, um,
if you're doing first visit that alone is sufficient.
Because you're getting an unbiased estimator that's converging
for all of your returns with only a
few of all the state action pairs in infinite number of times.
If you're doing it in this incremental fashion, um,
then if you're, if you're- if you're playing around with how about Alpha is,
then you need to have similar conditions to make sure it guarantees.
What I mean is, uh, how do you,
like, how do you know that condition will impulse?
How do you know that things are GLIE?
Yes. Like in Monte Carlo we did have condition rate.
It was that Epsilon decay as long as it-
Oh, so, um, great question.
so is like, how do you make sure something's GLIE.
Um, one sufficient condition is that Epsilon is 1 over- uh,
it's over T or one over I.
And do you know like, uh,
with that that work like, yeah.
Oh, if you need to like know if there are sufficient conditions like this?
Yeah, like, it will be GLIE if and only
if- if Epsilon put to 0 but there's some diverges or something like that?
Yeah I think it's quite similar as sequence,
similar like you're, sort of,
essentially you're- you're ensuring that you're doing infinite number of updates,
infinite amount of grid its like
random exploration but still its going down fast enough to converge in.
I think it's probably exactly the same but converges.
Okay. So then when we get into Q-Learning which
is related to the question which was asked, okay,
why are we just picking in that particular action
next why don't we just pick the max. Um, yeah.
We could just pick the max instead.
So SARSA is picking this particular action next,
Q-learning is picking the max action next. Yeah.
as you said what does it take to do better
early on because its not too often that statement that later.
Um, is there any way that we could mix SARSA and Q-learning,
you certainly could, um,
but then that also means that maybe I wasn't being clear enough with the earlier part.
So SARSA can do better in some domains early
on particularly if there's a lot of really negative rewards because it's being realistic,
um, another case is Q learning.
Will it be better even early on?
Because you're being more optimistic and as we talked about a little bit before,
often optimism is really helpful for exploration.
The cliff walk example in Sutton and Barto is a case where
some actions lead the agent to like fall off a cliff and so
some actions are really bad and so there
being optimistic early on means that you're gonna take
a lot of really bad decisions and suffer a lot of negative rewards for a while.
Many other domains are not like that so depends on a lot.
And yes you could certainly mix them.
Alright. So I guess in terms of Q-learning one thing that's interesting here is,
uh, so we can again sort of think about how are we're improving this and we're gonna,
sort of, be e greedy with respect to the current estimate of optimal Q,
and- and really this is quite similar to what we we're doing in
SARSA except for now when we update this Q we're really just gonna be doing this MAX.
So Q of ST, AT is gonna be equal to the previous value,
plus alpha or plus max over A.
So, now also note that you can update this a little bit earlier, so,
you don't have to wait until the next action is taken.
So, you only need to observe this part.
You don't need to actually see the next action that's taken and then
you can perform policy improvement,
and in general, in this case,
you're only gonna- you only need
to update the policy for the state that you were just in.
So you can do pi,
you can update pi b for ST for the action you just took.
You don't need to- particularly, in large state space, that can be helpful.
So we actually ended up talking about this a little bit
already about whether or not how you initialize Q matters.
It doesn't asymptotically, I mean,
if you have a case where your Q function is gonna converge to the right thing,
it will still converge to the right thing no matter how you
initialize it as long as it satisfies these other conditions,
but it certainly matters a lot empirically and so even though
often we think of just initializing it randomly or initialize it with 0,
initializing it optimistically is often really helpful.
So we'll talk more about that when we talk about exploration . Yeah
On the previous slide line six,
either max or a argmax?. Thank you.
[NOISE]
So now, um, if we do Q-learning.
Um, Let's see.
I think wha- I'm gonna leave this as just an exercise you can do later,
but you could just do the exact same exercise for Q-learning,
um, and see how these updates propagate.
Um, so just like Monte Carlo versus Q- Monte Carlo versus TD for policy evaluation,
there's some of the same issues with Q-learning.
Q-learning is only gonna update your Q function for the state you are just in.
So, even if it turns out later in the same episode,
you get a really high reward.
You're not gonna backpropagate
that information at the end of the episode in the way that you would with Monte Carlo.
So Q-learning updates can be much slower often,
um, than Monte Carlo.
Just like enter that has implications for how quickly you
can learn to make better decisions [NOISE].
So, the conditions that are sufficient to
ensure that Q-learning with the ε-greedy converges,
it's basically the same as SARSA.
We need to make sure that things are, um,
that are GLIE, and,
I see, and slightly revise this.
So, if you just wanna make sure that you converge,
that needed to be the all SA infinitely often.
I need to have these conditions on the Alpha.
So if you look at the same conditions,
in order for the Q functions to converge,
you need to have these conditions on how you're
updating your li- like what you're learning rates are.
Ah, and that you visit all state action pairs infinitely often.
But that just- that's sufficient to allow you to converge to the optimal Q values.
And then if you want to actually make sure that the policy
you're following is really the optimal policy,
then you need to be GLIE.
You also need the policy you chose to be more and more greedy.
All right, let me just briefly into the maximization bias before we finish.
The maximization bias is an interesting question.
Ah, so why are we going to talk about this?
Well okay, let's go back to this one.
So in Q learning, what are we doing?
In Q learning, we're computing
the Q function and then we're being e-greedy with respect to it.
Now, we're going to need some more data and we're re-updating
our Q function and we're being greedy with respect to it.
And so we're e-greedy with respect to it.
And so, we're always sort of doing
this dance between updating stuff, getting more evidence,
but then trying to kind of exploit that knowledge up to some random exploration.
And the maximization bias points out that maybe there can be some problems with this.
Okay. So, let's just consider a particular example.
Imagine there is a single state MDP which means there's only one state.
Um, but there are two actions and both of them actually have 0 mean random rewards.
So now, you can think of these as being like, Gaussians.
Right now, we're mostly talking about it when the reward is
actually deterministic but it doesn't have to be.
It could be stochastic reward.
But in this case, where you would imagine that whether you take action a1 or action a2,
your expected value is zero,
but the value you get on any particular episode- any particular step might not be zero.
Might be one or minus one or things like that.
The average is still zero but on any particular step,
you could have something different, okay?
But the expected value is zero um,
and so the Q value for both sa1 and
the Q value for sa2 is zero which is the same as the value.
And these are all the optimal Q and S values.
So let's imagine that there are some prior samples.
You've tried action a1 a bunch of times,
you've tried action a2 a bunch of times,
and you compute an empirical estimate of this.
And here again where um,
there's just a single state.
Um, and we can just average over these.
Let's imagine that it's super simple that we have um,
gamma is equal to zeros.
So, we're really just estimating over the immediate reward.
Okay, so there's no future rewards.
We're just saying all the times that we've tried this action before.
What were all the rewards we get when we average?
And now what we wanna do is we wanna take
our empirical estimates of the Q function for a1 and a2,
and we want to figure out what the greedy policy is.
And the problem is that it can be biased.
So even though each of these unbiased estimators of k- of Q are themselves,
even- even though the two estimates the ah,
actions are unbiased, when you take a max over it, it can be biased.
Let's just write out what that is. So our V Pi hat is equal to the expected value of
max over Q a1, Q a2.
So I'm going to
be taking the expected value of max of these two things because
that's how I defined my policy.
My policy says pick whichever of these two empirically looks best.
But we know that from Jensens,
this is greater than equal to if you switch the max and the expectation [NOISE].
And this is just equal to max of zero, zero.
So the important part is this,
and this is equal to the true V Pi.
So that means that whatever we compute um,
can be a biased estimator of the true V Pi.
So why did this happen?
Well if you get ah, you know,
if you only have a finite number of samples um,
I- if I have tried action a1 a finite number of times,
it might be on that finite number of times it happens to
look slightly positive like it's like,
a 0.1 instead of zero.
And then when I take my policy,
I'm going to maximize over those.
So I'm going to immediately exploit whichever one happens
to look better even if with statistical chance.
So that's why you can get this maximization bias.
And the same thing can happen in terms of MDPs.
So ah, this generally can happen.
You can also look at some nice examples from this paper by
Johns- Johnson Tsitsiklis and
Shie Mannor where they show how this can also happen in Markov decision processes.
Where essentially if you ah,
if your estimates for these Q functions ah,
then you're going to be sort of biased to whatever has happened to look good in
your data and so you can have a maximization bias.
So one thing that was proposed to try to
handle- deal with this case is called double Q learning.
And so the idea is instead of ah,
having one Q function,
we are going to have two different Q functions.
And we're going to create two independent unbiased estimators of Q,
and you're going to use one of them for
your decision-making and the other to try to estimate the value.
And that's gonna allow us to have an unbiased estimator.
And the reason that you might want to do this is because ah,
then it can sort of help- help with this issue that
you can end up being overly bias towards things that have happened to look good.
Yes, now you're separating like between the samples that you're ga-
that you're getting to estimate how good an action is versus ah,
the way you're trying to estimate your policy.
So I'm just going to be a little brief with this because of time.
Q learning basically- double Q learning basically
means that we're going to have these two different Q functions.
Um, and then with 50% probability,
we're going to update one, at 50% probability, we're going to update the other.
So, this was- and in this case,
I'm going to skip out all others um,
the final slides I want to show you the difference.
Um, the difference here can be significant sometimes.
So, in this case,
this is sort of looking at the percent of time that we're taking
bad actions in this domain where you can have,
in this case, you have a scenario where
it's actually the wrong thing to do but it's stochastic.
And so with a small amount of data,
it can end up looking better compared to another option where
the reward is deterministic and actually better but has no stochasticity,
and then Q learning can suffer quite a lot from this maximization bias.
Um, if you're using the same Q function to essentially
immediately define your policy as you are um,
for estimating the value of that policy,
whereas double Q learning does a lot better in this case.
So it's something to consider in terms of when you're implementing these things
and it's pretty small overhead
too because you can just maintain two different Q functions.
Right. I know that was a little bit fast but make sure to put details on there,
um, when I- we upload the additional slides today um.
The main things that you should know from today is to be able to understand how you do
this Monte Carlo on policy controls and same for SARSA and Q-learning.
It's useful to understand how quickly they update, um,
both in terms of whether you have to wait to the end of
the episode and then how quickly information propagates back.
And also to understand how to define the conditions on
the algorithms converging to the optimal Q function. Thanks.
 All right. Good morning, we're gonna go ahead and get started.
Um, homework [NOISE] one is due today,
unless you're using late days, um,
and homework two will be released today.
Homework two is gonna be over, um,
[NOISE] function approximation and reinforcement learning.
Um, we're gonna start to cover that material today,
and then we'll continue next week with deep learning.
[NOISE] Um, deep learning is not a prerequisite for
this class and so we're gonna be releasing a tutorial on TensorFlow,
um, later this week.
[NOISE] Uh, and then next week,
we'll also in sessions [NOISE] have the opportunity to
go into some more of the background to deep learning.
[NOISE] You're not expected to be an expert at it but you
need to know enough of it in order to do the homeworks and,
and do the function approximation.
[NOISE] We will be assuming that you're very familiar with things like,
um, gradient descent, and taking derivatives, and things like that.
Um, TensorFlow and other packages can do that automatically for you,
but you should be familiar with the general [NOISE] process that happens.
Um, before we continue the sim,
may I have any logistic questions.
[NOISE] All right. Let's go ahead and get started.
[NOISE] Um, as you can see I have lost my voice a little bit,
it's coming back but we'll see how we go and if it gets too tricky,
then will take over.
[NOISE] All right, so what we've been talking about so far is thinking about, um,
[NOISE] learning, uh, to be able to evaluate
policies in sequential decision-making cases,
and being able to make decisions.
[NOISE] All of this is when the world is unknown.
And what I mean by that is that,
we're not given in advance,
a dynamics model, or a reward model.
[NOISE] Um, and what we're gonna start to talk
about today is value function approximation.
[NOISE] Um, just so I know actually,
who of you, who of you have seen this before?
Who've seen some form of like value function approximation?
[NOISE] Okay, so, a couple of people, that most people know.
Um, uh, so when I say value function approximation,
what I mean is that so far we've been thinking about domains,
where we tend to have a finite set of states and actions,
and where it is, um,
computationally and memory feasible [NOISE] to just write down a table,
to keep track of what the value is,
of states or the value of state action pairs, [NOISE] um,
or that we could imagine writing data table to write down
the models explicitly of the Reward Model and the dynamics model.
[NOISE] But many real world problems have enormous state and action spaces.
So, if you think about things like the Atari games,
which we can debate about whether or not that's
a real-world problem but it's certainly a challenging problem.
[NOISE] Um, state-space we discussed at the beginning is really sort of a set of pixels.
And so that's gonna be an enormous space and we're not
going to be able to write down that as a table.
[NOISE] And so, in these cases,
we're gonna have to go beyond sort of this tabular representation,
[NOISE] and really think about this issue of generalization.
[NOISE] So, we're going to need to be able to say we
want to be able to make decisions and learn to make good decisions.
We're gonna need to be able to generalize from our prior experience,
so that even if we end up in a state action pair that we've never seen exactly before,
it's like a slightly different set of pixels than we've ever seen before,
that we're still gonna be able to make
good decisions and that's gonna require generalization.
[NOISE] So, um, what we're
gonna talk about today is we're starting with value function approximation,
[NOISE], um, for prediction,
and then talk about control .
[NOISE] Um, and the kind of
the key idea that we're gonna start to talk about in this case is that,
we're gonna be representing the state action value,
uh, value function with a parameterized function.
[NOISE] So, we can think of now as having a function where we input a state,
and instead of looking up in a table to see what its value is,
instead we're gonna have some parameters here.
So, this is, this could be a deep neural network.
[NOISE] This could be,
you know, um, [NOISE] a polynomial.
[NOISE] It can be all sorts of different function approximations but the key here
is that we have some parameters that allow us
to say for any input state, what is the value.
And just like we saw before,
we're gonna both sort of go back and forth between thinking
of there being a state value function,
and a, a state action value function.
[NOISE] Um, and the key thing now is that we have these parameters.
[NOISE] We're mostly gonna be talking about those parameters in terms of w.
[NOISE] So, you can generally think of w as just a vector.
Um, [NOISE] uh, with that vector could
[NOISE] be the parameters of a deep neural network or it could be something much simpler.
[NOISE] So, again, you know,
why do we wanna do this and sort of what are
the forms of approximations we might start to think about?
So, we just don't wanna have explicitly store
learn for every individual state action pair.
[NOISE] So, we don't have to do that in terms of learning the dynamics model,
you don't have to do that in terms of a value function,
or state action value function or even in terms of a policy.
[NOISE] We're gonna need to be able to generalize,
so that we can figure out that, our agents,
our algorithms can figure out good policies for,
um, sort of these enormous state spaces and action spaces.
[NOISE] And so we need these compact representations.
[NOISE] So, once we do this we're gonna get a multiple different benefits.
There would also gonna incur potential problems as well.
So, we're gonna reduce the memory that we need to store all of these things.
We're gonna reduce the computation needed and we might be able to reduce the experience.
[NOISE] And so what I mean by that there is, um,
how much data does our agent need to collect in order to learn to make good decisions.
So, this is really a notion of sort of how much data is needed.
[NOISE] Now, I just wanna highlight here that,
um, you know, there can be really bad,
it would be really bad approximations.
UM, [NOISE] and those can be great in terms of not needing a lot of data,
and not needing a lot of computation,
and not need a lot of memory,
[NOISE] but they may just not allow you to represent very good policies.
[NOISE] Um, so these are,
these choices of representation or defining sort of hypothesis classes.
They're defining spaces over which you couldn't represent policies and value functions,
and so you couldn't,
there's gonna be sort of a bias-variance trade-off here, um,
and add a function approximation trade-off,
in the sense that, if you have a very small representation,
you're not gonna need very much data to learn to fit it,
but then it's also not gonna have
very good capacity in terms of representing complicated value functions or policies.
[NOISE] Um, so, as a simple example,
we could assume that our agent is always in the same state all the time.
You know, all video game frames are always identical,
[NOISE] and that's a really compressed representation,
um, you know, uh, we only have one state,
[NOISE] but it's not gonna allow us to learn to
make different decisions in different parts of the game.
So, it's not gonna allow us to achieve high reward.
So, there's going to generally be a trade-off between
the capacity of the representation we choose,
so sort of the representational capacity
[NOISE] versus all these other things
we would like versus memory, computation, and data.
[NOISE] Others and always,
sometimes one gets lucky and,
and you can choose something that's very, very compact,
[NOISE] and it's still sufficient to represent
the properties you need to represent it in order to make good decisions,
[NOISE] but it's just worth thinking that often there's this explicit trade-off,
and we often don't know in advance what is
a sufficient representational capacity in order to achieve high reward. Yeah?
[NOISE] Is this, um-
What's your name.
Oh, sorry, .
Is this more or less an orthogonal consideration from
the bias-variance trade-off in inter-functional [NOISE] coordination?
Yeah, and you can think of it as right,
the best question is whether this is
an orthogonal trade-off to sort of bias-variance trade off?
[NOISE] Um, can think of it as related,
i- if you choose a really restricted representational capacity,
you're gonna have, um,
a bias forever because you're just not gonna be able to represent the true function.
[NOISE] Um, so it's,
be- and they all have consuming, uh,
a smaller variance for a long time because it's a smaller representation.
[NOISE] So, it's really didn't shift to, uh, related [NOISE] to that.
If you take a machine learning and then, uh,
talked about things like structural risk minimization,
[NOISE] and thinking about, um,
how you choose your model class capacity versus how much data you have,
in terms of minimizing your tests that are similar to that too.
[NOISE] So, you know,
how do you trade-off in terms of capacity to generalize,
um, [NOISE] versus the expressive power.
All right. So, a natural immediate question that we've started,
I've started alluding to already is what function approximation are we going to use?
Um, there's a huge number of choices.
Um, today we're only gonna start to talk about one particular set.
Um, but there's an enormous number probably
most of the ones you can think of have been tried with reinforcement learning.
So, pretty much anything that you could do in supervised learning.
You could also try as a function approximator for your value function, um,
could be neural networks or deep decision trees or nearest neighbors,
um, wavelet bases, lots of different things.
Um, what we're gonna do in
this class is mostly focused on things that are differentiable.
Um, these are nice for a number of reasons.
Um, but they tend to be a really nice smooth optimization properties.
So, they're easier to optimize for.
That's one of the reasons we're gonna focus on them in this class.
Those are not always the right choice.
Um, uh, can anybody give me example of where,
for those of you that are familiar with decision trees,
where you might want a decision tree
to represent either your value function or your policy?
Yeah.
Yes.
Uh, they tend to be highly interpretable.
You keep them simple [inaudible] us all with trees. All right.
[inaudible] actually understand that could be helpful.
Exactly. So, what he just said is that, um, you know,
depending on where you're- how you're using this sort of reinforcement learning policy,
this may be interacting directly with people.
So, let's say this is gonna be used as a decision support for doctors.
In those cases, having a deep neural network may not be very effective in terms of
justifying why you want a particular treatment for
a patient but if you use a decision tree,
um, those tend to be highly interpretable.
Um, uh, well, depending on what features you use but
often it's pretty highly interpretable and so that can be really helpful.
So, thinking about what function approximation you
use often depends on how you're gonna use it later on.
Um, there's also been
some really exciting work recently on sort of explainable deep neural networks
where you can fit a deep neural network and then you can
fit a sort of a simpler function approximator on top.
So, you could fit like, first fit
your deep neural network and then try to fit a decision tree to it.
So, you try to get the kind of the best of both worlds.
Super expressive, um, uh,
function approximator and then still get the interpretability later.
Um, but it's worth thinking about sort of the application that you're looking
at because different ones will be more appropriate in different cases.
Um, so, you know,
probably the two most popular classes, um,
these days and in RL in general are, um,
linear value function approximation and deep neuro networks.
Um, and we're gonna start with linear value function approximation for two reasons.
One is that it's been sort of probably
the most well studied function approximators in reinforcement learning but,
um, up to the last few years and second,
is because you can think of deep neural networks
as computing some really complicated set of
features that you're then doing
linear function approximation over at least in a number of cases.
So, it's really provides a nice foundation for the next part anyway. All right.
So, we're gonna do a really quick review of
gradient descent because we're gonna be using a ton over the next few days.
So, let's just think about any sort of general function J,
um, which is a differentiable function of a parameter vector W. So,
you have some vector W, it's gonna be a set of linear week
soon and our goal is to find the parameter,
um, W that minimizes our objective function.
Haven't told you what the objective function is but we'll define it shortly.
Um, so, the gradient of J of W is we're gonna denote that.
It's told to J of W and that's just us taking
the derivative of it with respect to each of the parameters inside of the vector
and so that would be the gradient,
and so a gradient descent way of trying to optimize for a function, uh,
J of W would be to compute the derivative or the gradient of
it and then to move your parameter vector in the direction of the gradient.
So, if your weights and generally we're going
to always assume the weights are vector, um, uh,
we're gonna be equal to your previous value of the weights minus some learning
rate of the derivative of your objective function.
So, we're sort of just we're figuring out the derivative of
our function and then we're gonna take a step size and that and move our,
our parameter weights over a little bit.
Um, and then we're gonna keep going.
So, if we do this enough times,
um, are we guaranteed to find a local optima?
Right. So, [OVERLAPPING] assume it yeah.
So, they could be yeah, there may be some conditions o- on the learning rate.
Um,ah, but yes, if we do this enough we're guaranteed to get to a local optima.
Um, no- notice this is local.
So, we started thinking about this in terms of the polis- uh,
in terms of doing RL,
it's important to think about where are we gonna converge to and if we're
gonna converge and I'll talk more about that throughout class.
So, this is gonna be sort of a local way for us to try to smoothly start changing
our parameter representation at the value function in order to try to get to a better,
um, better approximation of it.
Right. So, let's think
about how this would apply if we're trying to do policy evaluation.
So again, policy evaluation is someone's giving you a policy.
They've given you a mapping of, um,
first date what your action is and this could be,
it could be stochastic.
So, it could be a mapping from states to a probability distribution over actions.
So, but someone's giving you a policy and
what you wanna do is figure out what's the value of that policy.
What's the expected discounted sum of rewards you get by following that policy.
So, let's assume for a second that,um,
we could quer- query a particular state and then an Oracle would just give us the value,
the true value of the policy.
So, I, you know,
I asked you like, you know what's the,
what's the expected discounted sum of returns for starting in
this part of the room and trying to navigate towards
the door under some policy and it says,
okay the expected discounted number of steps it
would take you as on average like 30 for example.
So, um, that would be a way that the Oracle
could return these pairs and so you get sort of this pair of S,
V pie of S and then let's say given that,
we have all this data what we wanna do is we wanna fit a function.
We wanna fit our parameterized function to represent all that data accurately.
So, we wanna find the best representation in our space,
um, of the state value pairs.
So, if you frame this in the context of stochastic gradient descent,
what we're gonna wanna do is just directly try to minimize our loss
between the value that we're predicting and the true value.
So, right now imagine someone's giving us
these true S value pairs and then we
just want to fit a function approximators to fit that data.
So, it's really very similar to just doing sort of supervised learning.
Um, and in general we're going to use
the mean squared loss and we'll return to that later.
So, the mean squared loss in this case is that we're just going to compare
the true value to our approximate value and
our approximate value here is parameterized by a vector of parameters.
Um, and we're just gonna do gradient descent.
So, we're gonna compute the derivative of
our objective function and when we do compute the derivative of that then we're
gonna take a step size and we're gonna do
stochastic gradient descent here which means we're just gonna sample the gradient.
So, what I mean by that is that if we take the derivative of our objective function,
what we would get is we'd get something that looks like this.
[NOISE]
And what we're gonna do is we're going to take,
I'm going to use this as shorthand for updating the weights,
I'm gonna take a small step size in
the direction of this as evaluated for one single point.
So now, there's no expectation and this is just for a single point.
[NOISE] So this is stochastic gradient descent where
we're not trying to compute the average of
this gradient we're going to- we're trying to just sample this gradient,
evaluated at particular states.
And what I've told you right now is that someone's given us
these pairs of states and the true value function.
So you just take one of those pairs,
compute the gradient at that point and then
update your wave function and do that many many times.
And the nice thing is that
the expected stochastic gradient descent is the same as the full gradient update.
Um, so this has nice properties in terms of converging. Yes a name first please.
Um, so just to confirm, uh,
why is the expectation over policy and not over a set of states if you're saying,
if SGD is a single state?
So this is over the distribution of states that you'd encounter onto this policy.
Yeah, the question was you know wh- why do it
over- what does the expectation mean in this case?
In this case it's the expected distribution
of- of states and values you'd get under this policy.
[NOISE] And that's, uh,
it's an important point, will come up later.
It'll come up again later in terms of sort of what is
the distribution of data that you're going to encounter under a policy.
Of course, you know, in reality we don't actually have access to
an oracle to tell us the true value function for any state.
Um, if we did we'd already know
the true value function and we wouldn't need to do anything else.
Um, so what we're gonna do now is talk about how do we do
model-free function approximation in
order to do prediction evaluation um ah without a model.
Okay. So, if we go back to what we talked
about before we thought about EBV sort of Monte-Carlo style methods or these TD
learning style methods um where we could adaptively learn
online a value function to represent the value of following a particular policy.
Um, and we did this using data.
And we're going to do exactly the same thing now
except for we're gonna have to whenever we're doing
this sort of update step of um do- updating our estimator with new data,
we're also going to have to do function approximation.
So instead of just like um incrementally
updating our table entry about the value of a state,
now we also have to re approximate our function whenever we get new data.
All right. So, when we start doing this we're going to have
to choose a feature vector to represent the state.
Um, let me just ground out what this might mean.
So let's imagine that we're thinking about a robot,
uh, and a robot that,
well robots can have tons of really amazing sensors but let's
imagine that it's old school and it just has a laser range finder.
Um, a lot of laser range finders used to basically be a 180 degrees um,
and so you would get distance to
the first obstacle that you hit along all of this 180 degrees.
So maybe here it's like two feet and this is 1.5 feet,
this is 7 feet.
And this sort of gives you an approximation of what the wall looks
like for example. So here's our robot.
It's giving- it's got a sensor on it which is
the laser range finder and it's telling us the distance towards the walls.
And so what would this feature representation be in this case?
It would just be simply for each of these 180 degrees, what's the distance?
One degree, two degree.
[NOISE] That'll be example of a feature representation.
Now, why?
That sounds like a pretty good of it, maybe slightly
primitive but generally a pretty good feature representation,
um, but what's the problem with that?
Well, probably isn't mark off.
So a lot of buildings have hallways that would say, you know,
on my left and my right there's a wall about two feet away um,
and then there's nothing in front of me at least for
it perhaps out to my laser range finder,
you would say you know out of rage.
And that would be true for many different parts of
the same hallway and it will be true for many different hallways.
And so there'd be a lot of partial aliasing.
So this is a feature representation that probably is not
mark off um, but it might be reasonable.
It might be a reasonable one on which to condition decisions,
maybe if you're in the middle of the hallway and that's
what it looks like you was just wanna go forward.
And that's an example of a type of feature representation.
And again just emphasizes the point that
the choice of the feature representation will end up being really important.
Um, and for those of you who have taken through deep learning classes
you've probably already heard this but it's kinda before deep learning.
There was often amo- a huge amount of work and there's
still a huge amount of work on doing feature engineering to figure
out what's the right way to write down
your state space so that you could make predictions or make decisions.
Now, one of the nice things about deep neural networks is that it kind of pushes back
that feature selection problem so that you can use
really high dimensional sensor input and then do less amount of hand tuning.
So what do I mean by hand tuning?
Well, in this case, you know you could use
the raw features about like how far you are to on each of these
a 180 degrees or you can imagine having
higher level abstract features like trying to understand if there are corners.
So you could already have done some pre-processing on this raw data to
figure out what features you think might be relevant if you're going to make decisions.
And the problem with doing that is that again if you- if you pick
the wrong set you might not be able to make the decisions you want.
Yes, the name first please.
Uh, could you please elaborate why this is not mark off,
um, this [NOISE] ah kind of getting the 180 degrees.
Is it ?
Yeah. So, the question is can I elaborate why this is not markup?
Um, I, if just have a 180 degrees for a robot,
if you think about something say like a long hallway.
Let's say this is floor one.
This is floor two, like n gates for example.
So if you have your little robot that's walking
along and it's guiding its laser range finder,
to try and tell it to the distance to all of the things,
um, you're not going to be able to distinguish
with that representation whether you're on floor
one or floor two because your immediate sensor readings are gonna look identical.
And in fact you're not even able to tell where you are in
that hallway from this hallway. Yeah?
[inaudible] So, um, can we generalize that ah if we have partial aliasing then,
uh, we say its not Markov?
Great question. [NOISE] ask, can we generalize to say
if we have partial aliasing it's not Markov? Yes.
I mean, you could change the state representation to be mark off by including
the history um and so then each individual observation would
be aliased but the whole state representation would not be but in general yes,
if you have a state representation for which there is,
um, aliasing it's not mark-off.
Might still be that you could could still do pretty well with
that representation or you might not but it's just good to be
aware of in terms of the techniques one has applied. Good questions.
All right. So let's think about doing this with linear value function approximation.
Um, so what do I mean by linear value function approximation?
It means that we're simply going to have a set of weights and we're going
to.product this with um a- a set of features.
[NOISE] So you know maybe it's
my 180 degrees sensor readings
and then I'm just gonna have a weight for each of those 180 features.
Um, and we can either rep- use that to represent ah
a value function or you can do that for a state action value function.
Um, those of you who are already thinking about
state action value functions might notice that there's
at least two ways to do that once you start getting into q just mentioned that briefly.
You could either have a separate weight vector for each action or you could put
the action as sort of an additional um feature essentially, multiple different choices.
You get different forms of sharing.
Okay? But right now we're just thinking
about um er estimating the value of a particular policy.
So we're just going to think about values and we're gonna say that
remember W is a vector and X is a vector.
Now X and S is just going to give us the features of that state.
So it could be like the real state of the world is where the robot
is and the features you get out are those a 180 readings.
So we're again going to focus on mean squared errors,
our objective function is this mean squared error.
The difference between the values we're predicting and the true values.
And this is our weight update which is,
uh we want to update our weight by a learning rate times the derivative of this function.
So what does this look like in the case of linear value function approximation?
[NOISE] So what we're gonna do is we're just gonna take the derivative of J using
the fact that we know that this is actually X times W. Okay?
So, what we're gonna get in this case is W- delta W is equal to 1.5 alpha to P pi of S
minus S W times
X because the derivative of X times W with respect to W is X. Yes.
Is this expected value over all states or for a particular state?
Great question, remind me your name one more time.
Yes. So the question is,
is this is an expected value of all states or particular state?
When we're doing the update of the W we're going to be evaluating this at one state.
So we're gonna do this per each state, um,
tha- well, we're going to see different algorithms for it but um,
generally we're gonna be doing stochastic gradient descent.
So we're gonna be doing this at each state.
The expected value here you can think about is really over
the state distribution sampled from this policy.
So if you were to execute this policy in your real MDP you would encounter some states.
And if you, um and we'll talk shortly
more about like what that distribution looks like but
that's the- we want to minimize
our error over all- over the state distribution we would encounter under that policy.
They're good questions. Okay. So, if we look at this form, what does this look like?
It looks like we have a step size which we've seen before with TD learning.
And then we have a prediction error which is the difference between the value function,
uh, the true value function and the value function we're predicting
under estimator and then we have a feature value.
So that's one of the nice aspects of linear, uh, uh,
linear value function approximation is that these updates form into this sort of
very natural notion of how far off were you from the true value weighed by the features.
Yeah?
The question about the math here so that you have the negative [inaudible]
the negative inside V Pi s hat.
So, does the- it should be a negative excess there with the negative [inaudible] outside as well?
We're going to push this into either,
so the question is about just being careful about um the negatives they come out.
Um, yes you could push that negative out into here
in general alpha is a constant so you can flip it and be positive or negative.
Generally, you're going to want your, um,
if you're minimizing this is kinda be, ah,
you're going to be subtracting this from the weights but
you just want to be careful of depending on how you're defining
your alpha to make sure that you're taking
gradient descent- gradient steps in the right direction. Okay.
It's a good question.
Okay, so how would we do this,
remembering again that we don't actually have access to the true value function?
Um, so we don't actually know,
so in this equation, right?
This assumes this is true,
like this is if Oracle has given you the value of a state under that policy,
but of course we don't have access to that.
Um, so what we're gonna do is sort of use the same types of ideas wi- as what we saw,
um, in Tabular learning,
um, now with a value function approximation.
So, the return which is the expected or
the return which is the sum of rewards from timestep t till the end of the episode,
is an unbiased noisy sample of
the true expected return for the current state wherein on time step t. And so,
we can think about doing Monte Carlo value function approximation
as really as if we're doing supervised learning on the set of state returned pairs.
So now, what we're doing here,
is we're substituting in G_t.
It's an estimate of the true value.
[NOISE] So, we don't know what the true value is,
but, uh, we know that the,
the Monte Carlo returned is an unbiased estimator,
so we're gonna substitute that in.
[NOISE] Okay, so what does that mean if we're doing linear value function approximation?
It means inside of our wait update,
we have a G here.
[NOISE] So, we would take the state.
We would take the sum of rewards on that episode.
So again, this can only be applied in
episodic settings just like generally with Monte Carlo,
then we take the derivative and in this case that's just x,
our features because we're using a linear value function
approximation and then on the last line,
I'm just plugging in exactly what our,
um, V hat estimator is.
So, we're comparing our return to our current estimator,
um, and then we're multiplying it by our features.
And as usual, we have the problem that G might be a very noisy estimate of the return.
Yes, the name first, please.
[NOISE] Can we differentiate first time and every time like before?
Sort of.
Do we differentiate first-time and every time visit, uh, like before?
[NOISE] Great question to ask.
Do we, um, distinguish between first-time visit and every time visit?
Yes. The same exact distinctions
apply to Monte Carlo up to, remember that applied before.
[NOISE] So, [NOISE] I'm here,
I'm showing a first-visit variant of it,
but you could also, could also do every visit.
[NOISE] And it would have the same [NOISE] strengths and limitations as before.
Every visit is biased,
asymptotically it's [NOISE] consistent.
Okay, so what does the weights look like?
In this case, we would say weight is equal to the old weights plus
[NOISE] Alpha times G_t of s minus v,
uh, of sw, remembering that this is just x times w for that state,
[NOISE] times x of s. [NOISE] So,
it's very similar to what we saw before for Monte Carlo,
um, uh, approximate Monte Carlo policy evaluation.
[NOISE] Um, what we do is we start off,
in this case now instead o- of having a value function,
we just have a set of weights, um,
which is gonna now be the zero vector to start.
And we sample an episode,
you have to sample all the way to the end of the episode using the policy, [NOISE] um,
and then we step through that episode and if it's the first visit to that state,
then we compute the return from that state till the end of the episode,
and then we update our weights. Yeah?
Um, just to check on that, [NOISE] are you adding, uh,
the learning rate, uh, because of the mechanism, uh, reward?
[NOISE] Considering that, uh,
question is about, um,
the Alpha where, oh,
in terms of negative versus positive?
Right. Each one [inaudible] gradient.
Yeah. So, in general, this is gonna look like [NOISE] this.
I'm gonna be a little bit loose on those.
Um, Alpha is gonna be a learning rate,
that's, um, a choice.
Generally, we're gonna be, um,
trying to minimize our objective function that we're gonna be reducing our weights,
um, uh, and will need to be able, again,
be a little bit careful about how we pick Alpha over time, um,
and, and this has been evaluated at each of the states that we encounter along the way.
[inaudible]
and just to be, uh,
[NOISE] careful on step six,
read again factor or just adding up of notice now?
Good question. Um, uh,
on step six, um,
uh, was it ?
sorry.
said, um, "Do we need to have a Gamma function?"
Um, it's a good question.
Um, in episodic RL,
you can always get away with Gamma being one.
Um, so if it's an episodic place,
Gamma can always equal one.
It is also fine to include Gamma here.
[NOISE] So here, generally in episodic cases, um,
you will set a Gamma being equal to one because one of the reasons why you set our,
our Gamma to be less than one is to make sure
things are bounded in terms of their value function,
but then the episodic case,
it is always guaranteed to be bounded, um,
but it is also completely fine to include a Gamma here, yeah.
[NOISE] So, I got a couple of questions about same point,
um, about this, this G,
so when we do that, it seems like we'll and, uh,
sam- sampling G's that have reward- rewards over episodes of different lengths,
[NOISE] but, so doesn't that close
their distribution without stationary and more variance?
This question [inaudible] there's a problem with the fact that, um,
the returns you're taking are gonna be sums over different lengths.
[NOISE] It isn't.
Um, so, uh, you're always trying to estimate the value of being in this state,
um, which itself under this policy.
Um, and in episodic case,
you might encounter that state early on in
the trajectory or late in the trajectory, and your,
your value is exactly gonna be
averaged over whether you encountered early or late and one of the returns.
So there's no problem with, um,
we're assuming all of your episodes are bounded,
they have to be finite.
So there has to be with probability,
one, you're episode has to end.
If that is true, then, um, your,
your rewards are always bounded,
and then you can always just average over this and that's fine.
Sometimes you might encounter, um,
a state really early in the trajectory in a lot of rewards,
other times you might encounter at the end and have very few rewards, [NOISE] um,
and the value of interest the expectation over all of them.
[NOISE] Yeah?
[NOISE] um,
on this part of clarification,
so essentially is uplink,
you're updating this little video approximation at the episode.
So, [inaudible]
And not just once as the velocitor is in.
You're not just updating the weight once an episode many times, right?
So, you look at all of the states you encountered in that episode and for each of those,
you update your weight vector.
[NOISE] Which is equivalent of like generating
all the episodes and trying to feed them in a single, in a single-
[inaudible] Well, what if we did this in a batch setting,
so what if you generate it all every data and then afterwards tried to fit it.
So this is an incremental approach to doing that, um,
and now ends up converging to the same thing. Question, yeah?
Um,
[NOISE] I'm just wondering,
do we include Gamma,
should be Gamma our j minus t slowly start discounting,
um, like going forwards.
J minus t, oh, yeah.
Uh-huh. [NOISE] Catch. [NOISE]
Again, you shouldn't need a Gamma in this case.
[NOISE] So, in general in this case there should be
probably knows that there'd be no Gamma from the episodic case.
But it's good to be precise about these things.
Okay. All right.
So, let's think about this for a particular example.
Um, it turns out that when we
start to combine function approximation with making decisions,
um, ah, [NOISE] and doing this sort of incremental update online,
things can start to go bad.
Um, and there's, uh, um, and,
and what I mean by that is that we may not converge and we may not converge to
places that we want to in terms of representing the optimal value function.
So, there's a nice example, um,
when people are really starting to think a lot about
function approximation in the early 1990s, um, uh,
Baird came up with this example where it can illustrate some of the challenges
of doing function approximation when combining it with doing control and decision-making.
So, we're gonna introduce this example now.
We're doing MC policy evaluation and then we'll see it a few times throughout class.
So, what does this example showing?
So, in this example they're going to be two actions.
So, a_1 is gonna be straight lines and those are all
going to deterministically go to what I'm going to call state S seven.
And this is state S1, S2,
S3, S4, S5, S6.
And what you can see inside of the bubbles there
is what its feature value representation is.
So, remember I said that we would have a state and then we could write it down as,
um, a set of features.
So, what does S1 look like?
It looks like two, two,
three, four, five, six, seven.
So, weight one is two,
um, and weight eight is one.
So, what does S2 look like?
S2 looks like zero to one,
two, three, four, five.
S3 looks Like this.
And so on until we get to S7 which looks like this.
Okay.
So, S7 looks a little bit different than the rest of them.
That is the feature representation of those states.
Now notice that it looks pretty similar to a tabular representation.
In fact, there are more features than there are states.
So, there are only seven states here and there are eight features.
That's completely possible, right?
Like your feature representation could be
larger than the number of true states in the world.
So, then we have, um,
action a_1 and action a_1 always takes us
from any state to deterministically to state S7.
And then we have action a_2 which is denoted by dot dot dot.
And what action a_2 does is, um,
with probability one over six,
it takes you to state Si where i is n one to six.
So, basically uniformly spreads you across one of the first six states.
There are only two actions.
Either you deterministlly go to state S7 or if you take
the second action then you go to one of the first six states with equal probability.
And it's a pretty simple control problem because the reward is zero.
Everywhere, for all actions.
So, the value function for this is zero because there's no rewards anywhere.
Um, and yet we can start to run into trouble in some cases.
So, before we get to that part let's first just think about what,
um, like a Monte Carlo update would do.
Um, and, and let's just imagine also that there's
some additional small probability here that
from S7 that we actually go to a terminal state.
So, um, like let's say, you know,
with probability 0.999 we stay in S7 and or like
0.99 we stay in S7 and 0.01 we terminate.
And, uh, this is a slight modification but I'm
doing that just so we can do it for the Monte Carlo case.
So, we can think of episodes ending.
So, if you're in state one through six you can
either go to S7 or you can stay in states one through six.
If you're an S7, um,
you can either go to states one through six.
You can stay in S7 or you can terminate. All Right.
So, what then- what might an episode look like in this case?
So, let's imagine that we are in state S1.
We took action a_1,
that deterministically gets us to state S7.
Actually before I do that, I'll specify we got zero reward. Rewards was zero.
We went to S7.
We took action a_1.
We got zero reward.
We stayed in S7.
We took action a_1.
We got zero reward and then we terminates.
That's our episode. Okay. So, now
we can think about what our Monte Carlo update would be.
So, our Monte Carlo update in this case would
be let's start with state S1 and try to do the Monte Carlo update.
So, for state S1 the return is what?
Zero.
Zero. So, the return is zero.
Um, what is x? Um, I should tell you.
So, let's start with initializing all of our weights to be one.
So, what is our initial estimate of the value function of state S1?
[inaudible]
How many? So, it's all of the weights are one.
The state S1 representation is 200013.
That's right. Okay. So, and that's just equal to our,
ah, X times W. Okay.
So, then what does our update look like and
of course I would have to tell you what alpha is.
So, let's say alpha is equal to 0.5.
So, what our weights are gonna- our change in the weights is gonna be equal to 0.5
times 0 minus 3 times our feature vector for x.
Our feature vector for x is to 20001.
So, that means that we're gonna get simply minus 1.5
times 20001 minus 3 times minus 1.5.
One, two, three, four, five, six.
So, notice this is gonna give us an update for every single weight but
it's only gonna give us an update for the weights that are
non-zero in this particular state,
which is the first weight and weight eight.
And so then if we were to actually get the new weights,
so now we're going to have w is equal to w plus delta
w. Then our new representation would be minus two,
one, two, three, four,
five, six minus 0.5.
So, that would be one update of Monte Carlo for the first state.
Now you would do this for every single state in that episode.
Say, you would then do it for the first time you
see it and the algorithm I've defined before.
So, we'd next to this for state S7 as well,
where the return would also be zero but the value would be something different,
so we would get a different, um,
well actually in this particular case the value is also three.
Um, it depends on if you've already updated your w then your,
your value will already be different. Yeah.
So, we're doing SGD per state not per episode.
questions is are we doing SGD per episode or state?
We do it per state. Yeah.
In the previous slide where we had before every state- ev- every encounter,
does that mean that-
For every- for every first visit in that episode.
So, yeah. And it's within that specific-
if so then you go to a new episode that would be S7.
question is about through this first visit,
we basically step along that episode similar to what we did with Monte Carlo before and
the first time we are encountering state in that episode
we update the weights using its return.
And when we do that for every single unique state
and that episode the first time we see it.
And then after all of that we'd get a new episode.
Okay. All right.
So, this is what would happen.
Um, and you can see that the changes can be fairly large
because we're comparing like the full return to our value function.
Um, it depends of course on what our alpha,
alpha is an alpha can change over time.
And generally we'll want alpha to change over time in order to get convergence.
Um, this gives an example of sort of what Monte Carlo update
would look like in this case with linear value function approximator.
Okay. So, a natural question might be,
um, does this do anything reasonable?
Are we guaranteed that this is gonna converge to the right thing?
Um, and what does the right thing mean here?
Um, we're constrained by our linear value function approximator.
So, we're gonna say are we gonna converge to sort of like
the best thing in our linear value function approximator.
Okay. Before we do this let's just talk for a second about,
um, the distribution of states and how that influences the result.
So, if you think back for
maybe the first or second lecture we talked about the relationship between,
um, Markov processes, Markov reward processes,
and Markov decision processes.
And we said that once you define a particular policy,
then your Markov decision process is actually a Markov reward process.
Where you can think of it as, um,
a chain where the next state is determined by your dynamics model,
where you only use the action according to your policy.
So, if you run that,
if you run your sort of Markov chain defined by an MDP with a particular policy,
you will eventually converge to a probability distribution over states.
And that distribution overstates is called the stationary distribution.
It's a probability distribution its sayings are like
what percentage of the time you're going to be in state one,
on average versus state two et cetera.
Has to sum to one because it's a probability distribution.
You always have to be in some state and it satisfies a balanced equation.
So, it says that the probability distribution over states before,
um, I summed- yeah, I guess.
Let me just flip this. I think it's a little bit easier to,
to think about it the other way around.
You've got, um, d of S prime is equal to sum over S sum over a.
We're just doing the sum over a right now so that we can be sure that,
um, we allow ourselves to have stochastic policies.
So, we look at all the actions that we could take under the current state.
And then we look at where we could transition to you on the next state.
So, we're in some distribution over states.
We think of all the actions we could take from each of those states,
where we might transition to.
And then that gives us a new distribution over states S prime.
And those two have to be identical.
So, um, this is often also thought about in terms of
a mixing property when your Markov chain has run for long enough.
Um, this balance equation will eventually hold and this is just that
your distribution over states on the previous time step has to be
exactly the same as your distribution over states on
the next time step after this process is fully mixed.
And it's just telling you on average, you know,
how much time are you spending in,
in, um, what's the probability on
any particular time step you're gonna be in a particular state.
This is not telling us how long it takes for this process to occur.
So, this depends a lot on the underlying dynamics of the system.
So, it might be that this takes millions of steps
until you reach the stationary distribution or it might mix pretty quickly,
it depends on the properties of your transition matrix,
um, under the policy.
I'm not gonna get into any of that in this class.
Um, it's just important to know that you can't- it's not like you can just wait
a 100 steps and definitely you are going to be in
the stationary distribution that depends on the problem. Yeah.
Have there been any proof of-
[inaudible] meaning. Yeah.
Have there been any proof of [inaudible]
Any proven bounds on the mixing time of this type of Monte Carlo methods.
Not that I know of. There might be some.
Um, [NOISE] it's a really tricky issue, often,
because you don't know how long it will take to get to this,
sort of, stationary distribution.
There is a really cool paper that just came out in like,
a month ago at [inaudible] , um,
that talks about how,
when we're thinking about of- policy evaluation,
which we'll talk more about later today.
[NOISE] Um, instead of thinking about, um,
superstep, um, ratios, or whether
you'll be taking a certain action and a certain policy or not.
You can think about these stationary distributions,
and the difference between them, in different policies.
Problem is, you often don't know how long,
and whether your data has got to that stationary distribution.
So, would also be really nice if there were easy test to tell if this was true.
That's also really hard to know. Yeah.
Uh sorry, [inaudible].
And it's.
Yes. [LAUGHTER] Um, [inaudible] Why isn't it [inaudible].
Ah, yes.
So, question is about, uh.
So, um, I, sort of,
gave a long what
As when you gave a long prelude about saying, like,
that things might not converge,
but everything looked fine there.
We're gonna go into that bar. Yes, and we're gonna talk about the fact that,
actually, in the on policy setting where we're just doing policy evaluation.
Everything's gonna be fine. It's only when we get into the control case, um,
where we're gonna be using data from one policy to estimate the value of another,
where in this example and many others, things start to go right.
So, we'll use this as a running example, but right now,
there's no reason for you to believe this is pathological.
Okay. So this is the stationary distribution.
And then, the convergence guarantees are related to that.
Okay. So what we're gonna do is to find the mean
squared error of our linear value function approximator,
with respect to the stationary distribution.
Why is this reasonable?
Well, because you probably care more about
your function approximated error in states that you visit a lot.
There's a state that's really, really rare,
probably it's okay to have bigger error there.
You want your overall mean squared error to be defined on that stationary distribution.
[NOISE] So, this is the mean squared,
um, sort of, value prediction error.
Um, and it compares what we predict versus the true value,
um, weighed by this distribution of states.
And what we're assuming for right now,
is that the approximation we're using is a linear value function approximator.
[NOISE] Um, let me just note,
for historical reasons that, um,
John Tsitsiklis and Ben Van Roy.
John is a, um, MIT, ha- um,
I had the pleasure of him teaching me probability, which was great.
And then, um, Ben Van Roy is here.
Um, and was one of,
I think John's, uh, PhD students for postdocs.
Anyway, they, um, in about 1997,
people were getting really interested, in whether or not,
when you combine function approximation with, um,
reinforcement learning, what happened,
and whether things were good or bad.
And- and they're responsible for,
um, this nice analysis.
[NOISE] So, let's assume we have a linear value function approximator.
What you can prove is that,
if you do Monte Carlo policy evaluation,
linear value function approximators,
gonna converge to the wage which have the minimum mean squared error possible.
Just, kind of, best you could hope for.
So, um, this is saying the limit as you have lots and lots of data.
Um, ah then, an- and you run this many, many,
many times, um, then you're, kind of,
just converge to the,
the best weights possible.
Now, this air might not be zero,
because it might be that your value function is not
approximatable with your linears that have weights.
But it's gonna do the best job they can.
It's just gonna find the best.
It's, basically just doing the best linear regression that you
can do, okay, on your, on your data.
[NOISE] So, it's good.
That's, you know, sort of, a nice sanity check.
It's gonna converge to the best thing that you could hope to do.
Um, some people have been asking about, uh, okay, well.
I've knew me this, sort of, incremental method.
And maybe, in some cases, that's reasonable.
And maybe, you're running like a customer recommendation system,
and you're getting data over time,
and you're updating this estimator.
But in some cases, you might have access to just a whole bunch of data from this policy.
And couldn't you just do that to, kind of,
more directly. And the answer is, yes.
So, this is often called Batch,
uh, Monte Carlo value function approximator.
And the idea is that you have a whole bunch of episodes from a policy.
And the nice thing is, now you can just, kind of,
analytically solve for the best approximator.
So, again, our, our GI's are gonna be our unbiased sample of the true expected return.
And what you can do is,
now, N is just our set of data.
This is really a linear regression problem.
We're gonna use our, um,
unbiased samples as estimates of the true value function.
We're just gonna find the weights that minimize this mean squared error.
You take the derivative, you set it to zero.
It's linear regression. You can solve for this analytically.
Um, so, a lil- just like how
we talked about you could do policy evaluation analytically in some cases.
You can also do it analytically in this case for the linear value function approximator.
Um, and note again, this is Monte Carlo.
We're not making any Markov assumptions.
We're just using the full return.
So, this is also fine in non-Markov environments.
Yeah. Can you speak to the [inaudible] of this approach versus our,
our [inaudible] that we use [inaudible] policy evaluation.
Yeah. [inaudible] Okay.
So whe- wha- when we'd wanna do this versus the other derivative one.
This generally has higher computational cost.
X's can be a very large matrix.
It may not be possible to even write down X, X. Um,
is all of your data in the,
in the future representation form,
and it requires taking a matrix inverse.
[NOISE] Um, so that may not be feasible, if you've got, you know,
huge feature vectors, um, and,
you know, millions or billions of customers.
[NOISE] Um, Facebook can't do this,
um, and do this, er, er, directly.
Um, and also, you know, if you're doing this,
you could do this, sort of, incrementally,
but you're always refitting with all of your data.
Um, that also could be pretty expensive.
So most of it it's about memory and computation.
Um, if you have a really small case, it's probably a good thing to do.
And it also depends, whether you already have all your data or not. Yeah.
[NOISE] You could also do some batch as well, right?
And that could help with convergence and not having your,
um, radiant estimations fluctuating crazily.
[inaudible] So, this, of course there's an in-between.
So, you could do, you don't have to.
If you have access to quite a bit of data,
you could either do it completely incrementally or all batch,
or you could do some batches.
Um, [NOISE] and there's some nice, uh,
work by my colleagues.
And also us showing that in,
in terms of, um,
[NOISE] reading into deep learning,
there can be a lot of benefits to doing, sort of,
some amount of this analytical aspect over like, you know,
a sub batch of data [NOISE] because,
um, you're, sort of,
particularly when you get into TD learning.
Or, kind of, proper getting information a lot more quickly than you are,
um, if you're just doing this, sort of, incremental slow update.
Because, remember, in TD learning we're also, kind of, only doing like,
one step of backup compared to kinda propagating all of our information back,
like we do with Monte Carlo.
All right.
So now we're gonna get into temporal difference learning.
Um, so remember in temporal difference learning,
we're gonna use both bootstrapping and sampling.
Monte Carlo only uses sampling to approximate the expectation.
[NOISE] TD learning also use bootstrapping,
because we don't have to wait till the end of an episode.
Um, we just bootstrap and, like,
combine in our estimated ah,
expected discounted sum of returns by using our current value function.
So in this case,
what we used to do is, we would bootstrap.
This is the bootstrapping part.
And our- what we often call our target is
the reward plus gamma times the value of the next state.
And I remember the reason this is sampling is, um,
we're sampling this to approximate our expectation.
We're not taking the full probability of S prime,
given as a, and summing over all of S prime.
So before we did this and we represented everything as a table.
[NOISE] Now, we wanna not do that anymore.
Um, so let me just- before we get into this,
let me just remind us the three forms of like- of the,
the forms of approximation we're gonna have now.
Now, we're gonna have a function approximation, bootstrapping and sampling.
But we're still on policy.
What do I mean by that right now we're still just doing
policy evaluation which means we're getting
data from the policy that we're trying to estimate its value.
It turns out things are just way easier in that case when you're
on policy and perhaps they should be somewhat intuitive.
It's quite similar to the supervised learning case.
Supervised learning, you're generally assuming your data is
all IID or data is a little bit more complicated than that.
But our data's closer to that in this case because we have a single policy.
It's not sort of this non-stationary aspect
that comes up when we start changing the policy.
So, right now we have these three forms of approximation.
Function approximation, bootstrapping sampling but we're still on policy,
mostly things are still going to be okay in terms of convergence.
So, what does that look like?
We're again going to sort of think about doing the equivalent of supervised learning.
We'd like to just have our states and the Oracle tells us what
the value is and fit our function instead of having the oracle,
we're going to use RTD estimates.
So, we're going to use our word plus Gamma times our approximate value of the next state.
And that's going to form our estimate of what the true value is.
Okay. And then we're going to find
the weights to minimize the mean squared error in that setting.
So, if we do that,
what we can see is that if we're doing this with the linear case,
we write this out it's just this is the TD target.
Just as a quick side note,
I'm gonna use the words TD zero a lot.
We haven't talked about it in this class but there's actually a whole bunch of
different slight variance of TD which often called TD Gamma.
And so if you're reading the book that might be a little bit confusing and so I
just want to be clear that we're doing the TD zero variant,
which is probably the most popular and there's a lot of other extensions.
For simplicity, we're just going to focus on TD zero for now.
So, this is the TD target.
This is our current estimate and then we take the derivative.
In this case that means that we're going to end up plugging in
our linear value function approximator for both our current state,
the next state and looking at that difference weighed by the feature vector.
So, it should look almost identical to the Monte-Carlo update
except for the fact that now we're bootstrapping.
So, instead of this being G versus being
the G return of that we saw before for a particular episode,
now we're bootstrapping and we're getting the immediate reward plus the estimate of
the discounted sum of rewards which we're using
our value function approximator to estimate.
So, this is what the TD learning
linear value function approximation for policy evaluation algorithm looks like.
And again we're gonna initialize our weight vector.
We're gonna sample a tuple and then we're gonna update our weights.
So, we get to now update our weights after
every single tuple just like what we saw for TD learning.
And what we can see here is that we're just plugging
in our particular estimator minus old estimator times X.
So, let's see what this looks like on the Baird example.
So, again we have the same state feature representation as before.
State one is 200001.
We still have zero words for everywhere.
Let's set our alpha equal to 0.5.
Now we're in the case through or we can say that there is
no terminal state because TV learning can handle just continuous online learning.
So, we're just gonna assume that S7 always stays S7 under action A one.
So, A one is the solid line and A two is the dashed.
And we're gonna initialize our weights to 1111.
And then let's look at this tuple.
So, just like the first tuple we saw before let's imagine we're in state one.
We took action A one, we got a reward of zero when we went to state S seven.
So, why don't we take a minute and you
calculate what the new weights would be after we do
that update and maybe compare back to
the Monte Carlo case in terms of how much they have changed.
And feel free to talk to a neighbor.
Let's make this a little bigger so it's easy to remember what S seven is.
All right. Have they moved a lot, a little?
How much of the weight changed compared to what we saw with TD with Monte Carlo?
Seen some people indicates smaller. Yes, that's right.
Okay. So, the- the- value- the initial value of the states this is gonna
be so for X of S one times W it's still gonna be three.
For X S prime S prime is seven.
We look up what that is. This is also going to be three.
But now what we're gonna have,
is we're gonna have Delta W is equal to Alpha times
zero plus 0.9 times three minus three.
So, that's gonna be equal to Alpha times minus 0.3.
So, remember before it was actually minus three so it's a much bigger update.
And so then when we add that into our- our new weights,
we're gonna move our weights but we're gonna move them much smaller than we saw before.
And this shouldn't be too surprising that sort of consistent with what
we saw for Monte Carlo updating and TD learning.
TD learning is only updating these sort of
smaller local changes like one state action or word next highest state.
The- the Monte Carlo is saying this is an episode full episodic return.
It's not bootstrapping.
So, it's a no really the return from starting in state.
S one is zero. So, we're gonna move it a lot more here.
It's saying, okay, I'm going to pretend that the return from status one is 2.7,
which is close to three, it's not zero.
So, when we move our out weights over here,
the- the difference is gonna be much smaller than what we saw for Monte Carlo,
which is similar to what we saw without function approximator.
All right. Whatever theoretical properties in this case, pretty good.
So, these are also, uh,
if you look at TD zero,
you're gonna converge the weights which aren't necessarily quite as good
as Monte Carlo but they're within a constant factor.
So, they're going to be one over one minus Gamma of the minimum possible.
So, they're not quite as good as Monte Carlo but they're pretty good.
And depending on your, uh,
discount factor and the function approximator possible,
this varies in terms of benefits.
So, just to check our understanding for a second,
I've put up both of these results.
So, one says the Monte Carlo policy evaluator
converges to the minimum mean squared error one possible under
your linear value function approximators and TD zero
converges to within one over one minus Gamma of this minimum error.
So, again what is this minimum error that says if you could
pick any linear value function approximator,
uh, how good could that be at representing your true value of your policy?
So, let's take just another minute and this is a good one to talk to a neighbor about.
If the value function approximator is the tabular representation,
what is the MSVE for both Monte Carlo and TD?
We guaranteed to converge to the optimal solution,
optimal value for the true value for V of pi or not
and if it's not clear what the question is, feel free to ask.
[NOISE]
So when you, when you say it's a tabular representation,
do you mean that you are reducing the repre- representational capacity of the system?
Last week's session is, if I say it's tabular representation; what do I mean by that?
I mean that there is one feature for each state,
it's like a one-hot encoding.
So it's like the same representations we saw for the first few lectures,
where like, for each state you have a table lookup for that value of that state.
[NOISE]
Yeah?
Can you please explain-
And your name?
Can you please explain what the barrel mains into?
Like, if they're [inaudible] into.
Ah, good question. So TD0,
I- everything we're seeing in uh,
um question is about TD versus the TD0.
Everything we're talking about in class right now is TD0.
I'm using that because um,
there are multiple versions of TD.
And if you look at the book they'll have TD Lambda sometimes too.
So I'm just making sure it's clear.
So if you read other resources you'll know which version of TD to know too.
Alright. Well first question.
For the um, if we're using a tabular representation,
can we exactly represent the value of a policy?
[inaudible]
Well if we're using the,
if we- for every single state in the world,
you can grab a different- different table at representation,
can we exactly represent the value of the policy?
Yes. Yeah, we can.
So if you um, I,
if you have one feature for every single state in the world,
it's not going to be practical, we can't actually do this,
but you can exactly represent the value of that policy.
How could you do this? You could simply run the policy for every single state.
Um, I do Monte Carlo returns an
average and that would give you the true value of the state. So you could do it.
You could represent the expected discounted sum of returns
by representing that in every single table.
So that means that um,
this error is equal to 0 because um,
your functional capacity is sufficient to represent the value.
Let's go.
So what we're seeing is in expectation, right,
the difference between what you're function [inaudible] actually gets
to 0 but it's like any chart is going to be a bit different.
So there's um, I- i-in expectation at 0 but at any upsets, it-it's different.
In this case, if you have a tabular representation and this is in the limit,
so with infinite amounts of- of data, et cetera,
then this will be um,
this will be 0 for every single state.
So this, equals 0 for every state.
You will converge to the right value for
every single state if you're using a tabular representation.
And that's because if you think of just having like
literally infinite amount of data and you run your policy, just you know,
infinite-infinite amount of times then for every state you have
an infinite number of trajectories starting from that state,
and you can write down that value separately in the table. So it'll be 0.
So what that means is that the mean squared value error for
the Monte Carlo estimator is 0 if you're using a tabular representation.
And because it's 0, that is exactly the same
as the mean squared value estimator for TD except for -- so
this is just equal to MSVE of the Monte Carlo times one over one minus Gamma.
So, that means that this is also 0.
So if it's a tabular representation,
just to sort connect back to that,
um, not- none of these methods have any year.
Yeah, question at the back? Your name first.
Me?
Yeah.
Uh, I'm wondering where the one over one minus Gamma constant came from?
Yes, the question is; where does that one over one minus Gamma constant come from?
Um, in the interest of time,
I'm not gonna go through it too much.
Um, I encourage you to read the Tsitsiklis paper.
Um, intuitively, there is error that's a propagate-propagating here
because of the fact that we're Bootstrapping and so if you have a function,
what this- what this result is sort of trying to highlight is that,
if your function approximator actually has no error,
then there's gonna be no difference between Monte Carlo and
TD because for both of them the mean squared value error,
um, inside of that sum,
the minimum over w is going to be 0.
So it doesn't matter whether you're using TD or Monte Carlo.
But if that's not true,
like if you can't actually exactly represent the value function,
then you're going to get error and that error is going to um,
so like basically you can think of
one over one minus gamma is approximately a horizon each.
And basically that's getting multiplied because you're adding up these errors.
And the reason those get added up is because you're bootstrapping.
You're propagating that error back whereas Monte Carlo doesn't suffer that. Yeah.
Um, my name is . In general,
the mean squared error has taken over distribution uh,
of the states but-
Under the policy yeah.
-yeah, yeah. Under specific policy, uh,
but the only specific one we've seen as a stationary distribution.
Do you ever use another one? Like-
Great question, - Okay,
right now we're seeing this under the stationary distribution of the states that you're
gonna reach under the policy that you're exit that you care about evaluating.
For that, I think that's the right choice because
that really is the state you're gonna get to under this policy.
We start to think about control.
You might want others, like,
if you're gonna change your policy.
Okay. All right so let's um,
I guess just briefly more on this.
I- are they faster?
Is one of them better? To my knowledge that's not really understood.
If you come across any literature on that, I'd love to hear it.
Um, practically TD often is better,
is to empirically often the bootstrapping often helps to pull up.
Alright. Let's move on briefly to control.
Um, it's going to be pretty similar.
So instead of representing the value function
and we're going to represent the state action value function
which is what we saw before when we wanted to often
move from policy evaluation to control.
And now what we're gonna do is we're going to interleave sort of policy evaluation with
a value function approximator with performing like an e-greedy policy improvement.
Um, this is where things can start to get unstable.
Um, what are we doing in this case?
We generally involving function approximation,
bootstrapping, we're often a- are also doing sampling.
But really the- the really big issue seems to be the off-policy learning.
But when we think about before we had
this nice stationary distribution or converging to a stationary distribution over states,
we're not going to be doing that anymore because we are going to be using
our- changing our control policy over time,
and so that's changing the distributions of states that we encounter.
And um, setting the bar to often call it The Deadly Triad.
If you want to, start combining
function approximation and bootstrapping and off-policy learning,
things start to get a little bit um,
they can fail to converge or converge to something good.
Alright. But before we get into that let's think about it procedurally.
So now we're going to have um,
Q functions that are parameterized by a W,
and we can again do stochastic gradient descent,
so its going to look almost identical to what we had before.
And again, the stochastic gradient descent can sample
the gradient which means for particular state action pair,
then we're gonna do these updates.
So, here what we're gonna do,
is we're gonna represent, um,
our Q function by a set of linear state action, um, weights.
So, that means we're gonna have features that
sort of both encode the state and the action.
So, like what I saw when I was turning left as my robot for example.
Um, so, it's gonna be a combination of these two.
And then, once we have that then we're gonna just have
a weight vector on top of that for the Q.
So, we're not having separate weight vectors for each action.
Instead, we're having features that try to
encompass both the state and action themselves.
And then, we can do our Stochastic gradient descent on top of that.
So, how does this work for,
um, uh, like, Monte Carlo?
Um, it's gonna look almost identical to before.
We're just gonna again use our return.
Now, we're gonna be defining returns from a particular state action.
For doing first visit the first time we reach the state action in that episode,
we look at the return,
the sum of rewards till the end of the episode and we use that as our target.
Use that as our estimate of the oracle,
the true Q function and we update towards that.
In SARSA we're, gonna use a TD target,
so we're gonna look at the immediate reward of our tuple plus gamma times
Q of the next state that we encountered and the action we took.
And so, then we're,
again I'm just gonna just plug that in.
And then for Q learning,
it's gonna look almost identical to Q learning
except for again we're gonna plug in function approximators everywhere.
So, we're gonna plug in, um, this.
Remember, is gonna be a function of RX which is gonna be a function of S
prime and A prime times R
W. Whereas here this is going to be a function of the state in action.
Everything's linear and we're just doing different forms of
bootstrapping and comparing whether or not we take a max or not.
All right.
So, I went through that a little bit fast but it's basically exactly
analogous to the first part which we sort of stepped through more carefully.
Uh, so, far everything's with Q functions now.
Why might this gets or weird or tricky?
So, TD with value function approximation does not really follow in a gradient.
I don't have time to go into total details on that today,
but there's a ni- some nice explanations on this in Chapter 11.
So, certain Umberto Chapter 11,
um, it's a great resource and we also have lecture notes available online.
Um, and so, informally we're sort
of doing this interleaving or doing this like approximate sample Bellman backup
combined with what's often known as a projection step because we're trying to
sort of project our value function back into the space of representable functions.
And intuitively for why this might start to be a problem,
is that the Bellman operator we showed is a contraction.
Like when we're doing dynamic programming we showed that if you do
Bellman opera- Bellman backups you're guaranteed to converge to a fixed point.
When you do the value function approximator,
it can be an expansion.
What does an expansion mean.
Well, what a contraction,
just as a reminder what a contraction said is
let's say for an operator that's a contraction.
If you apply this operator and this is an operator like the Bellman equation.
I wanna back up, if you apply it to two different value functions,
the distance between this feel like a max norm or something
like that is less than or equal to the previous distance.
Which means as you apply this operator,
the distance between your old value function and your new value function keeps
getting smaller and smaller and smaller and eventually get to a fixed point.
The problem is this now we're not doing that anymore.
It's more like we're doing like O V and then we do some sort of projection operator.
I'm just gonna call it kinda weird P. So,
this is the projection operator which means when you compute your new value function,
it may no longer lie in your value function approximators space
and you're gonna have to refit it back into that space.
And when you do that, um,
that operator itself can be an expansion.
For those of you that are interested in some of the early sort of discussions of this,
Jeff Gordon, um, has a really nice paper on averages from 1995.
Um, but they talk about how linear value function approximators can be an expansion.
So, the Bellman backup is fine.
It's a contraction but when you do this approximation you
might expand the distance and that's one of the problems.
Okay. So, if we go back to our Baird example and,
um, think about this a little bit more in terms of the, the control case.
So, let's imagine that we have a setting where,
um, you have two different policies.
And the first policy, and this is the policy that you want to
evaluate you always take the solid line.
So, you always take A1 and in your behavior data, this is the data that you're,
this is the policy you're using to gather data,
you take A2 with six-sevenths of the time and you take A1 one seventh of the time.
Gamma is point nine nine.
Um, and what you do is you generate a whole bunch of data.
So, you generate data,
data under your behavior policy.
So, there's some really cool work on how you deal with, um,
sort of correcting for the,
the data that you're getting versus the data you wanna evaluate.
Let's imagine we, we don't go into any of that which I think is super cool and we're,
instead we're just gonna do something super simple which is
we're gonna throw out all the data that doesn't match.
So, imagine you just throw away data if,
um, A is not equal to Pi of S. So,
you generated all these data points.
So, what does it, what do I mean by data points here?
We had SAR as prime.
So, you take all these tuples.
If it turns out that the action that was taken there is not the same as the,
the policy you wanna evaluate but where you're only ever taking A1,
just throw out that tuple, you don't update.
So, now all of your remaining data is consistent with your policy.
So, let's imagine then you tried to do TD learning with that data.
The problem is, you can diverge.
And what do I mean by that?
It mean that, that, that your weights could blow up.
Super interesting why this happens.
Um, the main intuition for it is that your distribution data is
not the same as the data you'd get under your desired target policy.
In particular, if you were actually to run the,
the policy carve out Pi what would happen?
Let's say you start off in state S1. You take A1.
That determinant will get you to state seven but you're gonna stay
in a seven for a really long time because it's deterministic.
So, you'd get like S1. S7, S7.
Let's say even you did this maybe you,
maybe it wasn't episodic case and you have multiple episodes,
you'd still have cases where you'd like have very little bit amount of data
about S these S's and lots of data about S7.
But in the data that you get from
your behaviour policy because a bunch of the time it takes A2,
it'll keep teleportating you back to one of S1 through S6.
Which means the distribution of your data,
the data you have looks very different.
The distribution of states you visit looks very different than
the data that you get and the states you'd visit under Pi.
And that is the problem. If you,
if you don't account for this mismatch,
then the values can diverge.
Even though they're all sort of compatible,
all in the sense that you're only ever using
state action pair if you took the action under your desired policy.
And this sort of issue can also come up,
um, when you're using Q learning.
Um, uh, and you're doing generally like updating this policy over time.
So, in terms of this,
um, just to briefly summarize before we finish.
In the tabular case everything converges, it's beautiful.
Um, in the linear case,
[NOISE] I mean by this I mean that,
um, you can chatter.
You basically converge but you might,
um, uh, there might be some oscillation.
Uh, but Q learning like we are doing this off policy aspect can diverge.
And once we get into nonlinear value function approximation,
every- like mostly all bets are off.
Now, this is a little bit of an oversimplification.
Um, there has been a huge amount of work and huge amount of interest in this because
everyone wants to do function approximation with or else we can tackle real problems.
And so, over the last, like,
one or two decades, there's been a huge amount of work on this.
Um, and there are some algorithms now that do have convergence guarantees.
Um, and there's some coo- super cool really recent work, um,
where they're looking at batch URL which can converge with nonlinear approximators.
So, there's definitely a lot of work on this that we're not gonna get to.
Um, I just wanna highlight that it's
a really important issue not just whether it converges,
but what it converges to.
Like you might converge to a point which is a really bad approximation.
So, it's stable.
It doesn't, your dab- your weights aren't blowing
up but it's just a really bad approximator and,
and some of the critical choices here are
your objective functioning and your feature representation.
So, just before we close I think this is a really nice figure from Sutton and Barto.
Um, what they're showing here is you can think of it as like you have
this plane which is where you can represent
all of your linear value function approximators.
And what happens when you do a Bellman update,
um, or like you do a TD backup,
is that you're gonna now sort of have a value function that might not
be representable in your plane and the you're gonna,
you can project it back.
And these different, you can quantify different forms of error,
different basically this allows you to find
different objective functions that you're trying
to minimize in order to find the best approximator.
And we've seen one today which is sort of this
me- minimum mean squared error approximation
essentially over the like these Bellman errors but that's not
the only choice and it's not necessary even the best choice.
Um, because it might be that the one that has the smallest error there
is not the same one that has the best performance in your real problem.
So, that's a little bit fast but it,
it's super and Shane that's covered in,
um, Sutton and Barto in 11.
If you wanna go into more detail.
Just really quick, what are the things you should understand?
You should, um, you have to implement these ones on
linear value function approximator both for policy evaluation and control.
You should understand whether or not things can, uh,
converge in the policy evaluation case
and when the solution has zero error and non-zero error.
Um, and you should just just understand qualitatively
what are the issues that can come out so that some
of these solutions may not always converge and it's
this combination of function approximation bootstrapping and all policy learning.
All right. So, that's enough just to get started with the homework
two that we're gonna be releasing this week, today.
And then, um, next week we're gonna start to talk about deepening.
Thanks.
 So, homework two is out now.
I recognize that there's a really broad spectrum of
background in terms of whether people have seen deep learning,
or not before, or,
or taken a class, [NOISE] or used it extensively.
[NOISE] Um, just a quick humble, who,
which of you have used TensorFlow or, um, PyTorch before?
Okay. A number of you, but not everybody.
So, what we're gonna be doing this weekend sessions is,
we gonna be having some more background on deep learning.
You're not expected to become, or,
or to be a deep learning expert to be in this class, but we,
you only need to have some basic skills in order to do homework two,
um, be able to use function approximation with a deep neural network.
So, I encourage you to go to session this week if you don't have a background on that.
We're gonna today cover a little bit on deep learning but very,
very, very small amount, um,
and focus more on deep reinforcement learning.
[NOISE] Uh, but the sessions will be a good chance to catch up on that material.
Um, we're also gonna be reaching,
uh, releasing by the end of tomorrow.
Um, what the default projects will be, uh, for this class.
Er, and you guys will get to pick whether or
not you wanna do your own construction project or,
uh, the default project.
Um, and those proposals will be due,
um, very soon, er, in a little over a week.
Are there any other questions that people have right now? Yeah.
The assignments, [inaudible] are they limited to TensorFlow?
asked the question if,
if the assignment is limited to TensorFlow.
I'm, I'm pretty sure that everything relies that,
er, you're using TensorFlow. So, yeah.
Just a-feel free to reach out on Piazza and double-check that,
but I'm pretty sure any Oliver auto-graders is just set up for TensorFlow,
for, so for this one even if you use PyTorch,
some way please use TensorFlow.
Um, I'll believe you guys also should have access to the Azure credit.
Um, If you have any questions about getting setup
without feel free to use the Piazza, uh, Piazza channel.
We also released a tutorial for how to just sort of set up your machine last week.
So, if you're having any questions with that,
that's a great place to get started.
Um, you could look at the tutorial,
you can look at the video, or you can reach out to us on Piazza.
Any other questions? All right.
So, we're gonna go ahead and get started.
Um, uh, what we're gonna be covering today
is sort of a very brief overview about Deep Learning,
um, as well as Deep Q Learning.
Um So, in terms of where we are in the class, we've been,
we have been discussing how to learn to make decisions in
the world when we don't know the dynamics model of the Reward Model in advance.
Um, and last week, we were, we were discussing value function approximation,
particularly linear value function approximation.
And today we're gonna start to talk about other forms of
value function approximation in particular,
um, uh, using deep neural networks.
So, the- why do we wanna do this at all?
Well, the reasons we wanted to start thinking about, uh, er,
using function approximators is that if we wanna be able to use
reinforcement learning to tackle really complex carry problems.
Um, we need to be able to deal with the fact that often we're gonna have
very high dimensional input signals or observations.
Um, so, we wanna be able to deal with sort of pixel input,
like images, or we wanna be able to deal with really complex, um,
information about customers, or patients,
or students, um, where we might have enormous state and our actions spaces.
I'll note today that we're mostly not gonna talk so much about enormous action spaces,
but we are gonna think a lot about really large state spaces.
And so, when we started talking about those,
I was arguing that we either need representations of models.
Those mean that's sort of the dynamics,
or the reward models.
T, T or R or a state-action values Q,
or V, or our policies,
um, that can generalize across states and our actions.
With the idea being that we may in fact never encountered the exact same state again.
You might never see the exact same image of the world again,
um, but we wanna be able to generalize from our past experience.
And so, we thought about it,
instead of having a table to represent our value functions, um,
we were gonna use this generic function approximator where we have a W now,
which are some parameters.
[NOISE] And when we thought about doing this,
we said what we're gonna focus on is we're gonna focus on
function approximations that are differentiable.
Um, and the nice thing about
differentiable rep-representations is that we can use our data,
and we can estimate our parameters,
and then we can take use gradient descent to try to fit our function,
to try to write that into write,
or represent our q function or a value function.
So, I mentioned last time that most the time we're
gonna think about trying to quantify the fit of our function,
compared to the true value as,
um, a mean squared error.
So, we can define our loss j,
and we can use gradient descent on that to try to find the parameters w that optimize.
And just as a reminder stochastic gradient descent was useful because when we could
just slowly update our parameters as we get more information.
And that information now could be [NOISE] in the form of
episodes or it [NOISE] could be individual tuples.
[NOISE] When I say a tuple,
I generally mean a state-action reward next state tuple.
Um, and the nice thing is that
the expected stochastic gradient descent is the same as the full gradient update.
So, just to remind ourselves,
last time we were talking about linear value function approximations.
Um, and that meant that what we're gonna do is,
we're gonna have a whole bunch of features to describe our world, um,
as so, you know,
these features we will input our state,
state as the real state of the world and we would output our features.
And so, this could be things like a laser range finder for our robot,
which told us how far away the walls were in all 180 deg- degree directions.
We talked about the fact that that was an aliased version of the world because,
um, multiple hallways might, might look identical.
So, our value function now is a dot product between those features,
that we've got out about the world, um, with the weights.
Our objective function is again the mean squared error.
And then we could do this same weight update.
And the key hard thing was that we don't know what this is.
So, this is the true value of a policy.
And the problem is we don't know what the true value of a policy is,
otherwise we wouldn't have to be doing all of this learning.
Um, and so we needed to have different ways to approximate it.
And so, the two ways we talked about last time was inspired by a work on Monte Carlo,
or on TD learning is we could either plug-in the return from the full episode.
This is the sum of her words.
Or we could put in a bootstrapped return.
So, now we're doing bootstrapping.
Where we look at the reward,
the next state, and the value of our next state.
And in this case we're using a linear value function approximators for everything,
which gave us a really simple form of what the derivative is,
of this function with respect to W. Basically it's
just our features times essentially this prediction error.
So, people sometimes call this is the prediction error.
Cause it's the difference between the value,
or right now we're using GT as the true value.
Of course in reality it's just a sample of the value but, well,
it's the difference between, um,
the true value and our estimated value.
I'm gonna shrink that difference.
So, in this case I've written, um,
these equations of all for linear value function approximation,
but there are some limitations to use the linear value function approximation,
even though this has been probably the most well-studied.
So, if you have the right set of features,
and historically there was a lot of work on
figuring out what those rights set of features are.
They often worked really well.
And in fact when we get into,
I think I mentioned briefly before.
When we start to talk about deep neural networks you can think of,
a deep neural network is just a really complicated way to get out features,
plus the last layer being a linear combination of those features.
For most of the time when we're talking about deep RL with,
um, a deep neural networks represent the Q function.
That's the type of representation will be looking at.
So, linear value function is often
really works very well if you're the right set of features,
but is this challenge of what is the right set of features.
Um, and there are all sorts of implications about whether or not
we're even gonna be able to write down the true p- um,
value function using our set of features,
and how easy is it for us to converge to that.
So, one alternative that we didn't talk so much
about last time is to use sort of a really,
really rich function approximator class.
Um, where we don't have to,
have to have a direct representation of the features.
Er, and some of those are Kernel based approaches.
Um, has anybody seen like,
ah, Kernel based approaches before?
Or like k-nearest neighbor type approaches?
If you take a machine learning you've heard of k-nearest neighbors,
those are sort of these non-parametric approaches,
where your representation size tends to grow with the number of data points.
Um, and then they can be really nice and they have
some actually really nice convergence properties for reinforcement learning.
The problem is, um,
that the number of data points you need tends to scale with the dimension.
So, um, if you have let's say those 180, um, features,
um, the number of points you need to tile that 180 degrees space,
generally scales exponentially with the dimension.
So, that's not so appealing both in terms of computational complexity,
memory requirements, and sample complexity.
So, these actually have
a lot stronger convergence results compared to linear value function approximators.
Um, but they haven't far been used for in a very widespread way.
Yeah. Um, and everyone just [inaudible] name first please to stop me.
Yes.
Yeah. [LAUGHTER]
So, can you repeat again why the exponential behavior happening?
Yeah, student's question was why does the exponential behavior happen and
a lot of these sort of kernel based approximators or non-parametric.
The intuition is that if you
want to have sort of an accurate representation of your value function,
um, and you're representing it by say,
uh, local points around it.
For example, like, with the k-nearest neighbor approach.
then the number of points you need to have everything be close like in
an epsilon ball scales with the- the dimensionality.
So, basically you're just gridding the space.
So, if you think of -- if
you think you have sort of- if you want to have any point on this line, be close,
then you could put a point here and a point here in order to have
everything be sort of epsilon close for all points on that line to have,
uh, a neighbor that's within epsilon distance.
If you want to have it in a square,
you're gonna need four points so that
everything can be somewhat close to one of the points.
Generally, the number of points you need this going
to scale exponentially with the dimension.
[NOISE] But they are really nice, um, uh,
because they can be guaranteed to be averagers which we talked about really
briefly last time that views a linear value function approximator.
Um, when you do a bellman backup,
it's not necessarily a contraction operator anymore which
is why you can sometimes blow up as you do more and more backups.
A really cool thing about averagers is sort of by their name.
Um, when you use this type of approximation,
you don't- they're guaranteed to be- to be a non-expansion,
which means that when you combine them with
a bellman backup it's guaranteed it'd still be a contraction which is really cool.
So, that means these sort of approximators are
guaranteed to converge compared to a lot of other ones.
All right, but they're not gonna scale very well
and in practice you don't tend to see them,
though there's some really cool work by my colleague, Finale Doshi-Velez,
over at Harvard who's thinking about using these for things like, um,
health care applications and how do you sort of generalized from related patients.
So, they can be useful but they generally don't scale so well.
So, what we're gonna talk about today is thinking about deep neural networks which also
have very flexible representations but we hope we're gonna scale a lot better.
Um, now, in general we're going to have
almost no theoretical guarantees for the rest of the day,
um, and- but in practice they often work really really well.
So, they become an incredibly useful tool in
reinforcement learning and everywhere else really in terms of machine learning.
So, what do we mean by deep neural networks?
Well, a number of you guys are experts but, um,
what it generally means in this case is we're just gonna think of com- making
a function approximator which is a composition of a number of functions.
So, we're gonna have our input x and I'm gonna feed it into
some function which is gonna take in some weights.
So, in general, all of these things can be vectors.
So, you're gonna take in some weights and
combine them with your x and then you're going to push
them into some function and then you're gonna
output something which is probably gonna be also a vector.
Then you're gonna push that into another function,
and throw in some more weights.
I'm gonna do that a whole bunch of times,
and then at the very end of that you can output
some y which you could think of as being like our Q.
Then, we can output that to some loss function j.
So, what does that mean here?
It means that y is equal to hn of hn
minus one dot dot dot dot dot h1
of x. I haven't
written all the weights that are going in there but there's a whole bunch of weights too,
and then this is sort of loss function like
before and this you can think of as like our Q for example.
But these are- happen a lot in unsupervised learning
like predicting whether or not something is a cat or not or,
you know, an image, uh,
of a particular object, um, or for regression.
So, why do we want to do this?
Well, first of all it should be clear that as you compose lots of functions, um,
you could represent really complicated functions
by adding and subtracting and taking polynomials and all sorts
of things you could do by just composing functions together that this could be
a really powerful space of functions you could represent.
But the nice reason to write it down like this
is that you can do the chain rule to try to
do stochastic gradient descent. So, how does this work?
Well, that means that we can write down that dj.
So, we really want, you know,
dj with respect to all these different parameters.
So, what we can write down here is we can write down dj of hn and
dhn of dwn and we can do- do this kind of everywhere.
So, dj of h2,
dh of h2, and dh2 of dw2.
So, you can use the chain rule to propagate all of this the- the gradient of, um,
your loss function with respect to w,
all the way back down all of these different compositions by writing out the chain rule.
Um, so that's nice because it means that you can
take our output signal and then propagate that back,
um, in terms of updating all of your weights.
Now, I'm gonna date myself.
So, when I first learned about deep neural networks, you have to do this by hand.
Um, and, uh, so as you might imagine,
this was a less popular assignment and,
uh, it's called backpropagation.
So, you can derive this by hand,
um, and I'll talk in a second about what these type of functions are, you know,
you need differentiable functions for h. But I think one of
the major major innovations that's happened over there, you know, roughly what?
Like last 5 to 8 years is that there's auto differentiation.
So, that now, um,
you don't have to derive all of these, uh,
gradients by hand instead,
you can just write down your network parameter.
Um, and then your network of para- which
includes a bunch of parameters and then you have software like,
um, TensorFlow to do all of the differentiation for you.
So, I think these sort of tools have made it much much
more practical for people- lots and lots of people to use,
um, deep neural networks because you don't- you can have
very very complicated networks so very very large number of
layers and there's no sort of hand writing down of what the gradients are.
So, what are these h functions?
Generally, they combine, um,
both linear and nonlinear transformations.
Um, basically they just- they have to be differentiable.
So, you know, this- this h need to be
differentiable if we're gonna use gradient descent to fit them.
So, the common choices are either linear so you can think of hn is equal to whn minus
one or non-linear where we can think of hn is equal to some function hn minus one.
If it's nonlinear, we often call this an activation function.
Due to time, I'm not gonna talk in class much about
the connections with neural networks which is
what inside of our brain which was what's inspiring,
these sort of artificial neural networks.
Um, but inside of the brain,
people think of there is being the sort of
non-linear activation functions where if the signal passes a certain threshold then,
for example, the neuron would fire.
So, these sort of non-linear activation functions can be things like
sigmoid functions or ReLU.
ReLU's particularly popular right but- um.
So- so, you can choose different combinations, uh,
of linear functions or non-linear functions,
um, and as usual we need a loss function at the end.
Typically, we use mean squared error.
You could also use log likelihood but we need something that- that we can differentiate
how close we are achieving that target in order to update our weights.
[NOISE] Yeah? Name first.
So, this ReLU function is not differentiable, right?
It is differentiable, like,
you can- you- you can- you can take it to- the- the- differentiable and it's
ended up being a lot more popular than
sigmoid recently, though I feel like it [OVERLAPPING].
It's not differentiable at one point?
Yes.
But I don't see how gradient [inaudible] is gonna work on the part where it's flat.
Well, if it's flat, it's zero.
So, that ends up just- your gradient is just zero. [OVERLAPPING] Yeah.
The question is about how for- for ReLU,
there's a lot of it where it's flat.
Um, and so if your gradient is zero then your gradients can vanish there.
Um, in- in- in general actually,
we're not gonna talk about this at all in class but, uh,
um, there's certainly a problem is you start having very deep neural networks.
Um, but because of some of these functions you can sometimes end up sort of having,
um, almost no signal going back to the- the earlier layers.
But I- I'm not gonna talk about any of that.
We'll talk- we'll talk some about that in sessions.
Um, they're good to be aware of, um,
and we're also happy to give other pointers.
But yeah, if it's flat, it's okay, you can still just have,
uh, a zero derivative.
Okay. All right.
So, why do we want to do this?
Well, it's nice if we can use this sort of like much more complicated representation.
Um, another thing is that, um,
if you have at least one hidden layer,
um, if you have a sufficient number of nodes.
Um, nodes you can think of as a- if you're not familiar with this is
basically just sort of a sufficiently complicated,
uh, set of, uh,
combination of features, um, and functions.
Um, this is a universal function approximators which means that you
can represent any function with the deep neural network. So, that's really nice.
We're not gonna have any capacity problems if we use
a sufficiently expressive function approximators.
Um, and that's important because if you
think about what we're doing with linear value function approximators,
it was clearly the case sometimes that you might have too limited features and you
just wouldn't be able to express the true value function for some states.
What the universal function approximator, um,
property is stating is that that will not occur for,
um, uh, deep neural network if it is, uh, sufficiently rich.
All right. Now, of course,
you can always think of doing a linear value function approximator
with very very rich features and then that becomes equivalent.
So, given that, you know,
what's another benefit, um,
another benefit is that potentially you can use exponentially less nodes or
parameters compared to using a shallow net which means not as many of those compositions,
um, to represent the same function and that's pretty elegant and,
uh, I'm happy to talk about that offline or- or we can talk about on Piazza.
Then the final thing is that you can learn
the parameters using stochastic gradient descent.
All right. So, that's pretty much that, you know,
deep neural networks in like five seconds.
Um, we're now gonna talk a little bit about convolutional neural networks.
Um, and again this is all gonna be
a pretty light introduction because you're not gonna need to know the details
in order to do the homework except for mostly the fact
of understanding that these are sort of very,
um, expressive function approximators.
So, why do we care about convolutional neural networks?
Um, well, they're used very extensively in computer vision, um,
and if we're interested in having robots
and other sorts of agents that can interact in the real world,
one of our primary sensory modalities is vision, um,
and it's very likely that we're going to want to be able to use similar sorts of
input on our- our robots in our artificial agents.
So, if you think about this,
um, think about there being an image,
in this case, of Einstein.
Um, and there's a whole bunch of different pixels on Einstein,
um, of this picture of Einstein.
Let's say it's 1,000 by 1,000.
So, it's 1,000 by 1,000, you know, x and y.
So, we have 10 to the 6 pixels.
So, this standard often called feedforward deep neural network.
Um, you would have all of those pixels, um,
and then they would be going as input to another layer and, um,
you might want to have a bunch of different nodes that are taking input from
all of those and so you can get a huge number of weights.
So, you might have 10 to the 6 weights per- the st- the- often,
we think about sort of- I know I haven't given you enough details about this,
but often we think of there as being sort of
this deep neural network where we have many functions in parallel.
So, it's not just like a single line but we might have x going into, uh, h1,
h2, h3, h4 then all of those would
then be going in some complicated way to some other functions.
So, you can have lots of sort of functions being computed in parallel.
So, you can imagine your image goes and you've got
one function that computes some aspect of the image and
another function that compute some other aspect of the image and then you're
gonna combin- combine those in all sorts of complicated ways.
So, what this is saying is,
well for that very first one there's maybe gonna be, you know,
a whole bunch of n different functions were computing of the image.
There'd be 10 to the 6 parameters here.
So, if we have these weight times x,
then that would be 10 to the 6 parameters to take in all of that x. That's a lot.
Um, and then if we want to do this for doing different types of weights all in parallel,
then that's gonna be a very very large number of
parameters and we do have a lot of data off it now but that's
still an enormous number of parameters to- to
represent and it also sort of misses some of
the point of what we often think about with vision.
So, if we think about doing this many times and having lots of hidden units,
we can get a really an enormous number of parameters.
Um, so to avoid sort of
this space-time complexity and the fact that
we're sort of ignoring the structure of images,
convolutional neural networks try to have a particular form of
deep neural network that tries to think about the properties of images.
So, in particular, images often have structure, um,
in the way that our- our brain promises images also has
structure and this sort of distinctive features in space and frequency.
So, when you have a convolutional neural network,
we think of there being particular types of operators.
Having so operators again here are like our functions, h1 and hn,
which I said before could either be linear or nonlinear and then
convolutional neural network learn a particular structures for those, um, uh,
for those functions to try to sort of
think about the properties that we might want to be extracting from
images and kind of the key aspects here is that
we're gonna do a lot of weight sharing to do parameter reduction.
So, instead of saying,
"I'm going to have totally different parameters each taking in all of the pixels."
I'm gonna end up having sort of local parameters that are identical and
then I apply them to different parts of the image
to try to extract, for example, features.
Because ultimately, the point of doing this is gonna be trying to extracting
features that we think are gonna be useful for
either predicting things like whether or not,
you know, a face isn't an image or that are gonna
help us in terms of understanding what the Q function should be.
So, the key idea- one of the key ideas is to say that we're gonna have
a filter or a receptive field which is that we're gonna have some hidden unit.
Um, so it's gonna be a function that's applied to some previous input.
At the beginning, that's just gonna be a subset of our image and instead of,
um, taking in the whole image,
we're just going to take in part.
So, we're just gonna take in a patch.
So, we're gonna take the upper corner and we're gonna take the middle.
So, it's like we're just gonna try to compute
some properties of a particular patch of the image.
So, then we can imagine taking,
it's often called a filter, that little, um,
those set of weights that we're applying to that patch and we
could do that all over the image, um,
and we often called the- there's a stride which means sort of how much you move,
um, that little patch at each time point.
There's also this thing called zero-padding which is how many zeros to add on
each input layer and this determines sort of help,
um, helps determine what your output is.
So in this case, if you have an input of 28 by 28 and you have
a little five-by-five patch that you're going to slide over the entire image,
then you're gonna end up with a 24 by
24 layer next because
basically you just take this and then you move it over a little bit.
You move it over, and each of those times you're gonna take those 25.
So, this is five-by-five so you're gonna have 25 input x's and
you're gonna dot-product them with some weights and that's gonna give you an output.
So, here in this case that means we're gonna need 25 weights.
Okay. So, one thing is instead of having our full x input,
we're just gonna take in- we're gonna direct different parts of the x
input to different neurons which you can think of just different functions.
Um, but the other nice idea here is that
we're going to have the same weights for everything.
So, when we took those weights we're going to have sort of,
um, you can think of them as trying to extract
a feature from that sub patch of the image.
For example, whether or not there's an edge.
So, you can imagine I'm trying to detect whether or not there's
something that looks like a horizontal edge in that part of the image
and I try to- and that is determined by the weights I'm specifying and
I just move that over my entire image to see whether or not it's present.
So, now the weights are identical,
and you're just moving them over the entire image.
So, instead of having,
um, you know, 10 to the 6 weights,
I might only having 25 weights and I'm applying those to the same- uh,
just applying them to lots of different parts of the image.
Okay. So, this is sort of what that would look like.
You sort of have this input,
you go to the hi- um, the hidden layer and, yeah,
you're sort of do- also down-sampling the image.
Um.
Why would you want to do this?
Well, we think that often, the brain is doing this.
It's trying to pick up different sort of features.
In fact, a lot of computer vision before deep learning was,
um, trying to construct these special sorts of features,
things like sift features or other features that
they really think captures sort of important properties of the image,
but they're also may be invariant to things like translation.
Because we also think that, you know,
whether I'm looking at, um, the world like this,
or I move my head slightly, um,
that the features that I see are often gonna be identical,
whether I moved to the left or right a little bit.
There are particular salient aspects of the world that are gonna
be relevant for detecting whether or not there's a face,
and relevant for deciding my value function.
So, we want to sort of extract features that we think are gonna
represent this sort of translation in variance.
This means also that rather than just computing, uh, you- you can do this.
You'll use the same weights all the way across the feature, er,
all across the image and then you can do this for multiple different types of features.
So there's a really nice, um,
discussion of this that goes into more depth from 231-n,
which some of you guys might've taken.
Um, and there's a nice animation where they show, okay,
imagine you have your input,
you can think of this as an image,
and then you could apply these different filters on top of it,
which you can think of as trying to detect different features,
and then you move them around your image,
and see whether or not that feature is present anywhere.
So you can do that with multiple different fype- types of filters.
You could think of this as trying to look for whether something's like that
or something's horizontal or vertical,
different types of edges, um,
and, uh, these give you different features essentially that are been extracted.
Um, the other really important thing in CNNs,
is what are known as pooling layers.
They are often used as a way to sort of down-sample the image.
So you can do things like max pooling to
detect whether or not a particular feature is present, um,
or take averages or other ways to kind of just down,
ah, and compress the,
the information that you got it in.
So, just remember in this case and in many cases,
we're gonna start with a really high dimensional input,
like x might be an image and output,
a scalar, like, um, you know, the Q value.
So we're somehow gonna have to go for really high dimensional input and
kind of average and slow down until we can get to,
um, a low dimensional output.
So, the final layer is typically fully connected.
So we can again think about all of these previous processes
as essentially computing some new feature representation,
so essentially from here to here.
We're kind of computing this new feature representation of the image,
and at the very end,
we can take some fully connected layer,
where it's like doing linear regression,
and use that to output predictions or scalars.
Again, I know either for some of you, guys,
this is sort of a quick shallow refresher.
For others of you, this is clearly not in, ah, this is, ah,
would be a whirlwind introduction, um,
but we won't be requiring you to know a lot of these details.
And again, just go to a session,
if you have some questions and feel free to reach out to us.
Okay. So these type of representations,
both Deep Neural Networks and Convolutional Neural Networks,
are both used extensively in deep reinforcement learning.
So it was around in 2014, um,
where I- the workshop where David Silver started talking about,
um, how we could use these type of approximations for Atari.
So why was the surprising? I just sort of wandering back.
So in around 1994, personally in 1994,
um, we had TD backgammon which used Deep Neural Networks.
Well, they used neural networks.
I think there was someone that deep,
and I think out like a world-class backgammon player out of that.
So, that was pretty early on.
And then we had the results that were kind of happening around like
1995 to maybe like 1998, which said that,
"Function approximation plus offline off policy control,
plus bootstrapping can be bad,
can fail to converge."
So, we talked about this a little last time.
That in general, as soon as we start doing
this function approximation even with the linear function approximator,
um, that when you're combining off policy control, bootstrapping,
which means we're doing like TD learning or Q learning,
um, and, uh, in functional approximator,
then you can start to have this, uh,
challenging triad, um, which often means that we're not guaranteed to converge.
And even if we're guaranteed to converge,
the solution may not be a good one.
So sort of there was this early encouraging success and then there were
these results in sort of the middle of
the nineties that we're trying to better understand this,
that indicated that things could be very bad,
and the risk was some of the- In addition to the theoretical results,
there were sort of these simple test cases,
that, you know, these simple cases that went wrong.
So, it wasn't just sort of in principle this could happen,
uh, but there were cases which failed.
And so I think for a long time after that, the,
the community was sort of backed away from Deep Neural Networks for a while.
People were quite cautious about using them because they were clearly,
even simple cases where things started to go really badly with function approximation.
And theoretically, people could prove that it could go badly,
and so there's less attention to it for quite a while.
And then, um, there was the rise of Deep Neural Networks in sort of, you know,
the- the mid 2000s, like, to now.
So, uh, Deep Neural Networks became huge,
and there was called a huge success in them for things like vision,
in other areas, there was a whole bunch of data,
there's a whole bunch of compute.
They we're getting really extraordinary results.
And so, then, perhaps it was natural that, like,
around in like 2014,
DeepMind, DeepMind combined them and had some really amazing successes with Atari.
And so I think it sort of really changed the story of
how people are perceiving using, um,
this sort of complicated function approximation, meters,
and RL, and that yes,
it can fail to converge.
Yes, things can go really badly,
and they do go really badly sometimes in practice,
but it is also possible of them that despite that- you know,
the fact that we don't always fully understand why they always work,
um, that often in practice,
we can still get pretty good policies out.
Now, we often don't know if they're optimal.
Often, we know they're not optimal because we know that people can play better,
but that doesn't mean that they might not be pretty good,
and so we sort of saw this resurgence of interest
in- into deep reinforcement learning. Yeah.
[NOISE]
Um, I guess is there anything from your perspective that the,
the deep learning has solved the problems that they had come up with in the mid '90s?
Or, is it just that kind of through increases
in computational power and the ability to gather a lot of data,
that when it failed,
it kinda doesn't matter,
and we can try some different,
like we- you know,
try it again and kinda put it together and just keep trying until it works?
I guess my question is, did we actually overcome
any of the problems that arose in the late '90s,
or is it just that we're just kinda powered through?
The question is, you know, how we sort of
fundamentally resolve some of the issues of the late my '90s,
or, um, we kind of brute forcing it.
Um, I think that some of the issues that were coming
up in 1995 to 1998 in terms of convergence,
there are some algorithms now that are more
true stochastic gradient algorithms that are covered in chapter 11,
um, so that I- a- a- are guaranteed to converge.
They may not be guaranteed to converge to the optimal policy,
um, so there's still lot of- there's still a ton of work,
I think to be done to understand function approximator and off
policy control and bootstrapping.
I think there's also a couple algorithmic ideas
that we're gonna see later in this lecture,
that help the performance kind of avoid some of those convergence problems.
So I think people knew about this when they started going into 2013, 2014.
And so they tried to think about, "Well,
when might this issue happen,
and how could we avoid some of that stuff?"
Like, what's causing that?
And so at least algorithmically,
can we try to make things,
that people often talk about stability,
so can we try to make sure that
the Deep Neural Network doesn't seem to start having weights that are
going off towards infinity and at least
empirically have sort of more stable performance. Yes,
[inaudible] for me.
So with the Atari case specifically,
did you- did you avoid that problem?
Well, sort of, that if you tried it by having on policy control? I just don't know.
[inaudible] there wasn't the case that in fact,
that in your Deep Neural experiment,
they updated the performance to match the Apple policies [inaudible].
The question is whether or not in this sort of,
um, ah, if I understood correctly,
in the Atari case like, you know,
where they changed things to be more on policy or,
or which we know can be much more stable.
Um, ah, they are doing deep learning in,
in this case, Deep-Q Learning.
And so it can still be very unstable,
but they're gonna do something about how they do
with the frequency of updates to the networks,
to try to make it more stable.
Um, and it's a great question for me.
We'll see how it works here.
Anyone else? Okay, cool.
So, um, we'll- we'll see an example for breakout shortly, um, of what they did.
Um, so again right now,
we're gonna be talking about using Deep Neural Networks to represent the value function.
Um, we'll talk about using Deep Neural Networks to
represent the policy pretty shortly, next week.
So what are we going to do? We're gonna, again, have our weights.
We're gonna have our same approximators.
Now, we're gonna be using Deep Neural Networks.
And in this case, again, just, uh,
to be clear we're gonna be using a Q function,
um, because we're gonna wanna be able to be doing control.
So, we're gonna be doing control, in this case.
Um, and so, we're gonna need to be learning the,
the values of the actions.
Just to be clear here, an Atari, it generally,
doesn't have a really high dimensional action space.
It's discrete. It's normally somewhere between like four to 18, depends on the game.
Um, so, it's fairly low dimensional,
fairly, um, uh, it's discreet and fairly small.
So, though the state space is enormous because it's pixels it's images,
um, uh, the, the action space is pretty small.
Okay. So, just as a reminder, for Q learning,
what we saw is Q learning looks like this for our weights.
We have to have the derivative of our function.
This is not necessarily gonna be linear anymore, um,
but the way we updated our weights,
was we did this, um,
TD backup, where we have this target.
Um, but we're now gonna be taking a max over a,
over our next state and our action and our weights.
Now, notice in this equation,
all the W's you see are identical on the right-hand side.
So, we're using the same weights to represent our current value and we're using
our same weights to plug in and get an estimate of
our future value as well as in our derivative.
And whether we're gonna see is, is uh,
an alternative to that.
Okay. So, their idea was,
we'd really like to be able to use this type of function approximator.
These deep function approximators to do Atari.
They picked Atari in part.
Well, I think at least Dennis and I think, Dennis and David,
both had sort of a joint startup on,
um, video games, a long time ago.
I think it was before David went back to grad school if I remember correctly.
Um, so, they're both interest in this it's clearly,
uh, games are often hard for people to learn.
I'm so, it's a nice sort of, uh,
demonstration of intellect and they thought well,
we can get access to this and,
and there was a paper published.
I'm forgetting when, maybe 2011, 2013,
talking about Atari games and emulators as being sort of interesting challenge for RL.
So, what happens in this case, well,
the state is just gonna be the full image.
The action is gonna be the equivalent of
actions of what you could normally do in the game.
This is normally somewhere between four to 18,
approximately four to 18 actions.
Um, and the reward can be, well,
really whatever you want but you can use the score or some other aspect to,
um, uh, as proxy reward.
Generally we're gonna think about score.
So, what's gonna happen?
Well, they're gonna use a particular input state.
We've talked before about whether or not,
um, a representation is Markov.
In these games, you typically need to have velocity.
So, because you need velocity,
you need more than just the current image.
So what they chose to do is,
you'd need to use four previous frames.
So this at least allows you to catch for a velocity and position,
observe the balls and things like that.
It's not always sufficient.
Can anybody think of an example where maybe an Atari game,
I don't know how many people played Atari.
Um, uh, that might not be sufficient for
the last four images still might not be sufficient
or the type of game where it might not be sufficient.
Yeah.
[inaudible].
Microbes exactly right. So like things like Montezuma's Revenge,
things we often have to get like a key and then you have to grab that key and then, uh,
maybe it's visible on screen, it maybe sad, um,
and, er, maybe it stored in inventory somewhere.
So you have to sort of remember that you have it in order
to make the right decision much later or there might be
some information you've seen early on.
So there are a lot of games and a lot of tasks where, um,
the, even the last four frames will not give you the information you need.
But it's not about approximation,
it's much easier than representing the entire history.
So they started with that. Um, er, so, here in this case,
there's 80 joystick button positions,
um, may or may not need to use all of them in particular game.
And the reward can be the change in score.
Now notice that that can be very helpful or may not be it, depends on the game.
So in some games it takes you a really,
really long time to get to anywhere where your score can possibly change.
Um, so in that case,
you might have a really sparse reward.
In other cases, you're gonna win a reward a lot.
And so that's gonna be much easier to learn what to do.
What are the important things that they, um, did in their paper,
this is a nature paper from 2015,
is they use the same architecture and hyperparameters across all games.
Now just to be clear, they're gonna then learn
different Q functions and different policies for each game.
But their point was that they didn't have to use totally different architectures,
do totally different hyperparameter tuning for
every single game separately in order to get it to work.
It really was the sort of general, um,
architecture and setup was sufficient for them to be able
to learn to make good decisions for all of the games.
I think it was another nice contribution
to this paper is to say well we're going to try to get
sort of a general algorithm and setup that is gonna go much
beyond the sort of normal three examples that we see in reinforcement learning papers.
But just try and get to try to do well in all 50 games.
Again, each agent is gonna learn from scratch in each of the 50 games, um,
because it's gonna do so with the same basic parameters,
same hyperparameters and same neural network so same function approximators act.
And the nice thing is that, I think this is actually required by nature.
They, they released the source code as well.
So you can play around with this. So how did they do it?
Well, they're gonna do value function approximators.
So they're, they're representing the Q function.
They're going to minimize the mean squared lost by stochastic gradient descent.
Uh, but we know that this can diverge with value function approximators.
And what are the two of the problems for this?
Well one is that, uh,
there is this or the correlation between samples which means that if you have s,
a, r, s prime,
a prime, r prime, double prime.
If you think about what the value function or the return is
for us and the value function and the return for S prime, are they independent?
No, right. In fact, like you expect them to be highly correlated with their part,
you know, I mean, it depends on the probability of S prime.
If this is a deterministic system,
the only difference between them will be R. So,
so these are highly correlated,
this is not IID samples when we're doing updates,
there's a lot of correlations.
Um, and also this issue with non-stationary targets. What does that mean?
It means that when you're trying to do
your supervised learning and train your value function predictor,
um, it's not like you always have
the same v pi oracle that's telling you what the true value is.
That's changing over time because you are doing Q-learning to try to estimate what
that is and your policies changing and so it's huge amounts of non-stationarity.
So you don't have a stationary target when you're even just trying to fit
your function because it could be constantly changing at each step.
Um, so you change your po-, you change your weights,
then you change your policy and then now you're gonna change your weights again.
And so, perhaps it's not surprising that things
might be very hard in terms of convergence.
So the way that sort of, uh, DQN,
deep Q-learning addresses these is by experienced replay and fixed Q-targets.
Experienced replay, prime number if you guys have heard about this,
if you learned about DQN before is we're just gonna stroll data.
We've talked about a little bit before of how like
TD learning in their standard approach,
just uses a data point.
Now what I mean by a data point here is really one of these sar, S prime tuples.
In the simplest way of TD Learning or Q-learning,
you use that once and you throw it away.
That's great for data storage,
um, it's not so good for performance.
So the idea is that we're just gonna store this.
Uh, we're gonna keep around some finite buffer of
prior experience and we're gonna re-basically redo Q-learning updates.
Just remember a Q-learning update here would be looking like this.
We would update our weights,
that's considered one update to take a tuple and update the weight.
It's like one stochastic gradient descent update.
And so you can just sample from your experience, um, ah, replay,
your replay buffer and compute the target value given
your current Q function and then you do stochastic gradient descent.
Now notice here because your Q function will be changing over time.
Each time you do your update of the same tuple,
you might have a different target value
because your Q function has changed for that point.
So this is nice because basically it means that you reuse
your data instead of just using each data point once,
you can reuse it and that can be helpful.
And what we'll look at that more in a minute.
So, even though we're treating the target as a scalar,
the weights will get updated the next round which means
our target value changes and so, um,
you can sort of propagate this information and essentially the main idea,
is just that we're gonna use data more than once,
um, and that's often very helpful.
Yes, and, um, name first.
Um, my question is is this equivalent to keeping more frames in our,
uh, representation or is this, uh [inaudible]
It's a great question which is is this equivalent to
keeping more frames in our representation? It's not.
Though that's a really interesting question.
Um, more frames would be like keeping,
uh, a more complicated state representation.
But you can still just use
a state action or word next state tuple once and throw that data way.
This is like saying that periodically I- like let's say I went s1 a1
r1 s2 and then I keep going on and now I'm at like s3 a3 r3 s4.
So, that's really where I am in the world,
I'm now in state four.
It's like I suddenly pretend, oh wait,
I'm gonna pretend that I'm back in s1,
took a1, got r1,
and went to s2 and I'm gonna update my weights again,
and the reason that that update will be different than before is
because I've now updated using my second update and my third update.
So, my Q function,
in general, will be different than before.
So, it'll cause a different weight update.
So, even though it's the same data point as before,
it's gonna cause a different weight update.
In general, one thing we talked about a long time ago is that if you, um, uh,
do TD learning to converge it,
which means that you go over your data mu- like, um, an infinite amount of time.
Um, at least in the tabular case,
that is equivalent to if you learned an MDP model.
You learned the transition dynamics in
the reward model and you just did MDP planning with that.
That's what TD learning converges to,
is that if you repeatedly go through your data in infinite amount of time,
eventually it will converge to as if you'll learn to model,
a dynamics model, a word model,
and then the planning for that which is pretty cool.
So, this is getting us closer to that.
But we don't wanna do that all the time because there's
a computation trade-off and particularly here because we're in games.
Um, there's a direct trade-off between computation and getting more experience.
It's actually a really interesting trade-off
because in these cases it's sort of like should you
think more and plan more and use your old data or should you just gather more experience?
Um, but we can talk more about that later.
Yeah, question and name first please.
And so, the experienced replay buffer has like a fixed size.
Just for, like, clarification of understanding,
are those samples, like,
replaced by new samples after fixed amount of time?
Or is there, like, a specific way to choose what samples to store in the buffer?
That's a great question which is, okay,
this is presumably gonna be a fixed size buffer.
Um, and if it's a fixed size buffer, how do you pick what's in it?
Um, is it the most recent and- and how do you get things,
uh, how do you remove items from it.
It's a really interesting question.
Different people do different things.
Normally, it's often the most recent buffer, um,
can be for example the last one million samples,
which gives you a highlight of how many samples we are gonna be talking about.
But you can make different choices and there's
interesting questions of what thing that you should kick out.
Um, it also depends if your problem is really non-stationary or not,
and I want to mean there's, like,
the real world is non-stationary,
like your customer base is changing. Yeah?
Uh, I'm trying to strike the right balance between
continuing experience like new data points versus re-flagging it.
Can we use something similar to like exploitation versus exploration.
Um, essentially like with random probability just decide to re-flag [inaudible].
The question is about how would we choose between,
like, what, um, you know,
getting new data and how much to replay et cetera, um,
and could we do that sort of as an exploration-exploitation trade-off.
I think this is generally understudied but
there's lots of different heuristics people use.
Often people have some of- sort of a fixed ratio of how much
they're updating based on the experience replay versus getting,
um, putting new samples into there.
So, generally right now is really heuristic trade-off.
Could certainly imagine trying to optimally figure this
out but that also requires computation.
Um, this gets us into the really interesting question
of metacomputation and metacognition.
Um, but if, you know, your agent thinking about how to prioritize
its own computation which is a super cool problem.
Which is what we solve all the time.
Okay. So, um, the second thing that
DQN does is it first have- it first keeps route this old data.
The second thing that it does is it has fixed Q targets.
So, what does that mean? Um, so to improve stability,
and what we mean by stability here is that we don't want our weights to explode and go to
infinity which we saw could happen in linear value function.
Um, we're gonna fix the target weights that are used in
the target calculation for multiple updates.
So, remember here what I mean by the target calculation here is
that reward plus Gamma V of S prime.
So, this itself is a function of w and we're gonna
fix the w we use in that value of S prime for several rounds.
So, instead of always update- taking whatever the most recent one is,
we're just gonna fix it for awhile and that's basically like making this more stable.
Because this, in general,
is an approximation of the oracle of V star.
So, you'd really like an oracle to just give this to you every time you reach, you know,
an S prime or you take an action [inaudible] and go to S prime,
you'd like an oracle to give you what the true value is.
You don't have that, um,
and it could change on every single step because you could be updating the weights.
What this is saying is, don't do that,
keep the weights fixed that used to compute VS prime for a little while,
maybe for 10 steps,
maybe for a 100 steps, um,
and that just makes the target,
the sort of the thing that you're trying to minimize
your loss with respect to, more stable.
So, we're gonna have, um, we still have
our single network but we're just gonna maintain
two different sets of weights for that network.
Um, one is gonna be this weight minus.
I'll call it minus because, um,
well there might be other conventions but in particular it's the older set of weights,
the ones we're not updating right now.
Those are the ones that we're using them as target calculation.
So, those are the ones we're gonna use when we want to figure out the value of S
prime and then we have some other W which is what we're using to update.
So, when we compute our target value we, again,
can sample and experience tuple from the dataset from our experience replay buffer,
compute the target value using our w minus,
and then we use stochastic gradient descent to update the network weights.
So, this is used with minus this is used with the current one. Yeah?
So, uh, I guess two questions like intuitively,
why does this is help and, like,
why does it make it more stable and, like, secondly, like,
are there any other benefits on the stability from doing this?
These are two questions, one is,
intuitively, why does this help?
Um, which is a great question and second of all,
beyond the stability, is there any other benefits?
So, intuitively, why does this help,
um, in terms of stability?
In terms of stability, it helps because you're
basically reducing the noise in your target.
If you think back to Monte Carlo, um,
there instead of using this target like this bootstrap target where we're using GT.
So, in Monte Carlo, we used GT and I told you the nice thing
about that was that it was an unbiased estimator of V pie.
But the downside was that it was high variance
because you're summing up the rewards till the end of the episode.
Um, and so if things are high-variance when you're trying to regress on them,
it's gonna be more noisy,
um, and you could [inaudible] gradients.
Imagine that we do something- take the extreme of this, if we want to be,
um, for stability, you could always make your target equal to a constant.
You could always make it equal to zero for example,
and if you kept your target fixed forever,
you would learn the weights that- that minimize the error to
a constant function and that would then be stable because you always
have the same target value that you're always trying to predict and
eventually you'd learn that you should just set
your w equal to zero and- and that would be fine.
So, this is just reducing the noise and the target that we're trying to sort of,
um, if you think of this as a supervised learning problem,
we have an input x and output y.
The challenge in RL is that our y is changing,
if you make it that you're- so your y is not changing, it's much easier to fit.
Um, unless I convince whether or not there's, uh,
any benefit beyond stability.
I think mostly not, um,
I- the- this is also sort of reducing how quickly you propagate
information because you're using
an- a stale set of weights to represent the value of a state.
So, you might misestimate the value of a state because you haven't updated it
with holding permit- with new information. Yeah?
Uh, assuming we want to do [inaudible] approximator.
Is there something that's specific to the deep neural networks?.
That's a great questions which is,
is this specific to deep neural networks or can we use
this with linear value function approximate or any value par-,
you can use those to any value function approximators.
Yeah, this is not specific.
This is really just about stability and
that's- that's true for the experience replay too.
Experience replay is just kinda propagate information more- more effectively and,
um, this is just gonna make it more stable.
Uh, so these aren't sort of unique using deep neural network.
I think they were just more worried about the stability with
these really complicated function approximators. Yeah, in the red.
Do you every update the-
Minus at all, or is that [inaudible].
Great question. So, the- Di- Dell?
Dian.
Dian. Dian's question is whether or not,
um, we ever update w minus, yes we do.
We pu- can periodically update w minus as well.
So, in a fixed schedule,
say every 50 or, you know,
every n episodes or every n steps,
you, um, update sort of like every- and you would set w minus dw. Yeah?
I was thinking, like,
given that we know that this is work for
gradient descent and you're not using the same kind of structure as gradient descent,
you're using to create, you know,
different option subtracting the [inaudible] value of- of the function.
How is this supposed to, like,
not grade- grade- gradient descent,
like, all those assumptions from [inaudible]
Your question is, okay this- does this really work in terms of gradient descent?
This is not- I mean,
the- it's a great question,
and these sort of Q learning are not true gradient descent methods.
They're- they're are approximations to such,
they often do shockingly well given that.
Some of the more recent ones which,
um, uh, Chapter 11 has a nice discussion of this,
sort of the GTD's or
gradient temporal difference learning are more true gradient descent algorithms.
These are really just approximations and it's,
uh, this, um, uh,
as to the point, this has no guarantees of convergence still.
This is hopefully gonna help but we have no guarantees. Yeah?
Uh, George, uh, so in practice,
do people have some cyclical pattern and how can they refresh
the- the gradient that's used to compute, uh, the gradients?
Yeah, his question is, um, you know,
in practice are there some sort of cyclical pattern of how often you update w minus.
Yes, yeah there's often particular patterns or- or hyper-
it's a hyperparameter choice of how quickly and how frequently you update this.
Um, and it will trade-off between propagating information fester,
um, and possibly being less stable.
So. If you make, um, you know,
if n here is one that you're back to standard TD learning.
If n is infinity, that means you've never updated it.
Um, so, there's a- there's a smooth continuum there. William?
Uh, we notice, like, for w,
there are better initializations than just like zero,
uh, something, like, if you take into account,
I guess like the mean and variance.
Uh, would you initialize w minus just two
w or is there like an even better initialization for w minus?
Yeah, his questions is about, you know,
the- the impact of how we,
um, uh, initialize w ca- can matter.
Um, uh, and is the- how do we initialize w minus.
Typically, we initialize w minus to be exactly the same as w at the beginning.
Um, the choice of it will also affect, uh,
certainly the early performance. Those are great questions.
Let me keep going because I wanna make sure we get to some of the extensions as well.
Um, so just to summarize how DQN works.
Um, the main two innovations that data- it uses experienced replay and fixed Q targets.
It stores the transition in this sort of replay buffer,
a replay memory, um,
use sample random mini-batches from D. So,
normally sample in mini-batch instead of a single one.
So, maybe a sample 1- or whatever other parameter.
You do your gradient descent given those.
Um, you compute Q learning using these old targets and you
optimize the mean squared error between the Q network and Q learning targets,
use stochastic gradient descent,
and something I did not mention on here is that we're
typically doing E-greedy exploration.
So, you need some schedule here too for how to do E-greedy.
So, they were not doing, um,
sophisticated exploration in their original paper.
So, this is what it looks like.
You sort of go in and you do multiple different convolutions.
They have the images, um,
and they do some fully connected layers and then the output a Q value for each action.
Let me just bring it up.
Um, for those of you who haven't seen it before.
So, the nice thing is what they- so,
you're about to see breakout which is, um,
an Atari game and what they do is they show you
sort of the performance of what the agent is doing.
So, remember the agent's just learning from pixels here how to do this.
So, it was pretty extraordinary when they showed this in about 2014.
Um, and the beginning of its learning sort of this policy.
You can see it's not making- doing the right thing very much, um,
and that over time as it gets more episodes it starting
to learn to make better decisions about how to do it.
Um, and one of the interesting things about it is that as you'd hope,
as it gets more and more data,
it learns to make better decisions.
But one of the things people like about this a lot is that,
uh, you can learn to exploit,
um, the reward function.
Uh, so in this case,
um, it figures out that if you really just want me to maximize the expected reward,
what the best thing for me to do is to just kind of
get a hole through there and then as soon as I
can start to just bounce around the top [inaudible].
Um, and so this is one of the things where, you know,
if you ask the agent to maximize the reward,
it'll- it'll learn the right way to maximize the reward given enough data.
Um, and so this is really cool that sort of it could discover things that maybe
are strategies that people take
a little while to learn when they're first learning the game as well.
So, when they did this,
they then showed, um,
some pretty amazing performance on a lot of different games.
Many games they could do as well as humans.
Now, to be precise here- oh yeah
I'm sorry.
Uh, I'm just wondering why, um,
it's playing, like, why was it [inaudible] around a lot, like,
it wasn't sure of its movements like it moved
around places often, like [OVERLAPPING]. Yeah.
Yeah, so, um, you might see, uh,
I think she is talking- she is referring to the fact that the paddle was moving a lot.
As the agent is trying to learn, like,
when we see that, you sort of go,
"Why would he jerk a lot."
From the agent's perspective, particularly if there's a cost to moving,
then it may just be kind of babbling, uh,
and doing exploration just to see what works and- and it-
from our perspective that's clearly sort of an inexperienced player to do that.
That would be a strange thing but from
the agent's perspective, that's completely reasonable.
Um, and it does not give him positive or negative reward from that.
So, it can't distinguish between, you know,
stay in stationary versus going left or right.
If you put it in a cost for movement that could help. Yeah?
This might become a little bit of [inaudible] but is there a reason to introduce a pulling layer?
Puling layer? There might be one in there.
I- the- the- I don't remember the complete arc- network architecture, um.
The question is whether or not there's a pulling layer in there.
I think there prob- there might be inside.
There has- they have to be going from images all the way up.
But they have the complete architecture. It's a good question.
So, the next thing that you can see here is that, um,
they got sort of human level performance on a number of different Atari games.
There's about 50 games up here.
Um, just to be clear here.
When they say human level performance that means asymptotically.
So, after they have trained their agent, this-, uh,
they're not talking about how long it took them or
their agent to learn and as you guys will find out for homework two,
it can be a lot of experience.
Um, uh, a lot of time to learn how to make- do a good performance.
But nevertheless, there are a lot of cases where that might
be reasonable in terms of games.
So, they did very well on some domains.
Some domains, they did very poorly.
Um, there's been a lot of interest in these sort of games on the bottom end of
the tail which often known as those hard exploration games.
We'll probably talk- uh, we'll talk a lot more about exploration later on in the course.
So, what was critical?
So, I- I like the, uh, there's a lot of really lovely things about
this paper and one of the really nice things is that they did a nice ablation study,
um, uh, for us to sort of understand what were
the important features and if you look at these numbers.
Um, I think that it's clear that the really important feature is replay.
So, this is their performance using a linear network,
deeply network seemed to not help so much.
Using that fixed Q. Um,
fixed Q here means you seem like a fixed target.
Okay, that gets you a little bit three from ten.
You do replay and suddenly you're at 241.
Okay, so throwing away each data point what-
after you use it once is not a very good thing to do.
You want to reuse that data.
Um, and then if you combine replay and
fixed Q you do get an improvement over that but, uh,
it's really that you get this huge increase,
um, uh, at least in break out in some of the other games by doing replay.
Now, in some other ones, um,
you start to get a significant improvement as soon as you
use a more complicated function approximators.
But in general, replay is hugely important
and it just gives us a much better way to use the data. Yeah?
Um, you know, because here in this table it seems like you'd want to use replay and
fixed Q with the linear model and that it might be a mistake to be using,
uh, a deep model here.
Do you agree with that table reference table or?
So, the question is like, "Well, maybe we could use,
like, linear-" also I guess I should be clear.
So, this is all- everything on the- the next four were all deep.
So, they don't have here linear plus replay.
But you could certainly imagine trying linear plus
replay and it seems like you might do very well here,
it might depend on which features you're using.
There's some cool work, um, uh,
over the last few years looking also at,
uh, whether you can combine these two.
So, we've done some work, um,
using a Bayesian last layer,
using like Bayesian linear regression which is useful for uncertainty.
Other people have just done linear regression where the idea is you- you sort of, um, uh,
deep neural network up to a certain point and then you do,
um, kind of direct linear regression
to fit exactly what the weights are at the final layer.
So, that can be much more efficient,
um, but you still have a complicated representation.
All right. So, since then,
there's been a huge number amount of interest in this area.
Um, ah, so, again,
dating myself reinforcement learning,
we used to go and give a talk about reinforcement learning,
and like 40 people would show up,
but most of them you knew, and,
um, and then, uh,
and then it started really changing.
I think I was maybe in 2016, when, um, er,
ICML, I was in New York and like suddenly
there were 400 people in the room for reinforcement learning talks.
Um, and then, this year at NLP's which is one of the major machine-learning conferences,
it's sold out in like eight minutes.
Um, so, there were 8,000 people there,
and there was a huge amount of interest in deep learning,
and for the deep learning workshop,
you sort of have a 2,000 person auditorium.
So, there's been a huge amount of excitement based on this work,
which I think really a huge credit to- to
deep mind and to the work that David Silver and others have been doing,
um, to sort of show that this was possible.
Uh, some of the immediate improvements that we're
going to go through really quickly here is,
um, Doubled DQN, prioritize replay, and dueling DQN.
Um, there's been way,
way, way more papers in that,
but these are some of the early really big improvements on top of DQN.
So, double DQN is kind of like double Q learning,
which we covered very briefly at the end of a couple of classes ago.
The thing that we discussed there was this sort of maximization bias, is that,
um, the max of estimated state action values can be a biased estimator of the true max.
So, we talked really briefly about double Q learning.
Um, so, a double Q learning,
the idea was that we are going to maintain two different Q networks.
Uh, we can select our action using,
like an E Greedy Policy where we average between those Q networks,
and then we'll observe a reward in a state and we basically
use one of the Qs as the target for the other.
So, if, you know,
with a 50 percent probability,
we're going to update one network,
and we're going to do that by using picking the action from the other network.
This is to try to separate how we pick our action
versus our estimate of the value of
that action to deal with this sort of maximization bias issue.
Then with 50 percent other probability,
we update Q2, and we pick the next action from the other network.
So, this is a pretty small change,
it means you have to- you have to maintain
two different networks or two different sets of weights,
um, and it can be pretty helpful.
So, um, if you extend this idea to DQN,
you have sort of our current Q network,
w select actions, and this older one to evaluate actions.
So, you can put this in there to do action selection,
and then you can evaluate the value of it with your
other networks- other, other network weights.
So, it's a fairly small change,
it's very similar to what we were doing already for the target network, network weights.
It turns out that it gives you a huge benefit in many,
many cases for the Atari games.
So, uh, this is something that's generally very useful to do, um,
and gives you sort- of sort of immediate significant boost in performance,
sort of, you know, for the equivalent of like a small amount of coding.
That's one idea, and that's sort of a direct lift up from sort of,
you know, double Q learning.
The second thing is prioritized replay.
So, let's go back to the Mars Rover example.
Um, er, so, in Mars Rover we had this really small domain,
we are talking about tabular setting through just seven states, um,
and we're talking about a policy that just
always took action a1 which turned out to mostly go left.
So, we had this trajectory,
we started off in state s3,
we took action a1,
we got rewarded zero,
we went to s2, we stayed in s2 for one round when we did a1,
and then eventually went to s1,
and then we terminated.
So, it was this. And the first visit Monte Carlo estimate
of v for every state was 1110000,
and the TD estimate with alpha equal one was this.
That was when we talked about the fact that TD only uses
each data point once and it didn't propagate the information back.
So, the only update for TD learning was when we reached state s1,
we took action one, we got a reward of one, and then we terminated.
So, we only updated the value of state one.
So, now let's imagine that you get to do-
now let's think about what your- like your replay back up would be in this case.
You'd have something like this, you'd have s3, a1,
0, s2, s2, a1,
0 s2, s2, a1, 0,
s1, s1, a1, 1, terminate.
That's what your replay back up would look like.
So, let's say you get to choose two replay backups to do.
So, you have four possible replay backups,
you can pick the same one twice, if you want to,
and I'm going to ask you to pick to replay backups to
do to improve the value function, in some way.
Um, and I'd like you to think for a
second or talk to your neighbor about which of the two you should pick,
and why, and which order you do them in as well,
and whether it makes any difference.
Maybe it doesn't matter if you can just pick any of these,
you're going to get the same value function no matter what you do.
So, are there two- two updates that are particularly good,
and if so, why and what order would you do them in?
[NOISE]
Hopefully you had a chance to think about that for a second.
First of all, does it matter?
So, I'm going to first ask you guys, uh, the question.
Vote if you think it matters which ones you pick,
in terms of the value function you get out. That's right.
So, it absolutely matters which two you pick in terms of the resulting value function,
you will not get the same value function no matter which two you pick.
Um, uh, now as for another voting.
So, I will ask for which one we should do first?
Should we update- should we do four first?
Four is the last one on our replay buffer.
Should we do three first?
Should we do two first? Okay. All right.
So, 3s have it, does somebody want to explain why? Yeah.
I think.
You've got to back-propagate from
the information you're already [NOISE] have on step one to step two.
Right.
Yeah. So what the student said is right.
So, if you pick, um,
backup three, so what's backup three?
It is, S2, A1, 0, S1.
So if you do the backup, that's, zero,
plus gamma V of S prime,
S1. And this is one.
So that means now you're gonna get to backup
and so now your V of S2 is gonna be equal to one.
So you get to backup that information.
Yeah. So I, I wasn't extremely specific on what like,
the right thing to do here is, that,
that the- um, that my main thing is that
I wanted to emphasize that it makes the big difference and that,
and that, um, it's gonna matter in terms of order.
What's the next one we should do?
Should we do, raise your hand if we should do, three again.
Raise your hand if we should do two.
Raise your hand if we should do one.
Yeah. The ones have it. I- someone want to explain why? Yeah, in the back.
And that's the same as the last time I [inaudible]
That's right. Yes. So, um,
if you wanted to get all the way to the Monte Carlo estimate.
What you would wanna do here,
is you'd wanna do S3, a1,
0, S2 which would allow your V of S3 to be updated to one.
And at this point your value function will be exactly the same as the Monte Carlo.
So it definitely matters.
It matters the order in which you did, do it.
If you had done S3, a1, 0,
S2, your S3 wouldn't have changed.
Um, so ordering can make a big difference.
Uh, so not only do we wanna think about like, what, um,
was being brought up before but I think
to say like what should we be putting in our replay buffer,
not only do we wanna think about what- should be in
a replay buffer but also what order do
we sampled them can make a big difference in terms of convergence rates.
Um, uh, and in particular,
there's some really cool work from a couple of years
ago looking at this formally of like how,
at what the ordering, matters.
Um, so, there is this paper back in
2016 that tried to look at what the optimal order would be.
So imagine that you had an oracle that could, um, exactly compute.
Now, this is gonna be computationally intractable,
we're not gonna be able to do this in general,
but imagine that the oracle could go through and pick
and figure out exactly what the right order is.
Um, then what they found out in this case is that,
for this or a small chain like example,
um, you'd get this exponential improvement in convergence,
which is pretty awesome. So what does that mean?
The number, of, um,
updates you need to do until your value function converges to the right thing.
It can be exponentially smaller,
if you update carefully and you,
you could have an oracle tells you exactly what tuple the sample.
Which is super cool. Um, so you can be much much better.
But you can't do that. You're not gonna spend all this.
It- it's very computationally,
expensive or impossible in some cases to figure out exactly what that uh,
that oracle ordering should be.
Um, but it does illustrate that we,
we might wanna be careful about the order that we do it and- so, their,
intuition, for this, was,
let's try to prioritize a tuple for replay according to its DQN error.
So, the DQN error,
um, in this case is just our TD Learning error.
So it's gonna be the difference between, our current.
This is basically our prediction error.
So this is our prediction error [NOISE],
Almost our prediction error,
I'll just call it TD,
because it's not quite because we were doing the max.
So this is like, sort of our predicted,
TD error minus our current.
Let us say, if you have a really really big error,
that we're gonna prioritize, updating that more.
And you update this quiet quantity at every update,
you set it for new tuples to be zero and one method- they have two different methods for,
for trying to do this sort of prioritization.
That one method basically takes these, um, priorities,
raises them to some power alpha, um,
and then normalizes And then that's the probability,
of selecting that tuple.
So you prioritize more things that are weights. Yeah.
Doesn't freezing.
Name first, please.
Oh, Sorry. Doesn't freezing in the old ways
were a counter to propagating back the information there?
It's like, you first the old ways and uh,
example we're just going through,
after you like propagated the one back once,
you wouldn't be able to do anymore because your value's totally zero
It's a great point, which is,
if you are fixing,
um, uh, your w minus, then,
if you were looking at our case that we had before,
then you wouldn't be able to continue propagating that back,
because you wouldn't update yet, yet, that's exactly right.
So there's gonna be this tension between,
when you fix things versus her propagating information back.
Um, I, and, it's a tention that one has to sort of figure out,
there's not necessarily principled ways for,
exactly what the right schedule is to do that,
but it's a hyperparameter to do.
So why does it, what does ordering matter,
that if you're fixing,
and so you are not changing,
uh, like, then it wouldn't matter what order we sampled those previous ones, right?
Uh, okay. So basically, ordering matter at all, in that case.
It still matters because we're still gonna be doing replay, o- over,
uh, the weights will be changing during
the time period of which will be replayed over that buffer.
So that buffer could be like,
million and you might re-update your weights like every 50 steps or something like that.
So there's still gonna be a whole bunch of data points in,
uh, in your replay buffer,
that it's useful to think about,
now that your weights have changed,
what ordering do you wanna go through those? It's a great question.
Okay. So what method is this?
Lemme just, just to clarify, if we set, um,
alpha equal to zero,
what's the rule for selecting among the existing tuples?
So out- so Pi is our, uh,
sort of basically our DQN error.
If we set Alpha equal to zero, you know, it's right.
Yeah. So, so this sort of trades off between uniform,
no prioritization to completely picking the one that,
um, like if alpha's infinity then that's
gonna be picking the one with the highest DQN error.
So it's a trade-off, it's a stochastic. All right.
So, um, then, they combine this with,
sort of- the reason why I'm picking these three
[inaudible] they are sort of layer on top of each other.
So prioritise replay versus, um,
I think this is prioritise replay plus D, um,
double DQN versus just double DQN.
Most of the time, um, this is zero would be,
they're both the same underneath means flat,
uh, vanilla DQ, double DQN is better.
Above means that prioritize replay is better.
Most, the time prioritize replay is better
and there's some hyper parameters here to
play with but most of the time it's, it's useful.
And it's certainly useful to think about,
you know, we're order might matter.
All right. We don't have very much time left so I'm just gonna do,
short through this just so you're aware of it.
Um, one of the best papers from ICML 2016 was dueling.
Um, the idea is that,
if you want to, make decisions in the world,
they're working some states that are better or worse,
um, and they're just gonna have higher value or lower value,
but that- what you really wanna be able to do is,
figure out what the right action is to do, in a particular state.
Um, and so that- what you want us to have understand is,
this, this advantage function.
You wanna know, how much better or worse taking
a particular action is versus following the current policy.
That really like I don't care about estimating the value of a state,
i care about being able to understand which of the actions has the better value.
So I'm looking at this advantage function.
So, what they do to do this is that in contrast to DQN,
where you output all of the Q's.They're gonna
separate and they're gonna first estimate the value of a state,
and they're gonna estimate this advantage function,
which is Q of s. One minus V of s,
Q of s, a2 minus V of s. [inaudible] just gonna separate it.
It's an architectural choice and learning a recombine these for the Q.
And I get this is gonna help us refocus on the signal that we care about which is,
um, you know,after accurately estimate which action is better or worse.
Um, there's intruding questions about whether or not this is identifiable,
I don't have enough time to go into these today.
It is not identifiable.
I'm happy to talk about all of that off light, um,
uh, the, the reason this is, uh, important is,
they just forces one to make some sort of default assumptions about,
um, specifying the appendage functions.
Empirically, it's often super helpful.
So, again compared to double DQN with prioritize replay,
which we just saw, which was already better than w- double DQN,
which is also better than DQN.
Um, this again gives you another performance gain, substantial one.
So basically these are sort of threes,
three different ones that came up within the- for two years after DQN that started
making some really big big performance gains compared to destroy completely vanilla DQN.
For homework two, you're gonna be implementing DQ and not the other ones,
they welcome to implement some of the other ones.
They just good to be aware of- those are some and sort of
the major initial improvements to giving it substantially better performance on ATARI.
Um, I'll leave this up. We're almost out of time.
Uh, feel free to look at the last couple slides of this
for some practical [NOISE] tips that came from John Schulman,
um John Schulman was a PhD student at Berkeley,
that is now of the heads of open AI.
Um, I- just one thing that I will make sure to highlight,
it could be super tempting to try to start,
by like implementing Q learning directly on the ATARI.
Highly encourage you to first go through,
sort of the order of the assignment and like,
do the linear case,
make sure your Q learning is totally working,
um, before you deploy on ATARI.
Even with the smaller games,
like Pong which we're working on,
um, it is enormously time consuming.
Um,and so in terms of just understanding and deep again,
it's way better to make sure that you know your Q Learning method is working,
before you wait, 12 hours to see whether or not,
oh it didn't learn anything on Pogge.
So, that, that- there's a reason for why we,
sort of build up, the way we do in the assignment.
Um, another practical, to a few other practical tips, feel free to,
to look at those, um,
and then we were on Thursday.
Thanks.
 All right. So, homework two,
you guys are probably starting to work on,
and we're having sessions this week that
are good for if you don't have background in deep learning,
and feel free to reach out on Piazza. Oh, yeah,
I just have a question about the project.
I just want to make sure,
it seemed currently with the note on Piazza that like, I-50 was the default suggested one.
Can we also do something outside of that?
Oh, yeah, no, question is a great one.
Yeah, there's a, the post on Piazza,
you're always welcome to design your own project.
That's always completely fine,
and a number of you have come talk to me about those, or talked to other TAs.
These are an additional option.
So, if people are interested in looking at either
the default project which we released yesterday,
which has to do with bandits and warfarin,
or if you want to look at some of the suggestions from senior PhD students or postdocs,
those are great opportunities.
Particularly, I think if you haven't ever done reinforcement learning before,
it's often I wouldn't expect at three weeks in
that you'd be able to define a state of the art project.
So, if you're interested in learning more about RL research,
then it can be a really great opportunity to look at some of
those suggested projects then reach out to people.
All right.
The other thing that I just wanted to do a friendly reminder
about is we explicitly post FAQs for each of the homeworks.
Um, and as some of the TAs are mentioning that some of
the students coming into office hours right now
might not have had a chance to look at those.
So, if you ever have a question when you're going over the homework,
the first thing to do is to go to Piazza and
particularly to look at those pinned notes at the
very top which have very common FAQs about the assignment.
So, make sure to read those before you go to office hours,
and then, of course, feel free to come to office hours as well.
But those are a really good resource to look at.
Any other questions? All right,
so just in terms of where we are in the course right now,
we went through DQN on Monday.
We're gonna talk today some a bit more about,
we can wrap up some of the stuff that I had to rush through at the end of Monday in terms
of deep Q-learning and some of the recent extensions.
Then we're gonna talk some about imitation learning and
large state spaces before next week starting to talk about policy gradient methods.
So just to, we'll start off with sort of a refresher from what DQN was doing,
DQN was this idea of combining between
Q-learning and using deep neural networks as function approximators.
And the two key sort of algorithmic changes compared to prior work was that,
they used experience replay and fixed Q targets.
And by fixed Q targets there that was meaning that when we used our r,
rt plus gamma, max over a,
Q of sta, st plus one, right.
That the weights that were used for that Q representation were fixed for a while.
So maybe we'd update those every 100 steps or every 50 episodes or some interval.
And, uh, so this provided a more stable target for
supervised learning because the supervised learning part
again is that we were had this combination of,
of we want to have weights and we want to
minimize this error versus our current estimate,
sort of minimizing the TD error.
So the way that this preceded is that we'd restore transition in a replay memory buffer.
We do mini batches, where we would sample a bunch of state extra word and next state tuples and
then do these backups where we're sort of
updating our Q function and refitting our Q function.
Um, and like a lot of the linear value function methods we saw before,
it uses stochastic gradient descent.
And the really cool thing about this is that they did it on 50 games.
They used the same architecture for those 50 games and
the same hyper parameters and they got human level performance.
So we've talked quite a lot about that before.
And then we sort of briefly talked about
three sort of major extensions to that in the immediate following years.
And again, there's been a lot of
extensions and a lot of work in deep reinforcement learning right now.
The three of them were as follows.
The first was Double DQN.
And we talked before we got it,
the function approximation talking about the issue with maximization bias,
that when you're using the same representation
to pick an action and estimate the value of that action,
you can get into a maximization bias problem.
And the way that that's avoided in
Double DQN and I wanted to go
over this again because I had a couple questions after class.
We didn't have much time to discuss it.
Is what happens is we have a current queue network which is
parameterized by a set of weights and that is what is used to select actions.
Just to be clear here,
often we're doing some sort of E-greedy method.
So we'd used the current Q-network weights to decide on the best action.
And we would pick that with one minus epsilon probability.
And then there's an older Q-network that is used to evaluate those actions.
So if we look at how we're gonna be changing our weights,
we're gonna be having an action evaluation using these other weights,
w minus and then action selection using W. So when you look at this,
that might start to look pretty similar to what DQN was doing
because DQN was saying we're gonna use a fixed set of weights for,
for these target updates.
So what DQN was doing was this,
r plus Gamma Q.
I'll write the max in, max a,
Q of s prime, a, w minus,
minus the current s. So in the normal DQN,
they were also using a w minus.
But here in a Double DQN,
it can be a little bit different.
And the reason it's a little bit different than what we just
saw is that you can maintain two sets of
weights at all times and you can flip between them on every step or every batch.
So when DQN was introduced,
it was more of an idea of you fix your weights.
Let's say, from time step t to time step t plus 100,
use the same weights that whole time period for your target.
In Double DQN, you don't necessarily have to do that.
You can flip back and forth between these which is what we'd seen
with a double Q-learning that on,
you know, on step one you can use weights one to act and weights two to evaluate.
On step two, you could do weights two to evaluate and weights one to act.
So it means that you can propagate information faster.
So instead of waiting 50 episodes or 100 episodes to update,
um, the weights that you're using for your target,
so again this is your target,
you can flip back and forth between them
which allows you to update both networks a lot have,
update both set of net- network weights.
The networks are identical. Yeah.
Um, in general, when you're evaluating
these kinds of different approaches to improve these techniques,
is there, is there a trade off between
how fast information propagates and then how unstable it is?
So we might find that if the system we're trying to learn on is itself,
relatively well-behaved and stable,
we want to pick something that has faster information propagation but if it's
highly noisy or unstable that we need to do something that's more conservative.
uh, makes a good question which is, you know,
is there generally a trade-off in terms of these methods between sort of
characterizing the stability of the system and
then how fast you can propagate information back?
Unfortunately, I feel like it's not very well characterized.
So I feel like most of the time,
these are heuristics and people evaluate them,
they evaluate them with a lot of different benchmarks
and that's sort of the way we get generalization.
But I don't think that there's a good characterization
systematically of how to characterize the stability of the system,
with these deep neural networks,
particularly in the context of RL.
So there's a lot of great opportunities for
theoretical analysis here too or just sort of more formal understanding.
Right now, I think we're at the level of saying this either just
seems to consistently work a bunch across
Atari games and maybe MuJoCo or it doesn't try to characterize the, the successes.
Yeah. Is it
Yes. I was wondering if we kind of get a bit more about the switching then.
You're representing like why, how.
Yeah. So, question is about, you know,
how can we switch between these w and w minus,
and how would, you know,
um, why and how would you do this?
So, in the DQN setting,
um, you could set w minus.
So at the beginning, w minus is equal to w on time step zero.
And then, in DQN you would keep w minus to be the same maybe for the next 50 episodes,
but you'd be updating w. And then 50 episodes in,
you would update w minus.
The downside about that which we talked a little bit about before is that
you're not using the information you're getting to update this estimate.
Okay. Because you're using that old stale set of w's.
So essentially, you're just not using the information you've got over
those 50 episodes to update what would
happen if you were caught in S prime and then took action a.
So, an alternative would be to flip between,
let's say, instead of thinking of this as w and w minus, then you can think of it that way.
You can just think of maintaining two different sets of weights.
And imagine, um, I'll say,
this is t time equals one,
time equals two, time equals three.
So, imagine that we're just picking between what are the weights that we used
to select an action and the weights that we use to evaluate the action.
So, on the first time step,
you could use this to evaluate and this to the- to select the action,
and then you could flip it back and forth.
So that essentially means that,
both sets of weights are getting updated very frequently.
So, instead of updating only one of the- one of them every 50 episodes,
you're- you're continuing to propagate that information back quickly.
And there's of course tons of chart choices here about how frequently do you update,
you know, when do you switch back and forth between these.
Um, you can think of all of those as hyper-parameters you can imagine tuning.
But this is instead of keeping that- keeping
this target fixed for 50 steps, um, or, you know,
n steps these are all parameters,
you could flip back and forth between them which is what
double Q-learning did before. Yeah, .
Like in the normal DQN settings when we're using a target weight, uh,
wouldn't that target weight like for action selection or
for like evaluation still be another queuing network.
So, how is that was different from double DQN
except for the fact that you're searching double-Q more often?
It is. Or more of the- what question is,
how different is this from the previous year? It's almost identical.
So, I think the- the main difference here is that you could switch,
uh, as long as you're maintaining some set of weights for your target.
This is saying you could sort of switch.
Now you really just have- you've the same network,
two sets of weights that you have to maintain in memory.
And what this is saying is,
you can switch back and forth with those very frequently,
um, and help avoid the maximization bias during that time.
It doesn't always work, it frequently helps.
There is still the issue with stability, um,
but it can be better and it avoids the maximization bias.
We also talked about prioritized experience replay.
Um, we went through a small sort of
tabular example where we looked at the impact of doing backups.
So, if we have this experience replay buffer of SAR S-prime tuples,
which one should we use to do our backups and how do we propagate that information back?
Um, and the- in this algorithm,
they say the- or in this paper,
they talked about the fact, um,
if you can do this optimally.
In some cases you might get an exponential speedup and convergence,
uh, but it's hard to do that, it's computationally intensive.
So, what they proposed here is to prioritize something
based on the size of sort of the DQN error.
The difference between the current estimate of it
and your sort of target estimate that you're looking at.
And so, we talked about how you could use that as a priority, um,
and it could be a stochastic priority,
uh, to try to select items.
And we also talked about the fact that if you set Alpha equal to zero,
this becomes uniform and so then,
there's no particular prioritization over your tuples.
Another thing that we had almost no time to talk about was dueling.
So, dueling was a Best Paper, um, from,
uh, two- 2016, um, in ICML.
Um, let me just give a little bit of
a refresher on this because we went through it very, very fast.
So, the- the intuition here
is that the features that you might need to write down the value of
a state might be different than those need to
specify the relative benefit of different actions in that state.
And you want to understand the relative benefit of actions in order to decide what,
what your policy should be.
So, um, looking at things like game score,
it's obviously very relevant to the value.
Um, that it might be- you might want
other features try to decide what actions to do right now in a game.
And so, the advantage function that came up,
uh, that was designed by Baird a long time ago.
And this is the same Baird that had that counter example to
show why value function approximation can be bad.
Um, so, uh, Baird's work before it said,
well, look you can decompose, um,
if you think of your Q function which is representing the value of a policy starting in
state and taking a particular action versus the value of just that state.
So, this is sort of implicitly Q pi,
S pi of S. So,
like what is the difference between- difference between taking
this particular action versus just following your policy from the current state?
And he called this the advantage.
What's the advantage of that action for that state?
So, in dueling DQN,
instead of having one network that just predicts Q functions,
they use an architecture that separates into predicting
values and predicting these advantage functions and then
adds them back together with the idea being that you might get
different sort of features here and here.
So, you have to decouple for a little bit to make sure that you're capturing
the features that are relevant to capturing
the salient things you want to look at for Q's.
Now, one thing that I mentioned very briefly last time is that,
um, is the- is the advantage function identifiable?
And what do I mean by that in this case?
I mean that if you have a Q function which is what ultimately we're going to use,
um, can we decompose it into a unique a pi and v pi.
So, here ultimately we want a cube.
And the question is, if we then in our
architecture decomposing this into a value and an advantage,
is there a unique way to do that?
Is there? Um, but there isn't.
So, if you- if you add a constant to both Q and V,
um, then you can get the same advantage function.
So, there's not a unique, you can always shift your um,
shift your awards by a constant and that's not going to change your policy,
it will change your value function.
Um, I- so, there's lots of
different ways to decompose your advantage function and your values,
it's not a unique decomposition.
So, the way that they defined it there is to say, well,
let's force the advantage for state and action to be zero if A is the action taken.
So, here they compare
it to the action that's taken if you're using sort of say, a greedy approach.
Um, and this is really just a way to-
all of this we can think of it in some ways as an analogy to supervised learning.
And so, we want to have a stable target and we want to be able to learn
these advantage functions and
these value functions if we have lots and lots of data about them.
And so, this is sort of choosing
a particular fixed point for how to define the advantage function.
And then, they also said, well,
empirically you could just use the mean too.
So, you could just average over
your advantage functions, it's more of just a heuristic approach.
And what they find, again, so,
we sort of we're layering up these additional techniques.
We started with DQN, then we thought about adding, um,
double-Q learning to DQN and then we thought about adding prioritized replay.
And then this is dueling.
And what they find is dueling versus double DQN with
prioritized replay is a lot better most of the time.
Now, let me see if I can find Montezuma's.
Yep. So, for Montezuma's this new method is basically no better.
Like none of these methods are really tackling hard exploration problems.
But they are doing better ways of sort of propagating information
in the network and trying to change the way we're training the network.
Yeah, questions about that, and name first please.
Can you speak a little bit louder,
I'm unable to hear you well.
Okay. I'll try to speak a little bit louder.
Can- can people in the back over there hear me or is it just him?
Okay. Good. All right.
So, these were three of the methods that ended up making a big difference.
We talked very briefly about practical tips.
Um, I won't go in these too much.
The main thing is just that we try to actively encourage you to
build up your acuity representation first before you try on Atari.
Um, you can try different forms of losses.
Uh, learning rate is important, but in this case,
in our assignment we're going to be using
the Adam optimizer which means you don't have to worry too much about it.
There's a issue of sort of trying different exploration schemes,
is something that we're going to talk about later in this class.
So, for right now we're still thinking about just simple E-greedy approaches.
Um, a nice paper that came out,
I think it was start of 2018, um,
was Rainbow which was a paper that basically
just tried to combine a whole bunch of these recent methods,
um, to see really how big of an improvement do you get.
Uh, now again, note in this case and we'll come back to this in just a couple of slides.
This is a lot of data for a lot of experience in the world,
200 million frames of experience.
But they developed an algorithm called Rainbow that
combines a lot of the things we've just been talking about,
double DQN, prioritized, and dueling,
as well as some other recent advances.
Um, noisy is one that also tries to do some different forms of exploration.
And so, they found that kind of by adding these improvements together,
then you could get a significant improvement.
I think this is a useful insight because often it's not clear whether or
not these different gains are additive or if they're just, um,
sort of, um, you're,
you're- they're kind of doing the same thing but maybe in a slightly different way.
And so, it's nice to see that in some of these cases these different sort
of ideas are additive in terms of the resulting performance gain.
Um, these aren't sort of- this is still a very large amount of data.
[NOISE]
Okay. So just to summarize which we're wrapping up where we are with model-free,
ah, deep neural networks for RL right now.
Uh, they're very expressive function approximators.
Uh, you should be able to understand how you represent the Q function,
and you could do some Monte Carlo-based methods or TD style methods.
Um, and- and at this point,
it's sort of good to make sure you understand how you would do that with tabular methods,
with linear value function methods,
and with deep neural networks.
So it's sort of, algorithmically,
it looks very similar across all of those but then you- in some cases,
you have to do this step of doing function approximation and in other cases you don't.
Um, and then it'd be good to just make sure you can sort of list
a few extensions that help beyond DQN um,
and why they do. All right.
So now let's go back to our, um, sort of,
high level, uh, view of what we want from the reinforcement learning algorithms,
these are algorithms that are sort of doing optimization, handling generalization,
ah, doing exploration and doing it at all statistically and computationally efficiently.
And we've just been spending quite a lot of time on looking at
generalization as well as optimization.
Um, but we haven't talked very much about efficiency.
So one of the challenges is- is, that, um,
if you want to define efficiency formally like in terms of
how much data an agent lead, needs to learn to make a good decision.
Um, there are hardness results, ah,
that- that are known so our lab has developed some,
uh, lower bounds, other people have too.
Um, uh, I think we now have
basically tight upper and lower bounds for the tabular MDP case, um,
which indicate that there's some really pathological MDPs
out there for which we just need a lot of data,
lot of data, though, you know, would not scale very
well as you start to go up to really huge domains.
So some of these problems are really hard to do.
Formerly, you would just need a lot of exploration.
You can do something much better than E greedy but we'll talk about that soon.
But even when we do those much better things than
E Greedy we can prove that it's still really hard to learn in those,
we still might need a lot of data.
So an alternative is to say well there's lots of
other supervision that we could have in the world to try to learn how to do things.
Um, and so how can we use
that additional information in order to sort of speed reinforcement learning.
And so what we're going to do, is talk about some- about imitation learning today.
And then we're going to start talking about
policy search and policy gradient methods next week.
And those, you can also think of as another different way to impose structure,
um, because in policy gradient methods,
you always have to define your policy class.
Sometimes that can be a really rich policy class,
and so maybe that's not too much of a limitation,
um, but other times,
you're encoding domain knowledge by the class that you- that you represent.
Okay. And in particular, we're going to be thinking about imitation learning and
large state spaces which is exactly the place where you might hope to benefit from,
um, additional help or supervision.
So if we think about something like Montezuma's Revenge, um,
there's some nice work on looking at sort of how far did DQN get in this case.
So Montezuma's Revenge, for those of you who haven't played,
it is sort of a, um,
a long- very long horizon,
uh, game in which you're sort of trying to navigate through this world,
and like pick up keys and make decisions.
Um, and it involves a lot of different rooms.
So you can see here what the outline of the,
all the white squares are basically rooms.
And on the left-hand side, um,
sort of a DQN that was trained for 50 million frames,
um, only gets through the first two rooms.
Like, it's just doing very badly.
It's not making very much progress.
Um, whereas on the right-hand side,
we see something which is explicitly trying to explore.
Um, and it uses some of the techniques that we'll be talking more about later.
But notice that it still doesn't get all the way through the game.
Um, and so I think this sort
of illustrates the fact that some of these games are really hard.
Um, there has been some really nice additional progress since, um, ah,
both, ah, with me and Percy Liang's lab we now can basically solve Montezuma's.
Um, and there's also been some really nice work from Uber AI lab on solving Montezuma's.
But a lot of the places that people originally got traction on
this was by starting to use imitation learning and demonstrations.
Um, so in particular,
if we think about cases where RL might work well, RL works,
you know, pretty well when it's easy or certainly we've seen a lot of success.
So far when data is cheap and parallelization is easy,
and it can be much harder to use the methods that
we've talked about so far when data is expensive,
um, and when maybe failure is not tolerable.
So , if you tried to use the methods that we just described
to learn to fly like a remote control helicopter.
Typically require a lot of helicopters [LAUGHTER] because it would be very expensive.
And so there's many cases where this type of,
um, performance is just not gonna be practical.
So one of the benefits is that if you can give the agent a lot of rewards,
you can shape behavior pretty quickly.
Um, so one of the challenges in Montezuma's Revenge is that reward is very sparse that,
you know, the agent has to try lots of different things before
it gets any signal of whether it's doing the right thing.
Um, and where do these rewards come from?
I think it's generally it's actually a really deep question.
Um, but for right now,
let's think about sort of just even the challenge of specifying rewards.
Um, so if you manually design them,
that might be pretty brittle depending on the task.
Um, and an alternative is just to demonstrate.
So if you had to write down the reward function for driving a car,
it's quite com- complicated,
like you don't want to hit roads,
here or hit the ro- hit, um, people.
You don't wanna, um, drive off the road.
You want to get to your destination.
And so it's a very complicated reward function to write down.
But it's pretty easy for most of us to just drive to
a destination and show an example of maybe an optimal behavior.
So that's sort of the idea behind learning from demonstrations.
Um, there's been lots and lots of work on this but since people
started thinking about learning from demonstrations or imitation learning.
I would argue probably this was started roughly 20 years ago,
around 1999, 2000 was a paper
which started to think about learning rewards from demonstration.
Um, but then there's been lots of applications to it since.
So thinking about it for things like highway driving,
um, or navigation, or parking lot navigation.
There's a lot of these cases particularly in driving right now,
but, ah, where people have been thinking and- and robotics too.
To think about how do you do, um,
demonstrations of like how to pick up a cup or things
like that to try to teach robots how to do those tasks.
Um, there's also some really interesting questions too about like,
you know, how do you, uh,
do things like path planning or goal inference,
and again these sorts of cases where it's quite complicated
to write down a reward function directly or it might be brittle.
And the problem with brittle reward functions is that
your agent will optimize to that and it may not be the behavior that you wanted.
So- so the setting from learning from demonstrations,
and- and today I'm going to be somewhat informal
about whether I call things learning from demonstrations,
um, there's also inverse RL.
And there's also imitation learning.
And there are sort of differences but a lot of these things are somewhat interchangeable.
Most of this is about the idea of saying that you're- you have some demonstration data,
and then you're going to use it, uh,
in order to help boot- either bootstrap or completely learn a new policy.
So the idea is that you might get an expert and maybe they're a perfect expert or
maybe they're a pretty good expert to provide some demonstration trajectories of,
um, taking actions, um, in states.
And in many cases,
it'll be easier for people to do this but it's useful to
think about when it's easier to specify one or the other,
and what situations are- are common for each.
So what's the problem setup?
The problem setup is that we have this state space and action space.
Um, some transition model that is typically unknown,
no reward function and instead sort of a set of teachers
demonstrations from some particular we assume for now optimal policy.
Um, and the behavior cloning we're gonna say,
how do we directly learn the teacher's policy?
So how do we match, how do we get
sort of an approximation of pi star directly from those demonstrations?
Inverse RL is typically about saying,
how can we recover the reward function?
Once we have the reward function,
then we can use it to compute a policy.
Uh, and that often- that last step often is combined with the apprenticeship learning.
So that we're both trying to get
that reward function and then actually generate a good policy with that.
In some cases, you might just want the reward function, um,
can anybody think of a case where you might be interested in just the reward function,
maybe you don't want to recover the policy,
but you're just curious about what the reward function is of another agent. Okay.
If you're trying to understand, say, [inaudible]
Yeah, how the environment [inaudible].
Yeah, I think is a great example.
So, in a lot of, um, uh, science, you know,
biology et cetera you often want to understand
the behavior of organisms or animals or things like that.
And so, if you can just look at their behavior,
you could say track monkeys or things like that and then use that to back solve,
like, what is their reward function.
What are the- the goals or preferences.
Um, I think there's a number of cases where that's useful,
and maybe down the line, you know,
maybe there's some optimization that'll happen
but- but generally often there it's just about understanding,
like, what is the goal structure or what is
the preference structure of- of the organism or individual.
That could happen with people too that you might want to understand, like,
the choices people are making in terms of nav- you know, um, uh,
commuting or in terms of buying preferences or things like that.
Maybe later you want to optimize for that but also you're just curious
about how- how do people's behavior reveal the,
sort of, um, an underlying reward structure,
underlying preference model. Yeah,
Imitation learning just using the teacher's demonstration set,
like, an upper bound, I guess,
or, uh, have there been cases such that, yes,
that, like, agent learns to perform better than the actor did.
Like we find a new path side.
Yeah, asked a nice question of, like,
is the expert's behavior an
upper bound or are there cases also where the agent can go beyond this?
We're not gonna talk too much about this today,
but there's a lot of work right now on combining imitation learning with RL.
So, um, uh, there's a lot of work on say like inverse RL plus RL.
Where for example, you might use this to s- bootstrap the system,
um, and then your agent would continue to learn on top of this.
There's also some nice work from Pieter Abbeel's Group, um,
where they looked at assuming that the expert was providing like a noisy demonstration of
an optimal path and then the goal is to
learn the optimal path not the noisy demonstration of it.
So, often you do want to go beyond the expert.
There's limitations to that,
and we'll talk about that in a second actually.
What- what are some of the limitations that you might have if
you don't get to continue to gather data in the new environment.
Okay, so let's start with behavioral cloning which is probably the simplest one,
um, because essentially in behavioral cloning we're
just gonna treat this as a standard supervised learning problem.
So, we're going to fix a policy class which means, sort of,
some way to represent, um,
our mapping from states to actions.
And this could be a deep neural network.
It could be a decision tree,
could be lots of different things.
Um, and then we're just going to estimate a policy from the training examples.
So, we're just gonna say, we saw all these times.
We saw a state and an action from our expert
and that's just our input output for our supervised learning model.
And we're just gonna learn a mapping from states to actions.
And early on, so this has been around, uh,
really for quite a long time and I,
uh, should have said more like 30 years.
Um, there were some nice examples of doing this.
So, ALVINN, um, was a very early, uh,
paper, uh, and system about thinking about, uh, driving on the road.
See, it was a neural network and it was trained, uh,
at least in part using behavioral cloning or supervised learning to imitate trajectories.
Um, okay, so let's think about why this might go wrong.
Um, and to first,
let's think about what happens in supervised learning.
So, in supervised learning, um,
we're gonna assume iid pairs s,
a and we're gonna ig- ignore the temporal structure.
So, we're gonna- if we're just doing supervised learning in general,
we just imagine that we have these state-action pairs and then
maybe we learn some classifier or,
um, yeah, let's just say a classifier,
to classify what action, you know, we should do.
And it might have some sort of errors.
It might have errors of,
uh, well, you know, with probability epsilon.
And so if we were thinking about doing this over the course
of T time steps than we might have, you know, sort of,
an expected total number of errors of epsilon times T. So,
let's just take a second and think about what goes wrong when we're doing
this in the supervised learning or in the- in,
uh, in the RL context.
So, by the RL context, I mean,
the fact that the decisions that we make influence the next state.
So, let's just take, like, one minute maybe talk to your neighbor and say, like,
what do you think could be the problem with
behavioral cloning in these sorts of scenarios.
And if, uh, that's a simple thing to
think about then maybe think about how you would address it.
So, what you might do in that case if there is
problems that happen when we try to apply standard
supervised learning to this case where it's really underlying, uh, an MDP.
[OVERLAPPING].
All right. So, first of all let's just make a guess.
I'm going to ask you guys whether you think the, um,
the total expected errors if we're
doing this in- in where the underlying world is an MDP,
is gonna be greater than or less than the number of errors that
we'd expect according to a supervised learning approach.
Um, so who thinks that we're going to have greater expected total errors?
Okay. Who think we're gonna have less?
Who- many people must be confused.
[LAUGHTER] Okay.
So, how about somebody who thinks the answer is that we're gonna have greater.
Maybe somebody who thinks that's the case could
say why they think we might have more errors
if the real world is MDP and we've tried to do
the supervised learning technique. Yeah, and name first, please.
My idea is that kind of, like,
uh, as a human you're planning a more long-term horizon or, like,
you're doing one step and then you know how that action is gonna then give you, like,
another sequence, but since we've
just had taken a state and action and then, like, predicting.
Right from there we can't plan that long-term sequence,
so it's gonna, like, compound our errors as we go.
That's right. So, when says that, correct, we will compound those errors,
and one of the- the challenging aspects of this is that the errors can compound a lot.
Um, and this is because the distribution of states that
you get can- depends on the actions that you take.
So, if you think about this like a navigation case, like,
if I was supposed to go out the right-hand door,
um, and I watched go out this door and I saw that he went right.
And I- and I, you know,
tried to learn a supervised learning,
uh, classifier for what I should do here.
But my supervised learner was a little bit broken,
and so instead of going right here, I actually went left.
Well, now I'm in part of the room which never went
to because he was going over there to go to the door.
And so, like, now I have no idea what to do here.
All right, like, now I'm in the state distribution,
it's something that I haven't seen before,
it's very likely that I'm going to make an error.
In fact, my probability of error now may not be- my probability of error here is
under-assuming the fact that the data that you get in the future
is the same distribution as the data you got in the past.
Our supervised learning guarantee is generally safe when we have them.
Um, uh, that I- if- if your data is- comes from an iid distribution,
then in the future this is what your test error will be.
The problem is, is that,
in reinforcement learning or markup decision processes,
your actions depe- determine what is the data you're gonna see.
So, the fact that instead of following the right action here,
um, I went over here.
And now I have no data and my data distribution is different,
so now there's no guarantees from
my supervised learning algorithm because my data is different.
It's never been trained on anything like that so we can't generalize.
So that's exactly the problem.
Um, and this was noted by, like,
Drew Bagnell's group from CMU in 2011 of arguing that,
you know, this is a really big problem for what's called behavioral cloning.
So, even though there had been some nice empirical demonstrations of it,
uh, he and his former students,
Stephane Ross indicated why this might fundamentally be
a very big problem an- and sort of ill- uh,
demonstrated some things that people had sometimes seen empirically.
The idea is that as soon as you deviate,
so this is the time where you make your mistake,
then essentially the whole rest of the trajectory might all be errors.
You might make T more- T more errors.
Um, and so that means that the total number of errors that you make is not expected,
uh, uh epsilon times T but it's epsilon times T squared, it's much worse.
And it's really due to these compounding errors leading to- you to
a place where your distribution of states
is very different than what you have data about.
And this issue will come up again, again,
this sort of equiv- this thinking about what is the distribution of states that you get,
um, under, you know, the policy you're
following versus the policy that you want to follow.
That issue comes up again, and
again in- in reinforcement learning.
So, um, it really is just a foundational issue that, you know,
what is the data distribution you're gonna get under the policy that you've
learned versus the true policy and looking at this mismatch.
And so once you go off the racecourse you're not gonna have any data about that.
So, one of the ideas, uh,
that Drew Bagnell and his students came up with to think about this was to say,
well, what if we could get more data?
So, what if when I, you know,
my- I- I only have a little amount of data to start with.
I've learned my per- my supervised learning policy to say,
you know, what shall I do in each state, and sometimes I make a mistake.
So, sometimes, you know, I- I go out that way,
my race car drives off the racecourse.
What if I could know what to do in that, in that state?
So, I reached a state that I don't have any data about,
what if I could ask my expert, hey, what should I do now.
So, like, I go over here and I'm like,
oh my god I don't know, you know, what to do now.
And then you ask your expert, and they're like, oh just turn right.
It's fine, you can still get out of the right door, it's okay.
Um, so if you could ask your expert for labels,
well, now you're getting labels about states that you are encountering.
And as long as all the- as long as you have data that covers
all the states that you're gonna potentially experience,
then your- then your supervised learning should do pretty well.
But the bigger issue tends to come up with the fact that you are
encountering states that you don't have any coverage of in your training data.
So, the idea of DAGGER which is
a data set aggregation is essentially you just keep growing your data set.
So, what happens in this case is you start off, you don't have any data.
You initialize your policy.
You follow your policy.
So, in this case we're gonna assume that you have an expert,
um, some expert policy.
So, that might be, you know, an expert is taking some steps and then
also your other policy you can- you project a trajectory.
So, sometimes you're following one policy sentence you're at.
And, ah, and then what you get is you get this state.
You get to go ask your expert for every single state,
what would you have done here?
So for every state that you encountered in that trajectory.
And then, you add all of
those tuples to your dataset and you train your supervised learning policy on that.
So, everything inside of your dataset that you're using to train your policy on,
is with an expert label and you're slowly growing the size of that dataset.
So, the idea is as you're getting more and more experts, ah,
more and more labels of expert actions across the the trajectories you've actually seen.
And there's nice formal properties for this.
So you can be guaranteed that you will converge to a good policy, um, ah,
by following this, um,
under the induced state distribution. Yeah, .
Just to confirm, this is assuming we have pi star over all space,
like when we're doing the case where we don't have the expert to go to.
Yeah, no this is a great question, so
when I was looking at this just now, I would have to double-check.
I think this is assuming your expert can give you the action there.
It doesn't assume that you have explicit access to pi star.
Because if you had explicit access to pi star, you wouldn't need to learn anything.
So, I think in this case it's like tossing a coin
about whether the expert just directly gives you
the action in that case or whether you follow the other policy.
Which because you have to have the expert around all the time anyway,
because they're always going to have to eventually
tell you what they would have done there.
Get- that's how you, excuse me, that dataset.
So, that's- there was one.
I'm and, um,
I'm curious about how is this done efficiently?
I can imagine that for some situations having a person set
the command line while your, your GPU trains and inputs actions won't be efficient.
How do people generally do this in such a way that it doesn't require manual intervention?
Yeah, 's question is a great one which is, um, you know,
well, this requires you have an expert either around for
every single step or like at the end of the trajectory that can go back and label everything.
And that's incredibly expensive.
And if you're doing this for, you know,
millions and millions of time steps that's completely intractable.
I think that's why, um, this,
this line of research has been less influential in certain ways than the,
some of the other techniques that we're going to see next in terms
of how you do sort of inverse RL.
So, what this is really assuming is
that you have this human in the loop that's really in the loop,
um, ah, as opposed to just asking them to
provide demonstrations once and then your algorithm goes off.
And I think that practically in most cases it's much more realistic to say,
you know, drive the car around the block 10 times,
but then you can leave and then we'll do all of our RL
versus saying I need you to be in the car or,
you know, like, label all of the trajectories
that the car is doing and keep telling you whether it's right or wrong.
I think this is just very label-intensive. It's very expensive.
So I think that, um,
in some limited cases,
like, if your action rate is very slow,
like, if your action rate is, you know,
making decisions in the military or you know, others that are at
a very high level, with very sparse decisions, this can be very reasonable.
Because you could basically throw, you know,
infinite compute at it before, between each decision-making.
If you're doing this for sort of real-time hertz-level decisions,
I think that's very hard. Okay, yeah, .
Will this be compatible with like an expert taking over the system.
Right, like, somebody sitting behind the wheel letting an agent drive and then,
uh, like, recognizing that there's an emergency situation coming up and taking the wheel.
Yeah so, like, um, so what said,
is this compatible with sort of a- an expert taking over?
Yes. I mean I think.
And that might be an easier way to get labels.
So you might say if you have an expert there,
every action that's taken that's the same as the action the expert would take.
Maybe they don't intervene. Otherwise, they only provide
labels or interventions when it would differ.
But it still requires like an expert to be monitoring,
which is often still mentally challenging essentially. You know, it's still high-cost, okay.
All right. So that- that is a nice.
I mean there- it's very nice to see sort of the formal characterization
of why behavioral cloning can be bad and what is the reason for this.
Um, and I think that DAGGER is can be very useful in certain circumstances,
but there's a lot of cases where just practically it's
much easier to get a- example demonstrations,
um, and then assume that there's no longer a human in the loop.
All right.
So inverse RL is more of one of the- the second categories.
So, what does- what happens in inverse RL?
Well, first let's just think about you know, feature-based reward function.
So, well okay, we'll get to there in a second.
So again, we're thinking about this case where we
have some transition model that we might not observe.
Um, a- or maybe we're doing.
A lot of the techniques here,
to start with, they didn't assume that you knew the transition model.
[NOISE] That's pretty strong for a lot of real-world domains,
but in some cases that's reasonable.
So, for right now, we're going to assume that
the only thing that we don't know is the reward function.
There's some extensions to when you don't know the transition model too.
Okay. So, then we have again our set of demonstrations and the goal
now is not to directly learn a policy but just to infer the reward function.
So, if I don't tell you anything about the optimality of the teacher's policy,
what can we infer about the reward function?
Like let's just say, like its not an expert, it's just demonstrations.
If you get a demonstration of a state,
action state et cetera.
Can you infer anything about R?
If you don't know anything about the optimality?
[NOISE]
Like, I mean,
would it be the same as samples,
like as we get more demonstrations, R will approach like, R star, I guess.
This is assuming no assumptions about optimality.
So, if I don't tell you anything about the optimality of the policy you're seeing,
is there any or any information you can gather in that case?
I'd be able to say that the choice that the teacher made under
their policy just wrapped hot air under
the reward each other function that the alternatives.
So is saying, for that particular person you could say something about,
um, for their reward function,
assuming that they're a rational agent,
that that- that was, um, higher in their own function.
That's true. But if, if you wanted it to be about the general word function, um,
this would tell you maybe I'm understating it so that it seems a little bit
more subtle than I mean it to be.
It doesn't tell you anything, right.
Like if you- if I- if you see me like
wandering around and like if you see an agent flailing around,
right, and and you know nothing about whether it's making
good decisions or not with respect to the true reward function.
Demonstrations don't tell you anything,
like they don't give you any information about the reward function
unless you know something about, um.
Unless you're trying to make an assumption that the agent is
acting rationally with respect to the true reward function.
Or maybe you get some information about their internal one like what was saying.
But in general, if we don't make any assumptions about agent behavior and we don't, um,
and we don't assume that they're doing
anything optimal with respect to the global reward function,
there's no information you can get.
Now, the more challenging one is the next.
So, let's assume that the teacher's policy is optimal with respect to
the true global reward function which
the agent is also maybe going to want to optimize in the future.
So, you get expert driver perfo- driver, um, driving around.
Um, this is like,
think for a second about what you can infer about the reward function in this case.
Um, and whether in particular there's
more than one reward function that could explain their behavior.
So think about whether it's unique.
So let's say, imagine data has no issue.
I give you 10 trillion examples of-
of the agent following the optimal policy. 10 trillion examples.
Um, and you want to see if
you could learn what the reward function is and the question is, um,
is there a single reward function that is consistent with their data or are there many?
Maybe take a second to talk to somebody around you, um,
just to see whether there's,
the question really is there one reward function?
Is there a unique reward function?
If you have infinite data so it's not a data sparsity issue, um, versus money.
[OVERLAPPING]
All right, I'm going to do a quick poll.
Okay, we're going to- I'm going to poll you guys
then I'm going to ask whether you think there
is one reward function or if there's more than one reward function.
So, who thinks there's a single reward function?
Infinite data, single reward function that's consistent.
Who thinks there's more than one reward function?
Okay. Could someone give me a reward function that is
always consistent with any optimal policy?
I guess that's what we need to do is in the first step just get the older apart that
the feature is going to have as a policy and then just be random after that.
is saying maybe on the first step,
that you could maybe give most of the reward that, like,
the agent would experience and then be random after that.
So, and that might depend on the state.
I guess I was thinking of, if- can anybody tell me like,
um, a number which would allow or, you know,
specification of a reward function like a constant,
like a choice of a constant which would make any policy optimal. Yeah,
. You just give a reward of zero for every action.
Sorry, yes. So, what says is exactly correct.
If you give a reward of zero, um,
any policy is optimal in the respect that you'd never get any reward.
Anywhere, it's a sad life unfortunately.
And, um, uh, in this case,
all policies are optimal.
Right. So, uh, so if you just observe trajectories,
then one reward function for which that policy is optimal at zero but it's not unique.
So, um, this issue was observed,
I think it was by Andrew Ng and Stuart Russell back in 2000.
Right there is a paper talking about inverse RL where they noted this issue.
The problem is that this is, uh,
not unique without further assumptions, there are many reward functions that are consistent.
Um, so, we have
to- we're gonna have to think about
how do we break ties and how do we impose additional structure. Yeah in the back.
If you have a consistent reward function,
for instance, if you add a constant and what are the rewards also?
They're loss, oh, remind me of your name.
.
. So, yeah what said is,
there's, there's many many reward functions.
So, if you have, um, a constant, er,
everything has the same, uh,
any constant also be identical.
So, um, there's generally many different,
um, reward functions that would all give you.
There are many different reward functions for which any policy is optimal.
Instead, that would mean that if you're trying to
infer what function given some data there are
many reward functions that you can write down so that
that data would be optimal with respect to the reward function.
[NOISE] And, and that second part is really what we're trying to
get as we're trying to sort of, uh,
infer what reward function would make this data look like it's,
um, coming from an optimal policy,
if we assume that the expert is optimal.
So, let's think about also how we do this in,
um, enlarged state spaces.
So, we're gonna think about linear value function approximators, um, because, again,
often the places where we particularly need to be sample efficient is
when our state space is huge and we're not gonna be able to explore it efficiently.
So, let's think about linear value function approximator.
And we're gonna think of this reward also as being linear over the features.
So, our reward function might be
some weights times some featurized representation of our state space.
And the goal is to compute a good set of weights given our demonstration.
I already said that in general,
this is not unique but, um,
we're gonna try to figure out ways to do this in,
in different, um, methods.
So, the value function for a policy Pi can be expressed as the follows- following,
and you just write it down as,
uh, the expected value of the discounted sum of rewards,
and this is the states that you would reach under that policy.
Under the distribution of states that you
get to under this policy, these are their words.
So, now what we're gonna do is re-express this.
So, what we're doing now is that this is gonna
be- we're assuming a linear representation of our reward function.
So, we can write or we can re-express it like this.
So, we can write it down in terms of
the features of the state we reach at each time step times the weight.
And then because the weight vector is constant for everything, you can just move it out.
And then you get this interesting expression which is you basically just have
this discounted sum of the state features that you encounter. And we're gonna call that Mu.
So we talked about this very briefly earlier, but, um,
now we're talking about Mu as being sort of
the discounted weighted frequency of state features under our policy.
How much time do you spend, um,
uh, in different features, um,
or you know that basically how much time you spend in
different states, sort of a featurized version of that,
um, discounted by kind of when you reach those because
some states you might reach really far in the future versus now.
So, it's related to the sort of stationary distributions we were talking about before,
but now we're using discounting.
So, why is this good?
Okay. So what, er, we're gonna say now is
that instead of thinking directly about reward functions,
um, then we can start to think about distributions of states.
Um, and think about sort of the probability of,
of reaching different distributions of states,
different state distributions, um,
as representing different, uh, different policies essentially.
Different policies, um, for a particular reward function,
um, would reach different distributions of states.
So, we can think about using this formulation for apprenticeship learning.
So, in this case,
we have this nice setting for apprenticeship learning.
Right now, um, we're using the linear value function approximation we call
it apprenticeship learning because we're learning like
the agent is being an apprenticeship from,
uh, from the, from the demonstrator.
So, now we have this discounted weighted frequency of the feature states.
So, we're always sort of moving into the feature space of states now.
Um, and then we wanna note the following.
So, if we define what is the value function for Pi star,
that's just equal to the expected discounted sum of rewards we reach.
And by definition, that is better than the value for any other policy.
At least as good because either Pi is the same as
Pi is the same as the optimal policy or it's different,
and this is just equal to the same thing, um,
same reward function which we don't know,
but under a different distribution of states.
It's under the distribution of states you'd get to if you follow this alternative policy.
And so, if we think that the expert's demonstrations are from the optimal policy,
in order to identify W,
it's sufficient to find the W star such that,
if you dot-product that with
the distribution of states you got to under the optimal policy.
Remember, this is what we know,
these are- we can get this from our demonstrations.
This has to look better than the distri- than
the same weight vector times the distribution
of states you get to under any other policy.
Are there questions about that?
So, it's by making this observation that the value of the optimal policy is directly
related to the distribution of states you get under it times this weight vector.
And that value has to be higher than the value of, uh,
any other policy which is using that same weight vector and this gives you
a different distribution of states. Yes, .
[inaudible]
U in this case is sort of like the stationary distribution of
the proportions of its refugees state gave the policy.
[NOISE]
Yeah. Especially in serv- in terms of conceptualizing Mu.
What's the sort of a good way to think about it is it should we think
of it as like the stationary distribution of states?
Yeah. I think it's reasonable as everything that is
essentially the stationary distribution of
states weighted with this discount factor on top of it.
So, it's very similar to the stationary distributions we saw before.
All right. So, essentially it's the same,
we want to find a reward function so that
the expert policy and the distribution of states
you reach under it looks better when you compute the value function compared to,
um, that same weight vector under any other distribution of states.
Um, and so if we can find a policy so
that its distribution of states matches the distribution of states of our expert,
then we're gonna do pretty well.
So, what this says here is that if we have
a policy so the dis-discounted distribution of states that we
reach under it is close to
the distribution of states that you got from your demonstration, since it's the expert.
So, if that's small then your value function error will also be small.
So, for, for any w if you can- if you can basically match features,
match feature- expected features or matched distributions of states, then you're good.
Then, then you found, um, er,
found a value that's going to have a very similar value to the true value, okay.
So, this actually means for any w_t here.
So, it means no matter what the true reward function is,
if you can find a policy so that your, uh,
state features match, then no matter what
the true reward function is you know that you're going to be
close to the true value. Yeah, .
So, this, uh, this w that we will- we will
be finding would be used to calculate I guess, an expectation.
What your, I guess,
values or state. Right?
Yes. You could- once you have a w you combine that with your mu's to compute,
like, a value of a state or you can sum over it.
How do you, uh, I guess,
that translates directly to,
being able to use that to make decisions when they're out of state?
So, I think, I think question is about saying, like, okay,
so if we're getting these w's or,
sort of, what are we solving for?
Are we solving for the policy,
are we solving for w et cetera.
In this case, I think, uh,
a reasonable way to think about it is,
um, solving for w if it's solving for pi.
So, what this is saying is that let's say you're optimizing over pi.
If you found a pi,
so right now we know the transition model which is not always true,
but if you know the transition model,
for a given pi you can compute mu because it's just following,
like, you could do Monte Carlo roll outs for example.
So if you- someone's given you a pi and they tell you the transition model,
you can roll that out and you can estimate mu of pi.
Then it's saying that if you do that,
so let's say I have some policy,
I roll this out a bunch of times,
I estimate my mu and I check whether that seems to
be close to my mu of my demonstration policy.
If that's small, this is saying no matter what
the real reward function is you're gonna have the same value as,
like, uh, what you're,
like, like, you've matched,
um, uh, the value that you would get under the expert policy.
So, you're good. You can just use this policy to act.
[NOISE] Yeah.
. And I'm looking at the constraint on the [inaudible] for w,
and I, I don't quite see where that comes from.
I'm curious since I'm missing it here or we haven't gone over it.
's question is about why we have this constraint over w. Um,
I- my- I would have to double-check the details to
be careful about this but I'm pretty sure it's there, so that,
um, as we do these backups when we do this approximation,
that, um, your errors are all bounded [NOISE] so that things don't explode.
Um, and that when you do this proof that- I think I'd have to double-check it,
but uh, but I think you basically use Holder's inequality and then you use the fact
that the w is bounded to ensure that your ultimate value is bounded.
So, you can check that. [NOISE] In general,
you want your, your reward function to be bounded, um,
particularly the- in the- even with just counting, like, it's useful.
You always need to make sure that
your Bellman operator's like a contraction to have a hope of- I mean,
we've already talked about the fact that with linear value function approximated,
you don't always converge, um, uh,
but if your rewards are unbounded it gets worse. Yeah, .
Um, I'm trying to fit this into uh,
other things I'm familiar with, is this basically, like,
a maximum likelihood way of looking at the policy, right?
Like, if we flipped a coin 100 times and we got 99 heads and 1 tails,
it's possible that came from,
uh, a fair coin.
You know that uh,
we can't discount that but it's unlikely, right?
So, if we observe some expert agent doing the same thing 100 times
that could come from a reward function that's zero
everywhere but not as likely as some other reward function? Does that-
Oh, great question. So is asking,
like, so is this, sort of,
giving us some way to deal with the fact that the reward could be zero.
That we have this, sort of, unidentifiability problem.
This does not handle that unfortunately.
So, um, er, this is still not guaranteeing that we couldn't,
uh, learn a weight if it's zero everywhere,
but what this is saying here is that, um,
instead of thinking about trying to learn the reward function directly,
if you match expected state features,
um, then that's another way to
guarantee that your policy is basically doing the same thing as the expert.
So, if you have a policy that basically it looks like- that
visits the same states in exactly the same frequency as what the expert does,
then you've matched their policy.
And you still don't necessarily know that the w you've got is
accurate or is a good estimate of the reward function but maybe you
don't need it because if you really just care about being able to
match the expert's policy then you've matched it.
Because if you're- if you visit
all the states with exactly the same frequency as what the expert does,
you have identical policies.
So, it's, sort of, giving up on it.
It's saying, well, we still don't know what the real reward function is but it
doesn't matter because we've uncovered the expert policy. .
Um, is there, like, a, um, nonlinear analog to this that might be more effective?
Great question, yes. So, there's been a lot of
work also on doing this with deep neural networks.
I'll give a couple of pointers later to sort of, uh, other approaches.
Okay. So, this sort of observation led to an algorithm for learning the policy which is,
um, uh, you try to find some sort of reward function.
Like, that means, you know, a w and choice of w,
um, such that the teacher looks better than everything else.
Looks better than all the other controllers you've got before.
So, it makes it look like,
sort of, the, um,
this w for this state distribution looks better than w for all other distributions.
[NOISE] And then you find the optimal control policy for
the current w which can allow you to then get new,
uh, mu's because you have your transition model here.
And you repeat this until,
sort of, the gap is sufficiently small.
Now, this is not perfect.
Um, if, if your expert policy is sub-optimal,
it's a little tricky how to combine these.
Um, I don't want to dwell too much on this particular algorithm,
it is not something most people use anymore.
Um, people would use, uh,
more deep learning approaches,
but I think that the,
the key things to understand from this is,
sort of, this aspect of, kind of,
if you match state features that that's sufficient to say that the policies are identical.
It's actually bigger than. Yeah, and [OVERLAPPING] remind me your name first, please.
.
.
[NOISE] Is there any significance in using norm one
versus the other norm, like, norm two?
Uh, goo- great question. question is why do we use norm one in equation seven.
That is actually important.
Um, it's not necessarily the only choice,
but here this is saying you have to match on all states.
That's what the norm one is saying here.
So, you can think of mu pi as- are really being of s. I'm not showing
that explicit dependency here but it is of s. And so what norm one is saying is that,
um, when you sum up all of those errors it has to be one.
So, you're really, you're evaluating the error over all of this.
You could choose other things to change the analysis.
Um, uh, norm one in an infinity norm
norm tend be particularly easy to reason about when you start to do,
um, uh, when you're trying to bound the error in the value function.
Okay. So, um, there's still this ambiguity that we've talked about,
so there's the sort of infinite number of different reward functions,
the algorithm that we just talked about doesn't solve that issue.
Um, and so there's been a lot of work on,
on, on imitation, uh, learning and inverse reinforcement learning.
And two of the key papers are as follows.
The first one is called Maximum Entropy Inverse RL.
And the idea here was to say we wa- I don't wanna pick something, uh,
which has the maximum uncertainty, uh,
given that still respects the constraints of the data that we have from our expert.
So saying, we're really not sure what the reward function is,
we may not really be sure what the policy is,
but let's try to pick distributions that have maximum entropy,
sort of make Least Commitment, um,
sort of the opposite of overfitting,
you kinda want to like underfit as much as possible.
Um, and only makes sure that you match in these expected, uh, state frequencies.
So, both of these,
the- both of these methods and a lot of the methods think
very carefully about the expected state frequencies you get,
um, er- and comparing the da- the data you get versus,
um, the data you have from the demonstrated- demonstrator.
Um, these type of methods can be extended to where the transition model is a node.
[NOISE] Often that requires access to a simulator.
Often it means, so you can imagine for the thing we had before,
if you didn't have access to the transition model
but did- you did have access to it actually in the real world,
you could just try out new policies,
see where your distribution of states that are like and how that matches your,
um, expert demonstration and in fact that's often what's done.
So, maximum entropy inverse RL has been hugely influential.
And, and, and then the second one,
and this is also a note is from, uh,
also from Drew Bagnell's group,
who was the same person that came up with DAGGER,
so that group's been thinking a lot about
and did a lot of nice contributions to inverse RL.
And then, in terms of extending this to sort of much,
um, broader function approximators,
um, er, Stefano Ermon who's here at Stanford extended this to using deep neural networks,
um, and again, is doing sort of this feature matching.
So, the idea in this case,
both of these methods compared to the sort of
the DAGGER work or assuming that you have a fixed set of trajectories at the beginning,
and then you're going to do more things for the future.
And in particular, um, the general, uh,
generative adversarial inverse imitation learning, um,
has these initial trajectories,
and then it's gonna allow the agent to go out and gather more data.
So, they can gather more data,
it can compute the,
the state frequencies, um,
it can also use sort of a discriminator to compare- one of the challenges is,
you know, writing down,
um, the form of Mu,
can be hard when you have a really really high dimensional state space.
So, writing down, you know, a distribution over images is hard.
Um, so, what they do in this case,
they're mostly focusing on MuJoCo-style tasks like robotic-style tasks,
where you'd have a lot of different joints,
but it's still hard to write down,
you know, nice distributions over that.
So, what they focus on in this case is thinking about things
like a discriminator that could tell between
your expert demonstrations and the demonstration-
and the trajectories that are being generated by an agent.
And so, if you can tell the difference between those, then you're not matching.
That's it, that's a nice insight,
is to say that we could use these sort of, uh,
discriminators, uh, again, is, you know,
the discriminator function, to try to figure out how do we quantify what it means
to have the same state distribution in really high state- high dimensional state spaces.
So, um, uh, this is known as Gail.
And there's been a lot of extensions to Gail as well. Yeah, .
Uh, earlier, we said that there could
be real practical benefits to learning the reward function,
in certain situations, um,
but it seems like a takeaway to those that we can't actually do that,
is that the correct takeaway here?
Um, er, yeah. So, was saying earlier, well, earlier as
you were arguing that maybe there are times where we really want the reward function,
um, but maybe you're telling us that we can't really do that, um,
I think in this- so we've mostly
been talking about like frequentist-style statistics, er,
when we're talking about statistical methods here,
from that perspective, it's often very hard to uncover the word function.
One thing that's often people do when they want to say,
understand animal behavior or things like that,
so you have a prior, you can do it another way to do this,
is you have Bayesian prior reward functions,
and then you do Bayesian updating,
so that given the data that you see,
you try to refine your posterior over the possible reward functions.
So, it avoids like then you can just not
have your prior cover that reward is a zero everywhere, for example.
Um, er, so, if you have a structured prior,
that can be one way to still use information to try
to reduce your uncertainty over people's or agent's or,
uh, animal's reward functions, yeah,
[inaudible].
Yeah.
Um, it's a great, says you know,
what are, what are realistic priors for word functions?
Um, uh, it's a great question.
I think mostly it depends on the domain.
Um, that people do use, uh,
I think we'll talk a little bit about it for the exploration aspects,
people do use priors over reward functions for exploration as well,
um, things like Thompson sampling require you to do that.
If you want it to be as close to frequentist as possible, um,
often people do, uh,
Dirichlet distributions, over multinomials or things like that, um,
or Gaussians, and, um, uh,
and so you'd use conjugate, um,
uh, uh, exponential families,
so everything's conjugate, but those aren't necessarily realistic.
I think in, in real domains, um,
the benefit probably of using these sort of priors would
be to really encode domain knowledge about, you know,
uh, whether people are very sensitive,
what sort of rewards you expect,
uh, to be reasonable in these cases.
Yeah, I mean, I think I- to go back to point too,
I think a lot of it does depend on what you want out.
So if you really want to just understand
the reward function and the preference function,
then we need to maybe do something Bayesian or we'd
need to try to have a method that's gonna help us like cover it.
I think what a lot of other methods ended up saying is, well,
maybe we care about the reward function,
but mostly we just care about getting high performance.
So, if we can uncover a policy that's matching an expert policy, we're fine.
Behavior cloning wasn't a good way to do that because errors compound,
but now there are these other ways that can do that better,
and so we're fine with that part.
And again, I just wanna emphasize like Sergei Lamin
and others have done work which really combines,
like you can take Gail and,
and then go beyond that in terms of, er, exploration,
so you can end up with a policy that's better than your demonstrator,
which I think is good, because often,
like if your demonstrator comes from YouTube, um, uh,
which is nice, so that's a freely available place to get demonstrations.
Um, you don't actually know the quality,
so often you might want to use that to sort of
bootstrap learning but not necessarily be limited by it.
All right, so just to summarize,
um, you know, in practice,
there's been an enormous amount of work on imitation learning,
particularly in robotics, uh,
but in lots of domains.
Um, and I think that, you know,
if you're gonna leave class today and go out into industry,
um, that imitation learning, uh,
can be very useful practically, uh,
because often it's easier to get demonstrations and it can really bootstrap learning,
um, uh, complicated Atari games, et cetera.
Um, but there's still a lot of challenges that remain, uh,
particularly, in a lot of the domains
that I think about we don't know the optimal policy.
Um, so, I think about, er,
healthcare or like customers or,
um, education like intelligent tutoring systems,
and all of those one of the big challenges is that you don't
know the optimal policy and you're maybe
doing all this because you think you could do
something better than what's in the existing data.
Um, so, that's, that's a big challenge.
Um, and how do you combine sort of inverse RL?
Ah, and maybe online RL in a, in a safe way.
So, one of the motivations I said for imitation learning was,
oh well if you want to be safe, um,
but then if your, if your- the only safe things right now don't do very well,
then you have to figure out how to do safe exploration in the future.
All right. I think that's everything for today,
and I'll see you guys next week where we're gonna start to talk
about policy search [NOISE].
 All right. We're gonna go ahead and get started.
Um, homework two, it should be well underway.
If you have any questions feel,
feel free to reach out to us.
Um, [NOISE] project proposals,
if you have questions about that,
feel free to come to our office hours or to reach out, um, via Piazza.
Somebody have any other questions I can answer right now?
All right. So today,
we're gonna start- this is a little bit loud.
Um, today we're gonna start talking about policy gradient methods.
Um, policy gradient methods are probably the
most well used in reinforcement learning right now.
Um, so, I think they're an incredibly useful thing to be familiar with.
Um, [NOISE] whenever we talk about reinforcement learning,
we keep coming back to these main properties that we'd like
about agents that learn to make decisions about them, you know,
to do this sort of optimization, handling delayed consequences,
doing exploration, um, [NOISE] and do it all through statistically,
and efficiently, in really high dimensional spaces.
Um, and what we were sort of talking about last time in terms of imitation learning was
sort of a different way to kind of provide
additional structure or additional support for our agents,
um, so that they could try to learn how to do things faster.
Um, and imitation learning was one way to provide
structural support by leveraging demonstrations from people.
And we've seen other ways to sort of, um,
encode structure or human prior knowledge,
um, when we started talking about function approximation.
So, when we think about how we define q,
like when we define q as s, a,
and w, where this was a set of parameters.
We were implicitly making a choice about sort of
imposing some structure in terms of how we are going to represent our value function,
and that choice might be fairly strong like assuming it was linear.
So, this is sort of a quite a strong assumption,
or it might be a very weak assumption like using a deep neural net.
And so, when we specify sort of these function approximations, and representations,
we're sort of implicitly making, uh, uh,
choices about how much structure and how much domain knowledge we want to put in,
um, in order for our agents to learn.
So, what we're gonna start to talk about today and we're gonna talk about this week is
policy search which is another place
where it can be very natural to put in domain knowledge.
I mean, we'll see that in in some robotics examples today,
and it can be also a very efficient way to try to learn.
So, as I was saying,
before we sort of we're approximating where we're
doing model-free reinforcement learning,
and when we started to try to scale up to really large state spaces.
Um, I've been having several different people ask me about
really large action spaces which is a really important topic.
We're not gonna talk too much about that in this quarter,
but we will talk a little bit about when
your action space is continuous but low-dimensional.
But we have started to talk a lot about when
the state space is really high-dimensional and,
and, and really large.
And so, we talked about approximating things, um, uh,
with some sort of parameterization,
like whether it will be parameters Theta or we often,
or we often use w, but some sort of parameterization of the function.
So, we used our value function, um,
to define expected discounted sum of rewards from a particular state or state action,
and then we could extract a policy from
that value function or at least from a state action value function.
And instead, what we're gonna do today is just directly parameterize the policy.
So, when we talked about tabular policies,
our policy was just a mapping from states to actions.
And in the tabular setting,
we could just look- do that as a lookup table.
For every single state, we could write down what action we would take.
And what we're going to do now is to say, "Well,
it's not gonna be feasible to write down our table of our policy,
so instead what we're going to do is parameterize it,
and we're gonna use a set of weights or Thetas."
Today, we're mostly gonna use Thetas,
but this could equally well think of this as weights.
Um, just some way to parameterize our policy.
We'll talk more about particular forms of parameterization.
Um, but just like what we saw for state action value functions, um,
this is gonna have a big implication because this is effectively
defining the space that you can learn over.
So, it's sort of, um, it's determining the,
the class of policies you could possibly learn.
Um, [NOISE] and we're again gonna sort of focus on model-free reinforcement learning,
meaning that we're not gonna assume that we have access to
an a priori model of the dynamics or reward of the world.
So, we had thrown some of these diagrams up at the start of the quarter,
I just want to go back to it.
Um, we've been talking about sort of value,
we- we haven't talked so much about models,
the models are also super important.
Um, but we've been talking a lot about sort of value function,
based approaches which is this,
and now we're gonna talk about policy,
um, direct policy search methods.
And as you might expect,
there's a lot of work which tries to combine between the two of them,
and these are often called actor-critic methods.
Um, where you try to explicitly maintain a parameterized policy,
and explicitly maintain a parameterized critic or value function.
So, this is the policy, and this is a Q.
Okay, so, we're gonna start today and we're gonna be talking about policy-based methods.
So, why would you wanna do this?
Um, [NOISE] well, uh,
it actually goes back a little bit to also what we were
talking about last week with imitation learning.
For imitation learning, we talked about the fact that
sometimes it's hard for humans to write down a reward function,
and so it might be easier for them just to demonstrate what the policy looks like.
Similarly, in some cases,
maybe it's easier to write down sort of a parametrization of, um,
the space of policies than it is to write down
a parameterization of the space of state action value functions.
Um, in addition, they're often
much more effective in high-dimensional or continuous action spaces,
and they allow us to learn stochastic policies which we haven't talked very much about so far,
but I'm gonna give you some illustrations about where
we definitely want stochastic policies.
Um, [NOISE] and they sometimes have better convergent policy- convergence,
uh, properties um, that can be a little bit debated,
it depends exactly what- whether we're comparing that to model-free
or model-based approaches and how much computation we're doing.
Um so, this can be a little bit of a function of computation to computation can matter.
One of the really big disadvantages is that they are
typically only gonna converge to a local optimum.
So, where you're going to converge to something that is hopefully a pretty good policy,
but we're not generally guaranteed to converge to the global optima.
[NOISE]. Now, there are some techniques that are
guaranteed to converge to a local- to the global optima,
and I'll try to highlight some of those today,
but generally, almost all of the methods that you see in like
deep reinforcement learning that are policy gradient based,
um, only converge to a local optima.
Um, and then the other challenge is that typically we're
gonna do this by trying to evaluate a policy and then estimate its gradient,
and often that can be somewhat sample inefficient.
So, there might be quite a lot of data to estimate,
um, what that gradient is when we're taking a gradient-based approach.
So, why might we want sort of a stochastic policy?
So, in what I mentioned before, um,
in the tabular setting,
so let me just go back to here.
So, in a- now,
why do we want this? Do we want this?
If you think back to the very first lectures, um,
what I said is that if we have a tabular MDP,
there exist a Pi which is deterministic and optimal.
So, in the tabular MDP setting,
we do not need, um, er,
deter- we do not need stochastic policies because there always exists
a policy that is deterministic that has the same value as the optimal policy does.
So, this is not needed in the tabular Markov Decision Process case,
but we don't always- we're not always
acting in the tabular Markov Decision Process case.
So, as an example, um,
[NOISE] who here is familiar with rock-paper-scissors ?
Okay. Most people. Um, er,
possibly if you're not, you might have played it by another name.
So, in rock-paper-scissors, uh,
it's a two player game, um,
[NOISE] everyone can either pick,
uh, paper or scissors or rock.
And you have to pick one of those,
and scissors beats paper,
paper- rock beats scissors, and paper beats rock.
Um, and in this case,
if you had a deterministic policy,
you could lose a lot,
you could easily be exploited by the other agent.
Um, but a uniform random policy is basically optimal.
What do I mean by optimality?
In this case, I mean, that you,
you could say a plus one if you win,
and let say zero or minus one if you lose.
We're not gonna talk too much about multi-agent cases,
um, er, in this class, but it's a super interesting area of research.
Um, and in this case,
um, you know, the environment is not agnostic.
Um, the environment can react to, uh,
the policies that we're doing and could be adversarial,
and so we want a policy that,
um, is robust to an adversary.
So, a second case, um, is Aliased Gridword.
So, um, so in this case,
so why, you know,
why is being stochastic important here?
Well, because we're not really in a stochastic setting,
we are in an adversarial setting,
and we have another agent that is playing with us and they can be
non-stationary and changing their policy in response to ours.
Um, so it's not, uh,
the environment doesn't pick the next- doesn't
pick rock-paper-scissors regardless of our actions,
um, in the past it can respond to those.
[NOISE] Um, so it's sort of got this non-stationarity or adversarial nature.
Um, another case is where it's not Markov,
so it's really partially observable,
and you have aliasing,
which means that we can't distinguish between multiple states in terms of our sensors.
So, we saw this before a little bit when we talked about robots that, you know,
could have laser range finders and sort of tell where they were in a hallway.
By how far away each of the,
um, the, the first point of,
um, uh, uh, obstacle was for all of their 180 degrees,
and so that will look the same in lots of different hallways.
Um, so this is a simple example of that.
So, in an Aliased Gridword, um,
let's say that the agent because of their sensors cannot distinguish
the gray states and they have features of a particular form.
Um, they have a feature for whether or not there's a wall to the North,
um, er, or East,
or South, or West.
So, it can basically, like, if it's here it can tell, like,
"Oh I have walls to either side of me and not in front or behind me."
Um, but that could be the same over in that in the,
in the other [NOISE], um, grey state.
So, if we did
a value-based reinforcement learning approach
using some sort of approximate value function,
um, it would take these features which
are a combination of what action am I going to take?
And whether there are walls around me or not.
Um, or we could have a policy-based approach which also, um,
takes some of these features but then just directly
tries to make decisions about what action to take,
and those actions can be stochastic.
So, in this case, the agent is trying to figure out how to navigate in this world.
It really wants to get to here.
This is where there's a large reward. So, this is good.
It wants to avoid the skull and crossbones,
and those will be negative reward.
So, because of the aliasing,
the agent can't distinguish whether or not it's here or here.
Um, and so it has to do the same thing in both states.
And so either it has to go left or it has to go right,
Call it West or East [NOISE], um,
and either way that's not optimal because if
it's actually here it should be going that way,
not over here and down.
Um, and so it can distinguish whether it's in here or
here but it could just end up moving back and forth,
er, or making very bad decisions.
And so it can get stuck and never be able
to know when it's safe to go down and reach the money.
So, it learns a near-determini- deterministic policy
because that's what we've normally been learning with these,
um, and whether it's greedy or e-greedy and generally it will do very poorly.
But if you have a stochastic policy when you're in a state where you're aliased,
you could just randomize.
You'd say, "I'm not sure whether I'm actually in this state- in this state or this state,
um, so I'll just go, er,
either East or West with 50 percent probability."
And then it'll generally reach the goal state quickly.
Because note, it can tell what it should do when it reaches
here because that looks different than these two states.
So, once it's in the middle it knows exactly what to do.
So, that's just, again, an example where a stochastic policy has a way better value than
a deterministic policy and that's because the domain
here is not Markov, it's partially observable.
[NOISE] Okay.
So, that's sort of one of the reasons why we might want to- some of
the reasons why you want- might wanna be directly policy-based,
and there's a lot of other reasons.
Um, so, so what does this mean?
Well, er, we're gonna have
this parameterized policy and the goal is that we wanna find. Yeah,
Like you said, can we conclude that when the world is not
Markov, it is partially observed, stochastic policy is always better?
Your name is ? I'm sorry yeah.
So what said is can we conclude that, um,
if the world is partially observable stochastic policies are always better.
Um, I think it depends on the modeling you wanna do.
I think, in this case,
better than being stochastic because it's still doing something,
kind of, not very intelligent in the gray states, it's just randomizing,
would be to have a partially observable Markov decision process policy, um,
and then you could track,
uh, an estimate over where you are in the world.
So, you can keep track of a belief state over what state you're in,
and then you could hopefully uniquely identify that,
"Oh, if I was just in this state I have to be in the state now."
And then you can deterministically go to the right or left.
[NOISE] So, it depends on,
on the modeling one's willing to do. Good question.
Okay. So, when we, um,
start to do the- go to parameterize policy search,
what we're gonna wanna do is find the parameters that yield the best value.
The policy in the class with the best value,
and so similar to what we've seen before we can,
we can think about sort of episodic settings and infinite sort of continuing settings.
So, in an episodic setting,
that means that the agent will act for a number of time-steps often,
let's say, H steps.
But it could be variable, like,
it might be until you reach,
you know, a terminal state.
And then we can just consider what is the expected value,
wha- wher- what is the value?
What is the expected discounted sum of rewards we get from
the start state or distribution of start states?
And then what we wanna do is find the parameterized policy that has the highest value.
Um, another option is that if we're in
a continuing environment which means we're in the online setting,
we don't act for H steps we just act forever.
There's no terminal states and we can either use the, um,
average value where we average over the distribution of states.
So, this is, um,
like what we saw before thinking about the distribution,
the stationary distribution over the Markov chain that is induced by a particular policy.
Because we talked about before about the fact that if you fix the policy,
um, then basically, uh,
you get into Markov reward process.
You can also just think of the distribution of states you get is a Markov chain.
So, um, if we're acting forever,
we're gonna say sort of on average what is
the value of the states that we reach under that stationary distribution?
Um, and another way to do it is also to say we
just look at sort of the average reward per time step.
Now, for simplicity today we're gonna focus almost exclusively on the episodic setting,
but we can think about similar techniques for these other forms of settings.
So, as before and this is an optimization problem similar
to what we saw in the value function approximation case, uh,
for linear value functions and using deep neural networks, um,
we're gonna wanna be doing optimization, er,
which means that we need to do some sort of
optimization tool to try to search for the best data.
So, one option is to do gradient free optimization.
We don't tend to do this very much in policy search methods,
um, but there are lots of different methods that are gradient free optimization.
Just for us to find whatever parameters maximize this V Pi Theta.
Um, and just to connect this- just like what we saw for Q functions,
now we have Theta which is specifying a policy.
And it maybe has some interesting landscape,
and then we wanna be able to find where's the max.
So, we're really trying to find the max of a function as efficiently as we can.
And there are lots of methods for doing that that
don't rely on the function being differentiable.
Um, and these actually can be very good in some cases.
Um, so this is some nice work done by a colleague of mine and-
We have developed a method for automatically identifying
the exoskeleton assistance patterns that minimize metabolic energy costs for
individual humans during walking [NOISE].
During optimization the user first experiences
one control law while respiratory measurements are taken.
Steady-state energy cost is estimated by fitting
a first-order dynamical model to two minutes of transient
data. The control law is then changed and metabolic rate is estimated again.
This process is repeated for a prescribed number of control laws forming one generation.
[NOISE]
A covariance matrix adaptation evolution strategy
is then used to create the next generation.
The mean of each generation represents
the best estimate of the optimal control parameter values.
After about an hour of optimization,
energy cost was reduced by an averageg
of 24 percent, compared to no assistance.
So this is work that's done by my colleague uh, Steve Collins, um,
who's over in mechanical engineering and we've been collaborating
some on whether you can train people to- do this better, um.
So, the idea in this case is that, uh,
there's lots of instances for which you'd like to use exoskeletons.
Um, a lot of people have strokes,
a lot of people have mobility problems um,
and of course there's a lot of veterans that lose a limb.
Um, and in these cases one of the challenges has been is how do you,
sort of figure out what are the parameters of these exoskeletons in order to provide um,
support for people walking and generally it
varies on physiology and for many different people.
They're going to need different types of parameters,
um, but you want do this really quickly.
So, you want to be able to figure out very fast for each individual.
What is the right control parameters in order to
help them get the most assistance as they walk.
Um, and so Steve's lab treated this as,
uh, sort of a policy,
a policy search problem,
where what you're doing is you're having somebody wear their device,
you're trying some, uh, control laws,
um, that are providing a particular form of support in terms of their exoskeleton.
You're measuring their sort of, um,
metabolic efficiency, which is, how do you- you know.
How hard are they breathing? How hard are they having to work,
compared to if they weren't wearing this or under different control laws.
And then you can use this information to figure out
what's the next set of control laws you use and do this all,
in a, closed loop fashion as quickly as possible.
Now one of the reasons I bring this up is,
both because it was incredibly effective,
it's a really nice science paper that, um,
illustrates how this could be much more effective than previous techniques.
Um, and second because it was using CMA-ES,
which is a gradient free approach.
So even though most of what we're gonna discuss today in class,
is all with gradient based methods.
There's some really nice examples of not using gradient based methods
also to do policy search for lots of other types of applications.
So, I think it's useful to sort of,
know in your toolbox that,
one doesn't have to be constrained to gradient based methods,
and one of the really nice things about,
things like CMA-ES is that,
they're guaranteed to get towards,
uh, to a global optima.
So in some cases, uh,
you might really want to be guaranteed that you're
doing that because it's high stakes situation.
Um, and in general, it sort of is,
has been noticed repeatedly recently that sometimes these sort of
approaches do work kind of embarrassingly well, um,
uh, that they tend to be in some ways sort of a brute forced,
a smart brute force way,
um, that often can be very effective.
So they're good to consider,
in terms of the applications you look at.
But, you know, despite this,
um, uh, even though,
they can be really good and sometimes, um,
they're very, very helpful for parallelization.
Um, uh, they're generally not very sample efficient.
And so depending on, the domain that you're
looking at and what sort of structure you have,
often it's useful to go to a gradient-based method,
particularly if you might be satisfied with the local solution at the end.
Sort of locally optimal.
So what we're going to talk about mostly today- just like what we did for,
um, value, like, uh, value-based methods is,
gradient descent, um, and gradient based methods,
um and other methods that try to exploit
the sequential structure of decision making problems.
So CMA-ES doesn't know anything about the fact that this- the world might be
an MDP or any form of sort of, sequential stochastic process.
And we're gonna focus on ones that sort of,
leverage the structure of the Markov decision process,
in the decision process itself.
So let's talk about policy gradient methods.
Um, where again just sort of,
um, define things in terms of theta,
so that we're explicit about the parameter and we're
gonna focus on episodic MDPs, which means that,
we're gonna run our policy for a certain number of time steps,
until we reach a terminal state or for certain you know, maybe h steps.
Uh, we're going to get some reward during that time period,
and then we're going to reset.
So, we're going to be looking for a local maximum,
and we're going to be taking the gradient,
with respect to the parameters that,
um, define the policy,
and then use some small learning rate [NOISE].
So just this is- this should look very similar,
very similar to the,
similar to, uh, Q and V based search.
And the main difference here is,
that instead of taking, uh,
the derivative with respect to parameters that define our q function,
we're taking them with respect to,
the parameters that define our policy.
So, the simplest thing to do here,
is, um, to do finite differences.
Um, so for each of your policy parameters,
you just perturb it a little bit, um,
and if you do that for every single one of the dimensions, um,
that define your policy parameters,
then you're going to get an estimate of the gradient.
Here, just doing sort of a finite differences estimate of the gradient.
And you can use a certain number of evaluations for doing this, in each of the cases.
So you can- let's say you have this,
um, k dimensional, uh,
set of parameters that defined your policy,
you try changing one of them a little bit,
you repeat it, you get a bunch of samples for that, new policy.
Um, you do that for all of the different dimensions,
and now you have an approximation of the gradient.
It's very simple, it's quite noisy, um,
it's not particularly efficient,
but, it can sometimes be effective.
I mean it was one of the earlier demonstrations
of how policy gradient methods could be very useful,
in an RL context.
Um, and the nice thing is that the policy itself
doesn't have to be differentiable because,
we're just doing sort of a finite difference approximation of the gradient [NOISE].
So, one of the first examples that- I see- well,
um, I think of when- I think of,
sort of how policy, uh,
gradient methods or how policy search methods can be really effective,
is Peter Stone's work on doing,
uh Robocup and who here's ever seen- like Robocup?
Okay. A few people but not everybody.
So let's see if we can, get up like a
short demonstration of like what these robots look like.
So, let's- ah, okay.
So you probably can't see it do you?
We won't do that right now, um,
but essentially what you have is,
there's a bunch of different leagues of Robocup.
One of the goals has been that, um,
I think by 2050,
the goal is that, we're going to have, uh,
a robotic soccer, uh,
team that is going to be able to defeat- like able to,
you know, win the, the World Cup.
Um, so that's been one of the driving goals,
of this Ro- the Robocup initiative.
Uh, and there's lot of different leagues within this, and one of them is,
these sort of quadruped robots,
um, which try to score goals against each other.
And one of the key challenges for this is,
they look kind of like that.
Um, and, you have to figure out the gait for walking, um,
and you want them to be able to walk,
quickly but you don't want them to fall over.
Um, and so just simply that question of like,
how do you optimize the gait,
is an important question in order, to win,
because you need your robots to move fast on the field.
So, Peter Stone has been a leading person in,
in Robocup for a long time.
Um, and their goal was simply to,
learn a fast way for these AIBOs to walk.
Um, and to do it by,
uh, real experience um,
and data's really important here because it's expensive um,
you have these robots walking back up forth and you
want them to very quickly, optimize their gait.
Um, and you don't want to have to keep changing batteries and things like that,
so you really wanna do this with very little amounts of data.
So, what they thought of doing in this case is sort of to,
to do a parameterized policy and try to optimize those proper policies.
So this is where significant domain knowledge
came in and this is a way to inject domain knowledge.
So they, um, specified it by this sort of continuous ellipse,
of how gait works for,
um, the small robot.
And so they parameterized it by these 12 continuous parameters.
And this completely defines the space of possible policies you could learn.
This might not be optimal.
Peter Stone and his group have a huge amount of experience on doing Robocup,
um, at the time they were doing this paper and so
they really had a lot of knowledge they could inject in here.
And in some ways it's a way to provide sort of this hierarchical structure about,
what sort of policies might be good.
And then what they did is they did just this method of finite differencing,
in order to try to optimize for all of these parameters.
[NOISE] So, one of the important things here, um,
is that all of their policy evaluations were going to be done on actual real robots,
um, and they just wanted to,
have people inter- intervene every once in a while,
in order to, replace batteries which took- happened about once an hour.
Um, and so they did it on three AIBOs,
very small amount of hardware.
Um, they did about 15 policies per iteration,
and, they evaluated each policy three times.
So, it's not very many but it can be a very noisy signal,
um, and each iteration took about 7.5 minutes.
So- and then they had to pick some learning rate.
And so what do we see in this case?
Well, we see that,
in terms of the number of iterations that they have versus how
quickly they're- of course you have to define your optimization criteria in this case,
they're looking at speed of stable walking.
Um, and a lot of people have been trying to
figure out how to do these using hand tuning before,
um, uh, including, so they're the UT Austin Villa team.
Um, including, them- in the past people have found different ways to sort of hand tune,
um, I don't know if we'll be using unsupervised learning et cetera.
And you can see, as they do multiple iterations of trying to,
search for a better policy using this finite difference method,
that they get to faster than everything else.
And this is not that many iterations, um,
so this is something that was happening over,
you know, a few hours.
So, I think this was a really compelling example of how,
policy gradient methods could really do much better than what we-
had happened before and they didn't have to require an enormous amount of data.
That's very different than probably what you're experiencing in assignment two,
so this is, no total number of iterations.
Um, uh, I think this was on the order of, let's see,
like, this is on the order of, you know,
tens to hundreds of policies,
not millions and millions of steps.
So these things can be very data efficient.
But there was also a lot of information that was given.
So, um, if you think about sort of like,
uh, I have a little bit on here.
So, in their paper they discussed
sort of what was actually impacting performance in this case,
and there are a lot of things that impact performance.
So, um, you know,
how do we start?
Um, so I may have a sense of why, you know,
why does the initial policy parameters used matter for this type of method? Yeah,
Well, because we're not guaranteed to have a global optima,
only a local optima, so your starting point is gonna
affect which local optima you are able to find.
Exactly. So what just said is that,
um, because these methods are only guaranteed,
particularly this method is only guaranteed, to,
find a local optima and all of the sort of policy gradient style methods are.
Um, then wherever you start you're gonna get to
the closest local optima and you have no guarantee that that's the best global optima.
Um, so it's important to either try lots of
random restarts here in this case or to have domain knowledge.
Um, another important question here is,
how much you're perturbing sort of the size of your finite differences.
And then I think, really most critical is this policy parameterization.
Like just how are you writing down the space of possible policies
that you can learn within because like, if that's not a good policy space,
then you're just not going to learn anything.
Um, yeah.
on slide 26
What is an open loop policy can you explain a bit more on that.
Yeah. Um, uh, so,
question was about the open loop policy part.
So, these policies that we're learning don't have to be adaptive.
And open loop policy is essentially a plan.
It's a sequence of actions to take, um,
regardless of any additional input that you might have.
So, um, we typically have been thinking about policies as mappings from states to actions,
but they can also just be a series of actions.
And so, when we talk about an open loop policy,
that's a non-reactive policy because it's
just a sequence of actions that regardless of the state of the robot you just keep going.
So, maybe there's a really large wind in the middle,
and the robot's next action is the same whether there's a lot of wind or not.
It doesn't have to be reactive.
Okay. So, but in general,
um, you know, finite differences is a reasonable thing to try.
Um, often we're gonna want to use gradient information and leverage
the fact that our policy for function is actually differentiable.
So, what we're gonna do now is, um,
compute the policy gradient analytically [NOISE] excuse me.
This is most common, um,
in most of the techniques that are used right now.
Um, we're gonna assume it's differentiable wherever it is non-zero,
um, and that we can explicitly compute this.
So, when we say, what we- when we say know that means that this is computable.
And we can compute this explicitly.
And so, now we're gonna be, um,
thinking only about gradient-based methods.
And so, we're only,
[NOISE] we're gonna only converge to a local optima.
Hopefully, hopefully, we'll get to a local optima,
that's the best we can hope for in this case.
Okay. So, we're going to talk- people often talk about likelihood ratio policies,
um, and they're gonna proceed as follows.
So, let- we're thinking about the episodic case.
So, we're gonna think about it as having, um, trajectories.
So, state action reward,
next state, et cetera,
all the way out to some terminal state.
So, this is where we terminate.
And we're gonna use R of Tau to denote the sum of rewards for a trajectory.
Okay. So, the policy value in this case
is just gonna be the expected discounted sum of rewards we get by following this policy.
And we can represent that as the probability that we
observe a particular trajectory times the reward of that trajectory.
So, it just says given under this policy what are the, you know,
what's the probability of seeing any trajectory,
and then what would be the reward of that trajectory?
Because the reward is the deterministic function of the trajectory.
Once you know the state action rewards, et cetera,
then your reward is,
um, just the sum of all of those.
And so now, in this particular notation,
what our goal will be is to find policy parameters Theta,
which, um are the arg max of this.
Uh, and the reason we sort of- what have we changed here, um,
the change now then has been the fact that we've gonna focus on here.
So, notice now that the policy parameters only appear in terms of
the distribution of trajectories that we might encounter under this policy.
And this is, again, a little bit similar to what we talked
about for imitation learning before or where in imitation learning,
we talked a lot about distributions of states and distributions of states and actions,
and trying to find a policy that would match the same state action distribution,
as what was demonstrated by an expert.
Um, today, we're not gonna talk as much about
sort of state action distributions but we are talking
about sort of distributions of
trajectories that we could encounter under our particular policy.
So, what's the gradient of this?
Um, so, we wanna take the gradient of this function with respect to Theta.
So, we're gonna go for this as follows.
We are gonna rewrite what is the probability of a trajectory under Theta.
So, sum over Tau.
I wanna do probability of Tau [NOISE] times.
All right, first actually I'll whip it in here.
And then what we're gonna do is,
make sure I get the notation the same.
Okay.
So, then what we're gonna do is,
we're gonna do something simple where we just multiply and divide by the same thing.
So, we're gonna put in probability of Tau given Theta,
divided by probability of Tau given Theta,
times the derivative of the probability of Tau given Theta.
And the probability- if we instead had a log.
So, if you're taking the derivative of log of probability of Tau given Theta,
that is exactly equal to the one over the probability
of Tau given Theta times the derivative of p of Tau given Theta.
So, we can re-express this as follows;
Sum over Tau r of Tau,
p of Tau given Theta times derivative with respect to log of p of Tau given Theta.
Now, so far that doesn't necessarily seem like that's gonna be very useful.
Um, [LAUGHTER] So, we've done that,
that's a reasonable transformation,
but we'll see shortly why that transformation is helpful.
And in particular, the reason this transformation is
helpful is it's gonna be very useful when we think
about wanting to do all of this without knowing the dynamics or reward models.
Um, so, we're gonna need to be able to, you know,
get reward in terms of, uh, a trajectory,
but we wanna be able to evaluate, um,
the gradient of a policy without knowing the dynamics model,
and this trick is gonna help us get there.
So, when, when we do this, this is often,
this is often referred to as the likelihood ratio.
And we can convert it and just say, "Well,
we noticed that by doing this,
this is actually exactly the same as the log."
Now, why else does this start to look like something that might be useful?
Well, what do we have here?
We have, if we- this is the sum over all trajectories.
Of course, we don't necessarily have access to all possible trajectories,
but we can sample them.
So, you could imagine starting to be able to approximate this
by running your policy a number of times,
sampling a number of trajectories,
looking at the reward of those,
um, and then taking the derivative
with respect to this probability of trajectory given Theta.
So, typically we're gonna do this by just running the policy m times.
Um, and then, that p of the- Tau given Theta,
we're gonna just approximate that by the following.
So, that part drops out,
we're just gonna weigh all of the trajectories that we got during our sampling uniformly,
and then we look at the reward of that trajectory,
and the log of p of, um,
[NOISE] uh, Tau given Theta.
So, what is happening in this case?
Okay. So, this is saying that the gradient is this sort of,
um, [NOISE] uh, the reward that we get
Um, times the log of the probability
of that trajectory for the reward with associated word times Theta.
So, what's happening in that case?
So, in this case,
we have a function which for our case is the reward,
which is sort of measuring how good, um,
that particular, um, you know,
trajectory is or how good that sample is.
And so, what this is doing is we're just moving up and
the trajectory of the log probability of that sample based on how good it is.
So, we wanna sort of push up our parameters,
that, um, are responsible for us getting samples which are good.
So, um, we want to have parameters in our policy
that are gonna cause us to execute trajectories that give us high reward.
So, if we think of just sort of here f of x again is the reward.
And we are- this is gonna be our policy or parameterized policy.
We want to increase the weight of things in our space that lead to high reward.
So, if this is our f of x,
which is our reward function and this is the probability of our trajectories,
then we wanna reweight our policy to try to
increase the probability of trajectories that yield high reward.
So, you would end up having
larger gradients towards things that have high value, high reward.
Okay. So, then the next question is,
if I'm gonna do this then I have to be able to approximate the second term,
which is this log.
You know, the derivative with respect to the probability of a trajectory,
um, under some parameters.
So, I have to be able to figure out what is
the probability of a trajectory under a set of parameters,
and we can do that as follows.
And so, this is gonna be Delta Theta of
log of the pro- of Mu of S_0.
So, this is our initial starting state,
the probability of our initial starting state,
times the product over j equals 0 to t minus 1 of the probability of
observing the next state given the action that was taken,
times the probability of taking that action under our current policy.
So, there's like another bracket at the end.
Um, and so, since this is log,
we can just decompose this.
So, this is gonna be equal to Delta Theta of log of
Mu of S naught plus Delta Theta sum over Delta Theta because it's a log term,
of J equals 0 to t minus 1 of log of [NOISE] the transition model.
Remember, we don't know this in general. This is unknown.
You're just gonna hopefully end up with
an expression which means we don't need to have it.
Um, and what i is indexing here is which trajectory we're on.
Add sum over J equals 0 to t minus 1.
And this is gonna be our actual policy parameters.
All right, can anybody me why this is a useful decomposition?
And whether or not it looks like we're gonna need to, so,
let me just parameterize all these things,
um, does this look hopeful in terms of us not needing to know what the dynamics model is?
[inaudible]
How about everybody just take a second,
talk to your neighbor and,
um, then tell me which of these terms are gonna be zero.
So we're taking the derivative with respect to Theta.
And which of these terms depend on Theta
[OVERLAPPING].
Remember Theta is what determines your policy parameters.
Theta is what determines what action you take in a given state.
All right I'm gonna do a quick poll, um,
so I'm gonna call these items one, two and three.
Does the first term depend on Theta?
Raise your hand if yes, raise your hand if no.
Great, okay, yeah. So this is independent Theta. So this is gonna be zero.
Raise your hand if the second term is independent of Theta.
Great, so this goes to zero.
So the only thing we have less is this, which is great.
So, um, the nice thing and so now it sort of becomes
more clear why we did this weird log, um, transformation,
because when we did this weird log transformation,
it allowed us to take this product of
the probability of the action that we took in the state transitions,
then instead we can decompose it into sums.
And now once we see that we decompose it into sums,
we can apply the derivative separately and that means some of these terms
just directly disappear which is really cool.
So, it means that we don't actually need to know what the transition model is.
Um, we don't need to have a explicit representative.
Yeah question and name first please.
And the question is, I was wondering,
doesn't the dynamics of the system depend on the policy though, um, in general?
Great question. So question is,
does the dynamics of the system depend on the policy?
Absolutely, but only through this part.
So, it's like um,
the agent gets to pick what action they take,
but once they pick that,
the dynamics is independent of the agent and so it's this de-coupling.
So, if you have a different policy,
you will absolutely get different trajectories
but the way in which you get different trajectories,
um, is only affected by the policy in terms of the actions that are
selected and then the environment will determine sort of the next states that you get.
And so we don't need to know that in terms of, um,
estimating the impact of the actions on the environment.
It will also come through in terms of the rewards you get.
Because the rewards you get are also a function of the state.
So you'll absolutely visit different parts of the state depending on the actions you
take. Any other questions?
I'm and I just want to make sure I understand how I get
the estimate for the probability of how given Theta, I mean,
most likely we are just saying if we took m episodes and this one showed up,
you know, i times it's gonna be i over m. It is that what we are doing here is correct?
Great question. So, is asking, um, you know,
this is, what I put here is
just one of those internal terms that would cover one i, yes,
So, what we're doing here is, we're saying, we're gonna take this policy.
We're gonna run it m times.
We're probably not gonna get any trajectories that are identical.
And what we're gonna do is compute this log of
the probability of trajectory for each of those separately.
And then sum them up.
You might end up with that, I mean,
you know in deterministic cases you might, um,
if your domain doesn't have a lot of stochasticity and neither does your policy,
you might end up with multiple trajectories that are identical.
In general your trajectories will be completely different.
And so will your estimate of their local gradients.
So, this is really nice.
We're gonna end up with this situation where we only have to be able to have
an analytic form for the derivative of our policy with respect to our parameters.
So, we still need and we'll talk about this a little bit more later. We still need this.
We have to evaluate this.
This is about how we parametrized our policy.
And if we want this to be analytic,
we need to have parametrized our policy in a way that we can
compute this exactly for any state and action.
So, we'll talk more about some ways, you know,
some policy parameterizations which make this computation analytic and nice.
In other cases you might have to estimate this thing itself by,
you know, brute force or computation or finite differences or something.
But if we choose a particular form of parameterized policy,
then this part is gonna be analytic.
So, another thing is I don't find this, um,
er, I don't find
this additional terminology particularly helpful but it's used all over the place.
So I wanna introduce it which is people often call,
um, this part a score function.
Just the score function which is not particularly helpful,
I think but nevertheless is often used is called this.
So that's the quantity that we were just talking about needing to be able to evaluate.
So this really gets into,
um, er, well, we'll write it out again.
So, um, when we take the derivative of the value function we approximate that by getting
m samples and we sum
over i equals one to m. And we look at the reward for that treject- um,
trajectory and then we sum
over these per step score functions.
Can everybody read that in the back?
Yeah. Okay great. Yeah so these are sort of our score functions.
And these are our score functions, um,
that can be evaluated
over every single state action pair that we
saw and we do not need to know the dynamics model.
So, the policy gradient theorem slightly generalizes this.
How is it gonna generalize this?
Note in this case, what we're doing here is we're- this is for the episodic setting.
And this is for when we just take our raw, our raw reward functions.
So we look at the sum of rewards for that trajectory and then, um,
we weigh it by this sort of derivative with respect to our policy parameters.
Um, it turns out that we can also slightly generalize this.
And let's say I'm gonna call,
so this is a value function,
um, let's say that we had slightly different objective functions.
We talked before about how we could have episodic reward
or average reward per time step or average value.
So, we could either have our objective function be equal to
our normal value for episodic or we could have it equal to
what I'm gonna call J AVR which is average reward per time
step or we could have it as average value.
Let's say we're always continuing and we want to
average over the distribution of states that we encounter.
So we can think about that. It's a good scenario too.
It turns out that on all of those cases you can do
a similar derivation to what we did here for the episodic case.
And what we find is that we have the derivative of our objective function,
which now can be kind of any one of these different objective functions,
is equal to the expected value under that,
the current policy of the derivative with respect to
just those policy parameters times Q.
And Sutton and Barto in Chapter 13 which we also reference on the schedule, um,
have a nice discussion about a number of these different issues, um,
and so again, we're not gonna talk too much about
these slightly other different objective functions but just know that
this all can be extended to the continuing case.
Okay, so what we've said here so far is that we have
this approximation where what we do is we just take our policy,
we run it out phi m times,
for each of those m times we get a whole sequence of states and actions and rewards.
And then we average. And this is
an unbiased estimate of the policy gradient but it's very noisy.
So, this is gonna be unbiased and noisy.
If you think about what we saw
before for things like Monte Carlo methods,
it should look vaguely familiar,
same sort of spirit, right?
We have, um, we're just running out our policy.
We're gonna get some
sum of rewards just like what we got in Monte Carlo, um, estimates.
But, [NOISE] so, it'll be unbiased estimate of the gradient.
So, it's unbiased estimate of the gradient,
estimate of gradient. But noisy.
So, what can make this actually practical?
Um, there's a number of different techniques for doing that.
Um, but some of the things we'll start to talk about today
are temporal structure and baselines.
[NOISE] Okay. So, how do we fix this?
I'm gonna start to look at, you know, fixes, [NOISE] uh,
temporal structure and baselines.
And before we keep going on this, um,
based on what I just said in terms of Monte Carlo estimates,
um, what are some of you guys' ideas for how we
could maybe reduce the variance of this estimate?
Based on stuff we've seen so far in class.
Like, what are the alternative to cut up Monte Carlo methods? Yeah?
We could use bootstraps.
Um, can I get your name first.
Oh, I'm .
? Yeah. What said is exactly, right.
So, said we could use bootstrapping.
Yeah. So, we've repeatedly seen that, um,
we have this trade off between bias and variance,
and that bootstrapping, um,
like temporal difference methods that we saw in Q-learning that you're doing in DQN,
can be helpful in, um,
reducing variance and, and speeding the spread of information.
So, yeah. So, we could absolutely do things like bootstrapping, um,
to kind of replace R with something else or use,
um, a covariate in addition to R. To try to reduce the variance of R. Okay.
All right. So, what we're gonna do now is,
we're first gonna do something that doesn't go all the way to
there but tries to [NOISE] at
least leverage the fact that we're in a temporal, temporal, um, process.
[NOISE] Okay. Um, and for
any of you who have played around with importance sampling before,
this is closely related to,
um, per-decision importance sampling.
And basically, the, um,
thing that we're going to exploit,
is the fact that, um,
the rewards, uh, can only,
um, of the temporal structure domain.
Oh, I'll write it out first. Okay. So, what we had before,
is we said that, um,
the derivative with respect to Theta of the expected value of
Tau of the return is equal to the expected value under
the trajectories you could get of the sum over t equals
0 to t minus 1 of rt such as the sum of rewards you get
[NOISE] times the sum over t equals
0 to t minus 1 of your derivative with respect to your policy parameters.
[NOISE] Um, that's what we had before.
So, we just sum up all of our rewards, and then,
we'd multiply that by the sum over all of the gradients
of our policy at every single action state pair we got in that trajectory.
[NOISE] Okay. So, let's think about doing this for a single reward,
instead of looking at the whole sum of rewards. So, let's just look at.
We take the derivative with respect to Theta of expected value of rt prime.
[NOISE] So, this is
just a single time step reward that we might encounter you know, along our trajectory,
and that's gonna be equal to the expected value of
rt prime times sum over t equals zero to t prime of this derivative.
So, this is gonna look almost exactly the same as before.
[NOISE] Except, the only key difference here is that I'm only summing up to t prime.
Okay. So, we're only summing up,
um, you could think of this as just like a shortened trajectory.
I'm looking at the product of, um, the states,
and the actions and the rewards that I,
I reached all the way up to when I got to, um, rt prime.
Okay. So, I don't have to sum all the way over the future ones.
So, we can take this expression,
and now, we can sum over all time steps.
So, this says, what's the expected reward, uh,
or the derivative with respect to the reward for time step t prime?
Now, I'm gonna just sum that,
and that's gonna be the same as my first expressions.
So, what I'm gonna do is I'm gonna say,
[NOISE] V of Theta is equal to the derivative with respect to Theta of er,
and I'm gonna sum up that internal expression.
So, I'm gonna sum over t prime is equal to zero to t minus 1 rt prime,
and then insert that second expression.
Okay. So, all I did is I put that in there
and I summed over t prime is equal to zero,
all the way up to t minus 1, and then,
what I'm gonna do is I'm going to
reorder this and this by making the following observation.
So, if we think about,
how many terms one of these particular,
um, log Pi Theta at st appears in?
So, if we look at, um,
[NOISE] log of Phi Theta of a_1 s_1.
So if you look at how many times that appears,
that appears for the early rewards and it appears for all the later rewards too.
Okay. This is going to appear for r_1,
it's going to appear for r_2,
it's gonna appear all the way up to rt for t minus 1.
Because we're always summing over everything before that t prime.
Okay. So, what we're gonna do now is we're gonna take
those terms and we're gonna just reorganize those.
So, some of these terms appear a whole bunch of times,
some of them, the last one,
the [NOISE] log of Pi Theta of at minus 1,
st minus 1, it's only gonna appear once.
It's only re-responsible for helping dictate the very [NOISE] final reward.
So, we can use that insight to just slightly,
um, reorganize this equation as follows.
[NOISE] So, now, we're gonna say this is equal to
the expected value of sum over t equals zero to t minus 1.
So, notice before, I put the t prime on the outside and the t was on the inside,
and now, what I'm gonna do is put the t on the outside,
and I'm gonna say,
[NOISE] got Delta Theta, log Pi Theta,
at st times sum over t prime is equal to t all the way to t minus 1 of rt prime.
Okay. So, all I've done is I've reorganized that sum.
Yes? Is that ?
Yeah.
Yeah.
Um, on the second line from the bottom,
is it's supposed to be the derivative that- is
that a value function [NOISE] with respect to Theta?
Um, at the very [NOISE] left.
Yeah.
Okay. Yes.
Oh sorry. You mean[OVERLAPPING] it's supposed to be the derivative of this?
Yes. [NOISE]
Yeah. Thank you.
Okay. So, what we've done in this case was we've reorganized the sum.
We-we've just recollected terms in a slightly different way.
But it's gonna be the- in a useful way.
So, [NOISE] let's move this up,
and I'll move this one down.
[NOISE].
Okay. So, right now,
we're still working on the temporal structure.
[NOISE] And what is this going to allow us to do?
Well that second term there,
should look somewhat familiar.
What that's saying here is that's saying,
what is the reward we get starting at time step,
uh, t all the way to the end?
And that's just the return.
So, we had previously defined that,
um, when we are talking about,
like, Monte Carlo methods, et cetera,
[NOISE] that we could always just look at,
um, rt prime at I.
This is just equal to the return.
The return for the rest of the episode starting in time step t on episode i.
So, that, that should look very familiar to what we had seen in Monte Carlo methods,
where we could always say from this state and action,
what was the sum of rewards we get
starting at that state and action until the end of the episode?
Okay. So, that means we can re-express the derivative [NOISE] with respect to
Theta as approximately one
over m sum over all of the trajectories and we're summing over,
sum over all the time steps.
The derivative with respect to Theta of our actual policy
parameter times just the return.
And this is gonna be a slightly lower variance estimate than before.
Okay. So, instead of us having to sort of separately sum up all of our words, and then,
we multiply that by the full sum of all of these derivatives at the logs,
we are only kind of needing to take the sum of the logs, um,
for some of the reward terms essentially,
and so, we can reduce the variance in that case.
Because in some ways, what this is doing, this is saying, like,
for every single reward,
because you could re-express this as a sum of rewards.
For every single one of those rewards,
you have to, um,
sum it by sort of the full trajectory in terms of the derivative of the gradient,
uh, the derivative of the policy parameters.
And now, we're saying, you don't have to,
uh, multiply that by all of those.
You only have to multiply it by the ones that are relevant for that particular reward.
That means that you're gonna have a slightly lower variance estimator.
[NOISE] Okay.
So, when we do this we can end up with what's known as REINFORCE which,
um, who here has heard of REINFORCE?
Yeah. Number of people, not everybody.
REINFORCE is one of the most common reinforcement learning policy gradient algorithms.
So, you get the REINFORCE algorithm.
[NOISE]
So how it works is you, um,
then the algorithm is you initialize in it, theta randomly.
[NOISE] You just always will have to first decide on how you're parameterizing your policy,
so somewhere you already defied- decided how you're parameterizing your policy.
Now, you're gonna set the values for that policy randomly.
And then for each episode,
so you're going to run an episode with that policy.
[NOISE] Episode.
[NOISE] And you're gonna gather a whole bunch of actions and rewards,
[NOISE] and this is sampled from your current policy.
So, your sample, your current policy according to,
um, sample from your current policy,
you get a trajectory,
and then for every step in that trajectory,
you're gonna update your policy parameters.
[NOISE] So, [NOISE] for every time step inside of that episode,
we're gonna update our policy parameters.
[NOISE] So, it's going to be the same as before times some learning rate.
[NOISE] I will not use W there, I use alpha, um,
times the derivative with respect to Theta, log Pi Theta,
st at Gt, where Gt is just in this episode,
what is the sum of rewards from st at onwards?
[NOISE].
So, that's the just the normal return like,
what we had with, um, Monte Carlo methods.
So, just like, what we did when we were estimating like,
the linear value function,
and we were using rewards from the state and action onwards.
We're going to do the same thing here except for, um,
we're going to be updating the policy parameters, and we do this many,
many, many times and then at the end,
we return the Theta parameters.
Yeah? [NOISE]
I have a question. So, for each episode,
do you sample from the updated, um, policy?
We're gonna talk with you. Hope you're ready.
Yes. Uh-um. [NOISE] Yeah.
So, what just asked is, right.
Um, so, in this case you, um,
I- after you do all the updates for one episode,
so you could do these incremental updates.
Um, I- and then,
a- at the end of doing all of your incremental updates,
then you get another episode with your new updating parameters. Yeah, ?
Um, since we're doing every time updates would this be a biased method?
Um, good question.
So, since we're doing every time estimates, um,
this should be an unbiased estimate of the,
um, I- It should still be an unbiased estimate of the gradient.
It's stochastic, um, but, um,
we- there's not a notion of state and actions in the same way.
Um, this will be asymptotically consistent. It's a good question.
So, the notion of, um,
a state and action in this case is different
because we have just these policy parameters.
So, we're not estimating the value of a state and action here.
Um, so, this is certainly asymptotically consistent.
I think it's still just unbiased.
Um, if I, if I reconsider that later,
I'll send a Piazza post, um,
but I think it's still just an unbiased estimate of the gradient.
It's a good question. Okay. So, I go back to my slide notes.
Um, I think the last thing I just wanted- well,
I'll- I mention the, uh,
probably the things will, um,
[NOISE] one critical question here is, whether or not,
or how to compute this differential with respect to the policy parameters?
So, I think it's useful to talk about, you know,
what are the classes of policies that often people consider,
[NOISE], um, that have nice differentiable forms.
So, um, some of the classes people considers are things like,
Softmax, Gaussians, and Neural networks.
Those are probably the most common.
So, I- what do I mean by that?
I mean, that's how we're [NOISE] going to actually,
just parameterize our policy.
So, let's just look at an example.
So, Softmax is where we simply [NOISE] have a linear combination of features,
and we take sort of, um,
an exponential weight of them.
So, what we're gonna do is we're gonna have some features of our state and action space,
and we're gonna [NOISE] multiply them by some weights or parameters.
These are our parameters.
[NOISE] And then, to actually get a probability of taking an action,
so if we want to have our policy where we say,
what is the probability of action given the state?
We're gonna take the exponential of these weighted features.
So, we have e [NOISE] to the phi t Theta,
divided by the sum
over all actions [NOISE].
So notice, this is a reasonable thing to do when our action space is discrete,
action space discrete [NOISE].
So, [NOISE] a lot of Atari games,
a lot of different, um,
scenarios you do have a discrete action space, um,
so you could just take this exponential, divide by the normalized.
Um, sum over the exponential,
and that immediately yields our parameterized policy class.
And so, then, um,
if we want to be able to take the derivative of this with respect to the log,
that's quite nice because we have exponentials here,
and we have log term.
We're taking the log of this.
So, um, we want to be able to compute
this term from this sort of parameterized policy class.
What we get is the derivative with respect to Theta of log of this type of
parameterized policy [NOISE] is just equal to our features [NOISE].
So, this is whatever feature representation we're using,
like in the case of the, um, uh,
locomotion robotic case, this would be like all the different.
Um, uh, this could be something like,
you know, angles, or joints, or things like that.
Um, so, this is whatever featurization we're using minus the expected value for
Theta of [NOISE] your parameters,
um, with an exponent with, um,
your expected value [NOISE] over all the actions you might take under that policy.
So, it's sort of saying the features that you
observed versus the sort of average feature,
average, average over the action.
Okay? So, that's differentiable.
Um, and you can solve it,
then it gives you an analytic form.
Um, another thing that's really popular is a Gaussian policy.
[NOISE] And why might this be good?
Well, this might be good because often,
we have continuous action spaces.
So, this is good if we have discrete action spaces.
Often, we have continuous actions spaces.
This is very common in,
um, controls and robotics.
[NOISE] So, you have a number of
different parameters and i- in
their continuous scalar values that you wanna be able to set.
Um, and what we could say here, let's say,
we use mu of s. It might be a linear combination of state features, times some parameter.
Okay. And let's imagine for simplicity right now that,
um, we have a variance but that, that's static.
So, we could also consider the case where it's not,
but we're gonna assume that we have some variance term that is fixed,
[NOISE] so this is not a parameter.
This is not something we're gonna try to learn.
We're just gonna be trying to learn the Theta that's defining our mu function,
and then our policy is gonna be a Gaussian.
So, a is going to be drawn from a Gaussian using this mean per features.
Okay. So, we compare our current state to the mean,
and then we select an action with respect to that,
and the score function in this case is the derivative of this Gaussian.
[NOISE] So, score's a derivative of,
uh, of this Gaussian function [NOISE],
which ends up being a minus mu of s times our parameterization
of the state features divided by sigma squared [NOISE].
So, again, it can be done analytically.
And the other really common one is deep neural networks.
So, those are, um, [NOISE] those are- those are sort of very common forms,
um, I- that people use.
We're gonna talk next time about another common way.
Before we finish, um, we're gonna do,
um, spend about five minutes,
um, to do some early class feedback.
It's really helpful for us to figure out what's helping you learn,
what things do you think could be improved?
Um, so, what I'm going to do right now is open,
if you guys could go to Piazza.
Um, It would be great if you could fill this out.
All everything is anonymous.
Um, and my goal is to get feedback for you guys on Wednesday about this.
So, just- let me see if I could redo that.
So, it's posted.
Okay. Let me see if I can- I'll pin it,
so it's easier to find.
[NOISE] So, it should be pinned now at the very top.
Yeah. So, a class feedback survey,
if you go to Piazza, um,
it's a very short survey.
You can give us information back. That would be great.
What we'll do next time is we'll continue to talk about policy search.
We're gonna talk about baselines,
which is another way to reduce variance.
Um, and this again is a super active area of research,
so there's, um, a ton of work on deep reinforcement learning and policy gradients,
and so we'll talk about some of that work,
and then you'll have a chance to, uh,
play around with this a- and, uh,
do get sort of hands-on experience with policy gradients after the mid-term.
So we're releasing the assignment about this at post the midterm,
and that will be the third assignment.
 All right. Welcome back everybody.
Um, before we get started today,
I- does anybody have any questions about logistics,
or midterm, or anything like that?
We'll be doing a midterm review on Monday,
and the midterm will be on Wednesday.
Because there's a number of people in the class,
we're gonna be spreading everybody across a couple of rooms,
and will be, er, sending instructions out about that.
Does anybody have any other logistics questions? Yeah.
Midterm is during class time right?
Midterm is during class time.
Um, instructions will also be on the web,
but you're allowed to bring a one-page, um, of
written notes, um, aside from that everything is closed book. Yeah.
Is it okay to type said notes [NOISE] or it has to be handwritten?
I think we've already issued a policy on that,
Let me just double-check and see what it is.
Okay.
[BACKGROUND]
All right. Okay. Lets go ahead and get started.
Um, before we do that,
I just want to say thank you to all of you that, ah,
participated in the class feedback survey.
It's really helpful to me and to everybody else to
understand what's helping you learn and what's not helping you learn.
Okay. So in terms of,
um, the responses, so note,
you know, all of these things there's about 230 people registered in the class.
Um, so for some of you if you didn't give me feedback,
it's hard for me to know what- what's helping you or not helping you learn.
So we're just gonna go with what people gave us feedback on.
Um, about 65% of you thought it's the right pace,
about 27% of you thought it's going too fast,
and there's only about 8% of people that think it's going too slowly.
Um, so we're gonna keep roughly in the same pace as what we've had before.
Um, a number of people noted that they wished it was like a semester-long course.
Um, I will mention there's a number of other classes that
do reinforcement learning and I highly encourage you to take them.
I- I offer an advanced class,
and also Ben Van Roy offers a class normally in the spring that's more theoretical.
This was super controversial.
I didn't think it would be this split.
Um, so we offer sessions on Thursday and Friday.
Attendance has been really low.
Um, I think we've had around like,
three to seven people showing up for these sessions.
Um, so we thought this was gonna be something that everybody wanted to take out.
Because it's about evenly split,
I asked all the TAs to compare that people that are coming
to their office hours versus the people that are coming to the sessions.
Um, we probably have
about 4 to 5x people trying to go to office hours than trying to go to sessions.
So we're going to switch this to office hours.
Just so that we can kind of,
serve as many people as we can.
Um, so let me just write that there.
So we're gonna switch to office hours.
Now, the sessions will still be offered on the other days,
and they'll still be recorded.
So for anybody who wanted to go to them on Thursday and Friday, you can still go,
you can still participate on Zoom or you can watch the recorded lecture.
Um, but we're gonna switch this to office hours
because my TAs have been saying they've had
a number of office hours where they've either had to stay
really late or they feel like they're not getting to some people.
And so again, just in terms of serving the most people.
I will say when I was going through these responses,
it really made me think about the fact that in
reinforcement learning and sort of sequential decision making in general,
um, we're always optimizing for expected rewards [LAUGHTER]
And so that's kind of exactly the same thing we're doing here is that,
I know that everybody needs slightly different things,
and we're just trying to do our best job and expectation.
Um, but it's exactly why things like
intelligent tutoring systems and other stuff might be better.
Um, okay.
In terms of things that people thought were working well for them,
we've got a number of positive remarks about
doing worked examples in class, doing derivations.
Um, a lot of people really like the fact that my iPad had problems on Monday,
um, and so we did things on the board.
So we'll try to keep doing the same amount or more of derivations.
Um, people also were generally really positive about the homeworks.
In terms of things, er,
we saw repeatedly, so- so what I did is I got,
I just co- collated,
sort of all the free responses and tried to look for common themes.
And anything that came up, you know,
three or five times or more,
I considered was a common issue that people would love addressed.
Um, people would love even more focused on the big picture explaining,
um, as well as connecting from the toy examples to real-world examples.
So we'll try to do that where we can.
Um, I'll also try to make sure that I'm speaking loudly throughout.
Several people said that sometimes it was occasionally hard to hear,
so I'll try to do a better job on that.
If you can't hear me in the back please feel free to raise your hand.
Um, and people would like even more examples, um, worked examples.
And so in particular,
we're going to try to make sure that in sessions,
we emphasize worked examples even more.
And let me just say again, if this was not one of
the things that you were most concerned about,
I'm sorry, we can't address all of them this term.
Um, er, we definitely and it was kind of amusing to go through this,
would have people saying exactly opposite things.
Sometimes right in a row, um,
in terms of how it was collated about things like,
some people don't like the fact that the slides have gaps and I do derivations in class.
And a number of other people really like that I do derivations in class.
Um, some people felt like it was moving too slowly,
other people said it was moving way too fast.
Um, so again, we're just gonna try to do the best job we can to address
everybody's needs. All right.
So today we're going to continue talking about policy search,
which as I said before, is probably the most important
reinforcement learning thing you'll learn this term. [LAUGHTER]
Um, I think this is used really very widely right now,
um, in order to optimize functions.
And we can again think of policy search here as
a lot of things are gonna sound similar to
when we were doing value function approximation.
And what we're thinking about here is having a parameterized policy.
Often we're going to use theta to parameterize It,
but we could use W or anything else.
But we have a policy that's parameterized.
Um, and then we have some value of that policy.
And what we're going to want to be doing,
is trying to find, you know,
a good optima, trying to maximize the value,
um, of that policy.
And one of the reasons why we did this right after
imitation learning was to connect it with the idea of, um,
you have to choose a policy class,
a way to specify that parameterization.
And so because of that inherently,
it's a place to put in structure.
Okay. So just as a recap,
I mentioned that we've done a lot of work so far,
on model-free value-based methods.
We're starting to do work on direct policy search methods now.
Um, and today we'll also start to talk more about Actor-Critic methods,
where we maintain both.
We maintain both an explicit parameterized policy
and an explicit parameterized value function.
And also throughout all of, you know,
the last couple of weeks including yesterday or Monday and today,
we're mostly going to be talking about cases where we want to
be able to work in really really large state spaces.
So I'm just gonna do a brief refresher of last time.
Why do we want to do this?
Well, we're going to generally be able to guarantee that we converge to a local optima.
We don't always have those guarantees for value function based methods.
Um, er, and that might be important.
Er, it's a nice property to have.
Um, the downside is that,
if you use policy gradient methods,
typically we only converge to a local optima.
The last time I showed you the exoskeleton example,
where they are using a global optima approach.
So there are other ones which can policy based RL
isn't inherently always going to get you to a local solution,
but the gradient based methods typically will.
And the other issue that we were talking about, uh,
um, as ways to try to address this is, um,
or- or as tools to address the fact that evaluating
a policy itself might be rather inefficient, and high-variance.
So what we are defining before is a policy gradient where, um, now, er,
before we'd sort of thought about these things being parameterized by a theta,
so we can either think of the value being, uh,
the policy being parameterized by a theta or pi of theta means parameterized.
But we're often going to talk about value functions because ultimately,
the value function depends on the policy and the policy depends on the parameters.
And when we think about what we want out of these algorithms,
typically what we'd really like is to try to converge to a really good local optima.
Often we don't have very much control over that.
Um, but the things we often do have control over is things
like how quickly we converge to that local optima.
Um, and so we want to use sort of go as quickly as we can down that gradient,
if we're doing a gradient-based method,
um, and use our data as well as we can.
So one of the things we're gonna talk more about today,
is when we're doing this sort of policy gradient technique.
So we're going to be sort of moving down.
Now, we're gonna have our gradient.
We're gonna have our functions.
This is B pi, and this is our parameterized pi.
And as we're moving down towards some local gradient,
um, it would be nice if when we update our policy,
that it is monotonically improving.
So can anybody give me a reason why we might want monotonic improvement? Yeah.
To help guarantee convergence.
Answer is right. Can help guarantee convergence absolutely.
And while I love math as much as many of you,
um, that- that is a great reason.
But perhaps, I was also thinking of like
an empirical reason why we might want that as well.
Yeah, in the back.
[inaudible] like in a high-stakes situation.
So what we've seen before in fact,
um, one of my students, ah,
[inaudible] was giving a practice job talk yesterday,
and he was showing this graph for DQN,
which looks something like this.
Where this is, like, the performance,
this is the reward, and this is time.
Of course, it doesn't always look like this.
And typically when you go to- um, when you read papers,
people smooth over many,
many runs, but often it looks something like that.
That as you're going across multiple episodes or across multiple time steps, like,
you're really getting a very jagged up and down performance of your-
of your val- of the policy that you're running as you do DQN.
So why might that not be good in,
like, a high-stakes situation.
Yeah, over there. And name first, please.
If, like, it's something high-stakes and you have something good and then it goes down,
people are going to be upset with you that now it's done something worse,
even if it will later go back up.
Yeah, what was said is that if the system is a high-stakes scenario,
um, if you do- you know,
if your policy works pretty well and then the next one,
next episode, it works really badly,
even if it might go up later, um, you know,
your boss still might fire you [LAUGHTER].
I mean, I'm joking, but I think that people are often
loss averse and also it's often not tolerable,
um, in- it might not be okay, you know,
in a company to say, well,
this quarter we did really well and next quarter we're going to do, you know,
worse, but then eventually, you know,
after many, ah, quarters we're going to do well.
Like we often may wanna ensure that we're sort of monotonically going up.
Um, and in the case of something like patient treatment or
other sorts of high-stakes scenarios or airplanes or stuff like that,
um, it just probably will not be tolerable to people,
if you say we're- we're going to do much worse for this period of time.
Ah, no- now there are exceptions to all of this,
but I think there are many cases,
where you'd really like monotonic improvement,
i- if you can.
Ah, so I think it's a really- in addition to the theoretical,
ah, benefits, ah, it can help us prove things.
Ah, it also can just be,
um, something that's sort of appealing for people to be actually be able to deploy.
And we know that in general that people are very risk- like, very loss averse.
So having policies that are monotonically improving, um,
can be very nice and DQN and a lot of the value based methods do not have those guarantees.
Um, we can talk more also about whether that's always possible,
um, in terms of if you wanted to get to a global optima. Yes.
Just to be clear, the monotonic improvement in these cases
are data that you have access to or have seen, right?
So technically like if there's a distribution in terms of
your life environment where it may differ somewhat from your actual simulation or,
ah, environment, you may not necessarily quantify or improve it,
given all that secret data. Is that right?
Yeah, which is to say,
when we're gonna be- what's- you know,
what is this monotonic improvement?
What are the conditions under which this will be guaranteed or possible?
And are we sort of doing this based on our previous data and
making some assumptions about the future da- future data that's collected.
Absolutely. We're gonna assume that we're still on
the same decision process and that it's stationary.
And what I mean by that is that the transition model and
the reward model is the same across- you know,
you might not have observed all the states yet,
but it's the same across episodes.
So we're not dealing with the fact that, you know, um,
customer preferences have totally changed or,
um, you know, climate change is changing your environment.
We are dealing with the fact that if the world is stationary,
that then we're going to be guaranteed to have monotonic improvement.
Now the other thing that I'm going to show you that in some cases we can guarantee that.
Um, the other really important thing to know is,
this is going to- we're going to hope to show monotonic improvement in expectation.
So- so the value function has expected reward.
So what we're going to be able to hope to say is
the series of policies that we're deploying in
the environment that their value function is going up. So what does that mean?
That means that V_Pi1 we would like that to be less than or equal
to V_Pi2 less than or equal to V_Pi3 dot-dot-dot,
where this is, sort of, um,
the policy we deploy on each iteration or each round.
Ah, but it doesn't guarantee that for a single run this policy is better.
So you could easily have it that it on average you're deploying a policy that is better,
you know, for your airplanes or for patient treatment,
et cetera, but for individual patients that might be worse.
A- and I think a really interesting active area of research right now is,
um, safe reinforcement learning,
um, and safe exploration.
And a lot of different people are thinking about this,
um, including a number of people here at Stanford.
And one of the things that we're looking at in our group is,
how do you really efficiently get to a safe solution?
What do you mean by safe in this case?
I mean you might not want to max- maximize expected reward.
You might want to be able to maximize,
um, some sort of risk averse criteria.
And we'd like to figure out ways to really efficiently get to that solution.
But there's lots of really interesting stuff that says,
you know, how do we try to do policy search?
Or how do we do this improvement in cases
where we don't just care about expected outcomes?
All right. So what we're gonna be trying to do today is move towards,
sort of, ideally, not just monotonic improvements,
but large monotonic improvements.
Um, as you might guess,
it is easier to try to achieve
small monotonic improvements than it would be to
guarantee really large monotonic improvements.
Um, does anybody have any intuition for why that would be true?
That might be harder.
Um, this kind of goes back to the state distributions.
So if you change your policy a lot,
um, does the state- can the state
distribution change a lot in terms of the states you visit?
So- so intuitively that should- the answer should be yes.
So we've talked some about how any policy induces, um,
a state distribution, like if you run it for a long time
you're going to have sort of a stationary distribution over states.
Um, and if your policy is really different than your old policy,
then that state distribution might look really different,
which means you might not have very much data.
Um, whereas if you have almost exactly the same policy as before,
um, you're probably going to be able to have
a really good notion of what that value is in the estimate.
But we'll get more into that later.
So what we're going do today is to try to think
about moving beyond what we were talking about last time,
where we're trying to do policy gradient methods.
And we're trying to do it in a way that was sort of efficient.
We're going to talk about other ways to make it more efficient,
ah, and less noisy, and then try to go towards monotonic improvement.
All right. So that things that we talked about last time,
is we started off when we said what can we do in terms of policy gradient?
One thing we could do is,
kind of, you do Monte Carlo returns.
So these are Monte Carlo returns.
And sometimes people use big R of Tau,
where Tau, this is a trajectory.
So you could just run out your policy till
the world terminates or each steps or however you're defining your episodes,
um, and then look at the reward and you'll look at their reward across,
you know, per time step.
We can also use, um,
G_i_t to denote the reward we get from time-step t and- and in Episode i.
And what we've said before is that, um,
this is an unbiased estimate of the gradient, but it's really noisy.
And so we started talking about additional structure we
could use in the reinforcement learning problem,
where we were assuming the world is Markov,
um, to try to reduce the variance of this estimate.
And so what we talked about last time was using temporal structure,
which we did some on the board.
And the intuition there was the reward you get at some time point,
um, uh, can be- is not influenced by the later decisions you make.
So you didn't have to take kind of
this complete product of the probability of action given the states,
because future actions don't retroactively change
our earlier rewards for intuition.
So what we're going to start talking about now is, um, other things,
which is baselines and alternatives to Monte Carlo. Okay.
So what's the baseline?
Well, a baseline is as let's still think about looking at,
um, the sum of rewards we get from this time-step onwards.
So this is the same thing that we've often called GT.
So this is the reward we get from this time-step till the end of the episode.
And we're gonna subtract a baseline that depends on the state.
And what I'm going to show shortly is that by
subtracting this baseline which depends only on the state,
your resulting gradient estimator can still be unbiased,
but it might not, have much lower variance.
And in particular often a really good choice is the expected return,
which is basically the value function.
And so why would we do this?
We'll then we can kind of look at we- increasing the log probability of an action,
proportional to how much better it is than a baseline.
Which in general is going to end up being sort of a
little bit like an advantage function.
So why is this true?
Okay. So, um, what are we going to try to do here?
We're gonna say we have this high-variance estimate right now.
If we don't have, so imagine we didn't have this, we don't have that.
We have our, the standard estimate we were talking about last time.
And what I want to convince you is that if we
subtract off this thing which is a function of the state,
that an expectation that additional term we're subtracting off is zero.
Meaning that our estimator is still unbiased.
So I said our original estimator is unbiased.
We're subtracting off this weird thing,
we want to show that the resulting estimator is still unbiased.
And the way we do that is by showing there's,
the goal is to show,
[NOISE] that this is equal to zero.
So that's what we're gonna try and do,
and if we can show that then that's gonna justify why we can subtract this random thing.
And then we can start to talk about what that random thing should be.
But first, we're just going to show no matter what that random thing is.
If it's a function only of the state,
that this, um, expectation is 0.
So how do we do this?
Well, first of all noted on the outside there's an expectation over tau.
That is all the trajectories we might encounter by running our current policy.
Okay. So what I'm gonna do first is I'm just going to split it into two parts.
So this is still tau.
And all I've done here is I've split it into
the first part which is all the way up to time step t,
and the second part which is on time step,
um, t, all the way to the end.
So I've just sort of decomposed, that,
I-I I've just written out what, um, ah,
a trajectory is, and then decomposed into two parts.
So I'm just decomposing the trajectory.
[NOISE] And once we do that,
then we can see that the baseline term is only a function of S_t.
[NOISE] So we can pull it out of this inner term.
Right. So we're gonna pull this out because it doesn't depend on
any of these future time steps.
It's independent of those.
[BACKGROUND]
Okay.
Then the next thing we're gonna do is we're going to,
uh, write out or note the fact that in this case.
That all we have in this inner term here is S_t and a_t.
So we can drop all the future terms.
Again that's sort of the prob- the the only thing in
here is the probability of the action a_time-step t,
given the state t and theta.
So we don't need to worry about the future states or the future actions that are taken,
um, we- we're independent of those.
So now, so we just pull it up,
first we pulled out baseline.
And now, we're going to drop those things that we don't need to depend on.
[NOISE]
So all we have here is an expectation over the action that's taken.
Okay. And now what I'm gonna do is I'm going to read it,
so what is this expectation?
It's an expectation over a_t.
What is the problem, you know, what is that expectation?
We're just going to write that out explicitly,
that depends on the policy that we have.
So we're going to sum over a_t,
the probability of that a_t,
which of course just depends on the policy that we're following,
times the derivative of log.
So that is me writing out the expectation,
and I'm gonna take the derivative of the log.
[NOISE]
So that's just gonna be the derivative with respect to the policy itself.
Divided by pi of a_t, s t theta.
Okay. But now we note that there's a term on the numerator and a term on the,
denominator that we can cancel.
So this starts to simplify [NOISE] b of S_t times the sum over a_t,
just the derivative of the policy.
Just canceled numerator and denominator there.
And now we note that we can reverse the sum and the derivative.
This is the others, that kind of critical step of this proof.
So now what we're gonna do here is we're going to move the derivative out.
[NOISE] Well, this is just 1,
because the probability that we select some action,
under our policy always has to be 1.
And so now we see that we're just taking the derivative of 1.
So we are trying to take the derivative of 1,
and of course that's a constant so this is equal to 0.
[NOISE] So that's pretty cool.
So that means that we have added in this baseline.
That is some function that depends on the state,
and we haven't said we told me, you know,
we haven't talked about all the different ways we
could compute that gap or we say it doesn't matter what it is.
No matter what you added there it's always unbiased.
So just to check our understanding for a second if we go back to this equation,
if I set b of S_t,
to be a constant everywhere,
is the gradient estimator still unbiased.
[NOISE]
Just take like one minute and talk to your neighbor,
and say,
so based on what I just said if b of S_t is equal to a constant,
this is like a constant,
for all S_t is the gradient estimator unbiased,
just take like one second or one minute and talk to your neighbor.
[BACKGROUND]
All right. So let's start with,
um, everybody here thinks it's still unbiased, vote?
Yes. Great. Okay, yes.
It has to still be unbiased,
now it's a function not even of s, just a constant.
And so it's definitely a function of an s,
it's a tri- trivial function where it doesn't matter what the value of s is,
and so it's still unbiased.
Just to note here, um,
if s was a fu- if, um,
the baseline was a function of state and action,
do you think this proof would go through?
No.
No.
No. Right. Because one of the things that we did at the very beginning is
we moved b of S_t all the way out.
And if it depended on the action too, we couldn't have done that.
So this is specific to this being only a function of the state. Yes.
[inaudible] functions b that do not give you an un-
an unbiased estimate state.
So I don't know, is there any functions of b?
b is only a function of the state, all of them are unbiased.
Yeah, so it is always unbiased.
There could be really- I mean,
just like what I put here,
um, you could just put in a constant and it might not reduce your variance at all.
So there's certainly unuseful definitions of a baseline,
um, but all of them are unbiased.
So they're not gonna affect, ah,
whether or not your estimator is unbiased.
They could make your estimator potentially worse if they're really bad,
um, or [NOISE] they're really themselves, um,
very bad estimators potentially, um,
and they certainly couldn't make it better [NOISE] by choosing good choices.
Okay. All right.
So this ends up allowing us
to define what's sort of known as the vanilla policy gradient algorithm.
Um, so vanilla policy gradient operates by,
we collect a bunch of trajectories using our current policy.
And then for each time step inside of the trajectory we
compute the return from that time step to the end.
Um, and then we compute the advantage estimate.
So, all right, we'll write out vanilla policy gradient.
[NOISE] Okay.
So vanilla policy gradient works as follows.
You start up, you initialize the policy with
some parameter theta and you need to start with some estimate of the baseline.
Okay. So what happens with vanilla policy gradient,
is for iterations i = 1,
2 dot, dot, dot.
We're gonna collect a set of trajectories using your current policy.
And then for each time step,
for t = 1 dot,
dot, dot, the length of your trajectory i,
then you do two things.
You compute the return,
which is just equal to the sum of all the returns till the end of the episode.
And then you compute the advantage A-hat_i_t,
which is just equal to this return.
I'll parameterize it with i just to show that that's the i'th trajectory.
Um, [NOISE] - b of S_t.
So just to note here for a second,
um, this is a return of the sum of rewards till the end of the episode.
This is a baseline which is like a fixed function.
Um, so this could be, you know,
a deep neural network,
this could be a table lookup.
Ah, but this is a function and you input
the state at time step t and trajectory i [NOISE] and you output a scalar.
So that's what the baseline is doing there.
And then wha- um, in vanilla policy gradient we do is,
then we refit the baseline.
So in this case,
the baseline is gonna be an estimate of the average of the Gs [NOISE].
So in vanilla policy base,
bu- um, vanilla policy gradient.
What we do is the next step is,
we sum over all the trajectories we've got so far.
We just sum over all the time steps.
Um, we do basically, just a least squares fit.
[NOISE]
So note this can be done with like su- this is supervised learning.
We just have some a baseline function that can be parameterized [NOISE].
I'll make sure to put an i there.
So the baseline function that can be
parameterized with some totally other weights or parameters,
um, and then we have our returns g that we've
seen so far and we just try to minimize that distance.
And so then the baseline is really, er,
representing the expected sum of rewards.
Um, note that this is in some ways a little bit funny, right?
Because we're using all of our data that we've ever seen.
So this can either be done over, um,
all the data you you've ever,
ever seen or it can be done over just the most recent round.
There's lots of choices for how to do the baseline.
Um, I- if you use all the data you've ever,
ever seen, um, then which is what this would do.
Um, then you could be averaging over lots of different policies,
because you've got data from different policies.
If you just do this over the most recent round,
then you're just gonna be getting an estimate of essentially V_Pi i- V_Pi i,
like the- the iteration.
This is gonna end up approximating. All right.
If you ju- are only doing it over, um,
the trajectories, if you don't do this,
but you sum it over the trajectories for this round.
I guess the way I've written this is a little bit unclear.
So let me see if I can make that a bit clearer.
So let's say that we have a- a- a- we have d trajectories.
So if we do it this way,
then that's exactly equal to V_Pi i.
So i now is the iteration,
d is the trajectories we've gathered just on iteration i.
So this is only averaging over, um,
the policies for this particular- the- the trajectories for this particular policy.
I said a lot a bit out of order.
Does anybody have any questions about exactly what we're doing in this case?
So normally, in this situation there's a number of series of rounds.
This is for each- So we're gonna have a series of pi's, basically.
And then for each policy,
we're gonna have a set of trajectories.
And for each trajectory we have a set of time steps.
And what this is saying here is average
over all the trajectories you have for the current policy,
and fit the baseline to that.
All right. And then once you have that,
so this gives you the baseline.
Um, and then we do update the policy using your gradient.
And it's gonna be a sum of terms that include these derivatives with respect to
the policy and your advantage function.
Okay. So you're gonna take in this advantage function that you computed over here.
And then you're gonna be multiplying it by what was
the probability of the actions given the state and theta.
The log derivative of that.
And then we plug that,
this gr- this estimate of the gradient,
into something like stochastic gradient descent or ADAM or something else.
So this has been vanilla policy gradient.
[NOISE]
And what we're going to see during the rest of today
is just a number of different slight variants on this basic template.
So I'll get to you in just a second,
but I just want to emphasize that if you- if
you- when you walk away from unders- like from what I'd like you to understand,
from the- the main idea for policy gradient,
is essentially what's on the board right now.
Is that, what we're doing is we are running,
we take one policy,
we get a bunch of data from it,
and then we have to fit something like an advantage,
and there's going to be different ways to compute that.
We could end up doing bootstrapping,
to do some sort of TD estimate,
or we can just directly use the returns.
We often use a baseline,
um, that we're fitting over time.
And then we're going to update the policy,
and have to choose some step size with respect to the gradient.
So this is sort of the most important thing.
Is to say, hey, there's this basic template for almost all policy gradient algorithms,
I can choose different things to kind of plug in here,
and I can choose different ways to take my step sizes.
Um, and that's going to define
a whole bunch of the different policy gradient algorithms that you see.
So what function are we using to represent i so that we can take its gradient?
Great question. So, um, is asking, you know,
how- how do we represent,
um, the policy so we can take its gradient.
We have to be able to take this here.
Um, we talked briefly about this last time, um,
but it was also on the board near the end, Gaussians work,
Softmaxes both are- are- are- are- both of those you can analytically take the derivative
and often we use deep neural networks or shallow deep neural networks.
Yes. I saw a question back there? And name first, please.
Ah, I was wondering if there's any, ah,
issue with like non-states- if we're
getting like b of- the baseline with the neural networks,
there's like non-stationary issues with that?
Yeah, it's a great question.
So, ah, the question I believe is to say,
um, you're asking me about the baseline, right?
So like how- are there non-stationary issues with that?
Empirically what a lot of people, including myself,
have wondered is, um,
we have all this other data.
So when we're estimating the gradient right now,
typically we're running the policy, just you know,
for D trajectories and then we're estimating a gradient with that.
Um, and could we maybe use other data to do that,
but then ends up being off policy,
because then you're mixing together data you've gathered from different policies.
Empirically, I think people often end up using only the data from the current run,
and then you're essentially just estimating V_Pi,
with this and you're not necessarily mixing data for many other policies.
Empirically, it seems like often,
it's really helpful to be on policy.
A- and you could reweight the old data,
ah, but that introduces variance.
And so empirically often, it's best.
I think the jury is still out on it.
There's ongoing research on it.
We've looked at it, Sergey Levine's group has looked at it,
but most of the time using the on-policy data makes sense.
Yeah, is there another question? And name first, please.
I just want to confirm, so when you saying refit baseline,
we're setting baseline equal to the value that minimizes the function error.
[NOISE]
Perfect. Yeah, for error, if we do this if we're
only averaging over the data points that we have for this current policy,
when we do this, it's essentially- it- it's
essentially the same as when we were doing Monte Carlo policy evaluation.
So this is almost exactly like Monte Carlo policy eval.
Where we have a fixed policy and then we have a parameterized function to represent it.
Um, and then we just want to fit those parameters so we can best
estimate the policy value using Monte Carlo.
Okay. So I'm going to- there's a little bit of
information about auto diff you can check it in the slides.
Um, uh, the things we're going to go through next, um,
is thinking about this aspect as I was saying,
and then we'll talk some about this.
So this part is going to be where we think about monotonic improvement.
Because once we have a gradient,
we have to figure out how far to go.
An- and can we guarantee, um,
depending on how far we go,
whether or not we're going to get a monotonic improvement.
Um, and this part is about sort of giving better estimates of our gradient,
ideally with less data,
um, and reducing the variance of it.
So they're both important,
they're doing slightly different things. All right.
So let's talk about,
ah, could we move this up, please? Thank you.
Okay. So like we sort of started talking about before,
um, well, let's- let's talk about first the baseline.
So how should we choose the baseline?
Um, one thing that we can do for the baseline,
is just to- like what that what we're seeing there,
which is an empirical estimate of V_Pi i.
So we could say, in general,
we wanna just have- use V_Pi i as a baseline.
That means we have to compute it somehow.
And the way we estimate that could be from Monte Carlo
or it could be from TD methods. All right.
So what we've seen so far,
is using these as a- so- so I guess just to be clear here,
there's a couple different places we're going to be able to maybe switch
between doing Monte Carlo returns and doing something TD-like.
One is here, and another is our baseline.
Okay. So we have a baseline function here that we're sort of subtracting off.
And we also have a G_t prime here, okay?
And so if we think about our general equation again,
so what we have in this case is we have delta theta, v of theta.
This is our parameter, this is specifying our policy parameters.
And we've said this is approximately equal to 1 over m sum over i = 1
to m of some reward- Well, I'll put this inside.
Sum over t = 0 to t - 1,
of,- I change my mind.
Okay. I'm going to put this out here because it's going to end up being
sort of a function we can use in lots of different ways.
Okay.
So this is
our basic equation we've been working with.
We've said the derivative of the value with respect to our policy parameter is
approximately as sum over m trajectories,
where we've sampled those trajectories from that policy,
times the total reward we've gotten on that trajectory,
times the sum over all time steps of the derivative of the policy with respect to,
um- given the action we took in the state we were in.
All right. And we said it was very noisy, um, but unbiased.
And now we can think of changing this as a target.
So this here was an unbiased estimator of the value of the policy.
And now we can think about substituting other things in. All right.
So we can imagine doing all sorts of things here.
We cou- we could do, um, you know,
TD or MC methods.
If we do it with a value function or if we try to
explicitly compute a value function or a- or a state action value function,
then we typically call this a critic.
So a critic computes V or Q.
So when we talk about actor-critic methods,
that's when we have an explicit parameterized representation of the policy,
and we have an explicit generally parameterized representation
of the value or the state-action value.
And if we have that,
then we can imagine using that to change what our target is.
I want to emphasize here that so,
actor-critic methods combine these two,
combine policy plus critics.
And probably the most popular one of this is A3C,
which is by Mnich et al,
this was introduced in 2016 ICML.
And it's been hugely popular.
This is a version for deep neural networks.
Um, but actor-critic ideas themselves have been around for a lot longer than that.
But A3C is one of the most popular versions of this for deep neural nets. All right.
So how do we do sort of policy gradient formulas with value functions?
What you could do instead here, I shall put on this side.
What you could do is, you could have almost the same equation as we had before.
So derivative with respect to the value function is equal to
an expectation with respect to the trajectories that you might encounter,
the sum over all the time steps in that trajectory,
times the derivative with respect to your policy parameters,
times Q of S_t,
w - b of S_t.
So instead of having your Monte Carlo estimates in here,
you could plug in your estimate of the Q function.
And another way to represent that here is if we think this is an estimate of the value,
and this is basically our advantage function again.
But it could be our- so we had an advantage function over here.
You define that advantage function here,
but this one was a function of the Monte Carlo returns for that episode.
This is a different advantage function which
is the Q function which- where this could be maintained by a critic,
minus your baseline, which is an estimate the value function.
So they look pretty similar,
but you can plug in different choices here.
And these are going to have different trade offs.
So the Monte Carlo estimate of
the return is an unbiased estimate of the value of the current policy.
This is going to be biased generally,
but lower variance. All right.
So I also want to emphasize here that when we think about,
um, kind of getting this estimator,
which we often say the critic is going to compute this estimator, um,
It doesn't have to be only either a TD estimate or a Monte Carlo estimate,
but you can interpolate between these.
It's often known as n-step returns.
So what does that mean?
So let's call- let's write this in a slightly, well, I'll call this here.
So let's put this is a hat.
Okay? Just to note that you can think of this as kind of just a function.
It's going to be an estimate of your state action value function.
And so what we could have is we could have an estimator.
I could have estimator of the value from time-step t onwards,
which is equal to the actual,
this is the actual one we got on time-step i, so I'm going to call this.
I'm going to call this sort of i, 1.
And this is going to be then the actual reward you got on time-step t in episode i,
+ gamma V of S_t + 1 i.
So this should look almost exactly like TD(0) style estimates.
We talked about this before.
So this says, I got- I look at the actual,
immediate one-step reward I got and then I bootstrap.
I add in the value.
So this would be- I get this value function for my critic.
And I would plug that in and then that would be my target.
That then I would use in this equation.
Okay. So that would be one thing you do- you can do.
So we've seen this,
and we've seen a lot of this,
which I'm going to call the infinite or Monte Carlo version.
And this one is you sum over all t prime,
all the way to the end of the episode of gamma 2.
The t prime minus t times r t prime.
So this is the Monte Carlo return,
where we just sum up, we don't do any bootstrapping,
and we sum up all the rewards at the end of the episodes.
But as you can see here, there should be, you know,
there's probably some way to interpolate between these two.
And these are often known as n step returns.
And so for example,
you could do this, you could say,
I'm going to add in the reward at time step i and
the reward I got at time step t + 1.
And then I'm going to bootstrap.
So this is just sort of one of the estimators that are in between these two extremes.
One is you only take in one step of reward,
another one is do you sum up all the rewards,
and then there's a whole bunch of interpolation you could do
between those. Why would you want to do that?
Well, this one is generally going to be somewhat biased, but low variance.
This is going to be unbiased,
but really high variance.
And there's no reason to assume that
the best solution is on either of those two extremes.
And so you could interpolate between a TD estimate and a Monte Carlo estimate.
And all of these just form
returns that then you could subtract off a baseline for too.
So traditional and all this would probably be
sort of hyper-parameter that we can choose through validation or cross-validation.
Right.
Is that what people do here?
Is that kinda too computationally intensive?
So you just have to pick something.
Question is if we were doing standard machine-learning,
this would be just considered as some sort of hyper parameter.
You could turn this into n and you would decide like
how many steps do you do- do people do.
And the question was do- do people do that in
reinforcement learning or is it that considered too expensive?
You certainly could. I- I think it's an interesting question.
I feel like the tricks that people do in this case,
[NOISE] I think I probably see more of this but it varies [NOISE] more using the TD(0),
doing a lot of bootstrapping,
but it probably depends on your application domain.
Another thing that it would likely depend on is if your domain is really Markov or not.
So this case this is still working,
and this is giving you a real estimate of the return of
your policy even if your domain isn't Markov.
This case you're making a much stronger Markovian assumption.
So you also might want to make- do different things depending on your domain.
And also how expensive it is to collect data. All right.
So this gives us sort of a different way to plug things
in to that vanilla policy gradient algorithm I put over there.
So you could plug in these sort of targets instead,
over there to trade off between bias and variance,
when you're doing this estimate of the gradient.
So what this is doing here is it's changing what our targets are,
and it's changing how we're computing our gradient [NOISE].
But then the next thing I wanna talk about is this part.
Which is once we've actually got our gradient,
however we've chosen to get it.
We have to actually figure out how far to go along that gradient.
[NOISE] So why might this be important?
Well, it might be important because,
this is just a local approximation.
You're giving your local estimate of the gradient. Yeah.
How often do you update the parameters of the critic?
The question was how often do you update the parameters of the critic?
It's a great question, again, it depends.
So you can either- you can do this often asynchronously.
So you can have different threads and different networks for
your critic and for your policy and principle,
you could just be updating your critic all the time like, you know,
you can be using DQN for this and doing lots and lots of backups.
In general, it depends,
I think you'd have a schedule.
Yeah. So often you might do something like 10 or 100,
it really varies by application, um.
Uh, but there's no reason that the critic
needs to be updated only on the same schedule at which you're updating the policy.
And doing it asynchronously often makes a lot of sense.
All right. So if we think about what's happening here,
here's our parameterized policy.
Here's our value. We have some crazy function.
Okay. And then we are computing our gradient.
And this gradient, locally is pretty good.
So kinda round here things like linear and things look pretty good.
But of course, as we get further out like here, it's gonna be bad.
Like if we- if we try to follow the gradient too far,
we're going to get an estimate that's very different
than the real va- real value function.
So when we're taking step sizes in this case,
it ends up being important to consider this fact of sort of
how far out do you want your step sizes to be.
Let me just get this back to one [inaudible] Okay.
So we want to figure out how far we should go in the gradient and that's important.
Now, you might say,
okay this is always true.
Right? Like you always need to be careful when you're doing
gradient descent or ascent in any supervised learning problem.
Whenever you're using stochastic gradient descent.
Of course, you don't wanna go too far along your step size because you could overshoot
and you're using this linear approximation and it's bad.
Why does anybody- does anybody have any sense about why this might
be even worse in the reinforcement learning case?
Why might it be even more important to think about this step size.
And it has to do with where the data comes from. Yeah.
So when you have a bad policy
that affects the data you collect, and you might just go down a bad road.
Sure. So she's exactly correct.
In a supervised learning case,
your data is being generated by an IID distribution,
it doesn't matter what choices you just made for your stochastic gradient descent.
In RL that is determining the next policy we're
using inside of our iteration to gather data.
So we're not going to, you know, if we take a really bad, if we get really,
really bad policies, we just may be getting
no data towards the actual optima of this function.
So it's even more important to sort of carefully
think about where we're going along here and ideally hopefully get monotonic improvement.
So this is- this is the really,
it's very important in the reinforcement learning case,
to think about how we're doing this step size because
this determines the data we collect.
pi and therefore data.
And one version of, um,
one of my colleagues talks about a similar problem.
He sort of has the picture of the Roadrunner running off the cliff, right?
And like them you if you're in a part where your
I- your policy is just really really bad.
You may get no more useful data.
Then you can't get a good estimate there of the gradient and then you're just stuck.
You might get in a really really bad optima.
Okay. So we'd like to think carefully about this part.
So one thing that we could do,
is do something like line search.
So we're talking about right now sort of how do we do,
so how to do step sizes.
So one thing we could do is try to do some sort of line search along the gradient.
[BACKGROUND]
And this is, um, okay
but it's a little bit expensive.
So it's simple but it's expensive.
And it tends to sort of ignore where the linear approximation is good.
So we'd like to do better than this.
Okay. So now we're gonna go back to that point that I mentioned at the start
which is what we'd really like to be able to do is when we're doing
this updating we would like to ensure monotonic improvement.
And so can we kind of choose our step sizes in a way or
choose how far along the gradient to go in order to achieve monotonic improvement.
So what our goal is gonna be is we'd like to have it.
So that V pi of i + 1 is greater than or equal to V pi i.
And we're hoping to achieve this by changing how big of a step size we take.
All right. So let's think about what our- our objective function is again.
Um so we're getting- we have our value, our parameterized value.
So V of theta is equal to the expected value under our policy that's defined by theta of
just the sum over t = 0 to infinity of gamma t_r S_t a_t under our policy.
Okay. And this is where we just sort of
look at the series of states we get under our policy.
So that's our basic equation here in terms of
expressing the value of a parameterized policy.
And what we would like to do here is we would like
to get a new policy that has a better value.
But the problem is that we have samples from an old policy.
So when we're doing this we're gathering policies with
pi i and then we're trying to figure out what our pi i + 1 should be.
So this is gonna fundamentally involve.
Um so we have access to- we have access to
trajectories that are sampled from pi of theta.
And we now wanna sort of predict the value of v of pi of theta.
I'll put pi i, i + 1.
So we'd like to sort of now figure out what a new value would be
if we update these, update these parameters in some way and take like a max.
You know we'd like to figure out what the new parameters are.
But this is fundamentally an off policy problem because we have
data from our last policy and we wanna figure out what our next policy should be.
Okay. So what we're gonna do is we're gonna first re-express um the value
of our policy in terms of the advantage oh- the value
of our new policy in terms of the advantage over our old policy.
So I'm gonna move down to vanilla policy gradient for now.
[NOISE]
Okay. So what we have is we have V of Theta tilde.
So that could be like our new parameterized policy is
gonna be equal to the value of our old parameterized policy.
So whatever we had before plus the following.
The distribution over the states and actions we'd get if we
were to run our new policy. Now we don't know that.
But let's ima- let's ignore that for a second of a sum over
t = 0 to infinity gamma to the t, the advantage pi.
Okay. So this- this just generally holds.
Um, this doesn't have to do anything necessarily with being um, parameterized.
This is just saying the value of any policy which is here
parameterized by pi tilde is equal to the value of another policy plus
the sum over the states and actions you'd reach under
your target policy of the advantage
you get of taking this new policy over the old policy.
Okay. So that um, that just expresses how we can say what the
va- how uh, the value of a new policy relates to the value of the old policy.
It's exactly the same as the old values policy
plus the advantage you'd get if you were to
run the new policy and look at the state action distribution you'd encounter. Yes?
Should the subscript be Pi tilde on the advantage?
Should the subscript be Pi tilde on the advantage?
[OVERLAPPING] policy?
Yeah. So we're doing in it- let me write this up thing.
So [inaudible] question is a good one, let me just write this out to be for- for long.
Okay, so we've got V of theta plus sum over all the states
and we're gonna use mu pi tilde of s. So remember this was the stationary distribution.
Um we use this to denote the stationary distribution over states that we'd
reach um if we were to run our new policy which is parameterized tilde.
This is theta tilde. Okay? Um times the advantage function.
Okay. So what this is saying here is this S_t and a_t are under our desired policy.
And the advantage here is using the old one.
Okay? So this is allowing us to compare.
So what does this do? It's allowing us to compare
s_a S_t a_t minus
Q of S_t our old policy.
And this is under- so
it's allowing us to compare how much better is that if we take like our new action.
Okay? All right.
But one of the problems of this is we don't know this.
Yeah?
Sum over from t = 0 to infinity somewhere?
Oh, thank you um.
And answering- yeah. Yeah.
Yeah.
And also again [OVERLAPPING]
So the question is is [inaudible]?
No. And thank you for make me uh allowing me to clarify that.
So in this case what we're doing is we're taking um
an expectation over all time-steps and
this is saying over the trajectories that we'd get to under our new policy.
I've now reformulated and said well we have a stationary distribution over states.
If we look at what is the probability of reaching
those states and then we weight that by the advantage.
So we've went from a time averaging to a state averaging.
Does that makes sense?
So we can either think of our value function or averaging our value function across
time-steps where we can think here is
averaging across all the states and what is for each state.
What is the relative value you get by following your new policy versus your old policy?
[inaudible].
Oh sorry. Thank you. Then that- those are typos.
Okay. All right.
So this would be under-
so we look at the states that- for each state what is
the probability we'd reached that state under
our new policy and what is the relative advantage?
The thing that you're pointing to should not have
[inaudible].
Oh sorry. We have a tilde over here, this is our value of our original one
plus this we get the advantage term over all the states. Yeah.
Is there a difference between pi theta tilde and tilde pi?
I was just doing this here to make it clear.
Pi L- say Pi tilde is also- Pi tilde is
parameterized by, this is a policy parameterized by theta tilde.
Yeah. I'm just saying it's like in the expectation they wrote in the first slide
[inaudible]
Okay. We can vote on that side.
But either- I- I wanted to just be clear um, often this notation goes back
and forth between using do you wanna make
the policy explicit as opposed to just the parameters.
Um I think it's more clear to have a policy um and parameters here but often we also
use the- you can just directly parameterize the value function in terms of
theta as opposed to V pi tilde of theta.
But I'm gonna just use any of these is fine.
Is anybody confused about what this is?
I mean if it's easier I can just go like this.
Okay. So I can just remove all of this.
Okay. So this is just whenever I say pi tilde
that's the policy that's parameterized by the new- that's your new policy.
Okay. And that's this.
I know it's a lot of notation.
Does anybody have any questions about that notation last? Yeah?
Yeah.
Name first please.
So I guess I'm a little bit confused mostly
just because it's a little bit different from the slides.
And I'm just wondering-
Shouldn't be different by the slides but I'm trying to go-
[LAUGHTER] I was wondering sort in this case do we
sum over the possible actions for a given state or
as we've noted here do we assume that we take
a single action per uh by using the policy which is what we have here on the board?
You are right. I forgot that right.
I'm gonna go. Repeat that again.
Okay. Okay. Let's say V of theta
tilde I'll go with the same exact same notation as the slides is equal
to V of Theta plus sum over states or
stationary distribution over Pi tilde of s. This is the distribution we get.
Um this is the discounted weight of distribution under our target policy.
Under pi tilde. Okay. Sum over A.
Okay.
So this is the weighted affair,
this is the weighted distribution under the states.
We went from the time domain to thinking about the distribution over
states times looking at all the actions we might take under
our target policy and the relative advantage of each of those
over our previous policy, [NOISE] okay?
And this should look very, very similar to imitation learning in certain ways, right?
Like, so we're again sort of thinking about,
um, instead of thinking about subbing rewards over time steps,
we're thinking about what is the stationary distribution we might get to under
a new policy and how that compares to
the stationary distribution we would have had under our old policy.
Um, and what we're looking at so far is,
we're looking at the [NOISE] stationary distribution under our target policy.
The problem is, we don't know this,
so we can't calculate this.
This is just an expression.
Um, uh, but this is unknown [NOISE] because we don't have any samples from pi tilde,
we only have samples from pi, okay?
So we can't compute this.
Why would we, and just to go back,
why are we trying to do any of this?
We're trying to do this because when we do vanilla policy gradient,
[NOISE] we're gonna be trying to figure out a new policy at a,
that has a value that's better than our old policy.
What we did here is, we tried to estimate the
derivative out of the current pol- of the current policy,
um, but we don't know anything yet about the value once we take that step.
And so what we're trying to do here is to say, well,
can we somehow understand what the value will be of a new policy before we execute it?
And we're gonna do that by trying to relate it to what is our value of
the previous policy plus some sort of distance between the old policy and the new policy,
ideally computed in terms of things we can actually evaluate using our current samples.
That's where we're trying to go to, okay?
[NOISE] Right.
So we have this nice expression, but we can't compute it.
So we're gonna make up a new objective function, okay?
We're gonna do this one sort of backwards because we're [NOISE] gonna make it up,
and then, we're gonna show why it's a good thing to do.
So what are we trying to do?
We'd like to use something like this.
If we had this,
then we could compare the value of the new policy to the value of the old policy.
The problem is, we don't have this because we don't have the, uh,
the stationary distribution under the new policy.
So what we're gonna do instead is,
we're going to define an objective function L_Pi,
[NOISE] which is as follows.
It's the value of your old policy plus a sum over all your states,
the stationary distribu- discounted distribution of your old policy.
This is where it's different.
So this is [NOISE] the old,
your current policy, okay?
And then, the rest of the expression looks the same.
[NOISE] Now, notice, we can compute this, okay?
This is, um, we could just
average over all the trajectories we have for the current episode,
we could estimate our stationary distribution from our current data,
we could know for a new policy what its action or,
so if someone gives me a new policy pi,
I could evaluate this,
and I could also evaluate the advantage, okay,
because this advantage is defined only in terms of my previous Pi.
So as long as if I have a, uh,
a representation of the state action value function
for my old policy, I could evaluate this.
So now, all of this is evaluatable.
[NOISE] This might not be a good thing to do,
but we can compute this.
[inaudible]
and then, like, giving that to pi itself?
Yes. [inaudible] in terms of sort of notation where, like, yeah,
I'm using pi tilde interchangeably with,
you can, they- they're just some new parameters for computing.
So our policy is always parameterized by some set of parameters.
You can either think of that as just being a new policy,
or you can think of that as the new parameters, either is fine.
Uh, so could quickly explain why [NOISE] if you're given, um,
the pi tilde, why you won't be able to calculate the, uh, new [OVERLAPPING]?
You don't have to. So, um, so the question is,
if you're given pi to, uh, the new policy,
why can you not compute mu?
It's a good question because you don't want any data from that.
So someone has given you a new polic- [NOISE]
the- there could be ways to approximate [NOISE] this,
but the only data that you have right now is from [NOISE]
the current old policy, [NOISE] from Pi.
So you've run this out M times,
you've got M trajectories,
which are M trajectories gathered under your old policy pi, okay?
You don't have [NOISE] any data from pi tilde.
And in general, if pi tilde is not the same as pi,
you're going to get different trajectories.
So you don't have any direct estimate of this.
Does that makes sense to everybody?
So if we go back to [NOISE] the vanilla policy gradient, what do we do?
We had a policy pi_i,
we ran it out,
and we got D trajectories from that pi_i.
We could use that to estimate that mu.
That just gives us on policy data of what are
the states and actions we experience when we're [NOISE] following Pi_i.
We don't have any data of Pi_i + 1 yet,
we haven't run it. Okay. Yeah.
Uh, just to be clear,
to get an estimate of the stationary distribution for the old policy, [NOISE] uh,
you basically [NOISE] look at all the data that you have, uh, like,
all the trajectories, and see basically what fraction of
the time you're spending at one state. Okay.
Exactly. So [inaudible] is exactly right.
[NOISE] How would you go from this just raw data,
these te- D trajectories to mu?
You could just count, you know.
I mean, in general, if you're in really high dimensions,
you want to do something smoother than that,
you want to approximate the density function.
Um, but essentially, you can just directly,
i- in a type of so,
you could just count and [NOISE] just like,
how many times did I get to this state and take this action, and then,
that would give you a direct estimate of, um, the mu's.
[NOISE] In general, you're gonna want some sort of
parametric function in high dimensions,
but you couldn't fit that using, you could,
you could imagine this is parameterized itself,
and you can fit that using your existing on policy data.
Uh, intuitively, does this work because we
assume that distribution of the states [NOISE] won't change too much between policies?
Oh, yeah. The question is, intuitively, why does this work?
I've not told you why this works,
I've just said this is something we could do and that it's computable.
[NOISE] Um, and I haven't told you yet why this is a good thing to do.
But we're gonna show that, um,
that this is going to allow us to get to something which is a lower bound,
um, and then, we can improve on those lower bounds.
Okay. [NOISE] So just a quick thing to notice here,
which is, if you do this,
if you do L_pi of pi,
[NOISE] so that's just what is
this objective function if you plug in the old policy there,
this is just equal to [NOISE] V of the theta, [NOISE] okay?
So if you evaluate [NOISE] this function under the same policy,
it just gives you the value, okay? All right?
[NOISE] All right.
So conservative, [NOISE] I'll just briefly,
we'll have to continue this further later, but, um,
[NOISE] so we can use this to do what's known as conservative policy iteration.
So [NOISE] conservative policy iteration,
um, and the intuition here is,
let's just first just [NOISE] start with mixed,
um, a mixed policy.
[NOISE] So imagine that you have a new policy,
which is a mix of an,
of, um, an old policy and something different.
So you have 1 minus alpha times
your old policy plus alpha times some new policy pi prime, okay?
So that just means, you take some ol-
your current existing policy and you [NOISE] mix in something else, okay?
Then, in this case,
you can guarantee that the value [NOISE] of your new policy is greater than or equal to,
if you'd to take this objective function here and you evaluate it with your new policy,
so you take your new policy,
you evaluate under your old policy, you plug that in,
that's computable because you have data from
your old policy minus [NOISE] 2 epsilon gamma,
1 minus gamma squared alpha squared, okay?
So you can lower bound [NOISE] the value of
your new policy in terms of whatever this objective function
is when you compute it minus this expression, okay?
Um, I just wanna close with two other thoughts, which is,
note again that if you plug in alpha = 0,
[NOISE] that means that pi new is the same as pi old,
and this goes to 0,
[NOISE] which means that,
and since we know that this is equal to that,
that just says that your new policy has to be greater than or equal to your old policy.
And since the same, their policies are all the same, this is tight.
Okay. So we, um, this is a,
a little bit different than we expected because of,
um, [NOISE] the technical challenges with PDF.
Uh, so what I'll just close with here is that the next steps we'll go from this is to
show we can use this to essentially derive a lower bound on the new value function.
And we can show basically that if we improve across the lower bounds,
that we're guaranteed that the actual value function is monotonically improving.
[NOISE] So we will go through that.
Um, I haven't decided yet whether we'll go through
that on Monday because that's [NOISE] the midterm review
or if we'll wait on that until the following week after the midterm.
Um, the policy gradient,
uh, homework won't be released until after the midterm.
So we have a bit more time for that.
Um, and I'll go through also, like, the main takeaways
with policy gradient stuff when we conclude this part. Thanks.
 So before we get started, I'm just gonna say a
brief note about the logistics for the midterm.
We're gonna be split across two rooms,
the room you're in, it depends on your normal Stanford ID,
whatever the first letter is.
We'll send an email out about this to confirm it.
But we're gonna be in either Gates B1 or Cubberley Auditorium,
and it depends on the first- the first letter of your Stanford ID.
In addition, you're allowed to have one page of notes,
typed or written is fine, one sided.
Anybody have any other questions about the midterm?
Okay, well, reach out to us on Piazza if you have any questions about the midterm.
Um, what we'll do today is we're gonna
split- we're gonna go finish up the rest of policy gradient.
So in terms of where we are in the class, right now.
We are almost done with policy search.
We're gonna have the midterm on Wednesday,
Monday is a holiday and then we'll also be releasing the last homework this week,
which will be over policy search.
And so we're gonna have policy search,
and then we're gonna have the project.
That's the remaining sort of main assignments for the term.
And then we're gonna be getting into fast exploration and sort of
fast reinforcement learning after we come back from the midterm.
So I wanted to make sure to get through policy search
today because you're gonna have the assignment released later this week.
So we'll spend hopefully around like 20 to 25 minutes on policy search,
and then we're gonna do a brief review before we get
into the midterm- before- about the midterm material.
Does anybody have any questions?
Oh, and just a friendly reminder to please say
your name whenever you ask a question because it helps me remember.
It also helps everybody else learn your names as well. All right.
So where we were is that for the last couple of lectures,
we've been starting to talk about policy-based reinforcement learning,
where we're specifically trying to find
a parameterized policy to learn how to make good decisions in the environment.
And so just like what we saw with
value function approximation and what you're doing with Atari,
for our policy parameterization we're gonna assume there's some vector of parameters.
We can represent policies by things like softmax or by a deep neural network.
And then we're gonna wanna be able to take gradients of these types of
policies in order to learn a policy that has a high value.
So we introduced sort of the vanilla policy gradient algorithm,
where the idea is that you start off,
your initialize your policy in some way.
And you also have some baseline,
and then across different iterations you run out your current policy.
And the goal is we're, by running these
out that we're gonna be able to estimate the gradient.
So we're gonna be doing this part to estimate the gradient of our policy at the current, so
we wanna get sort of dV d theta with respect to our current policy.
So what we talked about is that you would run out trajectories from your current policy.
So you'd use your policy to execute in the environment.
You would get state action rewards, next state,
next section, next rewards.
And then you would look at returns and advantage estimates,
which compared your returns to a baseline,
could refit your baseline,
and then you could update your policy.
And so this was sort of the most vanilla policy gradient algorithm we talked about.
And then we started saying that there's a number of different choices um,
that we're making in this algorithm and almost all sort of
policy gradient based algorithms are gonna form uh,
follow this type of formula.
So, in particular, we are making a decision that's
sort of estimating some sort of return or targets.
Often we're picking a baseline,
and then we had to make some decision about after we
compute the gradient how far along the gradient do we go?
So this is sort of helping us to determine how far do we move on our gradient?
Okay, so for the first part,
we talked about how do we estimate sort of the value of where we are right now,
that we're gonna be using to try to estimate our gradient.
And we talked about the fact that the most vanilla thing that we could
do is just roll out the policy and look at the returns,
that this was really similar to what we'd seen in Monte Carlo estimates.
So we could do that,
we could get sort of an estimate of the value function by just
rolling out the policy for one episode,
but that was just like what we saw in
Monte Carlo, an unbiased estimator but high variance.
And so we talked about how we could play all the sorts of- we use all the same tools
as what we've been doing in the past to try to balance between bias and variance.
So in particular, we talked about how we could
introduce bias using bootstrapping and function approximation.
Just like what we saw in TD MC and just
like what we talked about with value function approximation.
So we repeatedly see these same sorts of ideas of the fact that we're trying to
understand what the value is of a particular policy.
And when we're estimating that value,
then we can trade off between getting sort of unbiased estimators of how
good decisions are versus biased estimators um,
that might allow us to propagate information more
quickly and allow us to learn to make better decisions faster.
We also talked about the fact that they're actor-critic methods that both maintain
an explicit parameterized representation of
the policy and a parameterized representation of the value function.
But the thing that we really started getting into last time is to say, "Well,
both you know there are all these sort of existing techniques that we know
of to try to estimate these targets and estimate the value function."
But then there's this additional question of how far do we move along the gradient?
So once we estimate the gradient of the policy,
we need to figure out how far along that gradient do we
go in terms of computing a new policy.
And the reason we argued this was particularly important
in reinforcement learning versus supervised learning,
is that whatever step we take,
whatever new policy we look at,
is gonna determine the data we get next.
And so it was a particularly important for us to think about how
far along do we wanna go on our gradient to get a new policy.
And one desirable property we were talking about is,
how do we ensure monotonic improvement?
So what we would like here is we'd really like
monotonic improvement, that's our goal.
And we talked about wanting monotonic improvement,
which was not guaranteed in DQN or a lot of other algorithms.
Because in a lot of high stakes domains,
like finance, customers, patients,
you might really wanna ensure that the new policy you're deploying is
expected to be better at least in expectation before you deploy it.
So we talked about wanting this,
but there's this big problem that we don't have
data from the new policies that we're considering.
And we don't wanna try out all the possible next policies
because some of them might be bad.
And so we wanted to try to use our existing data to figure out how do we
take a step and determine a new policy that we think is gonna be good.
So, in particular, our main goal for policy gradients is to try
to find a set of policy parameters that maximize our value function.
And the challenge is that we currently have access to
data that was gathered with our current policy,
which we're gonna call pi old.
That's parameterized by a set of thetas,
that we can also denote by theta old.
And- and throughout policy,
the policy gradient lectures that I've been through,
going back and forth between talking about policies and talking about thetas.
But it's good just to remember that there's, sort of,
this direct mapping between pi and theta.
You know there's- for each- for a policy,
it's exactly defined by a set of parameters.
[NOISE] So whether we're talking about policies,
or we're talking about parameters,
those are referring to exactly the same thing.
Um, so the challenge is- is that we have data from our current policy,
um, which has some set of parameters,
and we want to predict the value of a different policy.
And so this is a challenge with off policy learning.
So what we tal- were talking about last time, um,
is how do we express the value of a policy in terms of stuff that we do know?
Um, and we talked about how we could write it down in
terms of an advantage over the current policy.
So if we think about a value being parameterized by
a new set- a new policy with a new set of theta tilde parameters,
it's equal to the value of another policy parameterized
by a set of Theta plus the expected advantage.
So we can write that as the distribution of states that we would expect to get to under
the new policy times the advantage we
would get under the old policy if we were to follow the new policy.
And the reason- what- what we're trying to do in this case is just to- to
keep us thinking about what the main goal is here.
Is what we're trying to do is figure out a way to do
policy gradient where we're guaranteed to have monotonic improvement,
where our new policy is gonna be guaranteed to be better than our old policy.
But we wanna do this without actually trying out our new policy.
So we are trying to re-express what the value is of
a new policy in terms of quantities we have access to.
So what do we have access to?
We have access to existing samples from the current policy,
and we wanna use those and the returns we've observed in order to
estimate the value of a new policy before we deploy it.
So that's kinda where we're trying to get to.
And we noticed here that maybe, you know,
we can have access to an explicit form of a new policy,
that's like whatever new parameters we're considering putting into our neural network.
Um, and we could imagine estimating the advantage function,
but we don't know the state distribution under the new policy,
because that would require us actually to run it.
So what we talked about is- let's just define a new objective function,
um, which is just a different objective function.
Might be good, might be bad.
I'm going to argue to it's a good- it's good but this right
now is just a quantity that we can optimize.
So the quantity that we can optimize here,
we're gonna call that sort of this new objective function,
and it is going to be a function of the previous value.
So here, remember this is always equal to [NOISE] direct mapping between thetas and pis.
So we're going to just say,
it looks like the objective function we just talked about,
which really was the value of the new policy,
but we don't know what that stationary weighted distribution is,
of states under the new policy.
So we're just gonna substitute in the stationary distribution under the current policy.
Now in general, this is not gonna be- so
it's not going to be equal to your new policy distribution.
The only time you're gonna get
the same state distribution under two policies is generally if they're identical.
Occasionally, you can get the same state distribution under two different policies,
but then that means they have the same value.
So in general, we're going to expect that this is going to be
different but we're going to ignore that for now.
We're just going to say this is an objective function,
this is something we can optimize.
And a nice thing about this is that we have samples from the current policy.
So we can imagine just using those samples to estimate this expectation.
The thing that I also want us to note here is that,
this is just this new objective function called
L. If you evaluate the objective function L,
um, at the current policy,
so if you plug in your old policy into your objective function,
it's exactly equal to the value of your current policy.
So this second term becomes 0,
[NOISE] because the advantage of the existing policy over the existing policy is 0.
So this objective function is exactly equal to the value of the old policy,
if you evaluated the old policy,
and another case is for new policies it's gonna be something different. Yes.
How's this similar to importance sampling?
That's a great question.
You asked, "How is this similar to importance sampling?"
Um, if we were gonna do importance- Well,
it's different in a number of ways.
Um, in importance sampling,
what we tend to do is we re-weight,
um, uh, the distribution that we want by the distribution that we have.
Um, in this case we're looking,
and we normally do that on a per state level.
In this case we are looking at the stationary distribution over states.
It's actually a really cool paper that just came out in NeurIPS,
like a month ago, um,
[NOISE] 2018, with Lihong Li and some other colleagues.
Um, they looked at,
how would you re-weight
stationary distributions to try to get off policy estimates of the value function?
Um, and so to try to directly re-weight what,
like, mu pi would be versus mu pi tilde.
So we're not doing that here,
there's some really nice ideas in that that could help
really reduce the variance in long horizon problems.
Um, in this case, we're just substituting,
so we're ignoring the difference.
We're not doing importance sampling,
we're just pretending that the distribution of states that we get to is exactly the same.
It's not, but we're gonna show that this is going to end up being a useful
lower bound to what we wanna- what we actually want to optimize.
Okay. So you might say,
if you take this objective function which might be good, might not be good.
If we optimize with respect to it,
do we have any guarantees on whether
the new value function that we get if we optimize with
respect to this wrong objective function is better than the old value function?
Because remember that's where we're trying to go to.
We don't really care what we're optimizing,
what we care about is that the resulting value function we
get out is actually better than the old value function.
[NOISE] So last time I said that if you have
a mixture policy which blend between your current policy and a new policy,
so let's say you have a pi old and you have some other policy,
I haven't said how you get it,
but just say you have some other policy,
and that defines your new policy.
So with probability 1 - alpha,
you take the same action as you used to.
With probability alpha, you take a new action.
In this case you can guarantee a lower bound on the value of the new policy.
So the value of the new policy is greater than or
equal to this objective function we have here,
minus this particular quantity.
So that says that if you optimize with respect to this weird L objective function,
you can actually get bounds on how good your new policy is.
So that seems promising,
but in general we're not going to want to just consider mixture policies.
Okay. So what this theorem says is that,
for any stochastic policy,
not just this weird mixture,
you can get a bound on the performance by using this slightly strange objective function.
So in particular, um,
define the distance of total variation as follows.
So DTV between two policies,
so I'm using a dot there to denote that there's,
um, there's a number of different actions.
The- the policies there are denoting a probability distribution over actions.
This is equal to the max over all a,
the distance between the probability that each of the two policies put on that action.
So it's giving us sort of a maximum difference,
in what's the probability of an action under one policy versus the other policy.
And then we can do- [NOISE] Bless you.
-we can do D max of total variation by taking the max of that quantity over all states.
So it's essentially saying,
over all states was the biggest difference that
the two policies give over a particular action.
So where do they most differ?
And then what this theorem says is that,
if you have that quantity,
in general we're not gonna be able to evaluate that.
But what that's saying is that,
if you know what that quantity is,
then you can define that.
If you use this objective function L,
that the new value of your policy is
at least the objective function you compute minus this quantity,
it's a function of the distance of total variation,
the max distance and total variation.
So this gives us some confidence that if we were to
optimize with respect to the objective function L,
then we can get a bound on the value function.
Now this distance- this max or
the total variation distance isn't particularly easy to work with.
So we can use the fact that the square of it is upper bounded by the KL divergence,
and then get a new bound which is a little bit easier to work with.
That looks at the KL divergence between the two policies,
and we again get the similar bound.
Okay. So why is this useful?
So what I've told you right now is that we have this new objective function.
If we use this new objective function,
we could- in principle get this lower bound on the performance of the new policy.
So how do we use this to ensure that we wanna get monotonic improvement?
So the goal is monotonic improvement.
We want to have the V_ Pi i + 1 is greater than or equal to V_ Pi i. That is our goal.
So i is iterations,
we want that the new policy that we deploy is actually better than
the policy we had before. So how are we gonna do this?
So what we're gonna say is,
first we have this objective function here,
this lower bound objective function,
[NOISE] and what we're going to define is that Mi of pi i
is equal to L of pi i pi.
So I'm just copying the equation from the previous, um, previous slide,
- 4 epsilon gamma divided by 1 - gamma squared DKL_ max of pi i.
Okay. So this is the lower bound.
That's what we just defined on the previous slide.
Okay? So what we said here is that,
the value of our new policy- so this is equal to this M function I've defined.
So we've said that the new value is gonna be at least as good as this lower bound.
So we're gonna say V_i + 1 is gonna be equal to Mi of pi i + 1,
which is equal to L pi i + 1.
I'm just writing out what the definition is here.
And again, what we're trying to do here is get to the point where we're
confident that we can get something that's better than our old value function.
Now, the thing that I want to now look at is, well,
what is- if we were to evaluate the lower bound at the current policy what would that be?
So let's look at Mi of pi_i.
So that's going to be equal to L pi_i of pi_i.
I'm just plugging it into this equation up there,
- 4 epsilon gamma - gamma squared DKL max of pi_i, pi_i.
Okay, so why is this nice?
Well, this is nice because
the KL diver- divergence between two identical policies is 0.
So these are exactly the same.
This is equal to 0. So now,
this is just equal to L pi_i of pi_i.
But what I told you before is that if we go back
a few slides to what the definition is of pi_i,
L of pi_i is that if you evaluate it at
the current policy it's just equal to the value of that policy, okay?
So if we evaluate this objective function at the current policy,
it's just the same as the value of the current policy.
So now, if we go back here this is just equal to V of pi_i.
Okay. So what does this say?
This says, that if I wanna look at how my I- my-
the value of my i + 1 policy looks compared to the value of my old policy,
we know that's greater than or equal to Mi of pi_i + 1.
So because we said that the V that we knew from this theorem,
that the new value of the policy is greater
than or equal to this lower bound we computed.
So it's greater than or equal to Mi of pi i + 1 - Mi of pi_i.
So what does this say?
This says that if
your new value function has a better lower bound than your old value function,
you have monotonic improvement.
So if this is greater than 0, then monotonic improvement,
which means that if you optimize with respect to this lower bound and
you can evaluate that quantity and your new
lower bound is higher than your old lower bound,
then your value has to be better.
So we can guarantee monotonic improvement. Yes.
So just to clarify. So for these value comparisons,
are we implicitly considering as an infinity norm,
in terms of saying one is better?
Yes, generally. Yeah, I think, I mean,
they probably go through with L squared 2.
But yeah, yeah.
Um, question is whether or not we're always defining
this with respect to L infinity norm almost always.
Um, ah, there certainly is some analysis particularly when we
get into a function approximation which looks at an L2 norm.
Um, but most of this is all with respect to an L infinity norm,
which means that when we're looking at this,
for example, we're looking at, um,
ensuring that for all states, um,
the value of those states is at least as good as the previous value of the states. Yes.
So the- the claim was if our lower bound improves,
then it must be the case that what is the lower bound must also be improving, right?
Yeah.
Uh, so there's never a case where, for example,
your lower bound might improve even though the actual value of the policy
evaluated decreases, it seems like.
That's right. So what this- this is- what this
is asking us if you know this lower bound and what that relates to the actual value.
What this is stating is that if you improve
your two low- like if you have a lower bound of your existing policy and you get a new
lower bound with for some new policy and that new lower bound is
higher than your lower bound of the other one, that you're guaranteed to be improving.
So this is what guarantee- so this is assuming you can solve this.
But if you, um,
if you can get this lower- if you optimize with respect to this lower-bound quantity,
um, because when you plug in the lower bound for the current policy under that,
that's exactly equal to th- the value of
that policy then you are guaranteed to be improving.
Because you're basically saying, here's my lower- here's my existing value.
I have something whose lower bound is better than my existing value.
And so I know my new thing has to be better. Okay. Yeah.
How do you like, um, you get the epsilon term back because it seems
like the epsilon changes depending on your pi and it's also like a global property.
Absolutely. So, um, now this discussion is a great one.
I- few might ask us about this.
Um, ah, so note this,
that your lower bound is in terms of epsilon.
Epsilon is a max over all states and actions of your advantage.
Um, in principle, you could
evaluate this [LAUGHTER] particularly if you're at a discrete state and action space.
I- in practice, that's something that you would not wanna do.
Um, this, I view this part as sort of
saying this is formally if you could evaluate this lower bound.
Um, and what we're gonna do now is talk about, um,
a more practical algorithm which tries to take,
um, this guarantee of conservative policy improvement and actually make it practical,
in terms of quantities that are a little bit easier to compute.
Because that's right. Yeah, in general,
it- it would be very hard to evaluate this this epsi- uh, this epsilon.
Now you could take upper or lower bounds on it.
Um, but you won't generally know what this epsilon is.
And note that this, uh, as co- just pointing out,
this epsilon is dependent on the policy.
Um, so- okay. All right. But this is pretty cool.
So it means that you can do this guaranteed improvement.
This is a form of mineral- minimization maximization.
Um, and it's this nice idea of sort of saying you can have this new
lower bound that's guaranteed to be better than the value of your current policy.
So you can get this sort of conservative monotonic improving policy.
All right. So I
just- I wanna make sure we have enough time to go through some of the midterm review.
But I wanna briefly talk about how we would make this practical.
Particularly because, um, trust
region policy optimization is an extremely popular policy gradient algorithm.
So I think it's useful for you guys just to be aware of.
Um, you- some of you might use it in some of your projects.
Um, we won't cover it in uh- won't be a mandatory part of the homework,
um, or on the midterm.
But I think it's just a useful idea to be familiar with.
So again, if we look at sort of what this objective function was that we just discussed.
We said we had this L function and then we turned it into a lower bound
by subtracting off this constant that might be hard for us to compute.
And so what we do in this case is we take this constant
here and we turned it into a hyperparameter.
So you could turn it into a constant C. Um,
but the problem wi- always is that even if you could compute this,
um, often we don't know what this is.
But even if you could compute it or compute a bound on it, um,
generally, if we use this,
we would take very small step sizes.
So intuitively, this is because, um, you know,
it's often very hard to extrapolate far away,
um, from your current policy.
And so this would say if you wanna be really sure that
your new value is better than your old value,
then just take a very small step size.
And intuitively, it's because if you change your policy very,
very small amounts at least under some smoothness guarantees,
um, the value of your policy can't change that much.
You know, it should also be intuitive that, like,
your gradient is often
a pretty good estimate very close to your current value of your function.
But we also need to quickly try to get
to a good policy so this is not generally practical.
Um, and so the idea of sort of TRPO,
one of the main ideas is to think of it being kind of a trusted region.
Um, and- and use this to constrain our step sizes.
So again, if we go back to this sort of the gene- generic, um, ah,
template for policy gradient algorithms,
we have to make this choice of how far out to step in our gradient.
Um, and the idea is we're going to sort of define a constraint.
So we're going to have our objective function here.
And instead of explicitly subtracting off our lower bound,
we're just going to say you could move.
You can change your gradient but not too far.
We're gonna put, uh, a constraint on how far
the KL divergence can be as a way to just sort of say
you're kind of having this region of which in your- in
your parameter space that allows you to know how far you can change your policy.
Okay. Yeah. All right.
So I'm just going to talk very briefly about how this is instantiated.
Um, so the main idea is that if we look at, um,
what these objective functions are,
um, this may or may not be easy for us to evaluate.
So if we look back at what,
um, our theta is, um, even here, right,
we have sort of our discounted visitation weights under the current policy,
but we don't have direct access to that.
We have only access to samples from rolling out our current policy.
So the first idea is that instead of taking an explicit sum over the state space,
where that state space might be, you know,
continuous and in-infinite, we're just going to look at
the states that were actually sampled by our current old policy and re-weight them.
So that's the first depa- the first substitution we do.
Yeah. What we're trying to do right now is say we have
this objective function and we wanna make it so that this can be part of
an algorithm where we can compute all the quantities we need to in order
to take a step size where we think the new policy is gonna be better.
Um, the second thing we do and this relates to,
um, question about importance sampling.
Um, I- is we have this second quantity in here,
where this is the probability of an action under our new policy.
Um, we do have access to that, in the sense that,
if someone gives us a state we can tell, um,
we can say exactly what our probability would be under all the actions.
But again, this often can be a continuous set.
And so instead of doing sort of this continuous set, we are just going to
say we're gonna use importance sampling and we can take samples.
This is typically goi- going to be from pi old.
So we look at what times we have taken an action given our current policy and we
re-weight them according to the probability
we would have taken those actions that drive the new policy.
So it allows us to approximate that expectation using data that we have.
And then the third substitution is switching the advantage back to the Q function,
and it's just important to note that all of these three substitutions don't
change the solution to the object- to the optimization problem.
These are all sort of taking at, uh,
these different substitutions or different ways to evaluate these quantities, okay?
So we end up with the following: um, uh,
we have this objective function that we are optimizing.
This is after we've done the substitutions I just mentioned,
and we have this constraint on how far away we can be.
Um, and empirically, they generally just sample, um,
this sort of alternative sampling distribution Q is just your existing old policy.
So there's a bunch of other stuff in the paper.
It's a really nice paper.
Um, a lot of really interesting ideas.
Uh, I will just,
I will skip through sort of exactly how they do some of the additional details.
There's some nice complexity there.
Um, but I will just say briefly the main thing
they're doing here is they're sort of running a policy.
They're computing this gradient.
They have to, um,
consider these constraints, um,
and they do this sort of line search with a KL constraint.
And perhaps the most important thing is just to be aware of this and just sort of
understand kind of them being inspired by this conservative policy improvement,
and then trying to make that more practical and fast.
Um, they've applied it
to a lot of different problems.
Um, there's some really nice stuff on locomotion controllers,
cases where you have continuous action spaces, continuous state spaces.
These are cases where policy gradient is often very helpful,
uh, and they have some very nice results.
Um, I won't step through this here.
Um, the main thing to know is that empirically this is a really good tool to know about.
Often, if you're doing policy gradient-style approaches,
TRPO can be a very useful thing to build on,
um, and it's been incredibly influential.
This was came out in ICML in 2015.
There's hundreds of citations to it already.
So this has sort of become one of the main benchmarks for policy gradient.
Okay. So if we go back just to kinda what
the, to summarize what the policy gradient algorithm template is,
whether you're looking at the existing algorithms
or whether you're trying to define your own,
generally, they look like something like the following.
For each iteration, you run your policy out and you
gather trajectories of data by running that policy.
You compute some target that might be just the rewards,
that might be a Q function.
We can trade off between bias and variance in that, um,
and then we use that to estimate the policy gradient,
and then we may want to smartly take a step along
that gradient to try to ensure monotonic improvement.
Um, the things to be aware of and some of the things you're going to have practice
on soon i- is that you should be very familiar with
these sort of vanilla approaches and REINFORCE,
um, and this general template and sort of understand how some of
the different algorithms we're talking about might instantiate these different things.
Um, you don't have to derive and remember
all the formula that I just went through quickly for TRPO,um,
and you will have the opportunity to practice these more in homework 3,
but we'll only cover these lightly in terms of the midterm.
All right. So is somebody may have any questions about this before we go into
sort of a short overview of the stuff we have done so far for- before the mid-term?
[inaudible]
Okay. All right. Let's switch over.
Okay. So what this is going be is sort of a very short recap of what we have,
uh, done so far.
And in terms of why this is useful, um,
there's certainly a lot of good evidence from learning
sciences that space repetition of ideas is really helpful,
as is forced recall,
which is one of the other benefits of doing exams.
Um, uh, so, uh,
that's what we are going to do today is just sort of do
a quick recap of a lot of the different main ideas.
So again, uh, reinforcement learning generally involves optimization,
delayed consequences, generalization, and exploration.
We haven't really talked about that yet.
Um, so that's not really going to be on the midterm.
Um, we are going to start talking a lot more about that post the mid-term.
It's an incredibly important topic,
one I think is super fascinating and one of the main reasons why RL is interesting.
Um, but these other things are really important
too and we spent some time on those so far.
So in terms of thinking about the mid-term and indeed thinking about the class,
um, on the very first day,
I put up this sort of blizzard of learning objectives, um,
and I just want to highlight, uh,
a few of these which are the things that I mentioned
were going to explicitly evaluated in the exam,
which is that, um,
by the end of the class including on the exam, um,
you should be very familiar with sort of what are the key features of reinforcement ment-
learning that make it different than
other machine learning problems, and other AI problems.
So we spent some time on that on the first day,
um, and I've sort of tried to talk about it throughout.
But the fact that the agent is collecting its own data and that the data,
um, it gathers influences the policies it can learn.
So we sort of have this censored data issue.
The agent can't know about other lives that didn't li- didn't
live, it makes a very big difference compared to supervised learning.
Um, a second really important thing is that if you are given an application problem, um,
it's important to try to know why or why not to
formula- formulate it as a reinforcement learning problem.
Um, and if so, how you would.
Generally, there's not a single answer to this.
So it's good to think of like what is one or more way to define the state-space,
the action space, the dynamics,
and the reward model, um,
and what algorithm you would suggest from class to try to tackle it.
This is in general sort of, uh,
something that you'll probably run into much more
than like looking at any particular algorithm,
um, particularly in industry.
And then a third thing that I think is really important is to
understand how we decide whether or not an RL algorithm is good.
And so what is the criteria for performance and
evaluation we can use to sort of evaluate,
um, what are the benefits, strengths and
weaknesses of different algorithms and how they compare.
So this could be things like bias and variance.
It also could be computational complexity,
um, or sample efficiency or other aspects.
So what we have covered so far is planning where we know how the world works, um,
policy evaluation, um, model free learning how to make good decisions,
value function approximation, and then imitation learning and policy search.
And we've have also talked about the fact that for reinforcement learning in general,
you can think of either trying to find a value function of policy or a model,
and that model is sufficient to generate
a value function which is sufficient to generate a policy,
um, but they are not all necessary that you
don't have to have a model in order to get a policy.
So, um, I will go through
this part pretty fast and so I think a lot of you guys have also seen some of this stuff,
um, in previous classes.
So we're- almost everything we have been talking about so
far assumes the world is a Markov decision process.
But I have mentioned that often the world is not a Markov decision process.
Um, and in the MDP case,
we assume that the, the state is sufficient.
Um, a sufficient statistic of all the prior history.
So we don't have to keep track of the full set of states and
observations and actions rewards from the whole time period,
but we can just look at the current observation
in order to make good decisions in the world.
Um, in terms of this,
i- it's very useful to know what the Markov property is, why it's important,
why it might be violated,
what are things like models, value,
functions, and queues, um,
and what is planning, and what is the difference.
So in planning, we assume that you are given a model of how the world works,
you know the dynamics model,
you know the reward model, it still can be really hard to figure out how to act.
This is like knowing the game of Go, and it's still really,
really computationally intensive and tricky to try to figure out what's
the optimal decision to take in Go even though you
know all the dynamics and all of the rewards.
In learning, we don't know the dynamics and rewards and we still
have to gather data in order to learn a good policy,
which has a high value, a high discounted expected sum of rewards.
We talked about the Bellman backup operator, which is a contraction.
If your discount factor is less than 1, um,
which means that with repeated applications you are
guaranteed to converge to a single fixed point.
We talked about value versus policy iteration.
In value iteration on the iteration k,
you are always computing the optimal value is if you only get to make k decisions,
um, and then you use that to back up aga- and get the k + 1 policy.
In policy iteration, you always have a policy
and the value of that policy if you were to act using it forever.
Um, but it might not be a very good policy and then you update this.
And as we have seen,
it's closely related to sort of
policy gradient-style algorithms where
you sort of try to estimate the gradient of a policy.
So in policy iteration generally,
and similar to what we have been seeing in policy gradient approaches,
we intermix evaluation and improvement.
So we compute the value of a policy and then we
use that in order to take a step and improve it.
Um, if we are in the case of being model-free, um,
and not having extra model,
we often want to compute Q-values instead so that we can directly improve the policy.
So let's just take a quick second.
Um, so these are check your understandings, they're good things to go back through.
These are all sort of like, you know, um,
sm- small conceptual questions of the type that we might ask you on the exam.
So let's just take a minute, um,
to check our understanding and think about for a finite state and action MDP,
the lookup table representation,
which means that we just have a table entry for each state and action.
Um, gamma less than 1, uh,
does the initial setting of the value function impact
the final computed values? Why or why not?
Does value iteration and policy iteration always yield the same solution?
And, um, is the number of iterations needed for
poli- policy iteration in a finite state and action MDP bounded?
And if, so how many? Let's just take a minute and think about those.
Feel free to talk to somebody next to you.
[BACKGROUND].
All right. We're-
We're gonna vote. So um, I'm gonna ask,
who thinks that the initial setting of value- of the value function does not matter?
Great. Yes, it does not matter,
and it doesn't matter, so no.
Doesn't matter and why not?
Because there's only a single fixed point.
Because it's like in- uh, the Bellman operator's a contraction operator. Just single.
And how about- who thinks the value
iteration and policy iteration always yield the same solution?
Yes? No. Who thinks what- uh, why no.
Give me an example where they might not.
[NOISE] Yeah.
Verbal [inaudible].
Exactly, yeah. So that it is correct.
You- you're gonna get the same, uh, value function.
So it depends which way you're trying to answer this.
They're gonna have the same value,
they could have different policies.
And that's possible if there's
more than one policy that has the same optimal [NOISE] value.
That can come up because often there's, um,
multiple policies where you're splitting ties.
Um, I- who thinks that the number of iterations needed for policy iteration is bounded?
[NOISE] That's right.
Um, anyone wanna tell me how many it is?
A S.
[NOISE] That's the- that's the total number of policies,
um, and policy iteration in tabular MDPs.
With like pol- or policy improvement in tabular MDPs,
you're guaranteed to be monotonically improving.
So you can at most go through every policy once and then you're done.
So it sort of relates to what we were just talking about.
In that case, um,
you definitely get guaranteed policy improvement,
because every- there's no function approximation there, there's no errors,
you exactly know what the current value is,
then you can take a monotonic improvement step.
All right. So now we're gonna talk briefly of
a refresher on model-free policy evaluation.
Um, so this is
model-free policy evaluation was this sort of passive reinforcement learning,
um, where we're just trying to understand how good an existing policy is.
Ideally, with not too much data.
Um, and so we want to either
directly estimate the Q function or the value function of a policy.
And so we talked mostly in this case about episodic domains.
When I say episodic domains, I mean,
that we are gonna act in the world for a fixed number of steps,
or where we are in a setting where we know we have terminal states,
so we know the episodes will end.
With probability 1, they have to end.
Um, and then at that point you reset to a start state with some fixed distribution.
[NOISE] And in Monte Carlo approaches,
we directly average the episodic rewards. It's pretty simple.
We take our existing policy,
we run it out for H steps or until the end of, um, the episode.
We reset, we repeat that a whole bunch of times, then we just average.
Um, but in TD learning or Q learning,
we use a target to bootstrap.
And I know you guys have seen this a number of times,
but just as a refresher, um,
and I like these diagrams to start thinking about the distinctions.
So when we've talked about dynamic programming here,
we've thought about the case where we know the transition model,
we know the reward model.
So when we think about what the value is of a policy,
it's exactly equal to the expected distribution of states and
actions we would encounter by following this policy of the reward
we would get plus gamma times the value of the next state.
So note that when we think about this expectation here there's really an s prime.
So that expectation is thinking about all the next states that we might get to.
And so in dynamic programming,
we just explicitly think about that sum.
That sum over all the next states we much reach- might reach,
and the value of each of those states.
So if we had started in a state,
we take an action,
we get to some next new states.
In general, we could repeat this process all the way out till we reach,
you know, the horizon H or terminal states.
Um, and this- what we think of here
is taking an expectation over the next states that we would reach.
And what we do in dynamic programming is instead bootstrap.
So what we mean by bootstrap here,
is that instead of building this whole tree,
we keep [NOISE] track of what the value is of all the states,
and we use that to take an explicit expectation over the next states we'd reach,
and average over the value of those next states.
And note that in this case,
we're assuming we know the model.
Now, there are ways to extend this where we don't know the model,
but we haven't talked very much about those,
this term so, um.
But when I say dynamic programming here,
I mean, that unless we otherwise specified that we know the models of the world.
So this is a case where we're bootstrapping because we are- our update is using V,
for V uses an estimate.
Okay? 'Cause those values are not going to be perfect estimates
of the true expected discounted reward for those next states,
because we're still computing them.
So then we looked at Monte Carlo policy evaluation,
and it looks pretty similar in many ways except for what we're doing
is we're running a trajectory all the way out to the horizon,
we're adding up all the rewards,
and that is our target.
And when we say that policy, um,
evaluation with Monte Carlo is sampling,
it means we're sampling the return.
What is the expectation that we're approximating?
We're expectation- uh, we're
approximating an expectation over that probability of s prime.
[NOISE] So we only got a single s prime,
instead of getting an expectation over all the next ones.
And the problem with that is that we said it was high-variance,
even though it's unbiased.
And then we talked about combining these ideas with temporal difference methods,
um, where we're both gonna bootstrap and sample.
So we're sampling because [NOISE] we are only looking at a single next state,
[NOISE] and we're bootstrapping [NOISE]
because we're plugging in our estimate of V. So we're sampling a single s,
t+1, and we're bootstrapping because we're not
rolling all the way out like we did with Monte Carlo,
which is plugging in our current estimate of that value function.
So let's do another quick understanding of,
um, for each of these cases, um,
it's good to know whether- uh,
whether it applies to dynamic programming, um,
which requires you to know the models,
um, Monte Carlo or TD learning.
So is it usable when we don't know the models of the current domain?
Does it handle continuin- continuing non episodic domains?
Does it handle non-Markovian domains?
Um, let me be clear by that- clear what I mean by that.
You can always apply any algorithm to anything.
It just may give you garbage out.
And so my question is, um,
when you- when I say handling non-Markovian domains,
is it guaranteed to do something good or does
it fumbling-fundamentally make a Markov assumption?
Um, does it converge to the true value of the policy and the limit of updates?
Right now, we're thinking about tabular case.
So the value function is exactly representable and
is it giving us an unbiased estimate of the value along the way?
The estimates still might be consistent,
which means that eventually with an updater they converge to the right thing,
but they could give you biased estimates along the way.
So again, let's just spend like a minute or two and, um,
they are- this- there's just binary answers to each of these.
So yes or no for each of them.
And feel free to talk to somebody next to you.
[BACKGROUND]
Converge to different quality [OVERLAPPING].
But you could start with [BACKGROUND].
Yeah, it's a great question. Policy iteration [inaudible] good question.
[BACKGROUND]
All right.
I'm gonna ask people to vote again.
Okay, so, um, I'll just ask you to raise your hands if the answer is, yes.
So, um, is DP,
is dynamic programming usable when there are no models of the current domain?
No. Is Monte Carlo usable?
Yes. Is TD usable?
Great. Okay, um, does DP handle continuing non-episodic domains? Raise your hand if, yes.
Correct. Yep. So, you can use dynamic programming.
You can use Bellman operators and contractions even if the,
you know, for infinite horizon domains.
You generally want your gamma function to be less than one,
so your values don't explode.
Um, uh, but you can do it.
It's fine. What about Monte Carlo estimates?
No. Monte Carlo only updates when you get to the end of an episode.
TD estimates? Yes. Great. Um, does DP handle non-Markovian domains?
No.
No. Does Monte Carlo?
Yes.
Yes. TD?
No. Again, you can run all of these things wherever you want,
but- Converges to the true value of the policy and the limit of updates for DP?
Yes.
Yes, Monte Carlo?
Yes.
Yes. TD?
Yes.
Yes. Unbiased estimate of the value,
DP it's kind of not applicable,
because we're not really using data.
It's sort of a little bit different.
Um, Monte Carlo was an unbiased estimate of the value.
Yes.
Yes, TD?
No.
Great. Okay. So, um,
and if we're asking you about this in the exam
we'd be sure to clarify whether we're talking about
the tabular setting or
the function approximation setting where everything can be very different. Yes?
Can you explain exactly why TD doesn't work for non-Markovian?
Yeah. So, yeah that's a good one.
So why does TD not work for Markovian?
Um, it's because it's fundamentally making a Markov assumption,
um, about the domain.
And the reason that comes up is here.
So the way it is writing down the value function,
is it saying that the expected discounted sum
of rewards from the current state is exactly
equal to the immediate reward plus
the discounted future sum of rewards for each of the next states,
where that's encapsulated only by St + 1.
So that is where you're making the Markovian assumption,
because your aliases- if,
um, if you have an observation space which was
aliased that would ignore the whole history.
Whereas Monte Carlo is summing up all the rewards from
that current state onwards. Good question.
[inaudible] has that assumed Markovian process?
Great question, and remind me your name.
Saying, um, we talked almost everything we've been talking about is TD
0 where we just have this reward plus gamma times the value function,
but we also talked briefly about N step.
Um, where you sort of would do r1 + r2 + gamma times r2 et cetera.
So for the n-step you'd have something like this.
You'd have rt + gamma rt + 1 + gamma squared
rt + 2 + gamma cubed V of st + 3.
So that would be like an n-step.
Um, and that is essentially making different notions of Markovian assumptions.
Because you can have continuums you can either have
completely non-Markovian domains or we can have things like n-step Markov domains.
Which essentially means that if you're making it-
keeping track of a certain amount of history.
[NOISE] So um, just to sort of give
an example that similar to some of the ones that we've seen before.
We can think of something like a random walk process.
So imagine that we have a domain where we have three states and two terminal states.
So we always start in state B and
then with probability of 50% we go left or right.
Um, and if you reached either the black nodes then the process terminates.
Um, and when you get there either you get + 1 on this one or you get 0.
And it's a random walk with equal probability,
um, until you get to a terminal state and then the process ends.
And so in this case,
we could try to compute like what is the true value of the state.
Um, so the true value of a state in this case, um,
would involve us thinking about what is the, uh,
distribution of states that you would visit under this random walk pro- process.
So for example, um,
if we think about what the value is of- I'll do this here.
So if you think about what the value is of state C,
that's always gonna be equal to the immediate reward plus
gamma times the sum over the next states,
value of S prime.
Well, let's call this one like I know,
SD and this one S0.
So SD's value, is always gonna be equal to + 1.
So V of SD is equal to + 1,
um, because you get that reward and then it terminates.
So this would say with, um,
gamma times half probability you would go to the value of SB,
SB plus half you get 1.
And eventually if you look at this distribution it's gonna be,
um, so you could do this process for each of the different states.
And what you would find when you do this is that you get
through this random walk terminating on the right side
or the left side in terms of the probability distribution
and you could compute the values for this.
Um, in an exam, we would probably make this a little bit easier,
but it's good to be able to sort of look at this example and work through it,
um, and see what this part would be in terms of the value function.
Um, then the next question is let's imagine that we have a particular trajectory,
we want to compare what would happen under different algorithms.
So let's imagine what we have is we have, um,
a trajectory where you go BC,
BC terminal + 1.
So that's our episode.
So what is the first visit Monte Carlo estimate of B?
One.
One. That's right.
So, so V of B is equal to 1. Why is that?
Because what we do in Monte Carlo is we add up, versus at Monte Carlo,
we look at the first time we visited the state and we add
up all the rewards we get from that state till the end of the episode.
In this case, that reward is just 1.
So, the estimate of this would be 1.
The only other, you know,
thing that we might want to know about there is
if you're doing sort of this sliding average
like an alpha estimate to update the Monte Carlo estimate,
you'd want to know what the initial values were and what alpha was.
But let's imagine that here you just look at exactly taking that return.
So this is equal to the return starting at B going to the end of the episode.
So then the next question is, um,
what are the TD learning updates given the data in this order?
C terminal + 1 BC0,
CB0 with a learning rate of A.
Maybe just take like a minute or two and,
um, do one or two of these updates.
And then think about what would happen if we
reverse the order of the data with the same learning rate.
So, this relates to a point we've talked about a couple of
times about whether or not the order of
updates we do given some set of data matters in terms of the values we compute.
So I guess I would go at this in the following way.
I would first commit yourself either way whether or
not the order matters in terms of the values we're gonna compute,
um, and then try to compute one or two of them.
So let's just spend like a minute or two to decide whether or not the order matters here
[NOISE] in terms of the resulting values and then we can also compute.
[NOISE]
I know I'm not giving you guys enough time to do all the computations here,
but this is mostly to just sort of do that forced recall aspect to try to
remember exactly what the formulas are and then remember whether or not this matters.
So I'm just gonna ask you to vote.
Um, who here thinks that the order matters in terms of some of the values we compute?
[NOISE] It's right. No, it won't always.
Sometimes do- you can do things in different orders um,
the fact that we've emphasized a lot might lead you to believe that it always matters,
but it doesn't always matter. But in this case it does.
So um, in this case,
if we look at what the value is in the first-order,
what we would do is we'd say V of C = 0 + alpha 1 - 0.
So the new reward we've observed.
That would be alpha, then when we're computing the value of B,
we could use the new B of C we just computed because when we have this update,
now we've already got a non-zero estimate for V of C. Um,
note to be precise,
I should have told you here exactly how we're initializing all the values.
So in this case, we've implicitly assumed that
the initial values are 0 which matters a lot.
[NOISE] We'll talk some more, um,
in a week or two about
smarter exploration and the fact that being optimistic often really is very helpful.
One challenge can be in deep neural networks is
how to set things so that they're optimistic.
Um, but in this case,
so we're assuming that everything is 0,
so V of B will be alpha squared,
V of C will be the following expression.
Um, these are basically me just applying TD learning to these cases.
[inaudible]
They're in the second line, yeah.
Good catch. [NOISE] In the, where?
Should be gamma squared.
Gamma squared, yeah, yes,
yeah, that final expression.
Thanks. Um, so which appears in the third line.
[LAUGHTER] If we do it in the reverse order,
V of C will be 0 for our first update because C goes to B,
B, B of B is 0.
Then when we update BC0,
the value of C is still 0,
and we only update V of C in the final one.
So this just points out that order matters.
This comes up also when we're doing function approximation and episodic replay.
Just in general, when we think about policy evaluation algorithms.
It's good to be aware of the bias-variance tradeoff,
data efficiency, and computational efficiency.
TD learning tends to be pretty good on computational efficiency,
um data efficiency-wise, it depends a little bit.
Sometimes if you do experience replay with TD, it gets better.
So um, it's useful to think about there's often a lot of variants of these algorithms.
And so just being precise in whatever your, whatever your state is.
And if you're just assuming the vanilla version, we're using or if you're like,
well if you do this additional experience replay,
this is how it can change.
Okay. Now let's think about how we can do model free learning to make good decisions.
Um, we've talked a lot about Q learning.
Q learning is a bootstrapping technique that assumes Markovian, a Markovian world.
Where we say that the value- the Q value is gonna be um
approximated by the reward plus gamma times max over a prime with the next Q function.
And we can use that as sort of our target,
and then we do the slow slewing.
We sort of have this updated learning rate, and our learning rate,
[NOISE] where we're slewing between the one sample we just saw versus,
um, our previous estimate.
And we slowly slew this towards, um,
we generally decrease alpha over time to try to converge Q to a single value.
Uh, we talked about some conditions under which for Q learning to converge.
Again this is all under sort of well,
this is both under reachability assumptions and
also we're right now we're talking about the tabular setting.
[NOISE] So there's no function approximation going on.
[NOISE] So if you act randomly,
Q-learning will converge to Q star under mild reachability assumptions,
um, which means that, you know,
you can't have a helicopter which if you crash it,
the world is over and you can't get any more samples.
So you have to be able to sort of repeatedly visit all the states,
an infinite number of times and try all of the actions an infinite number of times.
Um, [NOISE] and it has this interesting property that, um,
when you are doing Q-learning you can use data gathered by
one policy to estimate the value of another policy.
So this is where we're trying to estimate the optimal Q function,
but we can use for example a random data,
random samples, [NOISE] or random policy to try to estimate that.
And the reason for that is because we're doing this max.
We're always looking at what's the best thing we could do next.
So that's a pretty cool property.
Um, so then if we sort of think about in this case,
um, there's some different things,
we'll go through these I guess just briefly.
Um, if you have a Q-learning policy, um,
which has e-greedy, e-greedy here is with probability 1 - epsilon.
You take the action which is expected to be best under your current Q function.
Um, and with probability epsilon, you would act randomly.
So if you were in a lookup table,
this is guaranteed to converge to the optimal policy and the limit of infinite data.
So this is yes, with mild reachability.
[NOISE] Um, for this second one can we use Monte Carlo estimation
and MDPs with large state spaces?
Let's vote if yes.
[NOISE] Whatever, I'll take a second,
and just talk to your neighbor, and we'll vote again.
[NOISE] I'm not saying that those people are wrong,
I'm just saying that since most people didn't vote,
I'm assuming that most people would benefit from just thinking about it for a sec.
[NOISE]
All right.
Let's vote again. Um, vote if you think
Monte Carlo estimation can be used in MDPs with large state spaces.
Yes. Yeah. So it's not, um,
it's not- you're not restricted to whether it's a large state space or not,
you can use Monte Carlo estimation there, um,
I, that, that's really, I guess it can.
So no Monte Carlo can be used.
That number can be point positive depending [inaudible]
Yes. It's a great question.
So, um, uh, Monte Carlo,
if you- the number of data points per state could be very low.
If you have a single start state, that's not too bad.
If you have a distribution of starts space- states,
that can be trickier, or we're gonna want to move
into the value function approximation setting.
But there's nothing a priori,
which means you can't apply it.
If you can put in there it might be really bad.
[LAUGHTER] Um, we might need to start doing, ah, function approximation.
The last thing I put on there- this is something that's,
um, we haven't discussed a lot yet,
um, but I think it's an interesting start for one to sort of start
connecting these between the dynamic programming aspects we've talked about.
Um, a model-based reinforcement learning is not
necessarily always more data efficient than model-free,
though we- we talked mostly about model-free.
Um, so we haven't discussed this too much, but, well,
it's a good thing to be thinking about particularly as we start getting into exploration.
Um, and I mentioned briefly before that there's
a nice new paper by Wen Sun and some colleagues at MSR,
Microsoft Research New York City,
that is showing that in some cases, um,
model-based is strictly better than model-free.
And the intuition there is that you can compactly represent the model,
but you can't compactly represent the value function.
So you don't need a lot of parameters to learn the model,
and then you can plan with it,
but if you try to directly learn the value function,
you need a lot more. All right.
So as we're sort of starting to move into an even just in that discussion,
a lot of our recent focus has been in value function approximation.
I'm including in homework 2.
So um, we talked about if you were looking at Monte Carlo methods versus TD learning,
um, what sort of convergence guarantees do we have in the on policy case?
So this is, um, important to emphasize.
So we're looking at evaluating the value of a single policy,
and we talked about how we can think about the on policy stationary distribution.
That when we define a single policy,
then, uh, [NOISE] and we run it,
that's like we're inducing a Markov reward process or a Markov chain,
and we think- can think about the stationary distribution
of states that we would visit under that policy.
Um, and we talked about convergence properties.
And in particular, we said that what Monte Carlo does, um,
no matter what sort of function approximator you are using,
is it tries to minimize, um, the mean squared error.
So this style of techniques we've talked about with Monte Carlo, um,
is that it simply tries to minimize the mean squared error of your data.
[NOISE] And so we can think about this
for linear value function approximators shouldn't- this also
holds for other value function approximation- mators,
it's gonna minimize the error.
[NOISE] In the case of linear value function approximation with TD learner- learning,
it is gonna converge to a constant factor of the best mean squared error.
And what does that mean in this case?
Um, here, what we have is- we might have a gap,
so particularly if you have some like linear value function approximators, um,
you just might not be able to write to exactly represent the value of all the states,
use the- the chosen like a parametric family that you have.
And so there might fundamentally just be a gap between, um,
the value function that's representable with the space that you have,
and the true value function.
I often like to think about like this.
There's a nice picture and set no partner about this two.
This is sort of this, ah,
showing with your set of W,
what are the value functions you can represent,
and it might be that your variable value function lives up here.
You just can't with- for example,
with a line be able to represent all of the true value functions.
Um, if you think about this in two dimen- in, um, another dimension,
you can imagine for a state maybe your real value function looks something like this,
but you are using a line approximator,
so you just can't represent that exactly a- a straight line.
So Monte Carlo converges to the best mean squared error possible,
giving your value function approximator space,
and TD learner converges to that times this additional factor.
[NOISE] Oop, see.
Well, I think that's not going to like that.
Okay. Um, so note that there's this.
[NOISE]
Okay. Well, now I'm just gonna
make that not do that anymore, all right.
We talked about the fact that when you're doing off policy learning,
Q-learning with function approximation can diverge,
which means that it doesn't even converge with infinite amounts of data.
This is even separate than what it might converge to if it converges.
It just says that the actual- your parameters just may never stop
changing if you're doing sort of gradient updates. Yeah.
Can we initialize the function approximator [NOISE] in those parameters in
such a way that [inaudible] push convergence and not guarantee it's not like?
Well, we have conditions on whether or not, um,
the initialization of the parameters helps determine whether or not,
for example, you might converge or diverge.
Um, it's an interesting question,
I don't think there's work that I know
that formally tries to characterize this, like, you know,
are there places where you could formally do this, um, so that,
ah, in terms of your gradients,
for example, they wouldn't start to explode?
There might be, I suspect it depends a lot on the problem.
And I also suspect that there might be
pathological examples that you can construct where it's hard to do.
But certainly worth a try.
You can also ob- ob- observe whether or not
your parameter estimates are continuing to change.
Um, we talked quite a lot,
you guys had a lot of practice with deep learning and model-free Q-learning,
um, where we looked at having this Q-learning target and the Q network,
and we're doing stochastic gradient descent.
Um, we're using a deep neural network to approximate Q.
[NOISE] Um, we talked about the- some of the challenges with
this divergence might be that we have these correlated local updates.
The value of a state is often very closely related
to the value of its next successor state.
Um, and that also by changing these targets frequently,
um, then that might cause, ah, instability.
So that's a lot of the recent progress over
roughly the last five years has been in sort of
ways to modify this equation in order to make
it more stable when you're doing gradient descent.
Um, and in DQN,
it's sort of both introduced that we should do experience replay,
so don't use each data point once,
um, and also fix the target for a while.
So you're sort of saying, "I'm going to use this."
A fixed value function approximator my next state for a while,
and then we can minimize our mean squared error.
We talked about the fact that experience replay is particularly hugely helpful,
um, and the targets is also quite.
Ah, and there aren't good guarantees yet on convergence,
though there's a lot of interesting work that's being done in this space.
People are very interested in trying to understand
the formal properties of these type of networks.
Um, we also talked about double Q,
dueling, um, and, uh, like prioritized replay,
um, as things that we could look at to try to
improve how quickly our Q functions converge to something reasonable.
So I think this is the last one for today.
Um, ah so quick question.
Um, in finite state spaces with features that can represent the true value function.
Does TD learning with value function approximation always find
the true value function of the policy given sufficient data?
So this is for TD learning.
So we're- we're essentially doing policy evaluation right now.
So in this case, are we guaranteed to find the true value function given sufficient data?
Maybe take one, chat with a neighbor for one minute and then I'll ask people to vote.
Should we assume this is on on policy?
We're gonna assume this is on policy,
or at least with sufficient amounts of data from the on policy distribution.
[BACKGROUND]
Who wants to vote yes that we do find the true value function approximator?
That's right. Okay, so how could we have checked this?
So if we go back to what I was saying over here.
What I said is that we are going to converge to
a constant factor of the best mean squared error.
This mean squared error is always 0,
if you can exactly represent the value in the current space.
So that additional sort of constant factor is just a constant factor times 0.
So in this case, yes.
So I- because I said here that
it- with features that can represent the true value function.
So we've said that it is perfectly
possible to represent the value function of this policy in
the features that are given to you and so it will be possible to achieve that. Yeah.
Is it true for a non-linear parameterization?
For TD learning? Yeah. So for policy evaluation, um, uh,
if you have a nonlinear- if you have features- like if you have
a general representation that allows you to exactly
represent the value function and you're doing on policy learning.
So you're doing TD learning, um,
you will be able to get zero error,
with infinite, you know, sufficient data etc.
Finite data, this is all of it. Yeah.
One part that I-
Remind me your name, please.
I, I got a little confused given that we,
we might have all of the features that we want,
but we might not have
any representative value function approximation
that would actually be able to generate the Qs.
Like the- a- are those two things like identical?
Like, I guess the way I was thinking about this was we might have all the features,
but we might not find a space of functions that
actually would be able to represent the value function.
Okay. So I think the question is say, okay.
Well, what if we had a lot of features,
but like- does that actually give us a parameterization
of the value function that can represent the true value function?
When I say here sort of features and representation I mean that we
have picked a function class that can exactly represent the value function,
if we have an algorithm to try to fit it well enough.
So, um what I'm assuming- what I'm saying in this case is
that if your value function- if someone could- if
an oracle could give you those features- the- the- the parameter vector that would make,
um, uh, that zero,
that TD learning can find it.
So this essentially like [inaudible] because we can generate the table, keep it.
It doesn't have to be tabular.
So to go over- this does not to have to only hold for tabular cases.
It's that if like- so if we look at, um, something here.
Let's imagine this is your state space.
This is your value function.
So if someone gives you,
uh, a line, or a quadratic,
or a deep neural network with enough parameters to exactly represent that line,
what this statement is saying is that
TD learning can find- can fit those parameters exactly.
This is not true when we start to go into Q learning.
So in some cases, you can have
a representation that could optimally represent the value function,
but you can't find it.
Like Q learning will not identify it.
So that's sort of the difference that we're trying to make here,
is that in TD learning, if that exists and you're on policy, you can find it.
Q learning, you may not be able to.
Yeah. There's a question in the back. Your name first, please.
So can I just clarify this is to whether the value approximation is linear or nonlinear?
Yes. Yes. So what's we're trying- this is true,
um, for generic representations.
If your representation, whether it's linear,
or tabular or, um,
tabular generally always assume it's- it's exact.
So linear or otherwise,
then- then this is- this is true. Yes.
Uh, does this have anything to do with if
our value function approximator is a contracting operator or not? So.
Yeah. It's a great question. You asked whether or not this has to do with whether or
not our value function approximator is a contraction.
You can think of when we're doing this sort of TD learning, that we have two steps.
We're kinda doing our approximated Bellman.
And our Bellman operator,
if we could do it exactly we know is a contraction,
then we have to do this additional part of fitting a function.
And if you can exactly fit your function, um,
then you're not going to introduce additional error during that part.
Um, and that's- that's one of- that's one of the benefits in here.
That cannot- that can start to be divert-
So in this case again, it's all on
policies so it's much closer to the supervised learning setting.
When you start to be off policy this gets more complicated. Question or?
No. Okay. All right.
So let's just go really briefly through imitation learning,
um, and policy search.
This will be kind of at the same lighter level that,
um, you'd be expected to know it for the exam.
You haven't had the chance to practice either of these,
um, except where from lecture.
So imitation learning was the idea that
the specification of reward functions can be really complicated.
What if we could just have people demonstrate procedures and then learn from them?
Behavior cloning is where we're doing supervised learning.
So we're trying to learn a mapping of actions to states,
and we're treating this as a supervised learning problem.
So we just look at for an expert,
pairs of states and action,
and you can try to fit your favorite machine learning supervised,
like classification algorithm to- to predict that.
And the thing that can go wrong in this case is that your state
distribution that you induce under your approximate policy,
that's trying to mimic the expert,
can be different than,
um, the- the distribution of states you'd reach on to the expert policy.
Which means that you can end up with these sort of different state distributions,
and you don't know what the right thing is
to do under these new states because you don't have any data about that.
So things can go pretty badly in some of those cases.
We talked about imitation learning,
where the idea is that we have again trajectories of- of demonstrations.
And now the goal is to directly learn rewards.
A good thing to rethink about here is
how many reward functions are compatible with an expert's demonstration.
We talked about this before.
If it's not clear, feel free to reach out to me
either at the end of class or, um, on Piazza.
And we talked about policy search.
So just really briefly.
These are the types of levels of,
um, questions I would expect you to be familiar with.
So why do we want to do stochastic parametrized policies?
Can be a nice way to put it in domain knowledge.
It can help us with non-Markovian structure.
We talked about aliasing,
and we talked about game theory settings,
where deterministic policies would do badly,
but stochastic ones would do well.
Um, policy gradient methods are not the only form of policy search.
We talked about exoskeleton optimization by my colleague Steve Collins,
and the fact that, um, that worked pretty well.
But generally, we're going to talk mostly about gradients.
Um, the likelihood ratio policy gradient method does
not need us to have the dynamics model,
which is really important because when we don't have it.
And then two ideas to reduce the variance of
a policy gradient estimator is to use the temporal structure.
And here, it involves the fact that the reward you get at a timestep now can't be
influenced by your future decisions because of the structure of time.
And then, um- and secondly baselines.
So that's kind of the level,
the sort of- the stuff we talked about in class,
but not deep procedural knowledge.
So just to summarize.
Um, recommendations would be to go through lecture notes,
look at things, like check your understanding.
If you want to look at existing, uh,
additional examples going through session,
um, notes can be useful.
Um, the practice midterms particularly
last year will be more similar to the one from two years ago.
Um, if you see some topic that we haven't covered in this class,
it's not going to be covered on the midterm, um,
but feel free to reach out to us if you have any questions,
and you can bring a one-sided one page of notes that's
handwritten or typed. Okay. Good luck.
 Um, today what we're gonna be doing is we are gonna be
starting to talk about fast reinforcement learning.
Um, so in terms of where we are in the class,
we've just finished up policy search.
You guys are working on policy gradient,
uh, right now for your homework.
And then, that'll be the last homework and then the rest of the time will be on projects.
[NOISE] Excuse me.
Um, and then, uh,
right now we're gonna start to talk about fast reinforcement learning,
which is something that we haven't talked about so much.
So, so the things we've discussed a lot so far in the term are things like optimization,
generalization, and delayed consequences.
So, how we do planning and Markov decision processes?
How do we scale up to really large state spaces using like deep neural networks?
Um, and how do we do this optimization?
And I think that that works really well for,
um, uh, a lot of cases where we have good simulators or where data is pretty cheap.
Um, but a lot of the work that I do in
my own lab thinks about how do we teach computers to help us,
uh, which naturally involves reinforcement learning because we're
teaching computers about how to make decisions that would help us.
But I think there are a lot of other applications
where we'd really like computers to help us.
So, things like, uh,
education or healthcare or consumer marketing.
And in each of these cases we can think of them as being reinforcement learning
problems because we'd have some sort of agent like our computer, uh,
that is making decisions as it interacts with
a person and it's trying to optimize some reward, like,
it's trying to help someone learn something or it's
trying to treat a patient or it's trying to,
uh, increase revenue for a company by having consumers click on ads.
And in all of those cases,
the place where data comes from is people.
Um, and so, there's at least two big challenges with that.
The first is that, uh, you know, people are finite.
There's not [NOISE] an infinite number of people, um, and also,
that it's expensive and costly to,
um, try to gather data when you interact with people.
And so, it raises the concern about sample efficiency.
So, in general, of course,
we would love to have reinforcement learning algorithms that are both
computationally efficient and sample efficient.
But, uh, most of the techniques we've been looking at so far particularly these sort of
Q learning type techniques were really sort
of inspired by this need for computational efficiency.
Um, so if we think back to when we were just doing planning at
the beginning when we talked about doing dynamic programming versus doing Q learning,
um, in dynamic programming,
we had to do a sum over all next states and in TD learning we sampled that.
So, in TD learning we sort of had this constant cost per update
versus for dynamic programming where we had this S squared times A and cost.
Um, so it was much more expensive to do things like
dynamic programming than it was to do TD learning,
um, on a, on a per step basis.
And so, a lot of the techniques that have been developed in
reinforcement learning have really been thinking
about this computational efficiency issue.
Um, and there are a lot of times where computational efficiency is important.
Like, if you wanted to plan from scratch and you were sort of driving
a car at 60 miles per hour,
then if it takes you-.
Uh, so if you're driving a car at 60 miles per
hour and it takes your computer one second to make a decision about like,
you know, how to turn the wheel or something like that, um,
then during that one second you've traveled, you know, many feet.
So, in a lot of cases,
you really do have real-time constraints,
uh, on the computation you can do.
Uh, and in many situations like for,
you know, in the cases of, uh,
some robotics and particularly when we have simulators
[NOISE] we really want
computational efficiency because we need to be able to do these things very quickly.
Um, we can sort of use our simulators but we need
our simulators to be fast so our agent can learn.
Um, in contrast to those sort of examples are things where sample efficiency
is super expens- important and maybe computation is less important.
So, whenever experience is costly or hard to gather.
And so, this is particularly things that involve people.
Um, uh, so we think about students
or patients or customers,
like the way that our agent will learn about the world is making decisions,
um, and that data affects real people.
So, it might be very reasonable for us to take, you know,
several days of computation if we could figure out a better way to treat cancer, um,
because we don't wanna randomly experiment on people and we wanna
use the data as well as we can to be really really sample efficient,
um, versus in the case of like Atari it's, uh,
we wanna be really computationally efficient because we can do many, many simulations.
It's fine. No one's getting hurt.
But, um, but we need to eventually learn to make,
you know, to derive a good game.
So, one natural question might be,
okay, so maybe now we care about sample efficiency.
Um, and before we cared perhaps more about computational efficiency but
maybe the algorithms we've already discussed are already sample efficient.
So, does anybody remember like on the order of magnitude or like, you know,
somewhere in the rough ballpark how many steps it
took DQN to learn a good policy for Pong?
Maybe, there's multiple answers maybe someone could say.
Yeah. [NOISE] I think it varies somewhere partly between 2 to 10,
is my guess. 2 to 10 million.
So, um, that's a lot,
that's a lot of data [LAUGHTER] to learn to play Pong.
So, I would argue that the techniques we've seen so far, um,
are not gonna address this issue and it's
not gonna be reasonable for us to need, you know,
somewhere between 2 to 10 million customers before we figure out a good way to target
ads or 2 to 10 million patients before we figure out the right decisions to do.
So, the techniques we've seen so far are not gonna be, uh,
they're formally not sample efficient and they're also empirically not sample efficient.
Um, so we're gonna need new types of techniques than what we've seen so far.
So, of course, when we start to think about this,
we think about the general issue of.
you know, what does it mean for an algorithm to be good?
We've talked about computational efficiency
and I've mentioned this thing called sample efficiency.
But in general, I think one thing we care a lot about is, you know,
how good is our reinforcement learning algorithm and we're going to start
to try to quantify that in terms of sample efficiency.
Um, but, of course,
you could have an algorithm that's really sample efficient in the sense that
maybe it only uses the first 10 data points and then it never updates its policy.
So, it doesn't need very much data to find a policy but the policy is bad.
So, when we talk about sample efficiency,
we're gonna want both but we don't need very much data
and we don't need very much data in order to make good decisions.
So, we still wanna get good performance we
just don't need- wanna need very much experience to get there.
Um, so when we talk about what it means for an algorithm to be good,
you know, one a, one possibility is we can talk about whether or not it converges at all.
Um, that just means
whether or not the value function or the policy is stable at some point,
like asymptotically because the number of time steps goes to infinity.
And we talked about how sometimes with, uh,
value function approximation we don't even have that,
like, ah, things can oscillate.
Um, then another thing that's stronger than that that you might wanna us to say, well,
asymptotically as t goes to infinity,
um, will we converge to the optimal policy?
And we talked about some algorithms that would do that under some different assumptions.
Um, but what we haven't talked about very much is sort of,
you know, well, how quickly do we get there?
Asymptotically is a very long time.
Um, and so we might wanna be able to say,
if we have two algorithms and one of them gets the optimal policy here,
like if this is performance with this time and another algorithm goes like this,
intuitively, algorithm two is better than algorithm
one even though they both get to the optimal policy, eventually.
So, we'd like to be able to sort of account for either,
we can think about things like how many mistakes our algorithm makes
or its relative performance over time compared to the optimal.
And so, we'll start to talk today about
some other forms of measures for how good an algorithm is.
So, in this lecture and the next couple of lectures, we're gonna, um,
do several different things trying to talk about sort of
how good are these reinforcement learning algorithms and think
about algorithms that could be much better in terms of their guarantees for performance.
Um, we're gonna start off and talk about tabular settings.
But today we're only gonna talk about simple bandits.
But generally, for the next,
um, today and next lecture,
we'll talk about tabular settings and then, um,
hopefully also get to some about function approximation plus sample efficiency.
But we'll start to talk about sort of settings frameworks and approaches.
So, the settings that we're going to be covering like today and next time,
it's gonna be bandits which, uh,
a number of you- who, who,
who here is doing the default project?
Okay. So, a number of you are,
are already starting to think about this in terms of the project.
So, we'll introduce bandits today, um,
and then we'll also talk about this for MDPs.
And then, we'll also introduce frameworks,
and these are evaluation criteria for formally
assessing the quality of a reinforcement learning algorithm.
So, they're way- there's sort of a tool you could use to evaluate
many different algorithms and algorithms will either satisfy
this framework or not, or have different properties,
um, under these different frameworks.
And then, we'll also start to talk about
approaches, which are classes of algorithms for achieving
these different evaluation criteria for
these different frameworks in different settings, either for the MDP setting or,
or for the bandit setting.
And what we'll shortly see is that there's a couple of main ideas of
styles or approaches of algorithms which turned out to both have, um,
be applicable both to bandit settings and MDP settings, um,
and function approximation actually
and also that have some really nice formal properties.
There's sort of a couple of big conceptual ideas
about how we might do fast reinforcement learning.
Okay. So, the, the plan for today will be
that we'll first start with an introduction to multi-armed bandits.
Then we'll talk about the definition of regret on a mathematical formal sense.
Um, and then we'll talk about optimism under uncertainty.
And then, as we can, we'll talk about Bayesian regret,
um, and probability matching and Thompson sampling.
I'm curious, who here has ever seen this sort of material before?
Okay. A couple of people, most people not.
I wouldn't- is it covered in AI?
I don't think they would cover it. Oh, good. Okay.
All right. So, for some of you,
uh, this will be a review, for most of you, it will be new.
So, for multi-armed bandits,
we can think of them as a subset of Reinforcement Learning.
So, it's generally considered a set of arms,
the- there's a set of m arms,
which were, uh, the equivalent to what we used to call actions.
So, in Reinforcement Learning,
in our set of actions, um,
we're thinking about like, there being m different actions.
Now we're often gonna call those arms.
[NOISE] And then, for each of those arms,
they have a distribution of rewards you could get.
So, we haven't talked a lot about sort of having uncertainty over our rewards.
We mostly just talked about the expected reward.
Um, and for multi-armed bandits,
today we're gonna explicitly think about the fact that, um,
rewards might be sampled from a stochastic distribution.
[NOISE] So, there's some distribution that we don't know which is conditioned on the arm.
So, conditioned on the arm,
you're gonna get different rewards.
So, for example, it could be that for arm 1,
your distribution looks something like this.
This is the probability of that reward and this is rewards.
Um, and then for arm 2,
it looks something like this.
[NOISE] So, in this particular example, um,
the average reward for arm 1 would be higher than the average reward for arm 2.
And then they would have different variances.
But it doesn't have to be Gaussian.
You could have lots of different distributions.
Um, essentially, what we're trying to capture here is that whenever you,
uh, um, take a particular action,
which we also refer to as pulling an arm, um,
for the multi-armed bandit, um,
then the reward you get might vary even if you pull the same arm twice.
So, in this case, you can imagine that if you pull the arm once,
um, arm 1 once, you might get a reward here
and maybe the second time you get a reward there.
So, the idea in this case is it's similar to
MDPs except for now there's no transition function.
So, there's no state or equivalently,
you can think of it as there's only a single state.
Um, and so when you take an arm,
um, you stay in the same state.
There's always these m actions available to you, and on each step,
you get to pick what action to take and then you observe some reward that is
sampled from the unknown probability distribution associated with that arm.
And just like in Reinforcement Learning, um,
we don't know what those reward distributions are in
advance and our goal is to maximize the cumulative reward.
So, if someone told you what these distributions were in advance,
you would know exactly which arm to pull,
whichever one has the highest expected,
uh, as, expected mean.
[NOISE] So, we're gonna try to use pretty similar notation to what we had for,
um, the reinforcement learning case.
Um, but if you notice things that are confusing, just let me know.
Um, so we're gonna define the action value as the mean reward for a particular action.
So, that's Qa, this is unknown,
agent doesn't know this in advance.
The optimal value V star is gonna be equal to Q of the best action.
And then, the regret is gonna be the opportunity loss for one step.
So, what that means is that if instead of taking, if,
if you could have taken Q of a star and instead you took Q of at.
So, this is the actual arm you selected.
How much in expectation did you lose by taking the sub-optimal arm?
And this is how we're gonna mathematically define
regret in the case of Reinforcement Learning.
So, if you selected the optimal arm, your regret,
your expected regret will be 0 for that time step,
um, but for any other arm,
there will be some loss.
And the total regret is just, um,
the total opportunity loss if you sum over
all the time steps that the agent acts and compare,
um, the actions it took and the, um,
the expected reward of each of those actions
to the expected reward of the optimal action.
So, just to be clear here,
this is not known,
this is not known to the agent,
and this is unknown to the agent.
[NOISE] Just to check for understanding for a second,
why is this, so why is this second thing unknown to the agent? Yeah.
Because you don't know the probability distribution of Q.
Right. So, [NOISE], correct.
So, you don't know, er,
what the distribution is,
so you don't know what Q is.
You get to observe, um, a sample from that.
So, you get to observe R. You get to get an R which was sampled
from the probability distribution of rewards given the action that was taken.
[NOISE] But you don't get to observe either, uh,
the true expected value of the optimal arm
nor the true expected value of the arm that you selected.
So, this isn't something we can normally evaluate unless we're in a simulated domain, okay?
But we're gonna talk about ways that we can bound this and,
and think about algorithms or try to minimize the regret.
So, if we think about ways to sort of quantify it,
another way to think about it alternatively is that,
um, think about the number of times that you take a particular action.
We can call that Nt of a.
So, that's like the number of times we select action 1,
action 2, et cetera.
And then, we can define a gap which is the difference between [NOISE] the optimal
arm's value and the value of the arm that we selected, and that's the gap.
So, that's how much we lost by picking a different arm than the optimal arm.
So, this gap is equal to,
gap for a star is equal to 0, if you don't lose anything by taking the optimal arm,
and for all other arms, it's gonna be positive.
So, another way to think about the regret which is equivalent is to say,
this is equivalent to thinking about what are the expected number of
times you select each of the arms times their gap.
So, how much you would lose by selecting that arm compared to the optimal arm.
And what we would like is that sort of an algorithm, um,
it should be able to adjust how many times you pull arms which have large gaps.
So, if there's a really, really bad arm, uh,
like if there's really bad action which has a very low reward,
you would like not to pull that as much,
to take that action as much as the arms that are close to optimal.
[NOISE] So, one approach that we've seen before is greedy.
Um, in the case of, ah,
bandits, the greedy algorithm is very simple.
Um, we just average the,
the rewards we've seen for an arm.
So, we just look at every single time that we took that arm,
we look at the reward we got for each those timestamps, and then we average it.
And that just gives us, um,
an estimate of the, of Q hat.
And then what greedy does is it selects the action with the highest value,
um, and takes that arm forever.
So, it's probably clear in this case that because the rewards are sampled from
a stochastic distribution that if you are unlucky and get samples that are,
uh, misrepresentative, then you could lock into the wrong action and stay there forever.
So, if we think of that little example I gave before,
and I'll work out, uh, a bigger example shortly, so in this case,
imagine this is reward,
this is probability, and this was a2.
Okay. So, let's imagine that the first,
um, and I'll make this 1 and this 0.
So, if you sample from a1, in this case,
you could imagine there's some non-zero probability that the first sample you get is say,
0.2 for a1, and the first sample you get for a2 with non-zero probability might be 0.5.
So, the true mean of a2 is lower than the mean of a1.
But if you sampled each of these once, um,
then if you're greedy with respect to that,
then you will take the wrong action forever.
[NOISE] Yeah.
Wrong action forever, is the idea that
our policy is gonna be influencing what times we'll get in
the future or is the idea that there are
some set of samples independent of this greedy policy to begin?
Because it seems otherwise, if there is non-zero reward,
you just take that one forever.
Uh, great question. So, um, is it, yeah,
so what [inaudible] said is, um, you know,
is there an additional thing that we're doing kind of before this?
Normally, for a lot of these algorithms, um,
we're gonna assume that all of them operate by
selecting each arm once at least if you have a finite set of arms.
Um, and equivalently, you [NOISE] can say if you don't have any data,
you treat everything equivalently or, um,
but essentially most of these ones say,
until you have data for all the arms,
we are gonna do round robin,
you're gonna sample everything once.
And after you do that, either you can be greedy or we can do something else.
So, there has to be a pre-initialization space. It's a good question.
So, and we're also gonna assume for right now that we split ties,
um, with equal probability.
So, if there are two arms that have the same pro- probability, um,
and they both have the max actio- max value,
then you would split your time between those until the value is changed.
So, this is an example where if we first sampled a1 once [NOISE],
then sampled a2 once.
Um, and because there's a non-zero probability
that those samples would make it look that,
um, action a1 has a lower mean than action a2,
then you could lock into the wrong action forever.
Now, an e-greedy algorithm,
which we've seen before in class, um, uh,
it does something very similar except for with probability 1 - epsilon, we select,
um, the greedy action,
and otherwise with epsilon,
we split our probability across all the other actions or all the other arms.
[NOISE] So, in these cases,
um, we have some more robustness.
So, in this case, you know,
we would continue to sample, um, the other action,
but we're always gonna make a sub-optimal decision at least epsilon percent of
the time, well, approximately.
It's a little bit less than that because if you do it totally randomly not a,
um, uh, but it's order epsilon.
I mean, er, it's slightly less than that because, um, er,
if you uniformly split across all your arms with one over the number of arms probability,
we'll be selecting the optimal action.
[NOISE]
Okay. So let's see these in practice for a
second before we talk more about better algorithms.
Um, so let's imagine we're trying to figure out how to
treat broken toes and this is not a real medical example.
Um, but let's imagine there's three different surger- three different options.
Um, one is surgery. One is buddy taping the broken toe with another toe,
which is what the Internet might tell you to do.
Um, and the third is to do nothing.
And the outcome measure is gonna be
a binary variable of whether or not your toe is healed um,
after six weeks as assessed by an x-ray.
Okay. So let's imagine that we model this as a multi-armed bandit with three arms,
where each arm corresponds to um,
well, I'll ask you in a second what it corresponds to.
And, and there's an unknown parameter here.
So each arm, there's an unknown parameter which is the reward outcome.
So let's just take just you know,
one or two minutes just to say what does uh, um,
a pull of an arm correspond to in
this case and why is it reasonable to model it as a bandit,
instead of an MDP?
[NOISE]
Yeah.
I'm , and in terms of why we model it as a bandit,
one reason is that MDPs usually,
think of an agent walking through the world and each,
the state, or the world has many different states, and we analyze those.
Here we have just one state,
a toe is broken and various actions are considered.
Right. So great.
So here we just have one there.
And, and so what is, what is the,
what does it mean to pull an arm in this case or take an action?
Which does that correspond to in the real world?
[NOISE] That would be
a new patient coming in and then making a decision about the care for that patient.
Great. So that's like um, uh,
a patient coming in and then us deciding to either do surgery on them or giving them um,
er, er, in this case a um,
like a er, er, doing one of the three options for treatment.
Um, and so in this case too,
the, each pool is a different patient.
So how we treat patient one isn't gonna generally affect how we treat
patient two in terms of whether they healed or not,
or whether that particularly your you know,
surgery worked for them, doesn't affect the next patient coming in.
So all the patients are IID.
Um, and what we wanna figure out to do,
is which of these treatments on average is most effective.
Okay. So let's think about these in a particular set- setting.
So um, uh, a
par- particular set of values.
So let's imagine that they're all Bernoulli reward var- variables
because either the toe is gonna be be healed or it's not gonna be healed after six weeks.
Um, it turns out in this particular fake example, surgery is best.
So if you do surgery with 95% probability,
it will be healed after six weeks.
Buddy taping is 90%,
and doing nothing is 0.1.
So what would happen if we did something like a greedy algorithm? Oh, yeah.
Sorry, is it possible to incorporate other factors into like pulling the arms?
For example, surgeries like [inaudible] like cost effective versus buddy taping,
are there ways to incorporate that?
Yeah. It's a great question. So question's about like uh,
you know, could we you know,
surgery is a lot more invasive.
And there might be other side effects, etc.
There's a couple different ways you could imagine putting that information in.
One is, you could just change the reward outcome.
So you could say um,
maybe it's more effective but it's also really costly and I've
gotta have some way to combine outcomes with cost.
Um, another thing that one might be interested in doing in
these sorts of cases is that you might have a distribution of outcome.
So in this case, all of them have the same um, distribution.
They're all Bernoulli. But in some cases um,
your reward outcomes will not be,
will be complicated functions, right?
Like, so it might be that um,
maybe for some people,
surgery is really awful and for most people, it's really good.
But it's really bad for some people because they have you know,
some really bad side effects and so its mean is still better.
Um, but there is like, this really bad risk tail of like,
maybe people you know, react badly to anesthesia or something like that.
So in those cases, you might wanna not focus on expected outcomes.
You might wanna look at risk sensitivity.
And in fact, one of the things that we're doing in my group is looking at
safe reinforcement learning including safe bandits um,
and thinking about how you could optimize for risk-sensitive criteria.
Another thing that we're not gonna talk about today which you
also might wanna do in this case is that,
patients are not all identical.
Um, and you might wanna incorporate some sort of contextual features about
the patient to try to decide which surgery to do or no surgery versus buddy taping.
Ah, and hopefully, well,
those of you who are doing the default project,
we'll think about this definitely.
And we'll probably get to this in a couple lectures.
In general, we often have sort of,
a rich contextual state which will also [NOISE] affect the outcomes.
Okay, so in this case, let's imagine that we have
these three potential interventions
that we can do and we're running the greedy algorithm.
So um, as ah, brought up before,
we're gonna sample each of these arms once,
and now we're gonna get an empirical average.
So let's say that we sample action A1 and we get a plus one.
And so now our empirical average of what the expected reward is for action A1 is 1.
And then we do A2, and we also get 1 so that's our new average.
And then we do A3, we get a 0.
And so at this point,
what is the probability of greedy selecting each arm assuming
that the ties are split randomly? Yeah.
Two plus two, then epsilon pair at one, plus or minus a little.
And, and your name?
.
, yeah. So what said is um,
exactly correct for the e-greedy case.
So you're jumping ahead a little bit but that's totally right.
Um, ah, in this case for greedy,
it'll just be 50-50.
Yeah. You're already moving to e-greedy. So yes.
So the probability of A1 is gonna equal the probability of A2,
just gonna equal to a half.
So let's imagine that we did this for a few time steps.
So, so we can think about what the regret is that we incur along all of this way.
So at the beginning um,
we have this sort of,
an initialization period where we're gonna select each action
once and we're always comparing this to what
was the reward we could have gotten under the optimal action.
So the regret here is gonna be exactly equal,
so this is optimal.
And remember, the regret is gonna be Q of A star minus Q of the action you took.
So in the first case,
it's gonna be zero because we took the optimal action.
And the second case, it's gonna be 0.95 - 0.9 which is 0.05.
And then the third case, it's gonna be 0.95 - 0.1.
And then in the third case, it's gonna be
zero or fourth case it's gonna be 0 and then it's gonna be 0.05 again.
Now, in this situation um,
will we ever select a greedy?
Will we ever select A3 again given the values we've seen so far?
No. Yeah, I see people say you know,
no. So why not?
[NOISE]
Is it possible,
what's our current estimate of um,
the reward for A3?
. Yeah. So I guess I didn't show,
put those here but the, this was the actual.
These were the rewards we got.
So it's 1, 1, 0.
So our current estimate for A3 is 0.
We know our rewards are bounded between 0 and 1.
None of our estimates can ever collapse below 0.
Um, and we already have a positive 1 for these other two actions,
which means that their averages can never go to 0.
So we're never gonna take A3 again.
Now, in this case,
that's not actually that bad like [LAUGHTER] here, here,
that's not actually a problem because A3 is
a bad arm and it's got a much lower expected reward.
Um, in other cases er,
it could be that A3, we just got unlucky.
Um, and in that case,
it could mean that we never should,
never take the optimal action. Yeah.
I thought we used B star like in terms of the reward for an action.
Yes. And that's the same as this. Good question.
So this is the same as B star.
Yeah. And I'll go back and forth between notation but yeah, definitely just ask me.
So in this case, we're never gonna select A3 again and um,
now notice that in the greedy case,
there are cases where um,
if I had used slightly different values here um,
that you might have selected A3 again later.
Because if it's the case that um,
like if you didn't have Bernoulli rewards but you had Gaussians um,
it could be that the rewards for other arms dropped
below another arm later and then you start to switch.
So you don't necessarily always stick with which other arm looked best at the beginning.
But in this particular case with these outcomes
then you're not gonna select A3 ever again.
[NOISE] All right, now let's do um, e-Greedy.
So in this case,
we're gonna assume we got exactly the same outcomes for the first few.
Um, and then what said is that we're gonna um,
have ah, one-half minus epsilon um, over 2.
So we're gonna split ties randomly again.
So with probability epsilon,
we're gonna take um,
with epsilon over 3.
We're gonna take A1 or A2 or A3.
And with 1 - epsilon over 2,
we're gonna take A1 [NOISE] or A2.
[NOISE] Interesting, okay.
Okay. So in this case, it's gonna look almost identical except we're
still gonna have some probability of taking A3 in this case.
And we can do a similar computation here.
In this case um,
we've assumed that all of these are exactly the same.
So e- e- e-Greedy in this case,
will select A3 again.
Yes. Um, if epsilon is fixed,
not updating, yeah. [NOISE]
Um, if Epsilon is fixed,
how many times is it gonna select a_3?
Main question's whether it's finite or infinite.
Maybe talk to your neighbor for a second and decide whether if Epsilon is fixed,
whether it'll be- a_3 will be selected a finite or infinite number of times,
and what that means in terms of the regret?
[NOISE]
Okay. I'm gonna have everybody vote.
So if you think it's gonna be selected an infinite number of times, raise your hand.
Great. So what's that mean in terms of regret,
is there- it gonna be good or bad?
It's gonna be bad.
Great. Bad, bad regret.
I mean, in general, regret's gonna be unbounded unfortunately in these cases.
[LAUGHTER] Um, so we're always gonna unfortunately have infinite regret but, um,
but the rate at which it grows can be
[LAUGHTER] can be much smaller depending on the algorithm you do.
[NOISE] So, um, in particular and,
uh, yeah, so we can also think about it in this case.
So if you have a large gap which we do for a_3
here and we're gonna be selecting that arm an infinite number of times,
then e-greedy is also gonna have,
um, a large regret.
So I like this plot, this plot comes from David Silver's slides.
Um, uh, so if you explore forever,
like, if you just do random, um,
which we didn't discuss, but you could also do,
then you're gonna have linear total regret, um,
which means that with the number of time steps,
t, you're- you're gonna scale linearly with t. Essentially,
your regret is growing unboundedly and it's growing
linearly which is equi- essentially proportio- I mean,
it's gonna generally have a constant in front of it but
um, it's gonna be a constant times the worst you could do at every time step.
Because if you always select the worst arm at every time step,
your regret will also grow linearly.
So it's pretty bad.
Um, if you explore never,
if you do greedy, uh,
then it also can be linear,
and if you do e-greedy, it's also linear.
So essentially, it means that all of these algorithms
that we've been using so far can have really,
really bad performance, um,
certainly in bad cases,
and so the critical question is whether or not
it's better tha- it's possible to do better than that.
So can we have sub- what's often called sublinear regret. So we want to have regret.
If an algorithm's gonna be considered good in
terms of its performance and its sample efficiency,
we're gonna want its regret to grow sublinearly.
Um, when we think about this,
we're generally gonna think about, um,
whether the bounds that we create,
in the performance bounds are gonna be problem independent or problem dependent.
So most of the bounds,
i- it depends a little bit.
For MDPs, most of the bounds that we can get are gonna be problem independent.
For bandits, there's a lot of problem dependent bounds.
Um, problem dependent bounds in the case of bandits mean that, um,
the amount of regret that we get is gonna be a function of those gaps,
and that should be sort of intuitive.
So if you have- let's imagine that we just have two arms, like,
a_1 and a_2 and the mean,
um, the expected reward.
So if this is, the expected reward is 1 and this is an expected reward of 0.001,
intuitively it should be easier to figure out that arm one is better than arm
two that if this is like 0.53 and this is 0.525.
Because in one case,
really hard to tell the difference between the mean of the two arms and the other case,
the means are really, really far apart.
So somewhat intuitively if the gap is really large,
it should be easier for us to learn,
and if it's really small, it should be harder. Yeah.
Um, so if the, uh- [NOISE]
is that [OVERLAPPING].
[NOISE] Uh, so, uh,
optimal reward is deterministic, for some actions.
I mean, we have zero regret.
[NOISE]
Good question. Um, uh,
the question is if the optimal are- if the,
uh, optimal reward is,
uh, are you just saying optimal reward?
Yeah. The optimal reward is deterministic till we have zero regret, if you know it.
So if you know that all the rewards of the arms are [NOISE] deterministic,
then you just need to pull them once.
Then you can make a good decision and then you're done.
In general, these algorithms aren't going to know
that information if you don't- even if it
was deterministic you're still gonna have these other forms of bounds.
So what about the greedy case then? What if it's deterministic?
If it's determinant- if in a greedy [NOISE] case- it's a good question.
In a greedy case, if your real rewards are deterministic,
um, then you don't need- then you would have pulled all the arms once,
and you will make no mistakes.
You'll make, uh, you'll have zero regret basically from that point onwards.
So we'd consider that as just like you'd have some initial constant regret,
and then afterward it would be independent of t. Did you have a question?
Yeah. [NOISE] Um-
Remind me your name.
[NOISE] Is it also a function of the variance of each arm-
Oh, good question.
- [OVERLAPPING] because in that case like you take upon [inaudible]
Yeah. Great question. So we're not gonna ta- uh,
question is whether it also depends on the variance in addition to the mean.
Um, yes, we're not gonna talk about that,
um, uh, but in addition to problem dependent bounds,
you can certainly think about parametric, like,
if you have some parametric knowledge
on the distribution of the rewards, then you can exploit it.
Um, so if you know it's a Gaussian or other things like that.
Um, I think in general if you know them,
or like if you have information about the moments,
then you should be able to exploit it.
Most of the information that I've seen is,
um, looking at the mean and the variance.
We very frequently throughout a lot of
this stuff is are gonna assume that our rewards are bounded.
Um, that's gonna be important for most of the proofs that we do,
even without making any other parametric assumptions.
Okay. But then the other version of this is problem independent,
which just says [NOISE] regardless of the,
of the domain you're in, regardless of the gaps,
can we also show that, um, uh,
regardless of any structure of the problem,
can we still ensure that regret grows up linearly,
and that's what we're gonna mostly focus on today.
So I think lower bounds,
lower theoretical bounds are- are helpful to try to understand how hard a problem is.
So I said, is it possible for something to be sublinear, um,
and there's been previous work to look at sort of,
well, how much does the regret have to grow?
So, um, in this case,
this is regret we're mostly getting,
um, to, mu's, to write out regret.
But in this case they- they prove that in terms of the gaps, um,
and the similarity in the dis- distributions in terms of the KL divergence,
the KL divergence, that you can show a lower bound on how much regret grows.
[NOISE] So this is where there's a sort of
unfortunate [NOISE] aspect of regret growing unboundedly comes up.
If you don't make any other assumptions on the distributions of your rewards,
um, uh, in general,
the regret of your, um,
your regret will grow unboundedly,
and it will ha- do so in terms of these gaps,
and the KL divergence.
But it's still sublinear. So that's nice.
So it's growing logarithmically with t here. T is the time steps.
It's encouraging that, like, our lower bound
suggests that there's room for traction, right?
That we can definitely, um,
at least there's no formal result that says it has to be linear,
it says no, we should be able to grow much slower.
So how could we do- why- how would we maybe do this?
So this is now, um, we've talked about a particular framework which is regret,
we talked about a setting, which is bandits,
and now we're gonna talk about an approach which is,
um, optimism in the face of uncertainty.
And the idea is simply to choose actions which might have high value.
Why should we do this? Um.
I have a question on the- in the previous slide, um,
so is that true at every t, or only in the one
of because as it's really isn't that just saying that it's greater than infinity?
All right. Um, that's a good question.
Question is about whether i- it holds.
I think this holds on every time step,
I'd have to check the exact, like,
way that they wrote this, um, uh,
there's also constants, um,
but I think this should hold on a per time step basis.
We're really saying that as time is going along,
it should be true [OVERLAPPING]
Yeah. The limit as t goes large,
this is where I'd have to look back at the original paper, um,
there's probably additional constant terms which are transitory,
um, and so this is probably the dominant term as t goes large.
But in a lot of the regret bounds particularly in our MDP cases,
we often have transitory terms that are independent of the time step,
but are still large early on.
That's my guess. But great question.
Okay. So optimism in the face of uncertainty,
um, says that we should choose actions that might have a high value.
Why? Well, there's two possible outcomes.
If we pick something that we think might be good,
um, one is it's good.
So if it is good,
or I'll be more precise in this case.
So let's say we select, so, a_1 and 1 is a_1 has high reward.
So that's good. If we, um,
if we took an action and it actually does- and we- because we thought it
might have high reward and actually does have high reward,
then we're gonna have small regret. So that's good.
Um, what's the other outcome?
a_1 does not have high reward.
Has lower has, um,
when we sample this,
we get our r for a_1 with lower reward.
[NOISE]
Well, if we get something with low reward, we learn something.
So, we're like, hey, you know, we tried that restaurant again,
we thought it was great the first time.
The second time it was horrible.
So, now we've learned that that restaurant is not as good and we
update our estimate of how good that restaurant is.
And that means we don't think that a- that action has as high a value as it did before.
So, essentially, either the world really is great,
and in which case that's great, we're going to have low regret.
Or the world is not that great and then we learn something.
So, this gave us information.
And so this b,
acting optimistically gives us
both information about the reward or allows us to achieve high reward.
And so it turns out to have been a really nice principle.
It's been around since at least Leslie Kaelbling in 1993.
Introduced this idea of sort of interval estimation and then there started
to be a lot of analysis of these types of optimism under uncertainty techniques.
So, how can we do this more formally, or like,
where would we get to be more precise
about what it means for an action to have a high-value?
Let's imagine that we estimate an upper confidence bound for each action value,
such that the real value of
that action is less than or equal to the upper confidence bound with high probability.
And those upper confidence bounds in general are going to depend
on how many times we've selected that particular action.
Because we would like it to be such that if we've selected that action a lot,
that upper confidence bound should be pre- pretty close to Q of A.
And if we haven't selected it very much,
maybe we're really optimistic.
And then we can divide an upper confidence bound bandit algorithm
by just selecting whichever action has the highest upper confidence bound.
So, for every single action we maintain an upper confidence bound,
and then we just select whichever one has the max,
and then we update the upper confidence bound for that action after we take it.
So, a UCB algorithm would for t = 1 dot, dot, dot,
we would first have an initialization phase where we pull each arm once,
each arm once, and then we compute UT of at for all A.
And then for T = t dot, dot, dot,
we would select at equaling this arg max,
and then we would get a reward that is sampled from
the true reward distribution for that arm.
And then we would update Ut of
at and all other arms.
Turns out that often we have to update not just the action of the arm that we took,
but the action of all the other arms we took too.
And this basically comes into,
you, you don't have to do that,
but in terms of the theory we often have to do that in order to
account for high probability bounds.
And we'll see more about that in just a second.
So, every time you get a reward,
you update the upper confidence bounds of all your arms,
and then you select the next action and you repeat this just over and over again.
Okay. So, how are we going to define these U of T?
So, we're going to use Hoeffding's inequality, so a refresher.
In Hoeffding's inequality, we can apply this to a set of iid random variables.
We're going to assume right now that all of them are bounded between 0 and 1,
and we're going to define our sample mean just to be a,
the average over all of those variables.
And then what Hoeffding says is that the probability of that expected mean is,
like, this is the true expected mean.
So, this is the true mean.
This is our empirical mean.
And this is, you can think of this as like some epsilon.
This is just some constant.
Is the probability that your true mean is greater
than the empirical mean plus some constant U,
is less than or equal to expo- the exponential minus 2nu squared.
So, this is the number of samples we have.
Okay. So, we can also invert this to say,
if you want this to be true with a certain probability,
you can pick a mu so that Xn plus mu is gonna be at least as large as the real mean.
So, let's say what we wanna do is,
we want that this to,
we want that the empirical mean plus mu,
the probability that that's less than the real mean,
to equal to delta over T squared.
And we'll see why we might want to choose that particular probability shortly,
but let's imagine for a second that's what we want our probability to be,
since then we can solve for what mu has to be.
Okay, So, that's exponential -2nu squared has to equal to delta over t squared.
And then what we do is we just solve for what mu is. Thanks for letting me know.
Okay. So, mu in this case is going to be equal to
the square root 1 over 2n log of t squared over delta.
So, I just solve for that equation. What does that tell us?
That says that if we do,
if we define our ut of,
or in this case we define,
I'll keep with the same notation as for Hoeffding.
So, if we have Xn plus mu,
with that particular choice of mu,
that's generally going to be greater than or equal to the true expected value of X,
with probability greater than or equal to 1 - delta over t squared.
So, this, Hoeffding's inequality gives us a way to define an upper bound.
Because instead of these being Xs right now,
you can imagine those are just pulled from our arm and those are all rewards.
And so this says, if you take your empirical average of your words so far,
and you add in this upper bound,
which depends on the number of times we pulled that arm and t. So,
t here notices the total number of time steps we pulled any arms,
and n is the number of times we pulled that arm.
So, they're not the same thing.
So, we're [NOISE] inside of this competence bound,
we have a competence bound that is decreasing at
a rate of how many times we pull this particular arm,
and then we have a log term which is increasing at
the rate of the number of times we pulled any arm.
And that is the reason why after each time step,
we have to up- um, update the upper confidence bounds of all the arms.
So, you kind of have these two competing rates that are going on.
As you pull an arm more,
you're gonna get a better estimate of its reward, so it's shrinking.
But then you also have this slower growing term, this log,
which is increasing with the number of time steps. All right.
So, this is one way we could define our upper confidence bounds.
So, we could use this in the case of our rewards.
So, we might wanna say that ut of at is equal to our empirical average of that arm,
+ 1 over 2, the number of times we pulled that arm,
times log of t squared divided by delta.
So this is how, one way for us to define our upper confidence bounds.
[NOISE] All right.
So now the next question is.
Okay. So we've done that,
how is that gonna help us in terms of showing that maybe
the regret of something which is optimistic is actually sub-linear?
Okay. So what we're gonna do now is,
um, I'll do a quick poll.
Do you guys want me to write it on here or do you guys want me to do it on the board?
So raise your hand if you want it on the board. All right.
Raise your hand if you want it on here. Okay. We'll do
this next part on the board [LAUGHTER] that was easy. Was there a question in the back?
Yeah. I have question isn't this derivation about t because it seems like.
[OVERLAPPING] About what?
You first introduced it, about t?
Yes.
It seems you first introduced it there was basically a constant the way you moved
around but then later you're saying that
that's actually the time that we we're updating every time step.
So how are we able to do that?
Yeah. It's a good question.
So, um, you're right.
And I'm being slightly imprecise about this.
If you know what the time horizon is that you're gonna
be acting on a bandit, you could set t to be the maximum.
So you, if you know that you're gonna act for t time steps,
you can plug that in and then
your confidence bounds are, then that log term is fixed basically.
Um, in online settings if you don't know that,
you can also constantly be updating it with a time step.
It's a good question. Yeah.
How is delta decided?
How is what? Pardon.
How is delta, is like what is delta?
Okay. Good question. Um, so, question is what is delta.
What we're gonna, I did not tell you, uh,
in this ca- well, in this case it's telling us,
um, it's specifying what is the probability
this is holding like what this inequality is holding.
Later we're gonna pro- provide a regret bound that is high probability.
So we're gonna say we're gonna have a regret bound which is
something that like with probability is 1 minus a function of delta,
um, your regret will be sub-linear.
So that's how.
You can get expected regret bounds too and the UCB paper which, um,
one of the original UCB papers provides an expected bound but I thought it was a little,
the, this bound was a little bit easier to do in class.
So I thought I would do the high probability bound. Yeah.
So before we were talking about regret,
I didn't exactly understand how you use
regret to update your estimate of the action value.
Oh, good question. Um, so question is,
do we use or how would we use
the regret bound, the regret to update our estimate action, we don't.
Regret is just a tool to analyze our algorithm.
Great clarification. So regret is a way for
us to analyze whether or not an algorithm is gonna be good or bad
in terms of how fast the regret gro- grows but it's not used in the algorithm itself.
The algorithm doesn't compute regret.
And it's not used in terms of the updating.
Excuse me. Okay. So actually I'll leave this one up here.
So you guys can continue to see that.
All right. So let's do our proof.
So what we're gonna wanna do now is we're gonna wanna try to
prove something about, um,
the regret but before and how quickly it grows for the upper confidence bound algorithm.
But before I prove that,
I'm gonna try to argue to you that, um,
we're gonna look at, sort of, the probability of failure of these regret bounds,
of these confidence bounds.
So what I said here is that [NOISE] we're gonna define
these upper confidence bounds like this in
terms of the empirical mean for that arm so far plus
this term that depends on the number of times we pulled that arm.
And what I wanna convince you now is,
what is the probability,
what is the probability that on one step, that on some step,
step the confidence bounds will fail to hold.
Why is this bad?
Okay. So we wanna bound the probability that on
any step, excuse me, as we're running our algorithm that our confidence bounds fail to hold.
Why? Because if they all hold,
we can guarantee we're gonna be making some,
um, we're gonna have some nice properties.
Okay. So note, if all the confidence bounds hold,
like, on every step then we can ensure the following.
So if all confidence hold, bounds hold then Ut of at,
this is the actual arm we selected,
is gonna be greater than q of a star.
The real value. The real value of the optimal arm.
Okay. So why is this true?
There's two cases here, either
at is equal to a star, or at is not equal to a star.
So let's just take a second and maybe talk to your neighbor [NOISE] to say,
if it's the case that our confidence bounds hold which means
that really is the case that we have ut with,
um, that this confidence bound is,
um, gonna be greater than or equal to the mean for that arm.
So if these are true confidence bounds,
this equation is holding,
we're not getting a failure.
So we know that Q t which is this is gonna be greater than, um,
the real expected value for that arm.
Okay. So if that's true,
then this is gonna hold at every time step.
So maybe, let's take a neighbor or if it's not clear what I'm
asking or how to think about that, feel free to raise your hand too.
So there's two cases either the arm that we selected is a star
or the arm we selected is not a star and in both cases, this is gonna hold,
if the confidence bounds are correct.
So maybe let's take a second to to think about this or feel free
to raise your hand if it's not clear how to,
how to get started on that.
I wanna be just be clear here so.
[NOISE]
I just wanted to note at the top there that if the confidence bounds hold,
then that upper confidence bounds which is equal to that is going to be greater than
the real expected value for that arm. Yeah.
On the confidence bound on your other equation, you have written over there?
Yeah.
So you're saying that the optimal,
like your optimal Q value,
should be less than all the confidence bounds for any action?
No, good question.
So just need to clarify what it is.
This is saying for the arm that you selected,
the upper confidence bound of the arm you selected is- has
a value that- that- that upper confidence bound that you use to choose the arm,
whichever arm you selected,
the upper confidence bound of that arm is higher than the true value of the optimal arm.
That's what this equation is saying.
Saying that if the confidence bounds hold on all time steps,
which they might not, but let's say that they do
because these are only high probability bounds,
but if they hold on all time steps,
then whatever arm you selected,
its upper confidence bound is higher than the value of the true arm.
The real value of the true arm.
Okay, and I just wanted
to be clear what I mean for the confidence bound to hold, I put this up there.
So that means that the upper confidence bound of an arm holding means that
that upper confidence bound which is defined in
that way is greater than the true value of that arm.
So let's work- work through this a little bit.
So let's say there's two cases.
So if A T is equal to A star,
then that's what this is saying is that saying is,
is Ut of a star greater than Q of a star?
Does that hold if the confidence bounds hold?
Yes. By definition. So if you look up there.
So the upper confidence bounds if it holds for an action for
that- the upper confidence bound for an action has to
be bigger than the mean for that action.
If that upper confidence bounds hold.
Okay. So this ho- this works.
So if we really selected the optimal action,
we've defined our upper confidence bounds,
so they really are better than the mean of that arm, and so this holds.
The other case is that at is not equal to a star.
So what does that mean?
That means that Ut of at is greater than Ut of a star.
Because otherwise, we would have selected a star.
It means that some other arm had a higher upper confidence bound than the optimal action,
and we know that this is greater than Q of a star.
Okay? So if
the confidence bound holds,
we know at every single time step the upper confidence bound of the arm we selected is
better than the true mean of the optimal arm. Yes.
Is that true in the epsilon greedy case as well?
Is it true in epsilon greedy case as well?
I know I- I don't follow your question yet.
Like, you're selecting this arm using some strategy, right?
Yeah.
And it gets some maximizing action, right?
No. Ut, so this only holds,
this part only holds because we're picking the arg max, you're not going to be able to see this,
but the arg max A of Ut of at. So that first inequality,
only- well, there might be other algorithms that t holds for too,
but it holds in particular for the upper confidence bound.
Great- great question. Okay. So this says if we could get it,
and we will see shortly why that matters,
but kind of intuitively.
This says if the confidence bounds hold,
then we know that separate confidence bound of the arm we
select is going to be better than the optimal arm.
And the reason we're going to want that is later when we're doing the regret bounds,
we do not want to deal with properties we don't
observe namely the value of the optimal arm.
Because we don't know what Q of a star is.
We can't com- we don't know which arm it is.
So when we look at regret bound right now,
regret bounds are in terms of Q of a star.
We don't know what that quantity is.
So we're going to need to figure out some way to get rid of
that quantity and we're going to end up using these upper bounds,
but we're going to need the fact that the upper bound of the arm we
select is better than the Q star, Q of a star.
Okay. So- so this is saying that that's true if our upper confidence bounds hold,
what is the probability that that occurs?
Okay. So what that means is,
if we want to say that on all time steps,
this is a union bound,
this is our union over events.
The union over all the events for ut = 1 to t of the probability that
Q of a star minus the upper confidence bound of the action that we took.
We want this, oops, not that way.
We want the probability of this,
which is essentially the probability of failure.
So this is, what's the pro- this is if all confidence bounds hold things are good,
this says, what is the probability that that failed?
That the arm that you took it actually is not better.
The upper confidence bound is not better than the- the real mean of the optimal arm?
So this is the failure case, where the confidence,
but um, we're gonna say that if we look at that,
we- we don't want this thing to happen.
We can upper bound that
by making sure our upper confidence bounds hold at all time steps.
Okay. So these are the arms.
So what I said up there is that if all of
our confidence bounds hold on all time steps we can ensure that.
So we're now going to write that down in terms of what?
The probability that the upper confidence bounds do hold on all time steps.
And so we're gonna do probability that Q of at,
that's Q hat of at is greater than U.
Okay. This is the upper confidence bound we defined over there.
This is just saying that the upper confidence bound holds for each arm on each time step.
Well, by definition, over there we said we- we picked an upper confidence bound
to make sure this held with the least probability of delta over t squared.
Because that's how we defined our upper confidence bound.
We picked a big enough thing to add onto our empirical means so that we
could ensure that the upper confidence bound really was larger than our mean.
So now we have a union over all time steps,
a union over all arms,
delta divided by t squared.
And note that if you sum over t = 1 to
infinity of t to the -t, that's equal to pi squared over 6 which is less than 2.
So when you do the sum you get 2m delta.
So what this says is that the probability-
that your upper confidence bounds hold over all time steps.
So this is the negative,
this is that they- that they don't hold is at least 1 - 2m delta.
So all our- our- what we're gonna end up
doing is we're gonna have a high probability regret bound that says,
with probability at least 1 - 2m delta,
we're gonna get a smaller regret. Yeah.
So what about the infinite horizon case?
Great question. Yes. This is all for an infinite horizon case.
We're gonna look at the- we're gonna find
our regret in terms of t, the number of time steps.
Okay, all right. So why is this useful?
Do you guys want me to leave this up or we can move that now?
Everyone's written it down, who wants it?
Okay. So let's- can this go up?
Let me see or not.
Okay. So why is this useful?
Okay. We're now gonna define our regret.
So, this part of the board just says that we've
made it so these upper confidence bounds holds with high probability.
Now we're gonna try to show what our regret is gonna be, oh good.
Okay. Thank you. All right.
So, what's our regret?
Regret of our UCB algorithm after T time steps is just equal to the sum over
all those time steps t = 1 to t of Q of a star - Q of at. Okay?
Remember, we don't know either of these things.
We don't know what the real mean is of any arm
we pick and we don't know what the real mean is of the optimal arm.
So we need to turn this into things that we know.
These are unknown.
Okay? So what we're gonna do is one of our favorite tricks in reinforcement learning,
which is we add and subtract the same thing.
So we do sum over t equal- -t =1 to t of Ut, this upper bound,
at - Q of at + Q of
A star - Ut at. I just added and subtracted the same thing.
I picked the upper confidence bound of the arm that we selected at each time point.
Okay? So then the important thing is that what
we showed over here is that if all of our confidence bounds hold,
then the upper confidence bound of the arm we selected is larger than Q of a star.
That's what we showed over there.
So that means that this has to be less than or equal to 0.
Because the upper confidence bound of whatever arm we selected,
we proved over there,
is gonna be higher than the real mean of the optimal arm.
So this second part of this equation is less than or equal to 0,
which means we can upper bound our regret as follows.
So now we can drop that second term.
So that's nice, right?
Because now we don't have any a stars anymore.
We only are looking at the actions that we actually took at each time
step and we're comparing
the upper confidence bound of the- at that time step versus the real mean.
But remember the way we've defined our upper confidence bound and put it over here,
the way we defined our upper confidence bound Ut of at is exactly equal to, um,
the empirical mean, at, plus this square root 1 over 2
and at log t squared over delta. Okay?
And we said here that this was going to be the difference from
Hoeffding between Q of at - Q hat of at.
So the probability that this,
remember I called this U,
the probability that this was greater than U was small.
So now we're assuming that all of our confidence bounds hold,
which means that we know that the difference between
the real empirical mean and the true mean of this arm is bounded by U. Yeah.
Going back, for the bottom panel, sorry, it's a little hard to see, two questions. First of all,
where does this second, um, so you have a union over i = 1, the number of arms.
I don't see where that index actually factors in.
And then also if you could just go over the third line with the delta over t squared, and summation, how we derived that.
Sure? Yeah, so, um,
so what we did there is we said that if,
um, are you asking about the second line to the third line?
Yes. So what we did that in this case is,
um, we said for each,
we wanna make sure that on each of the time steps,
all of the upper confidence bounds hold.
Um, and so that's where we get an additional sum,
um, over here over all the arms.
So this is conservative, um,
trying to make sure we don't know- you could imagine
just doing this over the arm that's selected,
and but we don't know which arm is selected.
We want to be able to show, um,
this is going to be a looser upper,
upper bound saying this is sufficient.
So, um, we're saying that if you want to make sure that Q of
a star is greater than the upper bound of the arm that is selected,
it is sufficient to ensure that
your upper confidence bounds are valid at all time points from this,
from this, um, reasoning up here.
And so this is the probability that
your upper confidence bounds are correct on all times points,
for every single time point,
for every single arm, your upper confidence bounds have to hold.
And then, what we get in this case is,
we said that the probability on a particular time-step,
the upper confidence bounds holds is delta over t squared.
That's how we defined that U term [NOISE] so that according to Hoeffding's inequality,
it would hold with the least probability delta over t squared.
And then, so this is that,
and then I just made a side note that this is not something,
some of you guys might have seen this but I certainly wouldn't expected people to,
that just it turns out that in terms of the limit,
um, if you sum over t = 1 to infinity of t to the -2
that's bounded by pi squared over 6 which is less than 2.
Um, fun fact. Um [LAUGHTER] so,
so you can plug that in, right, because then you just get a 2 here,
and then you just get a sum over all arms which is m,
um, and you have a delta.
So this just allows us to take that infinite sum.
So notice also that,
um this goes to question before this holds for the infinite horizon
because when we did this summing we're basically
making sure that our confidence bounds hold forever.
So we're, okay.
Great. So we said here now that we're
doing all of this part under the assumption that our confidence bounds hold.
Our confidence bounds hold mean that the difference between our expected mean
and the true mean for that same arm is bounded by mu,
bounded by U with high probability where this is the definition of U.
So that's what our Hoeffding inequality allowed us to state.
So take that quantity now,
that's our U and plug it in here.
So this is looking exactly the difference between our upper confidence bound and Q.
So this is exactly equal to sum over t = 1 to t of U. Um,
it's a little confusing in terms of notation.
Um, so I'll just plug in the exact expression right there.
1 over 2 and t of a,
t log t squared over delta.
Okay, so we just plugged in that, the difference between
the empirical mean and the true mean is bounded by this quantity U.
Okay. All right.
So then in this case,
what we can do is we can split this up into the different arms that we pulled, okay?
So this is sum over all timesteps.
We can pull out,
note that this is, um,
if we upper bound this by big T,
this is equal to less than or equal to square root log big T squared over delta,
and then we're gonna get a sum over
all time steps and we're gonna split this up according to which arms we selected, okay?
So for, this is the same as if we look at for each of the arms,
how many times did we pull it?
So sum over n equals 1 to n, t, i,
square root of 1 over n. So we just divided this up,
like for each of the arms,
we selected them some amount of times.
That's here, i is,
i is indexing our arms.
So nt, i is the total number of times we selected arm i.
And then we sum that up, okay.
And then, if we note
the fact that if you sum from n = 1 to t square root 1 over n,
that is less than or equal to 2 square root t. You use an integral argument for that,
I'm happy to talk about it offline. Yes, in the back.
What happened to the 1 over 2 [inaudible].
Thank you, we have a 2,
we can put a 2 here.
Thanks. I'll be a little loose with constants but definitely catch me on them.
Because most of these bounds will all
end up being about whether it's sublinear or linear.
It's good to be precise.
Okay. So we have this quantity here,
um, when is this quantity maximized?
This quantity is going to be maximized if we pulled all arms an equal number of times.
Why? Because, um, 1 over n is decreasing with n. And so the,
the largest these can be is if you split,
if you split, um,
your pools across all arms equally.
So if we go back up to here,
and I call this a.
So a is gonna be less than or equal to, excuse me,
square root, 2 log t squared over delta times sum over i equals 1 to m,
sum over n equals 1 to t divided by
m. So this is as if we split all of our pools equally across all the arms.
1 over square root n, okay, and then we can use this expression.
Okay? So this is less than or equal to 1 over 2, we're almost there.
Um, t squared over delta and then we're gonna get sum
over i equals 1 to m of 2 square root t over m. Okay.
And then, when you sum this over m,
you get less than or equal to square root 1 over
2 log t squared over delta that brings in m into there.
When we look at another two in here times T,
m. And now we're done.
So what has this shown? This is shown that if we use upper confidence bounds,
that the rate at which our regret grows is sublinear square root times a log.
So t here is the number of timesteps.
So timesteps, so as if we are,
if we use upper confidence bounds, um,
in order to make our decisions,
then the regret grows much slower.
This is a problem independent bound.
It doesn't depend on the gaps.
There's, there's much nicer ones and tighter ones that depend on the gaps.
But this indicates why optimism is
fundamentally a sound thing to do in the case of bandits,
is that it allows us to grow much,
it allows us to have much better performance in terms of
regret than it does for the e-greedy case. Yeah.
Can you just display one more time the last one on the top board, um,
how you went from summation over t1 to big T and you just pulled out the t squared big T,
what happens to t = 1 to big T - 1?
Great question. Yeah, so this log term here t equals,
um, is ranging from t = 1 to t,
this log term is maximized when t is big T.
So we're upper bounding that log term and then it becomes a constant and we can pull it out.
Okay, so the cool thing here is that this is sublinear.
That's, that's really the main point.
Um, well, I go through an example and we'll go through,
um, more of this next time.
Um, I, I go,
we next go through an example for the toy domain for the broken toes of like what do
these upper confidence bounds look like in that case
and then what will the algorithm do in these scenarios?
Um, so that's what we'll look at next.
And then, after that we'll look at,
so this is one class of techniques which is this optimism under uncertainty approach,
which is saying that we're going to look at what the value is based on a combination of
the empirical rewards we've seen so far plus an upper confidence bound over them,
um, and use that to make decisions.
[NOISE] And then the next thing we'll see too
is where we are Bayesian about the world and we
instead maintain a prior and we update our prior and use that to figure out how to act.
So we'll go through this next week,
um, and I'll see you then.
 All right. We're gonna go ahead and get started,
and in consistent with this, uh,
theme of this section, we're gonna be optimistic under
uncertainty and hope that this will work but we will see.
Um, before we get into the content, um,
I wanted to ask if anybody has any questions about logistics or,
um, any other aspects of the general course. Yeah.
I just wanna double check with people who are doing the default project are okay,
either not submitting or just submitting
another thing for the milestone instead of doing the default.
Yes. Yeah, so, the question was asking about, um, to repeat.
The question was asking about whether or not people who were doing
the default project if they need to do anything in particular for the milestone.
Um, no, you don't.
Any other questions? Okay. So, just as a reminder,
where we are sort of in the class right now is, um,
last time we started talking about bandits and regret and we'll do,
um, a brief, uh,
recap of that today and we're gonna continue to talk about fast learning.
Um, and we're gonna go from Bayesian bandits towards Markov decision processes today,
and then on Wednesday we're gonna talk some more about fast learning and then we're
gonna try to talk about sort of fast learning and exploration.
And just to remind us all about,
like, why we're doing this, um,
the idea was that if we wanna move reinforcement learning into real-world applications,
we need to think about carefully taking the data that we have, um,
and how do we gather it and how do we best use it so that we don't need to
collect a lot of data in order for our agents to learn to make good decisions.
One of my original interests in
this whole topic was to think sort of formally about what does
it mean for an agent to learn to make good decisions and what are the sort of,
information theoretic limits of how much information
would an agent need in order to be able to make a provably optimal decision.
So, we've been thinking about sort of a couple of different main things here.
We're, we're talking about a couple of settings.
Last time we talked about bandits,
today we'll also talk about bandits and Markov decision processes.
We're talking about frameworks,
which are ways for us to formally assess how good an algorithm is.
Um, you know, these could be the framework of empirical success.
We, we talked last time about
the mathematical framework of regret and we'll talk today about
some other frameworks for evaluating in
general how good a reinforcement learning algorithm is.
And then, we're also talking about styles of approaches that
tend to allow us to achieve these different frameworks.
Um, and last time what we started to do is to talk about optimism under uncertainty.
So, just a quick recap from bandits.
Um, so, bandit was basically a simplified version of a
Markov decision process where in the most simple setting there's no state,
there's a set of actions, um,
and now we're going to think specifically about the fact that
the reward is through some stochastic distribution.
Um, there's some unknown probability distribution over rewards.
Um, at each timestep you get to select an action,
um, and see, you know, a reward.
And then your goal is to select actions in
a way that is gonna optimize your rewards over time.
And the reason this was different than a supervised learning problem is that you
only get to observe the reward for the action that you sample.
So, it's what's known as censored data.
Um, you don't get to see what would have happened if you'd went to Harvard.
So, um, we get to see censored data and
we have to use that censored data to make good decisions.
Um, and what we discussed here talking about regret, what we mean that in a,
in a formal mathematical sense is that we were comparing, um,
the expected reward from the action we took,
um, to the expected reward of the optimal action.
Now, notice that all of these things are stochastic.
Um, so, it doesn't have to have a particular parametric distribution
but imagine that we're thinking about, um, Gaussians.
Let's say, we had two Gaussians.
So, this is action 2 and this is action 1.
Okay. So, here's the mean of action 1.
So, Q of a1 is greater than Q of a2.
So, action 1 has the better expected reward.
But notice that on any particular trial,
you might sometimes get a,
um, uh, you could,
ima- imagine getting a result where, um,
the actual reward you get from
a sub-optimal arm is better than the expected reward of the optimal arm.
I just wanna highlight that.
So, imagine that you've sampled from action A2 and you got here.
It's a particular sample.
So, you could have a particular sample
be better than the expected reward of the optimal arm.
But because we're imagining doing this many,
many times we're again just looking at expectations.
So, we're saying, "On average,
which is the best arm,
and on average how much do we lose from selecting a sub-optimal arm?"
And the goal was to minimize our total regret
which is equivalent to maximizing our cumulative reward over time.
So, we then introduced this idea of optimism under, of, under uncertainty.
Um, and the idea was to, um, uh, uh,
estimate an upper confidence bound on the potential expected reward of each of the arms.
So, this was to say we wanna be able to say for each of the arms,
what do we think is an upper bound of their expected value,
and then when we act,
we're gonna pick whichever arm has the highest upper confidence bound.
And this was gonna lead to one of two outcomes.
So, this could, um, two things could happen.
So, either, either at is equal to A star,
and in that case, what's our regret?
0. So, if we select
the optimal action and we have our regret at 0. So, that's good.
And we have our regret at 0 like at per timestep or it's not.
And if it's not on average,
what happens to that upper confidence bound?
Yeah.
Yes.
Yeah, [NOISE] that answer is correct.
So, we lower it.
So, if we get, if we select an arm which is, um, not optimal,
then it means its real mean is lower, um, uh,
than the upper confidence bound we're averaging,
um, at least with high probability.
And so, then in general our UT of AT will decrease.
So, we're gonna gain information about what
the real mean is of that arm and we'll reduce it.
And if we reduce it enough,
over time we should find that the optimal arm's upper confidence bound is higher.
So, I'll ask you to play about what might happen if we do lower bounds.
Um, but that's one of the reasons why upper bounds is really good, and,
and I mentioned that these ideas have really been around for a long time,
um, at least around like 20, almos- 25-30 years.
So, I think the first one was 1993,
Kaelbling, by Leslie Kaelbling. She's at MIT.
Um, don't remember if it was her PhD thesis or if it was just after that.
Um, uh, but she talked about this idea of interval estimation of estimating,
um, the potential rewards.
She didn't do formal proofs of this being a good idea,
but she did it for Markov decision processes,
and they found that it was, uh,
a very good idea in terms of empirical performance,
and then a lot of people went around and did
the theoretical analysis and showed that provably this is a good thing.
And I think it's interesting often about which area ends up
being more advanced whether it's the empirical side or the theoretical side.
Okay. So, optimism under uncertainty in
the bandit case just involves keeping track of the rewards we've seen and we showed,
saw that we could use things like
Hoeffding inequality to compute these upper confidence bounds.
Because remember each of our samples from the arms is iid,
because they're all coming from the same parametric distribution,
which is unknown and we're given these samples.
So, what we found last time is that, uh,
if we use the upper confidence bound algorithm I did a proof on the board
to show that with high probability we had logarithmic regret.
Why was this important? Because we looked at
greedy algorithms and showed that they could have linear regret.
And what is it linear in?
So, um, this is the number of timesteps,
number of timesteps we act.
So why is linear regret bad?
Well, if it's linear,
it means that essentially you could be making the worst decision on
every single time point, and that's pretty bad.
So we'd like to have things that are going slower which means
that our algorithm is essentially learning to make good decisions.
Now, notice that in this case,
this is a little bit different, um,
statement of our result than what we saw before.
Um, it's related, this involves the gaps.
This is the gaps, delta a is equal to Q of a star, - q of a.
It's how much worse,
it is to take a particular action than to take the optimal action.
Um, and this is related to a bit different,
to the bound from last time.
Last time, we proved a bound that was independent of the gaps,
so it didn't matter what the gaps are in the problem.
Um, this is the bound that depends on the gaps.
Now, of course, you don't know what the gaps are in practice.
If you didn't know the gaps,
then you would know the optimal arm to prove, um,
but the nice thing about this is that i- it's always important to know whether or
not this sort of knowledge appears in the analysis or in the algorithm.
This is saying, you don't need to know what the gaps are,
but if you use this algorithm,
[NOISE] how your, how your regret grows depends on a property of the domain.
You don't have to know the property of that domain,
but that's what your regret will depend on.
And so this is saying that for upper confidence bounds,
if you have different sizes of gaps,
you're gonna get different regrets.
Um, your algorithm is just going to proceed by following upper confidence bounds,
it doesn't need to know about these gaps,
you're gonna get better or worse performance depending on what the gaps are.
And in general, we'd really like these.
We'd like our algorithms to be able to adapt to the problem so
that thi- this is known as a problem-dependent bound,
right here, and you would like those.
You'd like to have algorithms that are agnostic,
that are sort of saying,
provide problem-dependent balance, that they work better if the problem
is easier. All right.
So let's go to our toy example,
which we were talking about as a way to sort of see how these different algorithms work,
and we're looking at fake, um, ah,
ways to treat broken toes, um,
where we are looking at surgery,
buddy taping, or doing nothing,
and we're imagining that we had a,
a Bernoulli variable that determined whether or not these treatments worked.
So surgery on in expectation was the most expect- uh,
most effective thing, with 0.95 success,
ah, buddy taping was 0.9,
and doing nothing was 0.1.
So how would something like upper confidence bound algorithms work?
Well, in the beginning we don't have any data,
so let's sample all the arms once.
That's going to involve sampling from a Bernoulli distribution.
Um, so in this case,
let's imagine that we've got,
um, a1, a2, and a3 here.
And note that this is,
[NOISE], yeah, all right,
this is our, their empirical estimate where we just average,
over all the, all the rewards we've seen from a particular arm.
Okay? So we pulled each arm once,
which is the same as taking each action once, we got 1-1-0.
And now what we have to do is compute the upper confidence bounds
for each of those arms before we know what to do next.
Okay. So in this case,
we're gonna define our upper confidence bounds by being the empirical average,
plus the square root of 2 log t,
t is the number of times we pulled arms,
and N t of a is the number of times we've sampled a particular arm.
So this is total arm pulls.
This is a particular arm.
Okay. So what does that gonna be in this case?
Let's just define it for each of them.
So UCB of a1 is gonna be equal to 1,
because that's what we've got so far,
plus square root 2 log of 3,
because we pulled three arms so far, divided by 1.
UCB of a2 is gonna be the same,
because we've also pulled that arm once,
and it got the same outcome.
And then UCB of a3 is gonna be different,
because its current reward is- or current expected value is 0.
So it's just gonna be equal to 2 log 3 divided by 1.
That's how we could instantiate each of the bounds,
and now we've defined the upper confidence bounds for each of the,
ah, um, each of the arms.
So in this case,
after we've done that, we're going to pull one of these arms.
Um, let's say that we break ties randomly,
so the upper confidence bound of a1 and a2 is identical.
So with 50% probability, we select one.
With 50% probability we can select the other.
Okay. Um, and let's just compare that for a second.
So, um, if we're using UCB, I said that,
so I'll just redefine this here so people can remember,
is equal to UCB of a2.
This is- UCB stands for upper confidence bound is equal to 1,
plus square root 2 log 3 divided by 1.
And UCB of a3 is equal to square root 2 log 3 divided by 1.
Okay. So why don't we just take a second, um,
and define what would be the probability of selecting
each arm if you're using e-greedy with epsilon = 0.1.
And what about if you're using UCB?
As always, feel free to talk to anybody nearby.
[NOISE]
All right, so let's vote [NOISE].
I'm gonna ask you to vote if, um,
if two arms have, um,
non-zero probability or three arms have non-zero probability.
So if you're using UCB [NOISE],
do two arms have a non-zero probability?
Do three arms have a non-zero probability?
Somebody who know- who thought with UCB,
you only have two arms with non-zero probability want to explain why? Yeah.
Because since you're picking the maximum action you're only going to pick a1 or a2.
That's right, yeah. So, um,
if we're picking the maximum action here, um,
we're only gonna pick, um,
action a1 or a2,
we have zero probability of my third arm.
Okay. Let's do a quick vote for, um, e-greedy.
For e-greedy, do we have non-zero probability on two arms?
On three arms? That's right.
What's the probability of selecting a3?
[NOISE]. Yeah, [inaudible]
[NOISE] Um, 0.1 [NOISE].
Anyone else wanna add? Yeah?
It's 0.03.
Yes. Or 0323, yes, exactly.
So in the 0.1 in this case, just,
um, remember we normally define that by uniformly splitting.
Yeah. So we're gonna just have 0.1 divided by the number of arms.
Okay. So here, um, wh- why do I bring this up?
So I bring this up to indicate that while UCB is still, um,
splitting its attention among all the arms that look good,
it's not putting any weight right now on the arms that it doesn't think could be good,
which in this case is arm 3.
Whereas an e-greedy, um,
approach is gonna be uniform probability across anything it doesn't think it's best.
So this is one of the insights for why these arguments might be better,
is they're being more strategic in how they're weighing, um,
what arms to pull, um,
compared to what a- what an epsilon greedy,
which is just doing uniform.
Okay. So let's look at, um,
sort of, uh, what the regret would be in this case.
So, um, the actions we pulled is a1,
a2, a3, a1, a2.
Um, so why would,
why would we pull a2 again,
let's just go through that briefly.
So let's say, um,
so first, let's say we're, we're,
we're gonna pull a1.
Let's imagine that's which one we picked.
Okay, so we pulled a1.
So let me just go through one more step of what
the upper confidence bounds would be in this case.
So let's say we pull a1, okay.
So now, we need to redefine the upper confidence bounds,
we actually need to redefine them for all the arms,
because if the denominator of that upper confidence bound, it depends on t,
which is the total number of pulls
or the total number of pulls so far, if we're using that form.
So now, we're gonna have that UCB.
Let's say we pulled action a1,
and let's say I got a 1.
Okay? So UCB of a1 is gonna have the same mean which is 1,
plus square root 2 log 4,
because we've now pulled things four times,
but now, we pulled this arm twice.
So it's- we're gonna divide this by 2.
UCB of a2 is gonna have the same mean as before because we didn't pull it.
And then it's gonna also have the 2 log 4,
but we've only pulled it once,
and UCB of a3 still has a mean of 0,
and it's also gonna have 2 log 4 divided by 1 [NOISE].
Okay. So in this case,
what we're gonna find is that we sort of are gonna get
this trading off, we still have the same empirical mean for a1 and a2.
But now, we haven't pulled a2 as much as a1,
so we're gonna flip, and we're gonna pick a2 now.
So that's why if we imagine that we got,
well actually, as a quick check your understanding, um,
ah, this result would happen whether we picked 1,
whether we got a 1 or 0,
for a1 when we last pulled it.
Um, so it's a good thing to check, is all I'm saying.
But even if we got a 1 for a1,
we'd still select a2 on the next round,
because the upper confidence bound of it would drop,
even if its mean was the same.
So if we, if we look at this then,
so we're gonna compare it to what would be
the optimal action if we take him out the whole time, which is a1.
So what is our regret?
Our regret is gonna be 0,
and then here it's gonna be 0.05,
which is just equal to 0.95 - 0.9.
Here it's gonna be 0.85, because it's 0.95 - 0.1.
Here it's gonna be 0,
and here it is going to be 0.05 again.
So that's how we'd sum up regret, the regret.
Of course, we don't actually know this,
and we can know this if we're doing this in the simulated world,
but we can't knew- know this in reality because, again,
if we actually knew it in reality,
then we wouldn't have to be doing this.
You would know the optimal arm. May I have any other questions
about how UCB is working? Okay.
Now, just I guess, one other quick note here which is, you know,
these upper confidence bounds are high probability bounds, so they can fail.
That is possible that sometimes the upper confidence bound is lower than,
um, the true mean.
And so that's why when we did the proof last
time we had to talk about sort of these different,
what happens if the upper confidence bounds hold?
Um, uh, and there we did sort of a high probability bound.
Okay, so an alternative would be to always select the arm with the highest lower bound.
So what we're doing right now is just selecting the arm with the highest upper bound,
but you could select the arm with the best lower bound.
Um, so what might that look like, let's imagine that we have two arms,
a1 and a2, and this is our estimate of the Q of a.
Okay. So let's imagine that these are uncertainty bounds.
So in this case a1 has a higher upper bound,
but a2 has a higher lower bound.
So why don't we take a minute or two and think about,
why this could lead to linear regret?
I think two-arm case is the easiest one to think about for this.
Feel free to talk to anybody around you, if you wanna brainstorm.
And this is actually an important reason why it's good to be optimistic,
at least in reinforcement learning.
[NOISE]
What do you guys think?
Does it lead to linear regret?
[NOISE] Right, so we're going to get,
sort of like, confirmation bill,
so like, a2 is giving out smaller and smaller [inaudible].
Okay, so I talked to at least one person in the audience that gave the right answer.
Which is, um, in this case if you select a2,
you're gonna continue to get towards the mean.
The real mean is above the lower bound of a1.
And so you're gonna,
kind of, get confirmation bias,
like, a2 is gonna continue to look good,
and you're never gonna select a1.
So we're never gonna get information that allows us to disprove our,
our hypothesis, and we're never gonna learn what the true optimal mean is.
So that's why we can get linear regret.
So one thing I just wanna highlight is that, um,
upper confidence bounds are one nice way to do optimism, er,
and they can change over time in terms of upper bound,
but the simpler thing you might imagine doing is just to
initialize things really to a high value.
Um, so pretend, for example,
that you already observed one pool of each of
the arms and that it was really, really good.
So just initialize all your values at like a million or something like that.
Um, and then you just kind of average in
that weird fake pool when you're doing your empirical average.
So you can imagine you pretend you pull each of your arms once you've got a million,
you've got a trillion, and
then after that you just average in all your empirical rewards.
So this actually can work fairly well in a lot of cases,
the challenge is to figure out how optimistic you need to be for that fake pool.
So just in terms of sort of comparing that approach to other approaches,
recall that greedy gives you linear total regret.
Constant e-greedy can also give you a linear total regret.
If you decay e-greedy,
you can get actually sub-linear regret,
um, if you use the right schedule for decay in epsilon,
but that generally requires knowledge of the gaps which are unknown.
So this is sort of uh,
it's generally impossible to actually achieve.
But if, in principle,
you know, if you sort of, you know,
had an oracle, you could,
uh, you could figure out how to decay things.
If you're too optimistic in your initialization,
um, if you initialize the value sufficiently optimistically,
then you can achieve sub-linear regret, but again,
it can be pretty subtle to figure out exactly how optimistic those need to be.
I, and I'll talk later about an example where in that Markov decision process case they
have to be much more optimistic than you might think they
would need to be in order for this to work well.
All right, so we're gonna now start to talk about Bayesian bandits and Bayesian regret.
And so far we've sort of made very little assumptions about the reward distribution.
We've assumed that the rewards might be bounded,
so we typically assume that the rewards are gonna lie in sort of 0,
0 to 1, or 0 to some, you know,
0 to R max or something like that,
that we have bounded rewards that you can't have infinite rewards as nice as it would be.
Um, [NOISE] but we haven't been making
strong parametric assumptions over the distribution of rewards.
[NOISE] Um, Hoeffding doesn't, uh,
Hoeffding requires the rewards to be bounded but it doesn't assume,
for example, that they're Gaussian or Bernoulli or things like that.
So an alternative approach is to assume that we actually do have information
on the parametric distribution of the rewards, and it'll exploit that.
Um, so we're gonna talk about Bayesian bandits now,
where we sort of explicitly compute a posterior over the rewards given the history.
So given the previous actions,
we, the arms we've pulled and the rewards we've observed.
Uh, uh, and given that posterior we can use that posterior to guide exploration.
And of course, if your prior knowledge is accurate, that might help you.
Um, there's sort of somewhat of a debate between Frequentist and Bayesian,
um, views of the world.
We're not kind of get really too much into that in this class,
but the idea is that it's also gonna be a nice way to put in prior knowledge.
If you have prior knowledge about the particular reward structure of
your environment you can put those in and it can help in terms of exploration.
Okay, so in the Bayesian view now we're
just gonna do sort of a quick review of Bayesian inference.
Um, a number of you guys it's probably gonna be a- a refresher for,
for some people it might be new.
We're gonna assume that we have a prior over the unknown parameters.
So in our case the unknown parameters are gonna be
the parameters that determine the reward probability distribution for each of the arms.
And the idea is that given sort of observations or data about
that parameter like observing rewards when you pull an arm,
we're gonna update our uncertainty over the unknown parameters using Bayes' rule.
So let's look at that as a specific example.
So for example imagine that the reward of an arm i is
a probability distribution that depends on some unknown parameter phi i.
So note that this is unknown.
And we're gonna have some initial prior over phi i,
which is the probability of phi i.
So this was before we pulled that arm at all.
This is sort of our uncertainty over that parameter.
[NOISE] And then we pull arm i and we observe a particular reward, ri1.
And then we can use this to update our estimate of the distribution over
the parameters that determine our reward probability distribution for this arm.
So, we do that using Bayes' rule.
And we say that the posterior probability of our parameters
phi i given that we've observed that reward is equal to
our prior probability over those times provide data evidence or likelihood,
divided by the probability of
observing that reward regardless of what your parameters were.
And so this is Bayes' rule, um,
and the challenge or the important thing here is how do we compute all of those things.
So, that tells us how to update our posterior over the parameters,
and the question is how do we do this?
[NOISE] Okay.
So, in general, doing this sort of updating can be very tricky to do.
Because if you don't have any structure on
the sort of parametric form of the prior and the data likelihood.
So, this again is the prior,
and this is the data likelihood.
If you have no structure on this, um,
one of them is a deep neural network and another one of them is some random other, um,
parametric distribution, then it may be impossible to have
a closed-form representation for what the posterior is.
So in general, this can be really hard.
Um, [NOISE] but it turns out that there's particular forms of the
prior and the data likelihood that mean that we can do this analytically.
[NOISE] So, who here is familiar with conjugates?
Okay, some people but not everybody.
So, these are really cool conjugates.
Um, exponential families, for example, are conjugate distributions.
Um, [NOISE] the idea is that if
the parametric representation of the prior and the posterior is the same,
we call the prior and the model conjugate.
So, what would that mean so, for example,
what if this is like a Gaussian?
If this is a Gaussian and this is a Gaussian,
then we would say that this and this are conjugate.
Whatever thing we're using for the data likelihood.
It essentially means that we can do
this posterior update- updating analytically or in closed form, which is really nice.
So, we call it, this means that we sort of keep things in
the same parametric family as we're getting more evidence about these hidden parameters.
[NOISE] I'll give an example of this in a second.
Um, but there are a number of different parametric families which have conjugate priors.
Which means that, um,
if you have an initial uncertainty over the parameter in that distribution,
then if you observe some data you can
update it and you are still in the same parametric family.
So, they're super elegant, and come up in statistics a lot.
Um, [NOISE] all right. So, here's
a particular example that's relevant to us which is Bernoulli's.
Um, so let's think about a bandit problem
where the reward of an arm is just a binary outcome.
Um, and that this is sampled from a Bernoulli with parameter theta. So this comes up a lot.
This is things like advertisement click-through rates,
patient treatment succeeds or fails et cetera.
So, many, many cases we- when we pull an arm,
or when we take an action we're gonna get a binary reward, either 0 or 1.
[NOISE] So it turns out that, um,
the beta distribution, beta alpha beta is conjugate for the Bernoulli distribution.
So that means we can write down our prior over the Bernoulli parameter,
um, given alpha and beta as follows.
It's theta to the alpha - 1,
1 - theta to the beta - 1,
times a ratio of the gammas.
Gammas are related to the factorial, factorial distribution.
So, all of this can be computed analytically.
And one nice way I like to think about this is that we can think of alpha and beta
as essentially being the result of prior pulls of the arm.
So, we can use them also to encode sort of prior information about this.
And I'll show you shor- shortly an example of how these get updated.
[NOISE] But what happens is that if you assume that the prior over Theta is a Beta.
Um, so, if it looks like this, this is the prior.
Then if you observe our reward that's in 0, 1,
because that's what our reward, distribute rewards are whenever we sample one,
then the updated posterior over theta is a really nice form.
It's just the same beta distribution with
either 1 added to the alpha or 1 added to the beta.
So, essentially if you observe r = 1 then you get beta of alpha + 1 and beta.
If you observe r = 0 you instead add it to the Beta term.
So, you can think of alpha as being all the number of times that you saw a reward of 1,
and beta as all the number of times you saw a reward of 0.
Can you explaining how the fact that theta was Bernoulli factored into
this description and why this isn't just a description
of a beta distribution, the main equation?
[NOISE] Like why- why it is important to first say that theta is Bernoulli?
I'm bringing up that, uh, great question.
So, the reason I bring this up is that beta is a conjugate prior for the Bernoulli.
So, the idea is that in this case if we're thinking
about an arm which has binary outcomes,
then we can think of the average of
that arm as being represented by a Bernoulli parameter.
So, let's say like 0.7%, you know,
on average 0.7 times,
we get a reward of 1,
and 0.3 we get a reward of 0.
So, [NOISE], um, the mean of that arm is 0.7.
Okay. So, we're thinking about an arm
which has a Bernoulli parameter that describes its mean.
So, we're thinking of like you know an arm with mean equal to theta.
So, that's what the mean is for a Bernoulli distribution.
Um, and what I'm saying is that we want to now be Bayesian about that,
and we want to think about what is the probability of
that parameter given the data we've seen so far?
And if we wanna be able to update our estimate over theta,
not over rewards, over theta,
then we're gonna write down our distribution over what theta's might be possible,
um, [NOISE] as a beta distribution.
And we're going to update that as we see evidence.
And I'll show you shortly like what these betas look like, so we can think of so,
what is the probability distribution over thetas as we get more evidence.
So, for example, you might imagine if you see a reward which is 1, 1, 1, 1, 1, 1, 1,
um, then your beta distribution is going to
indicate that a theta which is really high is more likely.
If you get 0, 0, 0, 0, 0, your beta is gonna have a different shifted posterior,
which is gonna say probably your theta's really low, close to 0.
Cool. So, we'll, we'll see an example of this in just a second.
Um, so, the nice thing in this case is that for
Bernoulli's which is a really common distribution that we often want to think about,
we can write down, um,
a prior over that parameter and we can update it analytically just using the counts.
So, we just keep track of how many times we've seen
a 1 or how many times we have a 0,
and we use that to update our posterior.
Okay so how do we evaluate performance now we're in this Bayesian setting?
So in the frequentist regret,
we didn't think about having distributions over parameters.
We just thought of there being some parameter like,
you know what's the mean of that arm.
Um, and then we defined our regret with respect to the best arm.
Bayesian regret assumes there's this prior over parameters.
And so Bayesian regret says,
what is my expected regret by thinking
about what are the possible parameters given my prior.
Um, and then looking at the expected performance if I got a particular theta.
So it's a little bit different way of looking at the world.
Um, again, we're not gonna really get into the philosophical aspects of this.
But Ba- Bayesian regret is saying like well,
we're not sure, you know what the distributions are of these arms.
Um, and there'll be different worlds in which they'll take on
different values and how well do you do in those different worlds
on average. All right.
So how do we try to make good decisions for Bayesian bandits?
So one thing you might imagine is let's say we have
a parametric distribution over the rewards for each of the arms.
Um, we, we could have,
we could certainly have that in the Bayesian case.
The idea of probability matching which I think has been around since around 1929.
Its been around a long time, like almost 100 years.
Um, ah, is that we wanna
select an action a according to the probability that it's optimal.
So it seems quite intuitively appealing like we
want to select arms that might be optimal more.
Um, we want to select arms that probably aren't likely to be optimal less.
And it is optimistic in the face of uncertainty because [NOISE] in
general uncertain actions have a higher probability of being the best.
So uncertain actions mean we don't know very much about what their rewards are.
Um, the problem is this sounds really nice,
we'd like to sort of select arms according to the probability that they're
optimal but it's completely unclear how to compute that.
So this expression here is saying we wanna sample an arm given a history.
So the history here, here is prior pulls and reward outcomes.
So this is the history of the arms we
pulled and whether we've got what sort of rewards we've gotten.
And then we wanna pull an arm according to the probability that
that arm is better than all the other arms given that history.
So that's quite intellectually, um,
appealing but it's not at all clear how we would compute that quantity.
And so it's sort of somewhat magical that, um,
a very simple approach turns out to implement probability matching.
And the idea is called Thompson sampling,
and again this came out, you know,
roughly in the 1920s.
And one of the really interesting aspects of that is it sort
of disappeared for a long time in terms of bandits.
Certainly in the AI community and CS community.
And then around eight years ago,
eight to nine years ago,
people sort of got re-interested in understanding these, um,
in part due to a paper that [NOISE] a colleague of mine published which you'll see
results have shortly which indicated that empirically it can be really good.
Okay. So how does Thompson sampling work?
We're gonna initialize a prior over each of the arms.
Often we'd like this to be conjugate, doesn't have to be.
It's nice if it's conjugate.
But we gonna have a probability over each of the arms.
Um, now remember that this is
sort of a probability over the parameters determining the distribution.
So, um, it could be if we have Bernoulli arms,
it could be the probability of theta.
I for i equals 1 to the number of arms.
So, for example, this could be a beta distribution of 1, 1.
We could say the probability that
my ith arm has a Bernoulli parameter of theta is equal to,
um, uh, sampling from a beta 1, 1.
Okay. So we're gonna pick a particular parametric family
to represent our prior distribution over the,
um, reward distributions for each of the arms.
And then what we do is for each round,
we first sample a rewards distribution from that posterior.
Again we'll go through a concrete example of this in a second.
But this is like picking a particular theta.
So it's like saying I'm assuming that my mean for
this arm e- or my Bernoulli parameter for this arm is 0.7,
picking a particular value for that parameter.
And then once you have that you can compute
the action value function by just taking the mean of whatever that is.
So notice that if theta,
let's say we sample for arm 1,
let's say we sample 0.9.
We just happen to sample that arm 1 has,
um, uh, a theta parameter 0.9.
Well, the Q for arm 1 is then
gonna be the expected value of a Bernoulli parameter theta one which is just 0.9.
Because the expected value for a Bernoulli variable is just p,
just the, just the theta.
So we're gonna compute the action value function for each of these.
Um, in the case of a Gaussian,
it would just be its mean, for example.
So you just compute what the mean expected reward is for each of the arms under
the particular reward distribution we sampled and then you take
whichever action looks best for those sampled parameters.
So that's this. And then we take
that action and we observe a reward and then we update our posterior using Bayes' Law.
So we're just gonna take our priors over the parameters.
We're gonna sample a particular set of parameters.
These are probably totally wrong.
This is just us making up what the actual,
you know, parameters are for each of the arms.
Then we act as if that world is optimal,
we get some data, and we repeat.
So it's a, it's a fairly simple thing to do.
Um, we of course have to see how we can do this sampling.
Um, and I'll show you an example with Bernoulli's in a second.
Um, but nowhere here are we trying to explicitly
compute like what is the posterior probability that this arm is optimal.
We're just sampling some and then we're going to
be sort of greedy with respect to those samples.
Okay. So Thompson sampling
turns out to implement probability matching which is super cool.
Um, and for some of the intuition of this,
so this is what probability matching is.
Probability matching is that we wanna select an action given the,
um, history according to the probability that it's optimal.
According to the probability that arm really is the best arm.
And we can think of that as being equivalent to the expected value
of picking a reward given the H and that,
ah, the arm is equal to the arg max of QA given that history.
And that's what Thompson sampling is computing.
Okay, so let's see how this actually looks for the broken toe example,
because I think that'll make it a lot more concrete.
So again, in this case,
remember that we have, um,
three different arms and they each are looking at the success or failure of,
um, doing this treatment to try to make people's broken toes better.
Um, and surgery is a Bernoulli parameter with 0.95.
Taping is, uh, is,
uh, 0.9 and nothing is 0.1.
So if we wanted to then run Thompson sampling in this environment,
remember it doesn't know what those actual parameters are.
We're gonna choose a beta 1, 1
prior over the parameters.
So that means that we're gonna say the probability of theta 1 is equal to a beta.
Okay. So what does a beta 1, 1 look like?
It looks like a uniform distribution.
So this is 0 to 1. This is theta.
This is probability of theta.
So what this says is that if someone gives you a beta 1, 1 distribution,
it says you're going to select a Bernoulli parameter from that.
I have no idea what its value is.
Could be 0, it could be 1, it could be 0.5.
It- it's sort of an uninformative prior.
Okay. So it says that initially I have no idea what,
um, these theta parameters might be for each of the arms.
Um, so I'm just gonna pretend it's- I'm gonna start off and assume it's flat.
I have no information.
Okay. So this is what,
um, a beta 1, 1 looks like.
But what's gonna happen in Thompson sampling?
So this is our distribution over thetas.
And what Thompson sampling is gonna do is it's gonna
sample a parameter from that distribution.
So in this case it's just a uniform distribution between 0 and 1.
So we're just gonna select some value between 0 and 1.
So in this case imagine what we got is,
I'll just leave this up for a second so people can see 0.3, 0.5. and 0.6.
There's no reason that arm three would be
higher or lower than arm one or arm two or arm three.
In reality arm one is best,
but we have no information about that so far.
We have no rewards so far.
All we've done is we've just said,
I have a uniform distribution over what
my theta parameter might be, I'm gonna sample from it.
And so in this case, it's like sampling and you've got this value once,
you got this value once,
and you got this value once.
And we just sampled from that distribute-
that uniform distribution and these are the parameters we pegged.
And now we're going to pretend that's real.
So we're gonna say I'm gonna pretend that my theta for surgery is 0.3.
My theta for taping is 0.5 and my theta for nothing is 0.6.
So if that was the real world we lived in,
what arm would we select?
Third arm.
Third arm. Exactly. So in this world,
the third arm really is best because that's a theta of 0.6.
So we're going to select theta as 0.6. Yeah.
Using uninformative priors because I could have many values of beta 2, 2 or 5, 5.
So is that a significant advantage of using this for Thompson or something?
Yeah. It makes a really good point.
She said we currently use an uninformative prior,
is it better or worse to do like that compared to using an uninformative prior.
So you could have had a beta of like 3, 4 et cetera.
Um, a beta of 3, 4 or anything that's not 1, 1 is gonna give you,
um, is gonna bias your distribution,
it's gonna change the shape of how you sample things.
If you have actually good information that can be really useful,
um, because it's essentially like having fake pools.
Um, and it can- it can guide sort of your initial samples.
The downside is that if that isn't correct,
you can be misled for a while.
So we often talk about like how robust are we
to misspecified priors or to wrong priors.
Um, and so using the uninformative prior means that,
ah, you're not getting a lot of benefit
from prior knowledge but you're also not gonna get a disadvantage.
Okay. So in this case we're gonna select the arm- arm three
because that's just the arm that has the best expected mean,
um, under the samples that we did.
Okay, but arm three is actually not very good.
And we know that because arm three actually only has a reward of 0.1.
And so when we sample it and we get the patient's outcome,
we're gonna get a 0 in this particular case.
Because the real arm three is 0.1.
So if we sample from a Bernoulli with 0.1,
most of the time we're going to get a 0.
So now we have to do is we have to update our posterior over arm three.
Okay, we have to update what sort of what are our probability of
theta of arm three is, given that the reward was equal to 0.
Okay. [NOISE] So what we talked about is that the beta
is a conjugate prior for the Bernoulli.
And if we observe a 1,
we're going to update the first parameter,
also we're gonna update the second parameter,
so we just saw a 0.
So our new beta is 1, 2.
Because we just saw a 0 and so we update.
So this is our new parameter.
That's our new posterior over arm- the arm three and it looks like this.
Okay. So this is still theta,
always has to be between 0 and 1 because this is a Bernoulli parameter.
And this is what the probability looks like now.
So notice it shifted,
so it used to be flat,
and now it says well no,
I just observed that we've got a reward of 0.
So now I have a higher probability that theta is small.
So if I was going to sample from this,
it is more likely I would get a lower value compared to a higher value unlike before.
Okay. So this is our new posterior.
And what does the posterior look like for the other arm?
So this is for, um,
this is for theta three.
And for the other two,
they still look uniform because they're still a beta 1, 1.
So for beta 1, 1.
This is for the other arms,
theta 1, theta 2,
and this is probability of theta.
The other two are still uniform because we- we haven't pulled them yet.
We don't have any outcomes.
So they still look like uniform distributions.
But the probability over theta 3 looks skewed towards 0.
So now in Thompson sampling,
we again are just going to sample a value from each of these different ones.
Each of those three distributions.
And now imagine we get 0.7, 0.5 and 0.3, yeah?
Turn back a slide, should that say p of Q a3, not Q of a1 because didn't you say a3-
Thank you. Yeah, hold on, there's a couple of errors there, yeah.
Thanks for catching that. Any other questions?
Okay. So we updated our posterior over arm three.
Our posterior over arm one and arm
two is the same as the prior because we didn't- we didn't pull them.
Okay. So now we're gonna sample from those three distributions.
What Thompson sampling would say is now given our posterior over all of the arms,
let's select an actual parameter for each of the arm.
And this time we're going to get 0.7, 0.5 and 0.3.
So which arm where are we going to select this time?
Arm one.
Arm one. Right? So now the max is gonna be arm one.
Okay. So now we're gonna
have a posterior that looks like beta 2,1 because we update our pe- beta.
And remember we can just think of this as being the number of R equals
1s + 1 because we started with a beta
1, 1 and this is the number of r = 0s plus 1.
So now our new posterior for this one makes it look like this.
So this is theta 1,
this is 1, 0 probability of theta 1.
Okay. So now as we would expect,
we saw that arm- arm one had a good outcome
and so now our probability that that Bernoulli parameter
is higher than 0.5 is going up because we saw some positive results.
Okay. So now we have a- so what does our new distributions look like?
We have a beta 2, 1,
we have a beta 1, 1 because we haven't selected arm two yet,
and then we have a beta 1, 2.
All right. So what's going to happen next,
um, let me again are gonna sample a Bernoulli parameter.
So let's imagine that we got 0.71, 0.65 and 0.1.
And so that means we're again gonna select arm one.
And we again observe a one,
surgery is pretty effective.
And now our posterior is 3, 1.
So now this again is 0 to 1.
This is our probability of theta 1.
Okay. So now it's looking even more peaked.
So what's your guess of what's the next arm we're likely to sample?
So remember the three distributions that we have right now is for arm two,
looks like this, for arm three, looks like this.
So this is the probability of that arm.
Since theta a2, theta a3, 0, 1, 0, 1.
So who thinks that, um,
theta 1 is again gonna be sampled and look better than everything else?
That's right because it's going to have- has a posterior over
its Bernoulli parameter that is getting closer and more and more steep towards 1.
Theta 2 we still never- we've still never taken action a2,
but it just has a uniform probability.
So it's very unlikely that we're gonna sample a value
for it that is better than the value we sampled for arm one.
So again in this case,
we can imagine sampling again,
we get 0.75, 0.45, 0.4.
We select action a1 again and now we have a beta 4,1 and it's looking even more sharp up.
So notice this is quite different than what UCB was doing.
UCB was splitting its time between a1 and 2 at the beginning because,
um, they were both reliant on their empirical means,
um, but then a2 had been taken less times.
In this case, we still haven't taken action a2 yet.
And it may be hard for us to pull it for a while.
Now, that's not actually bad in this case because theta 1 is actually the best arm.
But there can be sor- some trade-offs. Yes.
Is this the only way we can update the Beta distributions?
Uh, is, is there a rule that we should increment it
it by one or [NOISE] of course we have different kinds of rewards.
Here rewards are 0 and 1, right?
So do you have some kinds of rewards probably of beta, beta distribution.
Great question. Category is like, okay,
so here we've got, um, binary rewards.
How would we do this if the things were not binary?
In that case we wouldn't use a beta in a Bernoulli.
So if you didn't have, uh,
for binary rewards, Bernoulli's a really nice choice and betas conjugate.
If you have real-valued rewards you might use a Gaussian.
Um, and then you'd have a, a,
a sort of Gaussian prior depending on whether you know your theta or not.
And in general, uh, like for multinomials you can use Dirichlet distributions.
Depends on what your reward distribution looks
like and then you wanna find a conjugate prior for that distribution.
So there's a lot of different families of parametric distributions
for which you can do this sort of updating. Yeah.
Then the other things we're talking about.
Remind me your name.
About being optimistic.
Yes.
And here is like a uniform distribution for initialization.
Is it better to use something more optimistic?
That is a great question. His question was, uh,
so we talked before about the benefits of optimism.
Here we just used a uniform prior.
Um, and wouldn't be better to use one that is optimistic.
It depends, um, I,
the empirically what, so what is this doing?
I- I'm just gonna, let me hold on that question for a second.
So we can look at sort of what these look like.
So if we did optimism,
we sampled all the actions first and then we sort of
got this interleaving of a1 or, and a2.
In Thompson sampling, we took a3 and we took a1,
a1, a1, a1, and a1, a1, a1, a1.
a1 is optimal in this case.
So by using a uniform prior here, essentially, um,
as soon as you see something that looks pretty good like better than 0.5,
um, you're gonna tend to often sample it a lot more.
Um, so you're sort of exploiting faster to some extent.
Um, you can put priors in there.
The question is often like,
how much to put that in there and if it actually helps.
So one of the cool things with Thompson sampling is it
turns out in terms of sort of the Bayesian regret bounds,
[NOISE] they're as good as the, as the upper confidence bounds,
but empirically often exploring faster is helpful.
So you could put optimism in there,
but it might actually hurt performance because it's gonna force you to take,
like in this case, r1 actually is optimal.
Um, now, you could have imagined maybe we were just lucky there and instead we got an a2.
In that case, you'd want something to help you eventually take a1.
Now note, we, we will still take a2 likely at some point
because there still will be a probability under that uniform
prior that you'll sample like 0.999 and then you'll take a2.
So you can still be sure to start taking other actions even using these uniform priors.
But it's a really good question about sort of,
you know, weird, what information to put in there. Yeah.
Um.
Remind me of your name.
[inaudible].it's stuck, it becomes very hard for a different action to catch up.
So the, uh, so in that sense that chance that is suck is more that important? And then [inaudible].
That is a really good one which is, okay,
so maybe if one arm is really good, then, um,
it becomes really hard for the other arms to, to catch up.
So in this case,
that's true because theta 1,
um, really does have 0.95.
Let's imagine a slightly [NOISE] different case where this was
like 0.7 or something like that.
Then in that case over time it would be likely that
your Beta distribution is going to
converge to around the real distribution of the parameter.
So if you keep sampling theta 1 forever, uh, eventually,
[NOISE] you know, it's gonna sort of collapse towards what the true value is.
Um, and so if the true value isn't very close to one,
there will be some probability you'll sample.
Like so imagine this versus a uniform.
That's not very close to 1.
At some point, there is a non-zero probability that you'd sample something that's higher.
[NOISE] So the beginning matters,
um, but you can outweigh it over time.
Just like what we can with the empirical distributions.
Okay. So if we look at this and we sort of look
at the incurred frequentist regret [NOISE]
which is not the same as Bayesian regret because in
that case we'd have to average over the parameters.
Um, in this case Thompson sampling is doing, uh, a lot better.
So [NOISE] in this case, here,
this would be 0.85 and that'll be 0, 0, 0, 0.
[NOISE] Okay. So in this case,
Thompson sampling would be doing a much better job.
[NOISE] Now, um, Thompson sampling,
uh, actually does achieve the Lai and Robbins lower bound
for the performance of a- an algorithm.
So, um, in terms of its lower bound is similar.
So that's one indication this might be a good algorithm.
But we have, there's a lot of bounds for optimism.
In general, um, the,
the bounds for optimism are better than the bounds for Thompson sampling.
A lot of the Thompson sampling, uh,
bounds end up converting to sort of upper confidence bounds.
A little bit like what was asking.
So if we, um,
if you want to make Thompson sampling have frequentist-like bounds, um,
often we end up sort of making our comfort, our sort
of being more optimistic in terms of Thompson sampling.
Okay, to put those. Um, but empirically Thompson sampling is often great. Yeah.
Can you mix them?
Can you mix Thompson sampling and upper confidence bounds?
Uh, maybe start with some form of upper confidence bound and
use that information to update like your priors on the Thompson sampling?
[NOISE] Um, I, I shouldn't just say,
can you, could you mix them?
Y- You probably could.
I, you could probably do it. I don't know.
So I guess to me one of the,
so maybe you could start with upper confidence bounds and then use Thompson sampling.
Um, to me one of the big benefits of Thompson sampling
empirically is that it is less optimistic than upper confidence bounds.
Upper confidence bounds tend to be too optimistic for too long.
And it's like, "Oh, you know, I ran into the door 30,000 times,
but maybe that 30,001th time I won't."
You know, like for your robot or something like that.
So, um, it often tends to sort of think about the extreme events.
Whereas, if you know that say the world is really Gaussian like the probability that
your robot is going to run into a wall again is still really
high even if it's only run into the wall 10 times.
So maybe you should pick a different path.
So I think often you probably would want to start with Thompson sampling,
but you would like to be robust to your prior.
And so some work that tries to combine these ideas is known as PAC-Bayesian,
where you try to get,
I'll define what PAC is in a second.
But, um, you'll try to get bounds that are kinda frequentist-like,
but also get the best of both worlds.
So you'd like to be like Bayesian if your prior is really good and, um,
and PAC if you like sort of frequentist, if it turns out that your prior's wrong.
Okay. So, um, one of the papers that I
think sort of changed a lot of people's minds about Bayesian and, uh,
bandits and also Thompson sampling being
a good idea was this paper by my colleague Lihong Li and also,
uh, Chapelle where they looked at
contextual bandits and we will hopefully get to this for a little bit on Wednesday.
But the idea in contextual bandits is that you have a state and an action.
So it's a little bit different than the bandits we've seen so far.
Unlike in MDPs your action does not affect the next state.
So for those of you doing the default project,
you're seeing examples of this where how you treat
the current patient doesn't impact the next patient that comes along,
but the patient characteristics can affect which arm is best.
So, so contextual bandits is a very popular and powerful framework.
Um, and so in this case they were looking at news article recommendations and they
were [NOISE] finding Thompson sampling did much better
than upper confidence bounds in a number of other algorithms.
It also can be more robust,
uh, when your outcomes are delayed.
So this happens a lot often in real cases.
You can imagine here you treat a patient,
you're not gonna find out whether or not that toe procedure helps for another six weeks.
But in the meantime other people come in whose toes needs to be treated.
Um, and if you use upper confidence bound
algorithms [NOISE] they tend to be deterministic. Bless you.
And, um, and so you just keep treating
everybody with the same thing until you get the outcome from the first,
whereas Thompson sampling is stochastic.
So you'll be sort of trying out a lot of things.
That's another good reason in practice why Thompson sampling can be helpful.
Okay. So I'm not gonna go through the proof today,
but I'll put some pointers so that, um, the,
the nice thing is that if you look at the Bayesian regret of Thompson sampling,
uh, it's going to have a similar result to what upper confidence bounds has.
So it essentially has,
has the same regar- regrets bounds as UCB, essentially.
I'm being slightly hand-wavey for that,
there's some important subtle details,
but roughly you can show that these also have
good Bayesian regret bounds if your prior's correct.
Um, and so that's sort of again a nice sanity check,
that you kind of get this logarithmic regret growth.
All right. Another framework that I just mentioned sort
of in passing just now is probably approximately correct.
So, these theoretical regret bounds specify how your regret grows over time.
Um, and one thing that's hard to know is whether you're
making a lot of small mistakes or a few big mistakes.
Your regret bounds are cumulative,
so it doesn't allow you to distinguish between those two.
So, you can imagine in the case of patient treatments,
this could be pretty important.
Like are you giving everybody a headache, um,
or are a few patients really,
you know, having really really bad side effects.
So, so regret- cumulative regret,
um, doesn't distinguish between those two,
because if a couple of people have really bad side effects,
that's the same as a lot of people having headaches when you average over those.
Um, and so one idea is to say well,
maybe we just wanna kinda bound the number of non-small errors.
So, we wanna bound the number of people that
experience really bad side effects, for example.
[NOISE] So, Probably Approximately Correct comes up in supervised learning.
In the context of decision-making,
we often define it as follows,
a Probably Approximately Correct algorithm or a PAC state that
the algorithm will choose an action who is- which is epsilon close to optimal,
with probabilities 1 - delta,
on all but a polynomial number of steps.
So, the probability part comes from here,
so it's not guaranteeing that you will do this,
but with high confidence or probably it will do this.
It's approximately correct because we're only guaranteeing epsilon-optimality.
And the important aspect is it's- only it does- makes these sort of, um,
makes mistakes that might be bigger than epsilon, so the number of, you know,
patients we might treat that have really,
really bad side effects is gonna be no more than a polynomial function.
Where the polynomial function is a function of the parameters of your domain.
So, things like the number of actions you have, epsilon and delta.
And you should be able to compute this in advance too.
So, you should be able to compute how many mistakes you might make.
Um, and one of the cool things is that you g- a lot of the PAC algorithms,
um, algorithms that are PAC are based on optimism or Thompson sampling.
Now, PAC for bandits is a much less common,
uh, approach than when we go to MDPs.
In bandits, most of the time we look at regret.
But for when we look at Markov Decision Processes,
PAC is more popular, and,
and we'll see one of the reasons for that probably later,
or feel free to ask me about it if we don't get to it today.
Okay. So, what would PAC look like in our little example we had here before?
So, let's use O to denote optimism,
TS to denote Thompson sampling,
and within epsilon, um,
means that the action that we select is within epsilon of the optimal action.
So, its value is epsilon close to the optimal action.
So, I've written down the regret in this case.
Um, here what we'd have is that the- for, um,
optimism, the first action that we pull is a1, so, um,
it's within epsilon, yes,
because we're close to the optimal action, a2,
um, has a mean of 0.9,
so that's within 0.05 of 0.95, so this is yes.
Action a3 is 0.1,
so it's not within epsilon of the optimal action,
so this is no, and so forth.
So, this essentially allows the algorithm to be taking
either a1 or action a2 under this definition of epsilon.
Because I don't care whether or not you're taking action a1 or a2,
both of them are really pretty good;
both of them are within 0.05 of each other, I mean, that's fine.
Um, but you- we don't want you to take action 3 very much because it's much worse.
Um, and then in this case,
uh, this one would say,
this is not within epsilon because the first action we
take is bad but then all the rest are good.
And what a PAC approach would do would they'd be counting
all these- counting the, the mistakes.
Okay. So, we just talked about for bandits,
um, different sorts of frameworks and criterias.
We talked about regret,
Bayesian regret and PAC, um,
[NOISE] and we talked about two styles of approaches,
either optimism or Thompson sampling.
And what we can see now is that Markov decision processes
have many of the same sorts of ideas being applicable,
but it also does get a lot more challenging.
So, in particular what we're gonna talk about right now
is we're gonna talk about tabular MDPs.
And it turns out that even from with tabular MDPs that things are a lot more subtle.
Um, so, how does this work?
The, the regret- the Bayesian regret in PAC is all gonna be applicable,
so is optimism, and so is probability matching.
So, let's start with thinking about optimism under uncertainty.
First, let's think about just doing optimistic initialization.
So, in this case, imagine that we just initialize all of our queue state actions,
um, to some value.
So, let's imagine that we initialize them to rmax divided by 1 - gamma,
where rmax is the highest reward you could see in any state-action pair.
Let's just take one minute, why is that value guaranteed to be optimistic?
Anybody wanna answer why that's guaranteed to be optimistic?
Right.
Yeah.
It's higher than like,
the possible, um, total value,
because like we've shown a couple of times
that rmax one line of scandal would be the highest value,
but it goes on a bit [inaudible].
That's right. Yeah, so what said is correct.
Um, we've shown that for a discounted Markov decision process
that the highest value you could get is rmax divided by 1 - gamma.
At best all of your states have that,
or else some of them might not,
so this is guaranteed to be an optimistic value.
So, you could start off and if you've, uh,
initialized all of your state action values to be rmax divided by 1 - gamma.
And then you can do Monte-Carlo,
you can do Q-learning, you can do Sarsa.
Um, and you could incrementally update using that.
And this can be very helpful,
it can sort of encourage systematic exploration of states and actions,
because essentially you're pretending that everything in the world is really awesome,
um, until proven otherwise.
So, on the downside,
unfortunately if you do this in general there's no guarantees on performance,
um, even though it's often empirically better.
So, even though this really is,
um, you- you know,
an upper bound, this is optimistic.
Um, a key issue is how quickly you're updating from those optimistic values.
So, as an early result in this case,
Even-Dar and Mansour in 2002 proved that,
if you run Q-learning with learning rates- this should say alpha-i.
So if you, uh, run Q-learning with particular alpha rates, um,
alpha-i on each time step i,
and you initialize the value of a state,
so this is the very beginning, um,
to be rmax divide by 1 - gamma times the product of those learning rates,
and t is the number of samples you need to learn optimal Q,
then greedy-only Q-learning is PAC with that initialization.
So, I just wanna highlight something here which is this part.
So, notice this is way, way,
way larger than just rmax over 1 - gamma,
because this is a product of all your learning rates.
Okay, so, this could be really enormous,
like you'd imagine that, um,
imagine that alpha = 0.1 for all time steps,
then what you have here is you have 1 over 0.1 to the t,
which is approximately- it would just equal to 10 to the t [NOISE].
So, this is like exponential in the number of time steps you're gonna make decisions.
It's incredibly optimistic.
Um, it turns out this is sufficient to be PAC,
but it's also not very good.
Um, uh, it's, it's very, very extremely large.
Okay? Um, now, there's been some really cool work
by Chi Jin and some others over at Berkeley that showed that,
um, if you use a less optimistic initialization,
um, that's strongly related to upper confidence bounds,
um, and you were careful about your learning rates,
so you have to change your learning rates,
but if you're careful about your learning rates,
they proved that, um, model-free Q-learning could also be PAC.
And this was a pretty big deal recently because almost
all of the work that's been going on has been in the model-based setting.
So this just came out in NeurIPS, uh,
about two months ago, um,
and so they- oh sorry, not PAC.
They, they showed the regret bounds.
Um, they're not optimal regret bounds, but they're good.
So, um, they're, they're not tight yet but, uh,
it shows that model-free algorithms can do pretty well.
[NOISE]
Okay. So what about model-based approaches?
And the model-based approaches for
MDPs are the ones where we really have the best bounds right now.
So there's a couple of main ideas or a couple different procedures we could go with.
One is that, you can be really,
really optimistic in all your estimates,
until you're confident that your empirical estimates of
your dynamics and reward model are close to the true dynamics in reward model parameters.
So these sort of algorithms proceed as if they say,
the reward for all state action pairs is amazing,
it's rmax divided by 1 - gamma.
And I'm gonna continue to pretend that's true,
until I think I have enough data for
that state action pair that I think that if I did a MLE,
maximum likelihood estimate of those parameters,
they will be close to the true parameters.
So you could say, I'm just going to be incredibly optimistic until I've got enough data.
And then when I've got enough data then I, um,
think I can get a good empirical estimate that is close to the true estimate,
and then I'll use those instead.
So it's almost kinda like a switching point.
You sort of keep, um, you pretend everything's really,
really great until you get enough data,
and then you switch over to the empirical estimate.
So these were some of the earliest ones, um,
that showed that MDPs could be pa- oh,
algorithms for MDPs could be PAC.
This is from 2002.
Uh, but they're also empirically not normally
so good because, um,
you're pretending things are really, really awesome,
even though you might have quite a lot of evidence
for that state-action pair that it's not awesome.
So another approach is to be optimistic given
the information you have. So what do I mean by that?
I mean that as your agent walks around and gathers
observations of the actions and rewards it gets,
it uses that to try to estimate,
um, how good the world could be given that data.
And so one approach to this is to compute confidence sets on dynamics and rewards models.
So we already saw this for bandits,
where we computed upper and lower confidence,
or we could compute upper and lower confidence bounds for the rewards.
Turns out we can also compute confidence sets over the dynamics model.
Or we could just add reward bonuses that depend on the experience or data.
And I'm gonna talk, um,
at least a little bit today before we finish,
about the second thing.
And the reason I'm gonna talk about this particular approach is because when we
start to think about doing this in the function approximation setting,
if the way that your dynamics model is represented is by a deep neural network,
um, then writing down, ah,
uncertainties over that can be really tricky.
And also a lot of the progress in
deep neural networks for RL focus on model-free approaches.
And if we have reward bonuses,
then we can easily extend that to the model-free case.
Um, and empirically these ones generally do
pretty much as well as if we use explicit confidence sets.
So I'm just gonna explain how the model-based confidence,
model-based interval estimation with exploration bonus works.
Um, so it's gonna assume that we're given an epsilon delta and some constant m. Okay.
And then what we're gonna do is we're gonna initialize some counts.
[NOISE] So this is just
gonna keep track of the number of times we've seen a state-action pair.
So we're gonna do this for all s and for all a.
We're also gonna keep track of the number of times we've
seen an actio- state-action, next state pair.
0 for all s, for all a, for all s prime.
And we're also going to keep track of the total sum of
rewards we've gotten from any state and action pair.
So we're gonna say rc of s, a = 0 for
all s. So essentially we are gonna keep track of the times that we've been in any state,
taking any action and went to any next state,
and what the sum of rewards are for when we've done that.
And then we're going to define a beta parameter.
Okay, I'm going to double-check, I get the-
Yeah.
Okay. All right. So beta is gonna be
a parameter that we're gonna use to define our reward bonuses.
Okay, it's 1 over 1 - gamma,
2 log the number of states,
number of actions, 2 times m, m is an input parameter divided by delta.
Okay. And then- yeah,
I think that's all I need here.
Now I'm gonna say t = 0.
We're going to initialize our state.
And to start, we can just say Qt of s, a = 1 divided by 1 - gamma.
And this assumes that all of our rewards are bounded between 0 and 1.
So they're bounded rewards.
Okay, so we start off when we initialize all our accounts to 0,
we said we haven't observe- observed any rewards yet,
and we pretend that the world is awesome,
and that our Q value is the highest it could possibly be in every state-action pair.
Um, here r-max = 1.
So r-max is going to be equal to 1
because our rewards are bounded between 0 and 1.
So what we do then is we take an action in the current state,
given our- let's do tildes given our Q function.
So getting, which is going to break ties randomly.
And then we're going to observe the reward and observe the next state.
And then we just update our counts.
So we update our counts for that particular state action pair.
We update our counts for s, a, s prime, s, a,
s prime, for the number of
times we've been in that state taking that action and went to that particular next state.
And then we update our rewards for that state-action pair.
It is equal to the previous rewards for that state action pair plus r_t.
And then what we're gonna do is we're gonna use, um,
those empirical counts to define
an empirical transition model and empirical reward model.
So our reward model is going to just be the MLE reward model,
which is just gonna be rc for s, a divide-
times- divided by the number of times we've been in that state-action pair.
That's just the average reward for that state-action pair.
And then our transition model is also just going to be the number of times a,
s prime divided by the number of times you've been in that state-action pair.
We're just gonna define our empirical transition model and our empirical reward model.
And it doesn't matter how we initialize things that we haven't seen at all.
But you can treat them as uniform.
Okay. So we're gonna do this for all s, a.
And then we're gonna compute some new Q functions.
Okay. And we're gonna compute some new Q functions where we do this.
Where we take our empirical models and we also add in
a reward bonus term that depends on
beta and the number of times we've tried that state action pair.
And we can do value iteration.
That's what I'm doing here. But you could solve it however you'd like.
But the main idea here is that we're gonna use our empirical estimates
of the reward model and the transition model by just averaging our counts,
or averaging the rewards we've gotten for that state-action pair.
And then we're gonna add in this as a reward bonus.
And note at the beginning of this reward bonus can be,
like, it can be infinity,
so you can- because if we have no counts for that,
so then we can just initialize for,
for any Q s,a.
So for all s, a such that nsa of s,a = 0.
You can just set this to be Q-max.
So to deal with if you haven't sampled that state-action pair yet.
So that means anything for which you haven't
sampled it yet is gonna look maximally awesome.
And anything else is going to be a combination of its empirical average parameters,
plus a reward bonus.
And that reward bonus is gonna get smaller as we have more data.
So I'll put this on here where it will be neater.
Um, so this is the reward bonus.
And what you can see here is that over time that's going to shrink.
Over time, um, you're going to get closer and closer to
using the empirical estimates for a particular state action pair.
But for state action pairs you haven't tried very much,
there's going to be a large reward bonus.
So the- the cool thing about this is that,
um, we can think about whether it's PAC.
So I'll just take one more minute,
which is in an RL case, ah,
an algorithm is PAC if on all but N time steps,
the action selected is epsilon-close to the optimal action,
where N is a polynomial function of these things.
The number of states,
number of actions, gamma,
epsilon, and delta, this is not true for all algorithms.
Greedy is not PAC.
Greedy can be exponential.
Um, we might talk about that on Wednesday. So no.
And the nice thing is that the MBIE-EB algorithm I just showed you is PAC.
So what does it PAC in?
It means that on all,
but this number of time-steps,
well I'll just circle it.
So this is sort of a large ugly expression,
but it is polynomial in the number of states and actions.
It's also a function of the discount factor and the epsilon.
In general, if you want to be closer to optimal,
it's gonna take you more data to ensure that you're close to optimal.
So it's inversely dependent on epsilon,
ah, it's polynomially dependent on,
ah, state and actions.
And this says on all but,
ah, this many time steps,
your algorithm is gonna be taking actions that are close to
optimal. So this is pretty cool.
It says like by just using these average estimates plus tacking on a bonus term,
and then computing the Q functions, um,
then you can actually act really well on all time-steps except for a polynomial number.
Okay. And then, um,
I put in here this are theoretical of that,
and I'll just say briefly that on some sort of hard to construct,
sort of simple toy domains.
These type of algorithms do much better even
than some other ones that are provably efficient.
And they can do much, much better than things like greedy.
So algorithms like MBIE-EB,
MBIE is a related one that uses confidence sets, um,
it can do much, much better than this is sort of be,
be optimistic until confident.
And these ones are generally much better than greedy.
So these types of optimistic algorithms can empirically be much better,
as well being provably better.
And on Wednesday we'll start to talk about how to
combine them with generalization. Thanks.
 All right, we're gonna go ahead and get started.
Um, I want to start the class with some stuff about some logistics,
um, as well as sort of to address some questions, um,
that have come up on Piazza about the grades on the midterm and some people have
concerns about what that might mean for their grades on the final class.
Um, it was interesting to go back and compare
the means and the distributions on the midterm last year, it's near identical.
Um, so last year the mean was about 69%,
this year it was about 71%.
Um, and you see pretty similar distributions.
Uh, the one on the top is our- Oh,
no, the one on the bottom is ours.
So you can see this is 2019,
this is 2018. [NOISE] Okay.
So they look pretty similar distributions.
Um, we don't do an official curve for the class.
If anybody's getting over 90%,
I always consider, even if everybody gets over 90%,
that that means that those people all
understand the material well enough to deserve an A.
Um, and then if we have really, um,
abnormal distributions that sometimes we curve below that.
But just to give you a sense, um, last year,
about 42% of people got an A in the class.
So for those of you that are concerned about your midterm performance and concerned about
your final grade and whether it's still possible to do
well in the class, it definitely is.
So, so does anybody have any questions about the midterm?
I know we've had some regrade- regrade requests
and we're going through those as quickly as we can. Yeah.
Is there any [inaudible] I'm supposed [NOISE] I'm very curious [inaudible]
but is there any distribution available per, [NOISE] per question?
Cause like- I feel like, for example, for me,
I just ran out of time on the last question and I wonder if that's.
Yes, we have that information.
Um, I'll double-check with the TAs that there's
no reason we shouldn't release that, I don't think there is.
So, um, we, Gradescope gives us
full distributions for all the questions. So we can release that.
Um, a lot of people ran out of time,
uh, the last problem was definitely the hardest.
So that was where we saw
the biggest variation and so that's where we
tried to be particularly careful on that rubric.
Um, and we very much tried to make sure that if you're
doing algebraic mistakes throughout the exam,
that that was worth very little and we were focusing on the conceptual understanding.
Any other questions about the midterm?
So I'll just write down per- per problem breakdown.
Basically, when we're going through it,
we try to look at any problem that had really high variance,
um, and then step through the rubric again to make sure that we're being fair.
Oh, one other thing I- which is we're gonna
continue to accept regrade requests for the midterm through Friday.
And after that, it will be closed.
[NOISE] Okay.
So that's the midterm.
Um, hopefully, that helps, sort of,
quell some concerns from at least some of the people in my class.
The other thing that I wanted to bring up right now is, um, the quiz.
So the quiz is gonna be in about two weeks,
a little less than two weeks.
It's a weird format.
We do this for a reason. Um, I think one of the big tensions in classes that have
big final projects is whether to do a big final project and a big final exam,
uh, which I think is a lot of [LAUGHTER] a lot for students to do both on,
um, and otherwise why go to class after the midterm?
[LAUGHTER] I mean, now why, why,
you know, how do we, uh,
make sure that there's a reason
to learn about the material in the second half of the course
which we do think is valuable and particularly
is often covering [NOISE] more important recent topics,
um, but without doing a really high-stakes large exam.
So I, I talked to a number of people in the sort of teaching,
uh, the teaching center here called VPTL.
Um, and the idea that we came up with is to do a low-stakes quiz,
and the idea is that it's fun.
Um, and I have heard from multiple people that this
is actually true, that's the design of it.
The design is it's gonna be a two-part quiz. It's multiple choice.
It's all supposed to be about, sort of, high-level conceptual questions.
We'll release last year's,
so you guys can see an exa, uh, example.
And so the idea is you do it in two parts.
You first do it individually,
that takes around 45 minutes,
and then you'll be paired up with random groups and you will,
um, have to do one joint quiz.
And your grade will be composed both of your individual part and your group part,
but you can only do better on your group part.
So if your group does worse,
then you're gonna just get the same as your individual grade.
So the reason that we do this is that, um,
for the group, then it's a scratch off exam.
So you scratch off answers as you de- decide on them as a group,
and the point is to- that you should be able to articulate why you
believe some of these answers are true or false and convince your classmates,
and in doing so, that can be a really useful way
to think about really knowing the material well,
um, and also hearing the perspectives of other people.
So that's how the last quiz goes.
Um, [NOISE] again last year everybody- there was
some concern about it before on Piazza, people were concerned.
There are certain game theoretic aspects that can come up.
Um, we carefully design this so that it's a very small part of your grade.
Um, again you could only do better with the group than you can on your individual,
so it's carefully constructed.
And empirically when we did this,
there was lots of laughter and lots of people seem to really enjoy this aspect,
and we've thought about whether we'll do it in multiple parts of the class.
But it's different, almost nobody has ever done an exam like this.
So does anybody have any questions about that?
It's about 5% of your grade. Yeah.
Uh, I remember you saying something earlier on in the course [NOISE] that you guys
have already decided the teams or will have decided the teams, is that true?
Yeah. So we haven't already decided that the question, um,
was how are the teams assigned and have we already decided them,
we'll do that by random assignment.
It'll depend also on which SCPD students are taking the exam on campus or not, uh,
but we'll release this a few days before, um,
and you'll be randomly allocated to
a team and then you'll just sit with that team for that part of the exam.
[NOISE] Anybody have any other questions about that?
And we'll- we'll release the, the sample one.
It will cover all of the course.
So it'll be more heavily weighted to stuff that's happened since the midterm,
but anything from the whole course will be game,
and the idea is that someone who has been attending lectures, um, or,
or watching lectures online,
um, should have to study for,
you know, on the order of a few hours and then be pretty well prepared for the exam.
You'll also be able to bring, uh, a cheat sheet,
just like what you did for the midterm.
[NOISE] Any other questions about that? Yeah.
And I feel like you probably mentioned this,
but I'm just missing which part of the quiz is individual versus with other people?
Do we sit down individually and then after like 20 minutes go be with other people?
Yes. So, um, that was a good one.
So how does- what is this individual group thing?
So the idea is that you come in,
everybody gets an exam,
you work on that exam, probably from around 45 minutes.
Um, you hand it in when you're done,
then when everybody- when that part of the,
the class is done, most people finish early,
it depends, um, then you as a group get a new exam.
And you do exactly the same exam as before,
but you just have to jointly agree on the answers.
And you scratch it off so we can see how many- it's,
uh- how many you have to scratch off until you got the right answer.
But essentially, we can just see whether or not you got
the answer on the first time or, or it took more than one.
Any other questions about the quiz?
If you have any concerns about that,
just write, um, email us on Piazza.
I'll just say briefly there- about what
was the one of the concerns that came up last year.
Um, the concern was game theory.
So you always get the max of, uh,
your score versus the, the group score.
So what people said is well what you should do is you should
answer the best you can on your individual part because that's worth the most credits,
and then on the group part,
you should- if you were torn between two,
you should get people to agree on the second answer to hedge your bets.
Um, [NOISE] you can do that if you want to [LAUGHTER] or try to,
your or your group members can outweigh you.
Again, the group part last year was about 0.5%, so it's very small.
Um, so there, there is that possibility- you would only want to do
that if you were genuinely really torn between two options, um, and again,
there's only one right answer, so I,
I think that we've observed in practice,
there was very little need to do game theoretic analysis of this.
[NOISE] But, no.
Again, it's- we're always welcome to hear how people interpret these things.
All right.
Any other questions about the quiz or logistics right now?
We also- most of you who have already turned in your, uh,
m- milestone for the project,
we'll be giving feedback on those over
the next few days for those of you that are not doing the default.
Okay. All right. So I put this up before.
This is for everybody that didn't see it before,
the grade distribution is basically identical.
So today, we're gonna do the last part on fast learning.
Um, this is, uh,
a really big topic,
there's tons of work on it.
Um, well- we'll spend some more time on it today,
and then on Monday, Chelsea Finn,
who's, uh, just finished her P- PhD at Berkeley and
she'll be joining the faculty here in the summer, um,
will come and talk about meta-learning,
which is also a really exciting area,
and she'll be talking about meta-learning for reinforcement learning,
where meta-learning is relevant to, sort of,
multitask or transfer learning tasks.
[NOISE] So just to
go refresh our minds about what we're talking about in terms of this fast learning,
we're thinking about cases where data matters.
So things like healthcare,
and education, and customers.
Um, I was getting an invite to talk at Pinterest on Monday,
they definitely care about these types of ideas as well.
Um, and we've been talking about
two different settings: Bandits and Markov decision processes,
as well as frameworks for formerly understanding whether an algorithm is
good or whether it is fast in terms of the amount of data it needs.
And I'll note there that we haven't talked much about
computational complexity for this part of the course,
but there are similar, um,
some of these frameworks can even easily be
extended to talk about polynomial sample complexity.
So often, you can extend these frameworks to also account
for computational complexity requirements.
Okay. So let's continue with Markov decision processes.
What we started seeing last time is that we built up
sort of this expertise on bandits so far,
of thinking of a couple of the main ways we evaluate whether or not
a bandit algorithm is good and approaches to try to achieve that.
So we talked about mathematical regret,
which was the difference between how well we could've
acted and how well we did act in bandits,
and, um, a lot of the work in bandits focuses on regret.
We also talked about two different types of techniques for trying to achieve low regret,
which was optimism under uncertainty,
and then also Thompson sampling.
So trying to be Bayesian and explicitly represent
posterior over what you think might happen when you pull an arm,
or take an action, and using that sort of information.
[NOISE] Then last time we started talking about Markov decision processes
where I argued that very similar ideas are important here,
but- but the problem is a lot more challenging in many ways.
I- and we were talking a little bit about probably approximately correct.
So, in particular, we were talking, uh,
a bit about model-based interval estimation,
which I mentioned was a probably approximately correct algorithm.
And so just to remind ourselves, what did PAC mean?
[NOISE].
And some of you have seen this probably in machine learning.
So probably [NOISE] approximately correct
[NOISE].
Probably approximately correct RL algorithm is one that given an input, epsilon and delta.
So epsilon is gonna specify sort of how good close to optimal we want to be,
and delta's gonna specify with what probability we're gonna want this to occur.
Um, with input epsilon and delta on all but N steps [NOISE].
Our algorithm will select
an action where there,
the Q value of that action,
the true optimal Q value is greater than or equal to,
I'll write down it as V. The best possible you could have for that state,
minus epsilon [NOISE] with
probability at least 1 - delta, and throughout
today I'm going to be a little bit loose about constants,
sometimes this will be 1 - 2 delta,
sometimes there might be a little constant in front of here,
sometimes there might be a little constant in front of there.
I'll put one here just so you can keep that in mind.
There might be small constants there.
Those are just, there might be two or four.
Um, but the important thing is that,
that you're close- very close to optimal,
except for maybe a constant factor away, um,
where N is a polynomial function
of S size of your state space size of your action space gamma,
um, epsilon, delta.
Yeah.
1 over epsilon [inaudible]
1 over epsilon, yes great.
Question was good is, um, are these going to depend on
epsilon or delta or 1 over epsilon or 1 over delta?
Yes, in- inside of all the expressions they'll
end up being 1 over epsilon and 1 over delta.
So you could equally write this as this.
[NOISE] Because essentially N is going to be larger,
if you want to be more accurate.
So that's going to, um, as epsilon gets smaller,
you're going to need more data to be more accurate.
And if you want to be more sure, you're going to be accurate,
you're going to also scale up with that delta.
Okay, and I just want to before we kind of continue further.
I'd like to briefly contrast this with regret
because in the bandit setting we mostly think about regret.
But it's nice to think about what the difference is between PAC and regret,
particularly in online learning.
Meaning like our algorithm's learning online in a MDP and it's learning forever.
Um, which is what regret is telling you.
So regret is saying [NOISE],
Is that large enough in the back, can you see regret?
Okay. So what regret is saying is let's say we start off at a state S_0.
Regret is saying, what if you did the optimal thing from then on wards.
Like, how great would your life have been.
So if you had won that,
you know, first coloring contest,
and that set you up for Harvard,
and set you up to the Supreme Court, like it's fabulous.
But if instead you didn't enter the, that, um,
coloring contest, and you got down here instead.
So you could have had like a +10 there,
but instead you had a 0.
And you went to a different state, which, all right,
you went to a different state in terms of MDP,
where now you're not the person that won the coloring contest.
And so then, you know, [NOISE] your life trajectory was irreversibly ruined.
Um, in this case,
[LAUGHTER] you are judged with respect to not just the,
the actions but the state distribution you could've got to under the optimal policy.
So you're always being judged with,
what could I have reached if from the very beginning I always made optimal decisions?
I always went into the coloring contest.
I always went to Harvard.
You know, I never went to that Stanford place and,
and you got up to the Supreme Court versus,
um, as soon as you make a different decision you
might end up in a different states distribution.
But you're going to look at these gaps.
Okay. So you're going to be judged by the state distribution you ended up
in and the rewards you got there versus
the state distribution you'd get in under the optimal policy and rewards you get there.
So regret in some ways is a pretty harsh criteria.
Because it's saying like you always have to be judged for,
like, if you'd made optimal decisions forever.
PAC is much more reasonable in certain ways.
PAC says, "I'm judging you under the state distribution you get to under your algorithm."
So because it says,
it will take an action that's close to optimal for the state that you're in.
So what does PAC say?
PAC says, okay you started off here.
You didn't enter the coloring contest.
You went to there. Okay, that's too bad.
Um, given that you could've then,
you know, I don't know, entered the next coloring contest, or you didn't.
And I'm going to be judged by that local gap.
I'm going to always only be judged by how optimal am
I given the distribution of states I'm getting to under my algorithm.
So PAC can give,
have much smaller regret.
I'm sorry, much smaller, sort of, um,
negative, ah, differences compared to regret.
Because imagine you have a really harsh MDP,
and you have to make the first right move and then you go to some wonderful land.
I'll see you always toil about in this horrible gridworld.
Um, so in that case regret would compare you to,
if you'd actually made the right first choice, whereas PAC would say,
okay maybe made a bad first choice,
but like you're making the best of things for where you're at,
and you're kind of be near optimal given this bad state-space you've ended up in.
So in some ways you can think of PAC is kind of making the
most of the circumstances you've got yourself into [LAUGHTER],
whereas regret is always judging you
from if you'd make good decisions from the beginning.
I saw a question back there. Yeah. And everyone please remind me of your names.
I know, I'm trying hard, but I sometimes forget. Yeah.
Episodic MDP's?
Great questions. The question is does this extend to episodic MDPs?
Um, so in episodic MDPs just to recall, um,
those are MDPs where we act for h steps or,
or a finite number of steps and then we reset.
In episodic MDPs, regret and PAC are closer becau se normally,
the PAC guarantees we get in that case.
I'm not going to talk too much about those today but, um,
are going to be with respect to the starting state.
So you're going to look at, like V star of S0 versus Q star of S0,
of like the actions you're taking,
or the policy you're following.
So in those cases they start to be closer,
because you're always being judged from the starting state and you can reset.
But in online, like continual learning,
um, for reinforcement learning they can be quite different.
Because the state distributions could be so different. Yeah.
Could you just explain more about where C1
and C2 come from because I don't see them as any like given parameters.
Oh, yes. I just put up,
question about C1 and C2.
I just, I'm going to be very loose with constants today.
Most of these type of regret guarantees are all about orders of magnitude.
So it's stuff like is N a function of S to the 6 or is it a function of S to the 4.
And we generally don't worry about constants too much.
So I just put these in there to say,
some of the different theoretical bounds will have different constants there.
But for today we're just going to kind of ignore those
but just so that you know that there might be constants there.
This might be 1 - 2 delta,
for example, instead of 1 - delta.
[NOISE] Okay, right so that's one of the differences between regret and PAC.
So let's go back to this algorithm now.
Um, and I'll highlight,
so we'll talk a little bit about generalization later today.
But I, I wanted to go through sort of one of,
how do we start to think about whether an algorithm is PAC or not.
I told you that this algorithm is PAC.
But I wanted to talk some about why it's PAC,
and what it means for an algorithm to be PAC,
and are there general sorts of templates that we can use to show an algorithm is PAC.
All the stuff I'm going to talk about right now,
involves tabular settings where we can write down,
um, the value function as a table.
And later we'll talk some about how these ideas extend.
We're particularly picking this algorithm
which is what's known as a reward bonus algorithm.
So we have this nice little reward bonus here.
Because it's going to be easier to extend to the model-free case.
Now one thing I just want to highlight when we look at the MBIE-EB is that,
if we go back and refresh our memories about this,
what we are doing is we are computing the maximum likelihood estimate,
or otherwise known as just adding up the counts and dividing,
of the empirical estimate of the transition model
and the reward model for every state-action pair.
So we look at how many times we've been in a state-action pair,
which next states we transition to,
and we use that to construct an empirical model.
And we do the same for the reward structure.
And then we want to figure out how to act,
we take those empirical models,
and you can think of this,
this operator here as if we're slightly changing our reward model.
So I put it here as the empirical reward plus this bonus.
But you can alternatively think of this as like an R hat prime which is
equal to R hat of SA plus this bonus term.
[NOISE] So you can think of this as like kind of defining a new MDP.
There's a new MDP where the transition model is
T hat and the reward model is R hat prime.
Which is the empirical reward plus this bonus term.
And it's- it's not a real MDP, but,
but that's an MDP we could solve and try to compute the optimal value for and
that's what we're doing here is we construct this sort of optimistic MDP,
where we're using the empirical transition model.
And then we use a reward model that has
really large bonuses in places we haven't visited very much.
[NOISE]
All right. And- and a key thing we're gonna see shortly
is the critical thing is how optimistic to be.
Um, and there's been tons of work on- on trying to make things more or less optimistic.
And if we have time,
I'll show you some other slides about some recent progress in this field.
Okay. So we talked before about this MBIE-EB PAC.
And then now, let's talk a little bit about sort
of what are the sufficient conditions to make something PAC,
and then, how does MBIE-EB satisfy those form of conditions.
So the conditions that I'm gonna talk about are derived basically from this paper, um,
with slight modifications that paper does not- I'll just write down here,
does not analyze [NOISE] MBIE-EB.
So things would have to be a little bit different.
But from a 30,000 foot perspective, this is basically,
a reasonable way to think about why MBIE is a- MBIE-EB is a PAC algorithm.
[NOISE] Okay.
So let's unplug this
or yeah, oops, I'll put these up on the board,
because I think it's helpful to kind of see all of it at once.
Okay. So what is a sufficient set of conditions to make something PAC?
[NOISE] I know that
I found this paper super helpful when I was starting to do PAC proofs in my PhD.
So- so what's a sufficient [NOISE] conditions for PAC?
And the theory is beautiful.
But even for those of you that aren't interested in theory,
I think looking at this is helpful because it gives one
an intuition about what types of properties do your algorithms need to
have in order to be efficient or wha- what types of
properties are sufficient for your algorithm to be efficient?
Okay. So the first one is optimism.
Again, this is not the only set of conditions that are
sufficient- that are sufficient for something to be PAC but here's a set.
So here's optimism.
Optimism. Okay and optimism simply says that,
the computed value you use.
Okay. So this is for this is s_t, this is a_t.
So this is the actual value we compute
like from MBIE-EB that optimistic value we compute.
So this is the computed value of your algorithm [NOISE].
Has to be greater than or equal to the true optimal value for that state-action pair
[NOISE] minus epsilon on all time steps [NOISE].
Okay. So it says that,
whenever we are doing the MBIE-EB calculation,
when we've taken our empirical models and we add in that reward bonus,
we have to pick a reward bonus so that whatever we compute for
the resulting state-action pair is optimistic minus some epsilon on all time steps.
All of this is only gonna need to hold with
high probability but I'll just write it out like this.
So this is the first condition.
The second condition is a little bit more [NOISE]
subtle but I'll say more in a second about the specific cases for this.
So the second thing is what's known as accuracy.
And I'll write down the accuracy first.
So the first thing says,
you need to be optimistic on all time steps.
The second thing is that, you need to be accurate.
Which means the V_t.
This is again, what the algorithm computes [NOISE].
Your algorithm is gonna compute this.
So it's what MBI computes using your optimistic model needs to
be - V pi t of a weird MDP.
And I'll tell you what that, about that weird MDP in a second.
It is not the MDP
I just said that is the nice optimistic MDP.
It is not the real MDP.
It is an MDP that's sort of in the middle of those.
And this type of trick in RL comes up a lot where we sort of construct,
you've seen probably in several proofs now,
where we add and subtract the same term which is sort of
halfway in between say, two different Markov decision processes.
We're gonna play a similar trick here and we're gonna construct
an MDP that is sort of half optimistic and is half like the real MDP.
Again, this doesn't exist in
the real world we're just gonna use it as a tool for our analysis.
So I'll say what this is shortly.
[NOISE] What there's different ways to define this but,
um, it says that,
something that's gonna be closely related to
both your optimistic [NOISE] MDP and the true MDP [NOISE] that your value.
So this is pi t is
the policy you're actually executing at time step t. The- the value that you
compute has to be close to this sort of weird hybrid MDP [NOISE] within epsilon.
So it has to be pretty close to this other MDP.
And the reason for this is that,
this is we're gonna be able to use this to try to bound
how far away we can be from the real MDP.
So why are we gonna need this?
We're gonna need this because optimism would be easy to
hold by just setting our values super high and never updating.
So that's fine but you need to be able to use the information
you have so that eventually you're gonna be acting near-optimally.
So if something really is bad,
you don't want to be really optimistic forever.
And so the accuracy condition is gonna say,
if we've got sort of enough information about
some state-action pairs or value for some of those needs to be fairly close to the,
to a real value [NOISE].
Okay. And then, the third thing [NOISE] is bounded learning complexity.
Okay.
[NOISE].
And this has two parts.
This says, the total number of updates,
total number of queue updates.
So in MBIE-EB, we would update our state-action values.
And we would rerun that sort of optimistic Q value iteration.
The total number of times we do that has to be,
is gonna be bounded as is,
the number of times [NOISE] we visit an unknown pair,
state-action pair and I'll say more what that is in a second [NOISE].
All right. So we're gonna classify
all state-action pairs and we're in
the tabular settings and this is reasonable for us to do,
um, we're going to end up classifying
every single state-action pair into being either known or unknown.
And we're gonna say the total number of times we visit
an unknown state-action pair both of these have to be
bounded [NOISE] by some function.
It is a function of epsilon and delta.
So it means you can't do an infinite number of queue updates and you can't
visit unknown state-action pairs an infinite number of times, like your algorithm can't.
These are conditions on your algorithm.
Okay. So if you can satisfy all of these, then your algorithm is PAC.
So if 1 through 3 are satisfied [NOISE] then,
it'll be epsilon order epsilon optimal [NOISE] on
all but and I'll write this out here just so you can kind of get
a sense of what these type of bounds can look like and all but N which is equal to order.
This- this bound is sample complexity divided
by epsilon times 1 - gamma squared times some log terms.
So essentially, this is saying that if you can be
optimistic accurate with respect to some weird MDP I haven't told you
about and that if your total number of queue updates and the number of times you visit
unknown state-action pairs which I often haven't told you about
exactly how we define that, if that is bounded then,
you're gonna be PAC.
You're gonna be near optimal on all time steps except for a number that
scales as a function of this which is also generally,
a function of the size of the state space and action space.
This is also function of say [NOISE] at epsilon of 1 - delta.
So this is kind of a template.
So if you can show that your algorithm satisfies these properties,
then you can show that it's PAC.
All right. So how does MBIE-EB satisfy these properties?
Well, the first thing we need to show is that MBIE-EB is optimistic. Yeah question.
Ah, so the first question was for three, part one and part two,
do they both have the same bounds,
or are there just two separate bounds with different magnitudes?
Good question, he's asking about,
do you mean the epsilon there?
The total number of Q-updates and the total number of times you visit other states.
Uh, good question, um, so for part three,
as the total number of Q-updates the number of times you visit state-action pairs, um,
they are going to be very closely-related essentially,
whenever you visited another state-action pair,
then you can do a Q-update.
For one and two epsilons-
are they the same?
Yes, yeah. Good question.
So that's what I thought you're asking.
In one and two, um,
the question was, are epsilon the same?
Yes, they're the same. Epsilon is same through 1, 2 and 3.
So if you're designing your algorithm,
1 and 2 and 3 all have to be the same.
Constants probably don't matter.
It can, you know, be 1 minus.
In some of these cases you can be- have a constant in front of the epsilon.
One just has to be a bit careful.
So for here, we'll just put them like that,
okay? Same epsilon everywhere.
Okay. So let's talk first about why MBIE-EB is optimistic.
Um, let's- actually, can we put this up please?
I think that'll be better.
So I'm gonna just reput up MBIE's,
um, bonus term so that you can, uh, see what that looks like.
Okay. So I think this is gonna go up in just a second,
and then just to remind us what the update was for [inaudible].
So when- what we were doing in MBIE-EB is,
we had a state-action pair.
We had our empirical reward for that state-action pair,
plus our bonus, beta divided by square root of n(s,a).
Is that too small in the back, is that okay?
It's okay? Okay, great.
Um, I see at least one person nodding.
So this is our sort of optimistic reward.
You can call this, like R tilde.
This is our optimistic reward.
Plus sum over s'.
This is our, again, empirical transition model.
S' given as a max over a' of Q tilde of s',a'.
Okay. So this would be a backup we could do.
So it's like a Bellman backup,
with our optimistic reward bonus.
And just to remind ourselves,
beta was still gonna be defined as 1 over 1 - gamma,
square root of one half log 2 S,
A, M divided by delta.
All right. So in this case,
what we wanna be able to show, we don't have to think
about known or unknown state-action pairs yet.
We want to show that this value,
when we compute it,
is an upper bound to the true Q star, up to epsilon.
So we wanna be able to show this first optimism condition.
We want to- what we're trying to argue right now is that,
that beta is sufficiently large as a bonus,
that when we do this procedure we're gonna be optimistic.
Okay. So let's step through it here.
Okay. So how do we show that?
Let's think about a particular state-action pair.
So let's think about one state-action pair.
So s,a and let's think about that we visited some n(s,a) times which is less than m, okay?
So in our algorithm,
we are only gonna update our empirical estimates until we have m samples.
So [NOISE] we only use the first m samples
of s,a to compute
R hat and T, okay?
After that we're gonna throw away our data.
So this is like saying the first, um,
m times you visit this particular state-action pair, um,
you can use that data to try to compute an empirical model,
and use it to compute an empirical model before you have m counts,
but after that you're never gonna update.
I'll- I'll just put a side note in there which is, um,
[NOISE] you might think why [LAUGHTER] why should we do this?
And in particular, uh,
there's a really lovely description of
the whole field of machine learning by Tom Mitchell who's one of the- really,
the founders of machine learning where he argues
the whole discipline in machine learning.
The point is to look at, um,
the foundations of how an agent can learn and also
that we design algorithms that continue to improve with more data.
And this is violating that to some extent,
because this is saying that even if you get
10 trillion examples of that state-action pair,
which surely would make your empirical model better,
we're gonna throw all that data out.
[NOISE] Just to give you a sense of why we do that or why this earlier analysis did that,
we do that for the high probability bound.
The idea is that, um,
the high probability bound is gonna work,
like what we saw for bandits of making
sort of upper confidence bounds and kind of guaranteeing that our estimates,
say if the transition model are close to the true values.
And those bounds all hold with high probability.
And so, who here has seen union bounds in different things?
Okay. A few people, but most people have not.
So union bounds are a way to make sure that if you have a number of different events,
all of which hold with high probability,
that the total of those events all hold with high probability.
And that is essentially why here we only use a finite amount of data.
Intellectually, this is completely unsatisfying [LAUGHTER] um,
because you should clearly be able to use more data and
your algorithm should do better with using more data,
and empirically, we use all the data.
Um, one thing that I find really satisfying for last few years is that, uh,
with my student and Tor Lattimore who's
over- who's one of the authors of the bandit book that we recommend,
um, we showed that you can remove this restriction.
You couldn't just continue to use data forever,
by using smarter things than union bounds.
But regardless, for today, we're gonna- we'll do this.
So we're gonna say- we're only gonna,
um, use up to m samples.
Now, let's think about cases where n(s,a) is less than or equal to m. Okay.
So we have up to m samples,
but in general, you know,
it may be one, it might be two.
Some number that is smaller than m,
which is some constant that we have not specified yet, okay?
So what we're gonna look at is for this state-action pair.
We're gonna look at all of the experiences for that state-action pair.
So let's call X_i,
to be defined to be r_i + gamma,
V star of s_i.
This should look quite like what- these were the targets that we had in TD learning.
Um, this is saying that the reward we got on the ith time,
we sampled s and a,
and the next state we got 2 on the ith time we sampled s and a.
So this is from ith visit to s,a.
This is the next state.
Next state [NOISE] Okay.
So we can define this.
So we can think of each of these are gonna have an expectation of
the true Q star of s,a, okay?
Because I've just defined this.
We don't have to know what V star is right now.
We're just analyzing what would happen with these samples.
So if we define our samples to be r,
the real reward we saw,
the real next state we saw.
On average, this is really just Q star of s,a.
All right. So if this is Q star of s,a,
we can think about how many samples do we need until we
have a good approximation of Q star s,a,
or how far away can our average B,
over the averaged- the real empirical average of X_is versus Q star
[NOISE] And we can do this using Hoeffding or other sorts of deviation bounds.
A little bit like what we saw of our bandits.
So for bandits, we looked at if you have a distribution over rewards,
if you have a finite number of samples of that,
how far can it be away from the true mean reward?
And similarly, here we're gonna say if you have a finite number of
samples of the next state and the reward received,
how far away can we be from the true Q star in this case?
Right. So there's some technical details
here because one has to be a little bit careful about,
um, the fact that the data that we gather depends on the history.
So this is, again,
one of the ch- more challenging things in, um,
Markov decision processes in this sense,
or particularly Markov decision processes in that
the data you gathered depends on your algorithm.
So you're gonna get more samples for state-action pairs that you think are gonna be good,
and less samples for state-action pairs that you think are gonna be bad.
So there's coupling.
The data isn't really IID, um,
across the whole distribution,
but sometimes conditioned on the fact that we're sampling for the state-action pair.
The next state and reward are IID because it's Markov.
[NOISE] So just to
give some of- that's just to say we have to be a little bit careful here,
but we can basically do the pro- use things like Hoeffding to say the probability that,
Q star of s,a,
- 1 over n(s,a),
sum over i = 1 to n(s,a) of X_i,
where X_i is just what we defined up there.
The probability that's greater than or equal to beta over square root of n,s,a,
is gonna be less than or equal to the exponential minus 2 beta squared,
1 minus gamma squared, okay?
This is using like Hoeffding or like a similar type of deviation inequality.
You can also use ones that depend on martingales,
for those of you that have seen some of those before.
Regardless, this basically just allows us to say,
as you get more and more samples for this particular state-action pair,
how far away could you be from optimal if you did know B star?
[NOISE] Now we know what beta is.
I put it up there. So if we plug beta into here.
So if you plug beta in [NOISE] the real value for beta in,
you get that this is gonna be equal to delta divided by 2 size of the state space,
size of the action space, m, okay?
So it just says that this holds with high probability.
That the number- as you have more samples, um,
the probability that you're gonna be far away from the true Q star is small, okay?
All right. So we can put that, like,
substitute this result back in,
and we can say what that therefore means is that,
if we look at the union bound across all of this,
um, well here, let me just write
down one more thing which is what does this X_i actually look like.
So X_i, if you say 1 over n(s,a) of sum over i = 1 to n(s,a) of X_i.
What is that? That's actually just the equation that we had up there.
Okay. So it's very similar to- it's your empirical reward,
plus gamma times T Hat s' s,a.
Almost that equation except for you've got B star here, okay?
So this should look really similar to that Q tilde up there.
We're using the empirical rewards here in the emp- empirical transition.
The difference is that here we're using Q star and up there we're using Q tilde.
All right. So what this means is that if you have a number of samples,
then you can bound the difference
between the thing that we're doing up the- the- this thing and the Q_star.
So let's do R_hat of s,
a plus gamma sum over s_prime transition,
the empirical transition model,
s, a, V_star of s prime,
minus Q_star s, a is greater than or equal to minus beta divided by square root n(s, a).
Okay? And this is gonna hold for all T,
s, and a. All right.
So we've used Hoeffding and now we can relate,
um, the empirical reward,
the empirical transition model,
and if someone gave us the optimal Q to what Q_star is.
And then, now, what we wanna do is compare this to what we're- that equation up here.
So I'll just- all right, this is one.
So that equation one up there is what we're actually doing in MBIE-EB.
We keep doing that over and over again until it convergences.
So we take our empirical transition model,
we take our empirical reward model,
we add in this bonus,
we do value iteration until we converge.
And what we would like to do now is to compare what happens to that quantity,
versus what is Q_star.
And we want to show that that quantity up there is
going to be greater than or equal to Q_star, okay?
So we're gonna do that by induction.
I think I'm gonna- [NOISE].
Okay.
So the proof is by induction.
So what we're gonna do is we're gonna get Q_tilde,
i of s, a be the i-th iteration of value iteration.
So this is using that equation 1.
So equation 1 up there.
And we're gonna let V_tilde_i equal the- for a value s just to be,
if we were to take the max action of that Q_tilde.
Okay? For every state and action pair.
And what we're going to assume is that we initialize optimistically,
and we're gonna initialize Q_tilde of 0 for s,
a equal to 1 over 1 minus gamma,
which by definition is greater than Q_star,
is at least as good as Q_star.
So that's our base case.
And again, what is the- what are we trying to do?
We're trying to show optimism here for MBIE-EB.
We're trying to say that if we do this procedure with MBIE-EB, we'll be optimistic.
We're going to do a proof by induction and this is the base case.
So we start off, we initialize our Q_tilde optimistically,
and now we're gonna, um,
assume that this holds.
So we're gonna assume Q_tilde_i of s,
a is gonna- we're gonna assume that Q_tilde_i of s,
a is greater than equal to Q_star,
of s, a for the previous time-step.
Okay? So we're going to- All right.
So let's write out what Q_i + 1 is going to be.
Q_tilde_i + 1 is going to be equal to for s,
a, is going to be equal to our empirical reward,
plus gamma sum over s_prime,
or empirical transition model,
times our V_tilde of i,
of s_prime, + beta over square root n(s, a).
Okay. That's just the same as equation 1.
So now we're going to say that this,
by definition, is going to be greater than or equal to R_hat of s,
a, + gamma sum over s_prime,
our empirical transition model,
times the true V_star.
Because that's our induction- inductive hypothesis.
We assume that this held on the previous time-point.
Okay. + beta, divided by square root n(s, a)
this is by my inductive hypothesis.
We assume that we knew, we have a base case where this holds.
We're going to assume that, that held on the previous iteration,
and then the last part that we need in here is that if we,
the- this part, we look at this part versus this part.
Okay? So if you rearrange this equation,
then you can see that R_hat of s,
a, plus all this stuff,
is greater than or equal to Q_star(s, a)
minus beta,
square root n(s, a).
Okay? So that means that if we substitute that back in, back in over here,
we can say this is equal to Q_star(s, a),
minus beta over square n(s, a),
plus beta, square root n(s, a).
Should go to Q_star.
Okay. So now we've shown optimism.
So the key idea in that case was to say,
we know that we're getting to- that,
we're going to relate this to what would happen if we had the true Q_star,
we showed that if we know the true Q_star on the next timestep,
then doing this one step backup, um,
we can bound how far away we'd be from Q_star in terms of our function beta,
and then we can do an inductive,
an inductive proof to show that if we were optimistic on the previous timestep,
we can always ensure that hel- held at the beginning,
because we used optimistic initialization.
Then we'll continue to be optimistic for all the,
for the resulting Q- Q_hat, Q_star.
So this proves optimism.
Anybody who may have questions about that proof?
Okay. So that's the proof of optimism.
The other key part, and I won't go through the other part in quite as much detail,
but I'll, I'll talk about it briefly at a high level.
The other really important part is accuracy.
I mean, bounded, I'll keep this up in case anyone's still writing.
Accuracy is really important, um,
and the fact that you will eventually become accurate is important.
So the- the intuition for this part is that, um,
you can think of defining a couple different,
well, you can think of defining things as being known or unknown.
Okay. Somebody want me to keep this up or is everyone finished writing?
Raise your hand if you'd like it up.
Okay. Okay. So in many, er,
PAC proofs for finite state-action pairs,
there's this notion of knownness.
So what does it mean? So known state-action pairs.
Intuitively known state-action pairs are gonna be pairs for which we have
sufficient data that their estimated models are close to the true model.
So the intuition here,
intuition (s, a) where R-hat
with (s, a) and T-hat of (s, a),
s prime given (s, a) are close
to true R (s, a) and T (s, a).
[NOISE] So intuitively, if you get more and more data for a particular state-action pair,
we know from Hoeffding etc.,
that your estimated mean is gonna converge to the true mean,
and your transition model is also gonna converge to the true tradition model.
And what we're doing here is we're sort of drawing a line in the sand and we're saying,
"When are things- when do we have enough data for a
state-action pair that we are satisfied with our empirical estimates?"
Where we can bound how close our empirical estimates are to the true models.
And if things are close enough,
then we know that using those allow us to compute a near-optimal policy.
[NOISE] So if everything is known,
if all (s, a) pairs are known,
then we can show that the V_Pi star under your sort of empirical model.
I'll denote that as hat. So this is like your empirical model - V_Pi star,
and I'll put this under your empirical model for your- of your true model,
um, that this is bounded.
[NOISE] I kinda go
through all the details so that you saw a little bit of this on the midterm.
This is often known as the simulation lemma.
If things are close, like if your models are close,
your transition model is close to
the true transition model and your reward models are close to the true reward model,
then if you use those to compute a value function,
your value functions are also close, which is a really cool idea.
It's basically saying, "You can propagate the errors in
your empirical model into the errors that you get in your value functions,
and the errors you get in your policies."
So you can sort of propagate error.
You can go propagate,
[NOISE] propagate empirical predictive error
[NOISE] to control error.
[NOISE] Bless you.
[NOISE] Bless you. So this is- this is really nice because
you can say like, if you have good predictive error,
then you can end up with small control error.
And the known state-action pairs are just providing
a way to sort of quantify whether- you know,
what level do you need in order to be good enough,
and the good enough you need is gonna depend on what that epsilon is you want.
So one really important idea is to think about these known state-action pairs, um,
and what they allow us to do in terms of defining some alternative MDPs.
So, in this case,
I will erase this.
[NOISE] Just how you-
so you know how to alter some of the parts of the proof,
um, we'll go forward.
I won't go through all of it in detail,
but I'm very happy to talk about it offline.
Um, [NOISE] so we can define two other sorts of MDPs.
We can ide- call an MDP M prime,
which is equal- which is a MDP,
where for this (s,a) pair,
it's R and T. So its transition and its reward
dynamic- its dynamics and its reward model are equal
to the true MDP on (s, a) in known set,
else, it's equal to the M tilde model.
So these are where we sort of start to define these slightly weird MDPs.
So this is a model that is not quite
our M tilde model that we're using to do in that equation one,
which we have these kind of like rewards plus bonuses plus our empirical transition model.
And it's not quite the real model.
It's saying, on things that are known,
we're gonna use the real-world model.
Again, we don't know what any of these are,
these are just tools for analysis.
And then on state-action pairs which are unknown,
we're gonna use this optimistic model.
It defines an MDP.
Um, uh, and the reason that this is useful as we can
end up using it to help figure out how does
the MDP that we have relate to the real MDP on state-action pairs that are known?
Okay? This is what MDP we end up using for this,
and then the other one is a similar one,
which is M-hat prime,
which is an MDP equal to M-hat.
So this is just the, uh, uh,
empirical- empirical estimates on
K and equal to M tilde on all others.
[NOISE] So on the known set.
Okay? So this is a MDP,
where for the known set,
we use our empirical estimates,
not the true- true est- not the true models,
and then we use M tilde on all of these other ones.
So the idea is that we can use these different forms of MDPs and we qua- can
quantify how far off the value is that we get by computing,
uh, using our- our Q tilde versus the value on these other ones,
and we can use it to basically do a series of inequalities to
relate the value we get by executing our policy versus the value we get,
um, in the true world,
and to the optimal policy.
So these are the types of tools that allow us to help prove
accuracy on state-action pairs that we know about. Yeah.
So is that M-hat and an M tilde?
Yes.
Whether.
It's M-hat. So M-hat here is the empirical estimates like the- um, uh,
if you use just, um,
[NOISE] the counts of the rewards and the count of transition model.
M tilde is the empirical estimates plus the bonus.
So the transition model in these two cases would be identical,
but thi- this one would have the reward bonus there.
Okay. So intuitively, this,
uh, allows us to help quantify the accuracy.
Um, [NOISE] the final thing that I guess I just want to mention briefly,
uh, of how these types of proofs tends to work is that
we need this bounded learning complexity.
We need to make sure that we're not gonna continuously update
our q function and we're not gonna continuously run into unknown state-action pairs.
So the last part is to sort of, you know,
how would we prove 3,
which is bounded learning complexity.
[NOISE] And the intuition
for this is a little bit like the general intuition for optimism.
So the intuition for optimism was to say if you assume the world is awesome,
either the world is awesome,
in which case you have low regret.
In the case of PAC, that means,
if you assume the world is awesome and it really is awesome,
that's like not making a mistake.
That's like picking the really- the- the action that you pick is good.
So then you won't suffer,
um, uh, a worse than epsilon decision.
And then we want to be able to say that the times where you
don't make mistakes or where you do make mistakes,
where you pick an action that's bad,
which is less than epsilon close is bounded,
and that's what this is about.
And the key idea here is the pigeonhole principle.
So the idea is that if you think about- you don't have to have episodic MDPs,
but if you ha- um, you can think of dividing your stream of experience into episodes.
And during each, um, sort of episode,
you can think about whether it's, uh,
likely that you're gonna run into an unknown state-action pair.
So you consider what's the probability that we reach an unknown state-action pair?
And remember, an unknown state-action pair here is one that we don't
have good models of like we've only visited it once or something.
So what's the probability we're gonna reach an unknown state-action pair in T steps.
[NOISE] Okay.
So what we can show in this case is that if this is low,
this is small, if small,
that probability is small,
we're being near accurate,
near- we're being near optimal.
So if- if it's
a really small probability that you're gonna reach anything that's unknown,
we can show that on the known state-action pairs you're being near optimal.
So if you're unlikely to reach anything that where you have bad models of it,
you're gonna be near optimal.
If it's large, [NOISE] this can't happen too many times.
So if it's large you're gonna visit it,
it can't happen too many times.
[NOISE].
Because if this is large, that means that you're really
likely to reach an unknown state-action pair.
Remember, I said that for every state-action pair,
you only update it at most m times.
So by the Pigeonhole principle,
this probability cannot stay high for too long.
Essentially, you're gonna have a function of like the number of states,
the number of actions,
and m. It's larger than that.
But this is saying that you need to be able to visit each state-action pair m times.
So that, that goes into our end bound here of the times we're gonna make mistakes.
And we, we're- we might sort of reach things that are unknown.
Um, it might take us more steps and we might make some bad decisions along the way.
But essentially, um, things can only be unknown for m steps for each state-action pair,
which means that our probability here has to be bounded.
So eventually, everything has to be known or you have to be acting near optimally.
And that allows us to show that things have,
uh, that things are PAC.
It has bounded. We're gonna make a bounded number of bad decisions.
So either, we're not going to be reaching
any part of the state-action space which is unknown.
So everything we reach,
we have good models of them and we're using them to make good decisions,
or we are reaching things, and then in that case,
we're getting information because we're getting a new observation of
what it's like to be in that state-action pair and we get only,
something can only continue to be unknown until we get m counts.
Do you mind putting down the thing again?
Okay. So that gives us an overview of why MBIE-EB is PAC as well as sort of why a lot of,
um, the types of proofs that you do to show things are PAC.
And I think the,
the key idea in this is really the sort of notion of optimism, and accuracy,
and the ability to make progress,
ability not to be stuck always wandering,
by decreasing these confidence intervals sufficiently fast.
Okay. So now let's go back to Bayesian-ness.
So that kind of concludes the PAC MDP part for a while.
There's been a lot of exciting recent work in this area.
Um, I guess let me see if I can just really quickly briefly bring this up.
Um, this is a form of presentation.
One of the TAs is giving up at Berkeley today on some of our joint work together.
Just to highlight here, um,
in terms of sort of PAC and regret analysis,
there's been a lot of progress, uh,
on getting better approaches.
Um, so over the last few years,
I mean, some of my grad students, some other really
nice groups who've been doing a lot of work on this,
and we're also now trying to have an analysis that is problem dependent,
which means that if the algorithm has more structure,
then we should need less data in order to learn to make good decisions.
Okay, so now let's be Bayesian and see how being Bayesian helps us.
So we saw Bayesian Bandits and in Bayesian Bandits we said,
we're gonna assume that we have some parametric knowledge
about how the rewards are distributed.
So we're going to think about there being a posterior distribution of rewards,
and we're going to use that to guide exploration.
And we particularly talked about this notion of probability matching,
which is we want to select actions with the probability that they are
optimal and it turns out that Thompson Sampling allowed us to do that.
So in these, sort of, approaches,
it was very helpful if we have conjugate priors,
which allowed us to analytically compute the posterior over the rewards of an arm given
the data we've observed and our previous prior
over the probability of different rewards for that arm.
So we saw as one example of that the Bernoulli,
which means that the reward is just 0, 1 and the beta distribution,
and the beta distribution is one where we can think of
the alphas as being counts of the number of times we've seen a +1,
and beta is the number of times we've seen the arm being
0 and as we get observations of one or either of those outcomes,
then we just update our beta distribution.
So that allowed us to define Thompson Sampling for bandits,
which was this algorithm where we say at each time step,
we first sample a particular reward for each of the different arms,
and then we sample
a reward distribution we compute the expectation
for those reward distributions and we act accordingly.
So we saw this for the toe example,
where we saw that we would sample different rewards, um,
and then use these to act and at least in that example,
we were seeing that we happen to exploit much faster
than what we were seeing with an upper confidence bound approach.
So a very similar thing can be done in the case of MDPs.
So now in being Bayesian for Models-Based RL,
we're going to have a distribution over MDP models.
So what's the difference here?
That means we're going to have both transitions and rewards.
So we're going to have the rewards should look very similar to bandits.
So the rewards, very similar to bandits.
You can also use betas.
If your reward distribution is 0, 1,
you could also use betas and Bernoulli's.
So this is very similar to bandits. T is a bit different.
We're not gonna talk a lot about the different distributions you can use.
But for example if we're in tabular domains,
T can be a multinomial,
and the conjugate prior for multinomial is a Dirichlet.
Its conjugate. So if you want your,
your transition model to be a multinomial,
which is the probability over all the other states and actions,
then a conjugate prior for that is a Dirichlet,
which has a very nice intuitive,
um, description similar to what beta is.
In beta, you could think of this as just
being the number of times you've observed 1 or 0.
If you look at a Dirichlet,
you can think of this as being the number of times you've reached each of the next states,
so S1 S2 S3 S4.
So the Dirichlet distribution would be parameterized
by a vector for one of each of the states.
And again, we can use this sort of posterior to update it to allow us to do exploration.
So in this case,
we're going to sample an MDP from the posterior and then solve it.
So if we look at what the algorithm looks like,
it's going to look very similar to Thompson Sampling for Bandits,
except for now we're going to start off and we have to define,
um, a prior over the dynamics and reward model for every single state-action pair.
So notice this is tabular.
We're assuming that we have a finite set of S and A.
So we can write this down. We can write down,
we have one distribution for
every single state-action pair for both the dynamics and the reward model,
and then what happens is that you sample an MDP from these distributions.
So for every single state-action pair,
you sample the dynamics model and a reward model.
And now you have an MDP,
and so once you have that MDP,
you compute the optimal value for it.
So this is obviously more computationally intensive than what we had for bandits,
but it's certainly a reasonable thing to do.
And then you act optimally with respect to the Q-star you have for that sampled MDP.
So this is known as Thompson Sampling for MDPs.
It also implements probability matching and empirically,
it can often do really well just like
Thompson Sampling did really well often for bandits.
So I think that's probably all I'll say about Thompson Sampling for MDPs.
There's been, uh, a number of different works on this.
Just to highlight some people that do some really nice work on this from Stanford.
Ben Van Roy, Roy's group has a lot of work on this and
sometimes they call it posterior sampling for MDPs.
So people like- some of his former students like Dan Russo and Ian Osband.
Ian's now at Deepmind.
Dan Russo is now at NYU.
Uh, they've done some really nice work on these types of spaces. Yes?
[inaudible] generalized these like non-tabular MDPs?
Yes. Great question.
So the question was whether or not we can generalize this to non-tabular MDPs?
Yes, and I'll talk about that in a second.
But kinda poorly. [LAUGHTER] But yes, that's the goal.
All right, anybody have any other questions about,
sort of, finite state and action scenarios?
Now we are going to talk a little bit about generalization.
Okay. All right. So of course,
everything I've just been saying right now is for finite state and action spaces,
which is not very satisfying because if we think about these types of bounds,
um, I said this was polynomial in the size of the state and action space.
So what if S is infinite?
I mean, it says we can make
an infinite number of mistakes and that seems sort of unfortunate.
Um, so it's not clear that this initial, uh,
framework is is it all helpful when your state space is either
infinite or insanely large like the space of pixels.
Um, though- even though,
the framing of this is really nice,
we'd like to be able to take these types of ideas up to generalization,
but we'd like to figure out how to,
how to use them in a way that can be practical.
Now, when we start to- so this is a very active area of current research,
there has been a lot of different ideas about this for the last few years.
I do want to highlight that on the theory side,
we still have a long way to go.
Uh, as we talked about some with function approximation,
even just function approximation and doing
control like off policy control like Q learning,
we said that we didn't have good asymptotic guarantees for some of the basic algorithms.
Um, so if we don't even have good asymptotic guarantees,
it's unlikely that we would have really nice finite.
These are often known as finite sample guarantees because they
guarantee that the number of mistakes you're gonna make is finite.
So we have relatively little theory
in this case it's something that my group is working on.
There's several other groups that are also working actively on this,
but there's a lot still to be done.
But there has been some really nice empirical results recently.
Okay. So let's think first about generalization and optimism,
like optimism under uncertainty where we're now in a really really large state space.
So let's think about what might need to be modified if everything was extremely large.
If we go back to this algorithm.
Well first of all, we had this,
these sort of counts that
we're keeping track of sort of for every single state-action pair,
what the counts were.
So this isn't going to scale because if
you're in a scenario where your state spaces is pixels,
you may never see this same set of pixels again.
Does anybody have any ideas of how you might extend this to the deep learning setting?
Like what we would like in this case is some way to quantify our uncertainty,
our uncertainty over sort of states and actions.
Um, but we don't wanna do it based on like
raw counts because then everything will just be one forever. Yeah?
Could you use some form of like bounded VFA?
So the suggestion was whether we could do some form of like a,
a VFA for example.
Um, yes, we could imagine trying to use some form of, uh,
some sort of density model or some sort of way to try to either get
an estimate of how much related information have we see,
or how many related states that we'd seen in this area.
[inaudible]
Somewhere like being able to [NOISE] [inaudible] some sort of [inaudible].
Yeah. So [inaudible] to use some sort of embedding, absolutely.
So another thing you could do here too is to
do some sort of- form of compression of the state space.
One of the challenges [NOISE] is the right compression in
the state space depends on the decisions you want to make.
And so generally, it will be non-stationary.
But that's not necessarily bad.
You might just want to go back and forth between those.
So those sort of ideas are great.
In general, we want some way to quantify our uncertainty that is going to have to generalize,
and say similar nearby states.
Um, if we visited that area of the world a lot,
then we should have less of a bonus on that in terms of optimism.
[NOISE] Okay.
So as I said,
the sort of counts of s, a and s,
a, s' are not going to be useful if we're only going to encounter things once.
Um, another thing that I want to highlight is,
the methods that I was talking about before were really model-based.
Lot of the work there is model-based.
In contrast, a lot less of
the things that- it's starting to change recently over the last year,
but in general there's been much less work on
the model-based approaches for deep learning in terms of RL.
And I think that's because the model-free and
the policy search techniques have generally been much better.
In part because the error that you make in
your models accumulates a lot as you do planning.
And so I always remember David Silvers' first talk about this,
or one of his earliest talks about this from, I think,
2014 where he showed this beautiful sort of model-based,
um, uh, simulation, which was horrible for planning.
So the errors really have to be very good,
uh, the- the errors have to be very small,
and it's not clear that the representation you get by
maximizing for predictive loss is always going to be your best for planning.
So we'd really like to be able to take the ideas we saw
before for things like MBIE-EB, um,
and translate them over to the model-free approach, and think about some way to,
uh, to encode uncertainty in the deep neural network setting.
So let's think about doing something like Q-learning, uh, like deep
Q-learning, and in this case- so I've been a little loose here,
you could- this could be a target.
So target, could fix this.
But think about something like sort of the general Q-learning and Q-target, um,
where we use the max of our current function approximation or we
use the max of our current target function approximation.
So one idea for- inspired from MBIE-EB would be simply to include a bonus term.
So again, this could be,
you know, a fixed target.
But the idea here is that when you are updating your parameters,
just put in a bonus term for
that particular state-action pair when you're
doing your Q-learning update or when you're refitting your weights.
Of course, we have to know what that Q bonus would be,
but this would help with the planning aspect.
So now we're in the model-free environment, or model-free setting,
and so we could- [NOISE] when we're doing our sort of Q-learning updates,
let's insert a bonus term.
So that's why I chose MBIE-EB is the algorithm to show you before
because I think that those sort of
reward bonuses are much easier to extend to the model-free case.
Now, of course, the question is what should that reward bonus be?
It's got to reflect some sort of uncertainty about
the future reward from that state-action pair.
And there's a lot of different approaches that have been trying to make progress on this.
So Mark Bellemare, um,
[NOISE] and some of the follow-up papers thought about sort of having a density model,
trying to, er, explicitly estimate
a density model over the states or state-action pairs that you've visited.
Um, other people have done sort of hash based approaches,
which is sort of more similar to the embedding in a way,
try to hash your state space,
and then use those- use counts over that hash state-space,
and then update your hash function over time.
So that's some of the work that's come out of Berkeley.
So there- there's different,
um, different ways to quantify this.
Another thing I want to highlight here is that
these bonus terms are generally computed at the time of visit.
When we looked at MBIE-EB,
you could recompute these later.
So if you- if you're storing things in your episodic replay buffer,
you might want to update those over time,
because those state-actions,
if you now- later sample that reward,
a next state pair from your, uh,
replay buffer, you may want to change that bonus term.
So there's a number of different subtleties when we try to bring us up to deep learning,
but there's been some really encouraging progress.
Um, so let me just- just- so in this case,
what we can see is the initial work from Mark Bellemare's group, um,
where we compare on Montezuma's Revenge,
which is considered one of the hardest Atari games,
so the progress of a standard DQN agent that was using e-greedy,
um, there's a number of different rooms in Montezuma's Revenge.
In this case, you can see after 50 million frames,
um, it was doing incredibly badly.
Don't- it had only been through two of the rooms.
Whereas if you use this sort of exploration bonus,
this one did much, much better,
so just enormously better by being strategic.
Okay? So I think that highlights
the empirical significance of doing this sort of strategic exploration.
Um, let's think briefly about Thompson sampling.
So in this case,
one of the ideas that we did a few years ago is to say,
you could do Thompson sampling both over
your representation and the parameters of your model.
What do I mean by that? I mean that if you have a really large state space,
you can imagine collapsing your states and doing a dynamic state aggregation,
and sampling over possible state aggregations
as a way to sort of do collapsing your state space.
Uh, and- and Thompson sampling can be extended to sampling over representations.
So that works well, but it doesn't scale up fully.
When you want to really scale up,
if you want to be a model-free,
it's a little bit different than what we saw before.
Because before we saw model-based approaches,
and now we're sort of wanting to sample from a posterior over possible Q stars.
And it's not clear how to write that down.
I don't think we've made good progress on that even at the tabular setting.
Uh, but that's what we're trying to do- people who are
trying to do for the deep learning setting.
And there's been a couple of different main approaches.
Uh, one is again sort of from Ian Osband,
who was here for his PhD.
They did bootstrap DQN,
the idea here is that you can use bootstrapping on your samples,
[NOISE] and you can build a number of different agents.
So now you kind of have C,
Q values instead of just one Q value,
and then you can act optimistically with respect to that set.
It's not incredibly effective.
It gives you some performance gain.
Um, another thing that we did, uh,
we sort of- we- someone I was working with before,
I was involved with one of the earlier versions of this,
um, and since then they've been continuing to push it forward.
The idea here is that you kind of fix your embedding,
you do linear regression on top.
And that is super simple,
but if you do Bayesian linear regression,
you get a notion of uncertainty,
and that actually gives a lot of performance gains in a lot of cases.
So you can sort of have like a really little bit of
uncertainty representation on top of a deep neural network.
But this is an active area.
There's a lot of people thinking about this different type of work, uh,
and I think it's going to continue to be
a really big area because we still haven't made sufficient progress,
in how to do exploration plus generalization.
So just to summarize, I know we've done,
um, quite a lot of theory in this section.
The things that you should have- should make sure you are familiar with is to
understand what is the tension between exploration and exploitation in RL?
Why this doesn't arise in on- other types of settings?
You should be able to define these different sorts of criteria for what it
means to be a good algorithm in terms of PAC or regret,
um, and be able to map the sort of algorithms that
we've discussed in detail in class to these different forms of criteria.
So if I say, you know,
is this optimism under uncertainty approach,
is that good for PAC or regret or both?
You should know that it's good for both.
So that's the kind of high level that you should
be able to understand from the things we've been doing,
and just know that there's a lot of
really exciting work that's continuing to go on there,
including defining new metrics of performance.
And next time, we'll hear about meta-learning. Thanks.
 Portion, and then a group portion.
You'll be assigned groups in advance, um,
and there'll be numbers on
the chairs for the room that you're in so you'll know where to go sit.
And the way that it will work is you'll first
do the individual part, you'll turn that in,
and then you'll receive another exam for the group part,
and then go in and discuss your answers,
agree on them, and then scratch that off
to see if you got it right, and then when you're done,
you'll hand that in as your group one.
[NOISE] And, yeah.
Uh, just, logist- logistical question.
So we're splitting into the two groups again without being asked later?
Yes.
Okay.
Great question. [inaudible] asked is whether or not we're splitting into two rooms,
yes, we're again gonna do that,
um, and that will be your assignment.
Um, I think it's likely to be the same as last time,
but I will confirm that.
We might make it slightly different because
some SC- SCPD students will be joining us for this one that didn't before and vice-versa.
So it's possible, particularly,
if you are right on the borderline,
we'll, um, you'll be in a different room this time.
So we'll announce that. Um, [NOISE] just as a reminder, right now,
we're basically done with all of the assignments so this is
a great chance to be focusing on the projects.
Um, you should have been getting feedback about all of those along with, with, um,
with a little bit of information back about your project and the person who graded it,
will have signed that.
So that's a great TA to go ask questions of and their office hours are on Piazza.
Um, but you're welcome to go to any of the office hours to ask about project questions.
[NOISE] And that poster session will be [NOISE] at the time,
the original time announced.
It is sub-optimal but what can you do?
We're gonna meet in the morning on, uh, the last day of finals.
[NOISE] So that was the one that we are assigned.
Anybody else have any other questions about this?
Who here went to Chelsea's talk on Monday?
[NOISE] Okay.
She does, uh, for those of you who did- didn't get to see it,
Chelsea's talk will be online,
it's a really nice talk about meta-reinforcement learning.
It will be covered on the quiz next week,
but pretty light, um,
because you haven't had much exposure to that idea.
So again, just as a recap for the quiz.
The quiz will cover everything, um, in the course.
But things that you didn't have a chance to actually think about, um,
uh, because you didn't get practice on it with an assignment,
uh, we will test more lightly.
It also will be multiple choice.
I highly recommend that you take the quiz from
last year ignoring any topics that we haven't covered,
[NOISE] um, and do that without looking at the answers.
Um, one of the robust finding for
educational research is that
forced recall is incredibly effective at helping people learn.
Forced recall is a nice way for testing [LAUGHTER].
So, um, this is, of course,
in the case where you're not getting assessed by it so you just can use that to,
to check whether or not you understand these things and
look at anythi- re-look up anything [NOISE] that might- you might have questions about.
Any other questions? All right.
So let's go ahead and get started.
Um, we're gonna talk today about batch reinforcement learning,
and in particular, safe batch reinforcement learning,
I'll define that what that is.
Um, this is a topic that I think is extremely important,
we do a lot of work on it in my own lab.
And most of the topics I'm gonna focus on today will be work that came out of,
uh, work that was done by my postdoc,
Phil Thomas, who's now a professor at UMass Amherst,
but the work he did before he worked with me, um,
some of our joint work, and I'll also highlight some other related work.
So let's think about a simple case.
Um, let's think about doing a scientific experiment, um,
where you have a group of people who we're- we're gonna call the A group,
and they get to first do this particular type of fractions problem.
This is a fractions problem for one of our tutoring systems, um, uh,
where people are having to add fractions
together and then reduce the sum to the lowest terms.
And then they have to do something where they do cross multiplication,
um, and then after that, they get an exam.
[NOISE] And, again, an average score of a 95.
And then we have the B group that does the same activities but in the opposite order,
and then they get an average score of 92.
And the question is, for a new student, what should we do?
[NOISE] And what would I- what would- what
additional information might you want to
know in order to be able to answer this question?
So feel free to shout that out.
So what would you need to do for like now, a new student comes along?
[NOISE] Now,
let's imagine our objective is to get a high score on this exam,
for that student to get a high score on this exam.
So what- which sequence of activities would you give to that new student in order
to maximize their probability that they get a good score on this exam,
indicating hopefully that they've learned the material?
So, yeah.
Do you know how big A and B are?
So one great question. [inaudible] to say,
um, how big is this group?
Um, so how large is an A and B?
And you might want to know this for a number of reasons.
For example, if the number of people in group A is 1,
and the number of people in group B is 2,
maybe these are just sort of statistical noise
between these di- distinctions. What else might you want to know?
[NOISE] Um, the variance.
Yes. So I suggest that you might want to know the variance.
That's another thing you might want to know. This is just the mean.
So what is the variance?
What other pieces of information might you want to know? Yeah.
Probably the difference between the median and the mean.
Yeah. So other sort of forms of statistics about,
sort of, you know, the distribution.
I'm- I'm thinking about something also more,
um, in a different direction. Yeah.
Um, the people in the group.
So is that bigger group A is all high school students,
group B is all like lower grade or something like that.
Exactly. So maybe the- maybe group A is kindergartners and maybe group B is,
um, you know, high schoolers and [LAUGHTER],
we're all regressing [LAUGHTER]. So yeah.
So- so who's in these groups?
[NOISE] And in addition to that,
often, you'd want to know who was the new student.
So is the new student then a kindergartner or a high schooler?
Okay. So there's really a lot of additional information that you'd
want to know in order to be able to answer this question precisely,
uh, and it involves a lot of different challenges.
And one of the challenges here is that, um,
if group B is different than group A,
then we have this fundamental issue of, um, censored data.
You don't get to know what would have happened in group A if they had had
the same seq- or in the group B if they'd had
the same sequence of interventions as in group A.
So this is sort of the fundamental challenge that you'll never know
what it would be like if you were at Harvard right now, um,
but, uh, but there's this, uh,
you only get to observe the outcome for the action that's taken.
We've seen that a lot in reinforcement learning,
and that's true in this case too where we have old data, um,
about sequences of decisions,
and so it requires this kind of actual reasoning.
Another thing that it involves is, um, generalization.
So here's a simple example where you can think of it basically as being just two actions.
Each of the two different prob- each of
the two different problems and you can think of this as the reward.
The delayed reward may be of a reward of 0,
reward of 0, and then a reward of the test score.
And so here there's only two actions,
and who knows how big the state space is,
it depends how we'd want to model students.
Um, [NOISE] but we don't want to think about
combinatorially all the different orders of actions.
Um, and even if we're writing down a decision policy,
that might start to be very large very quickly.
And so we're gonna need to be able to do some form of generalization,
either cross states, or actions,
or both so that we don't have to run
a combinatorial number of experiments to
figure out what's most effective for student learning.
[NOISE] So we're gonna talk about
this problem today in the context of batch
reinforcement learning which we can also think of as being offline.
So this has to be offline batch RL,
and this is frequently gonna generally be off policy.
Now, we've seen a lot of off-policy learning before,
Q-learning uses an off-policy algorithm.
Um, but I want to distinguish here that what we're gonna be mostly
focusing on today is the case where someone has already collected the data.
So we already have a prior set of data, um,
and then we're gonna want to use it to make better decisions going forward.
Now, this problem comes up not just, um,
in the case that I just mentioned, but in a huge number of different domains.
You could argue that areas like econom- economics, um,
and statistics, and epidemiology are constantly asking these sort of questions.
Um, it comes up in things like maintenance, you know, um,
what sort of order of, um,
actions do you want to do to make sure that your machines,
your cars run for longest.
Um, it comes up in health care,
like what sort of sequence of activity should we give to
patients in order to maximize their outcomes,
their quality-adjusted life years.
And in many of these cases, it's gonna be state-dependent because what's gonna work
best for patient A is gonna be different than what works best [NOISE] for patient B.
[NOISE] Now, one of the big challenges here too is
that when we think about a lot of the cases where we have this old data,
it's gonna be high-stake scenarios,
which means that whether it's because we have really expensive, you know,
nuclear power equipment which we don't want to go wrong, um,
or we're treating people,
um, for, you know,
really significant diseases, then we wanna
make sure that we make good decisions in the future.
So we might may or may not have a lot of data, um,
but the data that we have is precious and we
wanna make as good decisions as possible from that.
So that means we need to have some form of
confidence in how well it's gonna work going forward.
So we would really like to have some sort of upper and
lower bounds on its performance before we deploy it.
Um, and in general,
we just want good methods to try to estimate, um,
if we do this counterfactual reasoning,
if we think about how well people, or, you know,
how much more healthy people might be if we were to treat them in a different way,
um, how confident can we be before we convince a doctor to go actually deploy this.
So what I'm gonna talk about today is thinking about sort of
this general question of how can we do batch safe reinforcement learning.
Safety can mean a lot of different things.
Um, when I'm talking about safety today,
I'm mostly gonna be thinking about this in terms of this confidence.
This ability to say, um,
before we deploy something,
how good do we know it is.
Um, now, there's different forms of safety.
There's things like safe exploration,
making sure you don't make mistakes online, there's risk sensitivity,
thinking about the fact that,
um, each of us is only gonna experience one outcome,
not the expectation, um,
so we may want to think about the full distribution instead of averaging.
But what we're gonna talk about today is mostly still thinking about
expected outcomes but thinking about being confident in the expected outcomes.
And so, in general,
we would like to really be able to say with high confidence
this new decision policy we're gonna deploy for patients,
or for nuclear power plants,
or for other sorts of high-stakes scenarios, we think it's better.
We think it is better than what we're currently doing. And why might you want this?
You might want to, sort of, to guarantee kind of
monotonic improvement particularly in these high-stakes domains,
which is something that we've seen earlier this quarter.
[NOISE] Okay.
[NOISE]
So let's talk just briefly about some of the notation, some of this will be familiar.
I just wanna make sure that, we're all on board with that,
and then I'll talk about sort of some of the different steps
we might think about to try to create batch,
um, safe reinforcement learning algorithms.
So whereas usually gonna use pi to denote a policy,
um, we're gonna use T or H to denote a trajectory.
I'll often use big D to denote the data that we have access to.
This is like [NOISE] electronic medical records systems,
um, or you know data about power-plants, etc.
And for most of this,
we're gonna assume that we know what the behavior policy is.
So we know what was the mapping of states to actions,
or it could have been histories to actions that was used to gather the data.
[NOISE] Can anybody give me an example where that might not be reasonable,
where we might not know the behavior policy?
[NOISE] [inaudible] actions, and we don't know the questions.
Exactly.
In many, many cases where the data is generated from humans,
um, we will not know what pi_b is.
So when we look at medical health data,
we typically don't know what pi_b is.
So, you know, if this is generated by doctors,
[NOISE] we typically don't know what pi_b will be.
There's obviously guidance, but we don't typically have access to, um,
the exact policy people used and, and if we did,
they probably wouldn't have phrased it as like,
you know, a stochastic process.
[NOISE] But like when someone comes into their office with probability 0.5,
they're gonna treat them with this person 0.5.
They probably think of it, in deterministic terms and they
probably wouldn't think of it in terms of these if-then rules.
So there are many cases where [NOISE] we don't have that.
Um, we've done some work on that recently,
other people have as well.
I might talk about that briefly at the end.
But for most of today, we're gonna assume that we have access to this.
So can someone give me an example,
where it is reasonable to assume that we have pi_b?
[NOISE] Sure.
[NOISE] [inaudible] is based on a [inaudible] set of guidelines or something?
Yeah. So in some cases, you know,
[NOISE] [inaudible] like sometimes you have like an algorithm to make a decision or,
you know, a clear set of guidelines.
Were you gonna say something similar or different?
A little different. Um, so if you have a power plant with maintenance records,
ah, [NOISE] like an established maintenance
plan that has records [NOISE] match the plan then you basically have pi_b.
That's right. Yeah. So in those real cases where you have these fixed protocols.
Another example, I often think about is,
you know when your decisions were made by reinforcement learning [LAUGHTER] agents,
or, or they're made by supervised learning agents.
Um, if you go to a lot of different,
ah, like how, you know, Google serves ads.
We know exactly how it serves ads, it has to all be logged.
And there's a, there's an algorithm that is making that decision.
So in many cases,
[NOISE] we have access to the code that is being used to generate algorithms automa- ah,
generate decisions [NOISE] automatically.
In that case, we can just look it up,
as long as we've saved it.
So, and our objective is usual is to think about how do we get good,
good policies out, and good policies with good values.
Um, when we think about trying to do safe batch reinforcement learning in a setting,
we're gonna be thinking about,
how do we take that old data?
So we're gonna take our data's input and push it through some black box,
and get out a policy that we think is good.
So we're gonna have sort of some algorithm or transformation that
is instead of interacting with the real-world,
and getting to make [NOISE] decisions and choose its data,
it's just taking this fixed data,
and it'll put a policy,
and one thing that we would like is that,
if we feed data into our algorithm,
and our algorithm could be stochastic,
then the value of the policy it outputs.
So we can think A(D). So that's,
you know, this is being our algorithm A, it's gonna output some policy.
That might be a deterministic function, that might be a stochastic function.
Whatever policy it outputs, we want it to be good.
Ideally, at least as good as the behavior policy [NOISE].
So that's what this first equation says,
is it says the value of whatever policy is output,
by our algorithm when we feed in some data set.
We would [NOISE] like it to be as good,
as what [NOISE] the behavior policy that generated that dataset,
um, whatever that value was.
So this is sort of [NOISE] the value of the policy used to generate the data.
Now, we don't normally,
we're not normally given via pi_b, um, directly.
But can anybody give me an example of how we might learn that?
[NOISE] Given a dataset,
which was generated on policy,
from that policy generated, um,
using that policy. Yeah.
Just do like dynamic programming or whatever on that small [inaudible] approximately.
Yeah. So I think, like one thing is that,
you know, you could use that data.
Um, I don't know if you could do dynamic programming,
because you don't necessarily have access to the transition and reward models.
But you could do something like Monte Carlo estimation,
could average the reward, um,
let's assume in the dataset that you can see states, actions and rewards.
Um, so you could certainly just average it,
you know, average over all the trajectories.
So we can get an estimate,
we can estimate p pi_b,
by looking at, let's imagine,
it's an epic- episodic problem.
So you can look at sum over all,
equals 1 to number of trajectories,
of the return for that trajectory. This is a return.
Which is essentially just doing Monte Carlo estimation,
because you know that that data was generated on policy,
[NOISE] and so you can just average.
[NOISE] So that'll give us a way to estimate the pi_b but then we
need some way to estimate V of our algorithm,
outputting D. Um,
and that means we're gonna have to do something off policy,
because in general we're gonna be wanting to find policies that are better than pi_b,
which means that they would have had to be making some different decisions.
And I'll just highlight here that you know,
sometimes you might not just want to be better than the existing behavior policy,
but you might need to be substantially better.
Um, often, if we're thinking about real production systems,
it costs time, and money,
and effort, whenever we want to change protocols.
If you want to get doctors to change the way they're making decisions,
we want to change things in a power plant, there's often overhead.
So often, you might not just need to be
better than the existing sort of state of the art,
you need to be significantly better.
So the same types of ideas we're talking about for relative to sort of the current um,
performance, you can always add a delta to
that because they have to be at least this much better.
And so again, just to sort of summarize,
what does this equation says?
It says, I want to have situations,
where the policy that's output,
when I plug in my dataset,
I want that to be better than my existing policy with high probability.
So delta here, you know,
is gonna be something between 0 and 1.
So now, let's talk about how we might do this.
Um, we're going to start with off-policy policy evaluation.
[NOISE] So the idea in this case,
well, okay, first of all, go through all three of these really briefly.
Um, and then we'll, we'll go step through them more slowly.
So the three things in terms of stuff we
might want to do safe batch reinforcement learning,
and there's tons of variants for each of these depending on the setting we're looking at,
is we need to re do- have to do this off-policy batch policy evaluation.
Which is we need to be able to take our old data,
and then use it to estimate how good an alternative policy would do.
We might want to get confidence bounds over how good that is.
So this could just allow us to get some estimate of V
A(D), or V pi_e.
Pi_e is often used to denote an evaluation policy,
a policy we wanna evaluate.
So the first thing is just doing off-policy policy evaluation.
The second thing is saying how would we know how good that estimate is,
so this is an estimate could be really good, could be bad,
so you might want to have some uncertainty, over this estimate.
So that we can quantify how good or bad it is.
And then finally, we might want to be able to actually,
take like, you know, an argmax over possible policies.
So you might want to be able to do something like argmax,
[NOISE] pi V pi_e, with some confidence bounds.
So in general, you're not gonna just want to be able
to evaluate how good alternative policies will be,
you're gonna wanna figure out a good policy to deploy in the future,
which is gonna require us to do
optimization because we don't normally know what that good policy is yet.
So typically, we're gonna end up evaluating a number of different policies.
So we can think of it as sort of the first part is we're gonna take our historical data,
take a proposed policy,
plug it into some algorithm that we haven't talked about yet,
and get out an estimate of the value of that policy,
and we're gonna talk about how to do important sampling [NOISE] with that.
And then after that, we're gonna go into the high-confidence,
um, policy evaluation and safe policy improvement.
To get confidence bounds,
we're gonna look at Hoeffding.
We've seen Hoeffding before, um,
as something that we looked at when we were starting to talk about exploration.
So when we look at high-confidence and then think back- think back to exploration.
So in exploration, we're often trying to quantify how
uncertain we were about the value of a policy or
its models in order to be optimistic
with respect to how good it could be and use that to drive exploration.
But we also could have computed
confidence bounds that are lower bounds on how good things could be,
and that's gonna be useful here when we try to
figure out how good policies are before we deploy them.
And we'll do Hoeffding inequality for that,
and then finally we're gonna be able to wanna do things like safe policy improvement,
which is can you answer the question of saying,
if someone gives you some data and they say,
"Hey, can you give me a better policy?"
Can one have an algorithm that either gives a better policy that it is
actually better like when you go and deploy it with high probability?
Or can the algorithm also know its limitations and say,
"Nope, there's no- there's no way for me to give you a policy that's better."
So I think it's also really nice to have
algorithms that are aware of their own limitations.
We're doing quite a bit of work on that in my lab right now,
um, so that when people who are using these,
particularly for human in the loop systems, um,
that they can understand if the algorithm is giving out garbage or not.
And so in this case, the idea is that sometimes if you have very little data,
you can't do improvement in an, uh, confident way.
So for that example I showed you before,
we had like two different ways of teaching students,
and someone, you know,
made the good point of saying, how many people are in each of these.
If only one person has tried either of these and someone says,
"Can you definitely tell me what's going to be better
for students in the future?" You should say, "No."
[LAUGHTER] Because there is only one data point,
like there's no way we would have enough data from,
you know, one data point in each group to be able to
confidently say in the future how we should teach students.
So I think that the safe policy improvement needs us to be able to both say
when we can be confident about deploying better policies in the future or when we can't.
So we're gonna look at sort of a- a pipeline for how to answer that sort of question.
All right. So let's first go back,
go and think about off-policy policy evaluation.
So the aim of this,
um, is to get a- an off-policy estimate that is unbiased.
So we want to get sort of a really,
you know, a good estimate of how,
um, how good an alternative decision policy would be.
So we have data right now,
which is sampled, um,
under some policy, let's call it a behavior policy Pi 2.
So we have like this dataset.
This is D, which is giving us through these samples of these trajectories,
and then we want to use them to evaluate how good an alternative policy would be.
And while we could do this for something like Q-learning,
we want to do it with a different method that's gonna allow
us to get better confidence intervals,
um, and where it's going to be an unbiased estimator.
So in Q-learning, if we think back to sort of what Q-learning was doing,
you know, Q-learning is off-policy.
We could do this with Q-learning.
Q-learning is off-policy,
and it- it samples and it bootstraps.
And because it samples and bootstraps,
it can be biased, okay?
And so we're wanna do a different thing right now,
which still allows us to be off-policy,
but in a way that we're not biased,
that our estimator might not systematically be, um,
above or below, particularly because right now we're always gonna have finite data.
We're never gonna be in the asymptotic regime
where we have tons and tons and tons of data,
um, and we can sort of assume this went away.
So again let's think about the return.
G_t is the return.
It's just how much reward we got under a policy, you know,
either over a finite number of steps like for one episode or across all time.
And our policy is just or the value of
a policy again is just the expected discounted reward of that.
So the nice thing is that if Pi 2 is stochastic,
the data that you're using- that you're gathering from your behavior policy,
then you can use it to do off-policy evaluation.
This would have been essential for doing Q-learning too.
And one of the nice things is that because we're
kind of following this Monte Carlo type frame work,
you don't need a model and you don't need to be Markov.
That's really nice because we're gonna end up getting an estimator that is,
um, unbiased and it does not rely on the Markov assumption holding.
And in many cases,
the Markov assumption is not gonna hold,
particularly when we start to think about patient data or other cases
where we just have a set of features that happen to have been recorded in our dataset,
and who knows whether or not that system is Markov or not.
We've certainly seen in some of our projects that it is not.
And that in some of those cases,
if you make the assumption that the world is Markov,
you have really bad estimates of how good,
you know, an alternative way of teaching students might do.
Okay. So why is this a hard problem?
Well, um, it's because we have a distribution mismatch, okay?
So if we look at, um,
imagine we just had a two-state process,
where we thought about, you know,
kind of this is S and this
is- or like we can say this is the probability of your next state S prime,
and I've sort of made it smooth.
We can think of a Gaussian here,
and this is under pi behavior.
Under pi evaluation, it might look different, okay, versus this.
In general, the distribution of returns you're gonna get,
the sequence of the state-action, reward,
next state, next action,
next reward, so this sort of trajectory.
The distribution of trajectory is gonna be different under different policies.
So the distribution of
tau here is not gonna look like the distribution of tau here.
If it looks identical,
what does that say about the value of the two policies?
[inaudible].
Sorry what?
[inaudible].
Exactly. So what you said is correct.
If, um, I- if the distribution of
states and actions that you get under both of these policies are identical,
then the value is identical.
And we saw this idea also in imitation learning when we
are gonna be doing sort of state action,
uh, or- or state feature matching.
Now in this case, we're talking about not just states and actions,
we're talking about full distributions or
full trajectories because we're not making the Markov assumption,
but the idea is exactly the same.
The only way we define the- the value is basically
the probability of a trajectory and the value of that or,
you know, the sum of rewards in that trajectory.
So if a distribution is identical,
then the value is identical.
We don't care if the policies are different
because we already know how to estimate the value.
[NOISE] So the key problem here is that they're gonna look different,
which means that you would have went- done different things under different policy.
So it's like, you know, right now,
maybe you go and visit this part of the state space a lot,
[NOISE] excuse me, [NOISE] and this part infrequently.
And now you're gonna have an alternative policy,
which only goes here infrequently and goes over there a lot.
[NOISE] Excuse me. But thinking about it in this way gives us an idea about how
we can look at our existing data over there and make it look more like that.
Does anybody have any idea of how we could do that?
So if someone gives you a bunch of trajectories, um,
how might you maybe change them so they look like the distribution you care about? Yeah?
Importance sampling.
Right. So we can do importance sampling here, okay?
So let's just review and refresh importance sampling.
So the idea is that for any distribution,
um, we can reweigh them to get an unbiased sample, okay?
So let's imagine that we have data generated from, um,
or we want data generated from some distribution q,
we wanna estimate f(x), okay?
So we'd have- wanna get f(x) under the probability distribution q(x).
So we can multiply and divide by the same thing,
let's incorporate another distribution.
It's just a different distribution over x times q(x), f(x), dx, okay?
So we can just rewrite this as being equal to integral
over x probability of x times this quantity, which is q(x),
divided by p(x) times f(x),
and let's imagine that we actually have data from q(x).
So we want data from q(x) but we have data from p(x).
So we can approximate this by 1 over n, sum over i,
q(xi) divided by p(xi) f(xi),
where xi is sampled from p(x).
I remember when I first learned about this a number of years ago and I thought it was
a really lovely insight just to say we're just gonna reweight our data.
And so we're gonna focus on, um,
the data points that come from, you know,
that- that are ones that we would sample under the distribution we care about.
We're just gonna reweigh them so they look like they're having the same probability,
um, that they would under- under q(x).
In our case, under our desired policy.
Okay. So importance sampling works for any distribution mismatch.
If you have data from one distribution you wish you had it from another,
um, those can come up in things like physics.
Often you have really rare events like Higgs bosons.
And in those cases, you might, um,
there are different scenarios where you could reweigh things,
um, so you can get an estimate of the true- the true expectation.
Okay. This is for just generic distributions.
Let's remind ourselves how this works for- for the reinforcement learning setting.
So again, we're gonna have our episodes.
We can call them h, we can call them tau,
which is a sequence of states, actions, and rewards.
And then if we wanna do importance sampling or let me just write this out.
So, um, in this case,
we're gonna wanna get something like p of hj under our desired policy, okay?
So what does that gonna be?
That's gonna be our probability of our initial state.
Let's assume that's identical no matter what policy that you're in,
and then we're gonna have the probability of taking a particular action,
given we're in that state,
times the probability that we go to
a next state and the probability of the reward we saw.
And we can sum that over j = 1 up to n - 1 or lj - 1.
So we just repeatedly look at what was the probability we pick the action,
the transition model, and the reward model.
So that's how what we have for,
um, the probability of a history.
And then if we wanna do this for importance sampling,
so what we want is we wanna have, um,
the probability of this history, um,
we need to be able to compute this ratio of q(x) divided by p(x),
which for us is gonna be the probability of a history j under
or the evaluation policy divided by
the probability of a history j under the behavior policy.
So- and we wanna do this and we're hoping that everything is gonna
cancel because we don't have access to the dynamics model or the reward model.
So unfortunately, just like what we saw in some of the policy gradient work, it will.
So if we have probability of hj divided by pi b,
we're gonna have again the initial state distribution,
which will be the same in both cases,
and then we have this ratio of probabilities,
the probability of aj divided by sj.
And this is under pi e,
probability of aj divided, um,
given sj for pi b,
and then the transition model and reward model.
Okay. And so this is nice because this cancels and this cancels.
Because the dynamics of the world is what determines
the next state and the dynamics of the world is what determines the reward.
And notice here just to make this not incredibly long,
I- here, I've made a Markov assumption.
So this is the Markov version.
[NOISE] But you could do,
um, you could condition on the full history.
So this trick does not require us- does not require the system to be Markov.
Because no matter whether your- your dynamics
depend on the full history or the immediate last step,
they're gonna be the same in the behavior policy and in the dynamics.
And in the evaluation policies,
you can cancel these and same for the reward model.
So this- this, um,
insight does not require a Markovian assumption.
And what that means is that we just end up getting this ratio of, um,
the way we would pick actions under the evaluation policy divided by the ratio
of the way we would pick actions under the behavior policy. Yeah?
Assuming that, uh, same trajectory is generated by two different policies.
Great question. Um, yes,
we're assuming the same trajectory was generated by two different policies.
Yes. So we're saying for this trajectory,
what's the probability you would have seen this under
the behavior policy versus the evaluation policy?
And so if it was more likely under the evaluation policy,
we wanna upweight whatever reward we got for that trajectory.
And if it's less likely under that evaluation policy, we wanna downweight it.
So the intuition is that we have a bunch of, um,
uh, trajectories and their sum of rewards.
So we kind of have these h,
you know, h_1, G_1 pairs.
So we have these sort of trajectory, sum of rewards.
And if we had the same behavior policy as the
evaluation policy in order to know how good that evaluation policy is,
we just average all of those G's but they are different.
And so what we wanna do is we wanna say for
h's that are more likely under an evaluation policy, upweight those.
For h's that are less likely under a evaluation policy,
downweight those so that you get, um,
a true expectation when you do those G evaluation, G weightings.
Does that makes sense? Does anybody have any questions about that part?
So this is just so far telling us how we re-weight our data.
It's allowing us to get a distribution that looks more like the distribution of
trajectories we would get under our evaluation policy. Yeah?
In the final, ah, the final row,
we have denominator, ah, the behavior policy.
We get that as empirically?
Good question. You know, where does the behavior policy come from,
basically like- like what is these probabilities? Is that the question?
So I think it depends, So in the case,
um- if you're- if it's a machine learning algorithm,
you still generate your data, you just know it.
Like you would just look up in your algorithm and
see what the probability distribution is.
And for today, we're going to assume that this is known and it's perfect.
Um, that is obviously not true when we get into people data.
In those cases, um,
there's a couple of different things you can do.
One is that you can build estimators that are robust to this being wrong.
So you can use other ways to try to kind of be doubly robust if that estimator is wrong.
In others you can take the empirical distribution.
And actually there's some cool work recently,
I think was from Peter Stone's lab at UT Austin,
showing that sometimes you- it's better to
use the empirical estimate then even if you know the true estimate.
Like in terms of the resulting estimator, which is kinda cool.
Okay, so this is just writing that out in LaTex instead of me hand-writing it,
um, and so this just writes out,
um, [NOISE] the- the probability of a history.
And now we can see that equation that I put here.
So, um, you know,
the value of the policy that you wanna evaluate, um,
is gonna be this ratio of history's times the return of that history.
And what we said here is,
and this is, you know, one over n,
is that this is simply the probability of taking each of
those actions or I'll write it just in terms of the Pi notation.
So this is Pi e of aj given sji,
i equals one to the- the length of your trajectory divide by Pi b aji.
All times the return of that particular history.
[NOISE] So the beautiful thing is you can- this is an unbiased estimator.
This really does give you a good estimate of the value under a few assumptions,
um, which I'll ask you guys about in a second.
And, um, you don't need to know the dynamics, so no dynamics.
No reward.
No need to be Markov.
Can anybody, um, [NOISE] tell me case where maybe this doesn't work.
So, I was just seeing that if you use this for running,
then starting from your initial policy to your final policy,
they couldn't be that different, right?
Because otherwise then you- the
samples [BACKGROUND] from the previous strikers won't be useful.
That's a good question. That's exactly what I was asking about is, you know,
how different can Pi e and Pi b [BACKGROUND] and allow us to do this.
So can- that was exactly what I was about to ask you guys about.
So, can anyone give me, uh,
what they think might be a condition for this estimator to be valid.
Like, where might be some cases where you would expect this might do
badly in terms of differences between the evaluation policy and the behavior policy?
And it has to do with the probability of taking actions in a certain state.
If either of those probabilities are too small,
you are gonna have things blow up in bad ways.
Yeah, so in which that,
either of these probabilities are too small.
This might be bad. Which of these ways is worse?
If they- uh, it's not [inaudible].
Right. So pi b is really small or at, you know, 0 [LAUGHTER].
Um, this could be really bad now.
Um, pi b can't ever be 0 and us observe something.
So that's good, because we're getting data from pi b.
And so we have never observed a trajectory under which
pi b is 0 but it could be really, really small.
So it could be, you know, you see something and it's incredibly unlikely there,
but your behavior policy would have generated that.
Um, and what if it is 0?
So it might be 0 for some actions.
What would that do if it's 0 in places where pi e is not 0 ?
So what happens if pi b of a some particular a is equal to 0, uh,
but pi e of a greater than 0 is,
you know, greater than 0, might be 1.
[NOISE] What might be bad there?
Like do you think this is a- let's raise hands for a second.
If that happened, do you think we are hu- let's raise your hands, either
yes if you think this is a good estimator or no if you think, oh, that's rough.
So if the behavior policy is 0 probability of taking an action,
but the evaluation policy has a positive probability of taking an action.
Raise your hand if you think, um,
this estimator could be really bad. That's right.
Yes. So, you know, if there are cases where you're just never trying actions,
like you never saw actions in your data that
your new evaluation policy would take, you can't use this.
So we often call this as coverage.
So coverage or support.
So we often make a few basic assumptions in order for this to be valid.
So our coverage or support normally means that pi b of
a given s has to be greater than 0 for all a and s,
such that pi e of a given s is greater than 0.
So you kind of have to have support over.
It doesn't have to be non-zero everywhere.
But for anything that you might want to evaluate,
for anywhere if you're really going to generate data from
your evaluation policy and it might take an action,
you need to be able to get to that state and you need to be able
to take that action with some non-zero probability. Yeah.
So terminology questions. So, we're calling pi b
is the same as pi 2 in the other part, right?
That's right.
Okay. Originally I thought you said that the evaluation policy was
the one that you observed the data from. That's incorrect?
Thanks for making me clarify that.
Um, the- the- the behavior policy is always one you observe data from,
evaluation is the way you wanna look at.
And, I apologize, notation often here is a little bit [NOISE] snaggly because I think, um,
people sometimes call the evaluation policy the target policy or evaluation policy or,
you know, one or two, um,
and most of the time,
the policy used to gather the data is called the behavior policy. Yeah.
[inaudible] you just like not include it in the product.
Question is whether or not what if they're both 0,
um, is that a problem?
Would you ever get data from that?
So, yes. So if they're both 0, it's okay.
So you don't actually have to- it does not require you to
take- to have a non-zero probability of taking every action in every state.
Um, so it can be okay.
So pi b of a given s can't equal 0,
if pi e of a given s is equal to 0.
It's fine for that to be the case.
You just can't have any case where you would never
have either reached the state or generated
that action for things that you could have potentially done
under your, um, evaluation policy.
And that doesn't sound too strict.
Um, but in practice, that can be a big deal.
So if you think about like Montezuma's Revenge,
excuse me, or different forms of Atari,
like under a random behavior policy,
you're never gonna get to see a lot of states,
and you're never gonna take actions in those states.
Like you're just- uh, it's incredibly unlikely,
unless you have an enormous amount of data.
So in practice, you can think of sort of
the behavior policy you have is kinda of defining like,
um, like a ball [NOISE] under which you can evaluate other potential policies.
So if you have like- it's not actually a sphere,
but like, you know, if it's, um,
if you have a behavior policy here,
you can think of kind of having some distance metric under which you can still get
good estimates of pi e. So you kind of have a radius,
and it depends on these are sort of essentially the- the policies for
which you have support over and anything else you can't evaluate.
Okay, all right.
So just to summarize there, um, you know,
importance sampling is this beautiful idea that works for lots of statistics,
including for reinforcement learning.
I think- The first introduction to this,
um- I think it was first used for RL I think in,
um, Doina Precup's paper, Precup 2000?
Around then. It's been around for a lot longer, but, um,
but I think for reinforcement learning,
that was their first introduction of using this.
And of course these ideas also come up in policy gradients type- type methods.
Um, and the great thing is thi- is,
is this unbiased and consistent estimator,
just as a refresher consistent means that
asymptotically it really will give you the right estimate.
So, you know, as n goes to infinity,
the r estimated v pi e goes to e pi e. Just kind of a nice sanity check.
As you get more and more data, you will get the right estimator.
Um, and just to check here, this is, you know,
under- under a few assumptions.
You have to have support.
[BACKGROUND]
All right. Now, um,
in our particular case,
I- we can leverage a few aspects of the fact that this is a temporal process.
So again, like what we saw for policy gradient type work,
um, we'll call policy gradient methods.
We can leverage the fact that,
um, the future can't affect past rewards.
So when we think about generating these importance ratios,
we only have to for a particular time step t,
um, instead of multiplying it by- so I guess just to back up for there.
Remember that Gt is defined to be equal to, [NOISE] you know,
the rewards, [NOISE] like the sum of rewards.
So when we think about this equation for importance sampling,
um, let me just go back to here.
So this could be expanded into, you know,
r_1 + gamma times r_2 + gamma squared times r_3, dot, dot, dot.
And right now in that equation,
that's like multiplying your full product of importance ratios,
times e to the rewards.
Um, so we're not doing any,
it's the same ratio of probability of action given by probability of action,
multiplied by each of these different reward terms.
But since, you know,
r_3 can't be affected by any actions that are in r_4 longer.
In some ways you're just- this isn't wrong.
It's just that you're introducing additional variance.
So similar to what we saw, um,
in policy gradient stuff,
we don't actually- we only have to multiply by that product of ratios,
up to the time point at which we got that reward.
So this allows us to get to per-decision importance sampling.
So this is only up to- only [NOISE] up to
point got reward [NOISE] because the- the future can't inputs past rewards.
And again, this is independent of it being Markov or not.
So this is just the fact that it's a temporal process and we can't go back in time.
All right. So another thing just in general is that, um,
in importance sampling, um,
we're just sort of these weights,
these weights to these products.
You know, products of like,
um, picking an action under different policies.
So we often call these weight terms
nicely and confusingly with
all the weights we talked about with function approximation.
Um, and weighted importance sampling compared to
importance sampling just renormalizes by the sum of weights.
The reason you might wanna do this is that as we were talking about before,
if your pi_b is really tiny.
So let's say this, this might be super tiny,
super small [NOISE] for some trajectories,
then that can mean that your importance weight for those trajectories is enormous.
In fact, we have,
um, a proof that, you know,
that generally the size of your importance weights
can grow exponentially with the horizon.
Um, uh, and so these importance weights can be incredibly large in some cases.
And so what weighted importance sampling does is it just renormalizes.
So effectively, you're making it
so all your importance weights are somewhere between 0 and 1.
Then you're using that when you're reweighting your distribution.
So when you do this,
this is something that's very common, um,
to do again this pre- predates the reinforcement learning side,
but has also been used in reinforcement learning.
Um, this is, uh,
this is biased, um, still consistent.
[NOISE] So that means asymptotically it's
going to get to the right thing and lower variance.
[NOISE] So we're essentially going to play,
um, the bias variance trade-off that we've often seen.
We can make Q learning versus Monte Carlo estimates.
Monte Carlo were unbiased but very high-variance.
Q Learning bootstrap so it's biased but
often much better because you're so much lower variance.
Weighted importance sampling is much lower variance empirically much,
much better, most of the time,
particularly for small amounts of data. Yeah.
Yeah. I was wondering if you could comment on, um,
you know, before you're saying that we were intentionally designing something to be unbiased.
So we're going to ignore certain techniques and now we're reintroducing bias at the end?
Right.
I guess, what is the intuition behind when it's okay,
I guess, to introduce bias and like when and why?
It's great one. Okay. So you just made a big deal before about saying let's go for
unbiased estimators and now you're telling us that we're going to go back to bias in,
[LAUGHTER] that- that case, you know, how do we make decisions about when this is okay?
Um, I think it totally depends on the domain.
Uh, and it als- I think one challenge and issue that comes up is,
if things are unbiased,
it's often easier to have confidence intervals around them.
We know better how to do that, um,
when it's biased, it's often hard to quantify.
Um, I think, uh, I'll talk briefly a little bit later about times
where we really just want to directly optimize for bias plus variance.
Like we want to look at accuracy,
mean squared error, just the sum of bias and variance.
And so then I think you can- it provides you a way to
directly trade-off between those two because you're like,
I know I want to minimize the mean squared error.
And that's the sum of these two, that gives me a way like,
a principled way, how to trade those off.
I think another thing I often like just for sort of a sanity check is that,
if it's biased but still consistent,
um, that's sort of a nice sanity check.
It's like okay maybe there's a small amount of bias early on,
but eventually if I get a lot of data,
it's really given me the right answer.
And some of the function approximation stuff we solve for Q learning.
That's not true, like we just, all bets are off,
who knows what's happening asymptotically.
But again, it depends on the- depends on the day.
It's a great question. It's a big challenge in that area.
Okay. So as I was just saying in this case,
um, you know, weighted importance sampling, it's strongly consistent.
You're going to converge, um,
if you have a finite horizon or, um,
one behavior policy or bounded rewards, and, uh,
Phil and I, so that if you'd look at Thomas and Brunskill.
[NOISE] We think about this for
the RL case in our ICML 2016 paper. It's one reference for this.
Okay. So we're going to have these estimates.
Um, they may be,
if we're using weighted importance sampling,
there might be a little bit bias, but lower variance.
Otherwise, there might be high variance but low bias, or zero bias.
What's something else we could do?
So, uh, let's briefly talk about control variance
and be thinking in your head again at back to policy gradient,
and thinking about baselines.
So just from a statistics perspective,
um, you know, if we have an X,
we have the mean of that variable, um,
if it's unbiased, it means that our estimate of
the mean is matching the true expectation,
and then we have our variance.
So let's imagine that we just kinda shift these estimates.
Okay. So we're just going to subtract a random variable Y,
and then in your head you can think about this as like a Q function, something like that.
So, you know, Y might be a Q function,
and expected value of Y might be a V. Do you think about Q being,
um, as an a, an expected value of
Y being an average over all the actions you could take in that state.
So then, i- if you redefine your mu so,
um, X is gonna be like, let's,
we're gonna be going back towards trying to get our estimate of V_pi_e.
[NOISE] Then if you subtract off something else and add on this expectation,
you still get an unbiased estimator.
[NOISE] So we can just write that out here.
So X - Y + expected value of Y,
just equal to expected value of X,
minus expected value of Y,
plus expected value of Y.
So you can do this. I mean, you can do this in statistics.
You can subtract a variable and add on its expectation and on average,
that does not change the mean of your original X.
But you might ask, "Why would I do that?"
[LAUGHTER] So you can do this for anything like
any- these are where X and Y are random variables.
So these are general, just any random variables,
where Y is conc- called a control variant.
And this may be useful if it
allows you to reduce variance. And that means that. You know.
Y has to have something to do with X.
Okay. If you just subtract off something random,
this is probably not going to be helpful.
But if you subtract off something that allows you to have some insight on X,
in our case we're gonna be interested in things that allows us to help predict the value,
um, then we might get a lower variance.
So we can look this by looking at what is the variance of this weird quantity mu hat,
where we had this X - Y + the expected value of Y.
[NOISE] We can write it down as follows,
X - Y + the expected value of Y.
Now, the variance of the expected value of Y,
there's no more variability in that so we can just say that's the variance of X - Y.
[NOISE] So what that means is that we're gonna get something which is variance of X,
variance of Y, and the covariance of the two.
So if it is the case that the covariance of X and Y, meaning that the, you know,
there is relationship between these two variables,
one of them is giving us information about the other.
If that is bigger than the variance of Y,
then you're going to have a win.
Then you're gonna have that the resulting estimator.
So if this is true,
if true- if true,
variance of mu hat is gonna be [NOISE] less than variance of X.
So this is nice because it means that we
didn't change the mean and we reduced the variance,
which in some ways, kind of seems like a free lunch but it's not really
a free lunch because we're using
information that is actually telling us something about X.
And this is very similar to us using a baseline term and policy gradient.
Where instead of just relying on the Monte Carlo returns,
we could also subtract off a baseline like the value function.
[NOISE] So you can do this in importance sampling as well.
Um, so where X is the importance sampling estimator and Y is some control variate.
Typically, you know, this can either be a Q function which you
build from an approximate model of the Markov decision process.
It can be a Q function from Q learning.
Um, this gives you some way to get- we can think of this as Q, you know,
some- some estimate [NOISE] of state-action [NOISE] value, okay.
And doing this is called a doubly robust estimator.
Doubly robust estimator is again, um,
where in statistics for a long time, um,
in around 2011, were brought over to the,
I remember were brought to the multi-armed bandit community,
like the machine-learning multi-armed bandit community with Dudik et al.
And why do they call it doubly robust?
Well, the idea is that, um,
if you use information both from like your normal importance sampling, um,
plus some control variate like a Q value estimate,
um, you can be robust too if either of those are wrong.
So either if you have a poor model or you have
a bad estimate if your control variate is bad.
[NOISE] So you know, why would this be important?
Well, if we go back to sort of questions,
some other people's questions, um,
an alternative is just to like do Q learning on your data, right?
But Q learning might be biased or it might be
a horrible estimate and who knows if it's good?
Um, but it might be good.
So if it's great- good you'd like to be able to say,
"We've got a good estimate."
Or if it's bad,
you'd like to say that it's with your importance sampling can compensate for that,
and this says that either if you have a bad model,
um, or you have error in your estimates of pi B.
So this is like those cases where we've got data from positions,
so we don't really know what the behavior policy is.
So if you have inaccuracies there, then you, um,
also would like to if it turns out your control variate is accurate,
then you could also do well.
Now, if- in some cases both of these are bad and in that case,
kind of all bets are still off.
But it gives you more robustness about different parts of what you're
you're trying to estimate how good your evaluation policy is.
Okay, and Bill and I discussed sort of different ways
to do this as well as doing it in
a weighted way, so incorporating weighted importance sampling.
So what does this allow you to do?
Okay, I'll- I'll briefly show the equation.
Then essentially, the idea here is these are like the importance sampling weights.
This is the raw returns and then we can add and subtract.
This was Y and this is the expected value of Y and these can be computed
by computing like Q learning or
doing like an empirical model and doing dynamic programming on it.
You could get these sort of estimates of Q of pi E in lots of different ways,
um, and they might be good or they might be bad.
But you can plug them in and often,
they're gonna end up helping you in terms of variance.
So let's see empirically what this does.
[NOISE] So this is a really small Gridworld.
Think something on the order of like,
you know, maybe five-by-five, four-by-four.
This is a really small world and we're using it to try to illustrate and understand,
um, the benefits of these different types of techniques.
So what's on the x-axis?
This is the amount of data.
So this is the size of the dataset.
What's on the y-axis? This is mean squared error.
This is the difference between our estimate of
the evaluation policy and the true evaluation policy,
and you want this to be small.
So smaller, better and this is a log scale.
[NOISE] So what do we see here?
So one thing you could do is you could build an approximate model with your data.
That model might be wrong like maybe,
you're making a Markov assumption and it's wrong.
Or maybe, um, you know,
there's other parts where you just can't estimate well.
So this is the model-based.
So this is just we use a model,
and we compute V pi e for that model.
So we take our data,
we build an MDP model, then we, um,
use that to- then that's like a simulator and then we can
just compute V_pi e. So you can see here it's flat.
Um, in this case, um, I'd have to remember here.
I think we are using a different dataset.
Either I- I would have to double check whether or not
just after we have that number of episodes,
the model just doesn't change with further data.
Like the model just isn't great.
Maybe we're not using all the features that we should be, um,
or the world isn't really Markov,
and so you kind of have this fixed bias.
Your model can be asymptotically wrong.
The estimates you get from it can be asymptotically
wrong if your model is not a good fit to the environment.
[NOISE] The- the second thing we can do is we can do importance sampling.
So importance sampling is unbiased.
It's going down as we get more data as we would hope.
Eventually, it should collapse to 0.
Um, but we'd like to do better than that.
So now this is per decision importance sampling.
You can see you get a benefit from leveraging the fact
that rewards can't be influenced by future decisions.
That reduces the variance,
kind of gives you this nice kind of automatic shift down.
Um, if you do doubly robust,
you get a significant bump.
So what's doubly robust doing again?
It's combining our approximate model plus IS.
So you can see again, here we're getting,
ah, a significant bump.
Now, I talk about this mostly in terms of mean squared error
but I think it's really important to think about what mean squared error means.
Um, so mean squared error here is, you know,
how accurately are we estimating this V_pi E minus the true one?
But we can alternatively think about this in
terms of how much data we need to get a good estimate.
So look at this. This is- you want a mean squared error of 1.
Maybe that's sufficient. Maybe that's not.
That means that under a per decision important sampling estimator,
you would need 2,000 episodes and with doubly robust,
you'd need less than 200.
So that's awesome because it means that we need like an order of magnitude less data,
in order to get good estimates,
and a lot of real world applications we just don't have enough data.
You know, there might not be a lot of patients with
a particular condition and
you'd really like to still be able to make good decisions for them,
and so you need estimators that need much less data to give you good answers.
And so that's why this is important.
Okay? Right.
And then in these cases,
we can see also that if you do weighted
importance sampling [NOISE] and weighted throughput decision,
that also ends up helping a lot.
[NOISE] Here, is if you do weighted doubly robust,
again, sort of answering that question of when should you do,
you know, how do you trade off between this bias and variance?
Um, here, we can see that if we do some form of weighted,
uh, doubly robust which is one of the things we introduced in our ICML paper.
You again get a really big gain.
So we had this to there,
and this to there, right?
So now you are needing like five episodes to get to the same.
So again, some of these improvements can be- this is of course Gridworld, right?
Like this- you- you need- one needs to look at
this also for the particular domain one's interested in.
But it indicates there are just some of these cases by being, um,
a little bit better in terms of these estimators,
you can get substantial gains in terms of how accurate your estimators are.
[NOISE] All right,
again to continue on this slide of like, how do you balance between variance, um, and bias.
One thing that Phil and I thought about is, okay,
well, you know, you might want to have, um,
low bias and variance, ah,
and- and how do you do this trade-off,
let's just think about optimizing for it directly.
So our magic estimator just tries to directly- directly minimize mean squared error.
Okay. So- so again let's say mean squared error is gonna be,
you know, it's a function of bias and variance.
So if you knew what bias and variance was,
you could hopefully just optimize this directly.
Do we know what bias is?
So bias again just to remember is bias is the difference
between this minus this. That's bias.
Do we know that? No, unfortunately, if we knew that,
if we knew the bias, then we wouldn't have to do anything else because we would
know exactly what the real value was.
So a big challenge when we're trying to do this work is
how do you get a good estimate of bias?
Or like an under or overestimated bias?
And just briefly, the idea that we had there is to say,
if you can get confidence intervals,
which you can using importance sampling.
So let's say, I have these, you know,
this is my estimate from importance sampling.
And I have some uncertainty over it.
So this gives me some estimate,
[NOISE] with some upper and lower bounds.
So you know, I'm like, you can do, ah,
let's say the value is 5 plus or minus 2.
Okay. And then let's say I have a model, um,
I have a model that I built from this data and then I
used it to evaluate and I got another estimate up here.
So I have a V_pi,
and this is using a model.
Okay, so let's say this is 8.
So what is the minimum bias my model has to have?
How could you use these confidence intervals to get like a lower
bound on how- how bad my- assume these confidence intervals are correct.
Now that these are real confidence intervals so we know
the real value has to be between 3 and 7.
What's the- what's the minimum bias that my model would have?
One right, because it's this difference, okay.
So this gives you a lower bound on the bias.
So how far off you are from
these confidence intervals that gives you a lower bound on the bias?
It's optimistic, your bias- your model might be way more biased.
Um, uh, but it gives you a way to quantify what that bias is,
and that's what we use in this approach.
So we combine our importance sampling estimators and think about how variable they are.
We have to get an estimate of their variance, um,
as well as the bias on the model,
and that allows us to figure out how to trade off between these.
And again, you get, um,
you get a really substantial gain often.
This is still Gridworld but, um,
you're gonna get again [NOISE] roughly an order by magnitude difference in some domains.
You're gonna need an order of magnitude less data.
And in this case, I've just zoomed in so you don't even
see some of these other methods because they're so much higher up.
[NOISE] Okay, so, you know,
that's one thing that we could do to try to get sort of
good off-policy policy evaluation estimates.
Um, I haven't talked to you too much so far about like
how are we gonna get these confidence bounds over those.
But I've mentioned sort of a number of different ways that we
could try to get just an estimate of, you know,
V_pi E. So we want to get some estimate of
this new alternative policy that we might wanna unleash on the wild.
Um, I'll- I'm gonna skip this part, I'll just say briefly,
you know, there are some subtleties here with whether or not,
um, ah, you know,
what's the support of your behavior policy, um,
and how we do some different weighting and can we
improve over this sort of weighted importance sampling?
Um, [NOISE]. The answer is, is yes.
You can do some slightly different weightings,
um, and I'll- I'll defer that.
And then also, another really important question,
really important in practice is that,
um, your distributions are often non-stationary.
You know, imagine that like you're looking at
patient data and during that time period like
some new food pyramid came out from
the Food and Drug Administration and so everyone changed how they're eating.
Um, so now that, you know,
the dynamics of your patients are gonna be really different than before.
So you'd like to be able to identify whether or not you
have sort of non-stationarity in your data-set.
Like if the dynamics model of the world is changing.
So we have some other ideas about how to handle that.
[NOISE] Okay, but now let's go to and say,
let's assume we've done this off-policy policy evaluation.
We've gotten out some estimate, um,
of these, how good our alternative policy would be,
and we want to go beyond that and we want to get some confidence over it.
So again remember we're trying to move to a world where we can say, you know,
the probability that the policy output by
our algorithm being better than our previous policy, [NOISE] is high.
So with high probability we're gonna give you a policy that's better,
which means not only do we need to have an estimate on how good that policy is but
how much better it is or not than your behavior policy.
Okay. How would we do that?
So let's first consider using importance sampling plots Hoeffding's inequality.
Again let's think back to what we are doing with exploration,
to do high confidence off-policy policy evaluation.
So just as a refresher,
mountain car is the simple control task,
[NOISE] where, you know,
you have your agent trying to reach here, we get high reward.
And we're gonna have data gathered from
some behavior policy and we want to try to learn a better policy from it,
and be able to get competence bounds over its performance.
Okay, so remember that in Hoeffding's inequality,
it's a way to look at how different your- your mean is from,
um, your average so far.
So how different can we- how can your- how different can your empirical mean
be [NOISE] minus your true mean?
[NOISE] And it gives you a bound on that in terms of the amount of data you have.
Okay, so it's a function of the amount of data you have.
And it depends on the range of your variables, okay.
So thought about about this for arms,
which might have rewards of between 0 and 1,
and then b would be 1.
Okay, so we can use Hoeffding's inequality also,
we talked about it briefly for,
um, uh, [NOISE] you can use a model-based reinforcement learning.
Let's think about using that in the context of this off-policy evaluation.
So we can also use it using our old data to try to estimate, um,
what the- what our upper or lower bounds might be on the value of the evaluation policy.
[NOISE] So let's imagine that we use 100,000 trajectories.
And the evaluation policy's true performance is 0.19.
And if we use Hoeffding's inequality,
we're very confident that the new policy has at least -5 million.
Okay. And we know that the real reward is somewhere between 0 and 1.
But Hoeffding's inequality gives us this bound of -5 million,
and that's true, right?
Like [LAUGHTER] you know,
is like 0.19 is greater than -5 million.
But it's not particularly informative.
Um, like we know that the real returns
for the- for this domain is somewhere between 0 and 1.
And if we use Hoeffding's inequality there,
we're getting something that we'd call is vacuous.
Okay, so you're getting a bound that is true but entirely uninformative.
Because it is incredibly negative, right?
Like we know that this is
the true- true value is- for all policy is between 0 and 1.
[NOISE] Okay.
So why did this happen?
Um, [NOISE] let's look at importance sampling.
Importance sampling says we're gonna take this product of weights.
Okay, and as we've talked about before,
this might be pretty small.
So let's imagine this is like, you know, 0.1.
So then you have 10 to the L. Like if you take a series of actions,
let's say for every single action of that trajectory it was pretty unlikely
[NOISE] which you often need in domains like mountain car,
because in mountain car you have to take
a pretty specific sequence of actions in order to finally see some reward at the end,
and under a policy [NOISE] that is not optimal,
it might be pretty unlikely to see those series of actions.
So let's say, you know, most of your data you never get up the hill,
in like one or two of your data points you actually get up to the top of the hill,
and those were very rare trajectories.
Which means your, um,
importance sampling weights are gonna be extremely high.
It's gonna be this, you know,
1 divided by 0.1 up to the L. [NOISE] That's just enormous you know.
Um, so these can start to be incredibly large.
And Hoeffding's Inequality depends on this.
The range of the potential returns you have.
What are the range of our potential returns?
The range of our potential returns are g times or
product of i equals 1 to t of our importance weights.
[NOISE]
So b is equal to max over this.
It says, "In the maximum case,
what could it look like, your return is?"
[NOISE].
And so it's gonna depend most on what your real range is.
Our real [NOISE] b is going to be between 0 and 1,
and this product of importance sampling weights,
and that's where the problem is.
The product of importance sampling weights can be enormous.
Okay.
Because you have really unlikely sequence of actions,
and then you get this blow up. All right.
So if we look at that here,
we can get this distribution, um, and some of the,
some of these are incredibly large,
and that means that our Hoeffding Inequality ended up being,
because Hoeffding again is you subtract b,
basically -b times square root of 1 over n. So if b,
let's say- let's say,
your trajectory lengths are something like 200,
which is pretty, somewhat reasonable for Monte Carlo,
I'm sorry, for mountain car.
Then you'd have something like 1 over 0.1 to the 200 times 1,
times the square root of 1 over n, roughly right?
And so [NOISE] you'd have just this crazy,
crazy large term and you're subtracting this.
So that would mean that your bounds are vacuous,
basically I have no idea how good this evaluation policy would be.
So does anybody have any questions about that, about why that issue occurs?
Okay. So the insight that Phil had in some of his previous work is,
just get rid of those, just cut it down.
Um, so if you remove this tail,
what does that do to your expected value?
It just decreases it.
So if you ignore those like really,
really crazy high returns [NOISE] you're
not gonna get an estimate anymore that's as good,
but it's just gonna get smaller.
You're not gonna overestimate it.
So again, if we're thinking about say policy improvement,
we're concerned about deploying policies
that are worse than we think they are in practice.
What this is gonna do, is say, we're gonna
underestimate how good our behavior policy is,
or our evaluation policy is.
And so if we underestimate it, that's okay because that's safe,
like if we don't deploy things that actually would have been good,
maybe there's a lost opportunity cost,
but it's not gonna be bad for the world,
um, so that's what the insight was,
for here is that you can like remove this upper tail.
And so you don't need to read the proof, ah, um,
but the idea is that you can basically define a new confidence interval,
that is conservative [NOISE].
And you can think about how you choose that conservatism,
depending also on the amount of data you have.
So here's the beautiful take-home, um,
so let's say that you kind of use 20% of your data
to figure out exactly how to tune this confidence interval,
so this is sort of sets your confidence interval.
And then your next part to compute your lower bound,
so for mountain car with the same amount of data,
you've got 100k trajectories.
So this is the new estimator,
and you get the, um,
the mu, the estimator of the V pi,
so this is a lower bound on V pi e. [NOISE] It says it's gonna be at least 0.145,
and the true value is 0.19.
And this is compared to all these other forms of concentration inequalities,
which were all, except for Anderson,
really, really bad [NOISE].
So things like Chernoff-Hoeffding and these other
ones you don't have to be familiar with all of these.
But basically, it just says if you'd used other approaches,
to try to get this lower bound [NOISE] they would have been entirely vacuous,
whereas this one says, "Okay.
We're not sure exactly how good it is.
It's gonna be at least 0.145,
and the real value is 0.19."
It's not perfect, but you know,
if- if your behavior policy was 0.05,
that would be good enough to say you should use a new thing [NOISE].
So, um, they use this idea for digital marketing,
this is some work that Phil had done in
collaboration with some colleagues over at Adobe.
Um, and the nice thing about this is you can say,
you know, if I want it, I'll figure it out,
if I [NOISE] am gonna deploy something, and get, um,
more effective digital marketing,
and I have access to our previous data.
[NOISE] Can I say with what confidence,
I can deploy something that's gonna, you know,
generate higher clicks, get more revenue.
[NOISE] And these confidence bounds turn out to be tight
enough that you can actually know that
the new policy is gonna be better, which is pretty cool.
[NOISE] [inaudible] Yeah. You can also,
so this is one form of trying to get those confidence bounds,
turns out you can also use t-tests and empirically that's often very good.
Um, and some of you guys might have seen some of these in,
ah, some of your statistics class.
I'll just really briefly take, because we're almost out of time,
that you can combine these ideas,
and then think about trying to get these lower bounds,
um, here, and combine it with optimization.
So you can think about doing this for a number of different policies,
trying to compute lower bounds on all of them.
And then using that information to try to
decide which one to deploy in the future, in a way that is safe.
Okay. So you can sort of say,
"I'm gonna use some of my data to optimize,
some of my data to, um, ah,
to try to evaluate the resulting one,
and make sure that it's got a good confidence bound before I deploy it.
[NOISE] And again, you can do this in digital marketing,
some of the other work that Phil Thomas and I have looked at is,
using a diabetes simulator,
and looking at whether or not we can infer
higher-performing policies for things like [NOISE] insulin regulation,
um, ah, using similar ideas in something that in a way that you could be,
um, with high-confidence better before you deploy it.
I'm gonna skip briefly through this.
This- this is a really big ba- ah,
like this is an increasingly, ah,
big area of work in the community.
Um, I think a lot of people are thinking about,
it is counterfactual reasoning issue because we have
more and more data from electronic medical records systems,
that we'd like to use to improve patient health.
We have data, um, you know,
on- on online platforms etc.
There's a lot of additional challenges,
things like how do you deal with long horizons?
Um, [NOISE] the fact that importance sampling can be unfair,
ah, [NOISE] what do I mean by that?
I mean that, essentially,
different policies when you evaluate them,
might have different amounts of variance depending on
how well they match to your behavior policy.
Because of that, it may be hard to decide which of those you should deploy.
Um, we have various work thinking about when the behavior policy is unknown.
Where we combine these ideas with deep neural networks,
um, and we're also thinking about transfer learning.
So I know Chelsea talked about meta-learning on Monday.
Um, one of the interesting ideas here is,
you're building these forms of models.
Can you kinda use the same ideas of fine tuning in the reinforcement learning case?
So can you think about building models for our policy evaluation
that leveraged as data that does not match the policy you care about?
In order to get generally better models,
you know, things like health care,
I think that can be pretty helpful.
[NOISE] But there's a lot of additional work on this,
and there's a number of other groups that are thinking about these types of ideas.
Um, and also on campus if you're interested in these ideas in general.
There's also a number of great colleagues that are
thinking about this from other perspectives.
People coming at it from the perspective of economics,
or statistics, or epidemiology.
And it's been really fun to try to get to collaborate with these people as well.
So just to pop-up a level briefly, um,
the goal in these cases is to think about if you have some set of data,
we're gonna go from data to a good new policy [NOISE].
Okay. And you want it to be good in a way that you
know something about the quality of it before you deploy it.
And so that's really what's sort of safe off-policy policy evaluation and selection,
or optimization is about.
And so in terms of the things that you should understand from here,
you should be able to define and apply importance sampling [NOISE] ,
know some limitations of importance sampling.
[NOISE] List a couple of- of alternatives.
Know, you- why you might want to be able to do this sort of safe reinforcement learning,
like what sort of applications this might be important in.
[NOISE] And sort of the- define what type of guarantees we're getting in these scenarios.
So that's it, and then next week,
we'll have the quiz on Monday,
and then on Wednesday we'll talk
about Monte Carlo tree search. Thanks.
 Right. We're going to get started.
This is the last lecture for the term.
Um, uh, just a few logistics things that we're getting at the beginning.
Um, so just a friendly reminder that the project write up is due on the 20th at 11:59 PM.
There are no late days, and then the poster presentations are on Friday at 8:30 AM.
[NOISE] Uh, you need to submit your poster as
well and that should be submitted online to Gradescope by the same time.
Um, we'll open up a submission for that in advance.
Um, and there's also no late dates for that.
Uh, you should have received an email with some details about the poster session.
Any questions? Reach out to us.
Does anybody have any questions right now?
It's the last week of office hours.
We won't have office hours next week.
It's finals week for most people.
Um, but you can reach out to us on Piazza or if you have extra questions, you know,
we're happy to find the time.
Okay. All right.
So what we're going to do today is,
uh, so last time, of course, was the quiz.
Uh, and we're gonna be sending out- our goal is to send
out grades for everybody that took it on Monday to send them out today.
We're almost done grading those.
Um, [NOISE] and for the- there are a
few people who are still taking that late so though- uh,
we have to grade the SCPDs.
But everybody else should get their quiz scores
who took it Monday, should get back today.
And then today what we're gonna do is we're going to talk just a little bit about
Monte-Carlo tree search as well as discuss some end-of-course stuff.
So why Monte-Carlo tree search?
Um, who here has heard of AlphaGo? All right.
Yes. So I mean AlphaGo,
you could argue is one of the major AI achievements of the last,
you know, 10 to 20 years, um,
and it really has been
a spectacular achievement that was achieved much faster than was anticipated,
uh, to beat humans on the board game Go,
which is considered an extremely hard game.
So the Monte Carlo tree search was a critical part of,
you know, to achieve this success plus a lot of other additional things.
But it's one of the aspects that we have not talked about very much so far in class.
So I think talking about Monte-Carlo tree search,
so you're familiar with some of the ideas behind
that and therefore some of the ideas behind AlphaGo,
um, are useful, uh, to be aware of.
Uh, and then also because when we start to think about Monte-Carlo tree search,
it's a way for us to think about model-based reinforcement learning,
which is a very powerful tool that we haven't talked about as much in part
because we haven't seen as much success in the deep learning case with models.
And I'm happy to talk more about that either today or offline.
But I think that going forward it's likely to be a really productive avenue of research.
And we can talk about why that might be particularly useful in alpha in AlphaGo.
Okay. So what we're gonna do first is we're gonna sort
of talk a little bit again about sort of model-based reinforcement learning.
And then we'll talk about simulates- simulation-based search,
which is where Monte Carlo tree search comes up.
Actually, just because everyone takes different classes and I'm curious,
who here has covered Monte Carlo tree search in a, in another class?
Okay. Just two. What class was it?
[inaudible].
238.
Yeah.
Yeah.
Same?
Same one. It was mentioned a little bit like [NOISE]
It was mentioned briefly.
Ah, yeah. Very brief.
Yeah. And yeah?
217.
Which is?
General game play.
Oh, yeah. General game play would be a good one to come and bring it in.
Okay. Cool. Awesome. Oh, and also I just think people are interested.
At the end, I'll also mention some other classes
where you can learn more about reinforcement learning.
All right. So model-based reinforcement learning.
Um, [NOISE], most of what we've talked about this term though not all of it,
but most of what we've talked about this term [BACKGROUND]
particularly when we're talking about learning,
which means we don't know how the world works, um,
is that we're thinking about either learning a policy
or a value function or both directly from data.
Um, and what we're gonna talk about more today is
talking about learning a specific model.
So just to remind ourselves because it has been a little while.
We're gonna be talking about learning the transition and/or reward model,
and we talked about this a little bit maybe, I don't know,
a number of- ah, a few weeks ago,
It came up also in exploration.
But once you have a model,
you can use planning with that.
And just to refresh our memories,
planning is where we take a known model of
the world and then we use value iteration or policy
iteration or dynamic programming in general
to try to compute a policy for those given models.
In contrast to that, of course,
we've talked a lot about Model-Free RL,
where there's no model and we just directly learn a value function from experience.
And now, we are going to learn a model from experience and then plan using that.
Now, the planning that we do in addition to sort of
the approaches that are known from classical decision, uh,
like dy- dynamic programming also can be
any of the other techniques that we've talked about so far in class.
So, you know, once we have this,
this is a- this can act as a simulator.
And so once you have that,
you can do model-free RL using that simulator,
or you can do policy search or anything
else you would like to do given that you have a model of the world.
It basically just acts as a simulator,
and you can use it to generate experience as well as do things like dynamic programming.
Okay. You can do sort of all of those things.
So once you have a simulator of the world, that's great.
The downside of course can be if that simulator is not very good.
What does that do in terms of the resulting estimates?
Okay. So just to think of it again.
We have our world.
It's generating actions and rewards and states.
Um, and now we're going to think about sort of explicitly trying to model those.
So in a lot of cases you may know the reward function, not always.
But in a lot of practical applications,
you'll know the reward function.
So if you're designing like a reinforcement learning
based system for something like customer service,
you probably have a reward function in mind,
like engagement or purchases or things like that.
But you may not have a very good model of the customer.
So there are a lot of practical examples where you'll need to
learn the dynamics model either implicitly or explicitly.
But the reward function itself might be known.
All right. So how do we think about this in terms of a loop?
We think about having,
um, [NOISE] some experience.
So this could be things like,
you know, state, action, reward,
next state tuples that we then feed into a model,
and this is going to output either a reward or a transition model.
We do planning with that,
which can be dynamic programming or Q learning
or many of the other techniques we've seen here, policy search.
And then that has to give us a way to pick an action.
So that has to give us an action that we can then use over here,
and we don't have to necessarily compute a full value function.
All we need to know is what is the next action to take,
and we're going to exploit that when we get to Monte Carlo tree search.
That we don't necessarily have to compute a full value function for
the world nor do we have to have a complete policy.
All we have to know is what should we do for this particular action next.
So some of the advantages about this is, um,
we've a lot of supervised learning methods including from deep,
uh, deep learning which we can use to learn models.
Some of them are better or worse super, uh, suited.
So our transition dynamics,
we're generally going to think of a stochastic.
So we're going to need supervised learning methods that can predict distributions.
For reward models we can often treat them as scalars.
So then we can use very classic regression-based approaches.
And the, the other nice thing about
model-based reinforcement learning is like what we talked about for exploration,
um, that we can often have explicit models over
our uncertainty of how good are our models.
And once we have uncertainty over our models of the world,
we can use that to propagate into uncertainty over the decisions we make.
So in the bandit case that was pretty direct,
because in the bandit case- so for bandits,
we had uncertainty over the reward of an arm,
and that just directly represented our uncertainty over
the value because it was only a single timestep.
In the case of MDPs,
we could represent uncertainty over the,
the reward and the dynamics model and forms of these sort of bonuses,
and then propagate the- that information during planning.
And that again allowed us to think about sort of how well do we
know the value of different states and actions and what could it be,
what sort of could it be optimistically.
Now the downsides is that, you know,
first we're gonna learn a model and then we're gonna construct a value function,
and there could be two sources of approximation error there.
Because we're going to get an approximate model and then we're gonna do
approximate planning in general for large state spaces,
and so we can get compounding errors in that case.
Now, another place that we saw compounding errors earlier in
this course was when we talked about imitation learning.
And we talked about if you had a trajectory and then you tried to
do behavior cloning and learn mappings from states to actions,
and how if you then got that policy and followed it in the real world,
you might end up in parts of the state space where you didn't have
much data and you could have sort of these escalating errors because,
um, again it could compound.
Once you get in parts of the state space where you don't have much data,
and then you're extrapolating,
then things can go badly.
So similarly in this case,
if you build a model and you compute a policy that ends up getting you to
parts of the world where you don't have
very much data and where your model estimate is poor,
then again your resulting value function in your policy might be bad.
I guess I'll just mention one other big advantage I think of with
model-based reinforcement learning is that it can also be very powerful for transfer.
So when Chelsea was here and talking about meta-learning,
one of the nice benefits of model-based RL is
that if you learn a dynamics model of the world,
then if someone changes the reward function,
implicitly you can just do zero shot transfer,
because you can just take your learned model of the dynamics and then your reward function,
you can just compute a new plan.
So like if I'm a robot and I learned how to navigate in this room,
and so like now I know like, you know,
what it's like to turn and what it's like to go forward,
etc., and before I was always trying to get to that exit.
But now I know what dynamic- I know my dynamics model in general for this room.
And then someone says, "No. No. No. I don't want to go to that exit because that one's,
you know, closed or something.
So go to that other exit."
And they tell me the reward function.
They say, you know, there's a +1 for that exit now instead of there.
Then I can just re-plan with my, my dynamics model.
So I don't need any more experience.
I can get zero shot transfer.
So that can be really useful.
So that's one of the other reasons why you
might want to just build models of the world in general.
And there's some interesting evidence that when people play Atari games,
that they are probably systematically building models.
What happens when I move the iceberg next to the polar bear?
And because then you can generalize those models to other experience.
Okay. So how are we gonna write,
write down our model in this case.
We're again just gonna have our normal state,
action, transition, dynamics and reward,
and we're gonna assume that our model approximately represents our,
our transition model and our reward model.
So we're assuming the Markov assumption here.
So we can represent our next state is
just the previous state and action in a distribution over that,
and we'll similarly have that for the- for the reward.
And we, we typically assume things are conditionally independent,
like what we've done before.
So we just have a particular dynamics model that's conditioned on
the state and action and a reward that is conditioned on the previous state and action.
And so if we wanted to do model learning,
then we have the supervised learning problem that we've
talked about a little bit before of,
uh, you have the state and action,
and you want to predict your reward in next state.
And so we have this regression problem and this density estimation problem,
then you can do it in all sorts of ways.
You can, uh, you know,
use mean squared error, you can use different forms of losses.
Um, and in fact,
one of the ways we've recently made progress on
our off-policy reinforcement learning is by using
different forms of losses than standard sort of maximum likelihood losses.
Uh, but generally here,
we're gonna talk about maximum likelihood losses.
So we can just do this, and of course,
in the- in the tabular case this is just [NOISE] counting.
So if you just have a discrete set of states and actions,
you can just count how many times did I start in this state and action,
and go to state one,
versus how many times they start in this state and action and go to state two.
And so you just count those up and then normalize.
And in general, there's a huge number of different ways that you can represent these.
Uh, and I think one of the ones that I think is particularly interesting is Bayesian,
not the- Bayesian Deep Neural Networks.
They've been pretty hard to tune so far.
Oh, another policy, you know,
Bayesian deep neural networks.
[NOISE] Um, I think one of the reasons those could be really
powerful is they can explicitly represent uncertainty,
but so far they've been pretty hard to train.
But I think that there's, you know,
a lot of really- there's some really simple models we can use as
well as some really rich function approximators for these models.
Okay. So if we're in the table lookup case,
we're just averaging counts.
So we're just counting as I said this state action,
next state tuples dividing by the number of times we've taken,
uh, that action in that state,
and we similarly just average all the rewards,
so this should be the reward scene,
for the times that we were in that state, took that action,
and what was the reward we got on that time step.
So let's think about an example for what that looks like here.
So a long time ago, we introduced this AB example,
where we have, um, a state that goes to the- a state A
that goes to action in state B, and then after that,
it either goes to a terminal state where it got a reward of
1 with 75% probability,
or it goes to a terminal state where it gets
a reward of 0 with 25% probability.
And imagine that we've experienced something in this world,
so there's no actions here, there's a single action.
It's really a Markov reward process rather than a decision process,
but we can get our observations.
So let's say, we start in state A,
and then we got a reward of 0,
and went to B and got a 0.
And then we had a whole set of times,
we have 6 times, where we started at B,
and we've got a reward of 1,
and then we got started in state B and got a reward of 0.
And now we can construct a table lookup model from this.
And just to refresh our memories, um,
so we talked about the fact that if you do temporal difference
learning in this problem with a tabular representation,
meaning one row for each state,
so that's just two states; A and B.
That if you do infinite replay on this set of experience that it's equivalent if
you took this data and estimated a Markov decision process model with it,
and then did planning with that to evaluate the optimal policy,
or the policy that you're using to gather the data in this case.
So that was an interesting equivalence that the TD is, um,
giving you exactly the same solution as what if you
compute what's often called a certainty equivalence model,
because you take your data,
you estimate, you take the empirical as average of that data.
So you can say, "If this was all the data in the world,
what would be the model that could be associated with that,
with a maximum likelihood estimate." And then we do planning.
So TD makes that assumption.
Let's just do a quick check of memory.
Do Monte-Carlo methods converge to the same solution on this data?
So maybe take a minute, turn to somebody next to you,
and decide whether or not they do, and why or why not.
Do you have a question?
Uh, as an offering, yes.
[LAUGHTER].
Okay.
I think that Monte-Carlo methods will converge to solution with
the minimum MSE's opposed to have MLE effect?
Correct. The Monte-Carlo methods do not make an assumption of Markovian.
Um, so the- they are suitable in cases where the domain is not Markovian.
So in this case, they will converge to the- well in all cases for this policy evaluation.
They're gonna converge to the minimum mean squared error.
Yeah, question?
So you're saying that if you used an ML model you probably converge them into MSE,
what if you are using a different loss? [OVERLAPPING]
Good question. This is- this is- this is going to
converge to the minimum MSE not the MLE.
[inaudible] If you are using a different loss [inaudible].
Would the Monte Carlo methods converge to the-
I mean, depending on the loss or if you regularize.
It's a great question, if you regularize it may converge to
different solutions than the minimum mean squared error
depending on how you regularize or the loss you use.
But in general, it will not converge to the same thing as if
you got the maximum likelihood estimate model,
and then did planning with that or policy evaluation.
And the key difference is Monte Carlo is not making a Markovian assumption.
So it does- it does not assume Markov.
And so in particular in this case, um,
because I may have a guess of what the value of A will be under the Monte Carlo estimate.
There's only one sample of it.
0.
Yeah, um, yes. So there's only- for Monte Carlo here,
we'll say I- I'm only looking at full returns that started with this particular state,
and there's no bootstrapping.
So, um, the only time we saw A was when the return from A was 0.
Uh, but, you know,
if the system is really Markov,
that's not a very good solution because we have all this other evidence
that B is actually normally has a higher value,
and we're not able to take advantage of that,
um, whereas TD does.
So TD can say, "Well,
I know that V of A was 0 this one time."
But in general, we think that V of A is equal to the immediate reward plus,
in this case there's no discounting, so value of B.
And I have all this other- other evidence that the value of B is, in this case,
actually exactly equal to 0.75, um,
because we have six examples of it being 1,
and two examples of it being 0.
So we would, uh,
have V of B is equal to 0.75,
and both Monte Carlo and,
um, TD would agree on that.
Because for- if you look at every time you started on B,
75% or how- you know,
75% of time you got a 1,
the rest of the time you got a 0.
So the- the value of that is 0.75.
So the TD estimate would say also V of A is equal to 0.75, the TD estimate.
So one of the reasons this comes up here,
a- and notice this is a- this is not due to a sort of finite number of backup,
or sorry, I'll be careful, a finite amount of use of the data.
So this is saying, if you sort of run this through TD many,
many times, and the Monte Carlo estimate is also getting access to all of the data.
It's just saying this is all the data there is.
So an alternative would be,
if you take this data and you build a model.
So now we have a model that says,
the probability of going to B given you started in A is 1,
you always go from- from, um, A to B.
In fact, you've only ever seen this once,
but the one time you saw it,
you went to B, uh,
and we can use this to try to get simulated data.
So let me just- well, I'll go a couple more.
So the idea in this case is that once you have your simulator,
you can use it to simulate samples,
and then you can plan using that simulated data.
Now, initially, that might sound like why would you do that
because you hide your previous data and maybe you could have directly, you know,
put it through a model-free based approach,
like why would you first build a model,
and then generate data from it.
But we'll see an example right now from
that sort of AB example of why that might be beneficial.
So you- what we can do is we can get this- we can get
the maximum likelihood estimate of the model or other estimates you might want to use,
and then you can sample from it.
So in that example we just had here, our estimated transition model,
is that whenever we're in A we go to B.
So when I'm in A,
I can sample and I will go to B,
and that generates me a fake data point.
Okay? And I could do this a whole bunch of times,
we get lots of fake data.
Now, this fake data may or may not look like what the real-world does,
it depends how good my model is.
But it's certainly data I can train with,
and- and we'll see, in a minute,
in a second why that's beneficial.
Okay? So if we go back to here,
this is the real experience on the left.
So on the left-hand side we had all this real experience,
and then what we did is we built a model from that,
and then we could sample from it.
So we could have experience that looks very
similar to the data that we've actually seen,
but we could also have experience like this.
Now, why could we have that, because we now have a simulator, and, uh,
in our simulated- In our model,
we've seen cases where we started in A and we went to B.
And in our model there have
been other times where we've started in B, and we've got a 1.
So essentially we can kind of chain that experience together,
and simulate something that we never observed in the real world.
And we're leveraging the fact here that it's Markov.
So if the domain isn't really Markov,
we could end up getting data that looks very
very different than what you could ever get in the real world.
But if it's Markov, then, um,
it may still be an approximate model because
we only had a limited amount of data training our model.
But now we can start to see conjunctions of states and actions,
uh, that we maybe never saw in our data.
But we could update [NOISE] our model as we sample?
Uh, well, okay great question.
Could you update your model as you sample?
You could, but right now we're just sampling from our model.
So this, this is not real-world experience.
So that could lead to confirmation bias,
because it's like your model is giving you data,
and if you treat that as real data,
and put that like into your model.
It's not from the real world.
So you could end up sort of being overly confident, in,
um, uh, because you're generating fake data and then treating it as if it's real.
How do we judge how confident we would be in our sample's experience?
I guess like relative to how much training data we'd have to put in the model.
Exactly. So that's like, how would be,
how do we know how confident to be and,
And in general this is the issue of your models are gonna be pretty bad.
Sometimes if you have limited amounts of data.
So some of the techniques we talked about for exploration, where we could, uh,
drive these confidence intervals over how good the models are,
they apply here as well.
So, um, if you only have a little bit of data you can use things like Hoeffding to say,
how sure am I about this reward model for example.
Um, for most of today,
we're not gonna talk about that that much,
but you can use that information to try to quantify
how uncertain should you be, and how would that error kind of propagate.
Yeah.
Um, so I guess I'm trying to like conceptually think about the next step is that we're,
we're building this model.
We're gonna use a method to learn some sort of a policy or some sort of like,
way to act in the real world.
If we have the model,
can we just used a model when you are acting and
just basically run our state through the model and get,
maybe like a distribution and just take the maximum action,
the m- The action that maximizes our reward?
So, I guess, once you have the model
you could use it in lots of different ways to do planning.
So one is you could do,
if it's a small case, like here it's a table.
So you could use value iteration and solve this exactly.
There's no reason to simulate data.
Um, but when you start to think about doing like
Atari or other really high-dimensional problems,
the planning problem alone is super expensive.
And so you might still want to do
model-free methods for your planning with your simulated data.
And one of the reasons you might want to do that is because, um, we've,
we've talked about different ways in Q learning to be more sample
efficient like you have a replay buffer and you can do episodic replay.
But another alternative is you just generate a lot of data from your simulated model,
and then you replay over that a ton of times.
And so that's another way to kind of, um,
make more use out of your data. Yeah, question in the back.
If, um, if you don't want to make the Markov assumption,
can you so do the same but, uh,
condition on the past of [inaudible]?
Yeah. Question, what if you don't wanna make the Markov assumption?
Yes, and can you condition on the past, you absolutely can.
Um, that means you would build models that are a full function of the history.
The problem with that is, you don't have very much data.
So you have to, if you want to condition on the entire history as essentially your state,
you're always fine in terms of the Markov assumption,
but you'll just have really really little data to estimate the transition models.
Particularly as the horizon goes on.
So it's often a trade off, like do you want to have better models?
Well, it depends on your domain. Maybe it's really Markov.
If it's not really Markov,
do you want better models with very little data?
So, um, in general this sort of gets to
the function approximation error versus the sample estimation error.
You can have error due to the fact that you don't have much data.
Or you can have error due to the fact that your function approximation is wrong.
And often you're going to want to trade off between those two.
So, so this example, you know,
you can end up getting this sampled experience
that looks different than anything you've seen in reality.
Um, and then in this case if you now run Monte Carlo on the new data,
you may, you can get something that looks very
similar if you've done TD on the original data.
So basically, instead of taking our real experience and then
doing Monte Carlo using that for policy evaluation.
An alternative is to say,
we really think that this is a Markov system, let's simulate a bunch of
data and then you could run Monte Carlo learning on this, or TD learning on it.
Um, and you would get probably the same answer.
So this is, you know, in contrast to what Monte Carlo would've converged to
before which was V(A) = 0 and V(B) = 0.75.
Now again you might say, okay but do I really want to do this, like if,
I, if I really didn't think the system was Markov,
I wouldn't have run Monte Carlo on my fake data either.
But I think this illustrates, um,
what you can do once you have this sampling and just
shows that allows you to make all sorts of choices.
So maybe there you wanna have
sort of a two-step Markov process or you want to do different,
make different assumptions in your model.
And then after that you wanna do a lot of planning with it.
So that just allows you to first take your data, compute a model,
and then you can decide how you're going to use that to try to do planning.
And we'll see a particular way to do that shortly.
Now as you guys were just asking me about, um,
if we have a bad model then we're gonna compute a sub-optimal policy in general.
You know, we might be incredibly lucky.
Um, because ultimately for your decisions,
we only need to decide whether,
you know, V(s,a1) is greater than V(s,a2).
So ultimately we're gonna be making comparison, pairwise comparison decisions.
So you might be, have a really bad model and still end up with a good policy.
Um, but in general,
if your model is bad and you do planning in
general, your policy is also gonna be sub-optimal.
Um, and so one approach if the model is wrong,
um, is to do model-free reinforcement
learning on the original data.
So not to do model-based planning.
It's not clear that always solves the issue.
So it depends why your model is wrong.
Um, if your model is wrong because you've picked
the wrong parametric class or because the system is not Markov,
you're doing Q learning that's not gonna solve your problem
because Q learning also assumes the world is Markov.
So model-free, you know,
it depends on why, you know, why is it wrong?
It depends on why? Whether or not this helps.
Um, another is to reason explicitly about your model uncertainty.
And this is goes back to the exploration, exploitation.
Now again this only deals with a particular form of wrongness.
Um, it deals with the fact that you might have sampling estimation error.
But it's still making the basic assumption that your model class is correct.
So for example, if you're modeling the world as, um,
a multinomial distribution, and you don't have
very much data then your prior metric estimates will be off.
But if the world really isn't a multinomial,
um, then all bets are off.
So it's always good to know sort of what assumptions we're making in
our algorithms and then what sort of forms of uncertainty we're accounting for.
Now I guess I'll just say, say one other thing which is a
little bit subtle which I find super interesting which is,
um, if you have a really good model,
it is, a generally said it or if you have a perfect model,
and perfect estimate of the parameters, that is sufficient to make good decisions.
If you were trying to train your model and you have
a restricted amount of data or restricted sort of expressivity of your model,
um, then a model that has higher predictive accuracy can
in fact be worse when you use it to make decisions.
And the intuition I like to think of is this, we,
we discovered this a few years ago, um,
others have discovered it too,
we were thinking about it for an intelligent tutoring system.
Um, the challenges, let's imagine that you have like a really complicated state-space.
Say, um, you're trying to model what a kitchen looks like when someone's making tea,
and there's all sorts of features,
you know, like there is steam,
maybe it's a sunset outside or when there's also the temperature of the water.
And in order to make tea you need the water to be over 100 degrees.
And in fact that's the only feature you need to pay
attention to in order to successfully make tea.
But if you were trying to just do, um,
build a model of the world,
you're trying to model the sunset,
you're trying to model the steam etc.
And there can be a huge number of features that you're trying to capture in your sort of,
you know, maybe slightly improvisional network.
And so if you just try to fit the best maximum-likelihood estimate model,
it might not be the one that captures the features that you need to make decisions.
And so models that look better in terms of
predictive accuracy may not always be better in terms of decision making.
So that was this issue we encountered a few years ago,
and I think it just illustrates why the models that we need to build when we want to make
decisions are not necessarily the same models we need for predictive accuracy.
So it's important where possible to know which of
your features do you care about in terms of utility and value.
Okay. So let's talk a little bit about simulation-based search.
Um, who here has seen forward search before in one of their classes?
Okay, a number people, but not everybody.
Um, so forward search algorithms.
What we're going to talk about now is different ways instead of
doing Q learning on our simulated data.
Okay, we've got a model, what's another way we can use it to try to make decisions?
One way is to do forward search.
So how does forward search work?
The idea in forward search is that we're gonna think about all the actions we could take.
So let's say here we just have two actions, A1 and A2.
And then we're going to think about all the possible next states we might get to.
So let's say it's a fairly small world,
so we just have S1 and S2.
So in this current state,
I could either take action one or action two and after I
take those actions I could either transition to state one or state two.
And then after I get to whatever I state- I get state,
I get to, I again can make a decision A1 or A2.
Because that's my action space here.
And then after I take that action,
I again my transition or maybe sometimes I terminate it.
So this is a terminal state.
Maybe my robot falls off or falls down or things like that,
or maybe else I go to another state.
[NOISE] And I can think of sort of like these branching set of futures,
and I can roll them out as much as I want to.
Let's say I want to think about the next h steps for example. And then I halt.
And once I have that,
I can use that information and the-
my sort of reward- well let me just say one more thing which is,
um, as I do these sort of like simulated features,
I can think about what the reward would I get under these different features.
Because right now I'm assuming I have a model.
So this is given a model.
So I can think about if I took this state as t and
took action a2 what reward would I get?
And then down here, I can think about if I got what reward I would get in s2, a2.
So I can think of sort of generating different features and then summing up the rewards,
um, it will give me a series of reward along any of those different paths.
And then in order to figure out the value of
these different sort of actions or the best action I should take,
what I can do is I can take a max over actions and I can take an expectation over states.
And I always know how to take that expectation because I'm assuming I have a model.
So I can always think about what would be the probability of me
getting to that particular state given my parent action and the state I'm coming from.
So what happens, in this case,
is as I roll out all these potential futures up until a certain depth.
In the case of Go or something like that,
it would be until you win the game or lose the game.
And then I want to back up.
So you can think of this [BACKGROUND] as sort of doing
a not very efficient form of dynamic programming [NOISE].
Because, why is it not so efficient?
Because there might be many states in here which are identical.
And I'm not- I'm not aliasing those or I'm not treating them as identical.
I'm saying I'm going to separately think about
for each of those dates I would reach what would be
the future reward I would get from that state under
different sequences of actions and resulting states.
And then if I want to figure out how to act,
I go from my leaves and I take a max over all the,
so let's say in this case I just made up for like a symbol small one.
This is a1, a2, and let's say at that point I terminate.
So I got like r(s, a1) and r(s, a2).
So anytime I have a structure that looks like that,
what I do is I take a max and the reward here is equal to whichever of those was bigger.
Let's imagine that it was a2 that was bigger.
So if I want to compute I basically roll all of these out,
computing like getting a sample of the reward,
and the next state is I go all the way out.
And then to get the value at the root,
whenever I see two- two, like a series of action nodes,
I take a max over all of them,
which was just like in our Bellman backup we're taking a max
over the action that allows us to have the best future rewards.
And then whenever I get to states- I'll do another one here.
So let's say now I have two states s1 and s2,
one of them has this value s1 and this one has value s2.
And I want to figure out for this action,
what my new value is then this is going to be equal to the probability of s1 given.
Let's say I came from s0,
s0, a1 times V(s1) plus probability of s2,
s0, a1 times V(s2).
This is exactly like, uh,
um, when we do a Bellman backup,
that we think about all the next states I could get two under
the current action and current state times the value of each of those.
Does that make sense?
So we construct this tree out,
and then in order to compute the value,
we do one of two operations for either we take the max over the actions,
if it's, uh, um, an action nodes, or we take an expectation.
So these are called expecting max trees.
Some of you guys might have seen these before in AI.
Sometimes people often talk about minimax trees.
So if you're playing game theory where the other agent gets
to try to minimize your value and you get to maximize it.
This is very similar except for we're doing an expectation over
next states and a max over actions, okay?
And it's exactly like dynamic program but more inefficient.
But we're gonna see why we would want to do that in a second.
So does anyone have questions about this?
Okay. All right. So this is all- and the way we do this is that we have to have a model,
because if we don't have a model right now we can't, uh,
compute this expecting max exact- exactly because
we're using that we know- like we're only expanding two states here.
Um, and we- we- in order to figure
out how much weight we want to put on each of those two states,
we need to know the probability of getting to each of them.
And so that's where we- we're using the model here,
and we're using the model to get the rewards.
So simulation-based search is similar,
um, [NOISE] except for we're just going to simulate out with a model.
We're not going to compute all of these sort of,
um, exponentially growing number of futures.
Instead, we're just gonna say I'm gonna start here,
and I have a model and I need to have some policy here.
But let's say I have a policy pi,
and then I just use that.
So I look at my policy for the current state and it tells me something to do.
So I followed that action,
and then I go into my model and I sample in s prime.
So I look up my model and I say,
"What would be the next state, given that I was in
this particular state and took that action and I just simulate one next state?"
This is just like how we could have simulated data from our models before.
So let's say that got me to here,
which was state s1. And then I look up again.
I look up to my policy and I say, "What is the policy for s1?"
Let's say that's a2 and then I also- then I follow it down to here.
So just simulate out a trajectory.
Just following my policy,
simulating it out and I go until it terminates.
And that gives me one return of how good that policy is.
So in these sort of cases, we can just simulate complete trajectories with the model.
Uh, and once we have those you could do something like
model-free RL over those simulated trajectories,
which either can be Monte Carlo or it could be something like TD learning.
So if we think of this as sort of doing- like,
in order to do that simulation we need some sort of policy.
So you have to have some way to pick actions in our sort of
simulated world when we think about being in the state and picking an action,
how do we know what action to take?
We follow our current simulation policy.
So let's say we wanted to do effectively one step of policy improvement.
So you have a policy, you have your model,
and then you start off in a state and for each
of the possible actions you simulate out trajectories.
So this is like doing Monte Carlo rollouts.
So I started my state.
So this is, let's say I'm really in a state s_t,
and I will need to figure out what to do next.
So I start in that state s_t,
and then in my head before I take an action in the real world,
I think about all the actions I could take,
and then I just do roll outs from each of them under a behavior policy pi,
then I pick the max over those.
So I'm really in state s_t, and then in my brain,
I think about doing a_1,
and then I do lots of roll outs from that under my policy pi.
And then I do a_2 and do lots of roll outs out of that.
a_3, this is all in my head,
and that basically gives me now an estimate of Q s_t, a_1 under pi.
So it's as if I was to take this action then follow pi,
what would be my estimated Q function,
then I do that for each of the actions, and then I can take a max.
So this is sort of like doing one step of policy improvement,
because this is going to depend on whatever my simulation policy is, does that make sense?
So we have some existing simulation policy,
I haven't told you how we get it, and then we use it to simulate out experience.
Okay. So the question is whether or not we can
actually do better than one step of policy improvement,
because like how do we get these simulation policies?
Like, okay, if we had a simulation policy and it was good,
we could do this one step,
but how could we do this in a more general setting?
Well, the idea is that, um, if you have this model,
you could actually compute the optimal values by doing this Expectimax Tree.
So like I was in this state St,
and instead of just thinking about- so remember in simulation
based search we're just going to follow out one trajectory,
but in the Expectimax tree we can think of,
well, what if I did a1 or a2,
and after that, whether I went to S1 or S2,
and then what action should I do there?
And I can think of basically trying to compute
the optimal Q function under my approximate model for the current state, okay?
The problem with that is that this tree gets really big.
So in general, um, the size of the tree is going to scale at least with
S times A to the H. If H is the horizon,
because at each step, this is why it's not as efficient at dynamic programming,
at- at each step you're going to think about all the possible next states,
and then all the possible next actions.
And so this tree is growing exponentially with the horizon.
And if you think of something like AlphaGo,
um, that are playing, you know,
for a number of time steps before someone wins or loses,
this H might be somewhere between 50 to 200.
So if you have anything larger than an extremely small state space like one,
then this is not gonna be, not gonna be feasible, okay?
So the idea with a Monte Carlo Tree Search is that,
okay we'd like to do better.
In any case we need some sort of simulation policy if
we're going to do this at all, and we can't be as
computationally intractable as full Expectimax.
So how do we do something in between?
So- so the idea with Monte-Carlo Tree Search is
to try to kind of get the best of both worlds,
where what we'd really like is the Expectimax Tree where we
think about all possible futures and take a max over those,
um, but instead, we need to do that in a little bit more computationally tractable way,
and why might that be possible?
Well, let's think about this. If we have our initial starting state,
let's say this is our general tree, that's going to all of these nodes.
Some of these potential ways of acting might be really really bad.
So some of these that might be clear, very early,
like with very little amounts of data
that, or very little amounts of roll outs that in fact,
these are ways you would never want to play Go,
because you're going to immediately lose.
And so then, you don't need to bother sort of continuing to spend a lot of
computational effort fleshing out that tree when something else might look much better.
So that's kinda the intuition here,
is that what we're gonna do is we're going to get us, construct a partial search tree.
So we're going to start at the current state,
and we can sample actions in next state,
just like in simulation based search.
So maybe we first sample A1,
and then we sample S1 again,
and then we sample A2.
So we start off and we're kind of,
the very first round it looks exactly like simulation based search,
but the idea is that then we can do this multiple times and slowly fill out the tree.
So maybe next time we happen to sample A2,
and then maybe we sample S2,
and then sample A1, and so you can think of sort of slowly filling in this Expectimax Tree.
And in the limit, um,
you will fill in the entire Expectimax tree.
It's just that in practice you almost never will
because it's computationally intractable.
So what we're gonna do is do this, and we'll do this for,
sort of a number of simulation episodes,
each simulation episode can be thought of is you start at the root.
This is the root node and current state you're really at,
and then you roll out until you reach a terminal state or a horizon H,
and then you go back to the start state and you again make another trajectory.
And when you're done with all of this,
you can do the same thing you would do in Expectimax,
in that you're always gonna take a max over actions and an expectation over states.
You only will have filled in part of the tree,
so part of the tree might be missing.
So in order to do this,
there's two key aspects.
One is, what do you do in parts of the tree where you already have some data?
So like if you already have tried both actions in a state,
which action should you pick again,
and then what should you do when you reach like a node where there's
nothing else there or like there's only been one thing tried so far?
So this is often called the tree policy and the roll out policy.
The roll out policy is for when you get to a node where,
you know, you've only tried one thing,
or there's no more data, or you've never been there before.
So for example, maybe you sample a state that you'd never reached before,
and so that's now a new node,
and from then on you do a roll out policy.
We'll show an example of this in a second.
And then the idea is, when we're thinking about computing the Q function,
we're just gonna average over all rewards that are received from that node onwards.
This should seem a little bit weird,
because we're not talking about maxes anymore,
and we're not talking about doing- considering
explicitly like the expectation over states in a formal way,
we're just gonna average this.
The reason why this is okay is because, um,
we're gonna, sort of sample actions in a way such that over time,
we're gonna sample actions that look better much more,
and so we expect that, uh,
eventually, the distribution of data is
gonna converge to the true Q. Yeah.
Just to confirm, is it [inaudible] the simulation before, um,
there's different kind of averaging and moving parts because it seemed before we
were also doing a bunch of roll outs and then combining this,
so that part is still the same, yes?
Yes, great question, in many cases it's very similar though.
We're still gonna be sort of doing
a bunch of simulations where we're gonna be averaging them.
The question is, what is the policy we're using to do the, the, uh,
the roll outs is generally going to be changing for each roll out,
instead of being identical across all roll outs,
and then the way we are gonna average them is also different.
And really the key part I think is that, instead of using,
um, like when I said for, um,
simple Monte Carlo search,
that assumes that you fix a policy and get a whole bunch of roll outs from that policy,
just starting with different initial actions,
but then always following it.
When you do Monte Carlo Tree Search and you also do say k roll outs, generally,
the policy will be different for each of the k roll outs,
and that's on purpose so that you can hopefully get to a better policy.
So again, just to, and just to step back again,
what are, you know, what's the whole loop of happening here?
So what's happening in this case is like you have your agent,
is our robot and it's trying to figure out an action to take,
and then the real-world gives back a S prime and an R prime,
and then what we're talking about right now is everything it needs to do in its head,
like all these roll outs in order to decide the next action to take.
So it's going to do a whole bunch of planning before it takes its next action,
generally it may then throw that whole tree away,
and then the world is gonna give it a new state and
a new reward and then it's gonna do this whole process together again.
And so the key thing is that we need to decide what action to take next.
And we want to do so in a way that we're gonna get the best expected value,
given the information we have so far.
Now in general, um, in Monte Carlo Tree Search,
you might also have another step here where you might compute a new model.
So if you're doing this online,
you might take your most recent data,
retrain your model, given
that model rerun Monte Carlo Tree Search
and now decide what you're gonna do on the next time step.
All right. So the key thing is really this tree policy and roll out,
both of them make a difference.
Um, the roll out often is done randomly at least in the most basic vanilla version,
there's tons of extensions for,
for this and the key part is that tree policy.
So one of the really common ways to do this is called upper confidence tree search,
and this relates to what we're talking about for the last few weeks with exploration.
So the idea is, is that when we're rolling out, so let's say,
we're in s_t and we're using our simulated models to
think about the next actions and states we would be in.
So let's say, um, I get to state one.
And at this point, I want to, let's say,
I've taken a1 and a2 in the past and I've ended up with,
you know, I've done lots of roll outs from these and a number of roll outs from here.
And let's say, three times I got, let's say, I won the game.
So three 1s and I got one 1 and one 0.
These guys.
So, so the key is what,
what action should I take next time I encounter s1 when I'm doing, where I roll
out and the idea is to be optimistic with respect to the data you have so far.
So essentially, we're going to treat this as a bandit.
Uh, think of each decision point as its own bandit,
its own independent bandit and say, well,
if I've taken all the actions I could for my current state,
what is my average reward I got from each of those actions under
any roll out that I've taken this from that particular node in the graph and action,
and how many times did I take it?
So you can get your empirical average for that state one in the,
in the graph a1 plus a discount factor that
often looks like the number of times you've been in that particular node.
Okay. This is really a node in the graph and this is also for that node.
So we think about sort of,
every time I've been in that node what,
and taken that particular action,
what's the average reward I've gotten plus how many times if I done that?
And it just allows you to be, um, uses optimism again.
Says, well, when I've reached different parts of the graph
before using my simulated model, what things look good?
I'm gonna focus on making sure that that's the part of the tree that I flesh out more.
Because that's the one where I think I'm gonna get
likely reach policies that have higher value.
And so hopefully, that's gonna mean that I have le- need
less computation in order to compute a good policy.
Does that make sense? So like if you had an oracle,
that could tell you what the optimal policy was,
then, you would only need to fill in that part of the tree.
And what we're using here is we're saying well, given the data we've seen so far,
we, kind of, only, you know,
focus on the parts of the tree that seem like they're going to be
the ones that when we take our max over our actions,
are gonna be the ones that we end up, uh,
propagating the values back up to the root.
All right. So we've maintained an upper confidence bound over the reward of each arm,
um, using, sort of,
exactly the same thing as what we've done before.
And we're treating each of the nodes,
so each state node as a separate bandit.
And so that means essentially that, you know,
the next time we reach that same node in the tree,
we might do something different because the accounts will change.
TD, uh, uh, [inaudible] this is kind of like TD in,
in the sense that [NOISE] if you reward for the bandit problem,
the value of that,
of the, of the state or if the, um,
existing action pair or is it the reward for just that transition?
It's a great question. So, and what is the reward for this bandit?
We are gonna treat it as, um,
essentially the full roll out from that node,
um, because that's what we're averaging over and we're cou- doing counts.
It's not like TD in the sense that we're doing this per node.
So as I mentioned before, you know, you might have s1,
a1 appear in this part of the graph and s1 a1 appear over here,
and we're not combining their accounts.
We're treating every node as if it's totally distinct.
Even though often the model we'll be using to simulate will be Markov.
But in ter- in sort of the tree, you can do it,
it's mostly that it becomes a lot more complicated for implementation.
If you wanted to basically treat it as a graph instead of a tree.
Now, I think the other point that you're bringing up,
we'll come back to in a second which is,
is this a good idea [LAUGHTER] is,
is well, what are the limitations of the bandit setting?
So we'll come back to that in a second.
All right, so let's talk about this in the context of Go.
For those of you that haven't played Go or aren't too familiar with it.
It's at least 2,500 years old.
It's considered the classic hardest board game.
Um, and it's been known as a grand challenge task in AI for a very long period of time.
Um, [NOISE] just to remind ourselves, um,
this does not involve decision-making in
the case where the dynamics and reward model are unknown,
but the rules of Go are known.
The reward structure is known,
but it's an incredibly large search space.
So if we think about the combinatorics of the number of possible,
um, boards that you can see, it's, it's extremely large.
So uh, just briefly there's two different types of stones.
Um, most people probably know this.
Uh, and typically it's played on a 19
by 19 board though people have also thought about,
you know, some people play on smaller boards.
And you want to capture the most territory,
and it's a finite game because there's
a finite number of places to put stones on the board.
So it's a finite horizon [NOISE].
I'm gonna go through this part sort of briefly there's different ways to write down.
You can write down the reward function, um,
for this game in terms of,
uh, you know, different, you could do different features.
The simplest one is just to look at whether or not white wins or black wins on
the game and in that case it's
a very sparse reward signal and you only get reward at the very end.
You just have to play out all the way and see which ga- which,
uh, which person won.
And then your value function is essentially what's
the expected probability of you winning in the current state.
So how does this work if we do Monte Carlo evaluation?
So let's imagine this is your current board.
And you have a particular policy.
Then you play out against,
um, a stationary opponent is normally assumed.
And then, at the end you see your outcomes and maybe you won twice and you lost twice.
And so then, the value for this current start state is a half.
Okay, so how would we do Monte-Carlo tree search in this case?
So you start off and you have one single start state.
So at this point you haven't simulated any part of the tree.
And so you've never taken any action,
so you just sample randomly.
So maybe you take a1. And then you follow your default policy and this is often random.
Though for things like AlphaGo you often want much better policies but, um,
you can just use a random policy and you'll just take random actions and get to
next states and you do this until the end and you see you either win or lose.
Now, um, in this case it's a two player game.
So we do a minimax tree instead of expectimax,
but the basic ideas are exactly the same.
So that's my first roll out.
I'm gonna do lots of these roll outs before I
figure out how I'm going to actually place my piece.
All right. So the next time, so this is my second roll out.
This is the second roll out of my head [NOISE].
And say okay well, last time, um, you know, I, I took this action.
So now I have my default policy.
So this time I'm gonna take the other action.
Generally, you want to fill in all,
try all the actions at least once from a current node.
Now, the particular order in which you try actions can make a big difference and, um,
early on there were significant savings by putting in
action heuristics of what order to try actions in.
For right now let's just imagine you have to try all actions, um, equally.
So in this case, now, you take the other action and then
after that you've never tried anything from that action.
So you just do a roll out.
Again, just acting randomly.
Okay, and then, you repeat this.
So now, when you get to here,
so let's say I've tried this before.
So now, when I get to this node,
I have to pick, um,
I have to do a max maybe using my UCT.
So I look at what was the reward for this one and the reward for
this one, plus you know something about the counts.
All right. This is Q to make it clear.
Okay. And so I pick
whichever action happened to have looked better
in the roll outs I've done from it so far.
And then, I'm gonna focus on expanding that part of the tree.
And you keep doing this, and you're gonna slowly sort of build out
the tree and you do this until your computational budget expires.
And then, you go to the bottom of the tree and you go all the way back
up where for each of the action nodes you're taking a max.
And each of this state nodes you're doing an expecti- or you're doing a,
in this case minimax.
You just construct a [inaudible].
So, so what does the opponent do, is it like a stationary opponent, like, what does that mean?
Great question. Okay. So, um,
in reality, I think one of the other really big insights to why people got
Go to work is self play.
So typically in this, you would use the current agent as the opponent.
So I take whatever policy I just computed for the other.
It- Look, particularly, let's imagine that like, I kept that tree.
So it already knows what it could do.
So, um, at each point,
I would look at I would have
the other agent tell me what action it would do in that state.
But one of the really so,
so I think self play was an incredibly important, um, insight for this.
And why is it so important?
Because if I play against a grand-master in Go,
I get no reward for a really long period of time.
Um, and that's an incredibly hard thing for an agent to learn from,
because there's no other reward signal.
And so basically, you're just playing these tons and tons and tons of games and like,
there's just no signal for a really, really long time.
And so you need some sort of signal so you can
start to bootstrap and actually get to a good policy.
If I play against me like,
five minutes ago, I'm probably gonna beat them [LAUGHTER].
Um, and or at least half the time maybe I'll beat them.
And so that allows because you can have two players
that are both bad and one of them's
gonna win and one of them is gonna lose and you start to get signal.
And so the self play idea has been
hugely helpful in the context of games, like, two-player games.
Because it can mean that you can start to get
some reward signal about what things are successful or not and then you- It's both so,
like, it both gives you, helps with
this sparse reward problem and it gives you a curriculum.
Because you're always kind of,
only playing in an environment that's a little bit
more difficult than perhaps what you can tolerate, can manage.
And actually I think, um, it would be really,
really cool if we could figure out how to take the same ideas to a lot of other domains.
Like if there are other ways to essentially make self play for things like medicine or,
[LAUGHTER] um, uh, customer relations or things like that.
It would be really great because it's often
really hard to get the sort of reward signal you want.
And that's one of the really nice things here.
Self-play, what if we get stuck in some local extrema?
Absolutely can happen, yes. So what in self play, how does it arrive if you get stuck in,
you could but you always try to max.
So it's a little bit like policy improvement.
You're always trying to do a little bit better.
You're still trying to win. Um, so it's
possible you could get stuck in a case where you're both just,
you know, winning half the time,
but then there should be something that you can exploit.
And if there's something you could exploit,
if you do enough planning you should be able to identify it. Yeah.
Do you imagine that there would be this kind of transition point where you might get
added benefit from transitioning to a more expert player to play against versus yourself?
You need to kind of start slowly and ease into it but then you might actually
do learn faster by playing against somebody harder modes.
Yeah, question is, like, you know, would you also always wanna kinda just,
like, self play against, you know,
yourself five minutes ago or maybe at some point you- It will be
more efficient to go at someone harder. I think it's a great question.
I think probably that's the case.
Like, probably there will be cases where you could do bigger curriculum jumps,
um, and that might accelerate learning.
But I think it's a tricky sweet spot there if like,
you still need to have enough reward signal to bootstrap from.
Absolutely. All right.
So, you know, the benefits of doing this is it
becomes this highly selective best-first search,
because you're sort of constructing part of the tree but
you're constructing it in a very specific way.
And the goal is that you should be way more sample efficient than doing Expectimax,
making the whole tree but you're gonna be much better than
doing just a single step of policy improvement,
with some fixed, um,
you know, simulation-based policy.
And it's also, you know, parallelizable anytime,
anytime in the sense that like,
whether you have one minute or you have three hours to compute the next action.
And you know, three hours can be very realistic if it's something like, you know,
a customer recommendation article or a thing that's gonna make or maybe you're
gonna make one decision per day and so you can run it
overnight for eight hours and then compute that one decision.
So, um, it allows you to,
to take advantage of the computation you have but then
always provide an answer no matter how quickly you need to do that,
cause you just do less roll outs.
Okay, um, I'm gonna skip this for now.
I just want to mention briefly,
um, I think it was a question.
It's a little weird that we do bandits in each of the nodes.
And intuitively, the reason it's a little bit weird is because in bandits,
why do we do optimism under uncertainty?
We do it because we're actually incurring the pain of making bad decisions.
And so the idea with optimism under uncertainty is that
either you really do get high reward or you learn something.
The weird thing about doing that for planning is
that we're not suffering if we make bad decisions in our head.
Um, essentially, we're just trying to figure out
what actions can I take as quickly as possible in terms of,
like, value of information so that I know what the right action is to get the route.
And so it doesn't actually matter if I simulate out bad actions.
If it allows me to get a better decision of the route.
We just give a really quick example of where that could be different.
So if you have something like this.
Let's say this is the potential value of Q.
Okay. So this is the value of A1 and this is the value of A2 and this is our uncertainty.
Well, if you're being optimistic, you're always gonna select this,
because it's got a higher value.
But if you wanna be really confident that A1 is better,
you should select A2 because likely when you do that,
you're gonna update your confidence intervals and now you're
gonna be totally sure that A1 is best.
But this approach won't do that.
Okay, because it's, it's, um,
it's like, no I'm gonna suffer the cost in my head
of taking the wrong action so I'm gonna take A1.
But if ultimately, you just need to know what the right action is to do,
then sometimes in terms of computation you should take A2 because now,
your confidence intervals will likely separate.
You don't need to do any more computation.
Um, so it's not clear that the bandits,
um, at each node is the optimal thing to do but it's pretty efficient.
All right. So that's basically all we're g- I'm gonna say about Go.
There's, um, some really beautiful papers,
uh, about this including the new recent extensions.
Um, and they have applications to chess as well as, you know,
a number of other games, uh,
which I think are really, they're, they're amazing results.
So I highly encourage you to look at some of those papers.
Let me just briefly talk about sort of popping back up to the end of the course,
um, because it's the last lecture. All right.
So I just wanted to sort of refresh on,
you know, what were the goals of the course, um,
as we end and some of these you guys had a chance to practice on, on Monday.
Um, but I just wanted to say
what I think of as kinda the key things that I hope you got out of it.
So then one is sort of, what is the key features of RL versus everything else?
Um, both AI and supervised learning.
And to me is that really this key, uh, issue of, um,
the agents being able to gather their own data and make decisions in the world.
And so it's the censored data
that's very different than the IID assumption for supervised learning.
And it's also very different from planning because you are
reliant on the data you get about the world in order to make decisions.
Um, this is probably- The second thing is
probably the thing that for many of you might end up being the most useful,
which is just how do you figure out given a problem, um,
whether you should even write it down as an RL problem and how to formulate it.
And we had you practice this a little bit on, uh,
Monday and also it's a chance to think about this a lot in some of your projects.
But I think this is often one of the really hard parts.
It has huge implications for how easy or hard it is to solve the problem.
Um, and it's often unclear.
There's lots of ways to write down the state space of describing
a patient or describing a student or describing a customer.
And in some ways it goes back to this issue
of function approximation versus sample efficiency.
If I treat all customers as the same, I have a lot of data.
That's probably a pretty bad model.
So there's a lot of different trade-offs that come up
in these cases and I'm sure all of you guys will,
um, think about interesting, exciting new ways to address that.
And then the other three things were, you know,
to be very familiar with a number of common RL algorithms,
which you guys have implemented a lot.
Um, to understand how we should even decide if an RL algorithm is good,
whether it's empirically, compu- you know,
in terms of its computational complexity or things like
how much data it takes or its performance guarantees,
um, and to understand this exploration exploitation challenge,
which really is quite unique to RL.
It doesn't come up in planning, doesn't come up in ML.
Um, and again, it's this critical issue of like, how do you
gather data quickly in order to make good decisions.
Um, if you wanna learn more about reinforcement learning,
there's a bunch of other classes, uh, particularly,
Mykel Kochenderfer has some really nice ones.
And also Ben Van Roy does some nice ones
particularly looking at some of the more theoretical aspects of it.
And then I do an advanced survey of it where we
do current topics and it's a project-based class.
Um, and I'll just I guess I'll, um, two more things.
One is that I think, you know, we see some really amazing results,
uh, Go is one example.
Uh, and we're seeing-starting to see some really exciting results in
robotics but I think we're missing,
most of us do not have RL on our phone yet
in the way that we have face recognition on our phone.
And so I think that the potential of using these types of
ideas for a lot of other types of applications is still enormous.
Um, and so if you go off and you do some of that I would love to hear back about it.
Um, uh, in my lab we think a lot about these other forms of applications.
And I think another really critical aspect of this is thinking
about when we do these RL agents, um,
how do we do it in sort of safe,
fair and accountable ways because typically,
these systems are going to be part of,
you know, a human in the loop system.
And so allowing the agents to sort of expose their reasoning and expose,
um, their limitations will be critical.
So the final thing is that,
um, it's really helpful to get you guys this feedback.
Um, it allows us to improve the class for future years, um,
either to make sure we continue to do things that you found
helpful or that we stopped doing things that you didn't find helpful.
So I'd really appreciate it if we could take about 10 minutes now
to go through the course evaluations, um,
and just feed it, uh, let us know what helped you learn,
what things we could do even better next year.
Thanks.