 Okay. Let's get started.
Uh, so hi everyone.
Welcome to CS330, if you're
expecting to be in a different class, then you're probably in the wrong place.
Uh, I'm Chelsea Finn.
I'm a Professor in the Computer Science Department.
Uh, yeah.
So let's get started. So first I'm gonna go over
course logistics and then we'll go over a little bit of content,
uh, to get started on some,
some aspects for motivation for why we care about multi-task learning and meta-learning.
Um, so first for logistics,
uh, some information and resources.
So uh, this is the full staff for the course.
Um, we have four TAs.
I think one or two of them are here today.
Uh, maybe you can kind of stand up and introduce yourself.
Hey, I'm-
And then there's- great.
So those are the first and last one- able to make it today.
Um, but there are also TAs for the course as well.
Uh, They'll be great resources for you as you go about taking the course.
Um, the course website is shown here
and there's a lot of information on the course website,
uh, beyond what we'll be kind of covering in the logistics here.
Uh, also we have a Piazza, this will be for,
for questions as, as they come up in the course.
This is the staff mailing list as well.
Um, and each of us will be holding one hour of office hours per week.
My office hours are Wednesday after class and
the other office hours will be posted on the course website.
We'll have office hours for the first time on Wednesday this week.
Great. Um, so prerequisites and enrollment.
Uh, the main prerequisite is something- is machine learning experience,
basically covering machine learning and reinforcement learning.
Uh, CS229 equivalent.
we highly recommend having some previous reinforcement learning experience because
a num- large portion of the course will include topics in reinforcement learning.
Uh, if you're not currently enrolled,
please fill out the enrollment form on the website.
Uh, and we will, ah, kind of go through that.
We do still have some open spots in the course.
Uh, and we'll, we'll go through that probably around Tuesday or Wednesday this week.
So please fill it out as soon as possible if you're not enrolled.
Um, yeah, and then if,
if you fill out the form and didn't receive a provision number yet,
it's either because you filled out the form very recently, or because, um,
we weren't quite sure if you had the kind of
necessary experience in order to succeed in this class.
Um, then we also we'll get back to everyone by,
Wednesday this week.
Okay, um, all the lectures are being recorded.
Ah, and the lectures will be
internally released on Canvas as soon as the recording is available.
Um, they'll also be publicly released after
the course for the benefit of kind of the general public.
Um, despite this, kind of things,
like student anonymity will be preserved in the recording.
Ah, and so please still feel free to ask questions,
ah, throughout the course as well.
There are around 20 remote students from
the Stanford Center for Professional Development that
are joining the course as well remotely.
Um, and SCPD is actually the,
the folks that will be, uh, allowing,
for us to release all the videos
online after the course by providing captioning services and other things.
Um, so if you have any, any questions about anything up here?
Okay. Assignments. So the assignments
will all require training neural networks using TensorFlow.
Uh, and so if you're not familiar with TensorFlow already,
uh, we'll be holding a review session on Thursday of this week.
Uh, and basically you should be able to understand kind of an overview of TensorFlow.
And if you don't understand an overview of TensorFlow,
I'd highly encourage you to go to the review session and ask plenty of questions.
Um, this is kind of your opportunity to,
to get information about how to use TensorFlow if you aren't familiar with it already.
Um, topics that we're covering in this course.
So the types of topics,
these are all listed, all the kind of lecture topics are,
are listed on the course website,
it's going to include things like what is
multi-task learning, uh, and meta-learning,
uh, the basics of these algorithms, uh,
including various approaches, ranging from black-box approaches to optimization
based approaches, metric learning methods, uh, as well.
Uh, how we can view these types of methods in a Hierarchical Bayesian framework,
uh, as kind of learning priors over multiple tasks and using those priors.
Uh, topics like multi-task reinforcement learning,
goal condition reinforcement learning,
hierarchical reinforcement learning, uh,
topics and meta-reinforcement learning,
uh, as well as some open problems, uh,
invited lectures and research talks from,
uh, faculty in the field.
Uh, one thing worth noting here is that there'll be an emphasis on deep learning, uh,
as kind of indicated by the title of
the course as well as an emphasis on reinforcement learning.
So, uh, a little under-half of the course is going
to be kind of focusing on different topics and reinforcement learning because,
uh, that is where some of these,
some of these types of techniques become
a lot more interesting and a lot more challenging.
Um, topics we won't cover, ah,
due to kind of the constraints of,
of how much time we have in this course,
we won't cover topics in AutoML.
Uh, this includes things like architecture search,
like hyper-parameter optimization and learning optimizers.
Uh, but beyond that,
I think we'll be covering most topics in meta-learning and multitask learning.
Uh, but we'll also have an emphasis on deep learning approaches so we
won't be cove- like thoroughly covering things in,
ah, like things without neural networks for example.
And I will kind of describe why,
uh, later in this lecture.
Okay, um, so the course format.
So we'll have three types of course sessions.
Ah, we'll have nine lectures,
uh, that are kind of given by me.
Uh, we'll have seven student reading sessions,
which will consist of presentations by
you presenting different papers and we'll have a discussion of different papers,
uh, in the, uh,
multitask learning and meta learning literature.
Uh, and then we'll also have three guest lectures.
All students that are enrolled in the course are responsible
for giving one group presentation of a paper.
Uh, and in these kind of student reading sessions we'll be presenting
four papers and the papers are all posted on the course website.
Um, and also further instructions on how to actually
prepare your presentation will also- is also posted on Piazza.
Uh, and beyond even if you're not presenting a paper,
we highly encourage, uh,
you to actually to participate in the discussion of the paper and, and come to class.
So while the courses are being recorded, uh,
I think that you actually get a lot more from the course,
uh, if you actually actively participate in,
um, various discussions of papers to better understand the literature
and kind of dig into why these papers are good or,
or what their limitations are.
Okay, um, and also this will change in future offerings.
So, uh, in future offerings,
we'll probably be introducing more lectures into
the course and fewer student reading sessions,
um, as, as years go on.
Okay. More details on assignments.
So we're gonna have three homework assignments.
The first homework will cover things ranging from
multitask data processing and blackbox meta-learning methods.
You'll actually be implementing, um,
how to actually kind of go about processing data in a multi-task fashion
such that you can apply these types of algorithms to them and also implementing,
uh, a black-box meta-learning algorithm.
In Homework 2, you'll be implementing, ah,
gradient-based meta-learning algorithms as well as metric learning algorithms.
Uh, and then Homework 3, uh,
that will be on things like
goal condition, reinforcement learning and multitask reinforcement learning.
Uh, so you'll get the opportunity to play around with
all these algorithms yourself, uh, and,
and see how well they work on various domains including image classification domains,
uh, and uh, simulated robotic control domains.
Uh, and then, lastly,
we'll have a final project.
Uh, so this is gonna be a research level project of your choice.
Uh, we encourage you to kind of integrate
any kind of research that you're doing into this, uh,
into this research pro- project, uh, for the course,
as long as it's on the topics that are, uh,
kind of pertinent to the course,
things like multitask learning and meta learning.
Uh, and for this you'll be allowed to form groups of 1-3 students,
uh, and you're welcome to start early.
So you're welcome to come start forming groups now and start thinking about what,
um, what you actually want to do for your final project.
Uh, for grading, it'll be 20% on your paper presentation which is in a group,
30% on homework,
10% each as well as 50% on the final project.
Okay, um, and then you're also given five late days across homeworks and
your project paper submission that you can kind of
allocate as you see fit depending on things coming up.
Great, and then the last thing, um, oh, yes,
one more thing about the project is that we'll also
be posting kind of guidelines for the project and
will have various milestones throughout the course for you to propose your project,
kind of, ah, I think like
communicate progress that you've made on your project
and then of course present the final project.
Um, the things that you need to do today,
um, are sign-up for Piazza if you haven't already.
Um, fill out your paper presentation preferences so
I- we have seven paper presentations with four papers each.
So fill out your preferences for which of
those 28 papers you're most interested in presenting.
Um, do this by Thursday so that we can try to get, uh, kind of assign,
assign those as quickly as possible especially for
the first presentation which will be happening next,
happening next week on Wednesday.
Um, and so these-
the instructions for the presentations and also how to sign up is on Piazza.
And then lastly, uh,
start forming final project groups.
If you want to work in a group and if you're not familiar with TensorFlow review, um,
review the TensorFlow intro or go to the review session on Thursday.
Okay. Anything else about logistics?
Any questions? Yeah.
Could this be done in PyTorch?
So we we'll be providing the,
a lot of infrastructure around the,
um, around these kind of different problems in TensorFlow.
If you want to do everything in PyTorch,
ah, and kind of produce the deliverables that, um,
the kind of the curves that you need to deliver and,
and produce as part of your write up of the assignment,
that is, uh, that's fine, that's completely fine.
So if you want to only use PyTorch, uh,
you can do that but it will probably involve writing
a fair amount of code in addition to
the code that you'd have to write in TensorFlow because of the,
the infrastructure being written in TensorFlow. Yeah.
Are all homeworks runnable on like laptops?
Would it require a GPU?
So the first homework will be runnable on a laptop,
the second homework will um,
will be a little bit more computer heavy and we're still looking into various um,
various cloud compute options for that,
that we'll be able to provide for people that don't have, um, GPUs,
for example, to be able to run their code on.
And the third one will probably be runnable on a laptop as well.
Uh, but we're still finalizing that, uh, assignment. Yep.
Will we be using TensorFlow 2 or 1.14?
We'll be using TensorFlow but not TensorFlow 2.
[LAUGHTER]. Yeah.
What's the policy on showing final projects [inaudible].
Um, that's a good question.
Let me get back to you on that.
I'll post- we'll post that on the project guidelines,
um, on kind of what we allow in that regard.
Okay. Um, two more things.
Uh, first ask plenty of questions.
You guys are off to a great start.
Uh and uh, I mean, this,
this course is really for your purpose not for, not for my purpose.
Is to sit here- to stand here and, and, and talk.
And so if there are things that are unclear,
things that you're kind of curious to learn more about,
please, raise your hand and ask questions.
Uh, and second, this course is new,
uh it's the first time that,
uh, that we're offering it.
Uh, and so, as a result,
the course will likely be rough around the edges.
Uh, in comparison to courses that are run every single quarter.
Um, and so please bear with me and the,
and the rest of the course staff.
Uh, and this is kind of all the more reason for you to ask plenty of questions,
as we, uh, kind of figure out how the course is going to be run throughout the quarter.
Okay. Great. So, um,
now I'd like to talk about, well,
like why should we actually care about multi-task learning and meta-learning and,
and what are the sorts of things that you'll actually be learning in this course?
And what I would like to start,
uh, this discussion with,
is actually talk about some of my own research
and some of the reasons why I care about multi-task learning and meta-learning.
And then I'll talk about kind of bro- more
broadly where these types of algorithms
are being used and why they're really important and a,
and a really fundamental aspect of machine learning research.
Okay. So, um, one of the questions, uh,
in my research group that we like to think about,
is how can we enable agents to learn skills in the real world?
Uh, and what I mean by the real world,
is I mean robots.
Uh, robots like, like these.
Robots that can use tools to lift up objects and put them in the bowls,
that can play with children's toys, uh,
they can watch, uh,
kind of a video of a human doing something and learn from, from that.
Uh, and then you may ask, "Well, why robots?"
That seems like robots might be a lot of work to, to deal with.
Uh, and I think that the reason why studying robots is really
exciting is that robots can teach us things about intelligence.
I know, okay, this might sound a little silly also because robots are not very smart.
Uh, but I think that robots are,
are faced with many of the challenges that humans are faced with, uh,
as they learn, uh,
and develop over the course of their lifetime.
Robots are faced with the real world and they have to deal with it.
Uh, robots have to be able to generalize across tasks,
across objects, across environments,
in order to be successful in real world settings.
Uh, they need some sort of common sense understanding in order to do well.
And lastly, uh, supervision can't really readily be taken for granted.
Um, it's not easy to just provide labels or
even figure out what labels mean in the context of getting a robot to do something.
Uh, so I think that if you can build intelligent robots in the real world,
uh, you can kind of convince yourself that you've solved
some important problems regarding intelligence.
Though, of co- of course, my goal in
this lecture isn't to convince you to work on robots.
Uh, my goal is instead to kind of discuss some of the challenges that come up and,
uh, and kind of guide this towards multi-task learning and meta-learning,
and to kind of describe where this,
um, where this fits in.
So I will start with a bit of a story here.
So, uh, at the beginning of my PhD, uh,
like five or so years ago,
um, I was, uh,
in a lab at Berkeley.
And, uh, I- there was this research project that was going on,
oops, the video is not playing. Here we go.
There's a research, research project going on,
trying to get the robot to learn how to assemble this toy airplane,
by putting the wheels into,
uh, the correct position.
And the robot here is, uh,
is learning how to do the task, uh, through trial and error.
At the beginning, the robot has kind of started with very random motions,
uh, and over time is able to kinda figure out how to do the task successfully.
Uh, so I thought this is, this is really cool.
Uh, the robot is actually kind of,
you can see it kind of learning really,
uh, this is 20x real time.
But if you kind of watch the robot in real time,
you'd actually be able to see the robot learning in real time,
um, through trial and error.
And I guess what's really exciting about this, is the robot isn't,
isn't just kind of- this isn't just an algorithm for learning how to do that task,
it's an algorithm for- that can do really a wide range of manipulation tasks.
Uh, and so in principle, this algorithm could be applied to a wide range of settings.
Uh, now, one of the things that was a little bit disappointing about this is that, uh,
it turns out the robots, uh,
the robot essentially had its eyes closed.
It was doing this, uh, completely blind.
Uh, and just kind of essentially blindly trying to get the,
uh, get the wheels into the right place.
Uh, and so kind of this motivated some of the work
that I did at the beginning of my PhD where we wanted to get the robot to do things,
uh, while also seeing.
Uh, do things that required, uh, vision.
In this case, it needed to actually see to figure out where are
the shape-sorting cube was in order to figure out where to, where to place it.
Um, again, here you can see it kind of learning over time starting, uh,
from scratch figuring out how to get the,
the block into the respective hole.
Um, and so, uh, I guess one thing that I mentioned here,
is the kind of the reason why these sorts of algorithms I think are really exciting,
is that the same sort of algorithm,
the same reinforced learning algorithm,
can be applied to many different tasks.
So you can't, uh, you don't just learn how to
insert the block into the shape sorting cube which it,
it kind of eventually figures out here.
Um, and here's actually the kind of
the final policy where I'm pulling the cube in front of the robot.
Um, it can also be applied to learn things like placing
the claw over to a hammer underneath a nail or,
uh, screwing a cap onto a bottle.
Um, or, uh, kind of
the most challenging thing that we got to do with this particular algorithm,
was use the spatula to lift up an object into a bowl.
Uh, and this was actually really
challenging, because you see the robot has to be fairly
aggressive in maneuvering a spatula underneath the object.
Um, so this was really exciting.
Uh, at least we're really excited about some of these results.
Uh, and there were- a range of other people that built upon that method as well.
Getting robots to do other tasks like,
uh, using a hockey stick to,
to hit a puck into a goal,
opening a door, uh, throwing objects.
Uh, and so while it kind of seems like this is pretty exciting and,
and indeed at the time it was very exciting,
um, we have a bit of an issue.
Uh, the issue is that
the robot didn't learn how to use spatulas to lift objects into bowls.
It learned how to use that spatula to lift that object into that bowl.
And if you give it a different bowl or
a different spatula or even a different table cloth in the background,
the policy wouldn't generalize, the policy would fail.
Because it was trained in exactly that environment.
So this doesn't necessarily seem like that much of an issue.
So you could just say, "Well, why don't we just give the robot more spatulas,
uh, and, and more bowls and more
tablecloths and have it just learn in, in more settings?"
Um, and this maybe this is kind of an easy problem to solve.
Uh, in many ways you'd be right. That would be kind of be
a reasonable approach to try to learn a more generalizable policy.
But the tricky thing is that this algorithm was designed kind of
with the intention of it being a single task learning algorithm in a single environment.
Uh, and so let's kind of look at this.
So, uh, behind the scenes,
if you look at one of these algorithms, um,
especially algorithms that kind of required moving objects in different ways,
this is kind of what the learning process looks like.
Behind the scenes you- the robot kind of tries to do the task and then human
comes in and [LAUGHTER] puts the puck back.
Uh, and so by,
by nature of kind of focusing on a single task learning problem,
the methods aren't actually scalable to learning more tasks,
without actually starting again from scratch.
Uh, and having, uh,
my friend Yevgen here,
kind of repeatedly reset the robot,
uh, after each trial.
Uh, and so Yevgen here is doing more,
more work than the robot arguably.
Uh, and it's also just kind of more
generally it's not practical to collect a lot of data this way.
Uh, so to build algorithms that are effective,
at learning across many different tasks,
we need to in many ways fundamentally rethink how we're actually,
uh, kind of designing these algorithms in the first place.
Um, so kind of the issue here was that it relied on, uh,
very detailed supervision,
very detailed guidance that can't be scaled to many different tasks.
Um, and this is not just a problem with robot learning,
it's also a problem with kind of standard reinforcement learning algorithms as well.
Uh, kind of things that learn how to play Atari games and locomotion,
they require a lot and a lot of data,
a lot of supervision in the form of reward functions,
um, in order to learn effectively.
And it's also not just a problem with reinforcement learning.
So, um, if you think about some of the, uh, uh,
biggest successes in supervised learning systems, are these systems,
are able to handle much more diverse data and
learn across different users, different languages.
Uh, but in many ways,
these systems are still learning one task starting from
scratch with very detailed supervision.
So essentially these systems are what I would call specialists
in that they're trained for kind of in a single task learning setting.
Um, instead if we kind of look about, think about well,
how could we get a, get a more generalist system?
Um, that's kind of in,
in many ways part of what this course is about in some ways.
Uh, and kind of it one
way to get inspiration for this is we could look at kind of how humans learn.
Uh, humans don't learn in the settings that I mentioned before.
Humans learn kind of by rolling around on the floor,
by interacting in kind of a very rich and diverse environment.
[NOISE] Uh, humans are,
are what I would call generalists.
Humans, uh, learn many simple skills such as crawling,
picking up objects, playing with toys,
before trying to learn much more complex tasks.
Uh, and perhaps to build machine learning systems that are generalists,
we need them to look a bit more like this.
Systems that can learn- can build upon previous experience and learn new things more
quickly and learn many simple things before trying to learn more complex things.
Um, and so if kind of if we make an analogy to, uh,
the kind of the way that machine learning systems learn uh, to how
humans learn this would kind of be like if we want to train a system to play Go,
uh, from scratch, this would be kind of like
trying to start from scratch and have a baby learn,
uh, how to play Go, for example.
Um, and so this seems a little bit sillier,
a little bit off, uh,
in some ways potentially.
Okay. Um, so that was- that's a bit why some of the reasons why I care about,
uh, multi-task learning, uh, and, and meta-learning.
Why should, uh, if you don't- even if you don't care about robots,
why should, why should we generally care about these algorithms?
Uh, and fortunately, there's a lot of reasons to care beyond
robots and beyond kind of trying to build more general purpose machine learning systems.
Um, and first I want to start by why should we
care about deep multi-task learning and meta-learning?
Why do we care about deep learning in particular?
Um, actually before I move on,
are there any questions on the part that I mentioned before on the robots?
All right. So, um,
if we kind of go back, uh,
at this point like, 10 years, uh,
the standard approach to computer vision looks something like this.
Uh, where you took some image.
Uh, you then try to extract some mid-level features-
some features of such- what were things called HOG features or SIFT features.
These are features that are designed by hand by,- by researchers.
Uh, and then on top of those features,
you may have more mid-level features like deformable part models, um,
kind of as visualized, uh, on the right,
and then on top of that you would have a little classifier,
uh, like a support vector machine on top of those features.
And now if you kind of fast-forward 10 years,
uh, until now basically,
what modern computer vision looks like,
uh, it's something that is more like this.
Something where you take an image and you pass it through
a neural network and have it produce the desired output.
Uh, and essentially kind of, one of the, uh,
most salient differences between these two things is that,
deep learning allows us to handle unstructured inputs.
It allows us to operate directly on the- the image pixels that are shown on the left.
Uh, it allows us to directly operate on things like language,
like sensor readings, uh,
without requiring hand engineered features like,
HOG or SIFT, or Deformable Part Models.
Um, and also with less domain knowledge.
So it means that we can kind of apply the single class of, uh,
of techniques of neural networks essentially to a wide range of problem domains.
Uh, and so- so that's one kind of benefit of- of deep learning systems.
And the second benefit of course is that they work really well,
uh, in, in, a variety of different situations.
So if you look at, uh,
results on the ImageNet dataset over the course of around five years,
uh, look at the error rate, um,
this dot right here is- is AlexNet, uh, in- in 2012.
This is, uh, the, uh,
in many ways one of the first neural networks that was
successfully demonstrated on the ImageNet dataset.
And before that, things were plateauing around, uh, 0.25,
0.3, uh, and then kind there was this really- this mode shift,
uh, this kind of mode transition.
Where after 2012, I think most if not all of the,
the dots are- are deep learning models.
Uh, and of course were- were way more successful than,
uh, than previous approaches that were relying on hand-engineered features.
Uh, and this isn't specific to computer vision.
So, uh, in machine translation,
for example, um, in 2016,
Google, uh, started switching Google Translate to a,
uh, a- a system based on neural networks.
Um, in this case PBMT is Phrase-Based Machine Translation whereas, uh,
GNMT, uh, or NMT more broadly
neural machine translation you start- starts using neural networks.
Uh, and we see a pretty big difference.
Um, this is showing the human evaluation scores.
Uh, we see kind of ranging from 50% to
80% improvements by using neural networks.
Okay, so this is why deep learning, uh,
and kind of in-in two slides, there are other reasons as well.
Um, now why the multitask learning and meta-learning?
So um, if there's something that we've learned from deep learning is that we
can- if we kind of give these neural networks large and diverse datasets,
they can achieve broad generalization.
Um, so for example, the ImageNet dataset,
uh, or things like like Transformers, GPT-2,
these have been kind of extremely successful at producing models that generalize across,
uh, across many different images across many different inputs.
But the caveat is here is that they require a large and diverse data set.
Uh, and what have you don't have a large data set?
uh, then you're- you're in a bit of trouble.
These are domains, and there are a
wide range of domains where we don't have large datasets.
Things like medical imaging,
like robotics like I mentioned before,
like education for you don't have a lot of
data for each individual student that you're trying to teach.
Uh, medicine recommendation systems, uh, translation systems.
You don't have a lot of paired data for every single pair of languages that's out there.
Uh, and so, as a result,
it's impractical to learn from scratch for each disease, for each robot,
for each person, uh,
for each language really for each task.
And so instead, that's where, uh,
these multitask learning techniques can come in.
Uh, and okay, so beyond the kind of settings where we don't have a large dataset,
what if your data has a long tail?
Um, what if your data distribution looks something like this,
where the y-axis is showing the number of
data points and the x-axis is showing different objects that you've encountered in
the world or different interactions with people or words
that you've heard over time or driving sit-situations, uh, etc.
When you're in kind of a-
a variety of situations on the left you're kind of in good shape.
This looks a lot like what you've seen before.
Whereas if you're on the right- in- in the kind of the tail on the right,
that's where these algorithms start to break down.
Where supervised learning methods really struggle to perform well.
And this is- for example,
this is a really big problem in autonomous driving where cars can
handle a wide variety of very common situations but when they see very weird situations,
uh, humans can handle them perfectly well but these cars, uh, really struggle.
Okay, um, and lastly,
what if you need to quickly learn something new?
Uh, what if you had previous knowledge and you wanna learn- learning something new, uh,
about a person, for a new task, about a new environment,
uh, without training from scratch?
Uh, It turns out that people are pretty good at this.
Uh, and in particular,
uh, let's see how good you are at it.
So I want to give you a little test.
Um, the left side is showing your training data.
Uh, there are six data points.
Uh, you have three paintings that are from
Braque and three paintings that are from Cezanne.
And your goal is to be able to classify the test data point that is on the right.
And so, if you think the painting on the right is from Braque, raise your hand.
And if you think it is from Cezanne, raise your hand.
Great, okay. I think that there are more for Braque which is the correct answer.
Um, and the way that you can tell that,
is you can kind of look at the the types of
kind of straight lines and curved lines that are prominent in- in the image.
Uh, and so, how did you actually go about doing this?
So this is the kind of problem that's known as
"few-shot learning" where you're only given a few data points,
in this case six data points,
and your goal is to make predictions about new data points from that very small data-set.
Um, the way that you accomplish this is that you weren't learning from scratch.
You have previous experience.
You haven't probably seen these exact paintings before and you probably
haven't necessarily even seen paintings from these painters before.
But you've seen paintings before,
you've probably seen, um,
kind of you know what objects are, you know what textures are.
Uh, and through that previous experience,
you're able to quickly identify,
uh, the painting corresponding to the,
um- corresponding to the correct painter.
Okay. Um, So all of these things
if you want more general-purpose machine learning systems,
if you don't have large datasets,
if your data has long tails,
if you want to be able to quickly learn something new, um,
these are all settings where elements of multi-task learning
and meta-learning can come into play, uh,
and can help us out, and basically make machine learning,
uh, more effective in these problem settings.
Any questions on- on these four things before I move on?
[BACKGROUND].
Okay, um, so,
I've been talking a lot about multi-task learning.
Um, but what is- what is a task?
Uh, and this is- this actually is really important,
because a task isn't necessarily
what we kind of associate with- with in kind of the English language.
Um, for now the way I'm going to define a task is,
something that takes us in a data-set- takes as input
a data set and the loss function and gives you a model.
Um, this is- we're going to kind of generalize this later and
make it potentially a bit more formal later in the course.
Um, but for now, this is what we're going to be considering.
Essentially you can view a task as a machine learning problem,
where you have some data, have some loss function,
you want to optimize a loss function in order to produce a model.
And so what this means is that if you have different tasks,
different tasks could vary in different ways.
It could vary for different objects like maybe, um,
one task is to kind of be able to classify
between one type of cat and another type of cat whereas
another type-a-another task corresponds
to being able to classify between different types of water bottles.
So in this case, different tasks would correspond to different objects.
Uh, they could also correspond to different people.
If you want to be, uh, able to kind of personalize these systems,
to be able to handle and effectively make predictions about, uh, new users.
They can correspond to different objectives.
Uh, maybe in one case you want to be able to classify, uh, the, um,
classify someone's age versus you wanna be able to
classify someone's height from- from an image.
Those are kind of different objectives that you want to be able to accomplish.
Could be something like different lighting conditions, uh,
of your model, different words that you're encountering,
different languages that you're encountering.
Um, so it can encapsulate kind of a wide range of different things.
So not just different tasks that you would think of a task as,
uh, in kind of in English.
Uh, really it can vary in a wide range of ways. Questions on that?
It's like a different distribution of the dataset and the loss function?
Yeah, so it could correspond to a different-
really different dataset or a different loss function or both.
Um, so for example,
in the case where you have different objects that would manifest in
potentially the same loss function but like
be able to match labels but a different dataset.
Um, whereas something that looks like
uh, different objectives like classifying between one thing
versus another thing that might be some-
something that looks like different loss functions with the same datasets.
In this case, when we're talking about
the loss function we're not talking about the cross-entropy,
we're talking about brochures versus datasets?
Y- ye- yes I think that, that is the case.
You could also have one task that has kind of like
a cross entropy loss and another task that has
like a mean squared error loss for example.
Although of course you can generalize both of those as like log-likelihood.
It's okay. I think we are
talking about different tasks here, we are not talking about that.
Yeah.
Okay.
Yeah.
[inaudible] going to be called as a task.
Let's say I have a network which is going from
circuit classification through the circuit.
Given a new specification,
can you say that's a new task? [NOISE]
Yeah, so you could say that, that,
that is that is precisely a new task.
You could also- I think that there's kind of this fluid notion of
what is a new task where it's kind of lumped
into a single task learning problem and I'll
actually talk about that kind of morph- that fluidness in a couple of slides,
but that is kind of one thing that you could view as different tasks.
All right, so in multi-task learning and meta-learning,
there's one critical assumption.
Uh, this is kind of where some of the bad news comes in,
which is that different tasks need to share
some structure in order to get a benefit from these algorithms.
If the tasks don't share,
if the tasks you're trying to learn across don't share any structure,
then you're better off just using single task learning independently on
each of those tasks and kind of then
morphing those into a single model if you want to produce a single model.
The good news is though,
is that there are many tasks.
And task distributions that have shared structure even if they
don't kind of on the surface appear to have shared structure.
So, uh, and one kind of simple example here something like screwing a cap
on to a lid versus screwing a bottle cap versus screwing,
uh, kind of a pepper, grinder,
all share a similar structure in
terms of the underlying motion that needs to be performed.
This is maybe an example where the shared structure is more explicit, uh,
and even if there are tasks that are seemingly unrelated,
uh, there is things that are still underlying.
So there is structure that is still underlying
those tasks because the laws of physics are,
are kind of underlying the real data that,
that we have, uh, people are
all organisms that have intentions and so even if two people are very different,
they still have some commonalities.
The rules of English are underlying all of
like- well not all but a fair amount of English-language data,
uh, and languages are kind of developed for similar purposes and, and, and so on.
So and these kind of may seem like superficial,
uh, relationships between different tasks,
but in reality this leads to
a far greater structure than having completely random tasks, uh,
because completely random tasks would essentially look like
completely ran- random inputs, random labels, uh,
and in practice kind of the real world is underlying,
uh, a lot of the data that we're looking at. Yeah.
Are these, uh, assumptions sort of [inaudible] multitask- I mean when you
separate meta-learning and multi-tasking will you kind of get
different things or [OVERLAPPING].
It applies to meta-learning as well.
Yeah. So I'll give
problem definitions of multi-tasking and meta-learning on the next slide,
but in essence like in order- meta-learning is
all about kind of learning the structure of underlying tasks
such that you can more quickly learn a new task,
and if you can't- if there isn't any shared structure
then you won't be able to learn more quickly than learning from scratch.
All right. So let's
informally go over some of the problem definitions in the coming lectures.
Actually in the next lecture we'll formalize these lectures,
these definitions, these problem definitions a lot more,
but just kind of just to give you a rough idea of what I'm talking about with these,
with these kinds of problem definitions,
here's kind of roughly what things look like.
So you can think of the multi-task learning problem as
trying to learn all of the tasks that you're provided
with more quickly or more proficiently
than learning the tasks independently from one another.
And then what the meta-learning problem is looking at
is given data or experience on a set of previous tasks that you're given.
You want to be able to learn a new task more quickly and/or more
proficiently than learning from scratch
by leveraging your experience on the previous tasks.
So essentially the difference between
these two things is that the first one you're trying to
learn a set of tasks and do well on those training tasks,
and then in the second problem setting,
you're trying to use experience on training tasks in order to do well at new tasks,
in order to basically be able to more quickly learn new tasks given a dataset.
Uh, and in this course,
we won't necessarily be covering everything that's considered a
multi-task learning algorithm or a meta-learning algorithm.
That will be, really, anything that solves one of
these two problem statements will be fair game for including in the course.
Uh, so things that allow you to build on previous experience to
quickly learn new tasks even if they aren't through learning to learn techniques,
um, I'll try to touch on them in this course.
Questions on these problem statements. Yeah.
How is meta-learning different from transfer learning in [inaudible].
Yeah. So I guess in many ways I think that this is
the tran- a form of the transfer learning problem statement where you wanna
take some data and use
that- use knowledge acquired from that data to do well at other tasks.
I think that one aspect about this problem statement is that you
want to be able to learn a new task more quickly,
whereas in transfer learning you may also want to be able to just form
a well- performing a new task well while in zero
shot where you kind of just want to share representations.
I actually kind of view transfer learning as
something that encapsulates both of these things, uh,
where you're thinking about how you can transfer information between
different tasks and that could actually
also correspond to the multitask learning problem,
uh, as well as the meta-learning problem. Yeah.
I thought meta-learning was kind of like learning to learn.
Would you say that is kind of like consequence of
that definition or kind of like [inaudible].
Yeah that's a good question. So I guess what I'm defining here is the,
the meta-learning problem and I think that meta-learning algorithms are
all learning to learn and they solve this particular problem.
Uh, and they're not the only way to solve this particular problem.
Does that answer your question?
There could be no meta-learning with one single task.
Uh, that's a good point.
Yeah, so in, in principle,
you could still perform meta learning in the context of
a single task and what you'll be doing in that case is probably- actually,
in some ways breaking down that single task into
sub-tasks or into kind of sub-components, uh,
and then using that when kind of- when you're facing something new in that single task,
using that experience to more quickly learn in the future.
Yeah, so that's a good point. The tasks in
some ways could be something that's kind of latent
to your underlying problem. Yeah.
The meta-learning problem strikes me as quite similar to the problem of domain adaptation.
Uh-um.
Are they the same would you say or are there some clear distinction?
Yeah, so I'll formally cover the distinction with domain adaptation in the next lecture,
but, um, in some ways they are similar.
I guess in some ways, uh,
one is more specific than the other and in some ways it's kind of the opposite.
So in domain adaptation, um,
you typically do want to,
it's kind of a form of transfer learning in
some ways where you want to transfer from one to another.
Um, one thing and I guess when I get into the more formal definitions of these problems,
this will become more clear.
One thing you typically see in the meta-learning problem is
that the tasks that you're seeing at
test time you assume to be in
the distribution of the tasks that you're seeing it during training,
whereas many techniques in domain adaptation are considering a setting where
your task domain may be out of distribution from what you're seeing during training,
uh, and yeah, so that's,
that's in, in many ways one of those distinctions there.
Okay. Um, now one question that was asked a bit before is kind of- I think it,
it was in the context of circuits is what,
um, is it- is, is something a
single-task learning problem or multi-task learning problem?
Uh, in some ways it's gets- gets out of the question of,
doesn't multitask learning reduce to just a single-task learning problem?
Uh, and in particular what you could do is you could just say, "Okay,
I have a dataset for each task,
and we're going to take the union of those datasets into, uh, a single dataset.
And likewise, I'll take the loss function for each
and just kind of sum and get a loss function.
And now we have a single task learning problem, uh,
where we have one dataset and one loss function.
Uh, and we're done [LAUGHTER].
Um, and, uh, in many ways, uh, yes, uh,
it [LAUGHTER] aggregating the data across the task and- and learning is- is one very,
uh, successful approach for multi-task learning.
But we can often do better.
And in particular, we can exploit the fact that
we know the data is coming from different tasks,
uh, and use this to achieve greater performance.
And basically, exploit that structure in the optimization in order to perform better.
Okay. Um, now, I think the- one of the last things I want to cover is- is why now?
Why should we be- be studying this topic now, uh,
rather than, uh, in- in 10 years or 10 years ago?
Um, well, people were actually studying this problem a long time ago.
Uh, so this is 12 years ago at this point.
Um, 22 years ago at this point [LAUGHTER], and, uh,
this is by a survey in which Caruana's thinking about how we
can train tasks in parallel while using shared representations.
Uh, they, uh, can do multitask inductive transfer,
and add extra tasks to a backpropagation neural network.
Uh, in 1998, Sebastian Thrun was thinking about this problem,
where we want to be able to, uh,
exploit an enormous amount of training data and experiences that stem from,
uh, from one another.
Um, stem from other related learning tasks in order to generalize to new tasks,
uh, even from a single training example.
Uh, and, uh, actually even earlier, uh, in 1992,
Samy Bengio and Yoshua,
and others were looking about the possibility of learning,
uh, a learning rule that can be used to solve new tasks.
Uh, so these ideas are,
are by no means new.
Um, people have been kind of studying these,
uh, for, for a very long time at this point.
Um, but I think that right now it's actually a particularly exciting,
uh, time to be studying these algorithms because they think that they're studying,
and actually continuing to play a fundamental role in machine learning research, uh,
especially with the advent of powerful neural network function approximators.
The amount of compute that we have right now,
as well as the- the kind of datasets that we're looking at.
Um, and so, so as some examples of
very recent works that have kind of leveraged some of these algorithms to do well.
Um, here's a paper, uh,
looking at machine translation across over 100 languages.
Uh, thinking about how you can learn, um,
algorithm things- algorithms surpass the strong,
uh, base- baselines that use only two languages.
Um, here's some work from my lab that actually previewed for at the beginning is,
uh, we can kind of use these types of algorithms to learn from a video of a human.
So uh, this is actually one of your TAs who's, uh,
showing a video to the robot performing a task,
and then the robot can use that video to learn,
um, learn a policy that can successfully place
the peach into the bowl just for that single example,
and the policy can generalize to different positions of the- of the bowl.
So what- we'll cover kind of the algorithms underlying this.
And people have also looked at, uh, multi-domain learning.
So in here, different tasks would correspond to different domains.
Um, and they- uh,
this paper they constructed
different simulated domains with different textures, different environments,
and they showed that they could use only this data in
simulation in order to enable a, uh,
a quadcopter to fly in the real world,
uh, and navigate in the real world.
Um, and actually just like within the past two days,
this paper was published on how YouTube is using
multitask object- multitask and multi-objective systems in order to make recommendations,
um, and being developing algorithms that can handle multiple competing objectives.
Um, so I think that these, these types of algorithms are playing a huge role, uh,
in robotics in- in kind of deployed YouTube machine learning systems,
uh, as well as in other research.
Uh, I also think that, uh,
they're playing an increasing role in,
uh, machine learning research.
So if you look at, uh,
Google search queries over the course of the past few years,
we see a trend that looks like this where
blue is meta-learning and red as multitask learning.
Uh, and we see an increase starting around 2014 and 2015.
Um, and if we also look at kind of
paper citations for things that cover things like fine tuning,
we see an increasing trend, uh,
meta-learning algorithms, uh, as well as multi-task learning algorithm.
We see that these algorithms are,
um, becoming of increasing interest.
And I think that's because these algorithms could be really important in the future
for enabling things like learning from small datasets, uh, etc.
Uh, and lastly, I think that the success of multitask learning algorithms and
meta-learning algorithms will be really
critical for making deep learning more widely accessible.
Uh, as I mentioned before,
the kind of settings where deep learning has been very successful
before is settings where you have 1.2 million images,
40.8 million paired sentences,
300 hours of labeled data, uh,
and in a wide range of settings that's just not feasible.
So if we look at for example, um, a,
a diabetic retinopathy detection dataset,
it has around 35,000 labeled datasets, I think labeled images.
I think this is actually one of the larger medical image datasets, um,
and yet it still has two orders of magnitude less data
than kind of the datasets on the left- on the top [NOISE].
Um, there was a really interesting paper that was
looking at reinforcement learning for epilepsy treatment.
Uh, in that case they had less than an hour of data.
Uh, and in some of the work that we've done in robotics,
we've had less than 15 minutes of
data per individual task that we want the robot to learn.
So if you care about it kinda making these- making deep learning algorithm successive,
deep learning more widely accessible to these types of domains,
then it's gonna be critical to build these kinds of algorithms.
Um, and lastly, beyond, um, the things that I've talked about,
there are still many open questions and challenges in multitask learning.
And I think that makes it a really exciting thing to study right
now because there's a lot of problems to be solved.
Great. Um, so that's it for today as a reminder,
please do these four things.
Please sign up for paper presentations,
um, sign up for Piazza, um, etc.
And on Wednesday, I'll be covering
multitask learning and meta-learning basics. I'll see you on Wednesday.
 Welcome to the second lecture.
First we'll cover just some logistics.
So, uh, Homework 1 will be posted today, uh,
and it's gonna be due Monday,
October 7th and we'll cover actually some of the topics that will be,
uh, in Homework 1 today.
Uh, so pay attention if you wanna be able to do Homework 1.
Uh, fill out preferences for papers by tomorrow so that we can start,
uh, assigning papers to you.
We'll- we'll assign papers as quickly as possible especially for
the first group that's presenting on Thursday next week.
Uh, also a reminder that the TensorFlow review session is tomorrow
on- at 4:30 PM in Gates, B03 that's gonna be led by, uh,
and then if lastly for enrollment,
if you filled out the form and, uh,
have not received a permission code then at this point I beli-,
unless you filled out the code like a minute ago,
I think we've handled, uh,
basically all of the cases so just send us, um,
send the staff an email or a post on Piazza asking about that.
And we can, uh,
look at that on a case by case basis.
If you need to- if you still haven't filled out the form please do fill it out.
Although at this point, um,
maybe let us know if you fill it out because we may not be
mo- be monitoring it quite as frequently.
Okay, any logistical questions before we- yeah.
What time is the homework due? [NOISE]
The homework will be due midnight on Monday October 7th
and the date will also be posted- the time and date will be posted on the assignment.
Okay, great, so the plan for today,
uh, we actually have a lot of material to cover.
So, uh, first we'll be talking about some of the basics of multi-task learning.
Um, things like the types of - the type of models and architectures that these algorithms
typically use as well as how we
actually go about the training process for multi-task learning.
Uh, some of the challenges that arise when you try to train across multiple tasks and,
um, a case study of actually trying to use
multi-task learning in a real-world application.
Uh, and some of the challenges that come up in some of,
like, the results that were there.
Uh, then we'll have a short break around like two minutes or so.
Uh, and then we'll be covering, um,
some topics- uh, some kind of basic topics in meta-learning.
We'll- we'll get into more advanced topics in future lectures.
This will include things like the meta-learning problem formulation, uh,
a general recipe of meta-learning algorithms, uh,
and kind of how you go about building these types of algorithms.
And then lastly we'll talk about a specific class of
meta-learning algorithms called the-
that I'm gonna refer to as black-box adaptation approaches.
Uh, and these last two things, uh,
actually how you go about setting up a meta-learning algorithm,
and implementing a simple approach to meta-learning will be the topic of Homework 1.
Okay, so let's get started,
um, with multi-task learning.
So and actually- before we even get into multi-task learning let's look at some notation.
So, uh, say we have some neural network.
The neural network is gonna take in some input x and produce some output y.
Uh, and the input might be something like a picture.
Uh, maybe the output is the class corresponding to that picture,
uh, the class of the animal for example.
Another example of an input might be like the title of a paper for example,
maybe you're, uh, trying to decide which papers
to read or which papers that you wanna review.
And then you kind of want to decide whether or not the paper's gonna be too long or not.
Um, this is essentially a variety of classification type problems you might imagine.
Um, the weights of the network we'll use,
uh, theta to denote the weights.
And then you can basically view this neural network as
producing a distribution over outputs Y given the input X.
Of course this could also be a deterministic function,
um, viewing it as a distribution as a generalization of a deterministic function.
Okay, um, so in single task learning the way that
you- this should mostly be review for all of you.
Uh, so single task learning,
at least the supervised single task learning problem we have assumed some data-set
where we have input-output pairs.
Uh, we probably have many input-output pairs corresponding to that data-set.
And our goal is to minimize some loss function that is a function of,
uh, the parameters and the data set.
We want to minimize that loss function with respect to the parameters.
And so one typical loss function,
uh, that we may use is something that looks like negative log-likelihood.
Uh, so our loss function is going to correspond to the expectation of
the data points in our data-set, of the log probability of our predictions.
Um, the log probability of the labels under our neural network predictor given the input.
Okay so this is kind of standard single task learning.
The way that you would typically optimize this log-likelihood
is by back-propagating into the parameters of the network.
And running something like SGD or your favorite, uh,
optimizer, Adam, Autograd, Momentum, whatever.
Okay. So now we'll actually call a task.
Uh, we talked about this a little bit, uh,
last time on Monday and we'll
try to talk about this a little bit more formally this time.
So, uh, the way that we're gonna define a task,
for the sake of
basically the first half of this course before we get to reinforcement learning,
is something that looks like this.
Where we're gonna have some distribution over inputs.
A distribution over the labels given the inputs and a loss function.
Uh, and essentially these two distributions P are gonna
correspond to the true data generating distributions.
Um, and so of course we don't actually have access to these data distribution- data
generating distributions but it allows us
to kind of reason about what we're gonna be seeing at training and test time.
In particular what we're gonna be seeing, uh,
the kind of things that we will have access to, um,
is we'll have a training set and a test set for each task.
Uh, and we'll assume that each of these distributions are drawn from
the same data generating distribution P or Pi.
So essentially what a different task corresponds
to- different tasks may vary based off of the input distribution,
the label distribution given the input or the loss function.
Okay. Um, one notational note,
uh, is that we're all- in future slides I'll be
using Di as shorthand for the training data-set.
And yeah we'll get to that in a couple- in a couple of minutes.
Any questions on kind of the basic setup? Yeah.
[inaudible]
Oh that just means that we're- we're defining the task to
be equal to the thing on the right.
Okay so, um, this is our task.
Uh, task- for each task we have corresponding data sets,
uh, sampled from, uh, some distribution.
Let's look at some examples of what different tasks may be.
In particular what different sets of tasks may be.
What- what kind of different problems they correspond to.
So for example, uh,
we may have a multi-task classification problem,
um, where we want to be able to perform different classification tasks.
Uh, and in this case,
the loss function will be the same across all of the tasks.
The loss function will be something like cross entropy loss for all the tasks.
Um, but the- the inputs like the images that you're seeing for each task or
the labels corresponding to those inputs may vary across different tasks.
For example, uh, maybe different tasks corresponding- correspond to writing,
uh, to be able to recognize handwriting digits,
uh, on a per language basis.
Maybe you wanna like one task is to be able to recognize, uh,
have characters corresponding to Braille for example.
While another, uh, corresponds to another language.
Um, another multitask classification task might be like a personalized spam filter.
Uh, the emails that I get are probably very different than,
uh, the emails that, eh,
a high-schooler gets for example.
Or, um, like a professional athlete for example.
And so the types of spam will be different.
Uh, so classifying spam for one person could be
one task and classifying spam for another person could be a different task.
Um, In that case, the- the loss function is the same across all tasks.
The P of X is different because the types of emails
that each person will be receiving- the distribution of emails will be different,
so you'll have a different Pi of X for each person.
And also different things might be, uh,
one thing- one thing that might be spam for
one person might not be spam for the other person.
And so you may have a different P- Pi of y given x.
Okay, um, so that's- that's one example.
Another example, uh, of
a multi-task learning problem is what's called multi-label learning.
In this case the- the loss function
and the distribution over the inputs is gonna be the same across tasks.
And what's gonna be different is you're going to want to be able to make predictions, um,
different, they basically will be able to predict different labels, different, uh,
perform different kind of output to tasks,
uh, given your input image.
So one example of this is attribute recognition.
So if you have a data-set of faces in this A data-set for example.
One task might be correspond to detecting whether or not the person is wearing a hat.
Another one might be correspond to detecting their hair color, things like that.
So there p of x is the same.
The loss function is the same but y given
x is different because you have different binary classification tasks.
Um, another example is something like seeing and understanding where maybe
given an image you want to be able to predict,
uh, the depth of that image.
The key- different key points, uh,
the surface normals, uh, of that image as well.
So this is a common problem in computer vision.
Um, so both these examples were- were settings where
the loss function was actually consistent across all tasks.
Uh, and what was varying was the data distribution.
Uh, and so what our settings were, the loss function might vary across the tasks.
So one setting is maybe one of your tasks
corresponds to predicting a discrete variable whereas
another one corresponds to predicting a continuous variable.
Uh, in that case, um,
maybe one task- one task might have
a cross-entropy loss function whereas another might correspond to mean squared error,
or something more sophisticated than that.
A different way to represent the likelihood essentially.
Another setting where the- it might vary is
actually if you care more about one task than another task.
Uh, maybe you care more about, uh,
accurately predicting the surface normals than about,
uh, the depth for example.
Um, then you might, kind of,
increase the - have a loss function weight
corresponding to one task that's higher than the weight of another task.
Okay. Ah, so here's some examples of kind of
different multitask classification or
different multitask learning problems that we might want to solve.
Ah, how do you actually go about solving them?
So, ah, we can go back to our neural network.
Ah, and if we want to- if we want our neural network to do multiple different things,
then we need to tell it what it should do.
And so in some ways we kind of- we need to condition on the task in some way.
And so what we're gonna have is we're gonna have some task descriptor Z.
And we're going to pass this task descriptor into the neural network in some way.
Ah, and so I'm just gonna draw an arrow towards the neural network to indicate that.
And in a minute we'll talk about different ways
that you can condition on that task descriptor.
And of course, now our neural network is not gonna be just a function of x
but it's also going to be a function of this task descriptor Z_i.
Okay. So for example, um,
in this case maybe x kind of we can go back to
the paper title example, maybe x corresponds to paper titles.
Uh, and now instead of- oh, it's back.
Okay. So maybe try,
try- instead of trying to figure out what, uh,
what paper you wanna read, uh,
you now need to review the paper, uh,
and produce a review for a conference and maybe you're a bit lazy.
So you, you ask your neural network to predict a summary of the paper for you.
So that you can then base your,
uh, review on the summary.
Or maybe, maybe you're even lazier,
and you just want to output the paper review directly.
[LAUGHTER] Uh, hopefully, these aren't the reviewers for your papers.
Uh, but, um, yeah.
Okay. So what is this task descriptor thing?
The very- the simplest example of what this task descriptor might be
is just a one-hot encoding of the task index,
uh, just a vector that tells you, uh,
if it's task 1, task 2, task 3, etc.
This is just kind of an index,
uh, corresponding to the task.
Uh, but more generally, it can also be, uh,
whatever meta-data that you have about that task.
Um, if your task corresponds to a personalizat- personalization,
then the task descriptor might include information about that person or that user.
Uh, if it's something, uh,
maybe you have kind of a language description of the task you wanna perform.
So you could provide that language description to the network.
Uh, or maybe you have some formal specification of the task or maybe you're
trying to design circuits like was- because we've mentioned it on Monday.
Then you might have kind of a formal specification of the type
of circuits that you might want your neural network to be producing.
Okay. So this is the task descriptor.
And then, our objective now becomes,
uh, basically the same as what it was before,
but now we're going to be summing over all of the tasks that, um, that we have.
Okay, so this is the basic setup.
Um, now we have a couple design decisions to make.
So we have a model and we have an objective.
Uh, and so we have a decision for both of them.
The first is, how should we design the model and the second is,
how should we optimize the, uh, the objective.
So ,um, particularly the big decision that we have to make in multitask learning
is how do we condition on this task descriptors Z_i and the,
uh, algorithm decision like I mentioned is how do we optimize our objective.
So let's first talk about this,
um, this first question.
So, um, this is all about conditioning on the task.
Let's assume that, uh,
Z_i for now is just gonna be the task index,
like a one-hot encoding over a task index.
And, uh, we only know kind of information about our task is this index basically.
So I have a question for you which is,
how should you condition on Z_i, uh,
in order to share as little as possible between the tasks in your neural network model?
Essentially, how should you structure your neural network model such that it
shares basically as little as possible between the different tasks. Yeah.
[inaudible]
Yeah, exactly. So what you can do is you can have separate networks for
each task where the- you have- each,
each of them have completely separate weights.
And then the way that you condition on your task descriptor is
you pick the corresponding task which corresponds to this sort of
multiplicative gating where you're just selecting which of the outputs you're
going to produce based off of your, your task index.
Um, this is basically, uh,
a complicated way to say that we're just gonna be training independent networks.
Uh, and it's kind of one way to represent
independent networks within a single model essentially.
Okay. Um, so we get independent training within a single neural network,
uh, and there's no shared parameters across tasks.
Now, what about the other extreme.
So the other extreme would be
something like this where we take our neural network and we just
concatenate the task index somewhere either at the activations or at the input.
Um, and then train the network as normal.
Um, and in this case,
all of the parameters are shared except for one tiny part of the parameter vector.
What is the, the kind of tiny part of the parameters that's not shared?
[inaudible]
Sorry, repeat that.
[inaudible]
Yeah exactly. So basically the weights that are, uh,
right after the Z_i, the kind of,
if you have a fully connected layer right after Z_i concatenated with your features,
the, the part of that matrix that corresponds to, uh,
that is basically right after Z_i will have basically different,
um, different components that,
that are not shared for each of the tasks.
Other than that half of that matrix,
all the other parameters are shared. Yeah.
There's also kind of forces [inaudible]
Yeah. Yeah, exactly. So in this case,
we assume that all the inputs have the same size,
the same, uh, dimensions.
One thing that you could do is,
uh, if, if different tasks have different sizes,
you can basically like, you would have some sort of,
uh, recurrent neural network,
or some sort of attention based model that basically
aggregates over the variable dimens- like if you- if one of them is time for example,
it aggregates over that, uh,
whereas maybe some tasks have- are text and others image.
Images in that case, you would probably wanna have different, um,
different first parts of that network to take in those different modalities of data.
And we'll show- we'll see like an explicit example of,
of how that has been done in the past. Yeah.
So [inaudible]
So this is, yeah, this is a good question.
It's, it's a f- it's a fairly nuanced point.
So basically each, um,
you can ba- you can view, uh,
the first- a fully connected layer corresponding to,
um, a weight matrix times a vector.
And when you, um,
when you take the,
the top part of that matrix will correspond to- or sorry the left, um,
rows of that- the left columns of that matrix will correspond to the,
the features and the right columns of that will correspond to the task,
uh, task vector that's being processed in this, in this input.
Uh, and if you essentially- if the D_i is a one-hot vector,
then you'll have one entry in that matrix that is 1,
and the other we- entries in that,
in that vector will be 0.
Uh, and so basically all the ones where all of the columns that have, uh,
an entry in Z_i that corresponds to 0 will be
zeroed out and won't be used for that task,
whereas the columns that have the one will be used for that task. Yeah.
[inaudible]
Uh, technically, although, um,
if you, kind of mathematically go through the gradients,
the gradients for those columns will be zero for the tasks in which,
um, for the tasks in which the input is zero. Yeah.
[inaudible].
Yeah. Exactly. So if z i was not a one-hot vector for example,
then you would have that- that part of the matrix would be shared parameters.
Uh, another kind of, ah,
somewhat interesting fact about this is if you could- if you could concatenate
z i at every single layer of the network you'll have more task-specific parameters.
Whereas if you condition it only on a single layer of the network,
you'll only have kind of one set of task-specific perimeters. Yeah.
[inaudible].
Yeah, exactly. So the output of that, um,
that part of the matrix is a linear transform on- on the one-hot which produces a vector.
Basically that- the- well,
the column of that matrix corresponds to the resulting vector, um,
which could give you as essentially as an embedding- a linear embedding
of that task. Yeah.
So ah, my understanding of [inaudible] is through [inaudible] of these other points.
Right? But if you have the, um,
z i [inaudible] one linear versus multiple layers.
Um, in any case it doesn't really affect the weights [inaudible] um,
could you go in a little more detail on what kind of differences those make in terms
of-of that [inaudible] was his first [inaudible].
Because they're not really affecting the weights per se.
Yeah, so if you add z i to every layer then you
are- you're increasing the number of parameters per layer of course.
And you're also, like the parameters that you're adding are all tasks.
Well, not there- they're not all task-specific,
there- they're mostly task-specific.
Um, maybe I [inaudible] it depends on the dimensionality of each vector.
It's somewhere around half- half of the new parameters are
task-specific and half of the parameters are shared.
Um, so as you add weights to neural networks,
they become more expressive,
as you add task specific weights then in some ways they've become even more
expressive because for different tasks that can represent completely different things.
[inaudible].
We'll get to that in a second.
Yeah. Okay. Um, so we've been talking a lot about shared parameters and,
ah, and task-specific parameters.
Ah, and one kind of alternative view on
the multitask learning problem is that you can split
your parameter vector into these shared parameters and toss
specific parameters which I'll denote as theta sh and theta i.
Ah, and then if you view, um,
if you view kind of that as your parameter vector, then your objective looks
something like this which is basically exactly the same as the previous objective.
Um, but it decomposes, um,
it decomposes, it makes it obvious that the task specific parameters are
only optimized with respect to the objective for that task.
Uh, whereas the shared parameters are optimized over,
um, for all i basically, optimized over all tasks.
Uh, so one of the things that's somewhat interesting ah,
to note based on both of the examples that we saw
that- the multiplicative gating example as well as the concatenation example,
um, is that choosing how you condition on z
in many ways is equivalent to choosing how and where to share parameters.
Um, yeah.
So essentially the, ah,
choosing how to condition on z corresponds to thinking about this sort of
optimization problem and thinking about where those shared parameters
should lie and where those task-specific parameters should lie.
Okay, um, so let's go
over a few more choices for how- how we might go about conditioning things.
So we saw how we could do concatenation based conditioning where we basically take
that task descriptor and concatenate it with
the features and then do a linear layer after that.
Um, another option to concatenation-based conditioning is to add the, um,
is to add the- the task vector to your- to your hidden units,
to your features where you basically have a linear layer, ah,
to make sure that those feet- that kind
of says that you can basically add it o-on in this-
such as the same dimensionality because
the dimensionality of z i may not be the dimensionality of your features.
Ah, you run it through that linear layer and then add it.
Um, Interestingly, I presented both of these as two different,
ah, two different options,
but they're actually just the same thing.
Um, so you can essentially view, ah,
additive and conde- concatenation based conditioning, um,
[inaudible] same so for example, ah,
on the left side of this figure you see that you're concatenating, um,
x and z and then you have this weight matrix that corresponds to the, ah,
corresponds to the kinda of the weights that are applied to each of
those layers and you can view that as you kind of apply to matrix.
Multiply it to those two parts of the- the vector,
you basically get um,
these two components that are added together.
Um, which illustrates how- how basically the right hand side of
this figure corresponds to the additive conditioning which is
equal to the left-hand side which is,
ah, concatenation-based conditioning.
Okay. Um, yeah.
[inaudible].
Yeah so this is assuming that these are one-hot vector.
[inaudible].
Yeah absolutely. So if you have more information about
how tasks are similar or dissimilar from one another,
then you can feed that into your network as opposed to z i.
Uh, interestingly, determining how similar tasks are- to two tasks are to each other,
is actually a big part of the multitask learning problem itself,
is kind of determining how you should be sharing, um, sharing content,
sharing parameters, sharing structure across these different tasks.
Uh, and in many multi-task problems you don't necessarily have that a priori.
Um, but there are- there certainly many examples and we'll actually
see a practical example where you do have
some information about the task and you can use that,
um, to condition your network on it,
ah, in a different way.
Okay, um, a couple other very common conditioning choices, ah,
this is again for the-the one-hot case where you don't
have a lot of information about how two tasks are shared.
One is kind of the- this- this form of multitask or
multi-head architecture where you have some set of parameters,
ah, and then you split off the network,
ah, into, ah, into different heads.
The citation here is for 2017,
that's just for the visual.
This has been something that people have used for a long time.
Um, and then one other i- interesting choice, er,
or one other kind of very common choice that you can do is
multiplicative conditioning which is basically identical to additive conditioning,
except instead of adding you multiply.
Um, so you project your task matrix, your task vector onto um,
a vector and then you multiply that onto your, um,
on the activations either in the input or,
um, at kind of intermediate representations of your neural network.
Okay, um, so why might this sort of multiplicative conditioning be a good idea?
There's actually a few different reasons.
So one is it's- its more expressive than additive conditioning.
You can represent multiplicative interactions whereas with additive things you can,
ah, you- it's- its much harder essentially.
Um, and the other reason is- if you think
about something like multiplicative gating that we talked about before,
this sort of gating is something that can very naturally be
represented by multiplicative conditioning.
Ah, and not just multiplicative gating,
but also things like a multi-head architecture, um, where,
ah, you want to basically be choosing which parts of the network
should be- should be used for different tasks.
So essentially there's sort of multiplicative conditioning where
you're multiplying this embedding onto- onto your activations,
allows it to- to modulate different features,
ah, such that you can completely turn off features,
you can completely- only use some heads for one task,
only use some parts of the network for one task versus another task. Yeah.
[inaudible].
That's a good question. I would say it's a very simple form of attention mechanism.
I think attention mechanisms usually involve dot products.
And in this case you're not actually summing across the dimensions,
you're just doing an element wise multiplication. Yeah.
[inaudible].
Yeah you can definitely do that and we'll see
an example of that in, in the case study actually.
Ah, and multiplicative conditioning allows you to do that.
Or it allows it to actually learn how to do that,
um, in a completely automatic way.
Yeah.
[inaudible].
Yeah.
[inaudible] wouldn't be as- as expressive as the previous architecture as you said
[inaudible] actually.
As expressive as the additive or
Or, yeah, additive with another linear layer or something like that that would actually use the input. [OVERLAPPING] Right so these diagrams had been,
ah, kind of looking at a single layer at a time.
Of course when you add neuron- add kind of layers,
a neural- neural network is a universal function approximator,
and so if you concatenate two things and then wanted it to give you the product,
a neural network can represent that function.
Um, and so technically the additive function
can- the neural network can also represent this when you concatenate.
It's just, ah, in practice and so technically all of
these are- are kind of all universal function approximators
for- in the multi-task setting.
Um, in many ways it's kind of more about what you're readily allowing the network to do,
um, and how, kind of the optimization process itself,
although it's much harder to say things about, ah,
optimization than, um, than about, ah,
the types of things that like an individual layer can represent for example.
Yeah.
[inaudible]
Can you repeat the question?
Do you have to know like if we already like the number that you only train on [inaudible].
Are there methods where you can actually use the training model for [inaudible].
A task that you haven't seen any training data for?
Yes.
So yes, so that's a good question.
Um, in that case if you want to be able to
generalize to an entirely new task that you haven't seen during training,
you'll need something other than a one-hot vector,
uh, because if you only have a one-hot vector,
then you are, uh, you don't know how
that new task relates to anything that you've done before.
But if you have a language description for example then,
um, then in principle these,
these models can learn how to generalize
from basically given a new language description to perform that task,
given enough data of other language descriptions that share,
uh, that share commonalities with what you're seeing at test time.
Um, and then in the second part of this lecture
we'll be talking about how we can quickly learn new tasks from data,
uh, by using experience on tasks, on previously seen tasks.
Okay. Um, there are also more complex choices,
uh, that we can go with as well.
So here are some architectures from, from the literature.
For example these things include various modules, various, um,
attention, uh, components, uh, different gating mechanisms.
We don't have time to go through,
uh, all of the possible choices that you do here.
Uh, um, although some of the,
I think one or two of the, um,
of the readings that we'll have will cover a couple different options there.
Um, yeah, so the main takeaway though that I'd like to give is that there are,
well, I guess one, there are a lot of different appro- like a lot of
different choices we have to make when choosing the architecture.
Uh, and unfortunately, these design decisions
are basically kind of corresponding to neural network architecture tuning.
Um, in that they are problem dependent.
So one architecture will work well on one problem but
won't necessarily work well on another problem or another set of,
um, another set of tasks.
Uh, in many ways they're largely guided by intuition or knowledge of the problem.
So if you know that two tasks are similar,
you might have them share more.
Whereas if you know two, that two tasks are different,
you might have them share less.
Um, and I would say that currently is in some ways more of an art than a science, uh,
and there, um, there isn't kind of like,
uh, any guide that can tell you exactly how to, uh, how to do this.
Given a new problem- a new set of tasks that you want to be able to learn.
How- but hopefully in- this a very active area of research and hopefully in the
upcoming years we'll have better, a better idea for how to do this.
Okay. So now that we've talked about the model architecture a lot,
what about actually optimizing this objective?
Um, in many ways,
this is- it's fairly straightforward compared to standard neural network training.
Uh, kind of a very basic version of optimizing this objective,
would correspond to first sampling a mini-batch of tasks.
So we have different tasks.
We used sample different tasks and then sample data points for each task.
So, um, unl- unlike the standard supervised setting,
we won't just be sampling kind of sampling once, we'll be sampling twice.
Uh, then we're going to compute our loss function on our mini-batch.
Uh, by kind of measuring a stochastic, uh,
basically kind of getting a stochastic gradient rather than a,
uh, rather than the true gradient because we
can't evaluate the gradient on our entire data set.
Uh, we back propagate the loss, uh,
to compute the gradient and then apply the gradient to your parameters using your,
your favorite neural network optimizer.
Um, so basically this is the same as standard single task learning except that we
will be sampling a batch of tasks and a batch of data points for each of those tasks.
Um, and note that kind of these first two steps is to ensure
that the tasks are sampled uniformly,
uh, regardless of data quantity.
So if you have one task that has a lot more data than another task,
then this will ensure that you're sampling them at equal rates.
Which is what we're optimizing for,
uh, in- on the top.
If you care more about the task that has more data then, of course,
it makes sense to weight those differently,
um, and sample at different proportions as well.
Okay, um, and one other tip here is that,
uh, if your different tasks correspond to regression problems,
this isn't really necessarily specific to the,
the optimization algorithm but, um,
it's pretty important to make sure your tasks labels are on the same scale.
Uh, otherwise inherently your loss function is
going to have a greater scale for which the,
uh, the labels have a greater magnitude.
Okay. So now that we've gone over kind of the basics of, um,
of the architectures and the challenges, or
the architectures and the algorithms,
uh, let's talk about some of the challenges that come up.
So the, in many ways,
the, uh, the kind of number one challenge that I view with
multitask learning is the problem of negative transfer.
Uh, and this kind of symptom of this, uh,
is when like if you train independent networks and that's
doing better than your multitask learning method,
then that means that you're getting negative transfer.
It means that the, uh,
kind of data and training from one task is adversely hurting,
is adversely, um, affecting the training of the other tasks.
Uh, so for example,
uh, and this is actually a problem that is, is quite prevalent.
Uh, in some cases maybe you care more about
computational efficiency, it's not practical to train independent networks.
Um, but for example we're actually recently trying to write a paper on multitask learning.
Um, there's this common, uh,
multitask version of the CIFAR data set where you break it down into um,
into different tasks corresponding to the different kind of,
uh, upper-level categories of CIFAR.
Um, and we kind of evaluated all the, uh,
multiple state of the art approaches,
uh, for multitask learning, uh,
and got various levels of accuracy on this data set and
then we trained independent networks and it was,
uh, performing better than kind of all the recent papers.
Uh, so why, why does this happen?
Why does negative transfer like why are we
actually getting benefit from training across multiple tasks?
Um, one is optimization challenges.
Uh, so if, um,
if basically gradients of one task or interfering with the,
uh, with the training of another task that basically when you apply it,
you compute gradient for task one,
compute gradient for task two.
If gradient one hurts,
the weights for task two,
uh, then the optimization will be more difficult.
Um, and also tasks we learn at different rates.
So uh, if one task is learning a lot faster than another task,
you might end up learning, um,
one task very quickly and then it,
it might get stuck trying to learn task two because it's kind of already learned,
uh, something and it doesn't want to learn something else.
Essentially how the optimization gets stuck it's,
it's, it's, it's kind of in this local optimum.
Okay, um, and the other issue that you might run
into is maybe you have limited representational capacity.
So, um, multitask networks in
general often need to be much larger than single task networks.
This is, is very intuitive but it can be easy, uh,
easy to forget and in this case the, um,
the- if, if it isn't as large as it needs to be then you're going to see under fitting,
uh, and that's a kind of, uh,
a symptom of limited representational capacity.
Yeah. So that's a good question,
uh, about basically how to handle,
uh, like optimization challenges especially when one task has more data that another.
Um, one is kind of the thing that I mentioned before as you sampled,
sampled the tasks and basically bounced them such that they have the same amount of data,
such that it doesn't favor one task over the other,
um, because oftentimes if you don't,
if you don't do that, then it will learn the task
that has a lot of data and it won't learn the other one.
Um, people have explored curriculum learning techniques and we'll talk a bit about that,
um, later on in this course.
In general, the- I would say that
the optimization challenges aren't completely well understood.
Uh, we're still trying to understand kind of in general
the single task optimization landscapes let alone the,
the multitask optimization landscapes.
Um, I do think though that there's been a lot,
like I'm actually pretty optimistic about, uh,
being able to understand these sorts of challenges very soon, um,
and we may even like later in the course cover, um,
try to get a better understanding of
these optimization challenges and how to mitigate them. Yeah.
So I'm confused about how you actually define this negative test [inaudible] because,
um, because we don't have perfect overlap
between the different tasks otherwise they would be different so.
So when you have one network versus,
um, like say 10, uh,
different networks rating out different tasks,
assuming that all the training are like perfect to the network's capacity.
You then, uh, you got to have like
worse performance with a single network versus like all 10 networks,
right, because you have, there's like
extra information that you know, maybe the network isn't going to be able to express.
You expect larger networks but how do you compare if your network is larger?
So you're saying that you're like,
why would you expect positive transfer? Why would you-
Yeah.
Yeah. So one reason to- one situation to expect
positive transfer is when you don't have a lot of data per task, um,
and so if the tasks are related, uh, enough,
the features learned for one task will be
useful or the representations learned for one task will be useful for the other task.
Um, so it's essentially a way to share data, um,
and as I'll talk about in like two slides, um,
you can basically kind of,
another way to view multitask learning is essentially a form of regularization,
uh, and the if,
um, if you're kind of problem setting,
if regularization would be helpful basically, uh,
and more data would be helpful then those are
some of the settings where you definitely expect positive transfer.
[inaudible]
Generalization essentially, yeah. In reinforcement learning,
we'll see other benefits as well relating to exploration. Yeah.
Um, so if all the tasks have exactly the same data,
is it usually [inaudible] to train independent networks?
That's a good question. So you mean all the same inputs,
but not- but different labels?
Um, all the- all of them had the same input but I
want to use the same input with the different things.
Yeah. So in that case, um,
you can definitely still see benefits from multitask learning.
Essentially, each task has its own form of supervision, and, uh,
when I was talking about data sharing,
it's- it's- it's not necessarily actually about sharing inputs,
it's about sharing supervision.
Uh, and so if you have, um, basically,
each of those tasks correspond to different amounts
of supervision and that those can be used for building,
uh, more flexible representations.
More powerful representations that can be used for,
um, for better solving one of your tasks. Ah, yeah.
So if you wanted to move [NOISE] problem data transfer, but still, uh,
like maintaining their leverage their representations to learn one task.
Uh, could you just do like
a simple transfer learning where it most likely to do that when, uh, kind of [inaudible]
Yeah. You can definitely do that.
Uh, in practice fine tuning.
I'll talk a bit about fine tuning in the next lecture.
In practice that makes, um,
f- fewer assumptions about the availability of data,
because you assume that you can't, kind of,
access that first data set,
and so in practice many times.
If you have both data sets available,
you- you should use both of them, uh,
and you'll be able to get better performance. Yeah?
Is there a simpler kind of like similarity metric that can you use,
before you do this kind of training.
Or do you just have to form
the optimizations to see if it performs well and that is sort of to measure,
how much shared structure there is?
Yeah. That's a very good question.
Uh, my- I don't- I don't think that this- this question is necessarily known,
and I think there maybe are techniques where we
could better get a sense for this before training.
Um, but my sense is that,
how similar two tasks are is intimately tied to how easy it is to learn them together.
Uh, and so, I guess the pessimistic side of me would
answer that it's actually not possible to
a priori know how much you should share ahead of time.
Um, but I think it is kind of-
the more optimistic side of me says that it kind of
the optimizer should be able to figure it out.
Throughout the- like in an online fashion throughout optimization,
um, during the learning process.
So I don't think we can probably do it before learning but I think it can
happen during the learning process. Yeah?
Yeah. I wanna understand better why, uh,
Multitasking networking is larger networks.
So assuming we have multi- like the same network, um,
but it independent on train on different tasks,
and it works okay. But [inaudible].
It seems to me that [inaudible].
Because they're sharing the same like activation is
similar to the [inaudible].Or else in bigger networks would just be,
um, the backpropagate gradients, might be
more sparse within the network so the drug can't sell as much.
Yeah, so it certainly could be that larger networks help
because they alleviate optimization challenges.
Um, people have observed in general in single task learning that if you have
a larger network they actually sometimes optimize faster than the smaller networks.
Um, but separate from optimization,
there is also just the representational capacity of like.
If you're doing a single task that's one function and if you're doing another task,
assuming that task is different.
Uh, the representational capacity needed to represent both of those functions,
is greater than just an individual function.
But it may be that actually the optimization challenges
potentially could even have a- a greater impact
on- on this sort of, uh, observation. Yep.
So sometimes, um, when we test the machine learning algorithms,
we sort of generate the data set where we know Bayes error.
So its like w-we know how well an algorithm in principle could do.
Are they similar to sort of word problems in this setting where
beforehand we actually know how much positive transfer is possible.
Not off the top of my head.
But there- there certainly could be literature
on that given the extent to which people have studied it.
Um, yeah, feel free to like post that on Piazza or something.
And we can try to find something.
[OVERLAPPING] I'm trying to look for that particular,
because it's been hard to even understand like
because then would be more obvious to find some clear example
Here there will be. Yeah, I do suspect that we will
be able to find an example with real data, uh, in real problems.
[OVERLAPPING] But I think it will generate the,uh-.
Yeah, I think that- that would be interesting. Yeah.
Uh, on the note of like meeting, you know,
obviously all those [inaudible] .
Um, If you have like records,
in vectors at every single stage in
the network and it's multiplying all these parameters,
like, some values that will typically be a hidden value.
Will that be equal to like in,
you know the equivalence [inaudible] we'll have
the equivalence rendered to like these different networks.
Um, in principle yes,
if you use mostly the kind of multiplicative conditioning.
Um, so in principle it does have
the representational capacity to represent completely separate networks.
Um, and that's- that actually is one of the good reasons to use
that sort of gating, the, um,
in practice the optimizer might have challenge finding that- that solution.
Um, yeah, and so it may not actually be able to- to find that solution.
But technically it should have that capacity.
Okay.
And I guess one thing I will add to that is we actually have observed very recently that,
uh, in many cases it's actually very
challenging for the optimizer to find that solution, um,
in tasks where the- where the type,
the things that you're learning are very distinct from one another. Yeah.
Um, if we have for example, uh,
like one neural network sharing one task and also sharing another task.
Shouldn't we combine them so that, uh,
it becomes a multitask network?
So you- you can combine them but very naively and
stupidly by just kind of using a task selector to pick which one you want,
what network you want to use.
Um, to my knowledge there aren't any techniques, well.
You actually use overlap so you get some advantage [NOISE]
I don't know if any w- uh,
we'll cover this when we get to reinforcement learning.
Yeah. So there are some techniques that- that do kind of take two
trained networks for tasks, and then try to learn a single network,
um, that fuses them in some way.
Okay, I should probably get onto the next few slides.
So, um, the takeaway here is
you I- I- I cannot - don't even know if I've even said this yet
but I think all of you have inferred it is that if you
have negative transfer you can share less across tasks.
Um, tra- train a bigger network, um,
and one of the interesting things here is actually sharing versus not
sharing parameters is not just a binary decision.
Uh, you could actually also,
uh, do what's called soft parameter sharing.
Um, and so what this looks like or one version of this is where you have, uh,
you can break down your objective as I showed before into
shared parameters and task specific parameters and then
basically add this term that
encourages the task specific parameters to be similar to one another.
And you could do this at different layers of the network.
You could do it, um, for different pairs of tasks,
for some pairs but not other pairs.
You could also do it with respect to a single set of global parameters and tie
them all to a single set of global parameters
instead of doing this sort of pairwise loss.
Uh, and so what this looks like is you have two separate networks.
Um, and what you do is you kind of softly constrain the weights to be
similar to one another rather than actually having them represent the same exact value.
Um, so one of the benefits of this is it allows for
more fluid degrees of parameter sharing.
Uh, where if you set the weight on this loss this-this right hand loss to 0.
Then you have zero parameter sharing.
If you set it to be a very high, uh,
value such that- such that essentially constraining the optimization,
to set them, uh, such that they are equal;
then that corresponds to hard parameter sharing.
Um, the downside is that it is kind of yet another set of decisions and hyper parameters,
that you need to tune, uh, and- and make sense of when you're,
uh, thinking about your- your algorithm.
Okay. Um, and so now that I've talked about negative transfer, um,
another problem which is in- in many ways somewhat orthogonal to
this chall- or kind of the opposite of this challenge is the challenge of overfitting.
Um, and so if you observe that if you have only a small amount of
data per task and you find yourself overfitting to those tasks then,
uh, it actually may be that you're not sharing enough.
Um, it may be that you just need more data,
but it may be that, if you, um,
shared more that would act as a stronger form of regularization.
Um, and you may be able to overfit less. Okay.
So now, we wanna go into kind of a case study of actually applying
these sorts of multitask learning algorithms in practice,
and so this was actually a very recent paper that came out, um,
in the past two months and they were looking at,
uh, making recommendations to users for YouTube.
Uh, and it seems actually it's- it's from Google and,
uh, to my knowledge,
this is- some of kind of what we'll be covering here are some of the decisions that they
have made would actually making recommendations to users.
Um, yeah so here's an example.
You have, uh, you're kind of watching this, uh,
this lecture on deep learning and then on the right,
you might be getting some recommendations about,
uh, what video to watch next.
So there are a couple challenges that come up,
um, in this sort of, uh, problem.
And the way that they formulate it as a multitask problem is predicting, uh,
is to making predictions about user engagement and about user satisfaction.
Uh, so you essentially have these conflicting objectives
to think about should I generate,
uh, a video with that, uh,
a user will rate highly, uh,
can I predict whether or not the users will share
that whether or not they'll watch that video, um,
and the types of things that you share are not necessarily
the types of things that you would watch for a long period of time for example.
Um, the second challenge that comes up that I
won't have a lot of time to talk about today, but, um,
is also a bit interesting is that in
these sorts of recommendation systems you also have a form of
feedback loop where users may have watched something simply because you recommended it,
uh, and this leads to bias in your data.
Uh, and so they also have an interesting technique that, uh,
they used to address this bias, uh,
and if you want to kind of find
out more information about that, you can look at the paper.
Um, Okay.
So how do they actually set up this problem in this framework?
So the input is what the user is currently watching which is called the query video,
and they also know some features for that user.
Uh, and then what they do is they first generate candidate,
a few hundred candidate videos that they might think that user might wanna watch,
they then rank the candidates, uh,
and then they served the top ranking videos
to the user in that recommendations bar on the right.
Um, the candidate videos, um,
this wasn't really the main focus of the paper,
but the way that they do this is they pull videos from
multiple- multiple candidate generation algorithms.
These algorithms do things like try to match
topics of the video that you're currently watching.
Um, they also try to, uh,
look at the videos that are most frequently watched
together with the video that you're currently watching,
um, as well- as well as a number of other,
uh, forms of metrics,
uh, and then kind of the central topic of- this paper was given,
uh, a large number of candidate videos,
can you rank, um,
can you figure out what basically rank the- the
that those candidates into an ordered list,
uh, where the top things correspond to the ones that you actually want to serve.
Okay. Um, so how do you actually formulate the ranking problem?
Uh, what this looks like?
Again, similar to before,
your input is gonna consist of the query video and features of the user.
It's also gonna have- the input is also gonna be one of
the candidate videos so there's gonna be operating on each of the candidate videos.
Um, and interestingly in this case,
things like the query video and the candidate video,
uh, these are actually literally, uh, the videos,
they correspond the information about those videos like, um,
features of a video such as,
um, the to- the topic, the title, um,
the upload time, uh,
and then also user features like, um,
user and context features such as their user profile,
the time of day, um, and a variety of other things.
So this includes both, uh,
textual data as well as non-textual data that they u- have
used kind of different shared embedding network
to embed into a common representation space.
Then the output of this model is to determine the engagement and
satisfaction of the candidate video for the user after watching the query video.
Um, and so engagement include things like whether or not they clicked on the candidate,
uh, that this, er, video,
whether or not they, uh,
spent time, uh, watching that video.
So this is both classification and regression.
Uh, and the satisfaction corresponds to things like clicking like on that video,
uh, as well as,
um, rating that video for example.
So again, this is a both a kind of a mixed discrete and continuous problem.
Uh, and then kind of the big question is how do you figure out the score from these,
uh, from these different metrics and, uh,
what they do is they actually just use a very simple weighted combination of
these predictions where the weights of those predictions are just manually tuned.
Um, and the- the reason why it, the kind
of the motivation that they had here is that the- the system is very complex, uh,
itself and also the, um,
the metrics for like for whether or not, um, for whether or not,
you're actually serving good videos are online metrics and they're ones that you cannot,
um, they are very difficult to optimize with respect to.
And so by doing this in a way that's kind of done manually,
um, it's a bit simpler,
it- it- it's easier to kind of, um,
to- to iterate on the- on the model essentially.
Okay. Um, so let's talk about
the kind of how we actually predict engagement and satisfaction.
So, um, they considered some basic option,
which corresponds to the multi-head architecture that we
talked about before where you have some shared layers and
also some layers that are predicting engagement and, uh, and satisfaction.
This was kind of their- their baseline.
Um, and one of the things that they
found is that when the correlation between the tasks is low,
that will harm the learning process.
As we've kind of talked about before it's sort of negative transfer, right?
Um, and so what they did instead, uh,
is they used, uh,
a form of soft parameter sharing.
And this looks like this,
uh, where I'll go,
I'll go through the details kind of step-by-step.
So you have some shared bottom layer,
um, and the goal is, uh,
the kind of a sort of mixture of experts model is to
allow different parts of the network to specialize.
So, um, you have these different expert neural networks,
uh, that- that you want to be specialized in different me- in different ways.
Uh, so you kind of- you can see, this example has two different experts.
Then you want to decide which expert to use for an input and for a given task K. Uh,
and so what this corresponds to is you have is a different set of, uh,
weight matrices for each task and you have,
uh, an input and you pr- uh,
do a linear combination and a softmax to produce, um, this, uh,
this distribution over which expert to use and
then you compute the features from the selected expert,
so you, um, there's this bottom layer corresponds to the,
uh, uh, sorry, the- the- the expert corresponds to fi of X.
You then, uh, multiply this with the gating function in order to produce the,
uh, the kind of the selected expert output.
Uh, and then lastly, you compute the output of this.
So, uh, then you take another neural network h that's gonna
take the output of the expert and produce the result.
Um, so this is an example of an architecture that starts out
shared, branches out into multiple- multiple experts where it actually made,
it may use multiple or it may use just a single expert and then comes back to a single,
um, part of the model.
Okay. Um, so in the experiments,
the- the set up here was actually quite interesting.
Uh, they used TensorFlow and TPUs as you might expect.
Um, they trained in temporal order where they're feeding in the videos,
uh, over time, uh,
and training continuously to consume
the newly arriving data and they said that this is really important so that, um,
because the distribution over time of use- user engagement is changing,
um, as, um, as different trends become popular, et cetera.
Uh, they had various offline and online metrics.
For offline, they looked at AUC for classification tasks
and squared error matr- metrics for- for regression tasks.
Uh, but the most important things were actually
the online metrics because those were actually, um,
the ones that you ultimately care about, uh,
and so they did AB testing, um,
in comparison to their production system.
Uh, and the, uh,
the live metrics for this online testing
corresponded to things like the time spent on, um,
on YouTube, uh, survey responses that
different users filled out and the rate of dismissals,
which I- I'd guess would kind of, um,
relate to whether or not they're actually kind of dismissing suggestions or,
uh, by the recommender system.
Uh, and then the last thing
worth noting here is that because this is a massive system, uh,
computational efficiency also really matters
and so this is one of the reasons why they didn't
consider independent training and why they really
want to do multi-task training such that they didn't have to,
uh, ha- basically compute everything separately for all of these different metrics.
Okay. Um, so here are the results.
So the- the top two rows are showing the-
the baseline network that had- that corresponded to
a multi-head network and the bottom two are corresponding
to the mixtures of- mixture of experts, uh, architecture.
What we see is that there's a a substantial increase
in engagement and in satisfaction, uh,
at around 0.4% and 3% for the larger mixture of experts model,
which is, uh, which is nice to see.
Uh, and also, they looked at how different tasks are utilizing the different experts.
So each of these bars,
um, correspond to a different task.
So you can see that expert 7 is typically doing things for task 4 and for task 1.
Whereas, expert 2 is typically, uh,
computing features for task 2 and for task 3.
Um, and then lastly, they found that 20% of
this sort of gating polarization where each,
uh, expert would, uh, would kind of, uh,
you- you would basically actually like not use an expert
at all and what they found is that,
um, if they use some sort of drop out such that, um,
to prevent it from like only using kind of
one expert for all the tasks so we are not using one expert at all, um,
this was- was helpful for mitigating that .
Okay. Yeah?
Why would you not [inaudible] ?
Sorry, can you repeat the question?
If, if for example one task was different but the other one for some-
for some reason given to chose for specific action.
Yeah. So I think that what they mean by polarization here is that they're
actually using one expert for all of the tasks,
um, or not using one expert at all.
Okay, um, let's take a quick like one-minute break.
Uh, you guys have been asking a lot of questions,
which is awesome, uh,
and then we'll try to get into some meta-learning.
But [inaudible] in the, in the [inaudible] Do they have any way to compare how
well these approaches, like some [inaudible].
Give an example of the other recommenda- recommender system.
Let's say, um, somewhere I kind of agree [inaudible].
I don't have a sudden- in reading the paper,
I guess I didn't have a sense for if they did a comparison of
this approach versus other things like collaborative filtering.
Um, but I guess I would encourage you to take a closer look at the paper to see if they,
they talked about that [NOISE].
Okay. Let's get started on the second part.
Um, so let's talk about meta-learning.
We may not have time to get through everything,
but if we don't then I'll,
um, I'll cover what we didn't cover at the beginning of lecture on Monday.
Well, um, so to start off, um.
In many ways I think that are kind of two ways to view meta-learning algorithms.
Uh, one is somewhat of a mechanistic view,
and another is somewhat of a probabilistic view.
So the mechanistic view is a view that I think is very helpful for figuring, uh,
out how you actually go about implementing one of these algorithms,
and really understanding kind of the underlying mechanics of,
of how things work and how the implementation works.
And the probabilistic view I find to be very helpful for understanding,
um, conceptually and intuitively what these algorithms are doing.
Uh, and so the,
the kind of mechanistic view is that you kind of have this deep neural network that
reads in datasets and make- makes predictions for new data points,
such that when you see a new dataset and you can make predictions for new data points.
Um, and training for this network uses a dataset of datasets,
um, each for a different task.
And, yeah, I could have- as I said this makes it
easier to implement meta-learning algorithms,
uh, whereas kind of at a high-level the probabilistic view is looking at, um,
meta-learning as learning priors, uh,
essentially learning priors over tasks tha- in a way that
enables kind of combining that prior with the small amount of
data for a new task in order to more efficiently learn.
Um, yeah, so kind of learning a task for new prior uses a small amount of data.
Uh, and I find that kind of thinking about it from
a Bayesian perspective can make it a bit easier to understand what's going on.
Uh, so I'm going to start with the probabilistic view, uh,
to give some intuition for what these algorithms are doing,
and then, uh, I'll go into
the more mechanistic view of actually how you implement these algorithms.
So for the first part,
don't worry about how you would actually go about implementing things.
Uh, it's more about some of the,
the conceptual aspects of things.
So, uh, we're gonna kind of go back to problem definitions here, uh,
and kind of redefine things from
a Bayesian perspective in order to view meta-learning as a Bayesian perspective.
So, uh, you can view supervised learning as, uh,
a maximum likelihood problem,
where you're maximizing the likelihood of your parameters given your data.
Uh, and in this case Phi is gonna be the model parameters and D is gonna be the dataset.
Uh, and of course,
it's kind of as before the dataset contains x-y pairs where you have inputs and labels.
Um, and you can also,
uh, view this, uh,
kind of redefine it as maximizing the probability of the data given your parameters,
and maximizing the marginal probability of your parameters.
Um, so the first term is your data likelihood,
and the second term may be something like a regularizer,
like something like weight decay for example,
which corresponds to putting a Gaussian prior on your weights with a fixed variance.
Okay. Um, and so then what this looks like is you have, uh,
some optimization over your dataset,
uh, and given your weights and then you have your regularizer.
Um, unfortunately if your data is very large,
uh, as we kind of, uh, if you're, sorry,
if your dataset is very small as we talked about,
um, then you might overfit to that dataset.
Uh, and even if you have a regularizer there,
it may not be enough to, uh,
prevent you from overfitting or it may, uh,
not- it may kind of not,
not be informative enough of actually
what you should be learning or kind of the model that you're trying to learn.
Um, so the question in meta-learning,
and kind of the key problem that it's trying to answer
is when we are solving supervised learning problems,
can we incorporate additional data in some way?
Like maybe we're not learning from scratch,
maybe we have experience from,
from before that we've,
uh, kind of, we've experienced the world before at this point.
Um, and that's what it's gonna be called the meta-training data.
We want to be able to use the meta-training data such that when we perform a new task,
we can, uh, learn it more effectively and more efficiently.
Uh, and in particular in,
in meta-learning, we- this additional data
is actually going to correspond to a set of datasets.
Uh, basically a set of tasks or datasets that's corresponding to tasks.
We wanna be able to use those datasets such that when we see a data for a new task,
we can learn parameters for that task.
Um, and then of course each of these individual data point- each
of these individual datasets will have k-data points.
Um, so for example,
if you want to solve a few shot classification problem,
maybe you're given five images and you wanna be able to classify new images,
uh, as being among one of those five classes.
Uh, if you try to train a neural network from scratch on this problem,
it will overfit massively or if your regularizer is too strong, it won't do anything.
Um, but if you have data from other image classes,
uh, like shown here,
then, then this is your meta-training data.
Then you may be able to use that meta-training data in a way that allows you to learn,
uh, from only five examples in a manner that is effective.
Um, and of course this doesn't need to be image classification problems.
This could be, um,
different kind of sinusoidal prediction problems.
It could be, um,
really a variety of other problems that,
I do language classification or, or kind of,
yeah, uh, decoding text for example, etc.
Okay. Um, so that's kind of the problem that we wanna solve, um,
but what if we don't wanna keep her- keep running our past experience forever.
What if we, uh,
if we're gonna be training on,
on all our past experience across tasks then,
um, we don't wanna assume that we
have to access that every time we want to learn a new task.
Uh, and so what we're gonna do is we're gonna try to kind of compile
our meta-training dataset down to a set of parameters Theta,
which I'm going to call the meta-parameters.
Um, and Theta is basically gonna correspond to whatever we
need to know about the meta-training data in order to solve new tasks quickly.
So, um, basically if you kind of
view the- if you want to introduce this kind of intermediary variable, um,
if we write out the likelihood of our parameters for
our dataset given our past data- our past meta-training data,
we can write this out, uh,
in a way that integrates over,
uh, our meta-parameters Theta.
Um, so we first and for our meta-parameters data given our meta-training data,
and then we can make predictions of our, our,
our prediction about our kind of
our task parameters given our data and our meta-parameters.
Uh, and note that this assumes that the, um,
our task-specific parameters and
our meta-training data are conditionally independent based,
uh, conditioned on Theta.
Uh, and we can basically approximate this,
uh, integral with- this is a fairly crude approximation,
but we can approximate this integral with
a point estimate for our meta-parameters, where the first,
uh, the- kind of the right-hand side of this is going to correspond to meta-training,
where we wanna learn a set of meta-parameters given our meta-training data.
And the left-hand side of this objective is gonna correspond to
adaptation where we wanna be able to learn new parameters for a new task,
given data from that task and our meta-parameters Theta-star.
Okay. Um, where Theta-star is equal to basically,
uh, argmax of log p Theta given our meta-training data.
Okay. So essentially this right-hand side is the meta-learning problem where we're
optimizing over our meta-parameters such that when we use them for,
uh, for adaptation, we can,
um, effectively learn parameters for that task.
Okay. Um, so meta-training is that cap- that second term and,
and, and adaptation is that, uh,
is the term now outlined in red.
Uh, let's first go over,
how we can go about adaptation.
So we can, kind of, look at a very simple example,
and we'll revisit this example,
um, at the end.
Uh, so say, we wanna be able to make predictions, about test data points.
Um, the parameters corresponding to this neural network will correspond to Phi star.
They'll be the parameters for that task.
Uh, and we want to be able to infer these parameters from data.
So [NOISE] one very simple example,
for how we might put- uh,
infer these parameters from data,
is by training a neural network to take in,
the dataset and output Phi star or Phi star for that task.
[NOISE] Uh, and so uh,
where each of these examples correspond to our dataset.
Uh, and so essentially,
what this would correspond to is,
um, we would have, kind of,
these different meta-training datasets [NOISE] for,
for this dataset D, we would pass them to the neural network.
We would then give it a, uh, a new data point,
and we wanna be able to have it make a prediction for that data point,
uh, based off of the computed task specific parameters.
Um, so you add this to your test input and then your inpu- uh, your test label.
Now, uh, one big question is okay,
we have this- this is, kind of, how we're gonna do the adaptation process.
How do we actually, go about meta-training this, uh, this model?
So, um, how do we actually basically,
learn the parameters of our recurrent neural network theta star,
such that it produces the right task specific parameters Phi star?
Um, and so the key idea here is we can, uh,
basically, match test-time to training time.
So the test-time, we're gonna be predicting Phi star,
in order to make predictions about test data points.
We want to be able to do the same thing in meta-training and basically,
train this recurrent neural network,
such that it produces parameters that lead to effective predictions or,
or, or accurate predictions.
So essentially, the key idea is that,
uh, train and test conditions should match.
Um, that's basically like the,
the principal rule of meta-learning basically.
Uh, if you wanna be able learn at test-time,
you should basically train it to learn during meta-training.
Essentially be learning how to learn.
Um, so and then you can,
kind of, view the learning process, uh,
the adaptation processes at
sort of, meta test-time,
uh, if, if the former process is meta-train time.
Okay. Um, right, so the process of optimizing for theta is meta-training,
the optimize for, for producing Phi is meta-testing.
Um, now, one of the things
that's pretty challenging if at meta-training time you're actually training it,
to make predictions about test data points,
um, where do these data points come from?
Uh, how do we actually,
optimize for it to make predictions on new data points.
Uh, and the important thing here is that, uh,
we need to be able to essentially reserve a test set
for each task that we're gonna be meta-training over.
Uh, so if we have, uh, a set of tasks,
then we have basically trained data for each task,
we also need corresponding test sets for each of those tasks.
Uh, and these test sets, uh,
are going to be held out images,
from those same image categories.
Now, you might say, okay now you're gonna be testing-training on the test set.
This doesn't seem right? Uh, [LAUGHTER] uh,
[NOISE] and that is- that's a good intuition to have.
[NOISE] But, uh, in this case,
we- now- are now moving from meta-training- from training set to the meta-train sets,
and test sets to meta test sets.
So each of these tasks,
the training set and the test sets for the task correspond to the meta-training dataset.
And then at meta-test time were given new tasks.
Uh, and we don't wanna train on the meta-test set.
Okay. Um, so we have our meta-learning data,
this corresponds to training and test sets for every task, uh,
where each of the training set- datasets corresponds to K data points.
Each of the test data- datasets correspond to a new set of K data points.
Um, yeah.
Okay. So the complete, kind of,
optimization problem is that at
test-time we're gonna be inferring a set of task specific parameters.
Which can be some, some function that takes as input
the training dataset and outputs the task specific parameters.
Where the parameters of that function or the meta parameters are theta star.
Um, and we essentially wanna learn a set of meta parameters such that,
this function is good for held-out data points,
after being- after ge- getting the training dataset as input.
Okay. Um, so essentially you can view theta star as optimizing this,
uh, objective [NOISE] where we want to optimi- optimize the,
the probability of the parameters,
um, being effective at new data points.
Okay. Um, cool.
So you could also,
look at this as a graphical model,
where theta is essentially your prior,
Phi i is your task specific parameters for a given task,
and you have a training set and a test set for,
um, for each task.
So Phi is, um,
Phi leads to, uh,
the data and theta acts as a prior on the task specific parameters for all the tasks.
So theta is essentially the information that's shared across tasks.
Okay. Um, and then this dashed line does means that we don't know the,
the kind of test labels for new tasks at meta task time.
But we do know the training dataset.
Okay. Um, some terminology
before I get into some of the more mechanistic view of meta-learning.
So, um, I talked about this.
We call- I call this the meta-train dataset.
Um, each of these- each of row- each of these rows is a meta-training task.
Uh, and each of these tasks has a corresponding train set and test set.
Uh, the, the left side of these can be viewed as the training data,
and the right side can be viewed as the test data.
Uh, but another common terminology for this,
is support set and query set where you have some set of
support images or training images and some query images or test images.
Uh, and f- if you have a K-shot learning problem or a few shot learning problem,
K is the number of data points in your training dataset.
So if you're doing one-shot learning you're doing- you're learning from one example,
if you're doing K-shot learning you're learning from K examples.
Okay. Um, how does this relate to other problem settings?
So multi-task learning like we talked about at the beginning of this lecture,
you learn a model with,
um, with a single set of parameters that solves multiple tasks.
Uh, you don't care about generalizing to new tasks.
[NOISE] Uh, and this essentially can be
seen as in somewhat of a special case as meta-lear-
as meta-learning where your task specific parameters and your, um,
your prior parameters are the same, uh,
such that you just, um,
kind of, have a single network that is representing all of the tasks.
Uh, and in many ways multi-task learning is a prerequisite
for meta-learning because if you can't solve the training tasks,
then you have no chance of being able to learn new tasks, uh,
more efficiently than, um,
than like learning from scratch for example.
Um, it's also related to things like hyperparameter optimization and, and auto-ML.
So, uh,
in hyperparameter optimization you can essentially view
theta as the hyperparameters and Phi is the network weights.
Uh, and in architecture search you could use theta as the architecture,
Phi is the network weights, uh,
and both of these are a,
kind of, a very active a-, uh,
areas of research and outside of the scope of this class, of this tutorial.
Okay, um, I think we're mostly out of time,
and so I will cover the next two parts of the lecture in the next lecture,
and we'll probably push back the,
um, the due date for the homework assignment, uh,
probably like a couple days, such that we,
um, you guys have enough time to complete it.
Um, are there any questions about what I covered with
meta-learning before we wrap up? Okay [NOISE].
A couple quick reminders before you leave the,
um, Homework 1 is posted today and is not due on Monday, October 7th.
Uh, fill out paper preferences by tomorrow, uh,
and the TensorFlow review session will also be tomorrow at 4.30 PM.
Bye.
 Hi, everyone. Let's get started.
So, uh, today we'll be covering
the things that we didn't get to last time with regard to, uh,
meta-learning and black-box adaptation approaches to meta-learning,
and then we'll cover topics in optimization based approaches.
Uh, so before we get started, a couple reminders.
So first, Homework 1 is due on
Wednesday next week and that homework assignment is now out,
uh, and yeah, encourage you to- to get started on that, uh,
early, and then also the first paper presentations
and discussions of papers will be happening on Wednesday this week, uh,
and so please, uh,
show up so we can discuss those papers and also,
uh, kind of be a part of the discussion for
the students that are presenting, uh, that day as well.
Okay, um, so as I was mentioning today we'll first,
uh, start by, uh,
actually by recapping the probabilistic formulation of
meta-learning that I mentioned at the end of lecture last time,
and then cover kind of a general recipe for different meta-learning algorithms,
uh, and cover black-box adaptation approaches.
These, uh, kind of these two things are
the topic of Homework 1 where you'll be implementing,
uh, a black box approach to meta-learning.
And then we'll be talking about optimization-based meta-learning,
and this will actually be part of Homework 2,
uh, and, uh, the rest of Homework 2 will be covered,
uh, in the next lecture on Monday next week.
Okay, so first, let's recap from last time.
So we were talking about, uh,
kind of a- a more intuitive or- or probabilistic
view to these meta-learning algorithms, uh, and in particular,
we can view meta-learning as a process of learning,
a set of meta-parameters Theta, uh,
that summarizes your meta-training data such that you can solve new tasks quickly.
Uh, and what this meta-training data looked like was,
uh, you had a range of tasks, 1 through n,
and for each task you had a train dataset and a test set,
uh, and the train dataset had k data points and the test set had k data points.
Uh, and so in particular,
what meta-learning was trying to do was to optimize for a set of meta, uh,
for a set of meta-parameters, uh, that, uh,
maximized the likelihood of those parameters.
So in particular, you could view kind of the meta-train processes
optimizing for these meta-parameters and
the adaptation process as adapting those parameters,
uh, to compute a set of parameters Phi, that, uh,
can solve a new task given
a train dataset for that task and the meta-parameters that you learned.
Uh, and so you could essentially view, uh,
kind of this adaptation process as this function f that's taking in a train dataset, uh,
and producing a new set of parameters five-star, uh,
and kind of under this, uh,
view of, um, of that adaptation process,
you can kind of view, uh,
meta-learning as optimizing for
the meta-train parameter such that the task specific parameters uh,
do well on held out data, your test set,
where the task specific parameters are computed according to
your training dataset for that task.
Okay, so this is like essentially the probabilistic view
on meta-learning where you can view, uh,
kind of the meta-training process as trying to optimize for these, uh,
these kind of prior parameters such that adaptation leads to good performance.
Okay, so now I'd like to talk about
how we actually kind of design algorithms that perform this optimization at- at a,
basically at a more mechanistic level and uncover kind
of how you actually go about trying to implement some of these things.
So, uh, in particular we, like,
can we think about a general recipe for meta-learning algorithms?
Uh, and before we actually cover a general recipe for the algorithms themselves, uh,
we need to have a sense for how we're actually gonna be,
going to be evaluating these meta-learning algorithms.
Um, so I want to first talk about how to evaluate,
um, a meta-learning algorithm,
and, uh, kind of,
the first thing worth mentioning here, uh,
the first thing, uh,
that we should mention is the- the Omniglot dataset.
So this is a dataset, uh,
that was proposed by Brenden Lake e- et al in 2015, uh,
and it actually really, uh,
kind of exemplifies some of the- the weak points
with neural networks that they're gonna be learning from small amounts of data.
So, uh, this dataset has six hu- six fi- 1,600 characters from 50 different alphabets.
Uh, here's some examples of the,
um, of the dataset.
So there's different alphabets uh like Hebrew, Bengali etc, uh,
and each character has- has only 20 instances.
Uh, so unlike something, uh,
like MNIST that has a few number of
characters and a huge number of data points per character,
uh, in- in many ways this is sort of like the transpose.
It has many classes and few examples per class.
Uh, and one of the things that I think is- is quite appealing like,
uh, to a dataset like this is that,
the statistics of this dataset are in many ways more
reflective of- of the types of things that we see in the real world.
Uh, for example, if you, uh, kind of are trying to learn how to recognize, uh,
forks for example, you're not going to see thousands,
uh, thousands of different types of forks.
You may see, uh,
uh, a wide range of objects,
but you're only gonna see per object,
a small number of instances of that object throughout your lifetime.
Um, okay, so this dataset has kind
of the breadth of classes and- and a small number of examples per class,
uh, and they propose a few different ways you could try to use this dataset.
So they propose both few-shot discriminative learning
as well as few-shot generative learning problems.
Uh, and in particular what these look like is, uh,
the few-shot discriminative learning is given a few examples of new characters,
can you learn to classify between those characters?
Uh, and the derivative problem is likewise given a few examples of some characters,
can you actually generate new instances of those characters?
Uh, and they essentially show that things like deep neural networks,
uh, struggle at- at this sort of problem if you're gonna be,
going to be training them from scratch because if you're only
training them on a few examples,
uh, we know that deep neural networks do best when you have a large number of examples.
Um, and initial approaches towards this kind of problem, um,
actually predating the Omniglot dataset itself, uh,
instead used things like Bayesian models
and non-parametrics in order to solve this problem.
Um, great, so this is kind of, uh,
one kind of canonical example for a meta-learning dataset and there are
a wide range of others that have also been used for meta-learning more recently.
Uh, these include things like the MiniImagenet dataset, uh,
the CIFAR dataset, um,
CUB, CelebA, uh, and a number of others.
Uh, in all of these datasets,
kind of the goal is to,
given a small number of examples,
be able to learn something from that small dataset.
Okay so this is, uh,
this is kind of on the dataset side, this is,
this is the kind of datasets that you can use to evaluate a few-shot learning algorithm.
Uh, now how do we actually go about evaluating an algorithm on these datasets?
Uh, so this is actually gonna look a lot like the tests that I gave you on
the first day where your goal is to classify new examples from a small dataset,
and so in particular, let's say that we have a 5-way,
1-shot image classification problem.
Uh, and in particular, we could have,
um, one example of five different classes shown here.
Uh, way means the number of classes,
shot means the number of examples per class, uh,
and then your goal is given these, uh, five examples,
classify new examples as being among one of the five classes on the left.
Okay so this is, uh,
the few-shot learning problem and in meta-learning,
our goal is to be able to leverage data from
other image classes in order to solve this problem.
Just like kind of, be- be able to leverage the meta-train dataset that I was mentioning
before in order to learn a few shot classifier that can,
kind of, learn from these data points on the left.
So the way that we can do that,
if we can structure,
uh, the data into training sets and test sets,
just like I was mentioning before,
where these are going to mimic what you're going to be seeing
at test-time, matching meta-training time and meta-testing time.
So you can take five other image classes- classes
and break it into a train set and a test set,
and do this for a wide range of other image classes that you've seen in the past.
Uh, these will be your training classes,
uh, and you'll perform meta-training across these training,
meta-training the classifier such that after it sees the images on the left,
it can successfully classify images on the right.
Uh, and then critically after you do this, uh,
you'll test it on held-out image classes as shown on the top, uh,
it will essentially be able to perform this few-shot learning problem.
Uh, and this isn't specific to image classification,
you can replace image classification with
a regression problem, language generation problems,
skill learning problems, uh,
kind of, as I alluded to you in previous lectures,
I- each of these tasks shown as rows is essentially a machine learning problem.
Okay, so any questions on this setup. Okay, yeah.
[inaudible]
Yes. The nuance here is that,
in multitask learning your goal would be to try to
solve all of the training tasks shown in this,
in this box, in the gray box.
Whereas in meta-learning your goal is to use
these training tasks in order to solve new tasks with small amounts of data.
So kind of being able to actually evaluate on new tasks and quickly
learn new tasks is the critical difference between the two problems.
Okay, um, so kind of more broadly and more
generally we can kind of view
the meta-learning problem from a more mechanistic standpoint.
Um, and so in particular if we say supervised learning is trying to learn
a mapping from X to Y given input output pairs,
we can view meta-supervised learning as trying to learn from
a dataset to make- where this dataset
contains k input output pairs for a k shot learning problem.
To make predictions about new test data point X test.
So our goal is to kind of produce a function that takes as input
a training data set and a test input and
produces the label corresponding to the test input,
so there's a more mechanistic view of
meta-learning is essentially that we want to learn this function f. Um,
the function f that takes in the training dataset
and the test input and produces the label.
Now the way that we learn this, uh,
this function is through a Meta-training dataset which contains
a set of tasks or set of datasets where each dataset consists of
X Y pairs where you'll use at least k to be used for
the training dataset and
at least one additional data point to be used to
measure generalization to actually train it such that,
uh, it does well on new data points.
Now, uh, why is this view use- view useful?
So we, we kind of saw the probabilistic viewpoint before, um,
one of the nice things about this particular problem statement
is that it reduces the problem of
Meta-learning to that of designing and optimizing this function f. Uh,
once you kind of design a function f and
optim- and kind of decide how you want to optimize it,
uh, then you've created a Meta-learning algorithm.
Okay, um, how does this connect to the probabilistic viewpoint, uh,
well you can view supervised learning as doing inference over parameters given a dataset.
Similarly, you can view the adaptation process of Meta-learning as doing
inference over your task specific parameters Phi i given a training dataset and,
uh, and a set of meta parameters and the Meta-learning optimization as doing, um,
maximum likelihood, uh, inference over the meta parameters,
uh, over all of your training tasks.
Okay, um, any questions on kind of the problem setup before we get into algorithms. Yeah.
Um, is it important to use the proper value for k or is it-
Yeah, that's a good question. So typically algorithms assume that you know,
um, you know something about the k that you'll be evaluated on at test time.
So if you're going to evaluate on 10 shot learning or 100 shot learning,
then you'll, uh, train for those values,
uh, and you can train for, um,
depending on the algorithm you can t- train for exactly the value that
you think you're going to have at test time or a range of values,
such that, um, it can adopt to a range of dataset sizes. Yeah.
What happens when you don't know before
[inaudible] all the tasks that you are going to [inaudible]
because of some feature parameter [inaudible].
So your question is what if you don't know
the test task that you're gonna- going to be evaluated on?
That- like say you are theoretically adding new [inaudible]
Yeah, so generally the assumption here,
uh, is that the,
the test task that you're being evaluated on
is from the distribution- the same distribution as the training tasks, uh,
and what some, some algorithms do
better than others when you break that- break that assumption,
uh, and I'll talk about that a bit more in the second half of this lecture.
Uh, there is also kinda this online setting where we're
incrementally adding tasks and that's a setting that has been explored a little bit,
uh, and I'll talk a little bit about probably,
um, as when we talk about lifelong learning in the course, uh,
and then we'll also talk a little bit about
set, like later in the course about settings where
you just know nothing and you just have like an unlabeled dataset and,
and how you might be able to try to construct tasks automatically.
Was there another question? Yeah.
Is Meta-learning required to extend network structure?
So you're asking is it required to use
the same network structure as supervised learning or-
Um, supervised learning.
I guess we'll, we'll get into the,
the kind of what different architectures you can use for different algorithms, uh,
later in the lecture
and then if you still have the- a question you can ask it maybe towards the end.
Okay, great. So, uh,
the general recipe for what an algorithm looks like, uh,
is basically what I alluded to before is choose some form of this function that is, uh,
that could be probabilistic or it could be
a deterministic function as I mentioned before where you're going to be
outputting a set of task specific parameters given
a training dataset and your meta parameters [BACKGROUND]
and then once you choose the form of this
then you need to just figure out how you want to choose
to optimize your parameter's data with respect to your,
um, Meta-training dataset, this,
this choice is usually somewhat relatively straightforward using standard,
uh, neural network optimizers.
Okay. So this is kind of the general form, uh,
and most Meta-learning algorithms vary based off of the first plan, uh,
basically how do you actually design this function that's going to infer
task-specific perimeters and so
the first class of approaches that we'll look at are going to,
going to be considering, can we treat this,
um, this, this distribution as an inference problem?
And in particular neural networks are pretty good at doing things like
inference and so can we just treat this function as a neural network?
Um, and this is where, uh,
what I'm gonna refer to as Black-Box approaches come in, so, um,
what these Black-Box adaptation approach is they essentially just
train a neural network to represent this function right
here that's at- a neural network that's going to be
outputting parameters given a training dataset and a set of meta parameters.
And so first for now we're going to be using
a deterministic or point estimate of this distribution,
um and we'll kind of get back to Bayesian approaches in a couple of lectures, uh,
and the way this looks like is you can have, uh,
some neural network that,
uh, has parameters theta,
it takes as input the training data,
it can take it as input in a sequential fashion or it can take it in, uh,
kind of all as one batch and it outputs a set of task specific parameters Phi i.
Uh, then you have a separate neural network that's
parameterized by Phi i that makes predictions about test data points.
And this is essentially your, uh,
the test data, you can basically train this, uh,
train everything using your test dataset, DI test.
Uh, and so this is like really simple, uh,
and some of the nice things about
this is we can just train it with standard supervised learning,
um, so we can say that the, um,
we want to be able to, uh,
maximize the probability of the labels under the distribution that G is producing,
uh, [BACKGROUND] for all of the test data points,
uh, and for all of the tasks in your Meta-training dataset.
Um, so essentially what you're doing is you're training
this neural network such that it outputs parameters that, uh,
represent an accurate classifier [BACKGROUND]
Um, so if you denote this right-hand part as
the loss for a set of parameters Phi given a test data point,
then you can essentially view this optimization as,
um, as the, uh,
the loss function between the parameters that are outputted, uh,
will also ensure that takes some parameters that are outputted by f Theta,
evaluate it on your test dataset averaged over all of your tasks.
Okay, um, any questions on this? Yeah.
Uh, so when you evaluate your model which Phi i do you use?
Great, so when you evaluate your model you're given a new task, uh,
and so you're given a training dataset for a new task and so what you do is your, uh,
for your test task you basically pass in that training dataset into
your network f Theta, uh, and,
and produce your parameters for that task. Yeah.
So are those data and Phi i learned in this neural network?
So in this case,
the, um, during the meta-training process,
the parameters Theta are learned and the parameters Phi
i are somewhat dynamically computed per task.
Um, so in this sense, Phi i is almost treated more as, um,
activations or a ten- or a tensor rather than actual parameters,
um, which is somewhat of an interesting concept.
Um, the- yeah, basically you can ba- back-propagate the loss with
respect to Phi into the meta-parameters Theta. Yeah.
[BACKGROUND]
Yeah. That's a good question.
So, uh, the question is relating to the,
um, basically should you be,
like, uh, in the homework where you're also passing in y test,
uh, as input to,
uh, the right-hand side and, and, and zeroing it out.
Uh, and the reason for that is that if you have this,
kind of, this type of architecture that's an LSTM, um,
and you want to basically be sharing weights across time for each of these units,
then you want the shape of the tensors, uh,
shape of the, the inputs at each,
uh, at each data point to be the same.
Uh, and so if you want that,
then you want to be able to basically pass in, um,
pass in the same shape, but,
of course, you don't want to give it the,
the ground truth label, uh, because the label is what it's supposed to be predicting.
[BACKGROUND]
Um, so the question
was to zero out the label value or the embedding value?
[BACKGROUND].
Of y. Um, I think that basically,
as long as you're not passing in y test as input in any way,
er, you're, you're in good shape. Yeah.
[BACKGROUND]
Are you asking about- can you maybe repeat your question?
[BACKGROUND]
Oh, um, right.
So you're asking basically that this top function here,
is Theta an input or is it parameters?
[NOISE] Uh, in this case it is, um,
it is the, kind,
of the parameters of that model.
Uh, and so maybe a more standard notation would be to either put this Theta,
um, as subscript to the p or, uh,
put a semicolon here to indicate that it is parameters rather than an input.
[BACKGROUND]
Uh, during meta-training, we're optimizing over Theta.
[BACKGROUND]
Yeah. In the inner loop you're producing Phi. Yeah.
[BACKGROUND]
Right. Yeah. So Phi- yeah, exactly.
Phi is computed at test time given the training dataset as input.
Yeah. Uh, and let's go,
let's go through a couple more of the details here,
uh, before we answer any more questions about this.
So the, um, kind of,
this is, um, the- I've just covered what the objective is.
Now, let's, let's actually look at this as an algorithm.
So, uh, what we do is we f- if we wanna actually optimize this, uh,
we first sample a task i or whatever meta-training tasks or a mini-batch of tasks.
Uh, then we sample disjoint datasets from that task dataset,
uh, which we'll refer to as D train and D test.
So if this is all of the data that we have for task i,
then what we're gonna wanna do is we're gonna wanna partition this
into a training dataset and a test set for that task.
Uh, and so in particular what we can do is we can basically pick, uh,
randomly select half of them to be used for the training dataset and half of them to be
used for the test set at this iteration of the algorithm.
Then, we'll take the training dataset, uh,
the- what's in the green box and use that to
compute the task per- specific parameters Phi i.
And then we'll update our meta-parameters using the gradient of
the objective with respect to
the meta-parameters using the computed task-specific parameters.
[NOISE] Uh, and then we'll repeat this, uh,
iteratively using your favorite, uh,
gradient descent op- uh, optimizer, things like Adam,
SGD, Momentum, et cetera. Yeah.
[BACKGROUND]
So, so the question was,
uh, we're not compu- computing gradients using the training dataset.
So what we're using is we're, um,
computing gradients using the meta-training dataset, uh, of tasks.
And so the task-specific parameters are computed using D train,
and the, uh, then we evaluate
those parameters using the test data-set for that meta-training task.
So we've, kind of, lifted the,
the training datasets from, kind of,
training datasets and test datasets to meta-training sets and meta-test tasks. So. Yeah.
[BACKGROUND]
You're asking if Theta,
Theta is all meta parameters?
[BACKGROUND]
Yeah. So all of Theta is, uh,
are the meta parameters and then Phi are considered
the task-specific parameters or the-
Theta- Phi is essentially not considered part of th- the meta parameters.
[BACKGROUND]
Um, we'll get into what Phi might be in a second.
It could be the base- basically, it could be the,
the parameters of an entire neural network, uh,
it could also be something that's more compact,
and I'll talk about that in a second. Yeah.
[BACKGROUND]
Yes. Yeah. That's the met-
[OVERLAPPING] [BACKGROUND]
Yeah. Yeah. Exactly. So we haven't touched any of the meta-test tasks,
uh, that are, kind of,
held out from the,
uh, task distribution. Yeah.
[BACKGROUND]
Right. So in this case,
for this particular network architecture,
the order of the training datasets mat- the order of the,
the data points matters.
Uh, and this actually isn't necessarily a good property because in many cases, the,
the, you have data sets not data lists,
uh, for which the order doesn't matter.
Um, and so we'll see some architectures that- later see
some architectures where you- they are permutation invariant.
Yeah.
[inaudible] .
So in this case we, we compute Phi in Step 3,
and then we, uh,
update the meta parameters Theta.
So we do not update Phi, uh, itself.
It's- that is basically dynamically computed
at every iteration of the meta-training process.
Uh, and then at test time,
we're also going to be computing Phi given our meta parameters Theta.
[NOISE] Yeah.
[inaudible] we compute Phi and then we do maximization step of Phi,
and then run that,
like, updated Phi back to the network?
Yeah. So it's similar to that.
So the- we essentially compute- you can,
you can view the, the,
the computation of the gradient with respect to
Theta as basically back-propagating the loss from,
from y into Phi back in- back all the way into theta.
We don't ever use that gradient to, to update Phi.
We only use it to update Theta,
but it has to go through Phi in order to compute that gradient. Yeah.
[BACKGROUND]
Um, I'll talk a bit about architectures in a minute,
but one of the nice- one of the things about LSTMs and,
and RNNs is that they can, er,
process variable amounts of data relatively easily.
Uh, so you don't have to assume any data- any particular dataset size,
although you should probably train it for the largest possible dataset size.
Um, but there are other neural network architectures
that I'll talk about that you can use [NOISE] for this as well.
[NOISE] Okay.
Um, so- now one of the challenges with this approach is that if, uh,
if Phi is literally representing all the parameters of another neural network, uh,
it may not be that scalable to actually output, um,
all of those neural network parameters because neural networks can be very large.
So, um, there are a couple of approaches for dealing with this, but the main,
kind of, way you can think about doing this is you don't need to
necessarily output all of the parameters of a neural network.
You could instead just output the sufficient statistics of
that task such that you could effectively make predictions for that task.
Um, and so what this looks like is,
instead of having a neural network that outputs all of the parameters Phi,
it will output, uh, some set of sufficient statistics h. Er,
and then this, uh,
like, some, some lower-dimensional vector h. And then your, uh,
your neural network on the right will use those, uh,
sufficient statistics as well as other parameters n theta in order to make predictions.
Uh, and so what
this lower-dimensional vector h might represent
is things like contextual task information.
Uh, and then your new parameters Phi i are gonna correspond to hi as well as, er,
part of theta that will parameterize g. Uh, and so essentially,
the way that you can view the size is you can, uh,
basically view this if you, uh,
basically view this as a single LSTM that's taking in, uh, data points.
Uh, so one of the reasons why I named this h,
is h is often used for the hidden state of LS- of an LSTM.
If you basically share all the parameters, uh,
between both f on the left and g as,
for example, an LSTM, uh,
then the task-specific parameters Phi are
represented by the hidden state of that LSTM as well as the parameters of,
uh, the function on the right that are shared with the LSTM parameters.
Um, so one interesting connection here is that if you recall, uh,
multi-task learning where we were concatenating task information z into the network.
Uh, you could view h, uh,
essentially as a summarization of
the task that is used to make predictions for that task.
So h and z, er, are very similar.
Uh, in this case, unlike z,
in the multi-task learning setting we're actually
learning the task representation h in this case,
and we're learning how to produce that task representation
h given a small dataset of that task.
[NOISE] Okay.
And so the,
the fully general form of black-box neural networks is
a function that takes as input a trained dataset and
a test input and produces a test output, um,
where Phi is, is somewhere, uh,
in the middle of this network and may not actually be something that is, uh,
actually representing parameters per se. Yeah.
[BACKGROUND]
Right, so the question is, can you explain what theta g means here.
Um, so here basically theta g, uh,
represents all of the other parameters that this, uh,
that this network g is representing other than h. Uh, so, uh,
this neural network right here that's making prediction about
test- making predictions about test inputs we'll take as
input h and we'll also have other parameters Theta g that we'll use to make predictions.
Theta g will be a part of the,
the full parameter vector Theta.
Uh, it may also share parameters with this part of the network right here. Yeah.
[BACKGROUND]
Yeah. So in this case, you might be sharing more,
uh, more parameters between test time and, and training time.
[NOISE] Okay. Um, so this is the,
kind of, overview of black-box approaches.
And let's now talk about what sort of architectures we could use for this
function f. So one of the first, um, well, I guess,
it's hard to say what,
what came- comes first in research in general,
but, um, one of the earlier approaches to these sorts of black-box approaches, um,
is using LSTMs or Neural Turing Machines, uh,
that take as input the test inputs and, uh, basically the dataset,
uh, and be able to make- and use that to make predictions about new data points.
Um, LSTMs are, are probably something that's familiar to you.
Uh, Neural Turing Machines are something that have more of
an external memory mechanism for which it can essentially store,
uh, information about the training data points and then
access that information when making predictions about new data points.
Uh, and it's it d- does this in a differentiable way.
Uh, you can also use something that [NOISE] uh, so, kind of,
as was noted before, this is
not permutation-invariant because you're taking the data points sequentially.
Uh, and you could also use an architecture that is permutation-invariant by
having a feed-forward function that takes as input each of your training data points,
uh, x and y, uh, x1,
y1, x2, y2, et cetera, uh,
and then aggregates that information using something like an average operation,
uh, to compute, uh, something that looks like, uh,
in this case, what's denoted as a or r. Uh,
and then that is passed into
another feed-forward network to make predictions about new data points.
Um, beyond these, er,
these two types of architectures,
there's a wide range of others that have been proposed that
have used other memory mechanisms,
um, as well in combination with, uh,
ideas from, kind of,
having slower weights and faster weights.
Um, often when people use the term slow weights and fast weights,
they refer to the task-specific parameters as fast weights and,
and the meta parameters as slow weights, uh,
because one of them is updated much more quickly than the other one.
Uh, this is a concept that, uh,
was developed by, um,
folks in neuroscience actually, uh,
that have looked at kind of, the, the, um,
how weights have been changing and how,
uh, how synapses change in the brain.
Uh, and then, uh,
there's also an architecture that has used
a combination of attention mechanisms and convolutions.
Uh, so in this case, convolutions are go- actually going to be
not permutation-invariant although
attention-based architectures can be permutation-invariant.
Um, and, kind of,
as a representative approach of, kind of,
the black-box approaches in general, um,
this, this type of method that uses, uh,
that uses, uh, convolutions and attentions is
able to do quite well on things like Omniglot,
uh, getting around 97-99% accuracy on,
uh, things ranging from 5-way one-shot to 20-way five-shot Omniglot, uh,
and also does well on the min- ImageNet dataset that is performing, uh, like,
five-way classification for actually real images from the ImageNet dataset. Yeah.
[inaudible] ?
Um, so the question is the- like,
is there any, um, mechanisms relating to neural?
[BACKGROUND]
What do you mean by HI?
[BACKGROUND]
Right. Okay. Um, I guess, I'm not, uh,
I'm not too familiar with the,
the neuroscience literature, uh,
to be able to comment on that in a,
uh, in a competent way.
The- I guess, one thing I will say that has been
somewhat inspired by the neuroscience literature is that people have looked at,
um, things that look like LSTMs,
but do more of a Hebbian rule- update rule on that,
uh, on H- on HI, um,
in order to, uh, kind of,
update the sufficient statistics with respect
to a given task given your training datasets.
Um, other works from, um, I guess,
one thing that is perhaps worth noting that we will
actually cover in one of the reading sessions, um,
is that the- I guess,
there are a number of, uh,
neuroscience, uh, researchers at
DeepMind that have looked at these types of meta-learning methods.
Uh, and they have focused on,
on actually these types of meta-learning methods more so
than optimization-based or pan- on parametric approaches,
uh, using things like, like LSTMs. [NOISE] Yeah.
[inaudible] .
Yeah. So in general, I'm,
I'm under the opinion that Omniglot performance has saturated,
uh, for the most part.
So, um, one of the algorithms that we'll
be talking about later in this lecture, uh, gets,
like, 99.9% accuracy on five-way five-shot Omniglot.
Uh, things that aren't solved are generation of Omniglot digits.
That's certainly something that's a lot harder and
was actually proposed in the original paper.
Uh, also, um, this is a bit of a nuanced point,
but the tr- the meta-trained meta-test split that they proposed in
the original Omniglot paper is actually not the one that's
used in all the machine learning papers because it is a bit,
uh, it proposes a train test split that doesn't have
quite enough training data points for these models to not overfit a lot.
Um, and so if, if you're look- interested in looking at very efficient learning, uh,
then you- I think that performance isn't quite as
saturated when you move towards the original meta-train meta-test split.
Um, but then it's just a matter of putting inductive biases into your network.
Okay. Um, so in homework 1,
uh, you'll be implementing the- kind of,
the data processing pipeline for
these meta-training algorithms that involve actually taking the Omniglot dataset,
for example, and actually loading images and,
and plugging them into a neural network.
This is actually a pretty fundamental part of these algorithms.
Uh, you'll also be implementing a very simple black-box meta-learner,
uh, and also training a few-shot Omniglot classifier.
Uh, and you can use, kind of, uh,
you can somewhat compare it to, uh,
some of the numbers in these papers.
Okay. Um, so to wrap up black-box adaptation, uh,
the pros and cons of this approach is that,
first, it's very expressive.
So given that neural networks are universal function approximators,
these methods can represent any function of your training dataset.
Uh, and they're also very easy to combine with a variety of learning problems,
for example, supervised learning or reinforcement learning.
Uh, and later in this course we'll talk about
how we can combine these methods with reinforcement learning.
Um, it's, it's- the,
kind of, the spoiler is that it's,
it's very straight forward, uh, as you might imagine with these types of models.
The downside to this approach is that, uh, in general,
these neural networks are, are fairly complex because they need to be
taking in datasets and making predictions about new data points.
They essentially need to figure out how to learn from data, uh,
and they need to do those- do this in a completely- like,
basically completely from scratch.
Like, at initialization, these,
these LSTMs were not built as, as optimization procedures,
and they need to learn those optimization procedures,
uh, from scratch from the meta-training data.
Uh, and as a result, they're often fairly data inefficient.
Um, and by this, I mean not data inefficient at meta-test time,
but they often require a large number of, um, kind of,
a large amount of meta-training data,
a large number of tasks in order to perform well.
Okay. Any questions on black-box approaches before we move on? Yeah.
[BACKGROUND]
So, um, I guess,
the- so the question was,
there are other algorithms that take X-test as input.
Um, and you could certainly, like,
you could certainly integrate X-test as much as possible in,
uh, on- into the,
kind of, left-hand side of this diagram.
Um, it's still, kind of, part of the input.
And if you look at, kind of, the,
the general form of, um,
of these algorithms, it,
it- something that takes in the train dataset and the test input and you can really
design whatever architecture you want to integrate those pieces of information.
Whether or not they're integrated,
kind of, somewhat separately,
or treated somewhat separately,
or integrated, uh, in the same part of the network, that, that's up to you.
Okay. Great. So let's talk about, um, optimization-based approaches.
So the- I guess,
the motivation here is that,
uh, as we talked about a bit before,
if we want to infer all of the parameters of a neural network,
uh, having a neural network output them isn't a very scalable way to do that.
Uh, and instead, what we could do is, instead of, uh,
treating this function as an inference problem,
we can instead treat it as an optimization procedure.
Uh, and this is similar to what we do in supervised learning.
We treat, uh, parameter- like,
i- inference of our parameters as an optimization problem not as necessarily,
uh, an in- inference problem.
Um, this is where optimization-based meta-learning approaches come in.
So the key idea behind these methods is that we're gonna acquire
our task-specific parameters phi i through optimization.
And then we'll differentiate through
that optimization procedure to the meta parameters to optimize
for a set of met- meta parameters such that
that optimization procedure for phi i leads to good performance.
Um, so how do we get started here?
So the- I guess, you can essentially break down the,
the meta-training problem as,
uh, as, kind of, having these two terms.
One that's maximizing the likelihood of your training data given
your task-specific parameters and one that is, uh,
optimizing, uh, your, uh,
task- the likelihood of your task-specific parameters under your meta parameters.
Um, and so you can view this, kind of, this,
this equation right here as the optimization procedure that you wanna be able to do,
uh, at test time and also the optimization procedure that you're
going to be integrating into your meta -learning problem during meta-training.
One that's basically going to be taking into account the training dataset, uh,
and your accuracy on the training dataset as well as, uh, your prior,
which is given by phi given data where,
where your meta parameters are,
are parameterizing your prior.
All right. So your meta parameters are serving as your prior.
Um, and now we need to think about, well,
what form of prior, uh,
should we, should we basically impose using our meta parameters.
Um, well, one very successful form of prior knowledge that we've
used in deep learning optimization is the initialization.
Um, and in particular, one of the things that's been quite
successful in deep learning is what's called fine-tuning
where we take some set of initial parameters and then
run gradient descent on training data for some new task.
Uh, and typically, this is not for just a single gradient step as written here,
but for many gradient steps.
Uh, and this has worked really well.
So, for example, if you look at, um,
so- something that pre-trains on im- on ImageNet versus training from scratch, uh,
those are the two rows shown here,
and fine-tuning either on the PASCAL dataset or on the SUN dataset.
Uh, and we see a huge difference in performance using pre-training, which is, uh,
labeled as original in this paper,
um, versus using a random initialization.
Um, great.
So typically,
like, this is i- in many ways a valid approach to, kind of,
the meta-learning problem where you first train, uh,
a cert- or pre-train a set of parameters on your meta-training data
and then fine-tune on your dataset at test-time.
Um, now some questions that might come up is,
where do you get your pre-training parameters?
Uh, the typical way to do this is through- for,
for, for vision problems,
the typical way to do this is by pre-training on
ImageNet classification as using supervised learning.
Um, in language, one very popular approach for doing
this is using models trained on large language corporas,
um, models like BERT or language models,
uh, or other unsupervised learning techniques.
So pre-training of neural networks actually has a very long history.
Before even- well before ImageNet,
our people were pre-training their,
their models using unsupervised learning techniques and then fine tuning them.
Um, although other than language,
it's not sure if- uh, it's not clear if
those approaches have really been that popular recently.
Um, but really, like,
if you have some domain, um, in many ways, kind of,
the thing to do is just to take- train on
some very large and diverse dataset and then fine-tune these- fine-tune
those parameters on whatever dataset you actually want to perform, uh, inference on.
Um, and it's also worth mentioning that pre-trained models are often available online.
Uh, and so you can- actually you don't even
necessarily need to do this stuff where you actually trained on ImageNet.
You can just download the parameters and then fine-tune from there.
Um, and then, I guess, the other thing worth mentioning here is that, er,
fine-tuning is a bit of an art, er,
like other, other aspects of deep learning unfortunately.
Um, and so there's a, a range of common practices for,
uh, performing fine-tuning successfully.
This includes things like fine-tuning with a smaller learning rate,
um, using a lower learning rate for lower layers of the network.
Um, uh, typically the low layers are- for many fine-tuning problems, typically,
the low-level features are the things that need the change the least and
the higher-level concepts are the things that need the change the most for a new task.
Uh, you may actually freeze earlier layers of the network.
Potentially even basically setting them to- with,
uh, setting a learning rate of zero for those layers.
Uh, you could also consider re-initializing the last layer.
Um, and then typically people search over these hyperparameters using cross-validation.
Um, and then the last thing worth mentioning here is
that architecture choices tend to matter,
uh, a lot when, when choosing how to fine-tune.
Er, for example, things like
residual networks tend to be actually quite good at fine-tuning,
um, because the gradients flow,
um, flow relatively easily through
various parts of the network when you have residual connections.
Yeah.
[inaudible]
So you're asking, basically when you're, when you're fine tuning,
you're not actually using any information about the target?
Either fine tuning or [inaudible] We never exclusively encode which task we are using.
It's never an argument about using procedures.
Yeah. So you're saying that, basically, we never are passing in any information about
the task as input to this approach or to the black box approaches.
Yes, that's, uh, that's correct.
There's actually, uh, well- for meta-learning- well,
there's actually some nuanced reasons for not doing that,
uh, which is kind of interesting in some way.
It seems like in many ways you should pass in as
much information you have about a task to the models so that they can use it.
But in fine tuning, for example, um,
if you say passed in a one-hot vector for ImageNet,
and then pass it in a different one-hot vector for your test task,
then, uh, it actually may,
like- that, that information will- first,
if they're just two separate one-hot vectors,
like, the, the test tasks that you're doing,
like, versus train time, they're completely distinct things to the network.
Um, and so that,
that information isn't something that can actually be
used during the fine tuning process to,
to help it because it- it's only ever seen one task.
And so kind of looking at another task,
for example, won't tell,
tell you that it's doing a different task.
Um, is it-
Sorry.
Go, go ahead.
I just want my [inaudible] because the only agent or person that
knows third tasks is
only the person that's training the network because when you're going to test,
you know you will test with the test set [inaudible] about task.
But if someone gave me a network and I was training
in this manner and I just got a bunch of clear images, how do I know
there's no index tracking, right?
Like we had earlier and we had to mention approaches.
So basically, [inaudible] network,
there's no way to know what parameters to use for a specific task, right?
The earlier, I know, oh,
we are trying to- we have like a vehicle on each side and,
uh, [inaudible] so maybe one has one [inaudible].
Yeah.
They're recording and then I'm like and I'm, oh, there's a vehicle I should use,
there's one, there's an [inaudible] , I should use this.
Yes. So you're asking me, so you could basically tell the network, um, like,
you could train it- you could pre-train it on, like,
multitask learning, say, like,
first, you- I'm gonna train you on, like,
recognizing animals and recognizing plants and recognizing cars and something,
and tell it that it's going to be doing that,
um, and we're, like, fine tuning on that, for example.
Uh, in this case the pre-training is just single task.
Uh, and the test task is also single task,
so you don't actually tell it any information about kind
of what task it's solving because it's kind of, um,
assume that you're gonna be fine tuning it on,
on a new task and kind of both pre-training and testing our,
um, our separate tasks.
Um, I'll get to the,
to the point about why meta-learning doesn't pass on task information a bit later. Yeah.
So in this case, we're fine tuning the task specific parameters,
but can we also do fine tuning on the shared meta-parameters?
So in this case,
we're actually not- um,
I guess the- in this case, this is- uh,
there isn't really a distinction between task specific parameters and meta-parameters.
So what we're doing is we're just pre-training parameters Theta, which, basically,
could be your meta p- could be your meta-parameters Theta,
and then the optimization process is producing your task specific parameters, Phi.
So it then includes both of them basically? [NOISE]
Um, I would actually
sa- I would actually say that the pre-train parameters Theta are the meta-parameters.
Okay.
Uh, and that initialization is affecte- is basically
serving as a prior on your optimization,
uh, and in a somewhat implicit way.
Uh, because basically the,
the meta-train- the pre-trained parameters are kind- like
affecting the solution that the fine tuning process will give you. Yeah.
Is the optimization [inaudible] [NOISE] like more generally trained neural network,
is that more of like a pruning thing or like using just at specificity?
You're saying, is this- is the fine tuning procedure pruning the network?
Yeah.
Like wh- is it reducing weights and then
be like- or adding specifically, like increasing weights?
So in this case, it's actually just changing the weights.
So it's not, uh,
removing or adding weights to the network.
But, but like generally,
in your experience, what have you seen?
Do you know if there is a general problem for that?
Uh, so you're asking what, what do,
what do fine tuning procedures end up doing?
Do they- um, I think that the- kind of the accepted wisdom of,
of what these are - things are doing are reusing features,
and often are changing how those features are used,
uh, for a new task,
but not necessarily changing the features that much themselves.
Um, so- and that's kind of how like, the,
the later layers of the network are changing a lot
and the features themselves are not changing a lot.
Um, although I don't know if anyone has actually proven anything related to that. Yeah.
So, uh, two quick questions.
What is- [inaudible]
Yeah. So typically, you'll use the same architecture or you might,
uh, use the same architecture but like chop off the last layer.
Oh, okay. And when you're doing these updates and fine tuning,
are you- you said you only provide one task at a time.
So-
Right. So, uh, we'll get, we'll get how,
how we can integrate this into a meta-learning approach on the next slide.
Uh, but yeah.
So typically, what you do is you just pre-train parameters on a single task,
uh, and then fine tune on your test task.
Okay. Um, great. So uh,
one other example of where this has been used is using,
um- basically pre-training using
language models and then fine tuning on text classification tasks.
Uh, and the plans in here are pretty interesting.
So they're showing that, uh, on the X-axis as you
vary the number of training examples you have for the test task,
how does the, um,
performance on that task vary?
And so what we see first is that there's a big difference between
training from scratch versus training- uh,
using pre-trained parameters from universal language models or ULM.
Uh, so that's the gap between the blue lines and the orange and green lines.
And then, uh, the second thing that we see is that,
as you have fewer examples, uh,
in your new task data set, uh,
performance gets worse, our error goes up.
Uh, and so essentially what we see is that when you only have, for example,
100 data points for your test task here, uh,
your performance actually isn't very good on your test task.
Uh, and you can expect that as you actually decrease that even lower,
you would do even worse.
And so, uh, essentially,
fine-tuning is, is much less effective when you have smaller data sets.
And now motivated by this,
how about we design a meta-learning algorithm with the goal of being
able to fine tune with small amounts of data at test time?
Uh, and in particular, what we could try to do is take our fine tuning procedure
and evaluate how well those task-specific parameters
did on a test data set or on new data points.
And then actually optimize for your pre-trained parameters
such that fine tuning gives you a set of test- uh,
gives you a set of parameters that do well on the test data points.
Uh, and you could do this optimization across all of the tasks
in your meta-training ta- in your meta-training data set,
such that fine tuning with small amounts of data leads to good generalization.
Um, so essentially, it'll be training for, uh,
a set of parameters Theta across
many different tasks such that it can transfer effectively via fine tuning.
Um, okay, so kind of at a more intuitive level what this might look like.
And say Theta is the parameter vector that you're meta-learning- uh,
your meta-parameters, and Phi i star is the optimal parameter vector for task i.
Then you can view the meta-training process of this optimization as the thick black line,
where when you're at this point during the meta-training process,
and you take a gradient step with respect to task three,
you're quite far from the optimum for task three.
Whereas, at the end of the meta-training process
you take a gradient step with respect to task three,
you're quite close to the optimum.
And likewise, for a range of other tasks.
Um, and we refer to this as
the Model-Agnostic Meta-Learning algorithm, uh, in the sense that, uh,
it embeds this optimization procedure in a way that's agnostic
to the model that's used and the loss function that's used,
as long as both of them are amenable to gradient-based optimization.
Um, and then one other thing worth noting here is that this,
this diagram, I think, can be helpful for get-
getting across the intuition of the method.
Uh, but at the same time, it can be a bit misleading.
First, because parameter vectors do not exist in, in two-dimensions,
uh, and also, or,
or neural network parameters do not exist in two dimensions, typically.
Uh, and then also, there often isn't a single optimum, but,
but actually a whole space of optimums, um,
for a whole space of optima for,
uh, neural network parameters.
And so in many ways it's more about, um,
not necessarily reaching a center point for these different algorithms,
but reaching a point, um,
such that fine tuning will eventually- um,
will, will get you to, uh,
a good part of the parameter space with,
uh, with, with small amounts of data.
Okay. Um, so that was the objective,
uh, what does this look like as an algorithm?
So we can take, um,
the black box adaptation approach that we mentioned before, uh,
and, uh, adapt it to the,
the optimization-based meta-learning case.
Uh, and essentially what this does is you first sample a task,
you can- you sample your data sets.
Uh, then instead of computing your task-specific parameters using a neural network,
you're going to be computing them using one or a few steps of fine tuning.
And then you update your meta-parameters by
differentiating through those fine-tuning steps into your- uh,
into the parameter vector Theta- into your initial set of parameters.
Okay. Any questions on this before I get into a few of the details? Yeah.
If you were to initialize a multitask network with
the- whatever these learned weights would be,
versus trained multitasks from scratch,
do you think the multitask network could do better with this prior?
So you're asking, um,
what if you use multitask learning as an initialization instead?
First, you- you do the string method for some- some theta and then I guess [inaudible] ,
and then you use those as your initial weights for the multitask learning.
I see, so you're saying that you can basically
pre-train to do this meta-training process to get an initia- initial set of weights,
and then use that as an initialization for multi-task learning?
Yeah.
Um, you could certainly do that.
I guess, what this is doing is optimizing
that the meta- the meta-learning process that I mentioned, uh,
on the previous slide is optimizing for
fast adaptation to individual tasks given an individual dataset.
You could also do the same thing for pairs of tasks or triplets of tasks.
Uh, if you wanted to explicitly pre-train for three-shot learning or it's not,
it's not three shot learning, three task learning.
Um, you could- I,
I guess you could also consider doing something like- like optimizing it for, uh,
optimizing it on, on for a single-task adaptation and
then fine tuning it on multitask adaptation, or multitask learning.
Um, it's hard to say how well that would do because it's not actually
explicitly training for what it's going to be doing at task time, but,
uh, conceivably could do something effective. Yeah.
When you talked about initializing the weights data
from a pre-train network like it was that- was that just
motivation for this algorithm or do you actually do that
with your parameters theta or could you start with random initialization?
Right. So in this case we start with
a random initialization before this meta-training algorithm,
and that was mostly serving as for- basically as- as
motivation for how well pre-training can work,
uh, in a ra- range of settings. Yeah.
[LAUGHTER] So does this work,
uh, well, even with, like,
one-shot learning because it seems like even with this approach that you
could risk over fitting on [inaudible]?
Yeah, so this approach actually works really well even for
one-shot learning, two-shot learning etc.
It's competitive with, um,
with the black box approaches that I mentioned previously.
So how long do you typically run that in optimization move [inaudible]?
Right, so for the one-shot setting you can you- typically
the data optimization is somewhere between one and five gradient steps.
Um, and even with only a few gradient steps you can get quite far.
[NOISE] Okay.
So one thing worth mentioning about this algorithm is that it brings up
second-order derivatives because we are optimizing,
um, basically because we're, ah,
we're optimizing for a set of, uh, meta parameters.
Ah, so this is- we have this gradient.
And inside of, ah,
this inductive term we also have this gradient right here.
Uh, so I know you might be a little bit worried about this.
So I- for example,
if you need to compute the full Hessian of,
uh, of the neural network,
we would be in a bit of trouble.
Uh, and, um, what if you want more than one integrated step.
Does that give us higher-order, um, higher-order derivatives?
And so I wanna go through a bit on the whiteboard, ah,
what this- what actually the meta gradient update looks like,
uh, such that we can, kind of,
figure out the answers to these questions.
[NOISE] Great.
So let's say that,
um, for the sake of notation.
So, uh, in this case I was writing out a gradient step as the update procedure.
Uh, and- and in this case I'm just gonna, uh,
use u to denote the, um,
the update rule and that's gonna be a function of theta
and your training data points D train.
So this is, uh, this is basically one, uh,
or a few steps of gradient descent,
theta minus alpha grad theta loss with respect to D train.
Um, and I'm gonna use,
uh, so you, kind of, just write out some- some notation.
I'm gonna use d to denote the total derivatives [NOISE] uh,
and [NOISE] the, uh,
nabla symbol to denote partial derivatives.
[NOISE] And we'll see why this distinction actually matters,
uh, in a second.
And this is just for the purpose of the white board,
in all the slides we'll just be using the nabla symbol- the gradient symbol,
um, for, for both.
Okay. So as you can see on the bottom of this slide, um,
the optimization procedure that we have [NOISE] looks something like an optimization
parameter- meta parameters theta over
our loss function with respect to our task specific parameters five.
[NOISE] And our test data points.
I'm gonna drop the i from the notation here just for notational simplicity.
And this is the same as [NOISE] an optimization over
meta parameters of L of our update rule with regard to our training dataset,
uh, and our test dataset.
Okay. So this should all be clear from the board.
And in order to optimize this objective function,
we need to be able to get the derivatives of this objective with respect
to our meta parameters if we want to optimize this with gradient-based optimization,
things like Adam for example.
Um, and so to do this, we need to be able to get, uh,
the derivatives [NOISE] of this objective
with respect to [NOISE] our meta parameters data.
Uh, and so let's try to actually write out what this meta gradient looks like.
Uh, so in particular we can view this meta gradient.
Um, first we can basically,
uh, compute the- with the chain rule,
compute the derivative of the outer function, uh, and then,
uh, use the chain rule to compute the derivative
of the- the inside with respect to theta.
So what this looks like is we'll take
the derivative with respect to, uh, I'll use, kind of,
a placeholder variable phi bar of, um,
L of phi bar and D test [NOISE] evaluated at phi bar equals u,
um, of theta comma D train.
So this is just the derivative of the outer objective, uh,
times the derivative of the, um,
the derivative of the update rule with respect to D train. Yeah.
Can you write a little larger [inaudible].
Yes, I can try to write larger from here forward.
Um, so basically this is the- the derivative,
the derivative of the first- of the outer loss,
and this is d phi d theta.
Um, this is why partial derivatives are- matter because if we,
kind of, just wrote this as, uh, as a full derivative,
then this would just be exactly the same as the- or basically be very similar to the,
um, to the, uh, what was originally written.
Okay. Um, right, great.
So notice that this is, uh, this is, like,
ah, a row vector. Uh, I need to write bigger.
[NOISE] And, uh, this is a matrix.
[NOISE] Um, and so the result is a row vector.
Um, this can be computed with a single backward pass through the neural network.
So this is, uh,
you could just set the parameters of your neural network to phi bar and
then compute the derivative of this loss function with respect to those parameters.
So this is just one backward pass.
Um, this is differentiating through the update process itself.
Um, so this is the part that is a little bit trickier to deal with.
Okay. Um, so let's try to actually compute what this looks like.
So, um, [NOISE] we can let u theta D train.
Let's just start with the case where we have [NOISE] a single gradient step.
Um, [NOISE]
then in this case,
uh, then we can try and take the derivative of this.
So, uh, derivative of the update rule with respect to theta.
This is going to equal the identity matrix, uh, minus alpha,
uh, d d theta squared of L of theta,
um, comma D train.
[NOISE] Uh, and this is the- [NOISE] this is the Hessian of the neural network.
Any questions with this?
Okay. So, uh, if we didn't plug this term into here,
then what we get, uh,
is that we- this is, we have this vector.
We simply need to be doing- do a vector matrix multiplication.
Uh, and fortunately this means that we don't actually have to compute
the full Hessian of the neural network because we ha- because we have this.
All we need to compute is this Hessian vector product.
Uh, and there are much more efficient ways to compute
Hessian vector products via backpropagation for neural networks than,
uh, that don't require you to construct the entire Hessian of the neural network.
Uh, it also turns out the standard neural network
different- automatic differentiation libraries like
TensorFlow and PyTorch will actually perform this Hessian vector computation for you,
uh, such that you- in an efficient way that amounts to essentially performing, um,
additional backward passes such that you don't
actually have to worry about coding this up yourself,
um, which is very convenient.
[NOISE] Okay.
So that's the case if we had just a single neural- a single gradient step in the,
um, in the inner loop.
What if we have multiple inner gradient steps in the inner loop?
Uh, and so in particular what if we have,
um, u theta comma D train.
What if we have two gradient steps.
So this is gonna equal theta minus alpha d theta
of L theta D train.
Uh, let's call this intermediate set of parameters theta prime.
And then we will have a second gradient step that is
with respect to theta prime of L of theta prime D train.
Is that behind the thing?
I think that's still there.
Okay. So this is two gradient steps.
Um, if we then want to compute the derivative of this,
[NOISE] then what we get is,
we first get the first two terms that we had before which is the identity minus the,
um, minus the Hessian. [NOISE]
Uh, and then, what about the second term?
So we want to compute the derivative of
this last term with respect to the parameters Theta.
Uh, and what this looks like is, uh,
first you compute the derivative of the outside.
So you get, uh, D Theta prime squared.
Uh, now I'll use Theta bar here of,
um, of Theta bar D train.
This is going to be evaluated at, um, at
Theta prime times D Theta prime D Theta.
Er, and so, er,
first this, this term right here is just equal to the first two terms.
Er, and one of the nice things that we get here is that,
er, we don't get third-order terms here.
So we get the Hessian evaluated at
Theta prime which is the parameters after the first gradient step.
Er, and we get the Hessian with respect to, er,
the original parameters but we don't get anything,
um, any third order derivatives basically.
Er, and again this is something that we can efficiently compute.
Well, if initially- basically compete with
additional backward passes without having to basically construct,
er, any full Hessians or without having to compute
higher-order derivatives which is nice.
Okay, er, and then as you might imagine if you continue to run this,
um, continue to kind of compute this for, um,
for even more gradient steps in the inner loop,
you basically continue to get these types of
terms that pop-up without higher-order terms.
Okay. Any questions on, um,
on some of the math?
Okay. So yeah.
Er, in the computation of the second derivative term,
aren't you trying to take the derivative with respect to
Theta of the derivative of Theta prime.
But then you wrote the derivative squared was like the Theta prime bar.
So how, how does that happen?
So if you're trying to differentiate this third term,
um, with respect to Theta,
you first kind of take the derivative of the outer function with respect to, um,
its arguments times the- times this term which is the d of the chain rule.
Okay and sorry that this likes to float upward but okay.
Cool. So, um, now we've talked about authorization ba- authorization based approaches,
um, or at least the kind of th- the basics of them.
Let's think about how they compare to black-box approaches.
So you can view Black-Box adaptation as
having this general form that it takes as input a training data set and a test input.
Um, for example using something like a recurrent neural network or something like that.
Now, you could also view MAML, or model-agnostic meta-learning as also, er,
taking a training data set and a test input where you have this function
Phi that takes as input the test input
and the parameters Phi are computed with gradient descent.
Um, so essentially you can view MAML as
a computation graph with
this funny embedded gradient operator inside that computation graph.
So if you kind of take this view,
that means you can potentially mix and match components of, um, of these approaches.
For example, um, one paper that looks at can you learn init- initialization,
er, but replace the gradient update that MAML does
with a learned neural network that produces that gradient update.
So for example instead of having, er,
instead of learning the initialization then running gradient descent,
you could learn initialization and have
a neural network output your, your gradient update.
Er, and this was done in,er,
Ravi and Larochelle in 2017.
And this paper actually precedes the MAML paper.
But I, I mentioned it here, er,
just for the purpose of understanding different things.
Okay. Um, and this computation graph view of
meta-learning will come back again, er, later.
Okay. Now, one other thing to think about, er,
is some of like how these approaches not just compare conceptually,
but also in practice and in theory.
So, um, one question to think about that was actually mentioned a bit before is,
er, what if your test task is different than
the meta-training tasks that you were optimizing on?
And so this is a question that we studied
empirically to some degree and we were aiming to
compare MAML to black-box type approaches,
er, such as SNAIL that the, the,
the architecture that used attention and convolutions, er,
as well as MetaNetworks which is also one of the architectures that I showed before.
Er, and we looked at, er,
Omniglot classification where we tried to vary the tasks, er,
and see how the performance did as you vary
the tasks away from the Meta-training distribution.
So in this case the X-axis will show the task variability,
and the Y-axis is going to show performance.
And so in the first study we looked at we, um,
we skewed the digits in the Omniglot data set.
So it was trained on digits that were,
er, kind of in the center.
And then we moved, er,
kind of away from the meta-training task distribution training it on, er,
testing its ability to adopt to tasks that involve skewed digits.
And what we saw at first is that all the approaches, er,
their performance deteriorated as you moved away from the meta-trained distribution.
But we saw that, er,
algorithms like MAML are better able to perform these out of distribution tasks, er,
as you move away from the meta-training distribution because they're performing,
er, an optimization procedure at test time.
So because you're running gradient descent at test time you can
still expect it to give you some reasonable answer.
Er, at least an answer that achieves good accuracy on the training data set for example.
Whereas black-box approaches that are,
are just taking in a data set as input and producing an answer.
Um, when you, when you move away from the training distribution, there's really, er,
nothing that you can say about what those algorithms are doing. Because they- yeah.
Yeah. Er, and then if you look at something like the scale of the digits, er,
we also see this sharp drop off as you move away from the training,
er, meta-training data set.
But we kind of consistently saw this pattern that
optimization based approaches were better at
extrapolating because they were still giving you,
um, a procedure at test time that looked like an optimization procedure.
Um, so this is one empirical trend that we noticed.
Er, and then you might ask well we're embedding
the structure of optimization into the Mediterranean process.
Does this come at a cost?
And in particular one very natural thing that was actually brought up a bit before
is how far can you actually get with a single gradient step or a few gradient steps?
Are these methods actually as
expressive as the black-box approaches that I mentioned before?
Um, and it turns out that you can show that,
um, for a sufficiently deep function F,
the MAML algorithm, the MAML function that I mentioned before can
approximate any function of the training data set and the test input.
Um, it can basically represent anything that the black-box approaches can represent,
under a few, er, fairly mild assumptions.
Under the assumptions is that the inner learning rate is non-zero,
er, that the loss function gradient doesn't lose information about the label,
the standard-like Mean Squared Error and Cross
entropy loss functions fall under this category,
er, and also that the data points in your training data set are unique.
And the reason why this is interesting, er,
is that it means that MAML has the benefit of the inductive bias of
gradient descent without losing expressive power. Yeah.
What do you mean by inductive bias?
Um, what I mean by that is that at
initialization like even before you do any Meta-training for MAML,
you still have, er, an optimization procedure
that's going to point you roughly in the right direction.
So you're still running gradient descent and you'll still be
able to improve on your training data.
Yeah.
Is that- are these assumptions- [inaudible] are there
any number of gradient steps or just assuming [inaudible]
This is actually only for a single gradient step.
What is sufficiently [inaudible]?
Very deep.
[LAUGHTER].
Do you know how- is there like an order of the [inaudible]?
Exponential. Yeah so the,
the, the- I guess the assumptions that I listed here are very mild.
The sufficiently deep function is, er, is not mild.
Er, it does need to be very deep.
And you could probably relax this assumption if you
made other assumptions about the gradient pointing in the right direction,
um, or other things about the, the optimization.
It sounds kind of like the sufficiently lied single hidden layer.
Yeah. Yeah. Okay so we're running out of time.
Um, the- let's see.
One thing I want to mention, um,
I guess we can probably just leave off where, um,
leave off where I left off on Monday next week.
But we basically covered the basics of,
of optimization-based meta-learning, er,
and I'll cover, er,
I'll cover the rest of it and some of it.
G- go into a bit more of the advanced topics on Monday next week.
On Wednesday this week, we have,
um, applications of meta-learning,
and multitask learning to things like imitation learning and generative models,
drug discovery and machine translation.
Er, I think that this will actually be pretty exciting to
actually see some of the real-world use cases of these algorithms.
Er, these will be student presentations and discussions.
And then on Monday I'll wrap up optimization-based meta learning and cover,
er, non-parametric methods and talk about how all of these different approaches compare.
Great. I'll see you on Wednesday.
 So today we'll be finishing up some of the things that we didn't have
time to cover last time on optimization based meta learning,
and then we'll also be covering non-parametric approaches to few shot learning.
Uh, before you get started with the technical content, a few, uh, reminders.
So Homework 1 is due on Wednesday and Homework 2 is coming out this Wednesday.
Homework 2 will include both uh,
you'll both implement MAML uh,
as we uh, as we covered last week.
And you'll also be implementing prototypical networks,
which is a uh, non-parametric approach to few-shot learning. We'll talk about it today.
Um, posted uh, a form for you to fill out on the poster preferences.
Uh, this is preferences for the date of the poster session. Please fill this out.
I think that last time we checked only like five people had filled it out.
Uh, we're trying to schedule a venue for the poster session now and the- we have
a better venue that's booked for Tuesday the 3rd but we wanna make
sure that people are- are available that day,
um, being it's not during the normal course session time.
So please fill out that form.
Um, we also posted details on the course project and instructions uh,
including instructions for the proposal,
the milestone and the final uh,
project and poster session.
Uh, so please take a look at that.
We also posted some suggestions that were from the broader AI community on Piazza uh,
and the proposal which is the first part of the project is due on Monday October 28th,
but we encourage you to get started early and find uh,
to either figure out what you wanna do before that alone or to find a group to work with.
Okay, um, so the plan for today,
uh, first we'll cover Optimization-Based Meta-Learning, basically recap
what we covered last week
very briefly and then discuss some more advanced topics.
Uh, then we'll cover non-parametric few shot learning.
Uh, this will probably be the bulk of the lecture um,
and this will include things like Siamese networks,
matching networks and prototypical networks as well as some other hybrid approaches.
Uh, And then lastly we'll be
covering properties of meta-learning algorithms and basically
how we can think about comparing the classes of approaches that we've seen so far uh,
and the types of things that we might want when developing new meta-learning algorithms.
Okay. So that's kind of a- a summary of what we'll go over today.
Uh, so first let's recap from last time.
So we talked about how fine tuning is a very effective way for uh,
leveraging information from previous data sets.
Uh, by pre-training the parameters on those data sets and then fine tuning by running
gradient descent or- or
your favorite optimizer to reuse those features that were learned.
Uh, for learning on your new data set.
Uh, and then we talked about well, uh,
can we think about how we might go about the pre-training process in a principled way.
Especially if we want to be able to fine tune with
very small amounts of data at test time.
Uh, and then we talked about one way to do that.
So one way to do that is basically by trying to embed this fine tuning procedure into uh,
the meta-learning process by taking this fine tuning procedure evaluating how
well the resulting fine tune parameters do well on held-out data uh,
then simply optimizing this objective with respect to a set of
pre-trained parameters across a wide range of tasks.
Okay. Um, so kind of, uh,
this was the- the model agnostic meta-learning algorithm that we discussed.
Uh, and this optimizes for an effective initialization for fine tuning.
Uh, we also discussed how well this
performs on extrapolated tasks and we found that it works uh,
quite well in comparison to the black-box adaptation approaches.
Uh, and we also looked at the expressive power and showed that,
uh, the expressive power of these models,
of these algorithms um,
is quite substantial, if you have a deep enough neural network.
Um, and- and kinda requires a bit more, um,
a bit more expressive power than architecture in comparison to black-box approaches.
Okay, Um, so now uh,
I'd like to talk a bit about some other properties of
these kinds of algorithms and different ways that we could extend the algorithm uh,
to address various challenges.
So uh, first, uh,
one- one thing that we had talked about actually
towards the beginning is how you can view uh,
meta-parameters Theta as serving as a prior for task-specific adaptation.
Where kinda this prior is encapsulating the um,
is encapsulating the knowledge in your meta training data set.
So can we make this more formal?
So it turns out we can actually uh,
make a bit of a deeper connection than just saying that it's gonna form,
it's gonna kinda form as loose prior as an initialization for fine tuning.
Uh, in particular to see this,
let's look at the following graphical model.
Uh, so Theta is representing our-
our meta parameters and phi I is denoting the task specific parameters.
Uh, the parameters for each task,
and then the shaded circles representing the data points that we have for
each task and it's shaded because we could
observe those during the meta training process.
Uh, now, uh, if you think about how um,
how you might go about uh,
doing inference in this graphical model,
Uh, we assume that we have uh,
this data set that we want ma- maximize the likelihood
of our data set given our meta parameters.
Uh, this is essentially how you go about doing inference in
this graphical model with respect to the meta learning parameters.
Okay. Um, and you can also write this out as kind of a sum of log likelihoods as well.
Uh, and then from there,
we can introduce this second introduce our task specific parameters phi I.
Uh, these are gonna be integ- these are integrated out uh,
because we- we're trying to optimize over our meta parameters Theta.
Uh, and so kind of we're just expanding this expression uh,
for p- probability of the data given in the meta parameters into probability of
the data given the task specific parameters and the
probability of the task specific parameters given Theta.
Uh, second time is representing the prior that Theta
is enforcing or imposing on our task specific parameters.
Um, this corresponds to uh,
Empirical Bayes uh, approaches to- for like,
optimize this entire Bayesian model.
Um, so now from here uh,
this- this integral over all possible task specific parameters is- is intractable,
particularly when we have large numbers of parameters, right?
So you don't really have a good way to think about performing
this optimization in any exact way.
Uh, what we can do is we can say well let's fairly crudely
approximate this integral with um,
a point estimate for phi I which is going to represent
the maximum a posteriori estimate of those parameters.
Um, this is a- of course a crude approximation but it is something that
will at least uh- the MAP estimate is a better choice uh,
for these parameters and other choices because it has the maximal probability of course.
Uh, now the question comes in well- okay,
if we're gonna make this approximation to try to uh,
to represent our objective at the top.
How do we compute the MAP estimate?
Uh, well it turns out that uh,
under specific conditions, gradient descent with early stopping, uh,
corresponds to MAP inference with under a Gaussian
prior with mean at the initial parameters and a variance that is uh,
determined by the number of gradient steps and the step size.
Uh, this is exact in linear case and approximate in the nonlinear case.
And so what this means is that if we're doing uh,
kind of getting the MAP estimate by running gradient descent with early
stopping that corresponds to the inner loop of the MAML objective.
Uh, and you can then loosely view MAMLs
approximating hierarchical Bayesian inference in this graphical model.
Of course this involves several approximations.
One approximation that is using
the map estimate and another approximation which is thinking about
gradient descent with early stopping as
map inference in the nonlinear case with neural networks.
Uh, but I think that this kind of interpretation is helpful for
getting some intuition for what these kinds of approaches are doing. Yeah.
What is the stopping [inaudible].
For- for early stopping?
So in the case of MAML we just pick a- a certain number of gradient steps.
We pick one gradient step or five gradient steps.
Um, and the particular- the variance of this Gaussian prior,
is determined by the number of gradient steps that you
use and the step size that you use for those gradient steps.
So I guess unlike typical neural network training,
we're just kind of picking uh,
picking the number of gradients steps rather than choosing
a stopping criterion based of a validation error for example.
So you can essentially view, like,
the initialization of these parameters as serving as
an actual explicit prior in a Bayesian model.
Okay, so you can view this form of initialization
with a few gradient steps as one form of implicit prior on the task specific parameters.
And there are other ways to think about priors that
we could impose on the optimization process.
So for example, one thing we could do is instead of having this implicit
prior that's imposed by only doing a few- small number gradient steps,
we could have a explicit Gaussian prior where we are
actually regularizing the inner optimization to be close to our meta parameters theta.
And so this would correspond to an explicit Gaussian prior,
with mean theta and a variance that is a function of lambda.
So this is basically the form of,
uh, like, the log-likelihood of the Gaussian.
Um, another form of prior that we could do is we can be even more-
even more explicit and actually trying to represent, um,
do basically for a Bayesian linear regression on top of learn features and represent, um,
the mean and variance of that, uh,
represent the mean and variance of,
of that Bayesian linear regression as meta parameters themselves.
Um, so these are kind of two forms of, of, uh,
of gradient-based meta learning algorithms that have
tried to place explicit priors on it.
Another, uh, class of methods have looked at, uh,
just having the prior be
imposed based off of the feature space in which you're learning on.
So kind of similar to this last approach,
where they're doing Bayesian linear regression on the last layer.
There are also a number of approaches that have done
an optimization on top of learned features,
such as performing ridge regression or
logistic regression on top of learned features in the inner loop.
Uh, as well as a support vector machine,
uh, on those learned features.
Um, so essentially these different- these correspond to different inner loops
of the meta optimization algorithm.
And then the meta-training process involves differentiating through
these inner loops by either treating them as a close form optimization,
as a convex optimization problem that you can differentiate through,
um, or other optimizations.
Okay. Um, in this last approach, uh,
this may be out of date now,
but as of a few months ago it was the state of
the art on few-shot image classification benchmarks.
Although, to do that they introduced a number of bells and whistles
in order to get it to kind of reach that.
It wasn't just the,
the, the approach itself.
In many ways those, those bells and whistles are often important for getting,
um, getting state of the art performance on, on benchmarks.
Okay, cool.
So now let's go over a few of the challenges that come up with these types of approaches.
So, uh, one challenge is that we,
we just talked about how MAML you may need much deeper networks in order to be
able to effectively get an expressive gradient update,
that can represent a number of different update rules to your parameters.
And so how do we think about choosing
the architecture for- that is effective for MAML-like algorithms?
And so one idea for this is there is a paper that looked at-can we do
neural architecture search on the MAML architecture such that meta learning works well?
They called this auto-meta in the sense that you're doing both Auto ML and meta-learning.
And one of the things that was interesting about this paper is that they found that
highly non-standard architectures were actually effective for MAML,
um, in contrast to their effectiveness for,
um, for kind of standard supervised learning problems.
So for example they found that deep and narrow architectures tend to work well.
Um, and these were kind of different from the architectures
that work well in standard supervised learning.
Uh, and so for example,
if you take MiniImagenet with the basic architecture,
uh, that reaches around 63% performance, uh,
MAML with this kind of optimized architecture saw an 11%
absolute improvement in performance which is pretty substantial. Yeah.
[inaudible].
Like specifically what the changes were?
Um, it involved-it was
a fairly complex architecture as many of these architectural search things give you.
It was much deeper,
maybe like two or three orders of magnitude deeper,
and from what I remember it was also narrower.
They also had some operations that were a bit nonstandard in Neural Networks.
Something like one-by-one convolutions or something like that,
but it's been a while since I read the paper.
Okay, so another challenge that comes up is that you have
this bi-level optimization procedure that we need to perform in order for meta-learning.
And this can exhibit some instabilities,
particularly if you don't have
as much expressive power that you'd like in your current architecture.
And there have been a few different ideas for trying to mitigate this.
So, um, one approach was to try to automatically learn
the inner learning rate of the, um, the algorithm.
Basically learn that alpha parameter that was in those equations.
And specifically one of the things that these papers have found to
work-to be particularly important here is to learn
in a learning rate that course that is different for each parameter or
different for each-for each layer of the network.
And this is because things like biases
and weights may want to have different learning rates.
Biases may want to have larger learning rates,
weights might-they want to have smaller learning rates,
and you want them to-be able to decouple from those different choices
for the different layers such that they don't have the sort of conflicting optimization.
Um, and then there's also the approach that tried to
tune the outer learning rate as well.
There are approaches that try to authorize
only a subset of the parameters in the inner loop,
subsets like some of the parameters that are,
like, affine transformations on each of the layers.
There are papers that have looked at decoupling
the inner learning rate kind of as I mentioned before,
as well as the batch storm statistics per gradient step,
since that you have a different learning rate for each gradient step,
or a different learning rate or a different set of
batch storm statistics for each gradient step as well.
And then lastly, there are also some papers that have introduced context variables for
increased expressive power that basically introduce variables that- additional, um,
variables into the neural network that are appended onto the activations at each layer,
and allow the gradient steps to store information in those parameters, uh,
in a way that doesn't interfere with the other parts of the network computation.
Um, so for me the kind of the main takeaway-for me these papers
that I think would be helpful for
you is that there's a range of simple tricks that can help the meta
optimization process significantly.
Are there any questions on these challenges before I move on to the next set?
Okay. So one more challenge that I'd like to go into a bit more depth on is
that backpropagating through many integrating steps
is going to be very compute intensive and very memory intensive.
So if you, uh if you have one inner gradient step or a few inner gradient steps
it's generally quite practical to think about how you
might backpropagate through only a couple of those.
But if you have an extended optimization process in the inner loop,
then it's very challenging to think about,
how you'd actually backpropagate through that in a way that doesn't
require storing the entire optimization process in memory.
And doesn't require backpropagating.
Ideally doesn't require backpropagating across-through
that entire- that optimization process.
Um, so there are two approaches that have been kind of proposed for dealing with this.
Uh, the first is a very crude approach for dealing with it,
which is just approximate the Jacobian of the task specific parameters phi i,
with respect to the meta parameters theta, as the identity.
This is a very crude approximation that basically corresponds to, uh,
a kind of truncated backpropagation in some regard where you just take the gradient at,
there's lots of parameters and copy it over to theta.
Um, somewhat surprisingly, this actually works pretty well,
on a number of simple few-shot learning problems like
MiniImagenet like Omniglot that we've discussed in this class,
but anecdotally from what I've found it doesn't
work in more complex meta-learning problems,
such as in meta-learning imitation problems.
Um, but it's something that's- that's probably worth trying if you have,
uh, a setting where you, uh,
are compute bound or memory bound.
It's also, I think aesthetically not as pleasing because it's a bit of a hack in some regard.
Like we know that this matrix is not actually close to identity.
And then, so I guess one way to think about this is,
can we try to compute the meta-gradient without
differentiating through this optimization process
and in a way that doesn't approximate this optimization process as identity?
Uh, and this is where I wanna go to the whiteboard to
kind of discuss how we might try to do this.
So, um, as you remember I guess from last time,
if you write down the, um, the meta-gradient,
you get a form that looks like a single backward pass at your,
er- at your task-specific parameters and then you have
a term that basically is differentiating through the update rule.
So if you have, uh- you basically need to be able to compute d of u,
uh, of Theta with respect to your parameters Theta.
Um, and this requires storing all of the iterates of this update rule.
Ah, if you try to do it kind of with standard back propagation approaches,
and so what we'd like to be able to do is compute
this derivative without differentiating through this- the,
the entire optimization path that got you there.
Um, so the first thing that's worth noting,
is we're gonna have to compute this update- this,
this forward process of the update rule no matter what.
Um, so we're still gonna at least have one full forward pass through the update rule.
Um, but there are things that we can do
to try to mitigate the meta-gradient optimization.
Um, so to write- first let's,
um, kind of write down what this update rule looks like.
Uh, and in particular we're gonna use the update rule that
corresponds- that has an explicit Gaussian prior on the,
um- on the parameters.
And we'll see why this matters in a second.
So let's say that, uh,
Phi is equal to the output of our update procedure.
This update procedure, uh,
takes as input a set of training data points.
Um, and let's say that this is equal to, uh,
the solution to some optimization problem
on the parameters with respect to our training data set,
uh, and then also with respect to
explicit Gaussian regularization that
tries to keep our task parameters close to the parameters Theta.
So this is just a Gaussian regularization with mean Theta,
uh, and variance that's a function of, um, Lambda.
Um, note here, in this case we're gonna be looking
at an inner optimization that is actually to convergence,
as trying to actually take the full argument rather
than just running one or a few steps of gradient descent.
Um, and if we're going to be doing that then actually having
this regularization term is really important because, uh,
if we ignore this term and just initialize it at Theta,
then it actually found the minimum of this function.
Ah, that minimum wouldn't actually be a function of Theta,
and if is not a function of Theta,
then we don't actually- we aren't actually imposing any
prior on that inner optimization process.
And so this is what- that's what the role of this,
um- what this regularization is doing.
It's basically imposing this prior on the inner optimization.
All right, so let's refer to this function right here.
As G of Phi prime and Theta.
Phi prime is- Phi prime is just our optimization variable.
Um, and if we actually find the arg min of this, um, of this function,
then we know that the gradient of G with respect to Phi prime is equal to 0,
uh, because we found kind of the local optimum, right?
Uh, so what we can write- is we can write down that,
if this is equal to 0 then we know that the,
um, gradient with respect to,
um- I'm sorry actually, this is evaluated at Phi prime equals Phi.
So if we know this, then we know that the gradient of
the first loss term- I'm gonna start dropping
the d train for, for notational convenience.
We know that the gradient of the first term plus the gradient of the second term,
which is just Lambda times Phi minus Theta, is equal to 0.
So this is just plugging in, ah,
the gradient of G. And then from there we can rearrange terms a bit.
So, uh, so let's bring Phi on the left-hand side of the equation,
and then we can write out that this is equal to, um,
Theta minus 1 over Lambda times the gradient of Phi- L of Phi.
So here we're just rearranging terms from this bottom equation,
or from this equation right here.
And then if we differentiate this with respect to Theta because we're
trying to ultimately get d Phi d Theta up there,
we get that d Phi, d Theta is equal to the identity,
which is just the derivative of that,
minus 1 over Lambda times the derivative of the outside, which is,
uh, the Hessian at Phi times the chain rule, d Phi d Theta.
Okay, and so interestingly what we could do here is,
we can, uh, arrange the terms a little bit and try to solve for d Phi d Theta, uh,
and the result of that is going to be
something that actually doesn't depend on the optimization path,
it only depends on the final point of that optimization.
Uh, so in particular what we can do is, we could arrange,
um- try to put both- let's put i on one side on its own, and we get,
um- we move this over to the left, we get, uh,
the identity minus 1 over Lambda L of phi,
all of this, times d Phi, d Theta equals the identity.
So this is just moving this term over to the right.
And then what we get is that we get,
d Phi d Theta equals,
basically the inverse of this term.
Okay. So one of the nice things about this is,
well, this is exactly what we wanted to get,
and this only depends on the Hessian at the final optimization point.
And so that means that, this is actually something that we can compute without actually
differentiating through that inner optimization process,
um, which is quite nice.
Uh, and I guess the,
the assumptions that we made to get there is that
the- that basically that this inner solve is accurate,
that it gets to a point where the gradient is 0.
Um, and that we of course can compute this.
Uh, and there are ways that we can try to estimate this by, uh,
using conjugate- the conjugate gradient algorithm,
which tries to estimate basically these Hessian vector products.
Of course this is just, this is just, er,
something that looks like a Hessian,
and then the, um,
the actual gradient that we'll be performing is the gradient
that we derived from last time that corresponds to the,
uh, the back-propagated vector at that optimization point times
this term right here. Yeah.
[inaudible].
Yeah. So the- there are- kind of as I was mentioning,
you can use conjugate gradient algorithms to try to estimate this inverted matrix term.
Um, and basically the number of iterations that you run of conjugate gradient,
the more accurate your estimate of this term will be.
And at convergence it will be- uh,
if you run kind of it for, for, uh,
infinite number of iterations you'll get an exact estimate or you'll get,
uh- you'll approach an exact estimate of this.
Um, if you do zero iterations of conjugate gradient,
you actually just estimate this as the identity,
which is actually just exactly the,
um- which is exactly the,
the kind of first-order MAML algorithm that's written up there.
Okay, so what does this give us?
Um, so this used what's called the implicit function theorem, um,
which is a way to kind of think about how to differentiate functions implicitly, ah,
to get these, uh- to get kind of this form of the gradient.
Ah, and so you
can look at this algorithm and one of the nice things that you get is that,
uh, particularly it's very memory, uh,
efficient, so you don't have to store that optimization process,
and then computationally, as you,
um, increased the number of conjugate gradients steps,
it'll become more compute-intensive, um,
and that allows you to basically trade off how accurate
you want the meta gradient to be with how much compute you want to spend.
Uh, and so in particular, what these plots are showing,
the first is showing the GPU memory on,
ah, a very simple,
um- very simple meta learning algorithm.
And as we see the kind of if- if we changed the number of inner gradient steps,
uh, both first-order MAML shown in purple and, uh,
implicit MAML as this algorithm is called, uh, showed in green,
are constant in memory as you increase the number of gradient steps,
uh, because they don't actually do anything to store those,
um- to store the optimization proce- procedure whereas the full MAML grad- uh,
MAML algorithm increases linearly in
memory as you have to increase the number of inner gradient steps. Yeah.
Since we derive this using the [inaudible] , uh,
does that implies that we want more inner gradient steps to get to the
more accurate, like so the gradient corresponds more accurately?
Yeah, so In practice you would want more inner gradient steps,
or I guess in theory you'd want more inner gradient steps in order to,
uh, try to have this- this be more accurate.
Uh, in practice, we found that the algorithm doesn't need, uh, well,
you could still run the algorithm in practice with,
with- without that condition being true and getting,
uh, gradients that are reasonably accurate.
Um, and then second plot is showing computation time.
Uh, and so we can see that,
for different numbers of conjugate gradient steps shown in green,
gray, and red, we see an increase in the amount of computation, um,
and then, uh, with MAML because you're computing- you're basically computing the,
the full gradient, uh,
you don't have an effective way to trade off how much computation you want,
uh, as a function of the accuracy of the meta-gradient.
Okay. So- and then I guess the other, uh,
benefit of this approach is that it means that you can use
second-order optimizers in the inner loop, uh, you can use really,
uh- you can also include like
basically non-differentiable optimizers in the inner loop because this really just,
uh, depends on the,
the final term and not actually the optimization process itself being differentiable.
Uh, and so for example, we used a Hessian free optimization approach, um,
in combination with this algorithm and we're able to outperform methods that use,
um, just like gradient descent based, uh, inner optimizers.
Okay. Um, and then the last thing worth mentioning about
this algorithm is that it's a very recent development,
and so all the,
all the typical caveats with recent work apply.
Not a lot of people have tried to, um,
kind of play around with this approach and fully, uh,
test its, its, um,
its capabilities and its limitations.
Okay. Any questions on this before moving on?
Okay. So I guess the takeaway for optimization-based methods is that,
uh, you do them by,
by constructing a bi-level optimization procedure.
Where the inner optimization is something like gradient descent,
like SVMs, uh, like Hessian free optimization procedures, uh,
and then differentiate or- or either differentiate through
the optimization procedure or do something like we did
here in order to compute the meta-gradients.
Um, the benefits of this type of approach is that you
get a positive inductive bias at the start of meta-learning,
because you're already had this optimization procedure in the loop,
and you can already expect it to do something reasonable at initialization time.
Uh, in contrast a black box approach is where
the initial optimization procedure is just some neural network.
Another takeaway or a kind of positive note about,
uh, these proc- these procedures is that they're consistent,
uh, and that they- the procedure that you run at test-time
corresponds to an actual optimization method.
And as a result it tends to extrapolate better to,
learning problems that are
outside of the distribution of what you've seen during meta-training.
And we'll get back to this point a little bit more at the very end of the lecture.
Uh, and they're also maximally expressive if you have a,
uh, a very deep neural network.
Oh, and last thing is, uh,
it's model-agnostic so it's pretty easy to combine with different model architectures.
Now, in terms of limitations,
it requires typically a second-order optimization, um,
either by differentiating through that, that optimization,
or by doing something like this where you still have second-order terms that pop up.
And as a result, it's usually pretty compute or memory-intensive.
Okay. So [NOISE] now that we've talked about
kind of embedding gradient-based optimization procedures into the inner loop,
one question you might ask is,
"Can we embed some other learning procedures into the meta-learning process,
without requiring a second-order optimization?".
This is kind of where non-parametric methods are going to fit in.
Er, so in particular what we've
been thinking about is how we can learn parametric models.
And there's this whole other class of machine learning methods, uh,
non-parametric methods namely, uh,
that are simple and work very well in low data regimes.
Um, these are things like, uh,
nearest neighbors for example.
If you have a small amount of data these methods are actually
quite effective, um, at learning.
And during meta-test time,
few-shot learning is exactly precisely in low data regime.
And so these non-parametric methods are likely to perform pretty well.
Er, but of course during meta-training we still want to be
parametric because we want to be able to scale to large datasets.
[NOISE] And so the kinda key idea of these types of approaches is can
we use parametric meta learners to produce effective non-parametric learners.
Um, great, so and I guess
one other kind of side note here is that a lot of these non-parametric methods,
um, preceded some of the parametric approaches that I've been talking about,
but for the sake of this kind of lecture we'll be presenting them afterwards.
Okay. Um, so the key idea here is to use some sort of non-parametric learner.
Um, and one kind of non-parametric learner is to think about doing nearest neighbors.
So if you want to be able to perform this few-shot learning problem,
one very natural approach that actually may have-
someone may have mentioned this earlier in the course is to think about well,
well we how would we just like take this test datapoint,
and compare it to all the training dataset data points.
And look at each of these training data points and find the one that looks the most
similar and then return the one-
return the label corresponding to the one that looks the most similar.
Um, basically compare the test image with your training images.
Uh, now, the kind of the key questions that comes up is how do you compare them?
With what metric do you compare your test image to your training image?
Uh, and if you weren't using something like meta-learning,
uh, what you might do with these types of methods is use something like,
uh, L2 distance between your data points.
Uh, unfortunately with images,
L2 distance is, uh, works very poorly.
Uh, so one really nice example of this is if you take, uh,
this query image on the right and compare it with the two images on the left,
uh, L2 distance is going to return the image on the left.
Uh, and it's doesn't- very good,
doesn't correspond well with, uh,
kind of perceptual distances and more semantic distances between images.
And so the key idea of these methods is instead of
comparing in the space of your observations,
can you learn how to compare using
your meta-training data in a way that is effective for new tasks?
Okay. So, um,
I guess the- the kind of the first type of approach that we might imagine doing here,
uh, was, uh, proposed,
uh, by Koch in 2015.
And what they did is they trained a Siamese Network to
predict whether or not two images are of the same class.
So you're essentially just learning to compare pairs of images,
and saying whether or not they're the same class or not.
And so what this could do is you can learn a more semantic,
uh, distance between two images.
And so you could take these two images,
we know that in our meta-training dataset these are of different classes.
So the label for this Siamese neural network would be 0.
Uh, for this pair of images these are two, uh,
two images that are from the same semantic class and so this would correspond to 1.
Uh, and you repeat this for different pairs of images in your meta-training dataset,
asking the neural network to predict whether or not they're from the same class or not.
Okay. So at meta-training time we are doing these pairwise comparisons.
And then if we want to be able to do few-shot classification at meta test time,
what we do is we compare each image X-test,
to each of your images in your training dataset for that task.
Basically just like we mentioned a few slides ago.
Uh, and then you output the label corresponding to the closest image.
So if your classifier for example outputs a probability of 0.9,
for the third image and a probability of 0.2 for the second image, uh,
and something lower than 0.9 for
all the others and you'd output the one that has the highest, uh,
highest likelihood prediction corresponding to your,
um, corresponding to the label of that, uh, that image.
So that the output the label that has-
that corresponds to the image with the highest likelihood of matching your test image.
Okay. So this is pretty nice,
it's also really simple, uh,
and what we're doing is, at meta-training time,
we're training this binary classifier and then at meta test-time,
we're performing an N-way classification by doing each of these pairwise comparisons.
Now, one thing you might ask is well, okay.
We- we talked a lot about meta-training and meta-testing
and trying to match what happens at meta-training and meta-testing.
Uh, and here we're not like these are,
these are different procedures, right?
We're not actually training it to do N-way classification,
we're training it to do something else.
We're training it to do binary classification.
So is there a way that we can try to match what
happens during meta-training and what happens during meta-testing?
Such that we're training it to be able to be good at N-way classification,
rather than training it to be good, uh, binary classification.
So this was kind of the,
the key idea introduced in,
uh, the matching network paper.
And in particular if we're gonna be doing nearest neighbors at test-time,
in order to match our test query image to each of our training data points,
how about we train an embedding space such that
nearest neighbors produces accurate predictions?
Um, so here's an example of what this looks like.
So we take each of the images in our training dataset,
we embed them, uh,
into a learned embedding space.
We then take our test query image and also embed that into an embedding space.
And we compare each of, uh,
each of these embeddings to make a prediction.
So, uh, each of these black dots here will correspond to a comparison between the,
uh, the test embedding and the training embeddings.
And then we'll take the label corresponding to each of our training images,
and our prediction will correspond to the weighted, uh, nearest neighbors.
Basically the, the weighted, uh,
labels of each of
the training images or the training labels weighted by their similarity score.
Okay. And then, uh, once you do this,
you can then train your neural network end to end in order to
make effective predictions on your test data points.
So the particular architecture they used in this paper
was to use a convolutional encoder, uh,
to kind of embed the images and they also used
this bi-directional LSTM to produce the embeddings of each of the,
the training data points, although in practice you could
choose- you could choose simpler things for these,
uh, for each of these models.
Um, as I mentioned the model is trained end-to-end, uh,
and most critically here meta-training is about showing what's happening at meta-testing.
So during meta-training, you're training it to make
comparisons to all the images in your training dataset,
and at meta test-time you're doing the same thing.
Making predictions, uh, for each of the, uh,
for the N-way classification problem that you're going to be doing. Yeah.
[inaudible]
So in this case I think that the motivation here was such that the,
basically information about two different classes,
can be spread to one another basically.
Like if you're trying to be,
uh, classifying between, uh,
four different types of dogs for example,
versus between a dog and a cat.
Uh, the way that you represent your embedding might be different.
For example, if you're trying to classify between dogs and cats,
your embedding space- your embedding of a dog should represent, uh,
something that is, uh,
something that is, is kind of general to all types of dogs.
Where if you're trying to do a more fine-grained classification
of classifying between two types of dogs,
then you want that embedding space to be more discriminative based on the type of dog.
Yeah.
[inaudible]
Right. So with- because of the LSTM,
it does actually impose an order for this- for this particular architecture.
In practice, the, um,
there- and there are other- if there are
other non-parametric methods that aren't order dependent.
L- and the next method that we'll talk about is not order dependent.
I think that for this particular paper,
they chose the order arbitrarily.
Okay. So how do we actually go about training this?
Uh, so the general algorithm looks basically the
same as the algorithms that we're taking- that we were looking at before.
So if we take the- the algorithm corresponding to the
amortized or corresponding to the black-box approach,
um, if we want to think about how you do this for matching networks,
we first sample a bunch of tasks.
We sample a train dataset and a test dataset for each of those tasks.
We then compute predictions using this learned similarity metric.
Uh, and note here that unlike the parametric methods,
we don't have these parameters, Phi.
They're essentially integrated out into this comparison.
And hence, it is a non-parametric approach.
And then once we have these predictions,
then we update our- the parameters of this learned embedding
function with respect to
the loss function of how accurate our predictions are on the test set.
Uh, although note here that I'm abusing notation a bit,
and that those kind of- this- this loss function would be something like cross entropy,
for example, and we'd use the predicted distribution over test labels,
rather than only the- only the output that the max labeled.
Okay. So any questions on how you would go about training matching networks?
Okay. So now, one thing that we might think about is, well,
if we're doing one-shot classification where we have one example per class,
this is pretty, uh, straightforward,
uh, because we're gonna be making, basically, uh,
we're gonna be making, uh,
making comparisons to each of those classes.
But what if we're in the case where we have more than one shot?
Um, if we have more than one datapoint per class,
then what matching networks is gonna do is gonna be
performing these comparisons independently.
And so if we have, uh- if we're doing dogs versus cats, for example,
we have two dogs and two cats,
it's just going to find the closest image,
and output the label corresponding to that.
Or to basically to do a weighted average of those,
and look at the two dogs independently,
and look at the two cats independently in our training dataset.
And so one thing you might think about is,
well, maybe it makes sense to think about a more aggregated, like,
to- to aggregate information per class in
a way that is smarter than just performing these independent comparisons.
Uh, and that's what,
uh, prototypical networks do.
So they think about how can we aggregate
class information to create a prototypical embedding of that class,
and then perform comparisons to each of
those prototypical cla- prototypical class embeddings
in order to predict the label corresponding to our test image.
Okay. So what this more concretely looks like
is we'll have a number of images for different classes.
So here, different colors correspond to
different image classes in our training dataset for a particular task.
And then we embed those- each of our training images into this embedding space.
And then take the average in this embedding space
in order to compute the prototypical embedding for class 1,
class 2, and class 3.
And then we embed our test image into the same space,
same exact space and compute, uh,
the distance to each of those prototypical class embeddings.
Uh, and then we can output the one- output the class
for which it is closest to in this embedding space.
So what this looks like in equations is we'll,
uh, embed our, uh,
each of our images in for a particular class, uh,
into this embedding space and then take the average for each of,
uh, for each of those images.
I think it can be this prototypical embedding C_k for class k. And
then to compute which class- to compute the class of our test datapoint,
we will take the distance between the embedded test datapoint in each of those classes,
and perform a softmax over each of those, um, each of those,
uh, negative distances, in order to compute the probability for the test datapoint.
Um, and then in this case,
d can correspond to Euclidean distance or cosine distance.
Some kind of your favorite dif- distance metric, uh,
in this, but computed in this learned embedding space.
Okay. So this is an algorithm that, uh,
you'll be implementing in homework too.
Are there any questions on how it works?
Okay. Cool. So this is prototypical networks.
Um, basically, what it corresponds to, uh, I guess,
basically what many of these approaches correspond to is basically embed your datapoints,
and then do nearest neighbors in that learned embedding space.
Now, one challenge that might come up is, well,
what if you want to do- reason about more complex relationships between datapoints,
rather than just doing nearest neighbors in your embedding space?
Um, in principle, if you have an expressive enough encoder,
uh, in your embedding space,
then nearest neighbors should be able to represent a wide range of complex relationships,
uh, particularly for high-dimensional embedding spaces.
But in practice, people have found it to be useful to think about
more expressive ways to perform these types of comparisons.
So for example, um,
relation networks basically takes prototypical networks and
learns the non-linear relation module on top of those embeddings.
This basically just corresponds to learning that function D in prototypical networks,
instead of using a Euclidean distance metric or a cosine distance metric.
So it's learning both the embedding and the distance metric.
Uh, another approach is to- instead of having a single prototype per,
um, per class, have a mixture of prototypes per class.
And this allows you to for example, represent, um,
more multimodal class distributions to maybe one class,
um, maybe if you have, kind of,
the class dog, maybe, uh, dogs are often either seen on snow or on grass,
and you want to be able to represent both of those, uh,
both of those kind of modes of your class in your embedding space.
It may be easier to try to allow your embedding space to have a multimodal distribution,
rather than trying to kind of change your embedding space in
a way that puts them all in the same part of the embedding space.
Uh, and then lastly,
uh, another paper has looked at.
Can we embed, uh,
perform an embedding on all of our datapoints,
and then do some sort of message passing
scheme in order to think about how these different,
uh, datapoints relate to each other,
and in order to make the predicted output?
And what this does is it uses graph neural networks in order to
perform this message passing and differentiate through it.
Okay. Um, so now,
that's kind of mostly it for non-parametric methods.
Uh, they're- they're quite simple.
And we'll talk a bit about some of the takeaways of these methods, uh,
in kind of the last part of this lecture,
where we talk about how we can think comparing these approaches.
Okay. Um, and so,
I guess, as a more meta-point, we have all these algorithms,
they're really these kind of three classes of approaches that we've talked about;
black-box adaptation, optimization-based approaches, and non-parametric approaches.
Uh, and so how should we think about how these different methods compare?
Uh, I think that there are a few different ways to think about this.
And so I'll present two different ways,
uh, to think about this.
Uh, and the first is to think about the kind of computation and graph perspective.
How do these different algorithms look like as different computation graphs?
And we- we visited- we kind of talked about this viewpoint earlier, um,
where the black-box approaches are
representing this computation graph at a completely back- black-box approach.
Whereas the optimization-based approaches,
you can view them as embedding an optimization into your computation graph.
Uh, and for non-parametric approaches,
you can also take this view.
And in particular, what the computation graph will look like,
uh, for prototypical net- networks,
for example, is something that,
uh, for your test datapoint, uh,
embeds it and compares it to each of your per class prototypes,
where those per task prototypes are computed, um,
using the embedding of each of those,
uh, each of those class's datapoints.
So you can essentially just view it as another kind of computation graph,
where we're embedding the soft nearest neighbors into the computation graph.
Okay. So with this view, um,
we can also think about how we can mix and match components of the computation graph,
uh, to get hybrid types of approaches.
Yeah.
[BACKGROUND]
Yeah.
So you could essentially view all of these methods as a computation graph.
And whether or not that compu- like,
the optimization-based methods and non-parametric methods are
essentially imposing a certain type of structure inside your computation graph,
that corresponds to things like gradient descent and nearest-neighbors,
whereas black-box methods tend to not impose any structure on that process.
And there is a bit of a gray line between like,
what- what computation graphs look more like
non-parametric methods versus what computation graphs look more like black-box methods.
Um, I think it's helpful to think about
this- this kind of different classification of methods,
because it allows us to think about the certain properties of these, these methods.
Although in practice, um,
there isn't a very clear cut line between them.
Yeah. [BACKGROUND]
Yeah, that's a good point. So it should be over x y such that y equals k. Yeah,
exactly. And I'll try to fix that.
I noticed that on the previous slide and I'll try to fix that on the,
um, before we post the slides online.
Okay, is there a question in the back? All right so, um,
we can again because we can think of these as computation graphs that also is
pretty easier to think about how we might try to mix and match components of these.
So one approach which is a bit of a hybrid of black-box approaches
and optimization based approaches or
maybe optimization-based non-parametric depending on the way you view things,
um, is an approach that tries to condition a model on an embedding of the data set,
of your training data set and also run gradient descent on that model.
Uh, in practice this sort of- these sources of information by conditioning on
the data with a direct way as well as through
gradient descent could potentially be redundant.
Um, although in practice it seems like this method found some benefit in doing that.
Uh, another idea here that you could do is have some sort of
embedding of your function and then do gradient descent on that embedding space.
And so in particular they have this paper user relation network
to embed your training data set and think
about how different data points relate to one another.
And then they decode this embedding into the parameters of
a neural network that makes predictions about new data points.
And then instead of running gradient descent on the parameters of that function,
they run gradient descent in the learned embedding space Z,
uh, which produces, uh, different functions.
So this you can essentially view it as running gradient descent on
a lower-dimensional space of your weights rather
than running gradient descent in the original space of your weights okay?
And then the last, uh,
approach that I'll cover is that there's also
an approach that looked at doing something exactly like MAML.
But initializing the last layer of the network to correspond to prototypical networks.
Um, so it's basically specific form of a particular choice of
the network architecture for MAML that
initializes it to do something like a comparison-based,
um, a comparison based prediction.
Okay so that's the kind of
computation graph view is one way to think about how these different algorithms compare.
And the other way that I like to think about how these algorithms compare is to think
about the different properties of the individual algorithms.
Um, and in particular I think there are two properties that are really important as
we start to think about developing meta-learning
algorithms and developing new meta-learning algorithms.
The first is thinking about expressive power of these algorithms.
So we talked about this a bit before, it's basically
the ability for that function f that I showed on
the previous slide to represent a range of learning procedures as a function of,
um, as a function of your training dataset.
And the reason why this is important is that it means that
as you get more larger and larger meta-training
datasets you'll be able to produce a more flexible range of learned optimizers.
And as you apply these types of algorithms to more challenging optimization problems,
you'll be able to do better than the kind of
the standard optimization procedures we have today.
So essentially it has to do with scalability and where these methods will
end up in the future. Ah, if you could only represent
a small class of algorithms then you're, you may not be as
effective when moving towards broader meta-training datasets.
Okay so this is the first property and the second property which I
alluded to very briefly before is the property of consistency.
And in particular what I mean by consistency is
that the meta-learning algorithm will produce
a learned learning procedure that will solve
the task with enough data regardless of the properties of that task.
It will essentially produce a consistent learning procedure such that given enough data.
What I mean by consistent learning procedure is one that will kind of
asymptotically solve the task given enough data at that task.
And so for example things like gradient descent correspond to
a consistent learning procedure because we're just running gradient descent at test time.
And you can expect at the end of
given enough data for that test task you'll be able to
solve a task regardless of what your meta-training data was.
Now what is, why is this
important? Well first getting meta-training data
that corresponds, oh, I'll get to your question in a second.
First getting meta-training data that
corresponds closely to what you'll be seeing at test-time is pretty hard.
So we haven't really talked about this much but
we've been assuming that we have this meta-training dataset.
And we can use this meta-training dataset in a set of
tasks in order to do well at new tasks.
But in practice, how do we determine what those tasks actually correspond to?
This is actually a really hard problem as we think
about where we are going to be applying these algorithms.
And so if we produce a consistent learning procedure, then we
can expect it to do something reasonable on tasks that aren't necessarily,
uh, especially close to the meta-training tasks that we trained it on.
And we can also get basically get better out of
distribution task performance. There's a question.
[inaudible]
Yeah, so the I guess the question was about, um,
like can't we have a catastrophic initialization
such that gradient descent doesn't actually give us a good solution.
Um, and I guess the short answer is yes.
I guess one thing we can
assume with gradient descent is that we'll at least get to a local optimum.
Whether that local optimum is good or not it is a kind of
another question and it could be that we could have an initialization
that puts us in a place that the local optimum is actually very
bad for that particular basin.
Um, so absolutely and
that's something I think that people haven't thought about quite as much yet.
And so thinking about how we might, um,
how we might try to tackle that sort of problem and does it actually happen in practice,
is do we actually get to a local optimum that are bad is kind
of another question. The-
[inaudible]
Something like that. Yeah yeah.
So there are other ways to try to think about how
you might formalize this problem as well.
And something like basically monotonic improvement could be one of them as well.
Uh, yeah.
And I guess theoretically thinking about that sort of problem may also
draw very closely to some of the theoretical questions in deep learning in general,
like are local optima good with high probability,
things like that, and people have
started- People have looked at that sort of problem in the past.
Okay. Um, and I guess one thing I was going to mention here is that,
[NOISE] we recall that is in practice things like gradient descent.
If we're running that at test-time,
do tend to empirically actually hold up with regard to getting better
out of distribution task performance in comparison to black-box adaptation approaches.
Okay. Um, and so in my mind these are the kind of the two properties that are
most important for many different applications of meta-learning.
Not necessarily the benchmarks that we have because the benchmarks are well defining
a nice set of meta-training tasks and sort of meta tests tasks,
test tasks that kinda closely align with that.
But in practice if we're thinking about applying these algorithms on real problems,
these are the two properties that I think that we're gonna care about the most.
Okay. Um, and so let's think about how these- how
the different algorithms compare on these different axes.
So for black-box approaches we know that they have complete expressive power,
ah, in the respect that things
like recurrent neural networks are universal function approximators.
Ah, and we also know that they're not consistent they- and that they won't, ah,
if you are imposing any structure on the function,
on- on your black-box function,
then they won't- there's no guarantee that they'll produce anything that is consistent.
Okay. For optimization based methods we know that, ah,
it reduces to gradient descent,
ah, at least for things like MAML.
Ah, and so in that sense, they're consistent, ah,
for some definition of consistent with regard to things like monotonic improvement.
Um, and we also know that they are expressive if you have,
ah, deep enough models.
Um, and in practice we've found these methods to perform, ah,
well on- on- on settings where you
do want to be fairly expressive with regard to few-shot learning algorithms.
Ah, I put an asterisk here because this
actually doesn't hold up in some reinforcement learning settings,
and we'll potentially talk about this a bit later in the course.
It mostly holds in supervised learning settings.
Ah, it also depends on the particular reinforcement learning algorithm that you use.
So it's a bit of a nuanced,
ah, a nuanced thing that we'll discuss later.
Ah, and then with regard to nonparametric approaches, ah,
these methods are expressive for most architecture choices.
Ah, for example, if you're using things like LCMs then there are a wide range of,
of functions that you can represent, ah,
although there's a bit of nuance, ah,
depending on the types of learning algorithms that you might want to learn,
ah, and that they're also consistent under certain conditions.
Ah, so they are consistent in the sense that if your embedding,
um, is not losing information that is, ah,
losing information about the inputs that is
not- that- that's important for making decisions,
then, ah, as you accumulate more and more data, ah,
you'll eventually get something I do- kind of
asymptotically it will eventually get to a datapoint that's
arbitrarily close to your test data point,
and then be able to make, um,
the correct prediction for that test data point.
Okay. Um, so beyond this aligor- beyond
these properties there's also other properties that are pretty
important for thinking about with regard to different applications.
Um, so things like,
I- being really easy to combine to- with a variety of learning problems.
Ah, this is true for black-box approaches because it's,
it's really easy to basically just plug in your,
ah, plug in different loss functions or
different optimization procedures into these types of architectures.
Um, the downside as I mentioned, ah,
last time is that it does invo- involve a challenging optimization in
that there's no good inductive bias at initialization to point it in the direction of,
ah, a real optimization procedure.
And as a result, they are often fairly data inefficient
because you have to learn how to learn completely from scratch.
Okay. With regard to optimization-based methods as we talked about earlier,
we have this positive inductive bias at the start of meta-learning because
we're initializing it with a real, ah, optimization algorithm.
Um, it can handle- I guess one thing I didn't mention it before is it can handle
varying K and large K, ah, relatively well.
If you have more data than what you trained on for example,
um, these approaches still tend to work well because they're consistent.
Ah, and they're also
model agnostic in the sense that you can plug in different architectures and apply them,
ah, with, with- conceptually with-without any difficulty.
The downsides as I mentioned before, you know,
we have a second-order optimization and it's
usually compute intensive and memory intensive.
Ah, and these two of, these two points are quite
important for our range of applications where you care a lot about compute,
ah, and memory particularly when you're scaling to large datasets.
Okay. And then with regard in non-parametric methods, ah,
we didn't cover the pros and cons,
of this one yet other than these two.
So the first is that,
one of the nice things about these methods is that they're
entirely feed forward architectures,
they don't involve any,
any backpropagation within that computation graph.
And so as a result,
they tend to be very computationally fast and they tend to be
very easy to optimize in contrast to architectures that involve recurrence,
that involve, ah, gradients pushing backwards etc.
Um, and then some of the downsides of these approaches is that,
ah, they're hard to generalize to varying K. This is more
of a empirical observation that people have found.
Is if you test them on more K than what they're trained on,
they tend to underperform what,
what other algorithms are able to achieve.
Ah, it is also harder to scale these to very large datasets,
ah, at test-time because they're using non-parametric approaches.
And so far these methods have also been limited to classification.
Ah, in principle you could also apply them to things like regression.
But the- with the caveat that you could
only interpolate between the labels if you kinda, ah,
if you naively apply these approaches to regression,
you could only interpolate between the labels that you saw in your,
ah, in your task specific training set.
Because we're just doing a weighted average of those labels at test time.
Okay. And then at a more high level,
it's worth mentioning that generally well tuned versions of each of
these algorithms tend to perform comparably on existing few-shot benchmarks
and as I alluded to before, things, various bells and
whistles like using ensemble as they're using, or tuning the architecture can lead to,
ah, are, are often the kind of the differentiating factor between these methods,
rather than the actual underlying method itself.
Um, this likely says more about the benchmarks than about the approaches themselves.
Ah, and I think that, basically in,
in many cases which method you want to use will depend heavily on your use case,
and whether or not you care about things like consistency,
whether or not you care about expressive power,
whether or not you care about computational efficiency, ah, etc.
Okay. Any more questions on these- kind of how these algorithms compare?
And when you might use one versus the other?
Okay. So I guess that's mostly it.
The, um, for today.
So kind of to, to recap.
We talked about, ah, two algorithmic properties.
We also talked about a computation graph perspective.
Ah, one third property that's useful to
think about is thinking about uncertainty awareness.
I- by this I mean kind of the ability to reason
about ambiguity during the learning process at test time.
Ah, and the reason why this is important is
that if you want to do things like active learning,
or have calibrated uncertainty
estimates when you're learning from small amounts of data,
or if you're in reinforcement learning settings and you want to
reason about what data you should collect in order to,
to reduce your uncertainty about the task,
then you need to have some notion of, of your uncertainty.
Ah, and uncertainty comes up especially in few-shot learning problems,
where you only have a small amount of data and your prior can't necessarily make up for,
ah, what the true task is.
Um, and the kind of- the other kind of place where this comes in is
that we talked about this really nice Bayesian motivation at
the beginning of the course in the- in the second lecture,
in the third lec- second or third lecture, and,
ah, we've kind of moved to be- moved to fully deterministic approaches.
And in the next lecture we'll talk about
basically more principled Bayesian approaches that get back to that initial motivation,
and also give us things like calibrated uncertainty or
more calibrated uncertainty and approaches that allow
us to think about how we could collect more data to reduce our uncertainty.
Ah, and we'll discuss all of those things on Monday.
Um, on Wednesday we have, ah,
student presentations again, ah,
that will be covering various, um,
various algorithms and extensions of the things that we've been talking about,
ah, and then a few more reminders again, Homework 1
is due on Wednesday.
Please fill out the poster, ah,
presentation preferences for the dates, ah,
we need to know kind of when you're available,
and information about the course project is online. I'll see you on Wednesday.
 Okay. Let's get started. So uh, today,
we'll be covering Bayesian Meta-learning algorithms.
Uh, and just first there is some logistics [NOISE].
Homework 2 came out last Wednesday and is due next week on Wednesday.
Um, your project proposal is due two weeks from today.
And so if you don't have a clear idea for who your group is going to be,
if you're gonna work alone, what your project is gonna be,
then we encourage you to come to office hours and discuss that.
Uh, or post on Piazza or e-mail one of us.
[NOISE] Um, also, we determined a,
uh- we've kind of set in stone a date and
time and location for the poster presentation.
It will be on Tuesday,
December 3rd at 1:30 PM,
uh, and we expect,
uh, for all on-campus students to have
at least one group member present at the poster session for at least part of it.
Uh, so that you can kind of present your work to
the broader AI and machine learning community at Stanford.
Okay. Um, so let's get started with the lecture.
Uh, and I'd first like to start with,
um, a couple of disclaimers.
The first is that Bayesian Meta-learning is a very active area of
research and something that's even more active than,
uh, other parts of this course.
And so, uh, there may be
more questions than answers in terms of the particular algorithms that we have.
Uh, but at the same time, this makes it a pretty
exciting topic because there's- I think there's a lot of
room for better algorithms and better ways to be thinking about,
uh, how to evaluate these algorithms.
[NOISE] Uh, and then [NOISE]
the second disclaimer is that this lecture probably
covers some of the most advanced topics of this course,
uh, and so I'll try to go slowly through,
through some of the technical content.
And, uh, I guess to both of these please ask [NOISE] questions, uh, if things aren't, uh,
aren't clear, uh, and I'll try to also go through the relevant,
uh, background material with regard to Bayesian neural networks, for example.
Okay. Um, so let's get started by recapping,
uh, the things that we talked about last time.
So we had covered different, uh,
meta-learning approaches and also a couple of
unified ways to thinking about these different meta-learning approaches.
So the, the first perspective was
this computation graph perspective where we
looked at all of these different meta-learning algorithms as,
uh, as different computation graphs.
Uh, some that are completely black-box,
some that have gradient operators or optimization procedures embedded inside of them,
and some that use non-parametric, uh,
algorithms kind of embedded inside of it as well.
Um, so this is kind of one way of thinking about
meta-learning algorithms as a function that takes in the dataset and,
and test input and makes a prediction about a new input- about that input.
[NOISE] Uh, and then second,
we also talked about an algorithmic properties perspective [NOISE] of,
of these methods which is that, uh,
what sort of properties do these algorithms have and,
uh, what are kind of the trade-offs between these different properties?
So we've talked about, uh, expressive power, uh,
basically the ability for these functions to
represent a wide range of learning procedures or
a wide range of functions of the dataset that's being passed as input.
We also talked about consistency which is that given enough data,
these algorithms will produce an answer that you're satisfied with.
Uh, and these important- these- probably both of these properties are very important.
Uh, and the third property that we haven't really talked about much
so far is this concept of uncertainty awareness.
Uh, and that's what we'll be covering today.
Well, uncertainty awareness is the ability to reason
about ambiguity during the learning process.
The ability to reason about whether or not, uh, whether or not,
the kind of it's very clear what the function is or
whether or not you should have uncertainty about the underlying function,
um, that your data is providing evidence for.
So uh, why is this important?
Uh, we'll motivate this a bit more in more detail but at a high level,
the reason why we care about this is that it allows us to think about, uh,
active learning, about explicitly reducing
our uncertainty if we have uncertainty about our function.
It also lets us think about how we might explore, uh,
new environments in a reinforcement learning context in order to reduce our uncertainty.
Uh, it also thinks about if we're in safety-critical settings,
we want to calibrate uncertainty estimates.
That's also, uh, pretty important,
and it also makes us think about [NOISE] uh,
from the Bayesian perspective of Meta-learning, uh,
what sort of principle approaches can be derived from those graphical models.
Okay. So that's all we'll be covering today.
Uh, at a high level,
we'll talk about first, uh, go,
go into more detail about why,
why it makes sense to use Bayesian and kind of set up their framework,
uh, for which we'll be talking about.
Then we'll talk about different meta- different Bayesian Meta-learning approaches, uh,
in the context of the different algorithms that we've discussed in this course so far.
Uh, and then lastly, we'll talk about how we can
evaluate a Bayesian meta-learning algorithm,
um, and the different types of ways
these meta-learning algorithms might be useful in practice.
Okay, good. So that's the plan.
Uh, let's first talk about, uh, some motivation.
So at the beginning of this course, uh,
we talked about some principles behind multi-task learning and meta-learning.
Uh, and one of the first principles we talked about is that- is kind of a,
a basic principle of machine learning which is that training and testing must match.
Uh, and if the conditions that uh,
that you're- if the conditions that you see at
test time are not matching the conditions that you're training for,
then you won't necessarily expect the ability to
generalize or the ability to learn a new task.
Uh, and then the second principle that we had talked about
was that the task must share some sort of structure.
Uh, any- without the structure,
you won't get any benefit from sharing across the tasks.
And this is the, the point that I'd like to go into a little bit more depth on,
which is that, what is structure even mean?
Uh, and I think that one way
that we can actually try to formally define what structure means here,
is that there's some statistical dependence
on some shared latent information across the tasks.
[NOISE] Uh, and in particular what we can do,
uh, is we can bring up this graphical model,
uh, which I have showed in,
uh, a few lectures ago, uh,
where Theta is the kind of the meta parameters that are shared across all tasks.
Phi i is the task specific parameters for each task i,
and then we also have a number of data points, uh,
index Phi j for each of the i tasks.
Um, and in particular in this graphical model,
you can see that there is some dependence on Theta.
Basically, there's- uh, each of the, uh,
each of the the parameters Phi i have an arrow coming from Theta.
[NOISE] And in particular if you condition on the information in Theta,
we know, uh, first that the task parameters become independent.
Uh, that is Phi, uh,
Phi i is independent of another Phi i conditioned on Theta.
[NOISE] And if you do condition on Theta,
on the latent information,
then they're not otherwise independent.
And in particular, if you then look at these two properties,
which is that they're not independent without any conditioning information and
they're- and that they're independent once you condition, um,
then you can say that you- the distribution over Phi given Theta is gonna
have lower entropy than the distribution of the marginal of Phi.
[NOISE] Essentially, Theta tells you information
about your task or specific, task-specific parameters phi.
Okay. This all makes sense?
So now, I'd like to give you a thought exercise,
uh, which is that if you could identify Theta,
uh, for example, through Meta-learning,
if you can identify what's shared across the tasks,
in which situations should it be faster to learn Phi in
compared to learning from scratch? Are there any thoughts on that?
[NOISE]
Or maybe I can rephrase the question.
In which sit- situations would it not be faster than learning from scratch? Yeah.
The first entropy is much faster than the second one. Should it be faster?
Yeah, that's a good point. So yeah, exactly.
So if the, if the first entropy is much,
much smaller then- that theta basically tells you a lot about the task,
and it should be much faster to learn, uh,
than just learning from scratch because learning from scratch a little bit just
be looking at Phi. Was there another?
Then without giving the Theta then we
should take just as long to go to any of your [inaudible] Theta
Can you repeat that?
Yeah. So basically if knowing Theta keeps
them independent then they should take this long to learn in the Theta.
Yeah, exactly. So that's also true.
So basically if they are,
um, if they're independent before knowing conditioning on Theta,
then basically these entropies will be the same,
and then learning from traps will be just as,
as, uh, just as fast as learning from the shared information Theta.
Any other thoughts on this?
So one additional point that's I guess worth mentioning is that if,
uh, if a single datapoint,
uh, carries all the information of Theta.
Like maybe, uh, your tasks correspond to
all possible image classification tasks sampled at random,
uh, and the fact that you see a single data point like an image,
it being an image, that doesn't really tell you that much.
Uh, or the image kind of- the, the basically the,
the shared information doesn't tell you that much because that information is
also exis- exists in a single data point which is that is an image classification task.
Uh, and in those situations,
uh, first the, uh,
the entropy conditioned on Theta is going to be,
uh, fairly high as well as the marginal entropy.
And the, um, and also th- kind of thinking about how this compares to the,
the entropy conditioned on one data point versus a large number of data points.
Okay. One other thought exercise; uh,
what if the entropy of Phi given Theta is 0?
What would happen in that case or,
or what, what situation does that describe?
We don't even need to learn Phi.
Yeah, so we don't even need to learn Phi.
If basically they're the same,
then- or if, if,
if this- if, um,
there isn't any additional information in Phi that isn't captured in Theta,
then Theta can solve all of the tasks,
and you don't need to do anything to learn from Theta.
All right. So these are, these are kind of interesting properties to think about.
Uh, for example, if you're in this last,
uh, in this last setting,
if you can basically learn a single set of
parameters that captures all their tasks and you don't
need to necessarily do any learning in order to adapt to new tasks.
And you can just basically use that single parameter vector for solving the new task.
[NOISE] Okay.
Um, so what information might Theta contain?
Um, we talked about this a bit earlier in the lecture,
uh, or earlier in the course.
Um, but one thing to kind of reiterate here is if we look at
kind of a couple of examples of the- that we saw earlier in the course,
for example, um, this toy sinusoid problem that we saw in some of the,
the student presentations, uh, last week.
What information would Theta contain in,
in this task family?
Any thoughts?
Yeah.
Now the task function is the shifted, uh, sign.
Yeah. So if the- basically if the s- if
the family of tasks corresponds to sinusoids with different,
uh, amplitude and different phase,
then Theta will correspond to tho- that exact- that family of sinusoid functions, right?
And it'll basically correspond to everything but the phase and amplitude,
which is the task-specific information that needs to
be inferred from data in this example.
Um, and further, if you look at the language translation case tha- that was presented a,
a couple weeks ago,
uh, in this machine translation example,
Theta is going to correspond to all possible language pairs of data, uh,
and the information in Phi that isn't present in
Theta is going to correspond to things that are specific to a particular language.
Okay. Um, and note that in both of these examples,
Theta is narrower than the space of all possible functions.
And it's because of this, this is why we can get a benefit
from using things like Meta-learning in principle.
Okay. Um, the last set of exercise that we also haven't really talked
about much in this course is what if you meta-learn without a lot of tasks?
So if you have an infinite number of tasks, you can, uh,
you should be able to recover Theta exactly,
um, or basically recover kind of that,
that family with, with high precision.
What if you don't have a lot of tasks, what happens then?
[BACKGROUND]
Sorry, what?
The bootstrapping of the tasks.
What do you mean by that?
I mean re-assemble the tasks can create a des- and a description of the tasks.
You mean make more tasks?
Yeah.
Yeah. So if you don't have, if you don't have a lot of
tasks you should definitely- you should pause.
It's good to try to make more tasks.
If you can't make more tasks, what happens? Yeah.
[BACKGROUND]
So yeah. So what it will do is if you have kind of a space of tasks
it won't necessarily cover the true distribution of
tasks but will potentially overfit, uh,
or what I would call meta overfitting to that space of tasks,
such that actually doesn't recover,
uh, Theta that corresponds to, for example,
all language pairs but it co- find the Theta that corresponds to a set of
language pairs that looks like the things in your training data
and not something that captures the full distribution.
Uh, and then as a result, you won't be as effective at
adapting to new things from that distribution,
uh, from the broader distribution,
unless they're very close to the training examples.
So basically a form of overfitting that's lifted and not from data points but to tasks.
Okay. So here are a couple of examples I think of how Bayesian Meta-learning or,
or this graphical model at least helps us think about these different, um,
properties of Meta-learning algorithms and,
and different things that will happen if you train in different situations.
Um, now a bit more motivation here.
So that was kinda motivation at the conceptual level,
what about in practice?
So, so far we've looked at parametric approaches a- a- and non-parametric approaches.
But for the parametric approaches,
what we recovered was the deterministic estimate of the task-specific parameters Phi,
given the dataset and your meta-parameters Theta.
So you'd essentially get a point estimate for this distribution right here.
So this- it seemed to work fine on your homework.
You implemented this and, and you perfe- presumably got decent performance out of this.
Uh, is this a problem or,
or why is this a problem?
Well, there are a few situa- there's some situations where,
uh, we need more than just a point estimate.
So, uh, for example,
some few-shot learning problems may not be fully determined by their data.
And in particular because you're in the few-shot learning regime,
it may be that the underlying function is
ambiguous given the evidence that you have and your prior information.
So as an example of this,
say you have, uh, 10 examples, uh,
five positives and five negatives and your goal is to classify between, uh,
between these two- two binary classification problem between these,
these, uh, two classes.
And in particular, all the images on the left are people that are smiling,
people that are wearing a hat,
and people that are young.
And all the people on the right are people that are not smiling,
not wearing a hat, and not young.
And then if you get a new test image of someone who
is smiling and wearing a hat and not young,
the underlying function or the underlying
label for this particular image isn't- i- is inherently ambiguous.
We don't know if we're supposed to be classifying, um,
with regard to smiling, with regard to wearing a hat,
with regard to being young or any pair of these attributes.
Um, and for ge- we can also look at another example
where someone is wearing a hat and young but not smiling.
So inherently, the- kind of the answer to- or the correct label for
these images is ambiguous because the dataset was so small.
Okay. Um, so to try to reconcile this problem,
uh, what if we can generate hypotheses about the underlying function?
So what if we can sample from this distribution?
Uh, if we, if we can sample from this distribution,
then we can reason about our uncertainty, we can, um,
then kind of reason about, uh,
basically whether or not we're gonna be,
be able to make an effective and accurate decision in the situation,
which is really important in safety critical settings.
Um, it can allow us to explicitly reduce
our uncertainty because we can get some measure of,
uh, of which examples have high uncertainty and which examples have low uncertainty.
And then we can try to ask for labels from the,
uh, from some user.
Uh, and there's been a number of works, uh,
that have looked at this and also- so one of the presentations on Wednesday is, um,
I think believe the third paper here which will be covering,
uh, specifically Meta-learning approaches for active learning.
Uh, and lastly, it will be important for learning to explore in,
uh, meta reinforcement learning which we'll cover in a couple lectures.
Okay. So at the kind of bare minimum,
we wanna be able to sample from this distribution.
Uh, we also may want to try to generate, um,
try to be able to do other things like evaluate
the likelihood of a function or to be able to,
um, measure like the entropy of this distribution, for example.
Okay. So that's kind of the motivation for Bayesian Meta-learning algorithms.
Any questions before I move onto the particular methods that people use?
Yeah.
[BACKGROUND]
Yeah. So for active learning,
if you have an estimate of
basically your- your confidence or your uncertainty regarding a particular example, uh,
like for example, in these, uh,
if you have- if you're trying to, uh,
acquire a more accurate classifier in this setting and you have some examples that
look like this and some examples that look like these ones on the left,
then you should- the kind of the correct thing to do is to
ask for a label for the ones that you are- are uncertain about
and such that you can explicitly try to reduce
your uncertainty about the underlying function.
And so these types of uncertainty estimates give you a means to reason about that.
Okay, so let's start talking about
some approaches for being able to sample from this distribution or being able,
being- basically being able to represent our uncertainty.
So, uh, the first
simplest approach that we'll talk about is something that could actually,
could actually be applied to all three classes of Meta-learning algorithms.
Ah, and in particular,
if we care about generating a distribution, um,
over our predictions, one thing that we could do is just let
our function f output the parameters of a distribution over our label.
Um, so for example, uh,
you could have, uh,
any of these functions f, uh,
from kind of the computational graph perspective output
something that corresponds to the probability of, uh,
discrete categorical distribution if your label y is discrete,
or if, uh, you can make it discrete.
I could also- you could use also output the mean
and variance of the Gaussian distribution to represent,
uh, a- a Gaussian distribution over your labels,
or if you have a more multi-modal distribution you could try to represent, uh,
the means and variances and mixture weights of a mixture of Gaussians, um,
or if you want to do something more sophisticated,
if you have a multidimensional,
uh, output label y,
then you could output the parameters of a sequence of distributions, uh,
sequence of conditional distributions to allow you to
represent the joint distribution over those variables.
Uh, and this would be what's called an autoregressive model.
Uh, and once you output the parameters of the distribution,
you can then train, uh,
any of these approaches with the maximum likelihood estimate optimizing both over the-
the mean of the distribution as well as the variance or basically all- the,
the full probability values of that distribution.
Okay. So this is really simple, um, and this is,
and fo- for example,
if you do, um, mean squared error, uh,
for example that corresponds to optimizing for the mean of a Gaussian,
uh, but not the variance where the me- variance corresponds to a,
uh, the identity matrix.
Um, so and then, then you actually are so- are sort of already
implicitly getting a distribution. Um, yeah.
[BACKGROUND]
Yeah, exactly. So in the first case, the, uh,
cr- the cross entropy loss corresponds to maximizing the likelihood of,
uh, doing maximum likelihood with a categorical distribution.
So in classification examples that,
uh, we're already doing it.
Um, in principle, this should give you reasonable, uh, uncertainty estimates.
In practice, uh, I guess this is,
I'll get to the pros and cons of the second but in practice, uh,
doing that doesn't necessarily give you good,
um, good calibrate uncertainty estimates.
And the, um, and neural networks,
for example, in general tend to be a bit overconfident in their predictions.
Okay, so this is a very simple approach.
Uh, the benefit is that it's simple,
uh, and you can combine it with a variety of methods.
Uh, downside is, um,
first is that it doesn't nec- always lead to accurate uncertainty estimates,
which I actually didn't write down on the slide but that's certainly,
uh, one of the downsides.
Um, another downside is you can't reason about uncertainty about the underlying function.
Just the uncertainty about the individual data points.
Uh, and so for example,
uh, you- you can't, uh,
disentangle model uncertainty versus label noise, for example.
Um, and is also limited to a particular class of distributions over y, uh,
and in some of the following slides we'll look
at more expressive classes of distributions,
um, where we can represent more,
basically distributions over the functions fairly expressively.
Okay. So this is version 0.
Um, now the first downside
here is that we can't reason about the uncertainty about the underlying function.
So one very natural thing to think is well, can we just use maximum,
the same maximum likelihood training but over the model parameters Phi?
So does anyone wanna comment on why we could,
wha- how we could do this or how we couldn't do this? Yeah.
[BACKGROUND]
Yeah, so we could have our,
our neural network output, er,
like a mean and variance for each of the parameters in phi,
and how do we, if we did that, say we did that,
how would we go about training the- the,
um, training our model to output that distribution?
Go ahead. Yeah?
It's while you're training,
I figured there are certain critical performance you can start to track
the mean and variance of the variables after you get past
a certain threshold of performance which would see like average mini-batch what their
mean and variance would be for optimizing for each particular sampling of a mini-batch?
I see. So you're basically saying that we could take our mean and the mean variance
of our function to optimize- like to judge how well that is, and then-
Yeah. Basically, I assume that after you've trained
over a certain number of [inaudible] box or something,
you are kind of getting into a stable equilibrium for that.
And once you've sample each mini-batch,
you check where this gradient descent,
descends to for those parameters and then start to track that and
generate a mean and variance from wherever those moved around, if that makes sense.
I see. So how would you get,
how- when you say you train them,
what do you mean by training the-
Yeah. It's something [OVERLAPPING]
I- oh so you- you- you- you're saying that you train the,
the parameters phi for a given task,
see where it ends up. And then.
Yeah, then after- [LAUGHTER] after it's had a certain like
acceptable performance then af- there after over each mini-batch,
it's gonna be, uh, moving around,
um, kind of in the fine tuning stages.
Within those fine tuning stages, you'll see, uh,
what the mean value is over
a certain number of fine tuning steps and also the variance within those fine tuning steps.
Yeah, so basically that would correspond to something like you kind
of run gradient descent on for a given task,
see what the- or SGD or something,
see what the mean and the variance are and then use that to try to
fit your- fit this, this estimator.
Yeah.
Yeah. So that would be an interesting approach.
I don't know if- I don't think anyone has done anything like that before,
but it'd be interesting to see. Yeah.
We've got a project.
Yeah, there you go. [LAUGHTER] Any other thoughts? Yeah.
I feel like if you try to [inaudible].
Yeah. So you can't- so the- we can't do it exactly the same way as we did,
uh, for the top one because we don't have labels corresponding to Phi,
unless we generated labels,
uh, like was saying.
So uh, yeah.
So it's an interesting thought exercise to think about,
and we'll talk about basically different ways that we can get, uh,
[NOISE] we can get- uh,
we can basically trained for this, uh,
without having to have labels for the particular parameters that we, uh, that we get.
Okay. So, uh, before I go into the different approaches,
I want to give a kind of a one slide overview
of kind of the Bayesian Deep Learning Toolbox which is, uh,
kind of an overview of the different types of techniques we
have available to us to- for combining,
um, [NOISE] Bayesian graphical models and deep learning.
Uh, it's worth mentioning that CS 236 provides an entire course on this topic.
Uh, and so one slide is,
by no means, gives it, uh, justice.
It's a very com- a very,
uh- there's been a lot of work in this area but, uh,
I think that this will be important for thinking about how we can
build Bayesian meta-learning algorithms on the next few slides.
Okay. So the general goal of these types of
approaches is to think about how we can represent distributions using neural networks.
Uh, and there's been a number of approaches for doing this.
Uh, and one of the most popular approaches are, or a very
popular approach is to use latent variable models,
uh, and then optimize them with variational inference.
Uh, and so in particular,
uh, what this corresponds to, uh,
is we have a graphical model or something like a graph- model- graphical model on
the right where we have latent variable Z and observed variables X.
Uh, this is the graphical model for example for a variational auto-encoder.
Uh, and then what you do is you can optimize,
um- you can optimize for a distribution over X using, uh,
the variational lower bound of- you basically
formulate a lower bound of the likelihood objective and
use that to optimize over a distribution over X where
that distribution over X may be non-Gaussian because it has,
uh, this latent variable.
Uh, I'll go into a little bit more depth on this approach,
uh, in, uh, a few minutes.
Okay. Um, another approach that has been fairly popular in some regard,
uh, and is quite simple,
is to use a particle-based representation of your- uh, of your distribution.
Uh, and in particular,
what you can do is you can train separate models on different bootstraps of your data.
And then get, uh- each of
those models will correspond to a particle of your distribution,
uh, and then you get kind of this- you- basically,
together, those particles represent,
uh, samples from that distribution.
Uh, so this is something that could be useful, uh,
it's pretty simple like a- pretty easy to combine with different,
uh, types of algorithms.
Uh, another approach which has been- somewhat alluded to, uh,
is to represent an explicit distribution over the weights of neural network parameters.
Uh, and then practice these distributions tend to be Gaussian with, uh, an independent,
uh, a diagonal co-variance matrix,
so you have basically an independent variance for each neural network parameter.
Uh, and this allows you to represent, uh,
a distribution over functions,
uh, with the caveat that, uh,
this independence assumption that two parameters are independent is violated in practice,
uh, basically all the time.
Uh, there's also things, uh,
like normalizing flows that try to- that represent a function of over,
um- over a data distribution by inverting some latent distribution into
your data distribution or transforming
from latent space into your data space and back into your latent space.
Uh, and these- these have been pretty successful for,
um- for a wide range of applications.
And then lastly, uh, is energy-based models and to some approximation GANs.
Uh, and what these types of models do is they tend to estimate a,
a normalized density where you have, uh,
some probability that you want- or
some probability over your data that you want to represent.
Uh, and the way that you do that is you [NOISE] , uh,
push down the energy to- have lower energy
for your data and higher energy for everything else,
uh, where everything else is approximated by your generator,
in a GAN, or something like that.
Um, yeah.
So [NOISE] these are the two- so this is kind of an overview of the general types of,
uh, tools that we have in, uh,
in deep learning to be able to represent distributions over data and over functions.
Uh, in this lecture,
we'll see how we can leverage the first two on this list,
uh, for Meta-learning approaches.
Uh, the others could certainly also be useful
in developing new Bayesian Meta-learning methods.
Uh, to my knowledge,
people haven't looked at these types
of approaches in the context of Bayesian Meta-learning,
uh, but they could certainly be, uh,
be interesting for developing new methods.
[NOISE] Okay.
Great. So that was,uh- I guess,
for those of you that ha- that haven't seen this content before,
that was a lot faster than,
than you probably would need to,
to learn these sorts of things, of course.
Uh, but this is just- but the goal of this- so first,
um, we're gonna look at how we can use
latent variable models in the context of meta-learning.
Uh, and to do this, I want to give a little bit more background on,
uh, variational techniques and,
and latent variable models and deep learning.
Uh, and in particular,
this is the graphical model that we'll be looking at.
Um, this is the graphical model that is used in,
in variational auto encoders where x is typically representing
the data and z is representing your latent variables.
Uh, but in many-
this graphical model is more general than just representing distributions over your data.
Uh, I will see that on the next slide.
Uh, but I'll kind of derive things from the context of this graphical model because it's a
very general graphical model corresponding to things that are
observed and latent variables that we want to be able to reason about.
Okay. So in this example,
the observed variable is x and our latent variable is z.
Uh, again these could be- this could be data and,
and something latent information but it could also be- uh,
x can also represent something else.
Uh, and what, uh, prior works have done is
derived a lower bound to the likelihood of our data which,
uh, is called the evidence lower bound or the ELBO,
and this looks like this.
So the- uh, we want to be able to estimate a lower bound on,
uh, the likelihood of our data.
This is so that we can optimize for the likelihood of our data,
and we can represent this by, uh,
what's shown on the right here which is the expectation with
respect to q, uh, of z given x.
What q is representing,
um, the variational distribution, uh,
of the probability of x and z,
plus an entropy, uh,
term that is regularizing or operating on q of z given x.
How many people- I guess,
how many people are familiar with kind of the background of,
of how you get this equation?
Okay. And how many people would find it
useful to actually go through how we get this equation?
Okay. Cool, let's do that. It's not- it's actually not that complicated.
So we have some- we're starting with the log likelihood of our- of our data,
and we also have some- some codes that looks like.
So we want to be able to approximate this,
where X is our observed variable.
And the reason why we wanna do this is typically we want to be able to
maximize the likelihood of our data with respect to
some parameters of our model such
that we can maximize the likelihood of things that we're observing.
So what we're going to first do is,
under that graphical model we can say that this is simply, uh,
the integral over some latent variable
of the joint distribution of our observed variable and our latent variable Z.
This is fairly straightforward.
And then what we're gonna do here is we're going to
introduce what's called a variational distribution.
This is- this can be really any arbitrary distribution,
and we're going to introduce it like this.
So we're just going to multiply it in and divide it out.
Now, QZ can really- uh,
is some arbitrary distribution.
We'll talk about exactly how we instantiate it and how we can
optimize for it such that we get a tighter bound later.
Um, this is equal to.
So if you basically bring this term over here,
you can see that we are getting- we can get an expectation with respect to q.
So this is equal to the expectation with respect to q of z of p of x,
z divided by, oh,
I'm missing the log here, sorry.
Okay. Um, and then when we
look into- what we can do here is that with Jensen's inequality we can show that this,
we can basically bring the log into
the expectation and we get a bound which corresponds to, uh,
the expectation with respect to q of z of log p of x,
z minus log q of z.
Okay and then from here,
uh, we basically get what's written on the board.
So this is equal to expectation of q of z times log p of x,
z plus the entropy of q of z.
Cool. I mentioned that q of z can be any arbitrary distribution.
That means we could also condition q of z on x as is done on the slides.
Um, yeah.
Okay. Any questions on- on how this works?
Should there be a log on the slide then?
Yes, that's a typo.
There should be a log basically inside the p of x, inside the expectation.
I'll fix that before- I'll fix that and update the slides.
I made these equations like yesterday so that's why there's a couple of typos.
Okay. Um, and this could also be,
there's probably a typo in the next one too.
Yeah, there's also a typo on the next one.
Um, so these can also be rewritten as- you can basically take out- You can
represent this as log p
of x given z times p of z.
And with this you can then, uh,
bring this term into- combine this term with entropy term here,
and you get a KL divergence between q and p. And so this may be the,
if you're familiar with variational autoencoders you may see
this term a bit more frequently, uh,
where the first term corresponds to the reconstruction loss of your decoder.
Basically the likelihood of your data according to your decoder after sampling from your,
your inference network q, and the second term corresponds to
the KL divergence between your inference network and your prior.
Okay. Um, so p corresponds to your model,
and q corresponds to this kind of variational distribution that we
introduced in order to approximate the, uh, likelihood objective.
And so then we have a couple,
couple of things, a couple of design choices.
So the first design choice is how do we represent p of z,
and the second choice is how do we represent p of x given z.
These are kind of the two, um,
the two parts of this,
um, of the model.
And p of x given z can be represented as a neural network.
So in the case of a variational autoencoder,
p of x given z corresponds to a neural network that takes in your latent
variable and outputs an image or whatever your,
uh, whatever datatype you're modeling.
And then p of z is represented as a- is typically represented as
just a diagonal Gaussian with unit variance.
Uh, you'd also learn your prior.
P of z could also be represented by a neural network as
well or represented as a learned mean and a learned variance.
In practice in variational autoencoders you typically don't learn it,
because the layer afterward can transform it into a learned mean and variance.
Um, but we'll see in,
in the Meta-learning case are actually gonna be situations where we'll want to learn it.
Okay. Um, and then q of z given x is also represented by a neural network.
This is also often referred
to as your inference network or your variational distribution.
Okay. And then to connect to the graphical model a bit,
typically in, in- uh,
when using variational inference in the context of
deep learning you often use theta to represent your model parameters,
and phi to represent the parameters of your inference network.
I dropped these in all of these equations because we're gonna be using theta and phi to
represent different things as you might imagine on the following slides.
Okay. So this is all nice.
We have an objective, we can optimize.
We have kind of neural networks that are representing different components of this,
and we can just backpropagate our objective into our neural networks. Yeah?
[BACKGROUND]
Yes, exactly. Yeah. So that solid arrows are representing the,
the model distribution and
the dashed arrows are representing the variational distribution.
Okay. So we have an objective.
Uh, we should be able to optimize it, right?
Uh, but we have a problem.
Uh, the problem is that we have an expectation with respect to
q and we need to be able to back-propagate into this q distribution.
Uh, and sampling unfortunately, is not differentiable.
Uh, so, uh, one of the kind of, uh,
tricks that has been actually pretty critical to making these, uh,
making this type of approach work in
the context of neural networks is what's called the reparametrization trick.
Uh, and in particular,
what we can show is that for a Gaussian q of z given x,
you can represent q of z given x in a differentiable way that is kind of
re-parameterized in terms of this noise that is sampled from a unit Gaussian.
So in particular, uh, if we're
representing the distribution of our latent variable as a Gaussian,
uh, which corresponds to the output of this neural network q,
which is outputting both the mean and a variance,
then you can represent,
uh, the output of that, uh,
neural network as kinda being reparameterized by this noise rather
than sampling from that particular distribution of the mean plus the,
the variance times that noise.
And fortunately, this equation is something that's differentiable.
Uh, we can differentiate with respect to this e- uh,
for this equation with respect to the mean and the variance,
parameters into the parameters of q,
into the parameters of that neural network.
Okay. Um, so this is something special that you can do for Gaussians.
I believe there's also some work that has looked at non Gaussian distributions as well.
Uh, although in practice,
I think that the kind of the easiest thing to do is,
is, is to do it with the Gaussians.
Uh, and if your inference network is expressive enough,
it should be able to transform
your data distribution into this Gaussian distribution over latent variables.
Okay. So that was the primer on variational inference for deep learning.
Any questions before I move on to how we can use this with meta-learning?
Okay. Um, also, I guess worth mentioning,
this is often called amortized variational inference
and that we're having an inference network that's, uh,
that's predicting what the variational distribution will be as basically
amortizing the process of estimating that distribution.
There are also variational inference techniques that
I estimate this distribution over Z by using something like,
uh, MCMC, uh, for example by,
by optimizing with respect to your, uh, your likelihood.
Uh, so can we use amortized variational inference for Meta-learning?
So, uh, first let's think about black-box Meta-learning approaches for simplicity.
Uh, and in particular,
what we wanna be able to do is have a neural network that takes as input a train
dataset and produces a distribution over our parameters Phi.
Uh, and then we're gonna take our parameters Phi and have that
parameterize a neural network that takes as input x and outputs y.
Okay. So in the standard VAE,
uh, our observed variables X,
our latent variable Z and we kind of derived what the,
um, what the, the,
the lower bound of our likelihood is.
In the Meta-learning case,
our observed variable is our data and our latent variable is our parameters Phi.
And so what we can do is we can basically use everything that we derived here
in for basically Meta-learning where the latent variable is now going to be Phi.
And in particular, what we can do is you can take,
uh, take the ELBO written above.
Again, sorry for the lack of logs on the left.
I'll, I'll, I'll fix that typo.
Um, we get basically,
something that looks like we'll have
a variational distribution over our task specific parameters Phi.
Uh, we'll sample from that distribution and estimate the likelihood of
our data given Phi and then we'll have this KL term that's saying that the,
uh, the variational, uh,
distribution and our, um,
our prior over Phi should be similar.
Okay. So this is pretty simple.
Um, there's a couple of design decisions.
So the, the first design decision is what should q condition on?
Um, in this case,
the- if we wanna be able to sample parameters,
so if you basi- I guess I sort of gave it away in the top left.
If you wanna be able to sample parameters as a function of our dataset,
then we should condition q on our training data.
Uh, so that's exactly what we'll do here.
And, uh, also, one question is
how does this training data differ from the probability of our data used here, uh,
and what we'll use for the data here will correspond to our test data points.
So you wanna be able to maximize, uh,
the likelihood of the test data points given our task-specific parameters
when sampling our task-specific parameters as a function of our training dataset.
Okay. Um, so that's gonna be what our inference network looks like and what our,
uh, what our objective looks like.
Uh, what about the meta parameters?
So we're missing the meta parameters right now.
Uh, they don't appear anywhere.
Uh, and that's because I didn't actually give any parameters to p and q. Um,
so the meta parameters will come in.
Is that there'll be, uh,
parameters of our model.
Um, so I added here,
uh, basically, we'll have our, our
prior over our task-specific parameters Phi be conditioned on Theta.
Um, although, you could also have a prior that corresponds to I guess,
a unit variance or something,
but if you condition on Theta you might get a bit more expressive power. [NOISE]
And then, we'll also condition our inference network on Theta as well,
um, because well, basically,
Theta will correspond to the kind of the parameters of our, our inference network.
[NOISE] Um, one other thing worth mentioning in this case,
uh, this- the distribution over our test data points is a function of only Phi.
We could also condition on Theta here.
Uh, and that will correspond to something that's a bit more like what you implemented
in your homework, where the, um,
where Phi is something that's like a low-dimensional vector and
Theta is representing like the parameters of a neural network.
Okay. Uh, so for completeness,
our final objective is written down here,
where we're gonna have- and again,
I'm missing the log in front of the p but,
uh, we're gonna be maximizing,
uh, with respect to our meta parameters and expectation over all of our tasks.
And then, uh, we'll sample task-specific parameters from,
uh, from our neural network q,
use those to maximize the likelihood of p. And then,
we'll also have this term on the right that
encourages q to stay close to some prior distribution.
Note that if you drop this right KL term,
we get something that looks basically exactly like what we've been optimizing before.
Uh, this right KL term is what actually ensures that it
corresponds to a distribution and that we're actually maximizing,
uh, a lower bound on the likelihood in this graphical model. Yeah.
So this, this, subject corresponds to the ELBO, uh,
for the same conditional variational inference [inaudible]
So what the variational inference biases do is it ensures that q is out- is,
is going to be outputting a, a proper distribution over Phi.
And so if you only did maximum likelihood over the labels, then,
you'll get a distribution over y but you won't necessarily
be guaranteed that the distribution over Phi is actually a proper distribution.
It's actually like something that represents the true distribution over your, um,
over your latent parameter,
over your latent task parameters. Does that make sense?
[inaudible] objective corresponds to ELBO let's say like?
Um. Yes. It corresponds to the ELBO of,
uh, of, of the, the,
the, the evidence lower bound of the,
the observed variables in that graphical model.
Yes. Basically, I yeah, to me,
what I advise you is, is that you can represent
a distribution over Phi and nothing more than that.
I guess- oh, the last thing it does give you also sorry is, um,
you can represent non-Gaussian distributions over y now or,
or not like if y is continuous, for example.
Uh, this because we're introducing a,
a latent variable the- this distribution, uh,
conditioned on Phi it will- it needs to be as Gaussian or something,
but because you're introducing this latent variable.
Now because it's conditioned on Phi this, um,
the marginal distribution of y given x can be something that's highly multi-modal,
can be, be really anything.
Anything that can be represented- that can be transformed from
a Gaussian to another distribution by a neural network.
Okay. Cool. So there's our objective, um,
the benefits of this approach is that you can
represent non-Gaussian distributions over y,
uh, and you can also get a distribution- I didn't write this.
We can get a distribution over your task parameters Phi,
rather than only getting a distribution over your labels.
So this allows you to represent of- about your uncertainty over the underlying function,
and not just about the underlying data points.
Uh, the downside is that this can only represent,
uh, Gaussian distributions of P of Phi given theta.
Uh, and that's, uh, there's I guess two reasons for that.
One is the reparameterization trick, which you can, uh,
which holds for Gaussian, uh,
Gaussian distributions but it's much more difficult to do for non-Gaussian distributions.
Uh, the second thing is that the, um,
the KL term that comes up in
the objective is something that can be evaluated in closed form for Gaussians,
uh, but can't be evaluated in closed form for other non-Gaussian objectives.
Okay. Um, and the second thing is, is a downside,
particularly if Phi is representing all of the parameters of a neural network.
If it, uh, if this distribution is also conditioned on Theta,
and for example, you do something in
your homework where Phi is representing a small vector,
and Theta is representing the majority of those neural network parameters,
then in my mind this is less of a restriction, uh, because you can,
um, because the- basically,
it is okay for Phi to be, uh,
Phi to be Gaussian because it could,
could be transformed by the neural network into a more complicated distribution.
Okay. Any questions before we move on to optimization-based Meta-learning?
Okay. So what about optimization-based Meta-learning approaches?
So, um, you might say okay,
well, we, we talked about this a little bit,
uh, a couple of lectures ago, uh,
where there is this paper, um,
that kind of recasted Gradient-Based Meta-Learning
as inference in this hierarchical Bayesian model.
Uh, and this is nice in that it provide a Bayesian interpretation of MAML as,
uh, as kind of doing, uh,
learning meta parameters such that you are- such that at
test time you're doing map inference under
this- under a Gaussian prior represented by your meta parameters.
So this is nice, uh,
but it isn't kind of what we set out for at the beginning of this lecture.
Which is, in particular,
what it's doing is it's using a map estimate of your task-specific parameters phi,
as a function of theta.
And that means that it's representing a point estimate of this distribution and it's only
giving you one set of parameters for this distribution, and you can't,
for example, sample from this distribution or you can't,
uh, more easily represent the full distribution of your task parameters.
Okay. So how can we develop approaches that actually allow us to do that?
So one thing we could do is build upon
the kind of, what we had derived before for black-box Meta-learning,
uh, where we have- uh, we're going to use amortized variational inference.
We have this objective, uh,
that corresponds to the likelihood of the data, plus, uh, uh,
minus this KL term, and in particular,
one thing to remember is that q can really be any arbitrary function.
Uh, and in particular, what we did before,
is we had q be a black-box that takes as input- that has
parameters theta and takes as input a data set and outputs a set of parameters,
but we'd also do other things for q.
Uh, for example, q could also include a gradient operator and in particular,
it's so you can have an inference network that is basically
performing gradient descent inside your inference network.
Because q doesn't need to be a neural network,
it could be anything that outputs a distribution over phi.
And so in particular, in this, in this paper what, um,
what this approach does is theta set q to be SGD,
with respect to the mean and variance of neural network weights.
So we're gonna be running gradient descent with respect to the mean of a set of
parameters and the variance of the set of parameters with respect to some training data,
and then everything else is the same.
And in particular, I guess one thing that's worth
mentioning is that in contrast to things like MAML,
this is not just running a gradient descent on a parameter,
is running gradient descent on the mean and the variance of the parameters, uh,
and that such that you actually,
at the end of this procedure you get both the mean and variance of the,
of the parameter, it's not just a mean.
Okay. So, uh, the benefit of this approach is that you're,
you're running gradient descent at test time, and so we,
we get an optimization-based Meta-learning approach
by basically stuffing gradient descent into your reference network.
Uh, and it's also- it's quite simple, uh,
the downside is that you need to model
this distribution phi given theta as a Gaussian, uh,
and the reason for that, uh,
similar to what I- the same reason why we need to do it for,
is, is the same reason for why we need to do it in the black-box case.
Which is that we need to be able to re-parameterize
in order to backprop gradients into q,
and we need to be able to evaluate the KL term that's on the right.
And in, in the case of optimization-based Meta-learning,
this is more of a problem
because these are actually going to be representing the parameters of our neural network.
Okay. So can we model a non-Gaussian
posterior with optimization-based Meta-learning algorithms? [NOISE]
So one thing that we talked about, uh, in terms of the kind
of toolbox of our approaches is that we don't
just have- there isn't just latent variable inference.
There isn't just variational autoencoders,
we can all see something like ensembles.
Um, and so this is what, uh, Kim et al did in, in 2018.
Uh, the first kind of basic version of
the algorithm was to just train m independent MAML models.
So you just run MAML on different, uh,
subsets of your data,
then you get an ensemble MAMLs.
It's also worth noting that you could do this with something other than MAML as well.
You could do- like you could do an ensemble of black-box models or non-parametric models.
Uh, and this is actually- it's pretty simple, and it actually works pretty well.
Uh, and so if you only like if you want to kind of- probably the simplest approach for,
for getting, uh, a distribution over models,
you could just train an ensemble of things. [NOISE]
Uh, one downside of this approach though is if you,
if you really care about getting accurate distributions,
this won't work well if your ensemble members are too similar.
And sometimes if you just train networks independently,
you might end up with, uh,
those models looking very similar depending on, uh,
some of the implementation details of how you initialize how,
how you sample the data for each model.
Um, and so, what, uh,
what the others did in this paper is they showed that there's,
uh, ways to make it more diverse.
So in particular, there's an approach called Stein variational gradient descent.
Uh, and what, what it basically does,
I don't want to get into some the- to all the details,
but what it basically does is it pushes the different ensemble members
away from each other to encourage them to represent different models.
Uh, and the way that you accomplish this is that you
have- when you're running gradient descent,
you have your typical likelihood term,
but you also have a term that encourages
the different ensemble members to be different from one another.
Uh, And there's different, uh, different kernel- kernel like
functions that you can do to represent the similarity between models.
Okay, and so the result of this is you get a more diverse ensemble of MAMLs. [NOISE]
Okay. Um, the only thing that they did, uh,
here as well, is instead of just pushing the particles away from one another,
they also optimized such that the ensemble gives you,
um, a distribution of particles that produce high likelihood.
And so instead of completely separately training these different models, they took, uh,
their ens- basically, they took their ensemble and they
optimized a term that depends- basically,
they optimized them all jointly together.
So you have- if you have M particles,
you're out- you're optimizing for the likelihood that's averaged over
those M particles. All right.
So the benefits of this approach is that it's simple,
and tends to work well.
It also gives you non-Gaussian distributions,
which is quite nice.
Uh, the downside is that in some instances- in
some scenarios you may not want to represent m different sets of
parameters and this is something that requires you to maintain m, uh,
instances of your model. [NOISE]
Uh, the authors said that one way to get around this is to only
do gradient-based inference on your last layer.
So you only have to have m copies of the last layer of your network,
rather than n copies of the entire, uh,
entire network, uh, although in practice you may, uh,
you may also want to be able to represent distributions
over all of your parameters not just over the last layer.
Okay. So, so far we talked about
how we can use amortized variational inference and how we can use ensembles.
Uh, there's one more approach [NOISE] that, uh,
I'll talk about here but any questions on the ensembles? Yeah.
In the last one, you mentioned where you have,
uh, sort of jointly optimizing for high likelihood.
Could it be the case so that all M of your different models just find
the point of- like all find the same point of
high likelihood and that would be [inaudible] likelihood,
but they wouldn't be [NOISE] distinct or independent from one another?.
Yeah. So that's- yes,
so basically if you only did this,
this right term they might just all kind of collapse to
a single point of high likelihood and that's exactly what this,
this left, uh, thing will do.
It'll try to push them apart from each other such that they
represent, um, they represent,
uh, different parts of the distribution.
[NOISE].
Okay. So is there a way to try to get it on Gaussian posterior over
all the parameters without having to maintain separate model instances?
So this is the last approach that we'll talk about and in
particular what we'll try to do I guess.
Yeah, I guess, yeah or yeah.
What we'll try do is we'll try to sample para- parameter vectors at
test time with a procedure that looks like Hamiltonian Monte Carlo.
And in particular what Hamiltonian Monte Carlo does is it
adds noise and then runs gradient descent,
uh, repeatedly in order to be able to sample from some distribution.
And so in particular what we'll wanna do is if we have,
uh, say we have our example, uh,
shown here where we have this ambiguous situation,
what we want to be able to do is learn a prior such that a random kick in
a direction will put us in different modes of our distribution.
Uh, so for example if this is our,
um, our loss landscape,
uh, and there's different,
uh, different, kind of, modes of solutions.
One corresponds to a classifier that classifies on smiling and wearing a hat,
one that classifies on smiling and being young.
We want, uh, essentially parameter vector here such that if we
add some noise to that parameter vector and then run gradient descent,
we get two different modes of this solution.
We get different functions that represent, uh,
different- we get different functions that
represent different modes of the correct answer.
Okay. So this is what we want to be able to do at
test-time, just add noise and run gradient descent.
Uh, the way that we do this, uh,
so first let's bring up the, our graphical model shown before.
Uh, in this case we're gonna have theta be a distribution.
So one, uh, one nuanced thing I didn't mention before is before if we just,
kind of, putting Theta as parameters we are not actually,
um, having a distribution over Theta.
In this case, uh,
Theta will no longer just be,
kind of, a single parameter vector that parameterizes things.
It'll actually be a distribution.
Uh, and then we'll also have
a set of- we'll also have kind of our task-specific parameters given Theta.
Our goal, uh, will be to sample
different task-specific parameters given all of our observed variables at test time.
And the things that we can observe at test-time are X train,
Y train, and X test. [NOISE]
Okay. Um, so this is our,
our- just like our goal is from before, uh,
I guess one of the things to note here first is that, uh,
we can actually cross out this term because X test is
conditionally independent of Phi given Y test or sorry.
When you're not given Y test they are conditionally independent or
that- when you're not given Y test they're independent and so therefore we can,
uh, we can ignore this term and just model, uh,
sample- try to sample from the function Phi given X train and Y train.
Now if we try to actually derive what this corresponds to in this graphical model
we get an integral of the, um,
of P of Theta times P of Phi I given
Theta times our data given Phi where we need to integrate out our prior Theta.
Okay. So unfortunately, uh, this is kind of the,
the exact solution for
this distribution but of course its integral is completely intractable.
Uh, and so what we're gonna say I guess first as a side note is if we knew, uh.
So this integral is- is completely intractable and so we have to do something different.
One thing we're doing is if we knew this distribution,
if we knew how to sample Phi given Theta X train and Y train,
then sampling becomes a lot easier.
So we- if we knew this distribution,
then we can just use ancestral sampling where we first sample
a Theta and then we sample from this distribution to be able to sample from Phi.
Uh, and particularly the graphical model would look like this where we first sample Theta,
and then sample Phi given those three variables.
Uh, so if we knew that distribution things would be easy,
how do we get that distribution?
Well, what we can do is we can use, uh,
an approximation similar to what was shown in the paper by Grant et al which is
estimate this approximation crudely using- estimate
this distribution crudely using a point estimate for Phi,
uh, and we gonna approximate this using MAP inference.
Um, this is again extremely crude,
but it's also extremely convenient, uh,
which is that if we can basically approximate this without MAP inference
corresponding to running a few steps of gradient descent, then, uh,
unlike the previous paper we can still actually sample from Phi
where we first sample from Theta then we run a few steps of gradient descent,
uh, and then get our model.
So this is what happens,
uh, at test time where we want to do inference.
Training is a bit more complicated but it can be done with
the tools that we mentioned before using Amortized Variational Inference.
Uh, So I'm not gonna go into how we do training,
but if you're interested you can- you can see the paper.
Um, what ancestral sampling looks like is basically exactly what we've tried before.
So first we sample, uh,
we have our, our,
our distribution. We have new Theta.
First we sample from P of Theta which has,
uh, a mean and a variance,
this corresponds to starting from this mean-variance adding noise like
the re-parameterization trick and then we run
gradient descent starting from that sample Theta,
uh, running gradient descent with respect to our theta.
And that gets us into these types of- it
allows us to represent these types of multimodal distributions.
Okay, uh, so the benefit of this approach is that it gives us a non-Gaussian posterior.
Uh, it's very simple at test time.
We can just add noise and then run gradient descent.
Uh, and we only have one instance of the model.
The downside is that it has a more complex training procedure and it
also has some some fairly crude approximations that are in the- uh, in the derivation.
Okay. So that's it for methods.
Uh, to summarize we talked about, uh,
first a very simple method which offers a distribution over Y.
The benefit of this is that simple,
you can combine it with a variety of methods.
The downside is you can't reason about uncertainty about the underlying function,
you have a limited, uh,
class of distributions over Y that you can represent, uh,
and in practice these methods tend to
produce, uh, uncertainty aspects that are not very calibrated.
Uh, we also talked about how we can use
Amortized Variational Inference with
black box approaches where we could represent non-Gaussian distributions over Y, uh,
but only Gaussian distributions over Phi which is fine if Phi is a latent vector.
Uh, well mostly we talked about optimization based approaches which I've looked at using
Amortized Variational Inference ensembles and the
sort of hybrid inference procedure that I mentioned,
uh, the benefits of the amortized variational inference
was that it was simple but we need
to model the distribution of our- of our parameters as Gaussian.
Ensembles allowed us to use non Gaussian distributions, uh,
at the cost of maintaining M separate models or doing inference on the last layer.
Uh, and then the hybrid inference approaches allowed us to represent
non-Gaussian posteriors with only a single model instance,
uh, with a more complex training procedure.
Any questions on methods before I talk about
evaluation and what we can actually use these methods for? Yeah.
[BACKGROUND]
Yeah, yeah. Exactly. So I guess, well,
so this is also a, a downside of ensembles as well, is we can only sample.
We can't actually- we don't, um, we can't,
uh, analytically represent things like entropy, um, likelihood.
It, it- it's much more difficult to represent those things.
Um, we can only sample.
But one thing you could do is you can sample and
then empirically estimate like what was the variance of my samples,
um, and we'll actually see, in a couple of slides,
how these methods actually work if you wanna estimate that sort of uncertainty.
Other questions? Okay. So how
do we evaluate these methods?
Um, because these papers are relatively new,
I've- I figured the kind of the way to cover,
this was just to describe a number of different ways for,
uh, for effectively evaluating these algorithms.
Um, I guess the first question is well,
can we just use standard benchmarks?
Can we use things like MinilmageNet to evaluate these Meta-learning algorithms?
Um, what do you think?
What do you think are the kind of pros and cons of using standard benchmarks?
Yeah?
[BACKGROUND]
Good, yeah.
The pros is that we can- to repeat the question for people who are remote,
the pro is that it corresponds to something we care
about the con is that it doesn't actually evaluate uncertainty.
Any other pros and cons?
Okay.
Most of the algorithms that have been made for
this task are like [inaudible] this, this the task.
Yeah.
[BACKGROUND]
Yeah. So a lot of algorithms have already
evaluated on this and so it may be overfitting to these benchmarks.
Uh, it may also be that they're- yeah, that the- yeah,
we're overfitting on these certainly.
Anything else? Yeah?
[BACKGROUND]
Yeah. That's a good point.
So in this example, in MinilmageNet,
there may not be that much ambiguity in the underlying task.
Uh, it may actually be- there may be actually no ambiguity in,
in the correct label given the dataset.
Okay. So, um, here's some of the things that I had done.
So one pro is that they're standardized and so it
makes it a little bit easier to compare across, across papers.
Although, in practice, some papers destandardize it
by using different architectures, using different protocols.
Um, another benefits that has real images,
it's kind of a, a very real problem.
Um, and it's also a good check that your approach didn't break anything.
Uh, if the- if for example,
you're protecting tanks on these benchmarks,
that means that something is probably wrong.
Uh, the downsides that I had written down is that,
first metrics like accuracy don't actually evaluate,
uh, things like uncertainty.
Uh, as was mentioned,
the tasks don't- may not actually exhibit that ambiguity.
Uh, and lastly, uncertainty may not actually be that useful on this dataset.
Okay. So what are some better problems and,
and better metrics that we can use?
Uh, I guess we're benching- it kind of really
depends on the problem that you care about, uh,
and how you measure, uh,
how you measure the- how good these algorithms are and,
uh, which problems to test on.
So I'll cover some, uh,
and these can maybe inspire different applications.
Uh, so one very simple way to try to look at these,
um- look at the performance of these algorithms is to look at varied toy examples.
So, uh, here's an example from,
ah, some of our prior work,
although there's a number of other methods that have
also looked at varied toy examples where you can
actually visualize the underlying function that you're sampling from.
Uh, so the top example corresponds to,
uh, a class of, uh, the underlying Meta-learning.
Functions are both sinusoids and linear functions where
there's noise in the labels to make the task ambiguous.
And so you can see that it, uh,
that it kind of- the types of functions that it's reasoning about, uh,
in some situations, it's ambiguous even if it's a sinusoid function or a linear function.
And you see this sort of multimodal output from the sampling distribution, uh,
and that's something- that's something that's nice to see because
it shows that the function can actually represent something that's multimodal.
Uh, on the second example is
a classification example where you're just given one data point.
All of the, the tasks correspond to circular decision boundaries.
Uh, and you're trying to classify between positive points and negative points.
The D_train only contains one negative point-
one positive point and D_test contains both positive and negative data points.
Uh, and so basically you get these different- you,
you can see a visualization of the decision boundaries
here shown in the- in the dashed lines,
where the function is- the neural network is
representing these different decision boundaries that kind of
represent the structure of the class of functions that it was trained on.
Um, so this is- these are somethings- things that are kind of cool to look at and,
and nice to see, although at the same time it's
not- it's not a problem that we actually care about.
Uh, so what are problems that we care about a bit more?
So, uh, Gordon et al looked at a task that corresponded to generating,
uh, derivative models from a few examples.
And so, uh, there is- in this case,
it was a one-shot learning problem,
where the one data point is shown on the left,
and the goal is to generate examples, uh, to be able to
build, build and generate new examples of that object instance.
Uh, and you can see that it's able to,
um- as showed in the second row,
it's able to generate,
uh, new, new instances.
And they also looked at,
uh, a quantitative evaluation.
So in this case,
the qualitative evaluation, uh,
allows you to actually judge the quality of the predictions to some extent.
Uh, and the quantitative evaluation allows you to look across the board.
Um, unfortunately, these metrics mean squared error and SSIM,
uh, actually aren't great metrics for, ah,
evaluating the kind of the predictive,
like whether or not this is actually covering the distribution
well and accurately representing that distribution,
uh, but it does at least give you
a quantitative measure of the performance on that dataset.
Okay. Um, another thing that we can look at is things like likelihood, uh,
or we can look at a combination of accuracy as well as
coverage of the correctness- of the distribution that you care about.
Um, and so this is something that, that we did, uh,
motivated kind of by the example that I showed before,
where you're given, uh,
a training dataset that, uh,
corresponds to three attributes,
and then you have test, uh,
test examples that you need to be able to label
that it corres- that have only two of those,
um, two those attributes.
And you can see that qualitatively,
that's able to learn classifiers that classify
on only pairs of attributes rather than all three attributes.
Uh, and then qualitatively,
we can see, uh, also look at this both accuracy and coverage.
Uh, in this case, coverage was measured by,
does it cover the three possible modes of this distribution?
Uh, and then lastly, also looking at average negative log-likelihood.
Uh, so what are the benefits of this?
This is that, these tasks are actually ambiguous,
and they're actually real tasks.
Um, the downside is that the, uh,
is that this task may or may not look
like the task that you actually care about depending on your application.
Okay. Um, another example that I thought was interesting is, uh,
this paper by Ravi and Beatson,
they were looking at- they're trying to look at,
how reliable are the uncertainty estimates that you get from different algorithms.
And so what they plotted is they plotted,
uh, the confidence of the predictor,
uh, versus the accuracy of the,
um, of the predictor.
So on a set of examples,
how well does its confidence correlate with its accuracy on those examples?
And in the ideal case,
you get something that exactly follows this diagonal line.
Uh, and, uh, this is the,
the- Ours is representing the amortize approach that I mentioned before.
Uh, Probabilistic MAML was the third approach that I
mentioned that uses a hybrid inference,
and then MAML is something that doesn't have- is,
I guess basically corresponds to the first approach
where you just output a distribution over y. Um,
and they looked at both these,
uh, both these, these diagrams as well as looking at the,
um, the overall accuracy of these approaches.
And I think it's important to keep in mind
both of these accuracies as well because it's, it's,
it's possible to do a lot better on these diagrams while
doing a lot worse on accuracy, for example.
And as you become more and more accurate,
it becomes harder to estimate the,
uh, your confidence about the things that you're inaccurate about.
Okay, uh, and then the last example that I'd like
to give is an active learning evaluation.
So, um, there were actually two papers that did this, uh,
one on a varied toy regression domain as well as one on MinilmageNet.
And in both experiments,
what the, uh, authors did is they looked at,
can we sequentially choose data points that we want labels for?
And in particular, the way that you choose a data point that you wanna label for
is the one that has the maximum predictive entropy,
uh, for that data point.
Uh, and there are different ways to measure
predictive entropy based on whether you're using, uh,
ensemble or whether or not you're using a,
uh, just kind of providing samples from that distribution.
Um, but I guess what we see across the board is that, er, uh,
the- these algorithms, uh,
Probabilistic MAML on left and, uh,
the ensemble methods on the right are able to show
an improvement by using either a diverse ensemble
or by representing uncertainty at all in comparison
to just randomly sampling the data points with MAML.
Okay. Um, so this is- one of the nice things about
this example is that something that we may actually care about in practice and so it's a,
it's a downstream application that's very
relevant for being able to learn from fewer labels.
In this case, we can easily get higher accuracy with fewer queries.
Um, the downside is that maybe,
maybe active learning on sinusoid or
ImageNet is not the thing that we ultimately care about,
we may wanna do active learning setting on a downstream application that we,
uh, where that amount of data is much more sparse.
Okay, so, uh, to wrap up, uh,
we've now talked about how we can reason about uncertainty during learning, uh,
and Meta-learning such that, uh,
Meta-learning is the way that
those uncertainty estimates are something that are actually,
uh, valid and correct.
Uh, next time on Wednesday, we'll see how we can look at Meta- learning
for active learning as well as Meta-learning for unsupervised learning,
semi-supervised learning, and weakly-supervised learning.
Which I think are actually, uh,
some pretty cool ways to think about these types of problems.
Um, and then on Monday,
we'll start talking about reinforcement learning.
Great. Um, so some reminders.
Homework 2 is due next week,
project proposal is due in two weeks and poster presentation date is fixed.
I'll see you all on Wednesday.
 So, so far we've covered multi-task learning and meta-learning topics in the context
of supervised learning as well as in the context of uh, hierarchical Bayesian models.
And so today we're gonna talk about what these types of
algorithms start to look like when we move into sequential decision-making domains,
uh, in reinforcement learning.
And so today we'll get started on that and we'll get started by talking about, kind of a,
a primer on, on reinforcement learning in
the multitask reinforcement learning problem, uh,
goal condition variants of that and, uh,
then we'll, in future lectures,
over the next couple of weeks,
cover additional topics in the context of
reinforcement learning when you have multiple goals, multiple tasks, etc.
Um, first some logistical items.
Homework 2 is due on Wednesday this week.
Uh, Homework 3 will be out on Wednesday this week and Homework 3 will cover,
uh, topics in goal condition reinforcement learning
and including some of the things that we're talking about today.
And the product proposal is due next Wednesday.
Okay, so, um, first,
uh, why should we actually care about reinforce learning?
So, uh, we talked a lot about supervised learning.
Supervised learning is, is used in a wi - wide variety of places.
Um, and I guess, first to, kind of, answer this question,
let's think about, well, when do you not need sequential decision-making?
Uh, and anywhere else
are things where you need sequential decision-making systems.
So, uh, you don't need sequential decision-making systems when
your system is making a single isolated decision,
uh, such as a classification decision or a regression decision.
Uh, and where that decision does not affect
feature inputs to the system and does not affect future, future decisions.
Uh, and so from this point of view,
uh, we don't need sequential decisions whenever we're, kind of,
in a very isolated black-box world,
uh, and actually in the real world, uh,
in many cases, our decisions are actually affecting the future or affecting,
uh, future aspects of the world.
So, uh, there are many different applications of sequential decision,
sequential decision making problems.
Uh, in many cases, in some applications,
people choose to ignore the dependence of, uh,
of future, of the current decision on,
on the future, uh,
which, kind of, makes a simplifying assumption.
Uh, but in many real world cases, there are,
um, there is this effect,
uh, of, of affecting the future.
And so for example, some very common
applications of reinforcement learning where you can't
afford to ignore this effect include things like robotics, uh,
include things like language and dialogue systems when you're interacting
with another agent or interacting with a human, for example.
Uh, in autonomous driving, uh,
decisions you make affect the,
the future observations that you make.
Uh, in business operations,
uh, in finance, uh,
these are all kind of in these sequential,
uh, in a sequential decision-making problem setting.
Uh, and really most kind of deployed machine learning systems that are
deployed in the real world and are interacting with humans,
are faced with a sequential decision-making problem.
Okay. Um, so this is, uh,
in practice, why this,
sort of, topic is important.
Uh, and also if you're interested in,
kind of, how humans, uh,
act in the world and how humans are intelligent in the world, uh,
these sorts of problems is also, kind of,
a key aspect of our own intelligence.
We also can reason about how our actions affect the future.
Okay. So reinforcement learning, uh,
or in general sequential decision making is pretty important.
Um, and so in this lecture,
what we're gonna talk about is first, uh,
what is reinforcement learning or what does multi-task learning look like
in the reinforcement learning context when you're making, uh, sequential decisions?
What does this look like, uh, in the formulation of policy gradients which is one,
uh, one form of reinforcement learning,
algorithm of one class of reinforcement learning algorithms?
What does this look like, uh,
in the, in the context of Q-learning?
So, uh, we'll give a,
a, like, a few slides of,
of review on Q-learning.
This should be a review for most of you because this,
this topic is covered in a,
a number of courses like CS 221,
CS 229, um, etc.
Although policy gradients is not always covered in those courses.
So we'll, we'll gi- give a little bit more of an in-depth overview of those.
Uh, and then finally we'll talk about approaches for multi-task Q-learning,
goal conditioning Q-learning, um, and algorithms that,
that significantly improve upon, kind of,
the naive approach to multi-task learning,
uh, in combination with reinforcement learning.
Okay. So first let's talk about, uh, the problem statement.
Uh, and we can do this by looking at an example.
So, so far we've been looking at things like object classification, regression,
these isolated, uh, problems where you need,
you make, um, predictions.
And, uh, in contrast,
you could think about something like object manipulation where we
have very much a sequential decision-making problem.
So you can view, uh, the,
the problem of object classification as a supervised learning problem
and the problem of object manipulation as a sequential decision-making problem.
Uh, and what are the differences between these two problems?
So in supervised learning,
so far we've assumed that we have iid data,
data that is independently and identically distributed according to some distribution.
Uh, whereas in sequential decision-making,
your action that you take effects the next state that you're in.
So the data that you're seeing is very much not iid.
Uh, second, so in supervised learning,
we have some typically assume some large dataset that's maybe
curated by humans to ensure that it has a distribution that you care about.
Uh, whereas in things like sequential decision-making,
you need to collect the data yourself in many cases.
Um, and it's also not clear what the labels are.
Uh, you aren't really,
you kind of need to figure out what this might be in a
different, in different applications.
And then lastly, in things like supervised learning,
you generally have a fairly well-defined notion of success which is
corresponds to some error or some prediction accuracy correspond,
uh, in relation to the, the labels.
Whereas in, uh, things like reinforcement learning,
success is a little bit more, uh, great.
Okay. So these are some of the, kind of,
biggest differences between, uh,
these two problem domains.
Uh, and so before we go into like what concretely the problem looks like,
let's look over some terminology and notation.
So, uh, somebody before will say that we have some neural
network that's gonna be making some predictions and in the classification setting, uh,
you might be looking at an image and then
classifying the class corresponding to that image.
Uh, corresponding to different,
uh, different types of animals for example.
Now, uh, in reinforcement learning, uh,
we're no longer gonna be making, uh, predictions like that.
We'll be instead, uh, using, uh, our policy.
This policy will be taking actions and the actions will affect the next state.
Um, so, all right.
There will be this feedback loop that goes from the action to, uh,
back to the observation, uh,
and our, our classes won't look something, won't look like this.
They might look more like this.
So we might, uh,
need to figure out if we should run away,
if we should ignore, if we should pet the tiger, etc.
Okay. So we need to make decisions, um,
always denoting the observation that the agent or the,
the system receives as input is denoting the action.
Uh, Pi is denoting the policy which is parameterized by Theta.
And typically, we assume that there is some underlying state of the world s. Uh,
and so in the fully observed setting,
we get to observe s,
uh, and in the partially observed setting,
we get to observe o.
Uh, what concretely is the difference between s and o?
Uh, one example of this is you may have,
uh, you may be, uh, trying to chase a,
uh, a hyena or something and, uh,
if you're given an image or something that would be an observation,
uh, whereas in contrast,
if you are given the pose of the,
uh, respective animals, then that would be the state.
You'll basically be able to fully observe, uh, the system,
under- underlying state of the system and the things that,
that matter for making decisions in the world.
Uh, and in particular, in partially observed settings,
uh, you might not just receive an image.
You may also have occlusions and the imagery can actually see, uh, part of the state.
Okay. So this is, kind of, the, the basic,
uh, kind of, terminology corresponding to reinforcement learning.
Uh, now one very basic approach to this,
sort of, sequential decision making
problem is to treat it as a supervised learning problem.
So what you could do is, you can say, "Okay,
I, I just want to, uh, perform.
I just want to imitate some expert for example."
So, uh, maybe you could collect a bunch of driving data, uh,
collect the observation that the person sees and collect
the action that they took in those states, uh,
put this into some big training data set, uh,
and then sample iid from this training data set during supervised learning to,
uh, to train your policy to predict actions from observations.
Okay. So we've already seen a little bit of imitation learning.
So there was a paper presentation, uh,
a week or two ago that was looking at,
uh, how we can apply meta-learning into things like imitation learning.
Uh, these approaches generally work,
uh, pretty well in some contexts.
For example if you have a lot of data, uh,
expert data of performing the right actions, uh,
then these, these systems can actually do something fairly reasonable.
Uh, the place where these kinds of systems tend to fail are when you have very long,
uh, very long horizon problems,
you have compounding errors, uh,
as, uh, basically as you make actions,
you'll start to move off of the manifold of the data and then your errors will,
will compound, um, until you're well off of the manifold of the training data.
Um, and also these systems don't reason about outcomes in any way.
Uh, they're just trying to mimic what the data is
doing rather than trying to accomplish some particular outcome.
Okay. So this is where reinforcement learning comes in.
Um, and for reinforcement learning,
we need some notion of what's called a reward function.
Uh, and this reward function should capture what states and actions are better or worse,
uh, for the system.
So this typ - typically takes in both a state and an action,
uh, and it tells us which states and actions are better.
For example, if we're driving,
we might have a very high reward, if we,
we look like this and have a low reward if we see something like this. Okay.
So the- um, in aggregate,
the states, the actions, and the rewards,
as well as the dynamics of the system define a Markov decision process,
because this is encapsulating the notion of a sequential decision-making problem.
[NOISE] Okay. Any, uh,
any questions up until here?
This, this should mostly be review for people.
Okay. Cool. So the goal of reinforcement learning,
uh, is typically to learn some policy that takes, uh, as input.
Uh, in this case, we'll look at the fully observed setting,
takes as input some state,
and makes predictions about auctions.
Uh, the goal is to learn, uh, the,
the policy- uh, the parameters of that policy.
So, uh, for in a deep reinforcement learning setting,
your policy will probably be parameterized as a neural network,
uh, where the states are being processed as input,
you're produ- uh, producing actions,
the actions are fed into the world, uh,
and then the world gives you the next state that's fed back into your policy.
Okay. Um, and so,
we can actually characterize a system as the graphical model here, uh,
where we have a policy that's taking in- in the- in this case,
in the partially observed setting a policy that's taking
the observation and producing an action, uh,
the dynamics are, uh,
taking in the current state and the current action
and producing a distribution over the next state.
Um, and one thing that's pretty important is that
this dynamics function is independent of the previous state.
All right. This is what's known as the Markov property.
Which is that basically, uh,
kind of the definition of a state in a Markov decision process is that,
uh, you can fully define, uh,
the reward function and the dynamics, uh, from the,
the information in that state variable independent of previous states.
Uh, and the way,
the way- you see guys if you look at this dynamics distribution here,
this only depends on S_t and A_t and doesn't depend on S_t minus 1.
Okay. And then, [NOISE]
the goal of reinforcement learning and typically kind of the way that we can
formulate a concrete objective here is that we want to be able to
maximize the expected reward,
uh, under our policy.
And in the infinite horizon case,
we can imagine the stationary distribution over states and actions
arising from our policy and maximize the reward function, uh,
under that stationary distribution,
uh, and in the finite horizon case,
we might have some horizon capital T and you wanna maximize the rewards of the states
and actions visited by our policy- when rolling out our policy. Yeah.
So are the actions here taken before the observation or after the observation?
So like is any one taken just before we
observe Observation 1 [NOISE] or is it taken just after?
It's taken just after.
So you observe Observation 1 and then,
uh, and that's shown,
that's shown right here and then
your policy predi- predicts an action from that observation,
and that action is then fed- uh,
then the kind of the world that actually, is actually executed in the real-world and
that produces the next state which produces, uh, the next observation.
It seems like you- uh, the states
produced- it seems like you'd wanna use the state to make
your action but it sounds like you're saying
the model doesn't try and convert it into a state first. You're saying.
So the- I guess there's a couple of
different versions of how you might handle the partial observability,
maybe one point of confusion here is th-
the arrowheads on these arrows are very hard to see and there,
there is an arrow going from state to
observation and not an arrow going from observation to state.
Um, you- what your policy could do is it could try to form some, like,
estimate of the current state from your observation,
and then, like, do some sort of inference
and then pass that to your policy to predict the next option.
You said here is the real state of the world rather than
an embedded state from [NOISE] [OVERLAPPING]
Yeah. Exactly. Yeah. Yeah so the state is the, the real state.
Okay. Cool. So [NOISE]
now that we've talked about kind of the reinforcement learning problem,
um, what is the reinforcing learning task?
Uh, and we're gonna define this for the sake of,
uh, thinking about the multitask learning setting.
[NOISE] So remember, in supervised learning,
we defined a task, uh, as this,
as corresponding to the data distribution, or the,
the data generating distributions,
p of X and p of Y given X,
as well as some loss function.
[NOISE] Uh, in reinforcement learning, uh,
our task will be defined as basically just a Markov decision process.
So, uh, the task will be defined by some state space as some action space a, uh,
some initial state distribution p of S_1, uh,
your dynamics function, uh,
S prime given s and a,
and the reward function.
So if you look at, uh, the, uh,
kind of- if you compare this to the supervised learning setting, uh,
the initial state distribution,
the dynamics are, basically,
are the same as the data generating distributions.
The reward function, uh,
corresponds to the loss function, um,
and the state and action space are just kind of telling you, uh,
what is the general set that your states and actions lie within.
All right. So this is just a Markov decision process.
Um, and I guess one thing worth mentioning here is that,
if these- if the different MDPs are different tasks,
then this is much more than just the semantic meaning of a task.
Uh, cause different tasks could have
the same exact reward function but have different action spaces,
for example, or have different dynamics.
Um, and so we use the term task loosely to describe
these different Markov decision processes. All right.
So what are some examples of what different task distributions might look like,
or settings where we might actually want to
apply multitask learning in the reinforcement learning setting?
Uh, so one example, uh,
that we saw earlier actually is a supervised learning problem but really
is a sequential decision making problem is a recommendation system,
where you want to be able to recommend videos,
or recommend treatments, or recommend other things to a particular person.
And you could imagine different people as being different tasks in the system.
Different people have different preferences,
have different- uh, operate in different ways.
Uh, and so, if you kind of view this personalized recommendation, uh,
problem as a multi-task learning problem,
then you can view it as a setting where, uh,
the dynamics and the reward function vary across tasks.
Um, the dynamics correspond to how that person will react to
a particular action that you take and the reward function corresponds to whether,
to whether or not what you do- uh, whether,
whether or not you recommend something to them
reduce- re- results in a state that is good.
In some context, the initial state distribution may also vary for different people,
it depends on the, uh,
particular, like how you formulate your problem.
Okay. So this is one example.
Um, another example where reinforcement learning has been applied, uh,
has been in character animation,
uh, and you can imagine,
uh, trying to animate different, uh,
characters in computer graphics across different maneuvers, for example.
Uh, so there has been some work applied in reinforcement learning to learning- uh,
for learning maneuvers like this.
Uh, and in this case,
if you treat this as a multitask learning problem,
different tasks would have different reward functions, um,
in the setting but the dynamics would be the same,
initial state distribution would be the same,
as well as the state and action space.
Okay. Uh, and then,
another setting where, uh,
reinforcement learning has been applied is for, uh, dressing, uh,
putting on clothes, and this is
actually a really challenging problem in computer graphics because of the,
the deformable objects and also has applications in assistive robotics, for example.
Uh, and in this setting,
uh, things like the initial state distribution, what,
what is the garment- what state is the garment in before you put it on,
as well as the dynamics are going to vary across
tasks but the underlying reward function might be the same,
uh, such as putting the clothes on the person.
Uh, and then one last example of,
of a task distribution might be if we wanna be able to do
reinforcement learning across different robotic platforms.
Uh, you may still want to do the same task across these platforms like,
like having them learn how to grasp things, uh,
but in this case, the state space and the action space would be- would vary across tasks.
The initial state distribution and the dynamics would also
vary across tasks as the robots have different,
uh, degrees of freedom and react to actions in different ways,
but the underlying reward function could be the same.
Of course, if you want the robots to do different things,
then the reward function would be different.
Any questions on these examples?
Or any questions about other examples?
Okay. Cool. So this is a reinforcement learning task.
Um, now, one alternative way to view multitask reinforcement learning,
uh, is as follows.
So we'll typically have some sort of task identifier that's part of the state,
and this is required to make it a fully observable setting or a fully observable MDP.
And the notation I'm using here is that S-bar is gonna
denote the original state space or the original state,
and Z_i is going to denote the task identifier as in previous lectures.
Now, if you take this view, then interestingly,
what you can take is you can look at- uh,
you can basically fold, uh,
looking at the task identifier and determining, um,
determining the dynamics and determining the reward
into a single dynamics function and a single reward function.
Uh, and then, basically, view, uh,
your set of tasks as just a single,
single task standard Markov decision process,
where the state space and the action space are just the union of
the state spaces and action spaces in the original tasks,
uh, that the initial state distribution just corresponds to
a mixture distribution over your initial state distributions for each of those tasks.
The dynamics and the reward function are folded into the- um, uh, ah,
they are just a single dynamics and single reward function that
takes as input the task identifier and produces either the next state or the reward.
So basically, you can essentially view- you can
still basically apply standard reinforcement learning
algorithms- standard single task reinforcement learning algorithms to
the multitask problem with this view on multitask RL.
Questions on this? [NOISE]
So basically, multi-task RLs,
the same as before,
is the same as the single task reinforcement problem
except we have a task identifier that's part of the state.
Uh, this task identifier could be something like
a one-hot task ID like we had described in the supervised learning context,
it could be a language description of the task, uh,
it could be a desired goal state that you want to reach, uh,
and this would be what's known as goal-conditioned reinforcement learning where you
condition it on a particular state that you want to be able to reach in the future.
Uh, and what is the RL function will it could be just the same as
before where it takes as input the task id and outputs
the reward function corresponding to that task for that state.
Uh, or for things like goal-conditioned reinforcement learning,
it can correspond to simply the negative distance between
your current state or your current original state and the goal state.
Uh, and some examples of distance functions might be Euclidean, uh, distance,
it could be Euclidean distance in some latent space, uh,
it can also be a sparse 0/1 or a function that corresponds,
that is 1 when s bar equals sg and 0 when they're not equal.
Okay, so you might ask, okay, if,
if this is just a standard Markov decision process,
why not just apply standard reinforcement learning algorithms?
And as I mentioned you can and this will work.
Well, it will be more challenging than
the individual single tasks because you will have
a wider distribution of things in general, uh,
but in general you can apply these,
these same types of algorithms,
but you can often do better, uh,
and we'll discuss that a bit in this lecture.
[NOISE] Okay.
Great. Any questions on how it can be formulated as a single task RL problem? Yeah.
[inaudible].
Yeah. So I will look conditioned RL is
a special case of multi-task reinforcement learning where, um,
the task descriptor corresponds to the goal state
and the tasks correspond to goal reaching tasks.
Okay. So let's get into some algorithms.
So, ah, the first class of, I guess,
these are, I'll start off by saying that it kind of goes,
we can look at broadly at kind of the,
the anatomy and like the class of reinforcement learning algorithms
and how these approaches relate to each other and then I'll talk a bit about,
uh, two classes of algorithms.
So we can generally review
reinforcement learning algorithms in the following flow graph
where we first are generating samples, uh,
in our environment, this is just running the policy forward typically,
then we fit some model to estimate the return,
ah, and then we use that model to improve the policy.
And then different algorithms typically correspond
to just differences in this green box and in this blue box.
So for example, one, ah,
one example of fitting a model might be just fitting something to the return,
estimating the empirical return such as using the Monte-Carlo policy gradient.
Uh, another example of estimating the return might be to try to fit a Q function, ah,
using for example dynamic programming algorithms, uh,
and another example of fitting a model would be to, uh,
estimate a function that models the dynamics.
Uh, and once we have any of these models we can then,
uh, for example apply the policy gradient to our policy parameters,
we can improve the policy by taking the max over Q values for our current Q-function,
uh, or in the case of model based algorithms we can optimize
a policy by for example back propagating through a model into our policy.
Uh, this is kind of a, a general outlook on
reinforcement learning algorithms where we have
different choices for fitting a model to estimate the return,
different choices for improving the policy,
we also have different choices for how we generate samples although those are,
uh, generally, that decision is generally orthogonal to the choice of algorithm.
And in this lecture, we'll focus on
model-free reinforcement learning methods such as
policy gradient methods and Q-learning methods,
uh, and in fact we'll, we'll stick with these algorithms for the next,
uh, two weeks about.
Uh, and then on, uh,
the lecture on November 6th we'll focus on
model-based RL methods and how they can be applied to the multi-task learning.
Okay. So let's start with policy gradients.
So, uh, this is our objective,
uh, in reinforcement learning,
so we want to be able to sample trajectories from our policy,
uh, and estimate the return.
So we'll refer to this objective as J of Theta and,
ah, this is just rewriting J of Theta.
You can view this or you can estimate this as,
uh, rolling out and trajectories,
uh, for the example shown here, uh,
and estimating the rewa- computing the reward for each of those trajectories.
So maybe the first trajectory has a high reward,
the middle trajectory has a medium reward and the,
the last trajectory has a bad reward.
Uh, and so this first sum is the sum over the samples
from our policy and the second sum is a sum over time.
So this is the way that we can, uh,
kind of estimate the,
the expectation shown on the left.
Now, what we could think about doing this,
can we differentiate through this objective directly into our policy?
Uh, so if our objective is
the expected reward and we can estimate this with the reward of a trajectory,
I'm just using shorthand to denote that as a sum over
time of the reward function of the individual states.
Uh, you can view this as, uh,
this expectation as an integral over Pi Theta, uh,
because the expectation is with respect to Pi Theta of r tau.
All right. So this is our objective, uh,
and if you want to be able to compute the gradient of
this objective with respect to our policy parameters,
uh, we get something like this.
So we can move the gradient, uh,
inside the integral because it's a linear operation and, uh,
then you basically have the,
the integral of the gradient of the policy [NOISE]
times reward function integrated over trajectories.
Okay. So this is the gradient.
Uh, now how do we actually go about evaluating this gradient?
So we do- we don't want to have to, uh,
integrate over all possible trajectories,
so what we're gonna do is we're gonna use this very convenient identity,
uh, which is known as the likelihood ratio trick.
And in particular what this identity shows is that,
oops, which is that if we, uh,
are looking at the,
the policy parameter, the, sorry,
the policy probability for a trajectory times the gradient of the log of the policy.
This is equal to, uh, the,
basically we just differentiate to the log, uh,
which is equal to the policy times the gradient of Pi divided by Pi.
And of course the two pis are on the top and on the bottom can cancel and this is
just equal to the gradient of Pi or the gradient of,
uh, of the policy with respect to the policy, the policy parameters.
Okay. So we have this very convenient, uh, identity,
and we can use it to,
uh, expand out this equation.
So, uh, we can basically replace this,
this term with the term on the left to get an integral that looks like
this and very conveniently this integral now looks a lot like an expectation.
So, uh, this is an expectation under Pi Theta, uh,
and so we can simply ,uh,
evaluate the gradient or estimate the gradient by, uh,
taking an expectation over a trajectory sampled from our policy, uh,
and using those samples to evaluate the gradient of the log,
probability of our policy weighted by the reward of that trajectory.
Okay. So I guess to kind of recap what we did there,
this first trajectory we don't want to have to
integrate over all possible trajectories and so instead we're
able to transform that into an expectation over
trajectories drawn from the distribution of our policy.
Okay. So when- the, once you have this gradient we can, uh, we can use it,
to actually differentiate in, compute this gradient
and actually apply that gradient to our policy parameters.
Okay. So this is all with respect to trajectories,
uh, one thing that is, uh,
one thing that's important to do is actually break this down into states and actions,
so we're denoting the, uh,
Pi of Tau as, uh,
Pi of the full trajectory which can be broken down into,
uh, the initial state,
uh, density times a product over time of the,
uh, policy probability and the dynamics probability.
So this is basically the probability trajectory under our policy.
Uh, if we take the log of both sides of the equation,
we get log, uh, Pi of Theta of Tau, uh,
and then just change the,
the products into sums using the log and we can basically,
uh, plug in the right-hand side of the equation
into the equation on the left into our form for the gradient.
Now unfortunately, if we just apply this, uh, naively,
we would have a term that corresponds to
the probability of our state or next state given our state and action,
uh, and we don't know that probability value.
Uh, but we can- one thing that you know is that because this,
uh, this is a gradient with respect to Theta,
these terms don't depend on Theta,
they're constant with respect to Theta and so the gradient
of Theta with respect to those terms is 0 and then we get,
uh, so we then get the, the kind of,
the final gradient which corresponds to this term right here.
So, uh, this is basically, uh,
log probability of Pi of a given s.
This is something that we can evaluate because
our policy will output a distribution over a,
uh, conditioned on s,
and this right term is just the reward function given the state and the action.
All right.
So this is kind of the vanilla policy gradient,
uh, and this is something that we can very clearly evaluate.
Um, and so, what this looks like, uh, as an algorithm, uh,
is basically we can estimate,
uh, basically we can run- rule out our policy to get trajectories.
We can then estimate the policy gradient by averaging over those trajectories, uh,
over time of the, uh,
the lo- the gradient- grad log Pi times the reward function,
uh, and then apply the gradient,
uh, to our policy parameters.
So if we go back to our diagram, uh,
collecting data corresponds to the orange box, uh,
evaluating the return corresponds to the green box and actually,
using that to improve the policy,
uh, in the last step corresponds to the blue box.
[NOISE] And then, what this looks like is an algorithm which is called,
uh, the reinforce algorithm, er,
is explicitly sampling trajectories from your policy and then computing the gradient, uh,
using those trajectories, and then using
that estimated gradient to update your policy parameters.
And then you can repeat this step to iteratively improve your policy.
[NOISE] Okay. So this is the algorithm.
Um, how does this compare to something like
imitation learning like maximum likelihood of expert actions?
So, uh, if you look at the policy gradient, um,
and you instead also look at kind of the,
the imitation learning approach where you do, uh,
supervised learning with respect to actions, uh,
the maximum likelihood objective looks pretty similar to the,
the gradient of the policy, um,
[NOISE] the, the policy gradient form.
And in particular, the difference is that, uh,
the- is just the reward term on the right.
So basically policy gradient will correspond to taking,
um, maximizing the probability of actions that have high reward.
And if they have low reward, then, er,
you- you'll have, uh,
you'll try to maximize it less essentially.
Okay. Now, one of the really nice things about this is that, uh,
because we- it's just basically a gradient descent algorithm,
it's very easy to apply multitask learning algorithms to it.
Uh, we can basically be- it corresponds, uh,
very similarly to maximum likelihood, likelihood problems.
So all of the things that we learned about in maximum likelihood supervised learning
can be applied to the reinforcement learning context.
Okay. So this is nice.
Um, let's go to one more slide kind of on,
on intuitively what this algorithm is doing.
Um, so if we look at the, er,
kind of the form of the, the gradient, er,
which corresponds to the kind of gradient log Pi of a given S, uh,
and look at maximum likelihood,
um, we can say that,
okay, we have trajectories.
If we do maximum likelihood imitation learning,
we're just trying to imitate,
uh, the best trajectories,
uh, whereas in policy gradient what we're trying to do is we have some,
some distribution over these trajectories.
And then we're going to try to, uh,
increase the probability of the actions that had a high reward, uh,
and place less probability mass on the actions that had low reward.
Uh, and so as a result, we'll basically just be making the good stuff,
more likely making the stuff that gets bad reward less likely,
um, and kind of formalizing this notion of trial and error.
You, you try a few things.
You do more of the good stuff and less of the bad stuff.
[NOISE] Okay.
[NOISE] So that's policy gradients.
Um, it's pretty easy to combine with things like multitask learning.
Uh, it's also pretty to com- pretty easy to combine with things like meta-learning.
Uh, so the meta-learning algorithms that we learned such as
MAML and black-box meta-learning algorithms,
uh, just assume that you can get some gradient,
uh, of your objective.
And so we can readily apply these to, uh,
readily apply, uh, these algorithms to- in combination with policy gradient algorithms.
Uh, so, for example, uh,
here's a very- toy example of MAML with policy gradients where there's just two tasks.
One of the tasks is running forward and one of the tasks is running backwards.
Uh, so we're not evaluating generalization in, in any way.
We're just gonna look at whether or not it can learn to
adapt its policy with a single gradient step for one of these two tasks.
Um, what we see is first at the end of meta-learning,
basically at this point right here at the end of meta-learning,
but before taking a [NOISE] gradient step to
a- one of the tasks we get a policy that looks like this.
Uh, it's running in place, essentially, like,
ready to, to run in either of the two directions.
[NOISE] And if we then, uh,
take one gradient step with respect to the task of
running backward- with the reward function of running backward,
uh, we get a policy that looks like this.
And if we take
a single policy gradient step with respect to the reward function of running forward,
we get a policy that looks like this.
[NOISE] Um, and so,
I guess, one of the interesting things that this shows is that, uh,
there does exist a representation under which
reinforcement learning is very fast and very efficient,
um, at least in the context of a few tasks.
Uh, and I guess one other thing worth mentioning here is that
the policy gradient was evaluated with respect to 20 trajectories,
uh, from Pi Theta.
Uh, so basically 20 trajectories similar to the video shown on the previous slide.
Okay. So this is pretty straightforward.
Um, what about black-box methods?
Uh, so we can also apply policy gradient to black-box methods.
What this corresponds to is using, um,
some LSTM policy, some policy with memory or recurrence, uh,
and training that policy with the policy gradient algorithm or,
or, uh, a variant of
the policy gradient algorithm that I mentioned on the previous slide.
Uh, so, for example,
in, uh, this, this previous paper that was actually, uh,
presented, uh, a few weeks ago in class, uh,
one of the experiments that they looked at was learning to visually navigate a maze.
Uh, and so what they did is they trained the algorithm on 1,000 different small
mazes and then evaluated the algorithm's ability to learn how to solve new mazes,
uh, including both small mazes and large mazes.
Uh, and so we can look, uh, at what it does.
So here, this is first showing,
uh, after meta-learning, uh,
the beginning of rolling out the recurrent policy.
Uh, so, in this case, it doesn't know the task and it needs to navigate the maze.
Uh, and the left is showing the, uh,
the agent's point of view and the right is showing the maze.
And then after it gets this experience,
it then is able to learn how to solve the maze with basically just a single trajectory.
Um, so at first,
navigate around that maze to explore and then at the end of that episode,
[NOISE] the memory of the, uh,
of the architecture is not reset,
and it- you keep on rolling forward that,
that memory- that black-box architecture and it can figure out how to,
uh, from there, based on what's stored in memory,
how to solve the task.
[NOISE] Uh, and they also looked at bigger mazes.
So here's an example of it navigating through a bigger maze.
At the beginning, it's just exploring.
It needs to figure out how to solve the task.
Uh, so it explores,
um, different parts of the maze.
This is one of the,
the, um, I guess,
both of these examples are successful examples.
There's also failure cases.
Um, so here's one.
In, in this case, after it sees a single trajectory,
it's able to very quickly navigate to the goal position. Okay. Yeah.
[BACKGROUND].
Yeah. So for MAML,
the number of, um, the,
the inner loop corresponded to 20 trajectories and one grad- one policy gradient step.
In this example, the inner loop corresponds to basically like two trajectories,
um, where you can basically see the trajectories on the,
um, on the thing.
So, uh, here's the- this is the first trajectory shown here.
And then the second trajectory is when it actually, um,
solves the task well. Does that answer your question?
[BACKGROUND].
This is after meta-learning. Yes. Yeah. So this is the inner loop.
[BACKGROUND].
Yeah. And then the outer loop is trained, uh, a lot,
uh, for, um, across the different tasks.
So this is trained across 1,000 mazes during the meta-training process.
Uh, and then- yeah. And it practices a lot for those, uh, mazes. Yeah.
[BACKGROUND].
Yeah. So in this case, it's just,
it's just gets this as input.
It doesn't get the, the layout of the maze.
Uh, and in the case of the ant example,
it just receives like joint angles, uh,
and other state information. Yeah.
[BACKGROUND]
So in both of these examples,
after this end of the first trajectory,
just- it's- it is reset to the initial position. Does that answer your question?
[BACKGROUND]
[NOISE] Um, so I think that after it reaches the end,
here it is then reset to this position again.
We can watch it again one more time if you want to verify it.
So he goes to the goal,
and then it's respawned right there again. Yeah.
[inaudible].
Yeah that's a good question.
Um, so I guess as of like last year,
I think the- these sorts of maze tasks are probably
the most complicated task that I've seen these algorithms do.
This year I've seen
more complex tasks that these algorithms have been able to learn quickly.
Um, ranging from being able to adapt to- learn how to run on
an entirely new agent or like simulated robot to, um, solving,
uh, like actually settings where the tasks themselves are partially observable,
not just the, um, this is,
this is also partially observable,
but partially observable to a greater degree I guess.
Um, and then I've also seen tasks where, um,
it's like there are different manipulation tasks and it
can generalize to an entirely new manipulation task,
like robotic manipulation task.
But those are, those all like very,
very recent works, but yeah.
There are more, more, more better things to come. Yeah.
[inaudible].
Yeah, so this is a good question.
Um, we'll- so we'll cover meta-reinforcement learning in more detail,
um, next week on Wednesday when there's going to be a guest lecture by Kate.
Er, but one thing that I'll say here is I guess first,
one of the things in the reinforcement learning setting we
talked about how MAML is very expressive,
um, in the supervised learning setting.
Um, in the reinforcement learning setting,
it's actually not very expressive because,
um, because of the policy gradient.
Basically, if the reward function is 0 for all of
your trajectories then your gradient will always be 0.
And so even if it gets lots of rich experience about the environment with zero reward,
it can't actually incorporate that experience to update the policy.
And this was just one example,
there's other examples where the policy gradient isn't very informative.
And so as a result, MAML with policy gradients isn't actually very expressive and has,
um, well is, is,
is yeah is not as good.
Um, in general, applying
these algorithms to the reinforcement learning setting
it's pretty easy to combine with policy gradients.
Combining them with methods like Q-learning and
actor-critic algorithms is a lot more challenging,
and Kate will talk about that certainly a lot during her lecture,
um, and some of the challenges that come up there.
Um, the biggest thing is that those algorithms aren't,
uh, a gradient-based algorithm there are dynamic programming algorithms.
So it's hard to combine these things.
Yeah, yeah.
Er, are this kind of algorithms will have
things like curiosity that can be expand on the existing algorithms?
Yeah. So curiosity based approach and other exploration methods in general,
can certainly be combined like that's
just a kind of an objective and you could use that objective as one of your tasks.
You can augment all of your tasks without objective,
or you could imagine trying to learn exploration strategies like learn curiosity,
like learn different forms of curiosity that are particularly
effective for a class of tasks or class of environments.
Um, and Kate will talk about kind of
learning exploration strategies in her lecture next week. Yeah.
[inaudible] can you use advantage estimation with value?
Advantaged estimation. You mean, uh,
GAE Generalized Advantage Estimation.
[inaudible].
[NOISE] yeah so, uh,
the- I guess there's different-there's different ways.
So I guess to,
to explain to other people kind of what the question is.
So one of the challenges with policy gradients is that,
uh, the gradient estimate that it gives you is high variance.
And one thing that people typically do, uh, to mitigate to,
to reduce the variance of this is to use what's called a baseline,
um, which corresponds to some- which basically
corresponds to something that's subtracted from the reward term here.
Er, is gives you an unbiased estimate of this gradient,
but that has lower variance.
Um, and there are different techniques for estimating that baseline and one of them
corresponds to things like generalized advantage estimation and other things.
Um, in the original uh,
implementation of the MAML algorithm,
we used a Monte Carlo estimator for the baseline,
uh, rather than a bootstrapped estimator.
I think applying a bootstrapped estimator would
be like applying MAML to that would be a bit tricky.
You could of course always do it from scratch on, on your batch of data,
but applying MAML to it is little bit tricky because- just because of
how- because bootstrapping isn't a gradient based algorithm,
it's a dynamic programming algorithm.
[inaudible].
Yeah, it's hard. Okay. So some of the pros of policy gradients,
um, to recap is that it's very simple.
I gives you-kind of just gives you a gradient of your policy which is very nice.
Um, it's also very easy to combine with
existing multitask algorithms and
meta-learning algorithms as we saw in the last couple of slides.
Um, the downsides is that first it produces a high variance gradient,
um, and this can be mitigated with baselines.
Um, and baselines are basically used by all algorithms in practice.
I don't have time to cover them, um,
in this lecture but uh,
feel free to come to office hours if you're interested in learning more.
It can also be mitigated with trust regions,
um, with which people have also used uh,
and both- both MAML and the black-box methods we're using
both baselines and trust regions in the optimization to,
um, make things more stable and, uh, more effective.
The other downside of policy gradient algorithms is that it requires on policy data.
Um, and in particular the way that
you can see this as you can see that this expectation is with respect,
respect to pi Theta, and pi Theta is your current policy.
So in order to improve your policy,
you need data from your current policy.
[NOISE] Um, and this is really important because this means
that you can't reuse any data from
your previous policies to estimate- to try and improve your policy.
Um, it also means you can't reuse data from other tasks or from,
from, from other things basically.
Uh, and this is-this is really challenging.
As a result these algorithms tend to be less sample efficient, than, um,
algorithms that are able to reuse data, um,
from previous policies, uh,
from, from other experience etc.
Things like importance weighting can help with this.
Uh, so you can basically add,
add a weight that, um,
corresponds to the ratio between the policy- between your current policy,
and the policy that you collected data with.
Um, but these importance weights also
tend to give you high variance especially when those two policies are very different.
Okay. Cool. So now that we've talked about policy gradients,
um, let's talk about value-based reinforcement learning.
Ah, and in particular, the, ah,
the benefit of value-based RL is that, ah,
first they, um, they tend to be lower variance,
ah, by introducing some amount of bias and they, um,
can use off-policy data which is the,
the bigger- which is like a really, ah,
important thing if you care about reusing data and being sample efficient.
Okay. Um, so for your very brief overview of these algorithms for those of you who,
ah, are a little bit rusty so a value function,
ah- first let's go over some definitions.
So a value function corresponds to the total reward that you will
achieve starting from state S and following some policy pi.
Ah, so this is the function of both the policy and your current state.
And a Q function,
oh in- and this kinda captures how good is a state basically,
how valuable is that state.
And a Q function corresponds to the same thing as a value function but, ah,
the total reward starting from state S,
taking action A and from there, following pi.
Ah, so the A that passed as input is a parameter and it does not depend on the policy pi.
Ah, and this is basically telling you how good is a state action pair.
Ah, both of these things are very closely related so as I alluded to,
the value function corresponds to, ah,
an expectation over action- the value function of
our current policy corresponds to expectation under a policy
of Q of the state as input and the pol- the action sample from your policy.
And one other thing that's really nice about your Q function is that,
if you know the Q function for your current policy,
you can use it to improve your policy.
Ah, so for example, ah,
one very naive way to see this is that if you just set, ah,
the probability of taking an action for your current state to
one for every action that is the max of,
ah, the arg max of the Q-value, ah,
this is just gonna increase the probability of taking actions that have maximum Q values
then the new policy resulting from this will be at least as good as the old policy,
ah, and typically better.
Okay. Um, so the goal value-based RL is to- to learn these um,
to learn at least the Q function.
Ah, and then use that Q function to perform the task or to perform,
um, in order to learn a policy.
Ah, and one kind of critical identity that's important for these types of algorithms,
is noting that for the optimal policy,
ah, we have this equality that is satisfied.
So we know that the Q function for the optimal policy is equal to, ah,
the expectation of states visited under the dynamics of the reward function plus, ah,
Gamma times the max of actions of the next Q function,
where Gamma here is representing some discount factor.
Ah, the way that you can see this, ah, is that,
ah, basically if you take a reward and then, ah, if,
if at the current timestep you observe some reward and then, ah, you know,
kind of the- the- kind of the reward in the future for the best action.
Um, that's gonna equal the best- ah,
the best, ah, value from your current state and current action.
Ah, and this is what's known as the Bellman equation.
Okay. So we can use this Bellman equation to, ah, learn a Q function.
So, um, what this looks like is, ah,
this is one example of a- of an algorithm that's called fitted Q-iteration.
Um, what this looks like is you first, ah,
collect a dataset using some policy,
ah, and the hyper parameters corresponding to this sort of the dataset size and the,
the policy that you use for data collection.
Ah, you can then set, ah,
the reward plus the max Q as, ah,
some target label and then improve your policy,
ah, to try to match those target values.
So you're essentially trying to, ah,
run a dynamic programming algorithm that leads
to the Bellman equation holding for your Q function.
Ah, and so for example if your Q function is represented,
ah, has parameters phi, ah,
that might be some neural network that takes this input,
the state and action and outputs the Q value,
a scalar value for that state and action.
Um, another way to parameterize this indiscreet action case is,
if you just pass in the state, ah,
and then output the Q value for each of the actions correspond- for- for that state.
Ah, that's, that's often used in practice.
Ah, and the other hyperparameter values that uh,
that you have in this algorithm correspond to, ah,
the number of gradient steps you take, ah,
and the number of iterations that you perform this for.
So in practice, you're gonna be iterating between collecting your dataset,
computing your, your target values,
trying to fit your Q function to those target values and then iteratively,
ah, fitting your Q function and also recollecting your dataset.
Okay. And then the result of this procedure is that you get- you can get a policy
by simply taking the argmax of your Q function for a given state.
So take the actions and the current state that
maximize your future reward or your future returns.
Okay. So this is a,
er, a, ah, Q-learning style, style algorithm.
Um, some important notes here.
Ah, first, we can reuse data from previous policies.
So this doesn't make any assumptions about the underlying, ah, algorithm.
There's no expectations with respect to pi Theta,
um, in, in any of this.
Ah, and so as a result, it's what's called an off policy
algorithm because it can use off policy data,
ah, and as a result,
you can use, ah, replay buffers.
So you can store data.
It can aggregate data across all of your experience into a single replay buffer and, ah,
when computing this update,
you can load from your replay buffer,
ah, any kind of- any sorts of data.
And this allows you to, ah, one,
kinda keep on aggregating data and reusing that data and two,
uh, get more data that is, ah, decorrelated.
So if you just, ah, kind of get some data online and make updates,
you'll have a very correlated data which will result in uh, poor performance.
Okay. Another thing to note, ah,
as I mentioned before is that this is not a gradient descent algorithm.
This is a dynamic programming algorithm.
You can see that by, ah,
the fact that this, um,
this update affects the targets at the next value right here.
Ah, and as a result,
it's tricky to kind of combine this approach with things like MAML,
um, and, and even black-box methods in practice.
Um, but it is relatively easy to
combine with algorithms like multitask learn- learning algorithms
and goal condition learning algorithms by simply conditioning your Q function
or your policy on the task identifier or the goal.
Okay. Um, so let's talk about Multi-task Q-learning.
Any questions on kind of the setup for Q-learning before I move on?
Okay. So, um, for Multi-task RL we can just,
kind of take our policy and condition it on
some task identifier likewise for our Q-function.
Um, and in each of these cases,
I'm using- again using S bar to denote
the kind of original state space.
where the, the kind of- the,
the new augmented state corresponds to the original state space and the task identifier.
In analogous to multi-task learning,
we can use a lot of things that we've learned about before,
like stratified sampling, like hard and soft weight sharing,
other architectural changes, etc.
So this is quite nice. Uh, we can reuse
the things that we've learned across supervised learning and reinforcement learning,
uh, now, what's different.
So there are some things that are different
that- about reinforcement learning that we talked about at
the very beginning of the lecture that affects the algorithm choices that we make.
So the first thing that's different is that
the data distribution is controlled by the agent,
it's no longer just given to us.
Uh, and so one of the things we can think about is can we reuse- can we think about how,
how should we explore data in a way that's effective for
multiple tasks and also can we think
about not just weight sharing but also data sharing
across tasks and how should when we collect a batch of data,
how should we choose to share that data across the tasks?
And second, you may also know
what aspects of the MDP are changing within your task distribution,
and if you know this you can actually leverage this knowledge in certain ways in
your algorithm choice by making assumptions about whether
or not one aspect of the MDP is going to be changing across tasks.
Okay. Um, so let's think about an example
for thinking about how we can- how we might wanna go
about sharing data or leveraging this sort of information.
Um, so say we are playing hockey, uh,
and we have some, uh,
some of our teammates and some of our opponents,
uh, and we may wanna be trying to practice different tasks.
Uh, so we may want to be able to practice passing the puck from,
uh, from yourself to your teammate,
and you may also wanna be able to practice shooting goals.
Now, if you're considering this multitask learning problem, uh,
what if during practice you accidentally perform a very good pass to your,
uh, to your teammate when you're trying to shoot a goal?
Well, this happens, uh, it makes sense, of course,
the story or experience is normal but you can also take that experience and say,
"Well, okay even though I was trying to shoot a goal,
I don't need to just use that for shooting a goal.
I could also say, okay,
in hindsight if I was doing task two, that would've been great.
I would've gotten a ve- a kind of a very high reward for that task," right?
And so you can relabel that experience with the task to identifier,
and with the reward function for that task and store that data for that task.
Okay, so this is something that's known as hindsight relabeling which is that, in hindsight,
you can kind of take some experience that you collected with the intention of one task,
relabel it for another task and use that in learning the other task.
Okay, it's also referred to sometimes as hindsight experience replay as well or HER.
Okay. So what does this actually formally look like?
So we can look- imagine a goal condition RL setting and first we're collecting
some data using some policy as in
the kind of standard of policy reinforcement learning setting.
Then we of course store the data in a replay buffer.
Uh, and then we perform hindsight relabeling.
So what we can do is we can take,
uh- we can relabel the experience that we just collected but take
the last state that we actually reached and
imagine that that was actually the goal for that task.
So we can replace the, um,
replace the goal that you're trying to achieve in that task with
the goal that you actually achieved and replace
the reward function with the distance between
the current state and that new hindsight goal,
and then once you have this relabeled experience you can
then store that in your replay buffer as well.
Uh, and then, of course,
update your policy using your replay buffer and repeat.
Okay. Cool. So what about other relabeling strategies?
So what this relabeling strategy used the last goal as input,
as a, as a thing that we're-
the last state as the thing that we're gonna re- relabel as the goal.
You can also use really any state from the trajectory,
uh, and these are also states that were reached.
Um, in general, you could,
you could choose, uh,
any potential state, uh,
to relabel with although in practice one of the things that's really nice about, uh,
relabeling with the state that you actually reached is that it can alleviate,
uh, it can alleviate a lot of the,
um, a lot of the exploitation challenges.
So, uh, if you're exploring for- in
the context of one task versus in the context of many tasks,
if you accidentally solve one task when trying to perform the other tasks then
you've already- then you've solved exploration problem for that task.
Uh, this can also kind of bootstrap the kind
of- it allows you to kind of bootstrap the learning process.
Okay. Any questions on how this works?
So we can generalize this also to the multitask RL setting,
um, and this is kind of similar to the setting that we showed,
uh, in the example.
So the way this looks like is we use- uh, in this case,
we just have a- we collect data, in this case,
the data core has a task identifier rather than a goal state.
Um, sort of that data, we relabel by,
um, for- by selecting some task J.
Replacing the task identifier with the task identifier for task J and
then replacing the reward function with the rewards of,
sorry, that should be- the negative shouldn't be there.
But bas- replacing it with a reward function for task J of the corresponding state,
and then storing the relabeled data,
updating the policy and repeating.
Um, another question that comes up here similar to the last slide
is what tasks should we choose to relabel with?
Um, you choose randomly, uh,
but one good choice in terms of exploration is you could
choose tasks in which the trajectory achieves high reward.
Uh, and that will help you,
uh, those are tasks that,
that solve the exploration problem to some degree for,
um, those tasks. Yep?
So is there a special way to handle hindsight experience replay when the,
when the dimensionality of the state space is really high?
Yeah. Um, we'll talk about that in a few slides. Yeah?
[inaudible] based on a particular state [inaudible] reward functions we use.
So given the, the task [BACKGROUND] [inaudible] different task?
Um, could you repeat the question?
So the R function like [inaudible] is that,
like, depend on what task we are doing like is that sort of like,
the, um, [inaudible] we're doing this task so the reward function B is for the state
and then for the task there should be something else or is
it kind of independent from the tasks?
Yeah. So if you initially collected the re- the dat- um,
if you initially collected the data for task I,
you will get reward labels for task I and then if you want to relabel for task J,
here, then you, you,
you wanna kind of replace the reward function in that because of experience
with rewards that would correspond to task J.
Does that answer your question?
[inaudible].
Okay. Um, now, you can't always apply this trick.
Uh, so you can apply relabeling when the form of the reward function is known,
uh, this is and evalutable.
If you can't evaluate the reward function in
all possible contexts or it's expensive to evaluate the reward function, like if it,
if it requires asking a human, for example,
it may be a bit trickier to do this.
Uh, it also requires that the dynamics be
consistent across goals or tasks that you relabel for.
Um, if they're not consistent, then,
uh, when you have two poles corresponding to state action and next state.
Those two poles will not- will no longer be dynamically consistent and
the resulting policy you get will have data
that corresponds to different dynamics that isn't accurate.
This is kind of one example of exploiting the knowledge that we may know that the,
the dynamics may be the same across tasks, um,
and you also need to be using an off-policy algorithm with
an asterisk that I believe that there are
some people that have looked at applying this on non-policy settings.
Um, but basically, uh,
if we're going to be relabeling experience for this task, uh,
and storing this in a replay buffer like we don't have the, um,
we don't necessarily have the policy that, uh,
that collected, um, this experience,
uh, when it was passed in a particular goal state. Yeah?
So in this case, when you have some data K and you come up with or,
or you choose a task whose traj- trajectories were [inaudible] that doesn't work.
Is there any reason to come up with
multiple tasks because they're- on the right theory asks which task to choose.
Could you say that let's come up with a few different ones [NOISE] and relabel
multiply- multiple times or is it enough - [OVERLAPPING]
Yeah. So I'm- maybe I'm not sur- maybe one answer to your question,
you can tell me if it answers your question or not is this is I
guess is- when I was making this slide I was assuming that you would
have an initial set of tasks that you cared about.
There may also be a setting where you just have one task that you care about and you want
to leverage other tasks to improve the learning for that task,
and in that setting,
it is important to think about, "Well,
what can we construct other like auxiliary tasks [OVERLAPPING] for improving? " Um,
and that will be something that is discussed a bit actually on
Wednesday in the paper presentations. Yeah?
Um, I think I had this similar thing I was thinking about, like why not,
um, why not like,
um, like why do we only take one task to relabel.
Why not like duplicate the data for each task, uh, [inaudible].
Yeah. So you can certainly basically choose tasks at, um,
at random and, and orders like choose all the tasks essentially, um,
and that, yeah- you could certainly do that, uh,
and then like you could essentially view this version like tasks when the
structure gets reward as a form of that where you just
relabel all of them and then when you sample data from your replay buffer,
you prioritize it to include data that where,
in which you get high reward, for example.
So yeah. You could, you could definitely do that
and then think about how you might prioritize later.
One downside of doing that is you do have to prioritize potentially or,
uh, I guess it kind of depends on the setting but yeah,
typically, if you did that, you'd probably want to prioritize so that you're not getting
a ton of data that you just have zero reward because
your policy was attempting completely different things.
Okay, so we just- we can just look at one kind of
quick empirical example of what this looks like in practice.
So, um, [NOISE] the paper from 2017 was looking at this, uh,
goal conditioned RL in the context of
simulated robotic manipulation where there are
tasks such as pushing shown in the top row,
sliding shown in the middle row,
and pick-and-place shown in the, um, in the bottom row.
I'm- I was looking at this now,
I'm not sure what the difference between pushing and sliding is.
Uh, but maybe- maybe that's the kind of details that are on the paper.
And empirically, if you look at it without relabeling versus with relabeling,
um, the two could er, eh,
value-based RL method called DDPG.
The green, uh, and dashed red lines show, uh,
show DDPG without relabeling,
and the red and blue lines show with relabeling with two different relabeling strategies.
Uh, you can see that in- in these settings,
relabeling significantly improves performance,
uh, likely mostly because of an exploration challenge.
So you see that in the pushing example and in the pick-and-place example,
the individual DDPG is basically getting that reward,
uh, which means that it's having trouble actually finding any rewards.
And- so this approach is helping it find rewards for some- for certain goals,
um, by essentially amortizing exploration across the different tasks.
Okay. Cool. Um, and then since we have a bit more time,
uh, we can talk a bit about image observations which is,
uh, one of the questions I asked, uh, before.
So one of the things that's- that's important in the goal condition
are offsetting is you need this distance function that tells
you how far you are from your state,
and this is- this corresponds to your reward function.
But when you have image observations,
you don't have good distance functions for images in general.
Uh, things like- like LT distance don't work very well.
Uh, and so one thing you could
imagine doing is well what if you have a binary reward function.
That basically, it's just 1 if the two images are identical and 0 otherwise.
[NOISE] Um, this will be accurate,
uh, but of course it will be very sparse.
Um, but there- there are things that- even though it's sparse,
there are things that we can do with it.
Uh, and things that we can- ways that we can use this for, uh, effective learning.
And in particular, one of the things that we can observe is that
under the sparse binary reward function,
we know that random interaction that's unlabeled, uh,
is actually optimal- if your goal function is to reach
the last state, at the last time step.
So for example, if you have some agent that's randomly you kind of,
ah, exploring in the world, you can say that, "Okay.
This is optimal with all we care about is reaching here at the last time step."
We don't care about any of the other time steps and how we got there.
Uh, and so there are things that you can do to actually, uh,
you can kind of leverage this insight,
uh, with a couple of different algorithms.
So, um, the first thing you can do with this- uh,
first of all, it's- it's easier to deal with
image observations because we can use a 0/1 reward function.
Um, the first thing you could do is you can use it for better learning.
So if you know the data is optimal with respect to that reward function,
what if we just use supervised imitation learning on that data.
Um, so in imitation learning we typically assume that we have optimal demonstrations.
Uh, here, if that's our reward function,
this- these random interactions for goal conditioned RL correspond to optimal behavior.
And so what we could do is we can collect data from- from some policy,
perform hindsight relabeling where we use the last date as the goal in hindsight,
store the relabeled data in some replay buffer,
and then update your policy just using supervised imitation learning
conditioned on the relabeled goal on your replay buffer.
Um, so it turns out you can- you can do
this and actually does decently well in a number of domains.
Um, one, uh, paper that did this,
uh, the way they collected data was actually, uh,
by using data from a human that was kind of interacting in, uh,
not completely random ways but in more directive ways but still in,
uh, less optimal ways as you might think.
So they collected data from human play,
uh, and performed goal condition imitation learning on this data.
Uh, so here is, uh,
an example of the- the play data,
so this is just a hu- human doing a bunch of
random stuff in this ex- environment in virtual reality.
Uh, and this data- there's no reward functions in this data or anything.
Uh, that way, what you can do is you can take some,
uh, one of the kind of states in this thing.
Uh, you can train, uh, a goal conditioned function,
basically, a policy that takes as input a goal image,
and the current image, and regresses to the actions
that the human took for different windows of this data.
Uh, and as a result, you get a policy that looks like the bottom rate,
um, that is able to reach,
uh, goals including pressing buttons.
Uh, for example, it's trying to press the green button,
and it's trying to press the blue button.
Now, it's trying to slide the door over to the left.
Um, it's able to do kind of all of these different goals just by using that- that data.
Okay. Um, are there any other ways to use this insight?
So another thing that we could do, uh,
with this insights is we could try to use it to learn a better goal representation.
Uh, so if we have these 0/1 goal representations,
this isn't very good for reinforced learning but we can use it to,
uh, learn about our goal representation.
And in particular, we can imagine the question,
"Which representation when uses
a reward function will cause a planner to choose the observed actions?"
[NOISE] Uh, and so we could first collect random unlabeled interaction data.
In this case, we'll collect data of, this is robot ran- like
sampling from a random Gaussian distribution, as shown here.
Uh, we'll then train a latent state representation and
a latent space model such that if we
plan a sequence of actions with respect to the last state,
we will recover the observed action sequence.
Um, so essentially, this corresponds to,
uh, embedding a, uh,
a planner in latent space into a goal condition policy
and train that goal condition policy with
supervised learning to match the observed actions.
Uh, so we could use this policy directly as in the previous paper, uh,
but we can also throw away the latent space model and return
the goal representation that the planner was
using- the planner was using inside that policy,
uh, and combine that- that goal representation with reinforcement learning.
Um, so this is referred to as
distributional planning that works in the sense that it's, uh,
performing this planning procedure inside
the neural network and outputting a distribution over action sequences.
Uh, and what you can do with this metric, uh,
is this metric is- the metric that you could use is much more shaped
because the planner has to be able to use that shape reward function,
if you only give it the sparse reward function it wouldn't be able to succeed.
Um, you could use, uh, you can take out this- this metric, uh,
run reinforcement learning with respect [NOISE] to this metric on
a variety of vision based robotic learning tasks,
uh, and then compare it to a variety of other metrics
such as pixel distance and distance in a VAE latent space.
And you can see that the metric that comes from
this procedure showed in green leads to, uh,
much more successful reinforcement learning because it's able to
recover both an accurate and a shaped reward function.
And so you can get behavior that looks like this
where it can figure out how to reach an image of a goal,
uh, or figure out how to- to push an object,
uh, to reach an image of a goal.
Uh, and it can also be used in the real world, uh,
for like reaching a certain goal image or for,
uh, pushing an object for example.
Okay. Um, so to summarize, uh,
what we talked about today is,
uh, what is the multi-task RL problem,
how we can we apply policy gradients to this problem,
and how we can think about doing, uh,
weight sharing as well as data sharing in both,
uh, policy gradient settings and in Q-learning settings.
Um, so there's a number of remaining questions,
some of which you brought up today that we'll cover in the next two weeks.
[NOISE] Uh, so for example, uh,
can we use auxiliary tasks to accelerate the learning process?
Uh, and this will be the, uh, the focus of the, uh, Wednesday.
What about hierarchies of tasks,
where we have subtasks and then we want to learn higher-level policies that operate,
um, on those subtasks?
Uh, can we learn exploration strategies across tasks rather than,
um, try to using a single kind- just using vanilla, um, vanilla approaches.
And also, uh, what do meta-RL algorithms actually learn,
uh, when applied to various settings?
Um, so we'll be covering each of these,
the first will be covered on Wednesday,
and the paper presentations, uh,
the second one will be covered on Monday next week in paper representations.
Um, next Wednesday, we'll have a- the guest lecture by Kate Rakelly, uh,
who's the first author on a recent off policy meta-RL paper,
that is, uh, I think currently
the state of the art method in- in meta-reinforcement [NOISE] learning.
Uh, and then on Monday, we'll, um,
we'll have paper presentations that study
emergent phenomena in meta-reinforcement learning.
Um, for those of you that don't have quite as much experience in reinforcement learning,
there are additional reinforcement learning resources such as the Stanford course,
the UCL course from David Silver, and the Berkeley course,
and I believe that all of these courses have lecture videos online.
Um, so if you're interested in learning more, those could be helpful,
uh, and it can also be useful for the homework.
Um, and then a couple of reminders,
homework 2 is due on Wednesday,
homework 3 covers hindsight experience replay and goal conditioned RL,
and that will be out this Wednesday and due in a couple of weeks.
After that, uh, and then the project proposal is due next Wednesday.
Okay. See you on Wednesday.
 Thank you so much for,
uh, coming and [NOISE] putting up with a few minutes of technical difficulties.
Okay. So, um, I'm here to give
a lecture today on exploration and meta-reinforcement learning.
And so, this is just a little outline of the things I'm planning to talk about.
So first we're gonna recap policy gradient and how we can use that to
build some meta-reinforcement learning algorithms that Chelsea touched on last week.
And then we're going to look at the problem of exploration a little bit more deeply.
And finally before the break,
see an approach to encourage better exploration.
Then we'll have a break and afterwards,
we'll come back and take a bit of a different view of
meta-reinforcement learning framing as a POMDP.
And then we'll look at how this framing allows us to design a,
a different algorithm that crucially is off
policy, so much more sample efficient and uses a bit of a different way to explore.
And at any point, um,
please raise your hands, ask questions,
uh, interactive is always better.
Okay. So just to kind of motivate the whole problem statement, um,
learning approaches have really excelled in producing agents that are specialists,
right, they're good at one specific task.
But in the real world,
we'd like our agents to be generalists, um,
skilled at a variety of behaviors and able to exploit
the structure in the world in order to learn new things faster.
Um, so here's a nice little toy example with some, some art, um,
surfing, skateboarding and sledding, all
involve bouncing your body on the board while it moves,
right, so we'd like the agent to be able to extract this kind of
shared knowledge between the tasks in order to learn a new similar task faster.
So let's briefly go back to how meta-learning works in
the supervised learning case just to build up some common formalism.
So in the supervised learning case,
say image classification, we'd like to be able to
recognize new classes from just a few labeled examples.
And we'll do that by training across many such,
uh, small classification tasks,
tasks in our training set.
So just to get some notation,
we'll say that we our dat- our training set is d-train and we're going to
use some function F-Theta which I'm not going to describe at all right now.
And just say that F-Theta extracts some task-specific information that I'll call Phi_i,
and then, I want to use Phi_i to make predictions on the test set, right.
And so then I compute the loss and I sum over all of my training tasks.
So my goal is to maximize performance across all the training tasks.
Okay, so now let's add the RL formulas in side-by-side.
So in RL, now, instead of, um,
adapting to a new classification task,
we want to adapt to a new MDP.
So suppose we have some, again, going back to
our set of MDPs that involves balancing yourself,
right, so we want to be able to train across the set of
MDPs in order to generalize to a new one.
And so the notation will be largely the same, again,
we have our F-Theta extracting task information Phi_i and we'll
use Phi_i to try to maximize our expected return in the new MDP.
So just to get some terms down now to be clear,
I'm gonna call this F-Theta entity the adaptation or sometimes the inner loop,
and we'll call this outer optimization,
the outer loop or the meta-training.
And so, as we look at these different algorithms,
we'll keep coming back to what does
meta-training look like and what does adaptation look like.
And so, meta-training will generally just simply be
stochastic gradient descent on this objective,
but we'll see a lot of different choices for how
adaptation can work throughout these algorithms.
Okay, so let's look a little deeper at the differences
between these two frameworks that I've written down.
So what's the difference in RL?
Well, in supervised learning,
notice that that new task that we're given,
it's all the data is given to us, right,
someone gave us the training examples that were labeled, um,
and so, all our job was,
was to figure out how to adapt quickly given that data.
But now if we consider the RL setting,
when I want to compute F-Theta of M_i where M is the MDP,
I have to figure out how,
like I'm not usually given the, say,
transition function or word function that explicitly define the MDP,
right, I'm given experience,
and I have to choose what experience I'm going to use to adapt.
So we introduced this problem of exploration in addition to adaptation.
So now your job is not only to adapt,
but also to collect the data that will best allow you to adapt.
Okay, so let's go back for a second to policy gradients
so that we can build-up our meta-RL algorithm.
So in policy gradient we're doing direct policy search,
um, with a policy Pi that we're going to parameterize with Theta.
And so we'll simply run the REINFORCE algorithm,
which I think, uh,
Chelsea mentioned last week,
where we'll sample data from our,
from our policy and then,
we'll compute the policy gradient to update the parameters.
And this is really just kind of a formalization of trial and error, right,
you're trying to make good trajectories more probable,
and bad trajectories less probable.
Okay, so how can we turn policy gradient into a meta-learning algorithm?
Well, intuitively what we wanna do is collect and remember information as we go,
right, as we collect experience and we want that information to
kind of inform the next action that we take.
So this kinda starts sounding like recurrent networks,
so let's just try plugging that in for the policy.
So the idea is our policy is just going to be a recurrent network,
we're gonna train with policy gradient as usual,
and we're going to train across a bunch of different meta-training tasks.
Right, so I'll sample a task,
in this case, sledding,
I'll roll out a bunch of experience from it and through my RNN.
And through each time-step that I
accumulate I'll be updating the hidden state of the RNN,
right, which will inform my next action.
And so, as I train across all of these tasks,
I'll be optimizing Theta such that I get Theta-star.
And what Theta-star will represent
is like an op- like a learning algorithm for a new task,
right, where the, the learning happens through the recurrent update of the hidden state.
So to kind of keep track of all this in the upper right hand corner,
I'm just going to keep that,
that RL formulism that we wrote down at the beginning.
And so now our choice of F-Theta here is just an RNN,
right, and we're still doing policy gradient from the outside loop.
And so, one, one note about this, is that, um,
you may be wondering like how is this different from just using your current policy in
general and one difference is that we want to persist the hidden state across episodes,
right, because we may want adaptation to extend across many episodes.
Okay, so what are the pros of this method?
Um, it's pretty general and it's pretty expressive,
right, RNNs are very powerful models.
One con is that it's not consistent.
And what I mean by consistent is that we don't really have a guarantee that
the RNN learning algorithm will converge or what it will converge to,
right, we're just hoping that by optimizing at the outer loop,
it'll do something reasonable.
Okay, let's think about a different approach.
So what if we made the inner loop look just like the outer loop?
So remember the outer loop is policy gradient,
what if we just do policy gradient on the inner loop too?
And you guys have seen this already, this is MAML, right, model-agnostic meta-learning.
And so the idea is we're trying to find some parameters Theta,
such that when we take a few gradient steps on that Theta we'll get to
a Theta-star that's optimal for a given MDP, right.
So here we'd expect Theta-star 3 to be optimal for
your skateboarding and Theta-star 2 to be optimal for surfing.
So again, in our little figure up on the right,
now the outer optimization is PG and the inner one is also.
So what's a pro of this?
It's consistent because now the adaptation step is gradient descent, right,
and we know what gradient descent converges to it,
converges in non-linear settings to local minima.
And it's clear that in the limit of enough data,
it would just reduce to the reinforcement-learning.
So we know that we would eventually acquire their optimal behavior.
So this is a nice property.
One con is that it's not as expressive as the recurrent model, um,
so question, can you think of an example,
an example MDP, um,
in which the recurrent method would be more expressive?
So what happens if the rewards are zero?
Or in general if the rewards are sparse?
So if the rewards are sparse then what does the policy gradient look- say,
say you get one trajectory and you get- collect zero rewards,
what does the policy gradient update look like? It's zero.
Like you don't update at all.
Which is kind of silly because you actually should've learned
some information from getting no rewards, right?
First of all, you learned about the dynamics of
the new environment, which may have changed.
And second of all, you learned that that's
a really bad place to go because you didn't get any rewards.
Um, but this formulation is not really able to capture that,
whereas, um, the recurrent method we saw before would be able to.
Okay. So let's look about- look at how,
um, MAML-based algorithms learn to explore.
So here's a computation graph of what's happening when you adapt with MAML.
So you start with your policy,
Pi Theta, right? On the left.
And then you collect some data, Tau,
some trajectories, and we use those trajectories to do an update u,
which is a step of policy gradient,
and that gets us our adapted policy,
Pi Theta prime, in the middle, right?
Now what are we going to do? We're going to again,
collect data with our adapted policy,
that'll be Tau prime,
and we'll evaluate, right?
We'll, we'll look at the rewards that we got for those trajectories.
So the goal is to assign credit- like give credit for
those rewards to both the adapted policy and the pre-adapted policy, right?
Because the pre-adapted policy should get credit for doing
whatever exploratory behavior led to good results later on.
So that's exactly what happens.
So if you write out, um,
the gradient, what you get is,
um, is this kind of
credit assignment where it takes the structure of the problem into account.
So it takes that causal relationship between, um,
the Tau prime and the Theta parameters into account when,
uh, when we compute that gradient.
And, um, RL-squared is similar, right?
Except for right now it's happening in the context of an RNN.
So through backprop through time,
the steps you took earlier in the trajectory are getting
credit for the rewards received at the end of the trajectory.
And if you want, um, a few more details and,
and the derivation for how this works out, it's,
it's kind of interesting and, uh,
you can find it in this paper, uh,
Rothfuss et al, 2018 and I also have the reference at the end of the slides.
Okay. So we've just kind of hand waved our way through with the, um,
with the idea that these algorithms are capable of learning to explore,
and if we could optimize them perfectly they would learn good exploration trajectories.
So let's look at how they actually do in practice.
So this, on the left,
is the recurrent approach that we saw first,
and the goal here is to adapt to new maze configurations.
So it's been trained on a bunch of different-looking mazes and
now we're asking it to adapt to this maze layout here.
And so the goal is you always start in
the blue square and you want to end up in the red square.
And so what we're seeing on the left is first some exploration trajectories,
and then on the right the final trajectory that the recurrent policy produces.
And so we see pretty nice coverage of the maze here, right?
It goes way out,
kinda explores one direction,
and then, and then goes back.
So it's probably covering more than 50% of the maze.
Um, and then on the right, we have a different kind of example where, um,
we have gradient-based meta-learning in a kind of navigation setting.
So what's happening here is the rewards are initially sparse around the initial position,
and then you start getting some reward signal as you move further away.
So it requires you to learn
exploration trajectories that explore far enough away from the initial position.
And, and in this case it's working quite well, right?
You have the blue trajectories as the exploration trajectories,
and they're successfully going far enough away to get more signal
such that the yellow trajectories to solve the task.
I have a question.
Yeah.
I'm still not sure I'm understanding the recurrent method [inaudible].
If you're learning a hidden state and an RNN that then just generalizes well at a certain task.
Yep.
Yep. Yeah so you just make the policy and RNN,
uh, you keep rolling out across
episodes and you train it across a whole set of tasks. Yeah.
[inaudible] and then, um,
for policy gradient, If we're reversing a policy gradient here,
would it be possible to swap in a Q-learning for policy gradients?
Um, it does not work well or at all,
in pretty much, uh,
all of the research so far.
It's definitely an active area of research.
Um, so in the second half of the talk,
I'm going to talk about one approach that does work for off policy RL,
but certainly combining these approaches with off policy RL is
a very interesting and actively researched topic but we haven't figured it out yet.
Okay, um, but in other cases of course it doesn't work so well.
So here's an example of- um,
here's gradient-based meta-learning trying to explore in this sparse reward environment,
and so I'm just showing two different environments here.
In the first agent is the ant and the second it's this kind of cart thing,
and on the right I'm plotting- um, so,
so the job of the agent is to navigate out to one of those little red circles,
kind of on the unit circle.
And so to explore effectively,
it needs to be able to reach that outer rim right so that I can
get any kind of rewards that it can figure out how to complete the task.
And so then what's being plotted in these diagrams is
the actual explanation trajectories that you get when you run MAML.
And as you can see,
they're not able to explore coherently enough in order to solve these tasks,
so there's some more work needed here.
Okay. So let's think about what the problem is,
and I want to start by just giving some intuition for the problem here.
Let's see if these work or not. Oh, yes.
Okay. So here's a video of a,
of a child exploring,
or what I would call exploring, right?
And as you can see,
her behavior is very kind of coherent.
Like she picks a strategy and then she executes on
that strategy for some amount of time before switching, right?
And as a result, she's able to kind of explore in much more meaningful states, right?
She's not waving her arm around out here away from the box.
She's exploring in the states that are relevant
for inserting that shaped peg into the box, right?
And then by contrast,
here's what exploration in RL usually looks like.
So this is at hyper speed.
But as you can see,
it's not doing- it's not exploring in the same kind of meaningful states, right?
It's spending most of its time not interacting with
the object and just kind of like doing random exploration in the joints.
So what we'd really like is to develop a method that,
that has this temporal coherency that we see on the left, right?
Okay. So now we're getting a bit more technical.
Let's think about this problem that more.
So we know that we want exploration to be stochastic, right?
Um, because we need to collect a whole bunch of diverse data from different behaviors,
and whereas optimal behavior is deterministic.
So optimal behavior and exploration policies are quite different, right?
And it can be hard to represent both of those in the same policy,
especially when you have time-invariant action distributions.
And typical methods of adding noise to get that stochasticity are time-invariant, right?
So you might have a stochastic policy that's on the left, um,
but then there is no correlation between time- between noise at each time step, right?
Or you could, um,
noise the policy parameters,
but again kinda same problem.
So we'd like a bit of a more, um,
explicit way of getting these temporally coherent trajectories.
And it seems like we should be able to use our meta-training tasks to do this, right?
We've got a whole set of tasks and we should be able to
learn some kind of strategy that would work well for other tasks of that nature.
So let's build on top of the gradient-based meta-RL that we saw so far.
And the idea here is we're going to augment the policy with
a latent variable Z that will inject the structured exploration into the policy.
So we're going to do this by sampling
a z and holding it- so I'm going to refer to that latent variable as z.
So we're going to sample a z and hold it constant across the entire episode, right?
Which will give us our kind of input correlation.
And then we'll enforce that, um, that,
that z gives us something relevant to the task through meta-training.
And the way we're gonna do that is by adopting the z to give us optimal trajectories.
So the way this is gonna work,
is we're start- we're going to start with a prior on z, right?
We'll sample a z from that prior and use that to collect exploration trajectories,
then we'll adapt the z via gradient descent and we'll
then enforce via meta-training that that gives us optimal trajectories for that task.
So in the RL diagram on the upper right again,
our outer loop is still policy gradient but
our inner loop is now policy gradient on the z variables.
Any questions about this? Yeah.
[inaudible]
Uh, no.
Here, I'm using the gradient-based approach.
So all the policies will just be feedforward,
um, and we'll do adaptation via gradient descent.
Okay. And of course,
the result we're hoping to get is if your job is to pick up one of
these shapes here rather than doing kind of random incoherent exploration,
we expect the agent to systematically explore by picking up each different block.
Okay. So this was a method that was published and called MAESN,
and so here we're going to look at some results from that paper.
So here the task is to push one of these colored blocks across the table,
and the agent is this kind of disembodied gripper here.
And so we're going to compare what happens when you do MAML and when you do MAESN,
which is that structured exploration we just talked about.
So when you do MAML,
we find that the explanation is kind of random and
incoherent and doesn't succeed at pushing any of the blocks across,
and so it doesn't- it's not able to get a signal about which one is correct.
Whereas if we train with this latent variable model,
the exploration policy systematically pushes blocks across
the table because it knows that the task distribution involves that, right?
So it just needs to figure out which one.
Okay. Any more questions about,
uh, this method? Yeah.
So this is like [inaudible] of, I guess, pre-training?
Yeah. So this is the meta-trained policy. Yep. Yep.
[inaudible]
Can you elaborate?
Um, I was just wondering if I should ask specific or if it was just, like,
I- because you said you were- you were- you were a- adding in Z,
like, variable for each episode.
And, like, wha- I was wondering if that
changed or updated across episode and what [inaudible].
Ah, I see, I see.
Um, right, so an episode only contains a single task.
Yeah.
You can't switch tasks in the middle of an episode.
Um, so if you were to think about
[NOISE] the more realistic setting of kind of continual learning,
then we would basically break it up into,
uh, we could potentially make episode and task be
the same boundary just whenever the reward changes or the dynamics change.
Um, right.
So, you have kind of choices here about
how long you want to stick with the current, like,
variable and that's kind of heuristic.
Cool.
[inaudible]?
The reward?
[inaudible].
Um, I believe the reward is sparse until you get so far to the right.
So you need the- you need, um,
yeah I'm not sure if- you might get a reward when you move the wrong block too.
I'm not sure. But I- I think it's only for the right one.
Okay. So, let's take a bit of stock of where we are.
So, what do we want from a Meta-RL algorithm?
Um, and we're going to compare the three that we've looked at so far.
So we have the recurrent one,
the gradient one or MAML,
and the structured exploration one or MAESN.
So, we talked about consistency.
We said that the recurrent one's not consistent,
but the gradient one and its derivative, of course, are.
Talked about expressivity and why the gradient based ones are ma- not as expressive.
We talked about structured exploration and how we can,
like, endow the policy with the capability for structured exploration.
[NOISE] But finally, what about efficient and off-policy algorithms?
So everything that we talked about so far has been based on policy gradient,
and policy gradient is very sample inefficient, right?
Because- partly because it's on-policy.
So it has to collect new data at each time it wants to do an update.
Um, so we haven't looked at this question at all yet,
um, but this question is pretty important.
And we know from single task reinforcement learning that off-policy algorithms
are one to two orders of magnitude more efficient in terms of samples, right?
And they can achieve just as high a performance.
So this is something that we'd really like.
And it also makes a huge difference for real-world applications.
So if we're talking, for example,
about wanting to run Meta-RL on a robot,
the- all the algorithms we just looked at would be kind of
hopelessly, uh, sample inefficient.
I calculated with kind of a,
um, frequency- control frequency of 20 hertz and, say,
you ran the robot continuously [NOISE],
and it would take about a month to train them all on the robot,
uh, for a- for a reasonable task distribution.
And so if we- if we could bring these, um,
off-policy methods to bear,
we would cut that down to maybe 10 hours and we could, uh, we could run that.
So this is a really important question that we're going to look at next.
Okay. So why is off-policy Meta-RL difficult?
As a disclaimer, this is a very much unresolved question
and there's active research into trying to figure this out.
And it's also very related to why off-policy RL is hard too,
so that's it- there- a lot of the reasons overlap [NOISE].
Um, but one reason that I want to point out is that a key characteristic
of meta-learning is that the conditions at
meta-training and meta-test time should match, right?
So what I mean by this is- let's go back to the image classification case.
And let's say that your meta-training distribution consisted of classifying these dogs.
These Dalmatians, German Shepherds, and Pugs.
[NOISE] Now, when you go to, um,
learn a new class you're gonna have a pro- much easier time learning to classify Corgis,
which are kind of in distribution versus,
say, motorcycles, which are not, [NOISE] right?
And so we can make the same statement about RL.
So when we have a meta-trained policy,
and we run it- we roll it out in our new task that's going to be on-policy, right?
We're going to be collecting on-policy data that we want to adapt from.
But we decided we wanted to make use of off-policy data during meta-training.
So now, we're presented with this kind of distribution shift
where we know that we have to be able to use on-policy data,
but we don't want to have to use it exclusively during meta-training.
So that's what we're, um,
let's take a short break now,
and then after the break we'll talk about,
um, how we're going to address this problem of off-policy RL.
Okay. So we're gonna switch gears just a bit
and talk about kind of a different way to think about meta-reinforcement learning,
and then we'll circle back to how this impacts our design of an off-policy algorithm.
Okay. So POMDPs.
A POMDP is just a partially observed MDP,
right? So what does that mean?
That means that we have some state,
which is unobserved to us,
but we have access to observations,
which are here at narratives O,
which give us kind of incomplete information about the state.
And so, an example of this, uh,
might be that you have,
uh, incomplete sensor data.
There might be occlusions.
Um, perhaps you- to do the task you need an estimate of your velocity,
but you only have an estimate of your position.
Things like this are kind of usual examples of a POMDP.
So it's a very,
very general framework and in general very, very difficult to solve.
So let's see how we can put our meta-RL problem into this POMDP framework.
So, if we write down meta-RL as a kind of graphical model,
and then what we have is essentially an MDP, right?
But where the rewards and
the dynamics functions are also dependent on this task variable.
And in general, we don't observe the task variable, right?
That's our job during adaptation to kind of- we could think of
adaptation as figuring out what that task is, right?
And so we can just redefine things a little bit and put this in the POMDP formalism.
So, let's just call that hidden state to be
the concatenation between our true state and the task,
and now we don't observe it anymore because we don't know the task,
and we'll just redefine our observations to be,
um, the state and the rewards.
So you could extend this a little bit more into a partially observed meta-RL problem,
right, where you- there's also some other part of the state that you don't observe.
Say, you are doing meta-RL from images or something.
Um, but for now,
let's assume that we do observe
the state and the only thing we don't observe is that task.
Okay. So, let's think about how we might go about trying to solve a POMDP.
So, a good strategy might be to maintain
a belief distribution over what state we're in, right?
So, let's take this really toy grid world just for an example.
So there's three states and they're called S0, S1,
and S2, and you're the,
the circle agent, and you start in the far left.
And in this MDP the- the goal is to get to state two, right?
And you get no rewards until you get to state two and then you get a reward of one.
So, when you first start out the episode,
you don't know where you are, right?
So you might assume a uniform prior over,
uh, what you think your state is assuming you have no other information.
So that's what I'm illustrating there on the right with the blue boxes.
Um, okay. So now,
let's say, we take an action left.
We hit the wall, we don't go anywhere,
we're still in state zero,
and we get a rewa- get a reward of zero.
So now, I know exactly where I am because I just hit the wall, right?
So I can say with 100% certainty,
I'm in the leftmost state.
Okay. So how does this work for when we're thinking about the meta-RL POMDP?
So now, we're considering a whole set of MDPs, right?
So let's take the same grid world,
but let's define three MDPs on it.
And I'm just going to change the reward function between MDPs.
So for MDP zero,
you get a reward of one if you're in the leftmost,
for MDP one, you get a reward if you're in the middle, and so forth.
So now, let's say you have observed where you are,
but you don't know what the task is, right?
That's, that's your job at meta-RL.
You wanna figure out what the task is.
So let's say again that you start out with no prior knowledge,
you have a uniform belief that you could be in any of the tasks.
And now, let's say,
you take the same action.
You go left, you hit the wall,
you're still in state zero, you get zero reward.
Well, now you can update your belief,
right, because you know you're definitely not at MDP zero.
Because if you were,
you would have gotten a reward.
So we could do this- and as we could take more actions,
we can update the belief more and more,
so we have a better idea of what task we're in, right?
And as we get a better idea,
we can act more and more optimally.
Okay. So now we have an idea for what
our belief is and how we would go about updating it.
Um, how are you going to use it to pick actions?
So basically what I mean is given this belief how does the policy use it to,
to do something to, to explore.
So the strategy we're going to talk about is posterior sampling or Thompson sampling.
And the idea is that you take a sample from
the belief and then act as if that sample was the truth.
So as an example of this suppose that we sampled,
um, that we were in MDP 0, from our belief we picked the first bin.
So we should then take the action as if
that was the MDP that we're in and the optimal action in that case
is to go left, right? Because then you hit the wall and you stay in the state and you would
accrue rewards if that was the MDP that you were in.
Of course in this case,
it's not the MDP that you're in, right?
So then you can update your belief as we have there on
the lower right and you can repeat the procedure. You can sample again.
Maybe this time you sample that you're in MDP 2, right?
And then you would move right because that would be the optimal strategy.
So as you do this posterior sampling,
you're smoothly trading off between exploration and exploitation, right?
Because as you collect more experience,
you're narrowing your belief distribution.
So you've got a better and better idea of in this case what task you're in.
Okay. So we're going to use that intuition to build an algorithm.
So just as we have before what we want to do is maintain
a posterior belief over the task and update it.
So we're going to call that P of Z given C. So C here is going to denote contexts,
but contexts or adaptation data, same thing.
Okay. So and we're gonna make it a continuous distribution.
So this is our continuous belief distribution P of Z given
C. And we're going to get it from our adaptation data, right?
Then we'll sample from that distribution.
And our agent will use samples from that belief in order to explore.
Just like we talked about with posterior sampling.
So now kinda going back again to our little RL framework on the upper right hand corner.
Now our function f which used to be for example an RNN used to be gradient descent,
now, it's a stochastic encoder, right?
It's that phi thing that's going to take our data and produce our belief distribution.
So before we get into how we're going to make all of this work practically,
let's take a look at the results that we want to get.
So this would be- this is
posterior sampling in action for a real continuous control domain.
So you're the agent, you start at that gray circle,
and you know from meta training that your goal
is somewhere on that unit circle kind of denoted as a- as a arc.
But you don't know where on the unit circle your goal is.
And you only get a reward when you're within the current goal.
So you- in this example, you only get a reward when you're within that blue circle.
So what we would hope is that if that belief represents our kind of task distribution,
right, which we know is all along that arc.
Then when we sample from the belief essentially what we're
doing is we're sampling different goals along that arc.
Right. And when we sample a goal what do we do?
We act optimally according to it.
So we act as if for example the goal is way over there on the right-hand side.
Of course, we seem to find out that it's not because we get zero reward,
but this gives us kind of an,
um, easy way to eliminate hypotheses and narrow our estimate for where the goal would be.
And this is actually data from the algorithm that we're going to build up.
Okay. So how do we make this work in practice?
Well, inferring the true posterior P of Z given C as intractable.
So we're going to use variational inference.
So we'll approximate the posterior with Q of Z given
C. And we'll parameterize that distribution by Phi.
So it will, that distribution will be the output of some non-linear neural network.
Um, and similarly, we will give an approximation for the prior.
And so we can write down the ELBO just like this.
And this is the same ELBO that you guys saw before when Chelsea drives
variational inference just probably with a bit of different notation.
Um, so the first term, right, is the likelihood term
and the second term we might call the regularization term or the information bottleneck.
So let's think about semantically what this equation means, right?
So we're trying to maximize
this likelihood term subject to some- to the regularization constraint, right?
And the likelihood term is supposed to give us a belief over the task.
So what's supervision can we use for a belief over that task?
Um, so we have choices.
We could for example reconstruct the reward and transition functions.
Right. If we did that,
then we know that our representation has the information of what
MDP it's it- it is because it's able to reconstruct the rewards and transitions.
Um, another option we can use which is the one we're gonna choose
here is we could adopt the Bellman error.
And so this will just be essentially training a Q function.
And of course Bellman error is not actually likelihood,
but if you squint a little and you read the control as inference tutorial,
it might start to look like one.
And so we're just going to wave our hands a bit and,
and say that that's okay.
Um, maybe a more intuitive way to think about it though rather than is
the Bellman error likelihood is to think about it as information bottleneck.
So in the information bottleneck point of view,
you want to maximize that first term, that
R term, right, subject to the- subject to the KL constraint.
And what that means is you want to pass enough information through that Z
such that you get good predictions for R, right? Such that you get good Bellman error.
But you don't want to pass any more information than that.
And that's what the KL term does.
The KL term tries to maximize the entropy so it says less information.
The denominator says more information.
And so as you have these kind of two competing objectives which you
get is just the information you need for the Bellman error.
Does that make sense? Any questions about this?
Cool. Okay. So one more thing before we're ready to put the algorithm together is,
um, we didn't really talk about what the structure of that phi is, right?
I just kind of waved my hands and said it was a, ah,
neural network that takes in all of your experience and produces this distribution.
Great. But let's look inside that a bit more and see how we want to design that element.
So remember our goal is to just infer belief over the task.
And to do that because of the Markov property,
we don't actually have to retain the order of the transitions that we saw them in, right?
So it doesn't matter, um,
for all the transitions you've seen so far,
it doesn't matter that we encode them in the order we saw them.
Because of the Markov property all we need is S, A,
S prime R, right? And that defines the reward function and the transitions.
So we can encode them in a totally permutation invariant way.
Which is exactly what we're gonna do.
And so what we're gonna do is just take each tuple S, A,
S prime R and encode it independently to produce
a Gaussian factor and then multiply
those Gaussian factors together to get a Gaussian posterior.
And we're just gonna use Gaussians because they make the reparameterization trick nice.
Um, so why would we want to do this as opposed to using an RNN?
Um, well mostly for implementation reasons.
Um, because this is- this approach is going to be,
um, faster to train,
more stable to train,
and generally kind of simple architectures are gonna- are going to work better.
Ah, you can also replace this with an RNN and probably with a bit more tuning,
ah, it will also- it will also work.
Okay. So we're almost ready to construct the algorithm,
but now we need to talk about off policy RL so we can put that piece in.
Okay. So here, ah,
we're going to build on the algorithm soft actor-critic.
Um, and I'm not gonna go too deeply into this,
but I'll give some pointers for references if you're interested.
Um, so let's just kind of look at it from a high level.
So the soft part, what does that mean?
That means maximize rewards, right?
So in this equation at the top, reward, maximum
expected returns that's same from policy gradient.
Um, but then also maximize the entropy of the policy.
And they do this in soft actor-critic because
they want- it gives them some better exploration properties,
ah, when they're- when they're learning.
Um. Oops. Okay. And then the actor-critic part. What does that mean?
Well, we just modeled both the actor and the critic as kind of separate agents, right?
And we're going to train the critic which is the Q function here with the Bellman error.
And then we're going to train the actor to take actions
that give us high Q values and also give us high entropy.
And for the derivation of how all of this kind of falls out, ah,
I would refer you to the control as inference tutorial
by Sergey Levine as well as the original SAC paper.
Is this enough detail for going forward? Yeah.
I have also [inaudible] is this [inaudible] when
people say the policy is getting [inaudible]
Right. So it's- here it's not, um, yeah.
Ba- yeah, it's basically the same.
Uh-huh.
[inaudible]
Um, I think that
would take maybe more di- divergence,
um, but basically, like they're- so
SAC is derived from a framework where you model the whole thing as,
um, a probabilistic graphical model.
And then you can, um,
basically relate the probability of your trajectory being
optimal to your- the exponent of your rewards.
And once you kind of make this assumption,
then you can, um,
then you can basically, like,
that assumption basically leads directly to this algorithm.
And, uh, HUC like doesn't start from that kind of framework.
Okay. So, uh, just of- just some results from this algorithm,
um, it does pretty good on humanoid and if they
ran it on this kinda cool claw robot at Berkeley and it learns to turn the valve.
Um, and so this- this algorithm right now is, is, uh, the
kind of highest performing in more sample efficient algorithm that, that we're using.
And so we're really interested in being able to build on top of
this algorithm for our meta-learning uses.
Okay. So here's kind of a simplified soft actor-critic in a diagram form.
Um, and so it's soft policy,
we're gonna maintain a replay buffer as we collect data.
And we're going to optimize both the actor and the critic with their respective losses.
Okay. So now let's add
our task belief distribution on top of this algorithm. So how's that gonna work?
So again, we're going to maintain this approximate belief posterior,
that Q of Z given C, right?
And we're going to take samples from it and pass those to the actor and critics.
So now the actor and critic essentially are getting a state that's
the concatenation of the original state of the MDP with the Z variable, right?
And where Z here includes,
is representing a, uh,
task ID in some sense, right?
And that KL term comes from the,
uh, the regularization from the variational inference as we talked about before.
And you'll notice that we back-propagate the critic clause into the encoder.
And that's the Bellman era likelihood term that we saw before.
Okay. So now let's just look at some results from this algorithm that we've put together.
Um, so I'm gonna show results on these four domains.
Um, and for the agents,
we're gonna be looking under the cheetah,
humanoid, art, and walker.
And we'll look at meta training distribution is that
differ both in reward function and in dynamics.
So for the first three agents,
we're gonna look at variable reward function.
So that means, um,
which direction you move in,
your target velocity, or a goal position.
And then for the final agent,
we'll look at varying dynamics.
So that will be like the- the masses and physical parameters of the agent.
So here is the first three ste- the first algorithms that we looked at.
Um, excuse me, evaluated on these tasks.
So in, uh, that light greenish color,
we have, um, RL squared which is
an instantiation of the recurrent method that we talked about at the beginning.
And then in purple and in yellow,
we have kind of two variants of the gradient based approach.
And so this is just kind of for baseline, how well things do.
And we're on a log scale here which is why the curves kind
of trend up like outward that way.
Okay. And then in blue here is the algorithm that we just put together.
So be- it's- as you can see it's about 20 to 100 times more sample efficient,
and is also outperforming the previous approaches.
And the reason it's able to do this is because it's leveraging that off
policy algorithm SAC that we just talked about in the meta-learning context.
Okay. So why does this view that we took
of meta RL as a POMDP and wanting to have a belief over the task?
Why does that make off policy RL easier?
Like going back to that question where I was showing the distribution of the dogs and
like it's better if you have a test SAS that's in distribution,
how does this help us with that?
So the interesting thing is that this approach allows us to separate the data we use to
infer the task right at the top with
the Phi from the data that we use to train the RL agent.
And so what this means is that we can address
the distribution shifts by making that adaptation data on policy,
but making the rest of the data used to train the actor critic off policy.
So- and this can still get us
big sample efficiency speed ups because if you think about it,
probably a lot of the data that you're using is to figure out how to,
um, to figure out policy behaviors that are like common across all of the tasks, right?
So in the example of navigating to different positions,
the knowledge of just how to walk at all is not task-specific, right?
And we can learn in that off policy through kind of
the bottom half of this figure whereas the top half which goes through S_c
can be on policy.
And we have this kind of knob that we can turn to test,
you know, how off policy can we make that data or not.
Does that make sense? Yeah.
[inaudible] [NOISE] after exploring updating
the- the inference parameters [inaudible]
Right. Uh-huh.
[inaudible] a new thing. The only parameters that are changing
as I accumulate data for the new task, is the- is the SP, is what you think?
So actually, no parameters change at all.
All the parameters of the model are fixed.
So it's like the RNN case in that- in that regard, right?
Where after meta-training is done,
your parameters are fixed.
So then what's- what's changing here is the Z the- the belief distribution,
the Q of Z given C. So as you accumulate data, right?
You're updating your estimate of the belief with,
with the meta-trained Phi parameters.
Yeah.
Have you seen the variance on that [inaudible] decreasing over the course of the task?
Um, yeah.
Okay, does it narrow down to, stays constant throughout the rest of the task?
Um, yeah. I mean, kind of by construction it- because of this multiplication,
it tends to decrease, um,
and then at some point it- it just kind of stabilizes.
Yeah. Yeah.
[inaudible]
Yeah.
[inaudible] like RL squared seem pretty [inaudible] well.
So I'm curious why ProMP which I'm not familiar with is doing better on humanoid direction TB, which is probably one of the harder tasks.
Yeah, yeah.
Whereas RL squared is doing much better on everything else.
Um, so why RL s, uh, oh.
Should- I use to know why RL squared or ProMP was better on that one.
There's like two versions of humanoid. Let me get back to you on that.
Um, but in general, the different algorithms.
Um, so ProMP is basically
MAML but it's implemented on top of a different policy gradient algorithm.
So MAML is built on top of TRPO which is trust reasoning
policy optimization and proMP's built on top of PPO.
And there's some, like little things you have to do to make that work.
PPO in general it's kind of- it is about better performing algorithms.
So in general ProMP outperforms MAML.
And then RL squared here also is implemented on top of PPO which actually
makes it improve a lot more over previous results which were usually plotted with TRPO.
So [NOISE] there's a lot of variables in RL and [LAUGHTER] um,
you kind of have to get all of them right to totally have a+ comparison.
Um, the other thing,
while I'm thinking about the humanoid thing, um,
the other thing I would say is that these curves are a bit misleading in terms of
if you actually care about what the agents are doing
it's not really working in in in some of the cases.
So for example in that humanoid case, um,
what you want is the human or to run forward or backward or in different directions.
But wh- none of these algorithms are actually achieving that reward.
So, uh, this algorithm in blue is
essentially just standing up for longer than other algorithms.
Uh, so I think that kind of points
to the difficulty of optimization in Meta-RL in general.
Because we know we can train a single task policy,
uh, with SAC on humanoid, right?
But when we go to the meta case, we're not able to achieve that same performance.
So that the curves can be a little misleading.
In some cases. And that's the only one that it.
That's the only environment here where that's true.
The other ones actually do the tasks to some degree.
Um, okay.
We talked about that.
So I'll talk about some limitations of this method.
Um, so posterior sampling is pretty good.
And, uh, there's a reference at the end for a paper
that that shows you can show a regret bound for posterior sampling. That's pretty good.
Um, but it's not optimal obviously.
And here like this
the same diagram that I showed it off with also shows off why it's not optimal, right?
Because if you're in this environment the best thing to do is to go out to
to the top to the arc and simply walk along the arc until you find the goal,
and posterior sampling can't do this.
And so the algorithm we built also can't do this.
Um, so it's, in some cases it's going to have you know,
potentially pretty bad exploration,
uh, pretty far from optimal exploration.
So one kind of interesting way to look at this I think is is
to look at the difference between
that MAESN algorithm that we saw in the first half and what we just designed here.
So to get a bit into the details,
um, the difference is in how you constrain that latent distribution?
And if you constrain it,
before you do the adaptation or after?
And so graphically the difference is that in the algorithm we saw before,
um, the- so let's let the yellow be the prior distribution, right?
You start off with samples from the prior and then when you adapt,
your distributions move away from that prior.
And that's because you're constraining
the- the pre-updated distribution to be close to the unit Gaussian, right?
Not the post- updated one.
So essentially that kind of separates your exploration trajectories which are
in the yellow region from your task
optimal trajectories which are in the different blue regions depending on the task.
Versus the method we just described what we just built up this,
er, the off-policy method PEARL.
What it's doing is it's constraining the post- adapted, um,
z-distribution to the prior,
right? Remember that KL-regularization term.
So essentially what that's doing is it's trying to get all of the task
distributions to kind of overlap with each other and with the prior.
And so essentially in the,
in the PEARL approach,
you're not going to learn- um,
you have no chance of learning optimal exploration trajectories, right?
The latent distribution you learn is is simply over the task distribution and so all the
exploration trajectories [NOISE] that you
sample will just be optimal ones for some other task.
[NOISE] Okay,
Um, so in summary,
we first looked at how building on policy gradient RL.
We can implement different Meta-RL methods with recurrence or with optimization.
And we looked at how adaptation of RL is both an exploration, and adaptation problem.
And we looked at how we can improve exploration
by conditioning the policy on latent variables.
And in the second half,
we looked at how meta-RL is a particular kind of POMDP.
And how using that kind of intuition.
Um, we can build up an algorithm that estimates
a belief over the task and integrates well
with off-policy RL. Any other questions?
Yeah.
Seems like this algorithm is a lot less stable in certain regions than the other algorithms, do you know why that would be?
Um, here?
Yes.
I'm not sure if that's actually
the case or if it's actually a problem with how we record it.
I think it might be a problem with the frequency of how we recorded
the, um, the data.
Okay, on a log scale type?
Yeah. It could- it could be a bit more and see what I mean
in general off-policy algorithms tend to be more unstable,
than on-policy ones,
though soft-actor critic in a single task results, uh,
is pretty stable.
Yeah.
[inaudible]
In where?
No matter wallpaper can be release by, um-
Oh, yes. Yeah, the meta-wallpaper.
Yeah. [NOISE] Yeah, so that paper is- um,
just to give some context, that paper is interesting.
Um, they made- so this is people at Berkeley and Centroid.
They made a data set of, uh,
robot manipulation tasks in MuJoCo,
like they have like 50 different tasks,
and they're quite different tasks.
So here we're looking at things like cheetah
running at 10 meters per second versus 20 meters per second.
But in this meta-world task distribution,
they're actually looking at things, you know, like,
picking, [NOISE] pushing, moving,
um, things that you would actually consider to be different tasks.
And they evaluate these algorithms that we've looked at on this,
ah, benchmark of tasks.
And, um, yeah, I'm
not- I actually haven't seen the results myself, I've just heard about it.
Um, so I would say that in general,
um- so off-policy RL to begin with is, is,
um, less stable than on policy,
ah, partly for like the same distribution shift reason
that we talked about earlier, right?
Like you're expected to learn from data that's not from your current distribution, right?
Um, so that's, that's one problem.
And the second problem going against it is the meta-training optimization is itself hard,
and that's hard even just in a multi-task setting.
So for example, forget about the adaptation stuff and just try to train a policy that
can do 50 different tasks given some indication of what task it's solving.
That in itself is a difficult optimization problem
because what ends up happening is the gradients for the different tasks cannot, kind of,
interfere with each other and, like, kind of,
lock and prevent the,
the model from learning anything.
So I think those two problems combined are,
are what's causing the issue and I,
I think looking at each of them in isolation will,
kind of, yield progress that we can then put back together. Yeah.
So I kind of have two questions,
the first is when you are like storing the, um,
experience, like the replay buffer,
do you store as well like the optimal Z, like,
like variables that you would find after doing
the inner loop updates step to the replay buffer?
[NOISE] Um-
Or is that not part of the experience- [OVERLAPPING]?
No, but, um, sort of,
in the sense that,
um- so, so sort of yes,
but through the data rather than through the Zs.
So what I mean by that is,
when we collect data, we collect,
um, some of it with a policy condition on a Z from the prior,
and some of it with a Z that's conditioned on,
um- sorry, with a Z that's computed from posterior trajectories.
So essentially, you get that effect,
but it's through the data in the buffer.
So you just start the [inaudible] from the data?
Uh-huh.
Okay. And then the second question was,
if you were to start at Z, um,
can you use that somehow as a way of, like,
prioritizing the experience that you're sampling from the replay buffer to not have
this distributionship problem because you're only going to select, like,
experience that's similar to the task that you're currently doing, um,
based on the latency variable that you are using in the posterior,
that I need to be able to stay current.
So you can do, like, a sort of, like, ah,
probabilistic sampling over the experience using Z.
Um, yeah so doing some kind of like priority- prioritization over the replay buffer. Um-
Like that, I don't know. You want it to- [inaudible]
So yeah, I'm not sure how addressing the distribution shift, shift explicitly,
I mean- I guess- So okay, we're kind of,
um, we're kind of,
we're kind of doing that, right,
when we sample the context from the replay buffers so that SC thing is,
is representing like a sampling operator,
which, essentially, only samples the most recent things so that the data is,
is on policy as possible.
Um, so that seems like,
kind of, what you're getting at in terms of addressing the distribution shifts.
Um, yeah, in general,
there's- I mean, there's other techniques for, like,
how you might better sample from the replay buffer that you could
probably marry with this. Yeah.
[inaudible] so at test time you don't sample
any other experience from the- from like training at all?
Or it's just basically just that same experience they are collecting on all [inaudible].
Right. Because, remember, at test time it's a brand new task.
Yeah.
Yeah. So one- um,
well, okay, that's another thing,
but, yes, yeah, sorry.
[inaudible] you're training both your encoder and your SAC in the directory, right?
Uh-huh.
So the actor-critic, when it comes to a complete variable is,
kind of, tracking the new new target, right?
Because your embedding is changing during the meta-training.
Yep.
Do you use, like, target networks
or anything [inaudible] or does everything kind of blend well?
Yeah. So SAC uses a target network, um,
already, and we just- yeah, we also use that.
Um, yeah, it is a bit of a moving target.
Um, one thing you could've considered doing is, ah,
if you- like if you are not using the Bellman error to train the encoder,
if you're using, like,
reconstructing the rewards and the transitions,
then you could train that offline beforehand,
right, and then do this.
Um, and I haven't experimented with that in particular,
I've experimented with training it all in parallel,
but with a reconstruction-
the reward reconstruction objective instead of development error.
And it, um, it does a bit worse.
Yeah.
So also at meta and test time you're sampling
your context framework at the beginning of the episode,
and then preconditioning your context distribution on,
like, the trajectory that you set,
could you imagine if you- at every step of your episode,
re-sample your context parameter?
You lose your coherence of [inaudible] , I don't know if you tried that.
Yeah. I think I spec- so it,
it- that does work, um,
in past situations where exploration is not as important.
So for example, in the MuJoCo benchmarks,
where you get, um,
dense rewards, ah, at every time step that actually works fine.
Um, but it, it doesn't work, for example,
in the sparse, uh- this guy, this case.
Um, but in general,
I think getting an off-policy method that can learn
op- that has that property of being able to learn optimal exploration trajectories,
is kind of an open question.
I haven't got anything towards that yet.
Actually, if you want to carry over some statement across all of
the different tasks, like, [inaudible] squared?
So in RL-squared you don't carry the hidden state across the task.
Okay.
Only the episode, right, because
the hidden state in RL-squared is supposed to tell you what task you are in,
essentially, or it's supposed to give you some task-specific information.
Um, so one interesting idea which we tried and didn't work, um,
is sort of getting at this idea of, like,
can we use different data from different tasks for,
for- so sorry, can I take data from task A and,
like, use it for task B?
Like why can't I do that? That's annoying.
Um, so if you wanted to do that,
what you'd have to do is, like, relabel the data, right?
So say- because say the reward function changes between task A and task B.
If I could relabel task A data with a task B reward function,
I could use it, and that'd be great, right?
Um, so what we tried to do is,
essentially, put a model for the rewards that change from Z.
So basically have Z reconstruct the rewards,
and so we would, essentially, have a generative model of rewards,
and we train this across all of the meta-training tasks.
So now, it should be that given a Z,
I get some reward function,
and then I could use that reward function to relabel some data.
Um, it didn't work.
But I think in- I think figuring out how to reuse data
from tasks for each other
would be really useful in terms of making this more sample efficient.
In theory, [inaudible] conditional relabeling of the rewards, right? [OVERLAPPING]
Yeah. Yep.
So in that case, like, why does that work?
Um, is it because they're not translate [NOISE] to
a generative model [inaudible] in that case?
Yeah, probably.
Um, yeah.
I mean, if you're in a sparser reward case than your, your,
ah- like any kind of signal is going to help you a lot,
versus if you are, in this case,
where you have some labeled data and you just wanna,
kind of, generate more to help you.
Like, it's, it's less clear if it'll help you, I guess.
[NOISE] Cool.
Um, I heard this ends at 2:50.
So thanks all for coming.
 So let's get started. First, ah, some logistics.
So Homework 3 is due tonight, ah,
and that's the last homework assignment beyond your, ah,
your projects and the project milestone.
The first milestone, um,
the only milestone is due next Wednesday.
Ah, and then after that,
ah, you will, um,
just have the, the poster session and the final, um,
presentation or the final, ah, report.
Uh, we'll be sending out feedback on your project proposals within the next few days.
Great. So, uh, let's go through the plan for today.
So, uh, today, we're gonna be talking about model-based reinforcement learning, um,
and how it can be used for multitask learning and meta-learning,
and also how it contrasts with model-free learning,
which is the kind of reinforced learning that we've been
talking about thus far in the course.
Um, we'll also be talking about how we can extend
model-based reinforcement learning to image
observations or other high-dimensional inputs.
Uh, this is one of- ah, a very challenging use-case for,
for our model-based reinforcement learning, and so,
we'll, um, we'll be covering that in, uh, in more detail.
And then, we'll be also talking about model-based meta reinforcement learning,
um, and settings where that might be applicable.
Ah, and also just kind of- by the end of this lecture,
some of the things that you'll hopefully be able to learn about are,
ah, how to understand and use, ah,
and implement model-based reinforcement learning methods, uh,
challenges and strategies for
model-based reinforcement learning with high-dimensional inputs,
and also, um, how this relates to multitask learning and meta-learning.
Okay. So first, let's talk about, uh,
reinforcement learning algorithms from a, a broader view.
Uh, so in previous lectures,
we showed this diagram where we looked at
reinforcement learning as an algorithm that, uh,
iterates between generating samples,
fitting a model to estimate the return from those samples,
and then using that to improve the policy.
And we talked about Q-learning based methods and policy gradient
based methods that correspond to estimating a return,
uh, or fitting a Q function.
Ah, and in contrast,
model-based approaches try to fit a model of the dynamics.
Ah, then each of these approaches improve
the policy by either applying the policy gradient,
uh, by taking a,
a max over your Q function,
or to improve your policy, um,
or to optimize actions or optimize the parameters of your policy using your model.
So previous lectures, we focused on
model-free methods like policy gradients and Q-learning.
Uh, and in this lecture,
we'll be focusing on what's known as model-based methods.
Ah, they're known as model-based methods because you're trying to fit this model of the,
what's known as the dynamics model.
Okay. Um, so the main idea of
model-based reinforcement learning is to learn a model of the environment.
Ah, and you might wonder,
hey, why do we wanna do this?
The previous reinforcement learning methods seem to work pretty well too.
Or maybe they don't depending on,
on what you found in your project,
or, or in your homework, um,
and there's kind of two main reasons I think,
um, at least from what I've seen, uh,
in terms of my own, uh,
experiments is that model-based reinforcement learning
tends to lead to better sample efficiency.
So if you care about, uh,
learning with not a lot of interactions in the environment,
fitting a model of the environment and then using that model to, uh,
optimize your policy can reduce the amount of data that you need in the environment.
Um, and this isn't true in all cases, ah,
but it's true- it has at least been empirically true in a number of, ah, different works.
And also, the model can be reused,
ah, for different tasks and different objectives.
And we'll talk a bit about what that means later in the lecture.
Um, and so, ah,
at a high-level what this- what these algorithms are,
are trying to do is they're trying to estimate a,
a model of the dynamics, ah,
and this just corresponds to a supervised learning problem
where you want to maximize the likelihood of the next state given
the current state and the current action for all of
the transitions in your buffer of data.
Uh, so for example,
you could treat this, ah, if,
if you have continuous states, ah,
and you wanna model the likelihood using a Gaussian, ah,
you could just use the,
the following optimization problem where you want to be able to, ah,
minimize the squared error between the predictions from
your model and the observed next state.
And this would be an example of a deterministic model.
You could also imagine using probabilistic or stochastic models as well.
That actually try to model but that that likelihood,
um, model the full distribution of the likelihood.
Okay. Um, and so,
this is kind of the- ah, there's different ways,
different model classes that you can use and different, ah,
ways that you'd go about maximizing that likelihood
but it typically just amounts to a supervised learning problem.
Um, and then, once we have our model,
we use that to improve our policy, um,
and I'll talk a bit about the different ways that we can do that in a minute.
Um, and then you can use that policy or use
the actions that you optimize to generate samples,
ah, and repeat this process.
[NOISE] Okay.
So now, what does this have to do with multitask learning and meta-learning?
So, um, let's go back to our notion of what a reinforcement learning task is,
ah, and in particular, we considered, um,
this reinforcement learning task is basically an MDP,
where different tasks may have different state spaces,
different action spaces, different initial state distributions, dynamics, and rewards.
Um, essentially, these, these tasks correspond to MDPs, um,
and one kind of observation is that in
many practical scenarios that we might care about, ah,
in multitask reinforcement learning and in meta reinforcement learning,
ah, it may be that the dynamics don't actually vary across tasks.
Uh, that there's basically one single dynamics model that governs the, ah,
governs the world that your agent is living in, ah,
and if this is true then we may be able to,
ah, kind of exploit that property.
Uh, so for example, in the real-world,
if your agent is manipulating objects,
or if it is, ah,
ah, walking around on the ground,
or if it is navigating, um,
in an environment the dynamics of the world- of
the underlying world for manipulating different ob- ah,
for doing different things in different objects,
for getting to somewhere in the environment through locomotion or through navigation,
in all these settings, the underlying dynamics of the world isn't necessarily changing.
Um, of course, when the environment is fully observed.
If you can't fully observe,
ah, phy- physical information about the objects,
or about locomotion, ah,
then there may be some variation across,
ah, across tasks, or across objects.
[NOISE] Another example of this is character animation.
So if you wanna animate a character to do things like, ah,
spin clicks, ah, spin kicks,
or cartwheels, or, or running, or back flipping, ah,
all of these- this agent lives in a single world with
consistent dynamics and what's varying is just the reward function and not the dynamics.
Ah, and likewise, if you, ah, have an agent that wants to, ah, converse, ah,
and accomplish a certain task through dialogue,
ah, such as helping you, um, order dinner,
for example, or helping you, um,
reserve a, a car reservation or something, ah,
the underlying dynamics of interacting with that person may be the same,
but the reward function of what you wanna accomplish is varying.
So here are a few examples of where the dynamics might be consistent across tasks,
ah, and all of these cases,
estimating the model is a single task problem.
If there's just a single model,
we only need to estimate a single function.
And so, as a result, then this learning problem may actually be easier than, ah,
some of the multitask model-free methods because we
only have to solve the single task learning problem,
and then once we solve that single task learning problem, um,
we can use that to, ah,
find a policy that optimizes, ah, different tasks.
Any questions on, on this?
All right. So um, how do you actually go about using our model to optimize for actions?
So uh, we wa- we want to be able to kind of optimize actions using the model,
and, and our objective might be to maximize our reward summed over time.
Uh, and so one way that we might think about doing this is,
uh, we can use this form of
computation graph where we have actions being passed into our model.
Our model is predicting the next state, um,
which is, uh, producing the reward function and also producing, uh, sorry.
The model is estimating, uh,
the reward function may also be estimating
the next state which is then passed to our policy
to produce the next action and the next, um, reward function.
And so if we want to optimize over the sequence of actions that,
uh, maximize our reward, uh,
we could imagine just backpropagating the signal from
our reward function into our actions through this computation graph.
Uh, and so for example,
we could use a gradient-based optimization over our actions to optimize for our actions.
Uh, so what this might look like is, uh,
you might run some policy,
for example, a random policy.
I collect some data using that policy,
and then fit a model to that data to minimize the, uh,
the prediction error of that model and then
backpropagate through that model in order to optimize for our sequence of actions.
Uh, and then once you have those actions,
you can just execute those actions,
uh, to accomplish the task that you'd like to perform.
Uh, so this is pretty straight forward.
Uh, another way that we could do this is say we don't want to use, uh, backpropagation.
Uh, for example, maybe the model that we learn,
uh, is doesn't have well conditioned gradients, uh,
or maybe, uh, it's discontinuous in someway,
we can also optimize the reactions via sampling.
Uh, this would be essentially a gradient free optimization over our actions.
Still, still the same underlying loss function,
the same underlying optimization,
we can just use a different optimization approach
for acquiring a sequence of actions that will maximize reward.
Uh, and so what this might look like is to run some policy,
learn a model to minimize model error, uh,
and then iteratively sample action sequences.
Run those action sequences through our model, uh,
and the action sequence you find that achieves the best reward,
we will then execute those corresponding actions.
Uh, and there are ways to uh, to sample action sequences,
uh, in a more intelligent way.
So you can imagine just sampling from some uniform distribution over action sequences,
and then taking the best one.
Uh, but you could also imagine after sampling from a uniform distribution,
you could take the best, uh,
10% for example rather than the best one,
and then refit a distribution around those 10- around those
top 10% of actions and resample from that distribution,
uh, and do this sort of iterative process to iteratively
refine the sampling distribution over actions.
Uh, that would be known as, uh,
something like the cross entropy method, uh,
and that would allow you to perform a slightly better optimization or, uh,
slightly better, um, slightly more powerful optimization over your actions.
Okay. So here are a couple different, um, approaches.
What's something that might go wrong with these approaches? Any thoughts on that?
Sparse reward.
Mm-hmm.
It's probably a vector.
Yeah. So if you're reward function is sparse and your optimization process isn't
able to sample action sequences that lead to
that reward or let that actually see any reward that it may be that,
uh, your optimization won't be powerful enough to actually,
uh, find a good sequence of actions.
So that's one good example. What's another good example? Yeah?
And so in the previous example,
you need to solve an optimization problem in order to get one single action?
Yeah, so in both of these cases,
you need to solve an optimization problem in order to
get a sequence of actions that will,
uh, that will try to maximize that reward.
[inaudible] propagated exponentially forward.
Sorry, can you repeat that.
When your accuracy is in the model you
[inaudible].
Yeah, absolutely. So if your model- I guess there's- I think there's two things here.
One is if your model is inaccurate,
then the optimization can exploit that and,
uh, and be overly optimistic about whether or
not an action sequence will accomplish high reward.
And second thing is that, uh,
if you're optimizing for an open loop sequence of
actions and then executing that sequence of actions.
If one of those actions, uh,
reaches a state that's slightly different from what you thought you would reach,
and then if at the next state you actually use another action,
you'll have this compounding errors such that you move
away from the trajectory that you,
um, that you thought you were going to follow according to your model.
Okay. Any other thoughts on what might go wrong?
Okay. Cool. So the approach that, um,
that I had written here was I guess there's a couple
of di- couple different things that we di- we discussed.
The first is that you can potentially have,
uh, imprecisions in your model, uh,
and that will cause you to,
um, to kind of be overly optimistic about what will happen.
And second, that these errors can compound and cause you to kind of go off track, um,
and have increasing amounts of errors as you roll out your,
um, as you roll out your,
uh, sequence of actions.
One thing that will help with this,
uh, I actually, do any of you have thoughts on, like,
how we might try to avoid some of these issues or,
or these two issues in particular? Yep.
So I have a question, intuitively it makes sense why this is bad.
But on a theoretical standpoint,
isn't this like minimizing regret by going
the most optimistic possible choice and then negating that if it turned out to not be true?
Yeah. So the, the key thing is
the second part that you said is that if that turned out not to be true,
we need to actually take that into account, right?
As on the algorithm that I listed on the previous slide,
we are actually just fitting a model to
our data and then executing actions according to that data.
But if we turn out- if it turns out that we take those actions and it was
actually not the correct thing to do, as you're mentioning,
then we should basically refit our model using the new data and use that to,
uh, continuously improve our model into settings where we're overly optimistic.
And so what you can do is you can go back to this previous algorithm,
and then actually execute those planned actions in the world,
append the data that you observed to your dataset,
and use that to refit your model, um,
to this, uh, growing dataset. Does that answer your question?
I guess, like, it seems like the answer is like that's really not that bad, is what you're saying?
Um, the- sorry.
You're asking the, um-
Just like making an overly optimistic decision we can just use that to our advantage through this?
So if you make an overly optimistic decision,
it can certainly- if that is,
like, if you're, if you're, like,
done running learning and you're not going to actually collect many more data,
then it can be very bad because you'll-
you won't actually be making predictions that maximize reward.
Uh, and it can be arbitrarily bad if, if it's,
like, erroneous outside of the states that you visited.
But if you have the ability to collect more data,
then you in principle should be able to correct for those errors.
Okay. So this is, uh,
this is one thing that we can do.
Um, but still, uh,
even if- so this will help certainly,
uh, doesn't completely solve the problem.
So in general, learning a good global model is pretty hard.
Uh, especially if you wanna learn a good global model everywhere, uh,
in your- in, in all possible states for example.
Another trick that can be helpful with this is,
uh, what's called re-planning.
So, uh, and this is, uh,
a fancier name for this is, uh,
model predictive control or MPC.
And what we can do is we can first run our policy to collect some data,
fit a model to that data,
use this model to optimize over our action sequence.
And what we can do is we can execute the first planned action,
observe the resulting state, um,
append this to our dataset.
Uh, and then after we take ones- one action,
we can then actually replan and reoptimize over a sequence of
actions from the state that we just observed.
And so what this can do is that if you end up at a state that you didn't expect,
uh, as a result of your model,
you won't keep on executing actions as if
you were in the state that you thought you would reach,
you'll then actually replan according to the state that you actually
reached out to try to correct for your mistake at that first time step.
And so this can help address some of
the compounding errors that we talked about before. Yeah.
What if it's the same dataset
or what if they're different like meta dataset?
Um.
[inaudible] or something like that.
Yeah. So we haven't got into the meta-learning setting yet,
but you could also imagine- so in this setting,
uh, there might be a loop here which is to, kind of,
refit your model in this,
like, slower outer loop.
Um, in here all- the only thing that you're doing is you're, um,
observing the state and then using your model to replan,
to like, reoptimize the action sequences from that state.
You're not actually- so you're using it to update your actions.
You're not actually using it to update your model.
And you could also imagine using that state to update your model,
uh, and we'll get to that actually at the- like,
towards the end of the lecture.
Uh, so we're a few steps ahead.
Um, so intuitively this can help with model errors because it can allow- if you, kind of,
go off track of where- where your model thinks you will end up,
uh, you could in principle start to correct for those.
Uh, so the- the benefit of this is you can correct some model errors.
One of the downsides of this approach which I think was maybe alluded to, uh, in one of
the other questions is that this is- in general planning for
actions is a fairly compute intensive process, uh,
because you need to continuously, I mean,
if you're planning at every single time step,
you're doing an optimization,
you need to do an optimization in real time.
Um, one thing that can help with this, uh,
is to optimize over,
um, instead of optimize over a sequence of actions,
you could actually back-propagate actions into your policy.
So if you have a parametric form of
your policies, instead of back propagating grad- gradients into the action,
you can back-propagate gradients into the policy parameters.
Um, likewise for a sampling based approach if you optimize for a sequence of
actions you could use those as targets to train a policy to produce those actions.
Um, and that can reduce some of the compute intensive challenges of things like MPC.
Um, now I also wanna get back to the problem of sparse rewards that was mentioned.
Um, learning a policy can also help with sparse rewards.
Uh, in that if you, um,
in aggregate at some point you see
a good reward function or a good reward then your policy,
uh, the policy parameters will be trained to try to, uh, accomplish those.
Um, in practice like with very sparse rewards model-based optimization and
model-free optimization will run into same- the same sorts of
issues if they don't actually ever observe rewards.
Um, and things like relabeling, uh,
as you saw in the pro- problem assignment can also help with that.
Okay. Um, any questions on kind of the basic algorithms before we talk
about the multitask setting? Yeah.
My understanding is that when people are more advocates of model-free,
say that model-free sometimes allows behaviors to emerge that
wouldn't have been possible model-based but the way that you've
presented it here where- where does that fall
apart for model-based such
that you can't get everything that you have with model-free?
So I actually haven't heard that argument before.
Um, so the argument you said is that there are behaviors that will emerge with
model-free that you won't be able to have emerge with the model-based method?
Yeah you're essentially, like,
confining yourself too much and confining the agent too much by
using the model-based method.
Yeah. I don't actually see why that would be the case.
Like, uh, in both cases you are, um,
you're optimizing some objective which is to maximize reward
and the behavior that emerges,
like, like- in both cases you are going to
be learning some behavior to maximize that reward function.
And I think that what comes out of that optimization is
more a function of this- the power- how powerful that optimization process is.
Uh, and if it's- if you have a very strong optimization process then, uh,
more interesting, uh, well,
and a reward function that's interesting then more interesting behaviors will emerge.
Uh, and I don't think that there's any difference, um- I think that, like,
any difference that you see in the outcome of
this model- of these approaches will more have to do with
the strength of the optimization and also how
hard it is to fit the model versus fitting a policy.
Um, there are definitely settings where it's
harder to fit the model than to fit the policy.
Uh, so for example if you wanna pour water from one container to another container,
modeling fluid dynamics, is it a hard problem?
Uh, but just twisting your arm is a relatively simple function to learn.
In those settings a model-free approach, um, may be easier.
In other settings, uh,
such as if you want to be able to, um,
push an object to any possible position, uh,
the dynamics may be relatively simple because it just corresponds
to one object on- on the table and the- the dynamics there.
Uh, whereas the policy may be more complex because for
any poss- you have to represent the policy for any possible goal,
um, whereas the dynamics are just- it's just a single task problem.
Okay. So we can look at,
um, what does this actually look like
and reusing this question about like what does
this have to do with multi-task RL and meta RL?
So, um, how you actually apply this to
the multitask RL and meta RL problem statement
depends on whether or not you know the reward function.
Um, and in particular, there are some instances where you actually know
the form of the reward function, uh, exactly.
Uh, so, um, we'll see an example of
this in a second and if you know the reward function for each task,
uh, then you could just learn a single model and plan with respect to
that reward function at each, uh, at test time.
Uh, so for example, um,
here's an example of, uh,
a work that was done by Anusha Nagabandi and colleagues,
and what they were looking at is, uh, they wanted to,
to learn how to write different trajectories with a pen and they were controlling the,
ah, the hand, ah,
in simulation and different reward functions
corresponded to different trajectories of the tip of the pencil.
And so in this case they assumed that they could observe the tip of the pencil and then
the reward function can be derived as just trying to track
a particular trajectory with the tip of the pencil.
So the form of the reward function is known,
uh, but optimizing that, uh,
the reward function I- by actually using the hand to draw is a very challenging problem.
And so they learned a model, um,
and they actually learned a model by collecting data with
this vector random trajectories and then at test time
they gave it reward functions for writing different digits.
Uh, it's a little bit hard to see. I think the first digit is a six,
the second one is a seven,
the next one is like a nine or a four,
and the last one is a five.
And you can basically use that model to plan to accomplish these different,
um, these different trajectories.
Um, another example of this is, uh,
maybe you want the hand to be able to manipulate,
uh, these bouncing balls, like, ah,
move them in a circle for example,
here the reward function is also known it corresponds to
the trajectory of the- of the two balls.
Uh, and you can also have a reward function corresponding to moving- uh,
the ball to a particular location in the- in the palm,
or also, um, rotating the balls in the opposite direction.
Cool. So here- here's an example of, kind of,
multi-task reinforcement learning with a single model and different reward functions.
Um, one caveat that I'd like to mention here is that, uh,
even though the the dynamics may be consistent across all of the tasks,
different tasks may require you to visit different state distributions.
And so if you collect data for
one task and another task is a very different state distribution,
then the model that you learned for that one task
may not actually generalize to the second task,
if it doesn't visit the,
uh, the same states.
Um, so the reward may change how you collect
the data and may affect the quality of your model in other states.
Um, in this work they found that if you actually train
a model only on this first task, uh,
that model can actually be reused for the second two tasks because
the distribution over states is sufficiently diverse.
What is the state representation of that case?
Um, in this case,
the state representation corresponds to the position of the two balls and the,
um, state information about the hands such as the joint angles.
I'm not- I'm actually thinking about this more- a bit more.
I'm not quite sure how they go from two balls to
one ball cause the state representation changes in that case.
Um, I would guess that maybe they just use the,
the model corresponding to one of the balls and they ignore the second one,
um, but I- I'd have to check the details of the paper for that.
They all seem like of different sizes.
It's possible that they're different sizes.
I- my impression was that they were, um,
the same size and maybe the videos- actually,
so the hand is also bigger in the left video.
So I think it's just that- that the video has been scaled differently.
Um, another cool thing about this approach is that, ah,
because it's able to learn a model, um,
pretty quickly with a relatively small amount of
data they actually were able to run this method on a real robot, ah,
and actually run the reinforcement learning process and collect
all the data on a real robot and fit a model to that, um,
and were able to get a,
a real shadow hand to perform this task.
Cool. Yeah.
When did the video come out?
When did this get done?
Um, it's actually pretty recent work.
Well, so I, I know Anusha well,
so I've seen it for awhile but,
I think that the video came out like within the last month.
Um, it was published at CoRL which was last week,
which is why I was actually not here. Um, yeah.
Okay. Cool. So that's what happens if we know the reward function.
Uh, what if we don't know
the reward function or the- at least the form of the reward function?
Um, one thing we can do is we can just learn a reward function, uh,
conditioned on the task and then use that reward function to plan to accomplish tasks.
Ah, and I have a typo on the next thing but,
ah, this should say meta-RL.
Uh, the only thing that you could do is you could
meta-learn a reward function from a small amount of
data and use that to- use that learned reward function to plan to accomplish goals.
Uh, this is pretty straight forward. Uh, one example of the second case, um, is,
some work here where the,
the training dataset corresponds to a few examples of the goal.
Ah, so in this case the goal is to, ah,
place the pencil case to- on top of
the- or behind the notebook and given a few positive examples,
you wanna be able to learn a classifier or a binary reward function that
corresponds to whether or not the task has been accomplished in the image.
Um, so you can, you can do this task with Meta training but,
but by- with meta-learning by collecting a dataset
of a bunch of positive and negative examples for different tasks,
meta learn your classifiers such that given
a small number of positive examples it can quickly learn a new reward function.
And then once you have that reward function you can plan
using your model to maximize reward.
Uh, so here's the kind of the result of running, ah,
planning and executing those actions on the robot, ah,
to accomplish the task with respect to this,
ah, meta learned reward function.
Okay. Um, and I guess kind of the,
the bigger takeaway here is that, uh,
model-based RL solves both the multi task RL problem
and the meta RL problem statements with these,
ah, with these different types
of approaches depending on whether or not you have the reward function or not.
Okay. Any questions before we move on to image observations?
So one thing worth mentioning here that I didn't actually tell you is that,
this is, this is all from images.
And so how do we actually go about doing model-based RL when we have image observations?
Right. Um, so, ah, in particular, ah,
if you only have access to images you might
have a graphical model like this where you can actually observe
the underlying state- the low-dimensional underlying state
of the world and you can only observe the O's shown here.
Ah, so for example maybe I have a robot that
looks like this and you want it to be able to like use
the spatula to lift up an object and put it into
the bowl and all it can observe is this image.
Um, so first, ah, with,
with these images we have to deal with learning models,
ah, in, in some space at least in learning how to predict.
And second we also don't have any reward function necessarily if we only have
those observations and so we need to think
about how we might go about learning our reward function,
um, such as using the meta learning approach that I showed on the previous slide.
Okay. So one option, ah,
for the reward function is learning
an image classifier like I showed before or to meta learning an image classifier.
Um, another example or another option is to provide an image of the goal.
Ah, and this would co- correspond to the goal condition reinforcement learning setting,
ah, that we have covered previously and that you looked at in your homework.
And so for example, you could give the, ah,
the robot an image of the goal like this, uh,
saying I want you to, to accomplish this,
ah, this goal state, ah,
and have it try to,
to reach that goal state.
So how might we go about doing model-based RL in the setting?
So there's a few different classes or approaches that we'll cover.
Um, the first is to learn some latent representation
and then learn a model in that latent representation.
The second is to try to learn a model of your observations directly.
Ah, and the last one is to try to predict
alternative quantities other than your raw observations.
And we'll talk about all three of these approaches.
Ah, so first let's talk about latent space.
So, ah, the key idea of learning in
latent space is that you wanna learn just
some embedding of your observation which we'll
denote as g of o,
and then learn a model inside that embedding space.
Ah, so if we take the graphical model that we showed before,
this corresponds to trying to, ah,
learn some form of inference network that maps from your observations to
your- back to your low-dimensional states or back
to some representation of your state space.
Um, so then, ah,
kind of the way it works I guess is, ah,
first there's a couple of papers that have looked at this kind of approach.
Um, shown here that we'll talk about and they're- more recently there are
a couple of other approaches that have looked at this kinda approach as well.
So um, the way this algorithm works is first you run some policy to collect some data, ah,
and then you learn this latent space of your observation and,
uh, a model in that latent space.
So you'll learn a G to go from O to S and then you learn,
learn a model that goes from S and A to predict S prime.
Then you use your model to optimize over a sequence of actions.
Ah, and then execute those pan- planned actions,
append the visiting tuples, ah,
in this case it may be the,
the state and action and the next state or it might be the observation,
the action and then the next observation.
Ah, and then you can add those, ah,
tuples and then re- retrain your embedding space and retrain your, ah, your model.
All right. So this is pretty straight forward.
Ah, there are a couple of questions though.
Um, the first question is what is
your reward function when you're trying to optimize over your actions?
Ah, so we talked a little bit about how
your reward function could correspond to a classifier,
um, in the case where you're given an image of the goal,
one thing you can do is you can use your reward signal as, ah,
some reward signal of your actions such as trying to minimize effort or
torque plus a distance function
and that there should be a negative sign in front of that.
Some distance- some negative distance between, ah,
the goal of your current observation and- sorry, the,
the representation of your current observation and the
representation of your goal observation.
So you can basically use the negative distance in your latent space
as a reward function for planning.
Okay. Um, and this makes the assumption that distance,
distance in your latent space is an accurate metric for the things that you care about.
Ah, and this, this assumption may or may
not be true depending on the form of your latent representation.
Okay. And then the second big question or
maybe kinda the most salient question is how do you
actually get this latent representation space?
Um, there's a couple of
different pe- approaches that people have taken to try to do this.
Ah, one of the more popular approaches, ah,
that was looked at in 20- 2015 and also actually more recently is to try to,
ah, form a graphical model of your transitions.
Ah, and this basically corresponds to a variational autoencoder that looks at
transitions over pairs of states
rather than a single variational autoencoder for a single state.
And as a result what you get is you're optimizing
jointly for the latent representation, ah,
of your variational autoencoder for example as well as the transition,
ah, distribution, your, your model in that latent space.
Ah, and so if you do this you can, ah, kinda get,
get a representational space that is good, ah,
both- that is both low-dimensional as well as,
ah, satisfies your model effectively.
Ah, and so for example they showed that,
ah, you could use this,
use the algorithm with this latent space for accomplishing different kinds of goals.
So, ah, the goal image was shown on- at the beginning of the video and
then in each case it's trying to reach
it's- executing actions that tried to reach that image.
So we'll see another example in a minute.
So here the goal state is to, ah,
curl up the arm and the left is showing the executed actions and
the right is showing the one-step predictions of
the reconstructed image through that generative model.
Ah, and so one of the things you can see is that it can, ah,
use a single model in a certain-
a single layer representation to accomplish these different goals.
Um, the other thing worth mentioning is, ah,
kinda alluded to at the beginning of this lecture that
the model-based methods tend to be fairly efficient.
This approach required about 300 trials to learn, ah, these,
these skills from the pixel representations which if you were to run
that in the real world it would probably correspond to about 25 minutes of real time.
Which is pretty fast as reinforcement learning goes.
Um, in practice, some of the more recent model-free methods take
around two to three hours to learn. Yeah.
[inaudible] I'm guessing you're also referring to SOLAR as a more recent work?
There are a couple of more recent work, SOLAR is one of them.
There's also, um, Stochastic Latent Actor-Critic which was covered, um, more recently.
That one's kind of more of a hybrid method but also has this form of graphical model.
[inaudible]
Yeah.
[inaudible]
Yeah.
[inaudible]
Yeah. So the question is this method and some of the,
the predecessors, uh, or successors of this method,
um, places assumption on the latent space,
which is that you can linearly or you can basically have
a local linear model on that space like it can
accurately predict the next state given the current state,
um, where local means that you may have a time-varying linear model.
Uh, I think that theoretically speaking,
if you have a universal function approximator that's producing your state,
it seems like it should,
it should be possible or in the latent space that is,
that does actually satisfy that constraint.
Um, in practice it
may be- that may be a very difficult optimization problem to actually find that,
that latent space and we haven't yet seen these kinds of methods
perform well on very diverse settings where you have, um,
many different objects in the scene and,
and, and, uh, kind of,
the kind of diversity and complexity that you see in like natural images and,
and in, uh, and in like ImageNet images for example.
Uh, that is actually- oh,
I'll come back to that point in a second as well, um,
as we talk about kind of modeling and latent space versus modeling
and observation space. Yeah.
[inaudible] like PlaNet?
Yes, PlaNet, PlaNet is also learning,
uh, a latent representation and doing planning in that representation.
The PlaNet approach that they use,
um, I can't remember the exact details of which PlaNet approach they use.
I think it actually may have corresponded to a model-free algorithm.
Um, sorry, what?
[inaudible]
Oh, Okay. Right. Okay. Yeah. So using- right.
Okay. So it's using cross entropy method, uh,
which is basically the iterative sampling based approach that we talked about.
PlaNet, PlaNet is also kind of has a very similar form of
this graphical model although in addition to having
the stochastic pathway they also had a deterministic pathway in their model.
Um, the PlaNet to my knowledge wasn't tested in the multitask case.
Uh, it was just tested in a single task case but in principle that
the model that's learned could also be used in the multitask setting.
Yeah. So there's been a number of approaches recently that have kind of
followed this form of kind of learning a latent space with
some sort of probabilistic or semi probabilistic approach
and then doing learning in that latent space.
Um, one other example of,
um, a latent space that, uh,
we used in 2016 and also was actually has been studied more recently
as well is having representations that are have structure to them.
Um, so in this case the structure that we're looking at were latent spaces where the,
um, the, the dimensions of the latent space correspond to key points in the image.
Uh, so for example, here are two example, uh,
two example key points that are in the representation and this is the trajectory that,
that, that representation follows as the robot executes the trajectory.
Um, and this- more recently there's been a kind
of a trend of approaches that try to learn object-centric representations or,
or key point-based representations, uh,
of images and then perform planning or perform
model-based RL in the context of those representations.
Um, and so this is kind of maybe alt- an alternative view.
I think that both of them have their merits.
Uh, and oh, yes, specifically for,
for this, this approach to the way that you actually try to get those feature points, um,
is you can take,
you can take the last convolutional layer of your network perform a softmax over
the spatial extent of the image to get a distribution over 2D positions in the image.
And then once you have the, uh, all these distributions over key points you can then,
uh, here's an example of a softmax for
the softmaxes over the x position and the y position.
Um, then you can take an expectation where you compute the, uh,
an expectation over that 2D distribution to get the x,
y coordinate of the, um,
of the, of the approximately the,
the key point of maximal activation.
Uh, so you can essentially view this as a form of spatial softmax.
Uh, so instead of doing a softmax over a one-dimensional operation,
you can do a softmax over a 2D space, uh,
and get some, uh, key point out of it.
Uh, and of course I guess the,
the important part is that this operation is actually
fully differentiable so you can, uh,
optimize for these kinds of representations,
um, with respect to the, um,
the objective that you care about what the- things like
reconstruction or something like the objective of your task.
Uh, and there are also other kind of, uh,
more recent approaches that have looked at, um, kind of other,
other ways of getting these key point-like representations
or object centered representations,
uh, in unsupervised ways or in like weakly supervised ways.
Um, and so what- the result that you get out of
this is if you train it to do reconstruction here,
two of the feature points that you might get out of it,
um, out of the in this case the 16 feature points that were used.
Okay. Um, and I guess there was also- I said smooth here
because there's also an auxiliary loss to
encourage these are representations to be- to have,
um, similar to have constant velocity, uh, through time.
Um, but I think that that detail maybe a little
bit less important.
Yeah.
[inaudible].
So in this case the number of
feature points tot- the number of total feature points need to be pre- predefined,
um, as part of your network, network architecture.
So in this case the number of feature points is, is 16.
Um, and the dimensionality is 16 times 2.
Uh, one thing you could imagine doing, uh,
that we actually did explore in this paper as well as have
a larger number of feature points and then prune
the feature points according to some metric.
Uh, for example, according to, um,
if some feature points you observed to be very noisy,
then you could prune those feature points out
if you don't think that they correspond to in,
in an automatic way if you don't think they correspond to, um,
things relevant to the task. Yeah.
[inaudible]
So in this case it was, uh,
completely unsupervised, um, setting.
So the, uh, the goal,
this autoencoder was, uh, it's an autoencoder,
so it was- it's, it's, it's,
its loss is to reconstruct the image.
It needs to find these 2D key points that allow it to reconstruct the image.
Um, and because different images have different positions of objects then, uh,
extracting the positions of objects or actually
extracting the positions of things that change in
moving the image lead to, um, good reconstructions.
So essentially, just a constraint on your latent space to have, to,
to have it be representing these 2D positions and how it uses that,
that representation space is up to the model.
What if you have [inaudible]
Yeah, so, uh, occlusions is challenging.
Uh, one thing you could imagine doing is having a recurrent model,
uh, and then the recurrent model in principle could try to track it.
Um, in this work one of the things we did to deal with
occlusions was we used a filtering based approach.
Um, we basically did a form of filtering
where you can basically look at the, the softmax distribution.
If it's very peaked, then it's likely that the,
the point is in view.
If it's not peaked, then it's likely that the point is occluded,
and if it is occluded,
then you can actually use your model to fill in, uh,
like a Kalman filter style update to fill in where you think that point is.
Yeah. So there's a lot of potential details here that I wasn't planning to cover,
but it's- yeah, there's different things that you could imagine doing.
Um, another challenge with this type of approach like that,
I guess is worth mentioning,
is that if you have two objects that are identical, um, this softmax,
since it's over the entire extent of the image,
it's going to have two peaks,
uh, if they look identical.
And therefore, if you take the expectation,
then you'll either get the average or get- you'll get the point that dominates, um,
or you might get this flickering between the two points and so there isn't, uh,
necessarily a satisfying way to deal with that. Yeah.
Is there a type of reference or is this to the, um-
Oh, this is actually from 2016.
Oh.
So the paper reference is there.
Um, so here's an example of the learning process.
So, uh, we actually- in this work we
gave it both the goal image as well as the goal position of the arm,
we initialize it with a policy that could mo- that
could reach the position- the goal position of the arm but not the goal image.
Uh, and then this is the course of,
uh, model-based reinforcement learning,
whereas in this case it's actually optimizing for a policy that
tries to reach both the goal image and the goal position of the arm.
Uh, and this is a toy task where the goal position with, uh,
the goal position is just to reach,
uh, to push the cube over to the left.
Uh, and then the final policy you get looks something like this where,
um, it's able to push it to the specified position on the,
um, on the mat.
The colors actually seem off.
This is supposed to be green.
But, um, anyway, uh,
then you can also perform the task the, the,
the specialized task that I showed before where the goal is to,
um, get the spatula to be in the bowl.
Cool. So one of the nice things about this approach
also w- with these sorts of key point-based representations, I don't,
I guess I'm spending more time on the key point-based representations,
but I don't think there's necessarily one approach that's necessarily better than the other,
but one thing that is quite convenient about this is you could
actually visualize the key points on the image.
So in this case, the X's correspond to the goal position,
and the blue- and the circles correspond to
the current position of the key points, and you can,
uh, with this ability to,
to visualize them directly on the image,
it makes for a very interpretable representation,
uh, and this is very useful for debugging.
Uh, because if your representation isn't
capturing and isn't tracking the objects that you care about,
then it's likely that, uh,
your algorithm won't work because your representation isn't capturing those things. Yeah.
So the key points are in the latent space, right?
Yes. The key points are the latent representation.
Okay. Um, so these skills were learned with
about 125 trials which corresponds to about 11 minutes of robot time per task.
Uh, in this case, the representation was actually learned,
uh, per environment or per task.
Uh, so it's kind of learning this environment specific latent representation.
Um, and so as a result the representations can
become somewhat specific to that environment.
And so for example, if you took this- these representations and try
to use it for a different task like the Lego block task,
they wouldn't necessarily track the objects,
uh, because they weren't trained on those images.
Okay. Cool. So one thought exercise.
Uh, so both the approaches that we looked at
were auto-encoder type approaches like generative models
where we're predicting the f- the, uh, the image.
We're trying to generate the image through some bottleneck,
and we may also be learning a model on that bottleneck.
So one question is, uh,
why do we need to reconstruct the image, right?
Uh, why don't just learn some embedding space like the feature points,
and then also learn a model on those feature points,
and train the representation such that the model is accurate.
This seems like somewhat of a reasonable approach.
Uh, why is this maybe not a good approach?
It's just difficult in practice?
Optimizing with respect to model error,
for the representation would actually- it's actually definitely a solvable problem.
Is the [inaudible]?
Um, right.
So the embedding does depend on the problem that you're solving.
And so if you're optimizing with respect to model error,
then it, uh, it may
not captu- it'll capture things about the model and not necessarily about the task. Yeah.
Um, if it's in latent space, it may not capture some small deviations in the real image?
Um.
[inaudible].
Yeah. That's actually also a problem with- that's, that's- yes. It's definitely a problem.
It's also actually a problem with reconstruction based approaches,
and we'll see that in a second. Yeah.
[inaudible].
Uh-um. And so what does- what does that mean?
So that means [inaudible].
Uh-um. And so what happens if
you optimize for both the embedding and
the model with respect to the error of the model?
[inaudible].
Sorry, what?
[inaudible].
Yes. Yeah. So there is a solution to this.
So this, this- to- basically to
the model error objective which is that if your embedding is always the same thing,
if it's a constant, uh,
if it's like always zero for example,
then it's perfectly- it's very easy to predict the next state, right?
Because this is just always zero.
Um, and then as a result- I mean,
that embedding isn't very useful because it's a constant, it's always zero.
Uh, but it achieves perfect model error.
Um, so there's basically this degenerate solution
that comes up if you try to optimize with respect to model error,
um, with- for both the embedding and the model.
Um, so it's not really a, a good idea.
[LAUGHTER] So this is why we need kind of other,
other forms of objectives to optimize for these representations in
addition to model error so that you can avoid that degenerate solution.
Okay. Does that make sense?
What is the other entropy [inaudible].
Yeah. So that's actually, uh, an interesting point.
So if you add, um, an entropy term,
interestingly that will actually correspond to maximizing the mutual information between
your representation and your, uh, your observation.
Um, so the- out- well, if you, uh,
if you want to maximize your mutual information between
your image observation and your latent representation,
uh, you can show that this is equal to, uh,
H of z minus H of z given S. Um,
so this would correspond to maximize the entropy like we've said,
and this would correspond to,
um, being able to predict z from S. Uh,
and so that's actually a pretty good thing to do,
and a lot of people have looked at these types of
objectives for learning representations.
Cool. Um, so to wrap up the latent space approaches, uh,
the benefits of this approach is that you can learn pretty complex skills, uh,
very efficiently and some structure- structured representations
enable very effective learning of these tasks.
Um, the downside is that, uh,
or I guess one of the main downsides is that we need
good objectives for learning these representations,
um, and things like reconstruction objectives
may not actually recover the right representation.
Uh, so as an example of this, uh,
when I was doing those experiments a few years ago with the- the spatula task,
we also wanted the robot to do another task which,
ah, was to manipulate a ping-pong ball,
uh, by like basically,
like kind of kind of-transferring it from one container to another container.
And so here's a downsampled image of, uh,
of that experiment where the white dot corresponds to the ping pong ball,
and the- you can see the arm of the robot as well.
And so I trained an auto-encoder on these images,
and the reconstructions that I got out of it look like this.
Uh, and so what you can see is that it learns to,
uh, learns a very good eraser of the ping pong ball.
Uh, and instead just learns to reconstruct the arm,
uh, because that's the thing that's larger in the image.
And so there's this mismatch between the objective of
the representation learning and the objective of the task that you might care about.
Um, so kind of the takeaway here is that we may need
better-unsupervised representation learning methods, uh,
be it reconstruction based methods or,
um, like mutual information based on the objectives.
Okay, and then one other side note is that
low-dimensional embeddings can be- also be very useful for model-free approaches.
Uh, so for example,
you could learn a- a low dimensional embedding and do model-free in that latent space,
model-free RL in that latent space.
Uh, so there's work back in 2012 that did this for this,
um, this slot car.
Uh, where they trained an auto-encoder and then to- down to
a two-dimensional representation and then did fitted
Q iteration on top of that two- two-dimensional representation.
Uh, and this work actually predates things like DQM,
so they're doing deep RL back in, back in 2012.
Uh, here's an approach that was able to run TRPO,
which is typically an algorithm that requires a very large number of samples.
Uh, but they learned a latent space of- actually,
both the state and the actions,
and were able to run TRPO on a real robot,
uh, to throw an object to hit the Pikachu.
And then there are also methods that use an embedding for their reward function.
So we talked a little bit about how, uh,
in the previous approaches we're using the,
the- embedding both for the state representation and for the reward representation.
Um, in this case,
this work was looking at, uh,
acquiring a reward function from ImageNet features.
Uh, so this is actually,
a supervised representation learning method.
They took, uh, they took a video of a human opening a door,
run that through ImageNet,
and then used that as a reward function for
a robot to try to reach the same features that the,
um, that the video on the left was, uh, was reaching.
Um, and the one other thing worth mentioning is that if you ha- if you have a reward,
you can actually predict it to form a better latent space,
and this can- this is one way to kind of help solve that degenerate solution prob- um,
degenerate solution that we observed if you just try to predict model error,
and there are a number of approaches that have looked into that as well.
Um, one reason why you may not want to predict reward,
is that maybe that you don't have a good reward function.
Um, so in the case of the spatula,
in the case of the, um,
the embedded control paper,
if you just have goal images we don't actually have reward functions.
Um, but if you do have a reward function it's, it's good to try to use it.
Okay. Um, now that we've talked about latent space models,
let's talk about modeling things directly in your observation space.
Uh, so we can recall the,
um, kind of the model-based RL approaches that we mentioned before.
Uh, in this case,
this is just the,
the same MPC algorithm that I showed before,
but where all the states are replaced with our observations O.
And what we can do is,
we can learn a model on our observations,
uh, and, and plan with that model.
So, uh, first we wanna run some policy,
uh, to collect some data.
Uh, so for example, we could collect data that looks like this.
Um, this is just robots randomly interacting with,
with objects picking them up and such,
and then the data corresponds to the images and actions.
Uh, and so it's very easy to collect data like this,
you don't need reward functions,
you can just run, run off your,
your robot or your agent in whatever environment.
Uh, then you can learn a model to minimize prediction error.
And so this corresponds to a video prediction model.
So you may get predictions that look like this for
different actions that are run through your model.
Um, also because we're, uh,
we're not imposing any representation on our,
uh, on our, uh, state.
We can also apply these sorts of models to
deformable objects because we're just predicting our raw sensory observations.
Uh, and then once you have that model we can use that model to optimize over actions,
uh, by actually sampling,
check through that model, and picking the actions that we think will accomplish our goal.
Uh, so this is pretty straightforward.
Uh, there are a couple of challenges though,
which is that we need to learn these models,
which are pretty challenging to learn,
and we need to be able to learn, uh,
the models by optimizing through these large video prediction models.
So, uh, first question,
how do we actually predict the video?
Um, we wanna learn this model.
Uh, this is a fairly complex model,
because it's a model of how images transform as a consequence of our actions.
So uh, this is a problem that people have been saying for a little while now.
Uh, maybe 5, 10 years, uh, at least.
Uh, although back in, in 2016,
it turned out that the models were pretty bad.
Uh, so, uh, so one of the, an example of a model that works,
ah, a bit better is something that looks like this.
So this is just a big neural network, um, because the,
the main points is that it's,
uh, it's deep neural network, and its recurrent,
uh, and each of these, uh,
each of the yellow arrows corresponds to recurrence,
and each of the green boxes and blue boxes correspond to convolutions.
Uh, it's performing multiple f- multiple f- ma- multi-frame predictions,
it was predicting multiple frames into the future.
Uh, it's conditioning on actions so the actions are,
are passed in here as well as any state information that
you might have about like the position of the robot's arm.
Um, and then the other thing about this model is,
there's actually explicitly modeling the motion of pixels.
So, um, rather than actually trying to generate pixels directly,
like having a neural network with output pixels, pixel values.
Uh, what this model is doing is that it's taking the previous image,
it's predicting, um, actually multiple convolution kernels,
and then applying those convolution kernels to the image to generate
multiple transformations of that image and then composing those transformed images,
uh, with these also predicted masks into a single image prediction.
Um, so it's essentially predicting the motion of- basically,
predicting how the previous image will transform into
the next image in a way that's differentiable that can be backpropagated through.
Um, and so here are some examples of some videos from a, a robot.
And if you took,
uh, some of the models back in, uh,
in 2015 for example, uh,
you would get models that look like this,
uh, or predictions that look like this,
which don't look very good.
Uh, whereas if you, um,
have recurrent models that are predicting
multiple frames and are explicitly modeling motion,
you get predictions that look,
uh, much cleaner, still blurrier.
Uh, and in general, the video predictions that we're
getting out of these models are even,
uh, in, in 2019.
Uh, still leave some to be desired,
but there are, um,
still things that we found can be useful for control. Yeah.
Does this, this model predict control as well?
Yeah. So let's talk about the,
um, let's talk about the planning approach.
So once we have our model,
we need to actually optimize the action sequence.
So the way that you can do this
is basically with the sampling-based optimization that we described previously.
So say this is our initial image.
We, we consider potential action sequences.
It's probably like a hundred or a couple hundred action sequences,
uh, including these two action sequences.
Then, predict the future for each action sequence by running those actions through
your model to get video representations that look like this.
Uh, and then you can pick the feature that you like the best, uh,
and execute the corresponding action, um,
or instead of picking the best one,
you could also iteratively re-sample,
and then pick the best one.
Uh, and then, what you do, is you could actually repeat
these first three steps in real-time in order to re-plan and do MPC,
and basically just planning at every single time-step.
Um, we felt that this is something that is practical to do.
But, uh, in the context of- uh,
with video prediction models it can be a bit slow.
So the sampling-based approach corresponds to
rolling out these big convolutional neural networks,
uh, and rolling out batches,
batches of like a hundred or hundreds of action sequences.
And so as a result, uh, the,
the time it takes to plan can be on the order of one hertz,
for example, um, depending on how many GPUs you parallelize it with.
That one hertz would probably be,
parallelized across like two to four GPUs.
Okay. Um, so you can essentially view this as MPC but in visual space,
uh, it's like visual MPC.
Okay. So that's kind of how you can do these sorts of
model-based RL methods in the raw observation space.
Uh, the way that it works,
um, at test time is,
you can, you need to specify some goal.
There are a few different ways that you can specify
goals as we, as we talked about before.
You could learn an image classifier, you could,
um, you could provide an image of the goal.
Uh, one of the things that we did in some of this work is,
specify the goal by clicking on a pixel
and clicking on where that pixel should be moved to.
Uh, so for example, in this case,
the goal would be to fold the left, uh,
pant leg of these shorts,
uh, by moving the red pixel to the green pixel.
Uh, and then we also specify another pixel right here to specify that, um,
the pants should stay in place, um,
if they're not, uh,
if they're not part of the folding part.
So once you have this goal you can run,
uh, MPC with respect to this specified goal.
And then, this is the video prediction corresponding to
the action plan that was found by MPC.
And then, execute the corresponding action on the robot,
uh, to try to accomplish that goal.
And so here's an example of, uh,
what the robot could try to accomplish by,
uh, with, with respect to the goal of moving the pixel upward.
Okay. Um, and so getting back to the, kind of,
multitask learning aspect of model-based RL,
one of the things that you can do is you can use
the single video prediction model to accomplish multiple tasks.
So, um, for example,
if your goal is to pick up an object,
you can click on an object,
click on where you want to move and the robot can figure out how to pick it up.
Um, also, if you want to manipulate the sleeve of a shirt, uh,
it can figure that out, um, or like a task like putting an apple onto a plate.
Uh, then we can also look at a few other examples like,
uh, folding the shorts,
re- like rearranging objects or, um,
like covering an object with a towel.
Um, yeah.
So one of the nice things about this is that it allows us
to accomplish many different goals or many different reward
functions with a single model without having to retrain our model for every single task.
Um, and then the other nice thing about this is the-
the model training part is s- self supervised.
You don't need to provide reward functions or supervision, um,
the robot can, kind of, just collect data,
uh, and train the model on that data.
Okay. So, um, the benefits of this kind of approach is that, uh,
this was, uh, able to scale to real images, uh, fairly effectively.
Um, there's also very limited human involvement.,
uh, and so the model training was fully self supervised.
Uh, and this was also able to accomplish many different tasks with a single model.
Um, these pros are also shared with many of the latent space approaches as well.
Although in practice, we've found that latent-space approaches, uh,
have trouble modeling some of the di- some of the diversity of the videos like this
because you have to capture all of those objects that you
might see in a compact latent space.
Um, and some of the downsides is that despite the fact that they're real images,
there's somewhat limited background variability.
So this is more variability than like the spatula example, for example,
but still less variability to things,
um, like ImageNet for example.
Um, you can't ha- can't yet handle as complex skills as the spatula example, for example.
These are just, um, kind of pick and place style tasks.
And it's also very compute intensive, uh, test-time.
Okay. Any questions on how that works?
Okay. Um, one other quick aside,
because I think we have a bit of time,
is uh, how can we think about actually doing more complex skills,
uh, rather than things like pick and place?
And one thing you could imagine doing, uh, as we talked about before,
is using your planner to collect
more data and then using that data to improve your model.
And I would expect something like that to,
uh, perform pretty well.
Although in practice, one of the challenges with that is
if your planner is very compute intensive,
then it may be very expensive to collect more data using your model.
Uh, so one approach that we've looked at in the context of this work, uh,
is if we can incorporate some forms of supervision such as
demonstrations in order to learn more complex skills.
So, uh, what you could do is you could collect, uh,
demonstrations from many different tasks and potentially use those demonstrations to
improve the complexity of skills that you could learn with this approach.
Uh, in particular, there's a few different ways that you could use these demonstrations.
Um, the first is to append it to your data set and use them to improve your model.
Uh, but you can also use it to improve the other two,
uh, approaches as well.
So what you could do is you could fit a model
to the behavior of the demonstrator to basically
predict the kinds of actions that the demonstrator might take based on an initial image.
And if you have this model of the kinds of tasks that are interesting to perform,
then you can use this to first to like direct your data collection proc-
process towards the more interesting kinds of behaviors and the more interesting tasks.
And you could also use it to guide the planning process.
Um, so if you know that you can be doing kind of a task that is, uh,
that may resemble some of the tasks that you saw in the demonstrations,
then you can sample actions, uh,
similar to the actions that the human would- would take in addition to the actions,
uh, that you would sample from some random distribution.
Uh, so an example of this is that,
uh, I guess, one,
one setting where we study this problem is in
the example where you want to have a robot manipulate tools.
Uh, so here are some examples of the demonstrations that, uh,
that the, uh, user collected,
uh, using different tools to,
uh, push them in different ways.
Uh, here are some examples of the samples from the action proposal models.
So this is, kind of, the, uh,
the types of actions that the robot thinks that the user might perform.
So these correspond to, kind of,
grasping towards objects and moving those objects.
Uh, these, these, uh,
actions are actually passed through the video picture mode.
So these aren't actually videos but these are actions that I think might be interesting,
uh, as paths to the video prediction model.
And then what you can do is that, uh,
you can specify your goal as before.
Then run planning guided by that action proposal model with respect to your goal, uh,
to get a prediction that looks like this
and a sequence of actions that corresponds to this video.
And then execute those actions on the robot to get,
um, to try to accomplish the goal in the bottom, in the top-left.
Okay. So by actually incorporating these diverse demonstrations we're allowed, uh,
the robot is allowed- is- can now, kinda,
perform these more complex tasks that involve grasping
an object and then using that object to perform the task,
rather than just, uh, pick and place tasks and pushing tasks.
Okay. Um, and again, because the,
the model is trained on a diverse set of, uh,
objects and tasks, and because the demonstrations are also diverse,
it means that this single model is reusable for these different kinds of tasks.
So, um, the model can be used to solve, uh,
tasks that weren't seen in the demonstrations such as,
uh, using a broom to push objects into a dust pan.
Um, it can be used for trying to like use
a hook to bring out of reach objects closer to the robot.
Um, so in this case, the,
the robot was constrained to move in that green shaded region
such that it actually had to use the hook in order to accomplish the task of moving the,
the, um, the blue object closer.
Um, it can also generalize to unseen tools.
And this is, uh, by nature of the fact that it has a,
a large diverse data set,
um, also unseen tools that aren't really conventional tools like water bottles.
Um, and because you're sampling from both, uh,
the demonstration, the action proposal distribution and a random distribution, uh,
they can also figure out when do you use a tool such as when there are
two objects that need to be pushed versus when not to use a tool,
um, when only a single object needs to be pushed.
Okay. Cool. So that was one side on h- one way that you might go about
incorporating demonstration data or incorporating
other forms of supervision in order to perform more complex tasks.
Um, the last kind of approach that I'd like to talk about with
regard to image observations is predicting alternative qua- quantities.
So, uh, it may be that you don't want to be reconstructing images,
uh, such as, uh,
video prediction models and audio encoders.
And it may be that you have some supervision or
some other auxiliary information that you care about for performing your task.
And in these contexts you can try to predict those sorts of things.
Uh, so for example, if you want to be able to learn how to grasp objects,
what you could do is that given a sequence of actions,
you can predict whether or not that sequence of actions will lead to a grasp.
Um, and so given for example,
one of these, these yellow actions,
you can predict the binary event of grasping or not grasping.
Um, and grasping is actually something that you could measure on the robot by looking at
how wide the- whether or not after performing those actions,
the robot was actually holding something.
Um, another example is, uh,
if you care about collision avoidance,
you can predict given a sequence of actions, will I collide?
Will I hit an object?
And so if you have a sensor that can measure whether or not you've collided,
then this is something that you can predict relatively easily.
Uh, then you can also predict, um, things like in a video game,
your health your- or your damage or other,
uh, information about the environment.
Uh, so this is something that's very nice,
if you don't wanna- in these settings you don't have to generate images.
Um, this also has a very close connection to Q-learning because
if you're predicting these types of events that will happen,
if they will happen in the future, then you are essentially,
uh, trying to predict the probability of some event happening which,
um, may correspond to your reward function if you
care about whether or not that event happens.
Okay. So the benefits of this approach, in general,
is that you can only pred- it allows you to only predict the task relevant quantities.
Uh, and then if you're in a multi-task setting,
you could predict the things that c- that are relevant for different tasks.
Uh, the downside is that you need to be able to observe those quantities,
uh, which isn't true in the general case.
You don't always know, for example,
where objects are or maybe if you are in a dialogue
setting you don't know the sentiment of the other person automatically.
You don't observe that automatically.
Um, and of course,
you also need to manually pick
these quantities that you think might be relevant to your task.
Okay. Cool. So that was all I had on model-based RL with image observations.
Um, let's see.
You have eight minutes. I think we have time to
cover though the last part rather than moving it to next week.
Are there any questions on this before I move on?
Okay. So what about model-based meta-RL?
Um, in some sense,
we've already been doing some form of meta-RL.
Uh, and I'll talk about that in a second.
So we talked a bit before about how in
many situations we have this dynamics model that doesn't vary across tasks.
Uh, and in these cases, estimating the model is a single task problem.
Um, but what if we- what if the dynamics are actually changing across tasks?
Um, and so for example, uh,
if you're interacting with objects and you see an object on the table and you don't know,
kind of, a priori how that object is going to move,
if you just see an image of that object,
you don't know the center of mass of the object.
You don't know the friction. And so you don't necessarily
know how it will actually move until you start interacting with it.
Uh, in, in that context,
it's actually somewhat of a partially observed problem
and you need to actually adapt your model
based off of a small amount of data in order to
accurately predict how that object will move.
Um, and so you can essentially view- if the dynamics are changing across tasks,
you can actually turn mod- the model learning problem
from a supervised learning problem into a meta learning problem.
Where you're now going to be conditioning your model on
some data and then using that data to learn a better model.
Uh, in this context, any of the kind of meta learning projects that we
talked about before could be applied to this context.
Um, so for example,
one meta learning approach that we talked about before is using things
like LSTMs or models with memory.
Uh, and we're actually already using LSTMs to- in,
in recurrent models to make predictions before.
So the- those vision based models they were- um,
you can essentially view them,
uh, in this way in,
in a sense as a meta learning problem because they're taking in the context of the past,
uh, few frames and predicting into the future.
Uh, so there's this somewhat of
a blurred line between what is- what constitutes a single model and what constitutes,
uh, like a meta-learned model.
Okay. So one thing you could do is simply kind of collect data,
uh, if you want to turn this into a meta-learning problem,
you can collect data in different environments and adapt
your model to an environment given a small amount of data.
Um, and you can actually also do this in a more online fashion.
So say you have some robot that's, uh,
interacting in the environment and, uh,
has this different- different parts of the environment have different dynamics such as
a terrain change or a motor malfunction that causes the dynamics to change.
Now, one of the things that you can do is you can kind of flatten out the, um,
the experience of the robot, uh,
and view these different, uh,
changes in dynamics as happening in different points in time.
And then if you take it- uh, if you take this viewpoint,
you could essentially view, uh,
the- the few-shot learning problem, uh,
or kinda the meta-learning problem as one of taking a slice- taking a window of
time and using that slice of data to predict
what will happen in the following slice of data.
Um, so you could essentially view this problem
of adapting your model or adapting to your environment
online as a few-shot learning problem where
different tasks correspond to different slices of the experience.
Uh, where basically for,
uh, if you have k time steps of experience,
this might correspond to the training data
set for one task and the following k time steps,
or, or n time steps may correspond to the corresponding test set for that task.
And then you can- this is one window of experience,
you can kind of continuously slide that window to get different tasks,
assuming you have some sort of
temporal continuity in the dynamics that you are encountering.
Okay. So you can use this- you can basically use your favorite meta-learning method,
uh, to solve this kind of problem.
Uh, and so what this might look like is that you
have your last k time steps of experience,
you then adapt your model using, uh,
your training data set and your prior, uh,
to learn a model that's specifically adopted to those k points in time.
And then use that model to actually take actions and to,
to plan using MPC.
Uh, and so, for example this update rule may correspond to one step of gradient descent
and theta star may correspond to
the initialization if you're using an algorithm like MAML.
Uh, and so the way this works is you may,
um, kind of collect data on different terrains,
uh, such as the terrain shown here.
This is a- a little- little six-legged robot called the VelociRoACH.
And the dynamics of the robot actually vary drastically
across different terrains and across different battery levels.
And then you can train it to be able to, uh,
estimate dynamics with only k time steps of experience where
k is something like eight time steps, and that,
in practice, corresponds to actually less than a second of experience if you're at around like
10-20 Hertz and then evaluate
the robot's ability to adapt to other types of dynamics like being on a slope,
or missing a leg,
or having a payload,
uh, or having calibration errors.
So what you can see is that if you try to put the robot on a narrow slope- so here's a,
a visual of the slope up close.
If you try to run a single model across these settings,
uh, what the robot will do is it will,
uh, kind of, diverge across.
It won't be able to run in a straight line because it
won't have learned an accurate model.
Uh, whereas, if you use meta-learning and actually adopt online, uh,
with each window of experience to the current model
and use that to plan and run in a straight line.
Uh, you could also do something like take off the front right leg of the robot,
uh, and see, uh,
if you try to fit a single model,
it isn't able to model the dynamics of these different situations.
Whereas if you train it to quickly adapt and then use- do that adaptation,
uh, at test time,
then you could effectively follow a straight line.
And so this is actually getting back to one of the questions at
the beginning of the lecture where you're not only using
the observed state to re-plan but you're also
using the observed state to update your model at every single time step.
Okay, cool.
So I think that we're basically out of time,
um, some takeaways for model-based versus model-free learning.
So some of the benefits of model-based learning is that it's very
easy to collect data in a scalable way, uh, without boards.
Um, it's pretty easy to transfer across
different reward functions because if- that model only depends on,
uh, the data it was trained on.
It has a less direct relationship on the reward than the policy.
Um, it also typically requires, um,
a smaller amount of data,
or at least a smaller amount of data that's supervised based on the reward.
Um, the downsides of models is that they don't optimize for task performance,
and so there may be a mismatch in the optimized the- the-
the objective you're optimizing for and the objective that you might care about.
We saw the same thing in their representation learning setting if we're trying to learn
a representation for reconstruction versus for the task that we care about.
Um, and sometimes, it's also hard to learn the model than to learn the policies such as
in the pouring example where you have to model fluid dynamics.
Um, and then sometimes you may also need assumptions to learn complex skills,
uh, such as the spatula, for example.
And then for model-free methods, uh,
the benefits is that it makes very little assumptions beyond a reward function.
Uh, it's very effective for learning, uh,
complex policies, uh, and learning complex skills.
Uh, the downsides is that it requires a lot of experience and can be slower to learn,
um, and in the multitask-learning setting in particular,
it's a harder optimization problem because you have to learn
a policy to perform all the tasks,
uh, rather than just learning a model and inverting that model for an individual task.
For each individual task, you have a test pattern.
Uh, and then, I guess the last thing
is that I don't think we necessarily have this dichot- dichotomy,
I think that ultimately we probably want elements of both,
such that when we're pouring water,
we use a model-free approach and such that when we're, uh,
maybe pushing objects around we have more of a model-based approach.
Okay. Um, in the next few weeks,
I will be talking about,
uh, kind- this is, I guess, sort of
the conclusion of some of the RL section of the course and, uh,
on Monday, next week,
we'll be talking about what about seeing tasks in sequence.
Uh, we will cover this both in
the supervised setting and in the reinforcement learning setting.
Um, on Wednesday will be have paper presentations on some miscellaneous topics that are,
are interesting with- relating to task interference,
differentiability, uh, sim2real methods and hybrid reinforcement learning methods.
Uh, and then the following three lectures will
be about really the current frontiers of these approaches.
So we'll have a guest lecturer from Jeff Clune who works
on evolutionary methods, lifelong-learning, and meta-learning.
Uh, we'll have a guest lecturer from Sergey Levine on information theoretic
exploration approaches and how that can be used for task agnostic reinforcement learning.
And, uh, on Monday of th- a couple of weeks after Thanksgiving,
I'll be giving some perspectives on some challenges and,
and frontiers of these topics.
Uh, and then, just a couple of reminders,
Homework 3 is due tonight and the product milestone is due next week.
I'll see- yeah, question.
The question is, um,
regards to model-based versus model free. Especially in a meta-learning context which one is the more
beneficial [inaudible] Like, which one is easier?
For sim2real, I've actually seen both used.
I've seen both model-based methods to try to learn a model that's robust to different,
um, different contexts and then using that model, um, to plan.
I've also seen model-free approaches and that we'll see- uh,
I think we'll see a model-free approach in the paper that's covered on,
on Wednesday or, uh, yeah, on Wednesday next week.
[inaudible] adaptations and model changes.
Yeah. So there are definitely very recent papers
that have looked at meta-learning for sim2real where you try to,
instead of learning a robust model,
you would try to learn an adaptable model such that you can adapt to
any possible simulator and then at test time you're
given the real world and you want to be able to adapt to the real world.
Um, there's at least one paper that came out in the
last like two months that studied meta-learning for that problem.
Um, so I think that's kind of like, the cusp of- of where current research is on.
Okay. Great. See everyone on Monday.
 So today, we'll be talking about lifelong learning.
Um, and all- it's first logistics,
uh, the project milestones are due on Wednesday, uh,
and next week we have two guest lectures, uh,
by Jeff Clune and Sergey Levine and I,
uh, highly encourage you to inten- attend in person.
I think that their talks will be, uh,
pretty neat talking about, uh,
probably talking- like I don't know the parti- particular topic of just the
lecture yet but it'll probably be covering things like evolutionary methods, um,
more advanced topics in
lifelong learning than the things that we'll be covering today, uh,
as well as other, uh,
kind of interesting topics in the context of control,
animation, evolution, those sorts of things.
Uh, and then Sergey Levine will be talking about
information theoretic exploration and
unsupervised reinforcement learning and how that can
be pertained to a form of
task-agnostic reinforcement learning where you want
to pre-train before knowing what task you're going to be doing.
All right, so, um,
the plan for today is, uh,
we'll be talking about lifelong learning,
and before I go over the outline,
I guess I'd like to preface this, preface this lecture by
saying that in many ways lifelong learning I think is, uh,
a very active area of research and a very open area of research,
and I don't think that, um,
as with a couple of the other lectures in
this course I think there may be more questions than answers.
Uh, and so, uh, I'm going to try to have some,
some interactions and some engagement with regard to, uh,
some of those questions so that we can,
um, think about them in the lecture as well.
So, um, first, we'll be talking about the lifelong learning problem statement, uh,
then we'd be talking about some basic approaches to lifelong learning, uh,
and then we'll think about can we do better than these basics and we'll
see two examples of ways that we can do better,
uh, and then revisit
the problem statement of lifelong learning from the perspective of meta-learning.
Okay. So let's first start by talking about the problem statement.
Uh, and this is actually probably one of the biggest topics of the lecture,
uh, which is how do you actually formulate the problem of lifelong learning?
Uh, and to review the,
the two main problem statements that we focused on in this course
have been multitask learning and meta-learning which may be in
the context of supervised learning or reinforcement learning, um,
and in multi-task learning the goal is to solve some set of tasks and then
perform those tasks and in meta-learning the goal is to use,
uh, experience from a set of tasks to be able to quickly learn a new task.
And so, uh, the motivation behind things like lifelong learning is that,
uh, in practice, unlike these problem statements, um,
are kind of the real world may look something like this where we
don't have access to a set of tasks right off the bat,
and instead we need to be learning, uh,
in a more incremental or online fashion where
we are given objectives one at a time and we save T data,
uh, in a stream rather than in, a batch.
And so this is kind of a general temporal view of multitask learning or meta-learning.
Uh, is, is kind of what underpins the,
um, kind of idea behind things like life-long learning.
So, um, some examples of these, uh, that I came up with,
there are many other examples, uh,
of lifelong learning systems, in,
uh, that we may want to be able to create.
So one example is a student, uh,
learning concepts in school, learning, uh,
for example math concepts ranging from, um,
simple addition to long division to, um,
to, uh, algebra and calculus, etc.
Um, another example might be,
uh, say you are,
uh, you have some image classification system and you want to deploy it to help, uh,
classify people's images, uh, for different users,
and if it's deployed in the world,
it will be seeing a stream of images and it will be given different,
uh, tasks over time.
And that may- may be a stationary distribution,
may be a non-stationary distribution, uh,
because the world is,
is changing and also the, the kinds of pictures that people take change over time.
Um, another example is, uh, something like a robot acquiring
an increasingly large set of skills, uh, in different environments.
So in practice we, uh,
we may want robots to do many different things that we might not know right
off the bat everything that we may want the robot to do.
Uh, so instead you wanna kinda have this sequential setting.
Um, also in the cut of in the context of language,
you can imagine a virtual assistant helping different users with different, uh,
different tasks at different points of time, um,
or something like a doctor's assistant aiding in medical decision making decisions.
Um, in each of these cases,
I guess one of the key ideas behind
lifelong learning is that when you deploy a machine learning system,
it shouldn't be fixed.
Uh, you shouldn't just kind of keep its weights the same and deploy it and run it.
Uh, instead it should be continuing to learn based off of the data that it sees.
Uh, and that will allow it to handle, um,
challenges related to non-stationarity,
and challenges relating to, uh,
it seeing tasks that it wasn't trained to do in the past.
Okay, so with that in mind,
uh, first some terminology.
So, uh, you could broadly view this class
of problems as a form of sequential learning problem where you're given data,
or you're given tasks in sequence,
but there are a lot of different terms that have been used to
describe these kinds of sequential learning settings,
ranging from things like online learning,
lifelong learning, continual learning,
incremental learning, streaming machine learning,
or streaming model or streaming data.
Um, I think all of these fall into the category of sequential learning.
Uh, in general, the community doesn't use
consistent terminology for many of these terms, unfortunately.
Uh, so I think that the- all,
all of you, I'll define a couple of these terms, uh,
throughout the lecture but otherwise I think that, uh,
when reading the literature I wouldn't necessarily assume
that these mean different things, they may all just mean the same thing.
Whether, and it's a bit dissatisfying I know, uh,
but and then maybe in the future we can try to actually assign different names to
these terms or better define the types of things that we're talking about.
Um, and it's also worth mentioning that this is distinct from sequence data.
Uh, for example if it's something like language is a
sequential form of data but that doesn't mean that you're learning from it in sequence,
you might- you could still have a batch of sequential data that you're learning from.
Um, and likewise, this is also different from sequential decision-making,
things like reinforcement learning are a form of
a sequential decision-making problem where
the actions that you take at your current time affect the next action.
Um but you could also do sequential decision-making in a batch learning
setting where you're just given one environment and
one batch of data from that environment and you want to learn a policy.
Uh, and so these kind of, there's like three different forms of viewing, uh,
sequential data, sequence decision-making and sequential learning,
and in this lecture will be focusing on sequential learning.
Okay, any questions? All right,
so I want to do a little exercise.
Um, so I don't think that there's really, as I kind of
alluded to you before I don't think there is necessarily
any consensus to what the lifelong learning problem statement is.
Uh, and I think that actually in many different contexts,
it will vary based off of the things that you care about in that application.
Uh, and so what I'd like to do in the next
like I don't know 5-10 minutes is I'll have each of you,
uh, pick an example problem setting, uh,
from the example problem settings that I showed
on the previous slide and I'll bring them back up again.
Uh, and then- or maybe also pair up, uh,
with someone or form a group of two or three and
discuss the problem statement corresponding to that problem setting with,
uh, with your partner.
Uh, and in particular try to answer the questions such as, uh,
how do you set up an experiment in order to develop and test your algorithm,
and this will kind of help you identify the sort
of actual underlying problem statement underlying that example.
Um, what are desirable, desirable or required properties of the algorithm,
uh, as they pertain to different things like memory, privacy,
uh, data, uh, those sorts of things,
what things do you want out of your algorithm and that problem
setting and then also how do you evaluate a system.
So if you have an algorithm that can solve the lifelong learning problem, uh,
how would you actually go about evaluating whether or not
it's doing a good job or doing a bad job.
Okay, so, uh, here are the examples that I showed on the previous slide.
If you want, you can also come up with a new example setting if,
uh, if you're, uh,
feeling creative and I'll give you a few minutes to chat with, uh,
chat with someone else in the room about these problem statements. Ready, go.
Oh, and then afterwards I'm going to ask you guys to say what you discussed and what was
specific to that problem-solving.
[BACKGROUND].
Okay. Are you ready
to regroup or do you need another minute?
Raise your hand if you think you'll need another minute.
Yeah. I should. [LAUGHTER]
Okay. Let's regroup and you guys- you guys can pick all-I'll,
go visit you guys last.
Okay, um, so let's start with this group right here.
So what- which example did you pick, uh,
and what were your considerations for A, B, and C?
Uh, so we picked the students learning concepts in school.
And then in terms of an experiment,
we kind of divided it into two sections.
So one, the student would be able- would each be
able to learn new concepts without forgetting old ones.
So kind of eliminate or reduce negative transfer.
Okay. So like ne- negative backward transfer?
Yeah.
All right. So this is for A?
You can keep on talking.
[inaudible] And then another property
would be to take in [inaudible] increasing complexity,
so sort of challenge themselves to kind of, um,
like push themselves and learn something new based on [inaudible].
Cool. So like they have the ability to generate new tasks.
Yeah.
And how- how would you evaluate that ability?
So in terms of the experiment and test- testing
we were thinking about- so we have a sequence of tasks,
uh, with increasing complexity.
All right. Cool. Yeah. So the experimental setup
would have like increasingly difficult tasks.
We weren't sure exactly how you were going to evaluate it,
but we were considering the decoding like the task generation,
with some latent distribution and then maybe measuring the entropy of
the distribution and try to kind of like increase that from task to task.
Okay. So- yeah. Seeing if they can generate a broader range of tasks.
And you probably want something in addition to just entropy of that test distribution,
maybe you have something relating to,
uh, whether or not they're solvable tasks or something.
Okay. Cool, anything else to add?
Well, maybe the [inaudible].
Right so you're saying that maybe you
don't need to- maybe it's okay to forget some things?
Yeah.
Okay. Cool. Okay. Uh, next group.
[inaudible].
Okay. So you're saying that basically, you want to
maintain a distribution over all of your previous tasks,
and evaluate it on all of the previous tasks?
Yeah.
Okay. Cool. Um, I'll put that under evaluation.
And then just- the second point was maybe,
uh- maybe you don't care about all of the past tasks,
but just a subset of them,
or a certain distribution over them?
Yeah.
Yeah. Okay. Cool. Anything that was different
from the first group that you felt like wasn't important?
[inaudible].
Yeah. So like where does the curriculum of tasks come from?
Right.
Yeah. Yeah. That's a good question.
Uh, I- I don't- I guess I don't really have a box for that,
but yeah, that's good. Okay, next group.
Yeah.
Okay.
[inaudible] At the same time,
[inaudible] sort of an abstract is possible thing [inaudible] transfer that
you're also thinking about how this might potentially have
to be evaluated [inaudible] in terms of
which sort of problems are presented to the robot as
[inaudible] really influence which kind of skills get acquired.
I see.
So you're thinking about both- like
how the evaluations kind of intermixed into the learning process itself. Yeah.
Yeah.
Okay, cool.
I feel like that's mostly covered all the things up here more or less. Okay.
Yeah.
Was there anything else? .
So maybe like- I guess
the desirable or required properties of the algorithm is
like you don't want to have a negative transfer if you want to be able to [inaudible].
But maybe like- in terms of like maybe the training scheme that you
use like when the desired property from that would be related to this tricked up thing,
is that you need to be is- like if you were to try to have like a lists
sort of- like a discreet set of like properties that you want,
that are- like- like properties for the tasks,
and then properties related to like the environment that you want to be able to like take
sort of combinatorial combinations of like
all the different possible tasks that you can get out in different environments.
And then that can be used to kind of construct your evaluation set,
and you want to be kind of- you want to assume that maybe like when you're evaluating,
that you can take any combination of these tasks and environments and be able to
like generate a new task that makes sense that you can work on.
So it's kind of like robust to all possible combinations of [inaudible].
So you are evaluating on how that combinations of, uh,
like tasks, environments, variations on the tasks?
Yeah.
Okay. Yeah.
[inaudible] increasingly large set of skills [inaudible] probably is
important that the skills be around present and continuous [inaudible].
Yeah. It's like a continuous parameterization of tasks.
Okay. Um, the group in the middle.
Uh, we talked about, uh, [inaudible].
Okay.
So we talked about a couple of things in general,
but one like, uh,
more specific example is like diagnosing like [inaudible] For example,
if you had a string of patients coming into
your office and you wanted to do an evaluation based on
their symptoms [inaudible] seasonal trends every year.
But you still want the algorithm to perform well no matter what time of the year it is.
And you also want it to be able to remember,
uh, information for many years past,
so maybe flu strains [inaudible] And you wanted to do well,
uh, on implications that you did,
um, as opposed to maybe the [inaudible].
Yeah. So you want some sort of kind of memory of the past, and also, uh,
you may have, uh, like periodic signals,
like seasons for example,
and you want to be able to take that into account in
your decision making to do pe- to perform well.
I guess in addition to transfer the- and generating new tasks,
there's also just generally like good performance is something that we care about.
Yeah.
Yeah.
[inaudible] performance, I think [inaudible] was mentioning maybe, uh,
after learning could be a good feature to have because oftentimes,
my system will recommend [inaudible]
Yeah.
And that's particularly relevant if you're seeing like
our distribution data for example. Yeah.
[inaudible].
Yeah. So considering like different sequences of
tasks and you want it to be potentially robust to those different types of sequences.
If you're in a, like
a medical decision-making situations because you don't know if you're gonna be seeing,
the easier cases, or the harder cases or if you
may see some intermixing of hard cases and easy cases.
Great, okay.
Um, the group in the back.
Yeah, uh, so we talked about the- the social system and I guess we were talking about uh,
kind of like two different spectrums here like the one spectrum was like on the one side,
we don't want to bias like,
towards a single set of users,
or a single set of classes for a single set of time.
But then on the other hand er, maybe for example,
if you're seeing this single task a lot more frequently at voice like
[inaudible] highest maybe a virtual assistant
should focus on learning that task over others.
So we are like, kinda of like asking about how we should balance those two er,
between like passing chords or set of rules and um, may- maybe not do.
And in terms of evaluation,
we also talked about how the performance on
the different tasks could be different depending on which training you look at.
And that is also an interesting question because at the [inaudible] checkpoints with
that training period we might wanna focus on different tasks er,
we might wanna invite [inaudible].
Yeah- yeah, that's a good point.
Like if you have- if there's one user that you've been repeatedly
seeing a lot and- and- and very recently,
then you may want to- like,
focus some of your representational capacity on
that user because it's likely that in the near future,
you'll see them again and you wanna be able to make good decisions then.
Great. Um, group in the middle.
[inaudible] You know young children's this needs to grow up.
They have like very broad but like
very few categories of God's finding so they can classify birds and they [inaudible].
And as they grow up,
they acquire more and more categories that are finer and finer manipulate levels.
And we thought about potentially lifelong learning system
around that and it would give their,
neighbors neuroscience showing that this here can break
the character dataset into several therapies.
There has been a learning process involves didn't make represented
the delayed activation of
the classes and overall along the hierarchical tree. [OVERLAPPING]
Like of coarse-to-fine type approach?
Yeah. And then we talk about how you best work a system like
that could [inaudible] as a professor,
you want to be able to learn this final grade classes [inaudible] based on er,
you know, the previous classification [OVERLAPPING].
Right. So you might have a kind of coarse to
fine type of tasks and may also you care about er,
like learning efficiency like be able to learn more
quickly as you see new concepts in sequence.
I'm not sure if course defined falls under evaluation or properties but uh,
I think that- this kind of gray area is
what makes life- these sorts of problems interesting.
Um, this group.
So I think we receive about B.
Uh, we are considering a sequential learning for image cast.
[inaudible] image cast within distance.
Uh, for example, at the first page,
user provide a data set of handwritten digits.
So say well, this is going to be able to identify an even digits.
And that's second stage you know,
different time this, you know,
the user may provide the dataset of ImageNet.
So obviously, you want to assist him to be able to do
classification on ImageNet of all the natural images.
So this would be my care, uh,
sequential learning of different uh, classification datasets.
Um, in terms of poverty,
I think we discussed, uh, first of all,
we would want the system to be able to adapt to
the new dataset faster than training from scratch,
and also, things more likely,
the training will be sequential, in a sequential way.
So uh, uh, which probably can lead to like some catastrophic forgetting for them.
So this does not mean forget what- what the levers rigorously.
Um, so it wouldn't allow that to happen so well, you know,
the system can be continually learning and building up your knowledge across the tasks.
All tasks in addition freshly tests.
Yeah. So yeah, like evaluation on like yeah- like yeah- I'm extra classes.
One tiny thing that we talked about is the idea
of maybe having it classify things that we don't specify at all.
[inaudible] Having it like if we were to show it images of cats,
like even having some sub classify those cats and create
classifications that we specify that it is just kind of cancels these [inaudible].
Yeah. So kind of like maybe a generation of new tasks but from unlabeled data. Yeah.
[OVERLAPPING] Another thing is that being
estimated we basically need to keep all that data from previous tasks uh,
during the evaluation stage,
I think it will be- ideally,
you need to remove keeping increasing the data as we uh,
[inaudible] become very different task over time.
So we don't want to keep all of the previous tasks.
So like O of N memory.
Like little o of n memory.
Yeah. Ideally, yes.
Yeah. Okay. And then, last group.
So one I gave the students learning in school is say
this had such that we want the students to have some introspective capabilities.
Like they should know who I'm not good at this pass.
Not only so that you focus on that task but also sort of they don't go on American Idol and start singing.
That's very useful. All right.
I'm gonna put this next to active learning because it relates
pretty closely to like understanding when you should ask for more information.
Yeah. Yea- yeah.
It looks like there's two aspects: One is,
yeah, what are you good at and what you do for example.
And the other one was uh,
I think this was said sort of what are actually if you're a student in school,
they can't learn everything focused on things that will be
useful later perhaps indicates the students.
You could imagine money,
how much will you make or like that that could be
a signal of one of the things you're going to actually useful.
You actually get paid later on when you use them.
Great.
Just something else when we were talking about there was thinking about is um,
wouldn't it be good for the model to be able to expand its parameters so
that its architecture so know how to fix it?
[NOISE] You're getting increasingly complicated tasks.
So like a dynamic architecture capacity.
[NOISE]
Okay um, did you have a comment?
Yeah. Um, I guess this is kind of like related to some of the things that
are being done but in the evaluation side of things,
it would be useful to, I think, have, uh, a,
a good understanding of, like,
what the tradeoff is between generalization and specialization for
this algorithm where like on one hand,
like if you're trying to get it to work better on like
increasingly difficult tasks and you want it to kind of get specialized,
and maybe your weight is more heavily on that- on
that like component of like evaluat- like
that evaluation metric and then at the same time,
if you wanted to be able to like,
like the robot to be [NOISE] able to adopt to like cleaning a bunch of different houses,
and it's okay if initially it doesn't perform that well, uh,
at cleaning that new house because it- it's just kind of like [inaudible] cleaning house,
then you kind of wanna be more general with it.
You want it to be focused on like the variety of like
different houses as opposed to how perfectly they're able to clean one specific house.
So like- yeah. I know I think that trade off might be its
own metric or its own certain evaluation criteria.
Yeah. And I also suspect that something like generalization
versus specialization is gonna be something application dependent.
Like, uh, in many situ- situations,
you may care about,
uh, like, it being, like,
very consistently performing very
well in this sit- situations where you really need it to perform well,
versus be able to do, uh,
something, like, do more things but slightly at a lower degree of performance.
It's like a robot doing things with, like,
a lower success rate or a,
um, or, uh, like,
more slowly than if it were a specialist that was specifically
tuned for doing one particular task. Yep.
Yeah. One thought I also had was,
in fact, this is more than one but two.
One, perhaps you want to make sure that they don't,
like, the agent doesn't learn dangerous things.
Like in the case of students, we don't want the student going
on the wild side to build weapons and things like that.
They could imagine, like,
if they had a group which is a large pool of tasks,
that they end up learning things that we don't want them to learn.
Right. So when they propose their own tasks they should-
Propose things-
Propose things that are useful and ethical.
Yeah. [LAUGHTER].
Okay. Great. Um, I think that was a great discussion.
Uh, and I think that I guess one takeaway from this,
is I think that in different applications, you care about different things.
Like, in some applications, you really,
uh, want it to, uh, kind of,
remember everything and be able to perform well on
a wide variety of situations whereas in other applications,
you care a lot more about just the,
the recent thing, uh,
uh, and the things that it,
it needs to do now.
Um, so for example, in decision-making situation,
you don't want to forget past events,
you wa- you want to be able to, uh,
recognize new trends when they're happening whereas in,
um, a student learning concepts in school, maybe, uh,
maybe they don't need to - uh,
maybe if they're not using, uh,
very basic long division skills, uh,
maybe they don't need to remember
those long division skills because they're not actually useful for their, uh,
current position, um, to- I don't know,
if, if- I don't per- I- I personally don't use long division a lot.
So, uh, I think it's okay if I forget that particular skill,
for example. Okay. [NOISE] Okay.
So let's, let's- um,
I wrote out some problem variations and, um,
and evaluation considerations, uh,
on the slide just to,
kind of- and I think that we probably covered most of the things here.
But we can go over some of these things.
So, um, one variation is how the tasks come, uh, to the agent.
So we talked about this a little bit.
They may come in the curriculum from easy to hard or they may be
more something like IID where you see a both a range of difficult tasks and hard- and,
and easy tasks, uh,
in some, sort of, stream.
Uh, there's also a setting,
uh, called predictable which is a general.
It's, um, it's, uh, less strong,
uh, notion of, of ideas similar to the seasons, for example.
If you know it's, kind of, the end of, uh,
summer you know that it, fall's likely to come.
So you can see that sort of trend in your stream of data.
Uh, and then there's also- I don't think it was covered in any of these settings,
but there's also settings where you have adversarial tasks coming in.
Uh, that might be if you have a competitive environment
and you're trying to learn how to,
um, I don't know, compete against another agent,
then the types of tasks that may be coming in are gonna be tasks that are particularly,
uh, targeted towards the types of tasks that you do poorly at.
Um, and so that's also, uh,
a variation of the lifelong learning problem statement.
Uh, and the type of algorithm that you develop may be very,
like, very much influenced by these different types of categories.
Uh, another variation of the problems is
whether or not you have very discrete task boundaries or more continuous task boundaries.
This came up, um, when we were talking about having,
kind of ah your task be continuously parameterized versus,
um, having ah, more discrete tasks.
Uh, and then there's also situations where you know where
the task boundaries are and where you don't know the task boundaries are.
Um, so for example, if,
um, if you're in grade school,
if you're going from first grade to second grade
or you're given one problem versus another problem,
there's pretty, uh, clear shifts.
But if you are,
are, are, um, diagnosing flu symptoms or, uh,
making diagnosis for the flu, you may not know, uh,
shifts in the data distribution that you're seeing. Do you have a question?
No.
Uh, cool. Uh, some considerations in terms of,
uh, like, the desirable properties that we care about, um,
model performance as we discussed, data efficiency, uh,
the ability to learn new things quickly, uh,
we didn't really talk about this but computational resources.
And this is actually gonna come up, uh,
a fair amount in thinking about the algorithms that we develop.
Uh, and it's related to, uh,
this, uh, me - memory but in terms of computation.
You don't want the computation to grow, uh,
with- uh, grow linearly and,
and for example, as you see more tasks.
Um, and then there's other considerations such as privacy,
interpretability, fairness, uh, and the amount of compute memory you use at task time.
Um, a lot of the- some of these things are things that you care about in
standard machine learning settings and aren't specific to lifelong learning.
Um, although one thing that comes up that we'll touch on,
[NOISE] uh, in a couple of slides is privacy.
Uh, if you're in the medical decision-making setting,
for example, it may not be- um,
you may not be allowed to actually store all of the data
perpetually and so that may motivate, um,
settings where you have to, uh,
store things in the memory of the neural network rather than storing them in,
uh, in plain texts.
Although in that setting, it's,
it's unclear if you're storing things on the weights of a neural network,
uh, maybe you're still storing them and maybe that's also violating,
uh, privacy- patient privacy.
Uh, so some pe- kind of, interesting,
philosophical questions there. Um, yeah.
So in general, substantial writing of a problem statement and unfortunately all of
these generally fall under the umbrella of lifelong learning or online learning.
And so it makes it, uh, rather difficult,
uh, a bit to tease apart some of,
um, the different advances in this area.
Okay. So, uh, now let's look at one very concrete instantiation of,
uh, the lifelong learning problem statement.
And this typically falls under what's called online learning. Uh, and it's very simple.
So you have, uh,
time that's passing and you observe an input.
You make a prediction about the label corresponding to
that input and then you observe the label for that input.
And then the loop, uh, iterates.
This is basically vanilla
uh, online learning um,
and in the very general case,
you could make no assumptions about what x is and what y is.
Um, and also, uh,
in the general case, you could just, you um,
you could p- not place any restrictions on memory or computation,
uh, and just want to be able to solve this problem, in the general case.
So that's the most general,
uh, formulation of the problem.
Uh, you may make some certain assumptions.
So one assumption that you can make is that the data is coming in IID.
So the x_t is drawn from p of x,
and y_t is drawn from p of y given x.
Uh, and note that p is not a function of t. So it's a stationary distribution.
This is, uh, one example of online learning and,
and this assumption can make it easier to think about algorithms, uh,
even if the data distri- density doesn't necessarily follow these assumptions.
Um, otherwise, if you assume that you're not in the IID setting,
then you assume some sort of time-varying,
uh, distribution over x and y given x.
Uh, and there's other- as I alluded to you before,
there's also a setting, uh,
called the predictable setting that I don't have, uh,
texts for but that's the setting where you can
actually predict something about the next distribution
based off of the trends you've seen in the past. Okay.
Um, so another assumption or another, uh,
consideration or restriction that you can put on this
is what I'm going to call the streaming setting.
Uh, and in this setting,
you can't store xt and yt.
You could only look at them once, uh,
and then you're kind of given the next thing.
You're not allowed to have any memory.
Uh, and so this could be very
useful if you're in settings where you have- don't have any memory,
or just the amount of data,
the quantity of data is so enormous that it's impractical to think about storing it.
Um, due to memory or due to computational resources.
If you are streaming video for example, uh,
every single second, uh,
it may be impractical to store that.
It could also relate to privacy considerations as I mentioned before.
Um, and it could also relate to, uh,
wanting to study neural memory mechanisms.
So there's a- a kind of a sub-field of lifelong learning that really
cares about how humans remember things,
uh, when they're given a stream of data,
and so if you care about kind of this biologically plausible, uh,
form of memory that is more biologically plausible than,
like, a hard drive,
then you could, uh, consider this setting.
Uh, and it's worth mentioning that
this is certainly true in some cases that you cannot store x and y,
but in many cases,
it is actually practical to store the data that you're seeing.
Uh, we have very large data sets like ImageNet.
We have pretty big hard drives,
uh, and so from a practical point of view,
it can be actually, uh,
very sensible to store the data that you're seeing for random problem settings.
Um, so for example, in reinforcement learning settings,
we store our data in replay buffers and we, uh,
typically replay our da- uh, that data to the agent, uh,
in order to allow it to continuously learn from that data.
Okay. And then the last, uh,
variation on this problem statement that I had written down is
that if you know where the task boundaries are,
you may also be observing, um,
some task descriptor zt that could correspond to
a one-hot vector or some other more descriptive,
um, description of the task.
Uh, and in that case,
you would observe both xt as well as the task descriptor.
Although you could also put this into the general case of online learning where xt
contains both the in- the input and the task descriptor.
Okay. Um, so this is the problem statement,
uh, or several different variations on a general problem statement.
Now, what do you want from your lifelong learning algorithm?
Uh, and in particular,
how do you go about evaluating lifelong learning algorithms?
Um, there are two metrics that I-
or- or two classes of metrics that I think are useful for thinking about this.
Uh, and I- actually I don't think that anyone really
gave a concrete metric when we were discussing this before.
So let's talk about that now.
Um, one of the most,
uh, classic metrics and online learning is what's called regret,
uh, and you want to have
minimal regret and you want your regret to grow slowly with, uh, the time.
And in particular, what regret refers to is the cumulative loss of the learner.
So essentially the integral of the loss for each of the data points that you
see subtracted by the cumulative loss of the best learner in hindsight.
Um, so this is basically a comparison between,
uh, your cumulative loss and what you could have done in the best case.
And mathematically what this looks like is the following.
So, say Theta t is the parameters that you choose at each timestep
to make predictions.
Um, the cumulative regret of a- the cumulative loss of yourself is the left term.
Where you're just adding up the loss for
the set of parameters that you use at each time step.
And the right term is referring to in hindsight.
Uh, if you had some model,uh, that,
that minimized the loss for those time steps,
basically kind of the best set of parameters that you could have done or they- you
could have used to perform that sequence of tasks.
Okay. Yeah.
[inaudible].
Yeah. So this cannot be evaluated in practice,
uh, typically but it's very useful for analysis.
And, uh, and in particular we're thinking about in,
uh, in simplified settings,
usually in complex settings.
Can we study how the regret grows for these algorithms?
Um, so one thing worth noting is that regret that grows linearly
in t is trivial to obtain.
Uh, can anyone tell me why that is the case?
[inaudible]
Um, let's see.
So if you- right.
So if you output kind of a random parameter vector,
it would grow linearly in t. You could actually do slightly better than that.
Um, and still have it be linear.
Are there any other? So that- that's, that's a good example.
Uh, are there any other examples of where you could get, uh,
a learner that is linear in t? Yeah.
Uh, you can just keep Theta fix.
Uh-huh. So if you keep Theta fixed at like a random initialization?
Yeah, or even like at any performance.
At any performance. What do you mean?
Like Theta key for any t. Then,
[inaudible] same loss, it will always be linear.
Um, right.
So you don't get to choose the loss.
You only get to choose Theta.
Right [inaudible].
Um, if you keep the Theta fixed,
you won't necessarily have the same loss for all of the tasks. Does that make sense?
Yeah.
Although if you have a random Theta you
would- you could- you would guess that you would probably have,
uh, the sim- a similar loss for all the tasks.
I guess the same thing for fixed.
Any other thoughts?
Yeah.
[inaudible] every t have a Theta that just minimizes the loss of that t?
Yeah. Yeah. So if you just train from scratch on each of the t tasks, uh,
and disregard any previous experience,
then you would get linear regret,
uh, because the loss, uh,
or assuming that the loss- that the tasks are similar difficulty.
If you just train on each of those tasks from scratch, um, yeah,
you would get linear regret, uh,
and because the, the loss for- the loss value would be constant.
Uh, and so the loss would, would increase.
Uh, and so in practice,
what we typically want is, uh,
sub-linear growth in your regret so that you can
actually use your previous experience to do better on the future tasks.
Okay. Um, cool. So that's one metric,
uh, that's used typically in the online learning literature.
And in practice, as I mentioned,
it's something that's very difficult to evaluate.
So let's think about some,
some metrics that we could actually evaluate in practice.
Uh, and so I think the most common, uh,
metric that people use in
lifelong learning is thinking about positive and negative transfer.
So I guess first you can look at performance,
uh, on, on your set of tasks.
But in addition to performance,
you can look at how well your tasks are allowing you to perform other tasks.
Uh, and so, so particular positive forward transfer refers to
a setting where previous tasks can cause you to do better on future tasks,
uh, compared to learning the future task from scratch.
So if you basically compare how well you do on
future tasks after learning the previous task compared to,
uh, learning future tasks from scratch,
that can give you a measure of positive transfer, uh,
and positive backward transfer would be if
a current task allows you to do better on previous tasks,
uh, and this would be compared to learning those past tasks
from scratch before seeing, uh, your current task.
Uh, and then if you want to find negative transfer,
just replace the word better with the word worse,
uh, and you'll get negative transfer.
If you're doing- if basically previous tasks cause you to do worse on future tasks,
and if the current task, uh,
allow you to do or cause you to do worse on previous tasks.
Okay. So this is a metric that can
allow us to study whether or not lifelong learning is allowing us to,
to learn well. Yup.
Is this symmetrical? So let's say I have two tasks;
A and B. [inaudible].
Right. So typically it's not a symmetric metric.
Uh, so, uh, for example,
if you learn how to, um, what is an example of this?
[NOISE] If you- if you learn how to grasp objects,
uh, and then you learn how to,
um, grasp an object and put it into a container,
uh, if you learn the grasping thing first and then,
then the container thing, uh,
then you'll be faster at learning the,
the, the task of picking and placing if you have a, a good learner.
Uh, whereas if you reverse the order you
learn how to kind of pick and place and then you learn how to grasp.
Um, the time it will take to learn how to
grasp should be trivial after learning how to pick and place.
And so basically if you flip the order of those two tasks,
uh, for one of them you get, uh,
you get positive forward transfer,
uh, to some degree and the other order you get
positive forward transfer to a much greater degree.
If you have a good learner. Does that answer your question?
Right. And that's something that's interesting and worth keeping in mind,
uh, as you think about- and it can be useful
for thinking about how do you actually order the tasks.
Okay. So now that we've spent a [NOISE] long time on
the problem statement, let's actually talk about some [NOISE] approaches for solving it.
Uh, and we can start with the very basic approaches.
So, uh, a- and by basic I mean really the most naive thing as you could imagine doing.
So, uh, the first approach you could imagine doing, uh,
under settings where you don't have a lot of constraints on
what you're allowed to do with the data is
just store all the data that you've seen so far,
and train on it,
uh, until you, uh,
achieve the optimal loss function for all the data that you've seen.
[NOISE] Uh, and this is- this actually has a name, uh, to it.
So it's called the follow the leader algorithm.
Uh, and it's actually if,
if you have the ability to do this,
then this is an extremely strong approach.
I basically saying given all the data that you've seen so
far train on it and minimize the loss of all that data.
Uh, and we will achieve
very strong performance because you won't necessarily
over-fit to the most recent data because you'll be,
uh, continuously training on everything.
Uh, yeah.
Downsides is that,
uh, it's very [NOISE] computationally intensive.
So if at every single time you see
a new data point you now train on everything you've seen.
Then, uh, the- your train time is
actually worse than o(n) because as you see more data you'll presumably be- uh,
have to do more computation.
Uh, with that said if you kind of continuously fine tune
instead of trying to train from scratch on- at every ti- time step,
you instead can fine tune from what you learned
before then it may actually be more practical to do this.
Um, it can also be memory intensive.
Uh, so if you're in applications where you can't store
all the data you've seen that this is a problem.
If you're in applications where you can't store all the data this is fine.
Uh, so this depends on the application.
Okay. Uh, second naive approach that you could imagine doing, uh,
is- particularly if you're in a setting where you can't store data,
you just imagine taking a gradient step on the data point that you observe.
Uh, and this corresponds to [NOISE] stochastic gradient descent [NOISE] as, uh,
many of [NOISE] us should all know, and,
and what you've been using in your, uh, homework.
This is very computationally cheap.
It requires no memory.
Uh, and the downside is that it's
subject to negative backward transfer if you- if the data points that it sees,
uh, do not, uh,
kind of cover the things that you saw previously.
Uh, and this is what is known as forgetting.
Uh, sometimes also referred to as catastrophic forgetting.
Uh, how catastrophic it is depends on the application that you're in.
Uh, so maybe it's fine that you, uh,
forgot how to do long division for example,
it just not really a,
a catastrophe but maybe,
uh, in other settings if you forget how to, uh,
make decisions based on a current season,
uh, then there is potentially more catastrophic. Yeah.
So to avoid catastrophic forgetting in the first setting,
you have to always be randomizing on how you sample the next batch?
Yeah. So the way that the first setting avoids
forgetting is that it just stores all the data that it's seen,
and continuously replays that data,
and continuously trains on that data.
So now they are mechanically stored? [NOISE]
Right. Yeah. So this- the top ver- the top version will- um,
in principle to follow the leader algorithm will actually train to
convergence on all of the data that you've seen so far.
Uh, where you store the data and you sample it IID from your buffer of data. Yeah.
You wanna talk about
a strong performance is this with respect to our regret or something else?
It's, uh, yeah. It's, it's with respect to something like regret, uh,
and in convex settings under certain, uh,
I think smoothness assumptions, uh, and,
and convexity assumptions or maybe actually be strong convexity,
uh, you can show that it gets o of log n regret.
Uh, and log n is, uh,
pretty good. [LAUGHTER] Yeah.
And so, does this not work so well if you assume like,
that here is your local minima that you can converge
to when you're training the sort of leader and all of the data number one.
And are there any cases where like,
training on less data can actually give you a better- like,
like, ge- get you a better reader performance?
Um, so for the first question if you are continuously fine tuning then like,
it could certainly be the case that at the beginning you get to a certain place in
that your optimization landscape from training on
a very small amount of data that is very hard to get out of.
Uh, and there's actually some interesting work by, um,
Stefano Soatto that looks at these sorts of critical periods in neural network training,
and how that affects training in later parts of,
uh, later parts of training.
And that's actually, uh, that will hurt potentially both of these two approaches,
uh, because you could over fit to small amounts of data
or get to a bad part of the optimization landscape.
Um, and so one way to do that is just to restart, like
randomly initialize your network at different points during training.
Um, and in principle,
like on paper the follow the leader algorithm would,
would do that- would at every single time step will reinitialize your neural network.
If you want to implement in practice you would probably
want to do some sort of continuous fine tuning or,
or periodic, uh, random initializations.
Uh, what was your second question?
Um, I guess it was just about like if sometimes using like,
a smaller subset of the data as opposed to all of the data up until the point that
you like train is in some ways like better to train the leader.
Like if you- for example,
if you selectively choose examples that are more instructive,
and leave out examples that are either
redundant or actually [NOISE] might affect their performance,
because [NOISE] let's say you collect way to many negative examples,
and you wanna somehow balance like,
positive and negative examples while training their leaders.
Yeah, so in general I think- yeah,
in general in machine learning not just in lifelong learning that that
like the distribution of the data that you're training on matters.
And so for example if you have a highly imbalanced classification task where you have,
uh, maybe you're actually, like, uh,
making- doing medical diagnoses and like 99% of your people,
uh, of your patients don't have the condition and 1% do,
then, uh, re-balancing that a little bit can actually help you get better performance,
um, and then also in reinforcement learning contexts.
For example, if- if- if the data that you're seeing, um,
is- is highly imbalanced then
it could be that you could do better by re-balancing it in different ways.
Typically, you don't re-balance by using less data.
Typically, you re-balance by just sampling the data in a different manner.
Uh, in general, I don't think there is that much
that is known about how best to optimally balance your data.
And I think that typically it depends
on the types of data that you're seeing at test time.
I mean in practice you want that the-
the balance to be the same as what you would be seeing at test time.
Okay. Oh, the other thing,
um, the other note or other downside as SGD is that,
uh, it is a somewhat slow learning process.
And so, uh, and that's why we typically replay our data multiple times.
I do multiple epochs of training.
Uh, and that's because SGD, uh,
doesn't update as quickly, uh, as possible.
If you just give it one pass through the data.
Um, and so we may want algorithms that can learn faster.
Okay. So the next question is can we do better?
Uh, so these are the two basics, uh,
to I think two most basic algorithms in online learning and continual learning.
Uh, and so I'll cover a couple- a couple of algorithms that allow us to do better,
uh, than these algorithms in different problems settings.
Uh, so the first, ah, case study that we'll look at is if SGD is slow,
can we use meta learning to make it faster in online learning problem settings.
And so in particular what we're gonna do is we will revisit
the example that we saw in the last lecture where we have,
uh, some agent that is learning to traverse different terrains.
And at different points in time it sees different terrains.
Uh, and we know- remember that we talked about how
this form of like online adaptation can be viewed as
a form of few-shot learning where
the tasks are different temporal slices of your experience.
But now what's gonna be different this time is we're
going to consider a setting where, ah,
we see a new terrain,
ah, and we see data from that new terrain.
Uh, and that setting might be
extrapolated from the kinds of things that we learned before.
Uh, and so this can be an example of an online learning program where we
continuously see data from that terrain and we wanna be able to eventually learn how to,
uh, handle that terrain.
And so because it's extrapolated from what we saw before,
if you just take the last k time steps of
experience that isn't going to be sufficient to learn,
to adapt, to an entirely new terrain.
So how do we solve this problem?
Um, one option would be to continually- just continuously run SGD.
So basically, uh, take the algorithm- take the SGD algorithm I talked about before.
And just for each window of k time steps you take another gradient step.
Uh, and this will do better than taking one gradient step on the last k. Ur,
and it will be pretty fast with a MAML initialization.
Um, the downside is though that if you see ice for a while,
you- you adapt to that by running SGD.
Uh, and then you go back to one of your other terrains,
uh, if you've adapted your parameter vector and specialized it for ice, uh,
then you may not be able to do well on other terrains and you
may also forget the- that meta learned
initialization that allowed you to adapt very quickly in the first place.
Okay. So how could we do better than SGD in this setting?
Um, the first thing we notice that we can do better by
meta-training with it like a MAML initialization
that will allow us to adapt more quickly.
Um, but we can also do better than that and try to solve this second problem.
So, uh, we can view this as a form of online inference.
So at each point in time we wanna be able to identify,
uh, the task that we're in.
Uh, and we may not be able to observe that task.
Uh, so for example when we're on ice you may not be able to
observe that we're on ice and when we move back to
another terrain you need to be able to infer from the data that we're on a new terrain.
Um, so what we'll do, uh,
in- in this case task is corresponding to the terrain that were
on or the model- the dynamics model that we're under.
Uh, and it may not correspond to that like
a semantic task that you're trying to accomplish.
Okay. So we wanna be able to infer,
uh, the terrain or the task in each time step,
and once we do that, uh, what we're going to do is,
we're going to have a mixture of- a mixture model of parameters.
One for each of the tasks that we have.
And once we infer the task that we're in we can then just run
the parameters corresponding to that task for, uh, planning.
So in particular we'll have, ah,
a mixture of neural network parameters over, uh, your task variable.
And for each of those mixture components,
we'll take an SGD step on that component.
When we think that we're in that data regime.
[NOISE] So you can actually formulate this as,
uh, an expectation maximization problem,
uh, where the E-step is gonna correspond to
estimating which terrain you're on or which task you're in,
uh, given your data.
So let's say that the , um,
you're- you're, let's just kind of simplify the notation and say that the last k time steps of
data s and a is just gonna correspond to x_t and y_t for a given time step.
Uh, and the way that we can estimate which task we're in will look like this.
So, uh, we want to be able to estimate the probability that we're in task i
given the data at the current time step and so what we'll do is we'll multiply the,
uh, likelihood of the data under each of the tasks.
So say we'll imagine that we're under task i,
look at the likelihood of being under task i.
And then we'll multiply that by the prior that we're in task i.
Obviously this can tell us the probability that
the current terrain correspond to a particular task.
Okay. So once we have a set of parameters, once we've identified which task we're in,
uh, we can then update the parameters corresponding to that task.
Uh, by just taking the SGD step.
Uh, and [NOISE] we can do that actually in a soft way.
So we could, uh,
we can have these probabilities be
continuous probabilities and then we'll take a gradient step.
Uh, so we'll have a great- uh,
a parameter, uh, vector for each task.
And then we'll take, uh,
a gradient descent step, uh,
that is on each mixture element weighted by the probability of the task.
So this is just the probability of the task right here.
And we'll take a gradient step on the data that we,
uh, observed for that task.
So if this probability is zero,
if we're not in that terrain,
we won't update the parameters for that task or that terrain.
And if we are in that terrain, for example,
the probability is one or very high,
then we'll take a gradient step using that data.
Any questions on how this works? Yeah.
[inaudible]
Yeah. So this is a good question.
So what we did for that is uh,
you can basically allow it to instantiate new tasks.
And so- and basically allow it to incrementally grow the number of tasks that's used.
In particular what you can use,
is you can use something called a Chinese restaurant process prior,
that corresponds the prior probability that you're in a new situation,
or in a new task or a new terrain.
Uh, and then, if you think that
you- the probability- so they'll be probability of being under a,
the terrains that you've seen previously and it will also be a probability of being in
a new terrain and to evaluate that probability will have the prior term, uh,
corresponding to the Chinese restaurant process prior and then we'll also have,
um, likelihood of the data,
if you reinitialize to the middle of the initialization to the prior,
take one gradient stuff on that terrain and see, um,
the likelihood of how well that explains the data versus all the other tasks.
Uh, one downside is that you may add a lot of tasks and so at
some point you may need to refresh the tasks or kind of remove
the tasks that you haven't seen recently. Yeah.
It's kind of like a follow-up question.
So it sounds like very similar to the type of modeling that you don't know,
how many topics of the head like then we decided
whatever [inaudible] prior or some transfers were all processed.
But the likelihood that you maximize doesn't necessarily serve, uh,
the potential probability that you adapt with new terrain that you going to see
is higher for anything or evaluated or this successful,
this for successfulness of, uh,
like entering into the right trace,
or right cap for your up role model, in this case.
So you're asking me how do you evaluate,
so it's definitely similar to topic modeling, um,
like Dirichlet processes, the- you're asking,
how do you evaluate whether or not you are in,
uh, like whether or not a task corresponds to a terrain.
Yeah, or how can you tell this passes to successful.
If you don't train or see this topic, uh,
in your training set and then you like arbitrarily
assign this new topic to the new data that you can process,
and how straight people tell this new topic you
haven't- you've got across before the right new topic for
your context in this terrain setting like the- the- the notations are still mysterious.
Yeah, so we are given both like at
each time step we are given we're doing model-based reinforcement learning.
So we're learning a dynamic model.
And so we have actually the labels,
uh, which is basically the next state.
And so that can tell us the- that can allow us to explain whether or not, uh,
to identify whether or not a,
a given point in our data stream,
corresponds to a certain model by looking at the likelihood of that data under our model.
Um, in terms of identifying whether or not you're in
a- kind of one of the tasks that you've seen previously versus a new task.
You can basically, uh,
for a new task you can just reset to your middle of
an initialization and take one gradient step,
and see if that explains the data better,
if that gives them a higher likelihood on the data,
compared to the, uh,
the other parameter vectors that you have.
Does that answer your question? Yep.
So and I think the question that was like a couple weeks ago but once you're up there and
this what is the benefit of maintaining models versus online system.
Yeah so, in this setting- so this is
maintaining multiple models is maintaining like a mixture model of- of -of models.
One of the benefits that you get is that, uh,
you couldn't- with multiple models you can potentially adopted in zero shot,
or in like one shot like once you've- once you basically see the- see a data point.
You can use that to identify which model you should use,
whereas a form of- kind of online system identification needs to
actually adapt using that data from a prior.
And it may be that you need more than one data point to adapt.
Um, so this recall is essentially can be faster,
if you're maintaining multiple models com- compared to having a single prior.
Uh, the benefit of using a
prior is that you don't have to maintain multiple sets of parameters.
Uh, and so that can also be- be,
um, nice. Does that answer your question?
Yeah. So when you're evaluating and, um,
you're giving it like, so let's say a completely new terrain, and you're not allowed to find that new terrain,
does it- basically learned to like mix the models that's it's learned
for over the distributions of the terrains that it's lined up to that point,
it's kind of like adding the average like its behavior on the new terrain,
so that kind of what it does or?
Um, so we do allow it to continuously fine-tune,
on- as it sees data,
and so if it sees it as a terrain that was like, um,
that is very different from anything that it saw before that it will
instantiate a new task or a new model for that.
Uh, if it seems to be like a mixture of two terrains and- in that
explains- those two terrains explain-
like a mixture of those two terrains basic- a linear mixture,
uh, explains the data well,
then it will use both of those.
It will use a linear combination of those parameters and then it will
update both of them with a gradient step that's weighted, uh,
by one half, update each model by one half of the string.
Yep.
[inaudible] initialization, did you do that before
starting this or did you do it also update it concurrently?
Yeah, that's a good question.
So in this case, uh, we- the initialization is,
is pre-trained and then we- and the next one is just like ahead of time, and then you,
run this form of online learning,
uh, from that initialization.
One really important thing to think about is can you then take
that data and use that to improve your initialization.
For example, if you start seeing ice you should
then kind of incorporate that into your priors such that,
uh, you improve your prior.
Uh, and maybe you can kind of discard the Ice model, uh,
as was all-, alluded to you before and just adapt to that from your prior.
But you can't do that until you actually start incorporating data into it.
Um, this work didn't
study that setting and it's- I guess one of the challenges that's worth
thinking about is can you- instead like- adaptation is
very fast because it's meta- meta-trained for fast adaptation.
But if you do an update to the meta learn parameters,
that update won't necessarily be fast.
That will be a slow update and so it may not be able to actually incorporate that data,
or a small amount of that data,
to change the meta-learning or the prior very quickly.
Um, you could imagine a form of like meta- meta
learning such that you train it also such that,
the update- the updates to the meta learn model are also fast.
Uh, but that would be tricky.
Okay, along these lines,
one thing is worth noting is that if
your neural network is randomly initialized and you do this procedure,
it wouldn't work that well because the adaptation steps won't be very fast, er,
and you won't be able to adapt very far to your,
um, to those new trains or those new situations that you're in.
So, um, having some sort of meta learn
initialization is very important for successful online learning.
Okay, um, so this is one approach for using meta-learning to make online learning faster,
uh, and also being able to alleviate the forgetting problem.
Uh, I'll quickly run through a couple of
the experiments that actually looked at how this works in practice.
So, uh, in this case, the experiments, uh,
looked at two different settings at test time,
one setting where, uh,
the legs or the, um,
the- the creature, I think it was called a crawler,
uh, it was in a constant crippled setting,
uh, versus, uh, the second setting was one where there
was regions of flipping between the legs being crippled or,
or being disabled and legs being fixed.
Uh, and then the y-axis to show the reward that's
normalized based off of the performance of all the methods in each of the two situations.
And then there are different methods that we can compare.
So the first thing that we can do is online learning with the MAML initialization,
as we described on the previous slide.
Uh, you could also do MAML initialization but just run SGD.
Uh, and this will do well at continuously adapting but may forget,
uh, and they've kind of specialized too much.
Uh, you could also do the original, uh,
approach of just always resetting to your prior and taking one gradient step,
at every single timestep.
This will, uh,
allow you to adapt quickly but won't necessarily allow you to accumulate data,
if you're in a setting that's extrapolated.
Uh, and then last two examples are,
uh, non-meta learning approaches that, uh,
either don't do adaptation, um,
and just try to learn a single model or learn
a model and fine-tune that model with gradient descent steps.
But those gradient descent steps may not be, uh, very fast.
Um, so a lot of different methods.
Uh, in the constant crippled setting, we don't, um,
we don't really expect to see much difference between these, uh,
these kinds of approaches because you can fit a single model to, uh,
to that setting and, um,
you don't really need to, uh, you,
um, it's- it's okay to count continuously just,
uh, keep on running SGD steps,
and so we see, um,
the methods do uh, somewhat similarly.
Whereas if we have these regions of
the legs being crippled versus the legs not being crippled.
Um, first, uh, meta-learning is
very important to actually be able to adapt to these different settings.
So we see a big difference between the first two approaches and the second two.
And also we see that, uh,
using always resetting and running SGD continuously,
don't perform well because they aren't able to, uh,
accumulate data and also very quickly switch between these modes.
And so that's why we see the green doing better than the red and the blue.
And the only thing that you can look at is the task distribution that's learned,
uh, and it's, it's,
it's kind of identifying which uh,
which mixture elements to use at which points in
time and so what you can see is that, uh,
in this case the- the tasks are switching every 500 steps and it learns,
um, it learns that, uh,
in, uh, these increments it learns to use these tasks.
So, ah, from 500-1,000 and
from 1500-2,000 it always identifies it very clearly as Task one or,
or Model number one and in the other increments it seems to prefer using the task with,
uh, ID of one ah.
Sorry, the first case it was ta- task ID
two and then second one it seems to prefer task ID one and it
ends up specializing that model to the terrain as it
sees more data on that terrain or in that situation.
Okay, any questions? Yeah.
We are doing experiments where it is needed to like use three in at once,
like we, we can make sure three [inaudible] environment.
Um, so in- in this I guess it's,
it's choosing which, like how many models it wants to use, uh.
Actually yeah, between 1000 and 1500 there's like green, red-
There's a little bit of green, yeah.
So it- it's, it is choosing in some of these to use three models.
I think it's mostly dominated by, uh,
one and three though rather than two.
Okay. Uh, so now let's talk about a second case study which is,
what if we're in the meta- we're not in the meta-learning setting,
we don't have a way to pre-train for initialization that's very fast.
Uh, can we modify vanilla SGD to avoid
negative backward transfer um, when we're learning from scratch?
Uh, and there has been actually a fairly large amount of work in this area,
um and I'm going to highlight one work that I think,
uh, considers a fairly principled approach to this problem.
Uh, and that was some work by, uh,
David Lopez-Paz and ah Marc'Aurelio
Ranzato called gradient episodic memory for continual learning.
Uh, and the key idea behind this approach is that you can first store, uh,
some amount of data per task in memory but only store a small amount of data,
uh, in contrast to storing everything, uh,
and you have actually a fixed memory size and then with making updates for new tasks,
ensure that the updates that you make don't unlearn your previous tasks.
Uh, and I guess- the second idea is or the second, uh,
second concept is what most methods are trying to doing at- at,
what- what both methods in continue learning are trying to do.
[NOISE] Uh, and so the key lies in how we actually try to accomplish 2.
Uh, and what this approach is gonna do is it's going to try to leverage the-
the small amount of data that's stored in memory to ah, ensure that future updates don't,
uh, don't hurt the previous tasks that are stored um, and by using the,
the data that's stored in memory.
Okay. So let's say that our predictor is parameterized by our or
is f theta um and it knows the input and the task,
uh, and the task corresponds to ZT.
And then we'll have some memory, uh,
corresponding to MK for task k or for task zk.
What this approach will do, um,
is for every time step it's going to take an update step on the data point xt, yt, uh,
with respect to the predictor,
subject to the constraint that the loss on the, uh,
on the stored data points in memory doesn't get worse than it was for the previous model.
Uh, and it's going to do this for all of the tasks stored,
all of the tasks that were previously seen.
This seems pretty reasonable.
I guess I just want to ensure that the,
the loss function doesn't go down for, uh, for those tasks,
and we're gonna be using the data points in a more, in a,
at a more weak way than if we're going to try to directly train on them.
Uh, the challenge with this is that it's,
it's hard to think about how you would
actually solve this constraint optimization problem.
And so, one of the things that was neat in this paper is they showed that ah,
kind of they came up with the idea that if you assume local linearity, uh,
of the optimization landscape,
then you can basically use the,
the gradient of the task [NOISE] to ah, rephrase this constraint. Yeah.
Do you update memory or it's just static distribution?
Right, sorry, I guess I didn't put this in here but yeah,
you also update the memory at each time step um, and store ah, a,
a set of data points for each task that you see.
And the memory storing mechanism that they used in this paper is quite simple,
they just took the total size of the memory divided by the total number of
tasks and then for each task they saw they
just added that number of data points to the memory,
um, such that, ah,
it kind of fillled to max capacity ah, but
I think the more sophisticated approaches would also probably do better.
Okay. So if you assume linearity of,
or local linearity of the optimization landscape,
then you can actually rephrase this second constraint as saying that you want the, uh,
the inner products between the gradient of,
the gradient that you take with respect to your current data point and
the gradient of the data points in your memory to be non-negative.
And this is basically saying that when you apply the gradient
on your current timestamp or basically apply,
uh, GT, you want GT to not, uh,
go in the opposite direction as the gradients for all of your other tasks
and if ends- if it points in the same direction as the task then it will, well,
if it is orthogonal then, and if the space is locally
linear then it won't- it will keep the loss the same for
all these previous tasks and if it actually points in the same direction it has
a positive inner product then- and the space is locally linear,
then, uh, you actually get positive backward transfer.
It will actually improve those previous tasks.
Uh, the other assumption that this
makes is that the small amount of data stored in your memory,
is representative of those previous tasks.
Okay. Um, and then what you can do,
uh, is you can replace this, ah, the constraint there with the constraint there, uh,
and you can then formulate this as a quadratic program,
uh, and use, ah,
QP solvers to, um,
to figure out how to modify your gradient such that the constraints are satisfied.
Great. Uh, so in our experiments they looked at a few different problems.
Ah, they looked at permutations of MNIST, uh,
where the pixels were permuted and you saw a sequence of
tasks corresponding to different permutations of those pixels.
They also looked at, uh, rotated MNIST digits,
same thing but rotating the pixels, uh,
and also CIFAR-100 where it introduces five classes of CIFAR, for each task.
Um, and the evaluated backward transfer as well as
forward transfer as well as the accuracy on each of the tasks in sequence,
uh, and the total memory size that they used was 5,000 examples across all of the tasks.
All right. So here are what the results look like.
Um, so you can see that the,
uh, in general so GEM is what this method is called,
it was able to achieve higher accuracy than training
a single model on everything and in comparative training,
ah, independent models on each of the tasks,
uh, and if you look at the, uh,
what the red plot is showing,
is it showing the performance,
um, of the models, uh,
as you add more and more tasks, uh,
and it's showing, I think the performance of
the first task after seeing each sequence of tasks.
You can see that it doesn't, uh,
for many of the methods it starts to decline due to negative, uh,
backward transfer, uh, whereas for this approach it's able to,
uh, maintain a high accuracy even as you see more data.
Uh, and you can see fairly consistent trends for
the other approaches as well or the other,
uh, the other benchmarks.
Um, one thing that I'd like to know, here, uh,
if you take a step back and think about this experimental setup, uh, you may think well,
do these experimental domains make sense for studying
problems in backward transfer and- and forgetting?
Uh, and if you're in a setting where you have, uh,
20 MNIST tasks or,
uh, 23 CIFAR-100 tasks,
those are both settings where it's actually very easy to store
the entire dataset in your replay buffer or,
ah, on your hard drive and so, uh,
they aren't necessarily reflective of the kinds of problems that we care
about in lifelong learning where we can't store data, uh,
and so an approach where you,
just store everything and train on it would actually probably do much better than,
uh, all the approaches pictured.
So something to keep in mind when you're,
uh, developing, uh algorithms.
Uh, there's also some algorithms that kind of combine the ideas of these two,
two approaches where we actually try to meta-learn
how to avoid negative backward transfer.
There's a paper at NeurIPS that is covering that.
Ah and because we're out of time,
I'll jump ahead to, [NOISE] I'll skip.
The last part was about [NOISE]
um, online meta-learning which we don't have time to cover.
Uh, the main takeaways that I want to get across is that there are
many flavors of lifelong learning that are all under the same name.
Defining the problem statement is often the hardest part.
It, it varies based off of what you care about in your particular application and, uh,
also in many ways meta-learning where you try to actually optimize for fast learning, uh,
can be viewed as a slice of the lifelong learning problem, uh, where you want,
if you really care about ah, positive forward transfer and data efficiency.
Uh, and lastly it's a very open and active area of research.
Okay. Um, a couple of reminders again,
project milestones are due on Wednesday and two guest lectures next week,
Jeff Clune and Sergey Levine.
I'll see you next week or on Wednesday.
 So let's get started. Uh, it is my pleasure to introduce Jeff Clune.
Jeff will be giving a guest lecture in the course.
Jeff is a Senior Research Manager at Uber AI Labs
and that was just formed after Uber acquired a startup that he helped lead.
Uh, and before that,
Jeff was the Loy and Edith Harris Associate
Professor in Computer Science at the University of Wyoming.
One of the things that I think has been really
exciting about Jeff's work is that it actually
has spanned multiple different subfields of machine learning
and artificial intelligence ranging from deep learning
evolutionary methods and evolving neural networks as well as robotics.
So let's welcome Jeff.
[APPLAUSE]
Hello everyone. Thank you very,
very much to Chelsea for the invitation.
It's an honor to be here and I'm excited to speak with you today.
So, um, today I'm going to kind of think really the long-term about
where are we- how we might achieve
our most ambitious objectives as an AI research community.
Um, but before I begin doing that,
I wanted to tell you about a little bit of work that my team
and my collaborators and I have done that I won't have time to talk
to you about in case you're interested in looking it up or in
case we have the opportunity to chat one on one some time.
The first thing I wanted to mention is that we've done some work
on automated ecological understanding.
So this is kind of a nice opportunity for people who work in
the deep learning community to kind of use our skills to help the world,
make a- you know, make the world a much better place.
So biologists really want to understand kind of the animals and their ecosystems,
what they're doing, how many of them there are, etc.
to manage endangered populations,
combat poaching, and just to generally understand ecosystems.
And so if you think about it motion sensor cameras plus deep learning are
kind of a perfect marriage to help wi- wildlife biologists do that.
And we were able to show in this paper that it works quite well.
So I'm excited to see the future of that.
My collaborators and I have also done about
a six paper arc in what we call AI Neuroscience.
So this is trying to understand how much
do deep neural nets understand about the images that they classify.
For example, including a lot of these papers which you might find interesting,
and then the final thing I'll mention is we've done a lot
of work in reinforcement learning,
particularly on the question of how to do
intelligent exploration in reinforcement learning.
So recently put out the Go-Explore algorithm which solves Montezuma's Revenge
and virtually solves again with Pitfall which
no prior algorithm had scored greater than zero on.
And so I think it's kind of interesting
and important to kind of figure out how could we make
these RL algorithms explore like humans
do so we can make them much more sample efficient.
So you can check that out if you're interested in it.
Okay. So here's the major main subject of today's talk.
The first bit is going to be wildly speculative. I hope you don't mind.
But we're going to have some fun thinking about like how might we get to the really,
really far goals that we've set ourselves as a research community.
And I want to warn you that, you know,
this is obviously because it's so ambitious,
and so thinking kind of big picture,
it's not grounded yet in a lot of experimental evidence and it's just a discussion.
So even if you completely go by anything that I say at that phase of the talk,
what it's gonna do is it's also going to provide motivation and
context for a lot of the techniques I am going to tell you about in detail,
which even if you disagree with the wild speculation in the beginning,
I still think you'll find interesting from a meta-learning perspective.
And then we'll go through those techniques in order.
So let's begin with the big picture stuff.
So how- we all I think in the back of our heads are
interested in trying to produce
artificial general intelligence or you might call it human level AI.
Or at least you're wondering whether or not that's possible.
And so I think this is, you know,
obviously like the most scientific- sorry,
most ambitious quest in scientific history.
And it'll change everything,
you know, every aspect of our economy,
culture, it'll revolutionize science, etc.
So the question is how will we get all the way there?
Not how would we make incremental progress this year and next year,
but how might we really achieve this thing at the back of our community's head,
we wanted- we want to accomplish.
And so I think that if you kind of take a step
back and you say what is the traditional machine learning community,
you go to NeurIPS, you go to ICLR,
you got to ICML, what are we doing as a community?
And I think that we're kind of implicitly committed to what I call the manual path to AI,
and nobody ever talks about this,
it's kind of like the fish that don't see the water.
I think that the community effectively is saying what we're
trying to do is identify the key building blocks to AI, you know.
And so if you go and you look at, uh,
a paper in any given conference, what does it look like?
Well, there you say, oh, I think that maybe we need this building block here.
It doesn't exist yet, so I'm going to propose that we add it.
Or I'm going to take this existing building block like a highway network.
I'm gonna replace it with a RES network.
I'm going to show that that works slightly better.
So we're kind of finding all the pieces or improving the pieces.
And that kind of raises some interesting questions, which is, you know,
how many building blocks are there that would be required to
build an actual really complicated powerful thinking machine?
You know, are there hundreds?
Are there thousands?
And like can we find them all one by one as a community manually?
I think that's kind of an interesting question to consider.
But even if you think that we can find all those building blocks,
we're implicitly committed to some Phase 2,
where we are eventually going to have to put all these building blocks together into
some giant complicated Rube Goldbergian thinking machine,
which is something that I just think is something that we should say
explicitly because if this is the path we're committed to,
we should kind of stare at it clear-eyed and know how daunting of a challenge it is.
And I just want to be clear that I think this is a really Herculean task.
You're talking about hundreds or thousands of nonlinearly interacting complicated parts,
each of which took a PhD or
at least a paper to get right to begin with and now they're all interacting.
So how would you debug that system?
And well, if it doesn't work, how are you going to fix it?
So I'm not saying that it's impossible,
but I think it's really, really difficult and we should know that.
Um, I also think that it doesn't really fit
our scientific culture very well because we typically have, you know,
each of you working in a small team on a paper every couple of years or in
Chelsea's case you know 16 papers for a year or more, I've lost count.
But either way, what we don't tend to do is stop and have like
an entire CERN like effort or
Apollo program like effort to put all these pieces together.
And that might be what's required to get all of those pieces I had on the,
the last slide all together into one functioning working machine.
So, uh, I think that if you look at the overall trend in machine learning recently,
there's a clear trend.
And that is that hand designed pipelines give way
to learn solutions over time once we have sufficient data and compute.
Okay. And so this we've seen this over and over and over again.
When we first try to solve a problem,
we typically try to hand code the whole thing.
That doesn't work, so we say I'm going to hand code part
of the pipeline then I'm gonna sprinkle some machine learning in.
And eventually, once we have enough data to compute,
we just realized we should have learned the whole thing from the beginning.
This has happened with features such as HOG and SIFT giving way to deep learning.
It's happened with architecture,
the best architectures now in CIFAR and ImageNet are learned,
they've been searched for automatically.
They have not been designed by humans.
We're seeing this with hyper parameters and data augmentation,
and increasingly a focus of this class,
we're seeing that hand designed learning algorithms are giving away
to learned learning algorithms.
And so this trend suggests an alternate path to our most ambitious goals
as a research community and that is what I call AI-Generating Algorithms.
So the idea here is that we would learn as much as possible.
This is an all-in bet on learning.
And the idea is that we have one algorithm that we start and
it starts off simple with simple origins and it
bootstraps itself up from simplicity to ultimately
being extremely intelligent and have a complicated AI.
It's gonna do this by having an expensive outer loop just like meta-learning.
That is, it requires a lot of compute,
but on the inner loop,
what it does is produce a very sample-efficient learner.
Which should be familiar to this class.
And we have an, an existence proof that this can work which is earth, right?
The very, very dumb computationally
inefficient algorithm of Darwinian evolution produced you
and all of you are extremely sample-efficient learners
inside of your life in the inner loop which is your lifetime.
So the question is, can we make such an algorithm?
And I think that if we want to pull this off,
then we need progress on three pillars.
So I call these the three pillars of AI-GAs.
The first one is that we need to meta-learn the architectures of the algorithm,
then look the neural net architectures, for example.
The second one is we need to learn- meta-learn
the learning algorithms themselves which is the focus of this class.
And then the third one, which is not very well, uh,
talked about and kind of the least researched and
the least understood is automatically generating effective learning environments.
So in this, what I want to point out is that hand crafting each of these things,
if you had to like, design your architecture, the algorithm, and the environments,
that's very, very slow and it's limited by our own intelligence.
In contrast, it's better to learn all of these simultaneously and let M- um,
like ML and compute do the heavy lifting.
So in today's talk, what I'm gonna do is I'm going to talk about work that
we've been doing in each one of these pillars in turn.
Quickly, I want to mention a little bit more about
AI-GAs and then I'll get back to the meta-learning algorithms themselves.
So one thing I do want to point out is AI-GAs
like any search algorithm are not a building-block-free approach.
You still have to decide what your search space is,
what your search operators are.
But the hypothesis is that the AI-G pa- AI-GA path has fewer building blocks that need
to be identified versus the thousands or hundreds that exist in the manual path.
And therefore, it's going to be easier to find them and get them to work together.
I also want to admit right out of the gate that AI-GA
is going to be way more computationally inefficient.
It's gonna require tremendous amounts of compute.
But if you look back at the history of machine learning,
I think that it's okay because, um, you know,
computation speeds up exponentially and some of
the best algorithms were built long before we had the compute to take advantage of them,
like deep learning, for example.
And the early research allowed us to pounce once the computation was available.
I also want to point out this top bullet,
which is probably where the most important research will happen and that is
efficient abstractions of whatever it was that
produced the miracle that happened on earth,
those will help us shave orders of magnitude off
of the planet-sized computer that was required to produce us.
So that's where the real interest is.
So are AI-GAs a faster path to AI than the manual path?
I actually think that it's very debatable,
but I ultimately have concluded after thinking about this that I do think
AI-GAs are a faster path to AI but I have high uncertainty.
So I do recognize that either one could win,
but in terms of the arguments for why,
I think it's in line with this trend in machine learning.
I think it scales really well.
It doesn't require human genius.
So I borrowed this slide from Pieter Abbeel,
which typically he only has these two pie charts and I've added this one over here.
And the idea is that we don't have to require, you know,
identify and combine all these building blocks.
So I think one thing that's interesting especially for an audience like this,
which are PhD students mostly,
is you know if you want to think about where you want to spend your time,
I would argue that it's really interesting to spend your time on things like AI-GAs
and meta-learning because it's kind of like if you can go back in time and ask yourself,
15 years ago, would you rather work on HOG and SIFT?
They were the dominant technique.
They would look way better than neural nets.
But ultimately, the learning of a solution took off and
surpassed and has been much more general and much more powerful.
So if I could go back in time,
I'd want to work on neural nets.
Actually, I was working on neural nets around that.
So that's already happened.
Um, but you should also if you had a time machine.
Okay. So I do think that community should reallocate more effort
because even if I'm right that there's some probability mass on AI-GAs,
virtually all of the probability mass right now is on the manual approach.
And so we should reallocate some of our effort to this approach.
So there's more discussion in this paper here.
I don't have time to go deep into AI-GAs today.
That's not what this talk is about.
But I discussed at, at length wh- who's going to win,
the manual path or the AI-GA path,
I argue that AI-GA is very interesting- intrinsically interesting even if they're not
the fastest path to AI that we really have to be worried about
safety and ethics concerns with them that are unique to this type of algorithm.
And also that this should be considered its own Grand Challenge of science.
So you can check all that out if you want in the archive paper.
But now what I wanna do is start telling you about
some meta-learning techniques that get us down this road a little bit.
So I'm going to systematically go through these three pillars.
So let's start with the first one which is meta-learning the architectures,
which also is known as architecture search.
So this is a project called Generative Teaching Networks.
It's under review right now at ICLR.
So fingers crossed.
It's with this fantastic group of collaborators that I really want to single out
Felipe here who has definitely done all of the heavy lifting on this project.
So the idea in general with architecture search is that architectures matter a lot.
If you look at for example ImageNet,
a lot of the gains have been architectures and therefore we should search for
them ins- instead of trying to manually design them. So how might we do that?
Well, a really common approach, uh,
like an idea that's at the core of all these NAS methods,
which stands for neural architecture search,
is that you train on real data for some short amount of time that gets you
an estimate of how good an architecture is and then you do something with that estimate.
You could do something really simple like
just do random search and take the thing that should, uh,
looked the best or you could do something really
fancy like modeling the space of searches, etc.
But the point is that all of them in their core
tend to have this idea of training for a little while on
real data, getting an estimate a- after of those, a moderate number of SGD steps and then,
uh, you're off to the races.
So the question is whether or not we could
speed this up and that's what we try to do in this project.
So instead of training on real data from moderate number of steps,
we're going to try to train which is a very,
very few number of SGD steps and see if that can perform better.
And then we're going to use few step accuracy.
So the accuracy after a few steps of SGD is an estimate of
the asymptotic performance of the network if I trained it,
you know, for 600 epochs for a lot of compute and data.
So why might that work?
Well, if you think about a, a typical training set,
it might look like this, is a T- TC plot.
You might have like lots and lots of this kind of
zero and lots and lots of this kind of zero, for example.
It might be the case that you could get away with just a
few of these samples and a few of these samples.
If you intelligently sampled these data points,
then you could do better than just training on
all of the datasets because there might be redundancies.
And there's been work that's just basically shown that that in fact does work by sub selecting,
uh, real data, you can do better.
But if you think about how humans learn,
we don't always learn a task by doing that task.
I don't only learn basketball by playing basketball.
And in fact, sometimes I do drills,
which do not exactly resemble the sport itself.
For example, this basketball drill is really common,
you dribbled two balls at once which never ever,
ever happens in a basketball game.
But you can still learn that way.
Additionally, you can actually watch videos of somebody
playing basketball and learn how to play that way via observation.
And you can also read a book about basketball which is really
crazy because [inaudible] does-
sorry, it actually does, uh- teach you a lot about the game,
which I think is really interesting because it means that we can generate
different types of data that might speed up our learning,
more than the real than- and it doesn't have to look like the real data.
So uh- also, I wanna point out that,
over time, teaching methods improved.
You get better drills,
better videos, and better books.
So the question is,
can we meta-learn to generate training data that allows us to rapidly learn a new task?
And if so, that could help speed up neural architecture search.
So there has been work already on this which is really interesting wi- this paper that
really blew my mind is this hyper-gradients paper from Maclaurin in 2015.
They treat the data as a hyperparameter of the algorithm,
differentiate that through all of the SGD to the pixels of the images for MNIST,
and then it learns what data it should produce
to end up having a learner that trains on that data do well on MNIST.
They only generated- they only lear- learned 10 samples and these are the 10 samples.
And as, as you can see, they look like kind of platonic digits.
And then this paper here did that with 100 samples.
So one of the things that we wondered in this paper is,
is instead of da- generating data pixel by pixel,
which kind of leaves a lot on the table in terms of
learning regularities about the data themselves,
and about the search space etc.,
our idea is that we're gonna learn a generator to generate data to accelerate this process,
and we call that a generative teaching network.
So here's the general method,
a GTN is going to generate data,
which a new never seen before learning neural net,
which is a new architecture,
new initialization will train on the synthetic data produced by the GTN.
And then we're going to optimize the GTN with meta-learning to produce
good data such that that learner performs well on the target task.
After a very few number of SGD steps.
So here it is in picture form.
We have the inner loop.
You could generate a noise vector Z,
pass it into the generator, it produces data.
This li- like a big batch of data.
The learner iterates over a few steps of SGD,
maybe 32, for example,
gets its final weights,
pass that out to the- to,
uh, evaluate on real data,
such as MNIST data,
see how well it does,
differentiate back through that entire process like MAML, for example,
and get back to the original weight to the generator,
and change the weights of the generator to generate better data,
then throw the learner out and repeat the process over and over again.
Does that make sense? Okay. So couple of things to note.
There's a- only a few number of steps here,
so you're implicitly incentivizing this generator to
train the- to create data that enables rapid learning by the learner.
And another thing is that we're also going to meta-learn
the inner loop hyperparameters like if it's got- like the momentum,
for example, in SGD.
And we're gonna do that for both the controls and
the GTN to have it be a fair playing field.
[NOISE] All right.
So our domains are gonna be MNIST and
CIFAR because this is really computationally expensive.
So the first thing we ran into when we tried this is that,
uh- it's very unstable.
So this is what would happen when we tried to train,
and then Felipe came up with a really good idea which was to take this idea from,
uh, Salimans & Kingma,
which is weight normalization.
And just reparameterize the weight vector w into a v vector, normalize,
and then by la- multiply by a scalar g,
and then we learn both the weight vector and that scalar,
and that dramatically improves the stability of meta-learning.
Uh, so to show you that- here,
these are different hyperparameters of the algorithm
without weight normalization and with weight normalization.
So before we did this,
we were having a lot of trying.
We spent a lot of time and compute,
trying to find the hyperparameters that would make this thing work.
And then after we did this,
it was just trivial for everything just kinda worked.
So we, as a side note,
hypothesize that this might be a good trick for all meta-learning.
We're gonna try to do a whole paper just on that.
Uh, but if your- if your project's not working,
I recommend you try this one line code change here, it might really, really help.
Here's a look at the performance difference between weight norm and not.
This was the unstable one,
and this is the performance curve with weight normalization.
So now that you see that,
I just want to stop and say,
it worked, like we didn't know we started this,
but then it would be possible to generate data, learn and consume,
and then do well on real data when the learner has never seen real data.
But it turns out that it does work.
So this is performance with the GTN,
it gets about 97.5% on MNIST.
So my question to you is,
what are the samples look like?
What do you think? Do you think that look realistic or unrealistic?
Unrealistic.
Unrealistic. We have one brave soul who's willing to speak up.
Here's what the samples look like.
I would mostly agree with you.
They- I mean you can tell that they're digits.
You can tell this is MNIST.
But they look pretty alien and weird,
which I think is quite surprising.
But the interesting thing is you can train a network on these digits and then it can
do just fine on real MNIST digits which is kind of crazy.
Some of them look really- pretty recognizable like
this three here and some of them like totally alien,
like this four, there.
And so the idea that unrecognizable images can meaningfully affect neural networks,
is reminiscent of this finding we had on our paper from 2015,
"Deep Neural Nets Are Easily Fooled,
" which I think is quite interesting.
And we have lots of hypotheses for why these,
uh, GTN samples are unrecognizable.
So if you're interested in that, you can ask me that in the Q&A,
Q&A at the end of the talk,
but we're still kinda speculative,
so I didn't put it in the core of the talk. All right.
So the next thing we noticed is that instead of randomly sampling this Z code here,
we can do even better because if you really want to teach somebody fast,
you shouldn't just give them random data.
You should teach them with a curriculum.
So instead what we can do is we can cross off
the noise generator and we can a put a- put a learned tensor here.
So now what we're gonna do is we're going to just learn the block of Z codes,
the fixed block that we're gonna use throughout training.
And the Z code is this length by
the batch size and then this is the number of inter-loop steps that you do,
and then you just learn each one of those numbers and pass that in to the network.
So we found that,
that greatly boosted performance.
Here is your learning curriculum versus your no curriculum, right here.
And so from here on out,
all of the results I'm going to show are with the curriculum version of GTNs.
Okay. So here's the first like really,
really kinda fun comparison of real data to synthetic data.
What you find is that if you only have 32 steps of SGD,
it is way faster to train on GTN produced
meta-learned synthetic data than it is to just take real data and train on it.
So real data is this blue curve here.
So my first pop quiz to the class,
why is that line going up?
If it's just real data,
why does it go up over time?
[NOISE]
Anybody? Yes.
Because we are learning hyperparameters?
Yes. Thank you.
I was gonna call on you first.
Uh, yes, that's right.
Remember, we're learning the hyperparameters of the real data algorithm too.
So it's getting a better SGD momentum and things like that over time. Yes?
So did you backpropagate to 32 steps?
Yes.
Did you-
Yes. That's right. And in fact,
what we'll see later is we can do even farther than that.
I think we got up to 128 and maybe Felipe- Felipe is amazing.
He was able to push it even farther than that, I believe.
But those results are not in this paper.
All right, cool. So the blue curve is real data,
eventually, it gets us good hyperparameters and it's stuck.
This is dataset distillation,
100 samples directly pixel by pixel learned.
And this is when you have a generator which can take
the advantage of all sorts of regularities across examples,
dconf, priors, etc., etc.
That's- this is outer loop training.
So this is meta-learning. This is inner loop training.
It's a little bit noisy, but this is real data and this is the,
uh, the GTN data.
And what we basically found is that it is,
uh, faster to train on this data.
So what we wanted then to do is go back to the original motivation,
which is neural architecture search and say,
can we use this synthetic data to more rapidly
figure out what architectures are good in the search space?
Uh, and so we're gonna do this in CIFAR because that
is the standard NAS benchmark that everybody has been working on.
And if you look at CIFAR,
the story is basically the same as with MNIST.
This is real data over time.
And this is GTN over time.
And you can see that it performs way better.
And then this is-that was outer loop.
On the inner loop, you can see that- and you can
basically get the fa- the performance, uh,
of real data at 128 iterations,
four times faster with GTN or for the same budget you can go to a better performance.
And so this is way faster to,
uh, to learn on.
Now like with MNIST,
the samples here are pretty weird,
pretty unrecognizable, and pretty alien.
I mean, you can kinda tell that CIFAR,
if you've been looking at CIFAR images for way too long, which I have.
Uh, but yeah, you can't really recognize what any of
these things are versus real CIFAR images.
But yet, if you train on these,
you end up performing really well.
So now the ultimate thing that matters in
architecture search is not the actual performance of your learner,
after learning, because we don't care about its real performance after 128 sets.
What we care is if the estimated performance after we do that 128 steps of SGD,
by the way that's 128, I don't know if you noticed it,
but the CIFAR ones, uh, go for 128, so even longer.
Anyway the- uh, what you care about is after 128,
then we think this architecture is pretty good or pretty bad.
Does that actually correlate with the true asymptotic performance if I take
that same architecture and I train it
forever on real CIFAR data or whatever my domain is?
And what we found is that there is actually a pretty good correlation.
So the top 50% of GTN architecture.
So the architectures that GTN thinks are the best,
tend to also be the best when you train them much,
much, much longer on real data.
And in fact, in the, uh,
in the top 50% that correlation is
0.56 and if you take the top 10 GTN architectures,
they are basically also a lot of them are in the top 10 of true performance.
So it doesn't matter that you pick this one which isn't
actually good because you also pick this one which is good.
So as long as you get enough of those, you're fine.
And so, um, also what we found is that you can get as good of a rank correlation with
the fake synthetic data as training on real data
for in 128 steps of GTN versus 1,200 steps on real data.
So to get the same rank correlation,
you can either spend 128 fake data steps or 1,200 real data steps,
which means that GTN is 9X faster,
giving you that estimated performance
for that new architecture that you've never trained before. [NOISE]
So, um, that's pretty exciting,
and what that allowed us to do is basically, um,
produce something that is competitive with
the state of the art for neural architecture search.
So, uh, what we- because GTN is a drop-in replacement for real data,
we didn't do this with really fancy neural architecture search algorithms
because these are often really expensive,
we just took random search.
We either did it with real data or fake data,
and then we added some tricks that people use and basically what you
find in every paper in machine learning is that our method is bolded, because it works.
Um, but what this is pretty cool is what it means is that,
you know, in just a few GPU days,
you can basically find something that's near state-of-the-art in CIFAR,
uh, computation, ah, using this sort of tricks.
And what's also interesting is that, er,
there are other neural architecture search algorithms,
like I mentioned, that do more fancy things but
their innovations are orthogonal to using fake data versus real data.
So you can hybridize this GTN technique with them and they would perform even better.
We haven't done that yet because we're not NAS experts,
but we think that's an interesting direction for future work.
So the next thing I want to point out which is really provocative but also fair warning,
it's preliminary, is that this idea should
work really really well for reinforcement learning.
Not a lot of people do architecture search on reinforcement learning,
but I think that will change if you think back to the AI-GA paradigm.
So here's pole balancing which is admittedly very simple.
And what you see here this is A2C over time,
and this is the performance up to that environment step.
The blue line here is taking a new neural net at every single iteration here,
a new neural net and doing one step of SGD with synthetic data.
Which means that this red line point here
took 100,000 steps and this blue line point here,
it took 100,000 steps to train up the GTN but at that point,
I can zap in the knowledge of how to do pole
balancing into that new neural network in one SGD step.
So if I stopped and launched architecture search right there,
with a whole lot of caveats like does it,
does it actually, is a predictive, you know,
out of distribution on new architectures, etc,
that's 100,000 times faster.
Now I don't expect that to hold up on a much harder problem,
but the same ideas that worked in supervising learning
might also work here in RL and actually,
I would argue that they're probably gonna work even better.
Why? Because in RL,
the hard part is exploration.
But once you've learned how to solve the problem,
you can teach the next network how to solve
that problem really really fast and see how good it is,
at least at performing the problem,
which is a little bit of a subtle different question
for whether or not it's good to learning the problem.
So it depends on what you want that,
uh, architecture search for.
But anyway, I thought that was interesting. All right.
So this is the conclusion to the first mini talk within this talk or the
second one if you count AI-GAs and this is for GTNs.
So GTNs produce synthetic data that trains neural nets faster than real data.
It generalizes to new architectures,
which it has never seen before.
It enables, I should have said by the way, when we train a GTN,
we train our distribution of architectures,
but then when we tested it,
we test it out on wildly different architectures because
the neural architecture search goes really far away from that initial distribution,
and we found that correlation works quite
well even on those totally different architectures.
Uh, it enables you to rapidly estimate the performance of new architectures,
er, and we think this is a really generic approach.
It could work with supervised learning, unsupervised learning,
semi-supervised learning and RL and we mostly focused on this and
then showed you a little preliminary results on RL.
It also produced a state of the art, uh,
competitive architecture search algorithm but through a totally different means,
which is like an exciting new tool to have in the toolbox for neural architecture search.
Cool. All right.
So that is the first pillar, meta-learned architectures.
Now we're going to get into the home turf of this class which is meta-learning.
And so the way that I see the world and I hope this doesn't conflict too
much with Chelsea's summary of the field from that in the other lectures,
is that there are, kind of, two large camps of meta-learning.
One of them I think is that you meta-learn
good initial weights and then you hand it over to SGD and you.
Er, let it, let it go and I would say that's, kind of,
the MAML style that Chelsea is pioneering,
and then I would say the other camp is this kind of
idea that you're just going to meta-learn a recurrent neural network and
it itself is going to invent its own learning algorithm within,
it's not going to use SGD at inference time,
it's just going to use the activations within the network to
kind of create its own learning algorithm.
This is the learning to reinforcement learning Jane Wang paper,
and also the RL squared paper that came out at the same time
from Rocky Duan and those at OpenAI.
I think they're both awesome, both really interesting,
you'll see work in this talk on both of them but I like to give that high-level picture,
and so let's just quickly focus on the second camp,
I know you know the first camp very well,
probably know both of them but this is the LRL camp.
So in the outer loop, you optimize a recurrent neural net
with parameters theta for lifetime performance.
So you take this neural net,
you deploy it in a world,
say like Montezuma's Revenge,
you let it play, you see how well it does,
and then either you differentiate back to the original parameters of theta,
so you- the next time you deploy that net it's a little bit
better or you could do like an evolutionary algorithm and just,
you know, mutate the parameters and see if it does better etc.
etc. So you have the outer loop thing optimizing
that initial neural net and then you deploy it and there's
no SGD going on within its lifetime if you- I like that metaphor of a lifetime.
Um, it does get the reward as an input.
So it can actually implement its own reinforcement learning algorithm.
Now, RNNs are Turing complete,
so in theory this thing can implement any learning algorithm that's out there which is,
kind of, exciting, um,
and what was nice is that in this paper here,
they show that it learns on its own in RL to
exploit and- to explore and exploit which is kinda cool.
So you put it in this maze here,
it doesn't know where the reward, is it explores over here,
then over here, then over here,
then over here, and once it finds the reward,
it changes the activations within its own network to stamp in that knowledge,
is to learn where to go and then it goes back there over
and over again which are all the blue dots here,
and you can see it vastly outperforms A3C which doesn't have that capability.
Um, they also show,
which is really interesting, that it
kind of invents, maybe,
its own model-based RL algorithm within
its activations and they have some arguments in here but there are also some caveats.
So I encourage you to go check it out but I think it's provocative.
So the thing that I'm going to put out there for
this talk is some new work that's pushing in this direction is while
it's great to think about an RNN as Turing Complete,
oftentimes just being Turing complete that- isn't enough.
We're not going to actually search for AGI in the space of Turing machines.
It's a very inefficient representation to have your program run in.
So the idea here is that materials matter.
You still have to choose those building blocks when you go to do meta-learning.
And so, um, you know,
LRL/RL squared, those recurrent neural networks,
they have to learn everything within their activations,
that's not how you learn.
In your life, if you learn something in this class, for example,
it is not just looping around in the firing of your neurons.
That's what happens if I give you like
my telephone number and you have to remember it for a couple of seconds.
But, if you remember something for, uh,
anything longer than about that,
it's going into the weights of your brain and not in the activations of your brain.
And so one of the things we could do is we could try to get learning to happen
within the weights of the neural net not just its activations.
So the first paper in this arc here is Differentiable Hebbian Learning.
This is work that's done by myself,
Thomas Miconi, and Ken Stanley.
And I really want to highlight Thomas here who's really been
pioneering this direction both on
this paper and the next paper I'm going to tell you about.
He's a fantastic scientist.
So the idea here is that we're going to deal with Hebbian Learning.
How many of you are familiar with Hebbian Learning?
About 40%.
Okay. So the idea that Hebbian Learning is that you
can store information in the weights of the network in addition to the activations.
You can now get to use both.
But, what's different about this work than anything you've ever heard of
before is that we're going to train the Hebbian learning with SGD.
So we're going to be able to take advantage of that very powerful tool
to sculpt very carefully tuned Hebbian parameters.
So just like LRL/RL squared,
we're going to have a recurrent neural network that's going to be deployed
at inference time and it's going to have no SGD inside it.
But now, it's going to be able to use Hebbian learning
to do so not just like change its own activations.
So the one slide summary of
Hebbian learning is that neurons that fire together, wire together.
You've probably heard that phrase is how quickly it's- it's usually described.
So here's how it works, the new weight in the network,
so the weight- the network- the weight between i and j at time t plus
1 equals the old weight plus a little bit,
this is just a learning parameter, a learning rate parameter,
of the output of the post and pre-synaptic neuron firings.
So if the neurons both fire in the same direction both positive or both negative,
you get a positive value, the weight increases.
If they both- if they fire in the opposite way,
so one fired positively,
one fired negatively, you get a negative value and the weight decreases.
And so these will kind of reinforce themselves.
Now, that might sound like it's hopeless and will never do anything interesting.
But in fact, people for a long time have been showing that this kind of
unsupervised learning rule can do lots of really powerful
unsupervised learning including like PCA
and associative recall where you give a few numbers of a phone number,
a few digits of a phone number,
and then it will return like the full phone number.
You give it a few notes of a song and like the whole rest of the song comes back,
and this is happening in your brain.
Uh, neuroscientists had been showing for a long time.
So what we're going to do here is we're going to do Differentiable Hebbian Learning.
And so the idea is that we're going to set up Hebb's rule
inside of a neural net and then we're going to let SGD sculpt its parameters.
So here's how this works.
You take a recurrent,
we're going to call it a plastic network because the weights themselves can
change and we're going to train it end to end with gradients.
So in the inner loop,
the net- network ends up updating with no SGD.
It's just kind of going through its own motions and
updating itself according to these Hebbian learning rules.
And then in the outer loop,
we're going to differentiate through the entire episode to update the training parameters
with SGD. This is a little confusing because we're using SGD in the outer loop,
but not on the inner loop.
So here's how it works.
The- the, uh, output of any given neuron is going to be a non-linear- non-linearity and
then your typical weighted sum where you have- now
the weights here where typical- the typical weighted center where you have a weight,
which is the whole parenthesis here,
times the incoming activation.
But, what you have is a fixed part.
So this is the normal, kind of like a normal weight.
This is just learned by, uh,
SGD and then you have this plastic parts.
So inside the lifetime of the organism this component of the weight will change,
and then this gets added to this,
and so the weight over time can change. That makes sense?
So this Alpha here is a term that allows
each weight to have a different ratio of the fixed part and the plastic part.
So SGD can choose.
These weights over here,
we always need them to be set to these values.
So just crank Alpha to 0 and we'll only use the green part of the weight.
These other weights here can be a mixture of
both and these weights over here can be like entirely plastic,
and have no fixed component whatsoever, and everything in between.
So SGD gets control Alpha in this case.
And then, what is Alpha being multiplied by?
Well, we call this the Hebbian trace here,
this h. This is a purely lifetime quality- quantity.
Gets initialized to zero every time.
And then, the way that it works is that the new Hebbian trace
is just equal to a frac- like the old Hebbian trace,
a bit of that, and then also a bit of whether or not the neurons fire together,
so it should wire together.
That'll make sense? Cool. So what we found on
a variety of experiences that this works remarkably
well and often really does better than LSDMs,
and otherwise non-plastic recurrent networks.
So on this, um, task here,
an Omniglot at the time this was tied for state of the art with Chelsea.
Here- here you go and some other, uh, papers.
And basically, it does about as well,
but uses this totally different method which we think is really interesting.
You can also take, uh,
this neural net here and you can take this really simple
like kind of associative recall pattern recognizing thing.
So you might give it a series of patterns here,
and then you give it a partial pattern,
and its job is to fill in what it saw.
So this is kinda like I give you a bunch of phone numbers.
I give you a few digits from one of the phone numbers that you
saw and you have to give me the rest of that particular phone number.
And what we show there is that an LSD, uh, sorry,
a non-plastic recurrent neural net does not perform very well on this task.
LSDMs actually, ultimately, perform well,
but look how much faster the Hebbian network learns this task.
Uh, so there's a pretty dramatic difference there.
And then we went to a much harder version of this.
And we said, "All right. We're going to give the network of series of images."
So this is now a pretty high dimensional search space.
We're going to flash these images,
and then will flash it half of one of the images,
and it has to fill in the rest of that image.
And it has to be able to do that with an image that it's never seen before.
So this is a lot of high density information storage, right?
You get shown like a couple of images and then you have
to remember every pixel from every one of those,
so you can reconstruct it when you only see half of it.
Storing all that information in your activations might be very hard,
but the Hebbian learning makes it easier because you can use the weights to do it.
Sorry. So, uh, I don't have a plot here.
LSDMs couldn't even solve it and our network actually does quite well at solving that.
And then, that's worth two million parameters which is
kind of interesting because in the history of the Hebbian learning,
it's always tiny little neural nets.
And so now, we're able to scale Hebbian learning up to
two million plus parameters which is
kind of a new era for Hebbian learning, which is pretty interesting.
So the next task that we have here is maze navigation.
And so, uh, this is kind of cool because it learns,
again, on its own to explore and exploit.
So you drop an agent,
which is this, uh, yellow thing, here in this maze.
It can only see its local neighborhood.
It has no idea where it is and what it's supposed to do.
What it eventually figures out over meta optimization is like, oh, I see.
You want me to explore this maze.
And then the second I get to the green thing,
you want me to go back to it as many times as possible.
That's the reward function.
So the initial random network you see here it just kind of bops around.
It does nothing, as you would expect.
But then, look at this thing.
What it basically, I don't know- yes.
So the first time it has to explore until it finds
this green thing and then it just shoots right back to it every single time.
So it has remembered the maze.
It knows how to explore that maze.
It knows how to find the treasure and then it knows how to return to it over,
and over, and over again, which is pretty cool.
So this is Learning to Reinforcement Learning/RL squared down here.
This, uh, [NOISE] black curve.
No, sorry. The non-plastic one, red.
And what you see, this is what like a normal LSDM does for example.
And then, uh, if you just have uniform plasticity,
so you don't get to learn a per weight Alpha,
but just one Alpha for the whole network, it also doesn't work.
It's only once you give the degree of freedom to SGD to
have that per weight Alpha within the network that it can do much,
much better on this task,
which I think is pretty interesting.
Okay. Any questions about that? Yeah. There's one right there.
[BACKGROUND]
I'm now forgetting whether it saw them all at training.
I think I'm pretty sure that it, uh,
at test time, we show a new images, and I can do this task.
Yeah. Virtually sure that we would have done that,
but- I'd have to check the paper.
I'd spent a little while since I read it. Cool, yeah? [NOISE]
[BACKGROUND]
Yeah. I think that's a fair thing to think about.
I mean, there's- obviously,
you can store information and activations.
You can store it kind of to disk in
the differentiable storage and you can store it to weights.
and I think there's gonna be pros and cons to the different approaches.
But I thi- both- I think it's kind of attention with
the differentiable plasticity is that the weights
are a part of the- kind of like computation,
and so, um, that has pros and cons like the-
the DNC style stuff you kind of write it to
disk and then it becomes like the input to this program.
Whereas, in the Hebbian cage,
you kinda get to change the program on the fly, and so, uh,
it's very hand-wavy but I think in some situations one is gonna be easier than the other,
and I don't think we know yet really where
like which type of problem one shines on versus another.
Um, I think you could get a DNC to do the image completion task, uh, for example.
But my guess is that there are gonna be other types of tasks where you really
kinda wanna bend the program in different ways temporarily
that it's gonna be hard to mold the DNC to have like a generic function that takes
different inputs and does wildly different things as
opposed to kinda changing a few ways to change the program.
It's just my instincts. But that's like a really interesting area
of work just to compare these different approaches to storing information.
Great, those are good questions.
Okay. I'll push on. So the next thing that we wanted to look
at was a subject that's near and dear to my heart.
I've worked on this in my lab in Wyoming on smaller neural networks,
and now we're getting to see it at scale,
and that's the idea of neural modulation.
And so here, the idea is that,
uh, this comes from another paper that was led by Thomas.
It's also written by Rawal et al.
Um, but Thomas did the heavy lifting here.
And so the idea here is we're gonna do Differentiable Neuromodulated Hebbian plasticity,
so it's taking it one step further.
And the idea is that Hebbian learning is very local.
If the- you know, every little connection is kind of
getting updated according to the data that's flying through it.
It's a very difficult optimization process to kind of harness and, uh, and heard.
And so what you might want is that you want learning in
a set of like a certain subset of the weights only in certain situations.
Like maybe once I've learned to solve the problem,
I want like all the ways to just stay put and not do anything.
I don't want the heavy and bouncing around,
and so I want to like freeze learning.
But then if something happens,
that I might wanna like crank learning up.
But I might not wanna crank it up everywhere,
I might wanna crank it only up in the part of the network that was
responsible for the task I was just solving, for example.
So that's the idea behind neural modulation.
In neural modulation, you can have one neuron in the network like
this one here inhibit learning in another part of the network,
and so we can like basically [NOISE] you could say for example,
if I'm playing chess that only turned learning on the chess-playing part of the net- uh,
network and turn it off everywhere else.
So I don't overwrite information and other parts of the network.
Ah, that's kind of a cartoon example.
So how do we pull that off in practice?
Well, we do, um, this Differentiable Neural Modulated Plasticity
or we call it Backpropamine, Thomas came up with that.
And the idea is we're gonna have the same Hebbian formulation,
the notation is slightly different but it's basically the same idea as before.
But the new part is now,
the new Hebbian trace is the old Hebbian trace plus whether
the neurons fired together or not multiplied by the output of some other neuron M,
and that other neuron can be entirely a learned function of data.
So that could be the thing that tells you whether you're playing
chess or not [NOISE] or you are doing- or Chelsea
whether or not reward just went way up or way down
or some other complicated function of the data.
Now, in addition to doing that which is pretty interesting,
there's also in the paper an eligibility trace version.
So what that says is,
don't just turn learning on and off in certain contexts but store
information about which neurons were involved in what situation.
But don't do anything with that, just store it.
And then if something happens later like, you know, in
100 steps like I had a really big reward or a really big prediction error,
then go back and change the neurons that were involved in a certain way.
So if you read the Sutton & Barto book,
there's a lot of work on eligibility traces in RL which is pretty interesting.
So here's an eligibility trace version of differentiable plasticity.
So in a nutshell,
this works even better than
Differentiable Hebbian Plasticity at least on the pro- some problems.
So on a simple task here where we were trying to, um,
basically recognize whether or not
a certain symbol had been given to us a couple of time steps ago,
then what you see is that the Hebbi- Differentiable Hebbian Plasticity which is,
um, the green and the non-plastic networks at all don't solve that problem at all,
and both the eligibility trace version which is we're calling retroactive and a
simple differentiable Hebbian neuro
modulation version which is kind of a normal backpropamine,
both of those solve that problem very well.
[NOISE] And then here on this maze task, um,
they both solve this problem way better than, uh,
non-modulated plasticity which is not even on the charts on this.
Oh actually, no, it's here in blue.
And then same thing down here on this Penn Treebank problem.
Cool. So [NOISE] the final thing that I wanna show you- ah,
I've just asked, is there any questions on that?
So I think it's pretty cool. Now, you have something wildly different than SGD.
That's controlling, uh, the- the neuron- the neural network at inference time,
and it can turn learning on and off at particular connections
and in different contexts in different connections,
which is a lot of extra degrees of freedom for meta-learning.
[NOISE] Okay, so the next thing I wanna talk about,
this is unpublished work.
I don't- we've never presented this outside of DARPA meetings,
so you're the first people to see it.
This is a project that, uh, uh,
I'm very interested in which is trying to learn to continuously learn.
This is done with this fantastic team here,
and I really wanna call out Shawn who has been
the lead author and has had come up with a lot of the innovations on the projects.
So in my opinion,
one of the Achilles heels of all of machine learning is catastrophic forgetting.
How many of you are familiar with this term?
Oh everybody, that's really good.
Okay. So then I will just
do this briefly because I know at least we're putting it on YouTube.
Um, the idea is that when you're sequentially learning a task,
you first learn task A,
[NOISE] then you learn task B.
And what you do is typically machine-learning models when they're learning B,
they have no incentive to hold on to any of
the information for A so that it is override everything they knew about A,
and it corrupts A and then they lose the skill for A, right?
That's your classic catastrophic forgetting.
Now, animals including yourself don't do this.
You're able to like, you know,
study for this class and go to some other class and study for that,
and then go like play badminton which you
haven't played in 10 years and just pick up where
you left off without corrupting your knowledge for all of these classes.
And through our lives, we get better and better at a variety of tasks,
and if we forget what happens gradually,
not catastrophically, which is what happens in machine [NOISE] learning models.
So I think we have to solve this if we want to make major progress in AI.
I think it's kind of embarrassing how little progress
we've made actually on this problem.
But that's not for want of trying, and we've been working on
this prob- prob- problem for very, very long.
And there's a lot of early work that was really
interesting that had all these different kinds of techniques.
But one thing that unifies that in my opinion is they're all kinda manually designed.
This is like the manual path to AI,
I think I know how to solve catastrophic forgetting what
I need are pseudo rehearsal patterns or,
uh, I need to have sparse representations,
so I might have an auxiliary loss for that, etc, etc.
There has been recent work that I love
also but I would also put it into the camp of stuff that's manually designed.
So EWC which is a wonderful paper is still kind of a hand design technique.
I think I know how to solve this problem,
let's use some really cool math and
Fisher information and we're gonna try to solve it,
same with progressive nets.
And there's been more and more and more and more,
and it's all kind of manually designed.
But, you know, my proposal or our AI proposal that it kinda comes from
the AI-GA perspective is like let's not try to figure out how to solve this ourselves.
Let's just set the problem up,
ask machine learning to figure out how to solve the problem,
which is to say let's learn to continuously learn and meta- learning the solution.
So a hypothesis that we're not smart enough to build systems that can continually learn.
So let's do, uh,
the AI-GA thing and, um, you know,
that is in contrast to the manual path which might say we might want
sparse representations so let's kinda like create
an auxiliary loss trained for that and hope that that works.
So I like meta-learning because you just get to
kinda ask the system to produce what you ultimately want.
So going back to these two camps of learning,
now we're going to flip to the other one.
That's more like the MAML school of meta-learning here to use that.
So I know that you're all very very familiar with,
uh, this kind of meta- learning and inner-loop learning.
But I'm gonna introduce you to some of the terminology that I'm gonna use
because this talk- this part gets really complicated. So I don't want to lose you.
So the general gist is that,
you know, with a typical MAML approach,
you start with your parameter vector here and then within one inner-loop of training,
you copy it and you start doing inner-loop steps.
Uh, and then at the end of all of that,
you evaluate on your meta loss and you differentiate back through
that entire block to the original parameter vector.
You take a gradient step,
and then you repeat the process, right?
Standard for this group.
So I want to introduce some terms.
I think you use these terms.
I think I've heard you and Sergey use these terms.
So I like that were maybe starting to create some standards in the community.
So we call this whole process meta-training and then in particular,
I don't know if you use these terms?
Oh, you do good. Okay. So you- this will be old news for you.
But we had to like kinda basically really commit to using
these amongst our team or we were just constantly talking past each other.
Probably it's happening in your class projects.
So we call this meta-training training data.
Whatever it's training on in here.
So meta-training training data and then
the stuff that you evaluate here we call meta-training test data.
You could also call this meta-training validation data at your preference.
But I'm going to call it test for symmetry and so
it's good to keep that in your head, and then,
what you do is that there's after meta-training you have
your final initial parameter vector theta m here.
You now pull that over for meta-testing and now,
you might want to test it on totally different data that it's never seen before, right?
So now you have meta-testing training data,
which is a bit mind bendy.
But once you get used to this language,
it really helps clarify things, right?
So the- this network here has never seen this training data.
So it's meta- testing training data. It does all that.
But then you still want to test this learner on something it's never seen.
So you can't test it on this stuff.
So now you have to go over here to meta-testing testing data.
Does that all make sense? Good. Glad you got it.
Okay, so with all that language in mind,
now I'm gonna make it a little bit more
complicated because we want to do continual learning.
It can't just be IID chunks of data.
Right? It has to be sequential data.
So now we're going to do that all of that,
but we're gonna do in tasks in a row.
So you have Task 1, Task 2, task,
you know t and then your meta-loss here because now you wanna say how
much did you remember not just the most recent thing
you heard- you trained on but all of it.
Now, you have this, you know,
your meta loss is going to be on all of the tasks,
that it saw. Does that makes sense?
And then you do all of that,
and that's, uh, that's the general framework.
Cool. So a couple, uh,
actually at ICML, uh,
Martha White gave a talk that I thought was really eye-opening.
So basically we had been working on his vision of trying to
learn to continuously learn for a really long time and it wasn't
working very well and then she basically put out
an algorithm that did exactly that and did it really well,
and it was called originally MRCL for Meta-Learn Representations for Continual Learning.
Uh, but they changed the name recently in an updated version of the paper to OML and,
uh, in our opinion this really validated
this vision that you could learn to continuously learn.
So we kind of chose to scrap what we've-
what we were doing and build on top of their algorithm.
So here's how O- OML works.
Uh, it does pretty well.
So what they do is they meta- learn a representation chunk here.
This red, uh, part of the network, uh,
over across a meta-learning outer loop utter- iterations and then in the inner loop,
they're only gonna learn this blue,
these kind of inference layers here.
So they call these the TLN and so then after meta-training,
when they go to meta-test,
they'll freeze this red block here and they'll only train in these blue layers here.
Okay? And what they show is that this performs really well.
Like, historically catastrophic forgetting, you learn one task,
you'll learn a second task, and you're hosed.
They were showing now that after 150 classes sequentially trained on Omniglot,
where each task is one class of Omniglot.
You could still do pretty well,
which was just kind of for me kinda mind-blowing.
That you could solve cash- cash out for getting across 150 tasks.
So you get on- on, uh,
meta-tests training, so the stuff you already saw.
You could remember 97% so you have near perfect memorization even on the first,
you can still remember the first class after seeing the 150th class,
which is pretty cool, of the stuff you actually saw.
Now, when you go to generalize,
so different instances of either the first-class or the 150th class,
you know, you're generalizing it's much worse.
It's like 63%, which is still way,
way better than chance and way better than anything that came before that I am aware of.
What's also really cool is that it learned on its own
the sparse representations are a very good idea, which is cool.
So, um, here is for example a method that was
explicitly trying to design sparse representations,
and you can see they're pretty sparse.
Uh, these are different instances of the- of
the training example and this is an average across the whole, uh, dataset.
But many of the neurons were dead.
It's just learned to never used most neurons.
If you just do normal deep learning,
where you get pretty highly active non-sparse representations.
But if you look at miracle,
it learned to be sparse but it also learned to use
all of its neurons in its representation, which is pretty powerful.
So neuron- OML gets a lot, right?
It sends for online- I think it's online aware meta-learning, maybe. I forget the acronym.
Anyway, it's- it gets a lot right,
but it's ultimately still subject to SGD because it's training that red
block and then it just hopes at inference time and meta test- test time,
sorry, meta test training time that SGD is
not gonna mess up the weights in that network.
So the question is, could we do better?
Could we try to allow more control to optimize the SGD too or at least modify
the SGD so that it can basically not cannibalize
its own information while it's learning at meta test time?
And so what we propose is a different type of,
kind of version of this which is based on neuromodulation.
So we're going to now- we're gonna allow neural modulation and modulate SGD.
But this is a different type of neural modulation that I was just telling you about.
Instead of directly gaining the plasticity,
turning and learning on and off in the network,
it's gonna get the activations of the network, okay?
And so if you gate the activations of the network,
we call that selective activation that also indirectly allows
you to control learning which we call selective plasticity.
So it gets a lot easier to see when you look at the architecture.
So here is a, uh, our system.
You've got this red network here,
which is your prediction network.
Sorry, I swapped the colors on you.
This is not meta-learn.
This is inner loop learned, uh,
during, like normal, like at meta test training.
And then this is meta-learned here.
This neuromodulatory network here and the output of
this layer is a multiplicative weight on this layer here.
So it gets to turn these neurons on and off depending on
whatever this network thinks about that input type.
So for some types of, uh, of neurons it might only let
the first 30% of the neurons fire etc,
etc, etc, depending on the context.
And so if you look at this, what this allows is, in normal deep learning you
have learned, the forward pass goes through the entire network.
So you have inference everywhere and you also have learning everywhere.
SGD goes back through the whole network,
so you're gonna get this process of catastrophic forgetting.
But with ANML, I don't know if I told you the name but it stands for
a neuromodulated meta-learning algorithm which is a play
on both MAML and reptile which we enjoyed.
So ANML here, a neuromodulated meta learning algorithm, it can activate,
it can gate these activations here and that means that it
can both select- it can cause selected activation.
So these are not going to fire.
So it's kind of taken the blue weights out of the network and that also
then will affect the backwards pass in SGD so you get selective plasticity.
All these purple weights will not be modified
because they weren't involved in that forward pass.
So you're giving more degrees of freedom to the network.
So here is the training domain here.
It's back to Omniglot and each of the classes from Omniglot are going to
be its own class or task and so what we do here during training,
is we're going to, you know,
what we'd normally ideally do is we would differentiate through 600,
uh, tasks in a row which is like 9,000 steps of SGD.
You differentiate to that entire thing back to
the initial weight vector and you'd be about your business but that is hard.
Felipe has not figured out how to do that 9,000 steps of SGD.
But instead, the OML paper came up with
a really cool innovation and that is this particular loss function.
So what they do is they say,
all right we're going to pick one particular class,
the current task and we're gonna start doing SGD on different instances of that class,
20 of them, for example, and then our meta loss
will be on both instances of that class that we just saw. [NOISE]
So how good can you remember what you saw and also other classes from the training set?
And what that means is that you have to learn the thing you're just asked to train on,
without messing up information from
other classes that you've learned in the past but we're only going to do that
on a little tiny sample and then we're gonna keep doing that over and over again.
That obviates the need to differentiate through 600 classes or tasks directly.
So then we'll do that for a different task and we'll repeat that,
or those are the meta, uh, meta optimizations.
Cool. And then what you can do is your inter-loop loss was all,
was those things and you can differentiate it all the way back
to the initial parameter vector.
So in MAML, they did it by choosing
that exact same trick to train the red and then they have inter-loop learning SGD,
normal SGD on the blue.
What we do is we use that meta learning to train the red network,
the neuromodulatory network which then masks
the activation of this thing which is not meta trained.
It's a normal prediction network.
Okay. And then at meta testing what we're gonna do is we're going to have a new class.
We do 20 instances of that class and then, sorry,
we do 15 of that class and then we check it
on five more examples of that particular class.
So the question is both at the end of 15 steps
here you can ask how well did it memorize all of these samples that's
kind of like how well did it fit the training set and that's meta test training accuracy
here and then you can also ask how well does it do on these other samples?
So how was it generalized to these unseen samples that it has never seen?
Clear? So you get that.
If you do it for at least one task,
you get that plot there.
If you do that for two tasks,
then you can put a plot there and there and then you can do that across however,
many classes you want to show the network and then you can
stop and ask both how well did we do at everything it
saw in meta test training and how well they
do at meta test- testing and you get these plots.
Right. So then the question is, uh, you know,
how well- what we really care about is how well does it do
on a meta-test test set. All right.
So I wanna remind you that in normal deep learning,
you have two things helping you out.
You have IID training and you have the fact
that you typically do multiple passes through the data, right?
Multiple epochs.
In sequential learning, in this particular setup,
we are going to be not have IID data.
So you have to contend with catastrophic forgetting.
And then we're only gonna do one pass through the data.
So it's also kind of like a few shot learning problem.
Uh, and so both of those forces are gonna hurt our performance.
So here's what the OML paper showed.
Um, if you look on meta-test training,
so how well did you memorize the stuff that you were just shown even after 150 classes?
You can see OML is doing really,
really well and so is ANML.
But then we extended that all the way out to 600 classes.
And now you see this massive difference between OML and ANML.
But then we actually started playing around with the OML a little bit.
And what we realized is that if you don't- if you
let it only fine tune- I'm not gonna get back to the picture.
But if you only fine tune the last of those two layers,
not just the- not both of them, then you can do much better.
That's this green thing here.
And then the gap with ANML is actually pretty small.
Now, if you look at just training from scratch,
you get this terrible performance.
This is the catastrophic forgetting problem
we've been dealing with in deep learning forever.
And if you just pre-trained on the meta-training set,
and then do transfer learning,
you still fall off a cliff.
So this is the normal catastrophic forgetting in deep learning. All right.
Now, the real big difference is when you look at meta-test testing.
So how well does the network generalize to new stuff that it has seen?
So during meta-testing, it's never gonna see anything that it saw in meta-training.
But it did see 15 of each of 600 classes.
And the question is, how well has to do with
the five things of each of those classes that it's never seen before?
And what you see is that, now,
you- there's a massive gap between ANML and OML.
Even the OML lack the better version of OML, as you go way out.
But kind of the thing with the big picture,
instead of comparing between different methods,
it's just just like, this is really cool.
Like, 600 classes in,
9,000 steps of SGD in,
and the network is not forgetting terribly.
I mean, this is not as good performance as you might hope for,
but it's still remembering a lot about what it learned about these really early classes,
which is pretty incredible.
Now, in terms of what you might actually be able to expect here, we actually said.
All right. Let's see if we can get rid of
catastrophic forgetting and see like what's the best you could hope for.
So the upper bound or Oracle for ANML is this red line here,
which you see, you just train IID.
In- instead of doing 600 classes in order,
you just train IID sampled from those 600 classes.
So you eliminate catastrophic forgetting.
But you still have one pass through the data.
So you still have a low-shot learning problem and that's this red line here.
And what you can see is that neural- the kind of ANML
is really close to that Oracle there.
So to some extent,
the problem is no longer catastrophic forgetting,
it might be just data efficiencies, sample efficient learning.
But it's really not the Achilles' heel you know,
catastrophic forgetting is, uh, what's harming us.
So that's really exciting.
The final thing I'll show in terms of data from this particular project,
is that it also like OML, it learns sparsity.
So here is the activation of that balloon network,
the, uh, for- the forward passed non-meta network.
Before it gets gated with the neuromodulatory network,
this is the output of the neuromodulatory network,
and 50% of these neurons are active for these individual example samples.
And after neuromodulatory activation,
it's down to 6%.
So it's intelligently learning to figure out- how to
figure out which small subset of the network should be on,
in order to learn without forgetting.
So any questions?
I hope I'm not horribly losing you. All right.
So to conclude this section, um,
both OML and ANML show the promise of
meta-learning for learning solutions to catastrophic forgetting.
And I would say more broadly.
Like whatever problem it is you're trying to solve,
just set it up as a meta-learning problem,
and then see if meta-learning can solve it for you instead of trying to get too clever.
ANML's advantage over OML also underscores that even within the meta-learning paradigm,
which is what everybody in this room is interested in; materials still matter.
You know, should you use neuromodulation,
should you use Hebbian Learning,
should you use a different architecture, etc.
Uh, I think there's lots of future work that could improve this even further,
including trying to replace SGD entirely.
So ANML is still using SGD where it is modulating it.
But you can imagine the version where you just have like
a giant and recurrent neural net with differentiable neuromodulation for example,
and then that thing invents its own learning algorithm that
learns to learn without forgetting.
That would be very exciting.
Okay, now we get to some,
uh, non-meta learning stuff.
The third pillar of AI-GAs is automatically generating learning environments.
So all of you know that in meta-learning,
you can't just train on one environment.
You have to train on a distribution of environments.
So the question is, where will you get them?
You know, how are you gonna provide the fuel to your meta-learning algorithm?
And if you'd pick some,
are they gonna be the right ones to catalyze learning?
And I don't mean learning on your particular problem,
although that's interesting too.
But what about learning to get really far,
to get sort of like really ambitious AI goals? How are we gonna do that?
We certainly wouldn't want humans picking
all of those problems because we're probably not good at it.
So in machine learning, we tend to do that.
That we pick the challenges and then we go and try to solve them.
But the question that I think is really interesting to ask is,
can algorithms invent their own problems,
while they're trying to learn how to solve those problems?
And so one thing that I find really,
really inspirational are these things called open-ended algorithms.
Now, this is an algorithm that you turn it on,
and you just let it run forever.
And it continuously innovates forever.
It just like runs and runs and runs.
You know, we do not have any algorithms
right now that you would want to let run, for like,
I don't know, more than a couple of years, a couple of months.
But I'm talking about an algorithm that could run for a billion years.
So can anybody in this room make an algorithm that you'd
wanna run for a billion years and then come back and check on it,
and it would still be doing interesting stuff?
So we have seen an example of that, and that's on Earth.
The very simple algorithm of
Darwinian evolution plus a planet-sized computer, plus a planet.
A billion years later,
is constantly innovating and doing really, really fascinating things.
So can we make algorithms do this?
I think that is a really fantastic and interesting challenge.
And no, you do not have to be committed to using
an evolutionary algorithm to do that. That's not at all the point.
The point is it can be dealt with whatever your favorite optimization algorithm is,
we should be able to make algorithms that endlessly innovate.
Now, evolution on Earth,
and it's not the only algorithm that does this,
human culture also does this.
We constantly are innovating and making new,
new types of novels,
and TV shows, and science,
and art, and dance, etc.
So what's- one thing that's kind of underlying all of
these open-ended algorithms is that they invent their own challenges and solve them,
and then the solution to those challenges end up being new problems that get solved.
So for example, natural selection invented the problem of leaves high up in trees,
and then the solution to that problem,
in the form of giraffes and caterpillars, right?
Which are two different ways to go eat those leaves.
And then giraffes in turn are their own problem,
that lions can prey upon and hyenas, etc.
So in this work, we have this, uh,
new paper called POET,
which is a Paired Open-Ended Trailblazer.
And I wanna highlight Rui Wang,
who is- who has done all the heavy lifting on this project.
He's done a wonderful job. And the idea behind
this approach is that we want to try to endlessly generate
increasingly complex and diverse learning environments and
their solutions altogether online i- in one big algorithm.
So here's how it works.
We're gonna periodically generate environments.
So to do that, that means we have to parameterize environments.
You have to have a way to search for environments.
So you may have a parameter vector that specifies a particular environment.
And, you know, you can change those parameters and you get different environments.
And then what we're gonna do is we're gonna add
newly generated environments to the population of environments,
if they're not too easy for the current set of agents and they're not too hard.
We're also gonna have a tiebreaker if they're novelty to
kind of like incentivize these environments to go out and be different.
And then that this is a population of environments.
And then each one of those environments has an agent that's paired in it.
And then once we generate those new environments,
we're gonna start optimizing the agents to solve those environments.
So that's our paired- that's why the algorithm has paired.
Oops, yeah.
So there's a paradox in life which is really I think interesting,
which is that, if you try too hard to solve a problem you'll fail.
For example, if I put you in this maze here and you're
this robot and you try to go to the, this is the start,
and try to go to the goal and your reward function says
only reduce your distance to the goal,
then what you'll do is you'll go up here and you'll
bang your head against this wall forever.
And that's what this optimization algorithm does is tries to reduce distance to the goal.
If you just try to seek out and go to new places in the maze or seek for novelty,
you'll trivially solve this problem which is what this,
uh, maze does over here.
Now, this is a more general lesson in life.
For example, it applies in all of science and technology.
If you go back a few hundred years to this technology,
you say, I want to heat up food more rapidly,
you will never invent a microwave if you only fund
researchers that put- that can heat food faster, um,
because to invent a microwave you have to have been working on
microwave technology to produce
that innovation and notice that a chocolate bar melted in your pocket,
which is how the microwave was actually invented.
And if you go back here a couple of millennia to abacuses,
and you say, oh that things cool, it does computation.
I want more computation and you only fund people who will make you better,
uh, uh, you know, the things that produce more compute,
then you will never invent the modern-day computer because to
produce this you have to had been goin- working on
lightning and vacuum tubes which had
no immediate application for computation whatsoever.
So the ideal- the idea is that we call this goal switching.
Um, let's skip that example.
The uh- the uh,
idea is that if you're optimizing for one thing.
This row-this scientist here wants to optimize for a robot that's
walking but suddenly one of your agents starts crawling will you- really well,
don't throw that out as a failure.
Instead, capture that and start optimizing for that too
because that might be an ultimately really good stepping stone to walking,
same with balancing on one foot.
Now, I do wanna mention one thing about POET,
I meant to say this at the top.
Uh, POET so far is not a meta-learning system,
but it could be used for meta-learning.
So let's say you had to generate environments and then you- that
can be fuel for your metal-learning algorithms.
So this idea of goal switching,
so you start optimizing your agents on one goal,
if they suddenly like start doing well in some other goal,
you catch chance on the wing and you start optimizing for that too,
that's called goal switching and we've shown over and over again in a lot of
different papers that this can really pay
off because it gets your agents off of local optima.
So we- in this paper that we had on the cover of Nature
produced state of the art adaptation in robots.
In this Go-Explore paper we use that
to help solve those two challenges that I talked about.
And so the way that goal switching works in POET
is that you have this population of different environments and
periodically you'll transfer an agent from
one environment to another environment to see if it's doing better.
Okay, so here's the particular domain in the POET,
it's just an obstacle course and its agents running through it.
Tries to run through it really fast without falling
and here the degrees of freedom in the environment.
So the environment can make envi- environments that have different degrees of
say stump height or roughness in the terrain. And here's how it works.
So we start with the parameter vector that specifies an environment phi
one and then we'll have a parameter vector that
specifies a theta that's paired with that environment.
We'll optimize theta to perform well on phi.
And then what we'll do is after Theta is pretty good at this,
we'll copy the environment to make
another environment phi two which is a little bit different.
And then we'll do transfer learning.
So we'll goal switch this agent into this environment and start optimizing it here.
Now optimization is in parallel both of
these things are kind of learning to solve this problem together,
then we might copy this environment that might be too hard,
so we throw it out.
We generate a new environment and we can test both of these in this environment,
see which one's better,
produce a new agent optimize it there.
This does not have to be a linear chain,
we can kind of produce this environment,
maybe the best parent for it is this,
and we go on and on.
Now imagine that- imagine that eventually,
we produce this really really hard challenge here,
and what you do is initially it looks like this is
the best stepping stone to see this population,
so you start optimizing this in this and it does okay but it's stuck on a local optima.
But we're doing goal switching in this algorithm,
so this thing might ultimately replace that thing,
and now with a little bit more optimization on this problem might
ultimately produce the best solution to that problem
there and ultimately we end up with this solution to
this problem which got us off of these local optima.
So in this particular case we have a three-layer neural network,
it's being optimized with evolution strategies but it could be any RL algorithm,
and here's some fun videos of it working.
So you have this little tiny robot here,
it's moving through this terrain,
the environments start off really simple,
and then over time they-the are-the algorithm
starts making harder and harder versions of these problems.
First, it's kind of giving it individual challenges like
only little stumps and only little gaps and only some more on rough terrain.
Over time it starts producing like bigger gaps and it's starting to
combine things like medium-sized gaps and medium-sized stumps and
eventually it kind of produces these really hard environments that
have like gaps about as big as they can do and it's using
its lateral sensors so it's kind of like timing its jumps and
it's really kind of pushing the limits of what this body is capable of.
Here's another challenge that was invented by
POET all by itself which we didn't even think was possible.
So one interesting thing about POET is we found is if you take
the most challenging environment it
produces and you try to directly optimize solutions in them,
it doesn't work because there's no curricula
whereas POET is automatically implementing its own curricula.
So what we did is we took the hardest environments here and we said we'll try to be
more fair and we'll create a direct path curriculum which is a pretty common technique.
So we take this parameter vector for this environment,
this one here and then we linearly interpolate along
that parameter vector and we start optimizing down it,
and it always fails every single time because curricula are hard,
designing them is hard and you- you will usually
get it wrong and the harder the environment the more that it failed.
So I wanted to show you this one nice example.
Here is our agent.
It's on the most simple version of the problem here and it's dragging its knee.
And what you see is that ultimately
the- the algorithm creates a slightly harder version of the problem,
this knee dragging behavior no longer works.
It ships the robot up and so the robot stands up and gets a better score,
and then we are further up and then basically eventually this thing
was better even when transferred back to this simple environment.
So the algorithm automatically goal switches
this thing back to the original where now it's standing up and it's
performing better and with further optimization it gets a much better score of 349.
Now we did the counterfactual.
We went and ran this agent in that environment for a long time and it never stands up,
it's always dragging its knee.
And so I think this just exemplifies why
curricular learning is hard because here you had to go and work on
a harder problem to get better at
this simple problem and get yourself off of this local optima.
So without- if you don't do goal switching, you don't do transfer,
you never solve these really hard problems and you never generate them.
So there's been a lot of work on learning curricula,
I don't have time to get into it all today but I just want to quickly kind
of fuel your intuitions with future work.
We'd like to do this in harder domains
and you know you could do it with multiple agents as well.
But I think it's really thought-provoking to put it in a world like this.
So if you had enough compute to run POET in a world like
this where there are other agents that you can barter with,
buy goods from, you fight with,
cooperate with, you can climb buildings deal with aerial predators, etc.
You can really imagine meta-learning some very sophisticated behaviors in such a domain.
This is also very related to Open AI.
So currently like I mentioned POET has not yet been learned with meta-learning.
Each environment is a little deterministic environment.
You don't have to learn to solve it but you could easily have
all of the tasks produced by POET as your distribution
that you train on or you could have each environment in POET be
its own little distribution that you're meta-learning
on and you still have goal switching amongst the learners.
And this is all very related, Open AI's automatic domain randomization,
if you saw that paper which worked really well and solves Rubik's cube.
So, uh, I don't have time to go through all of
those things but I think POET is really exciting because it deals with,
it invents its own curricula,
it hedges its bets,
it endlessly innovates, it provides possibility
for meta-learning and opens up a lot of really interesting research directions,
but mostly it's inventing its own challenges which is what we need for meta-learning.
So if you're interested in more on these- this subject related to POET and openendedness,
you can watch our ICML tutorial on
population-based methods which is at that link there.
So with that, I'll get to my last slide.
Thank you for your patience.
Um, and that is that I think that it's really
fair in- and interesting to this question whether or not the
dominant paradigm in machine learning or the manual approach to AI is really going to
get us ultimately to where we want to
go and I think that's just kind of interesting to discuss.
And I proposed to you the idea of
an alternative paradigm which is the AI generating algorithms paradigm,
where you really go all-in on meta-learning.
So you're gonna meta learn the architecture,
the learning algorithms and you're going to learn
the environments and generate them automatically.
And you can imagine those three things coming together to produce
one really interesting algorithm that potentially could innovate
forever and get us all the way to extremely powerful forms of
AI and perhaps- perhaps maybe human-level AI.
And then I also introduced you to for each one of these pillars here
some pretty exotic new approaches that we've
been working on including generative teaching networks,
Differentiable Hebbian Plasticity, Differentiable Neuromodulated Hebbian Plasticity,
ANML and then POET,
which can automatically generate environments.
So I hope that you found that interesting and I wanna say thank you for
your patience and for the invitation.
[APPLAUSE].
I also wanna thank my collaborators and there's at least a minute if anyone has
any high-level questions or low-level questions. Yes.
Um, what's the most pervasive argument that you come across [NOISE] [inaudible]
Yeah. Um, so I have a lot of them in
the paper that's on Arxiv and so you can read a more detailed thing there.
But I think that the- basically,
my view is that, uh, I'll give you two answers.
One, at a high-level,
engineering is wonderful; it's done really impressive things.
You know, we got to the moon, we got to Mars,
we built the International Space Station,
we've built Boeing, you know,
giant planes, very complicated machines,
and so you could say that will keep- that will continue,
and we'll do- we'll just build something really big.
But I think it's fair to question whether or not that will
work when the thing we're building, uh,
requires, you know, is- is- maybe is- you know,
maybe we're not smart enough to do that.
You know, when it's requiring something that's more
sophisticated than something we can understand.
So I think it's fair to question that.
The second thing I would say is that, um,
I think that the
manual path is making tremendous progress, and it's, you know,
like all of those stuff we've seen in the last couple years in AI
has been from the manual path from in most cases,
and so it's really doing well and so the question is- I think
eventually the AIJ path will kind of be under it for a while,
and under-perform it, and then it will cross over and surpass it,
but I don't know when that crossover point is.
The crossover point itself might come after we produce human level AI,
and so that's an argument that maybe both paths will get there but maybe
the manual path will get there faster. Yeah.
Can you comment, um,
sort of a general question.
Can you- can you comment on the relationship between the work that your group
does and Uber self-driving,
and- and here what I'm thinking of is the- you know,
these thousands of edge cases,
the long tail, and, uh,
is there- can you- can you elaborate on the relationship between this work?
Yeah. So, er, at the highest level,
I'm in AI labs which was formed after our startup Geometric Intelligence got acquired,
and it was definitely set up to be
a basic research lab that was focused on all areas of machine learning,
not just self-driving and then in the Uber,
we have an entirely separate organization that focuses on self-driving.
So just from an organizational perspective,
we have totally different mandates,
which is why you see here a lot of more basic machine learning fo-, uh,
research that's not just focused on that particular research application.
But that said, we find those challenges interesting and we work on them too.
Uh, and I can't comment too much about
internal applications of these particular ideas to that,
but I can just give you a flavor of how it might be useful.
You could imagine for example, in POET,
that you're automatically inventing as you mentioned,
long-tail edge cases that are very very difficult and are, you know,
hard for the current policy to solve,
and then they- they invent the solution to those problems.
So that's useful for a variety of ways.
One, is you're collecting hard- hard- you know
really hard corner cases that you can add to your set to train on,
and two, you have the solution to those edge cases.
So if your policy can- can consume, you know,
imitation learning or learning via observation or demonstration,
then it can use those individual solutions to then distill back in to the main policy.
So that's just kind of like one way that these algorithms might be useful.
But more broadly, like the idea of open-ended algorithms and
getting off of local optima, better search techniques,
improved memory usage, more efficient learning,
solving continual learning, which is definitely an issue with self-driving,
and many business applications.
There is a wide variety to apply it,
there is a wide opportunity to apply
these sorts of ideas within any company that has a lot of,
uh, challenging machine learning problems which Uber definitely has.
Good answer.
Thank you. Yes.
This is really interesting to see how these algorithms work in
these kinds of simulated environment and stuff and I was thinking, working in robotics,
I was thinking about how I can apply that as to what I do in robotics, and it's like,
think about it, there's a lot of, um,
problems in robotics that don't seem to be directly addressed by this.
So I was wondering if you could comment on how we take these to like
a physical world where you can't afford to fail.
We- we're still optimizing the hardware,
we even don't know what our feedback is yet.
So what can we learn from these kind of meta techniques in a more physical environment?
Yeah. So one thing that I would say is,
uh, I recommend this paper here.
This paper here, what I love about it,
is that it kind of marries
the very expensive power of
stochastic optimization and deep reinforcement learning or evolutionary algorithms,
any of these stochastic optimizers,
which are really expensive and really
sample inefficient but they can be done in simulation ahead of time.
So it takes like stuff that you learn in simulation,
and then it ports it over in
the real world to using that information that you've learned in simulation,
really really efficiently with Bayesian optimization.
So I don't have time to tell you the full method but the idea is that you can
use a lot of the algorithms that I've
talked about to kind of explore the set of possibilities,
learn a variety of different strategies that are all really high-performing,
like a diverse set of high-performing strategies,
and then in the real world when you only have
a limited amount of experiments you can do or time to learn,
you can just switch between a whole bunch of really good solutions.
That's one flavor of answer.
A second flavor of answer is the Open AI robotics cube strategy,
which is that you learn- you use things like meta learning and POET, to,
uh, train in simulation,
and then immediately deploy in the real world,
and because the OpenAI robotics cube what it did,
was it combined two of the three pillars from AI-GAs.
So it automatically generates an increasingly large,
diverse set of challenging environments to train on,
and then it uses the RL squared slash learning to reinforcement learn
recurrent neural network that invents its own learning algorithm and in simulation,
it kept on waking up in a simulated world that had a different like- you know,
like a different amount of gravity,
a different amount of friction, different amount of colors on the walls.
So it's always in a slightly different world and it has to
rapidly figure out like which world am I in?
What's the friction? What's the gravity?
Quickly solve the problem,
and it does that over and over and over again in simulation at great compute cost.
But then when you drop it in the real world,
the real world is just another new challenge,
and it's like, "Oh, what's the friction here?
And what's the weird lighting effect here?
And what- like how do I flip the cube?"
And then it can kind of rapidly- and be
a sample efficient learner that solves the Rubik's cube,
and that's what they showed that it did.
So you can harness- compute ahead of time to
learn- to learn efficiently and then just deploy that in the world.
So I think those are two different techniques,
both of which are key- can be used to great effect in robotics
and Chelsea's done a lot of great work showing that that works,
and so has Sergey, and several other people in the field and now Open AI.
And so I don't think that it's the case that robotics
has to wait for AI to get better and more sample efficient.
I think that now you can already start taking advantage of
these techniques and Open AI I think is the best showcase of that.
There was one more back there. I don't know when you have to call it though.
Just one more.
Okay, we'll take the one more question that was back there.
Ah, just a quick thought.
Can you guys consider applying
POET to the Minecraft RL challenge or something. [inaudible]
Yeah. We have talked about that.
I think that's a fascinating direction.
I'm not familiar with that particular challenge right now
but just the idea of like POET plus Minecraft,
I think is, uh, really great because it's
such an open-ended world it would be really fascinating to see what it does. Yeah.
Great question.
Thank you all again. I appreciate it.
 All right. Let's get started.
Uh, it's my pleasure to introduce Sergey Levine who will be giving a guest lecture today.
Uh, Sergey is an assistant professor at UC Berkeley.
He did his bachelor's and his PhD here at Stanford University,
uh, and his research is at the intersection between control, uh,
and machine learning with the aim of developing algorithms and techniques that can endow
machines with the ability to autonomously acquire skills for executing complex tasks.
Uh, and today, he'll be talking about
information theoretic exploration and unsupervised reinforcement learning.
Please join me in welcoming Sergey.
[APPLAUSE]
Thank you for the introduction.
So, um, yeah, I thought a little bit about what kind of topic to discuss today.
And I figured I would actually talk about something that's,
uh, you know, addressing some challenges in reinforcement learning,
but I think it ties also pretty deeply to topics in
multitask learning and meta-learning that are sort of the core focus of this course.
So let's start off with a little discussion of,
uh, what's wrong with reinforcement learning algorithms.
So you guys had a few lectures on RL.
Hopefully, you kind of, uh, have a decent idea of what the basic, uh, setup looks like.
But, uh, you know, the basic setup has some issues.
So here's an example of a task where you have, uh,
a little bipedal robot learning to run.
And you might look at this and say, "Well,
that's- that's pretty cool like this little guy learned to run essentially from scratch."
Uh, but what did it take for him to get there?
Well, that learning curve on the bottom shows, uh, the progression,
the- the reward function as a function of the number of time steps that,
uh, he had to actually attempt running.
And if we put a scale on this in terms of sort of, uh,
hours of time that it would take in
the real world if it was like an actual robot trying to run,
this little bit where he just starts making progress alone already takes 10 hours,
and then if you want to get to the end of this curve over here,
this would be like on the order of 100 hours.
So pretty cool if something is working,
but seems like doing this from scratch takes a really, really long time.
And if you run this algorithm twice,
you get two pretty different solutions.
They both look like reasonable, uh, running gaits,
but something here seems to be a little bit off if you
get totally different solutions each time you do this.
And then you might also think, "Well,
how did we define the task, uh, for this little guy?"
In, uh, you know when- when we learn about reinforcement learning, classically we're told that we
specify objectives or reward functions that tell the system what it's supposed to do,
and the reinforcement learning algorithm tell- it's supposed to figure out how to do it.
So you might think, "Well, if you want it to run,
then clearly, you just give it a reward function."
It says, "Like you get a 1, if you run, and a 0 otherwise,
go for it and then you can figure this out."
But of course, that's not how it works in practice.
In practice, if you actually use
a reward function like this you're not gonna get a very good solution.
And what people do instead is they defined
some complicated monster of a reward function like this that says,
"Well, have a large forward velocity.
Keep the torso upright.
Don't fall too much," all this other stuff.
So essentially encode a little bit more guidance about how the tasks should be performed.
So long training times, different outcomes
when you run at different times, complex reward functions.
So there's a lot of these challenges and
some of these challenges are local optima challenges.
Some of them, uh,
are challenges with needing to craft very detailed reward functions,
and some of them are challenges associated with long training times.
And there are a variety of different, uh,
ways we can start thinking about solving these challenges and I'm going to
discuss one, perhaps, somewhat non-obvious,
uh, direction, which is to think about, you know,
how is it that- that humans and animals avoid having to deal with some of these issues.
Well, maybe part of the key to that is that,
uh, we're not actually, uh, you know,
this very much goes to the core of this class,
we're not re- really learning all these skills from scratch.
So in standard RL settings,
you have very detailed goals,
you're given that goal and you just try to achieve that goal.
Uh, if you imagine, you know, uh,
the child interacting with its environment maybe,
it doesn't have very specific goals,
but it's interacting with the world and acquiring knowledge about
the world through largely unstructured interaction.
And now, we might start thinking, "Well,
can this kind of unstructured interaction help us,
uh, address these issues?
Perhaps the issues associated with local optima can be mitigated by
obtaining very good coverage over the space of
possible behaviors so that you know what's possible in the world."
And then maybe once you have the more global view of the possibilities,
you can avoid the local optimum when you're provided with a specific task to accomplish.
Perhaps, uh, the need for detailed rewards can also be
alleviated by having some kind of varied diversity seeking exploration where you, uh,
see what all the possible things that you can do in the world are,
and then maybe you can deal with objectives that are as simple as this,
because you've already seen things that look a little bit
like running and a little bit like not running.
And also maybe the long training times can be alleviated by essentially utilizing
unstructured experience like this as
unsupervised pre-training to build up representations,
build up some initial skills that can later be utilized to address user-specified goals.
So the particular, uh,
problem that I'm gonna talk about today is,
uh, the problem of recovering diverse behavior without access to any reward function.
And of course, I'll- I'll allude a little bit to how this can be used to
then achieve specific goals later down the line.
But I'll spe- I'll mostly focus on the unsupervised parts or the pre-training part.
So this part essentially. Why do we want this?
Well, uh, you might want it because you can learn
skills without supervision then use them to accomplish goals later on.
You can learn subskills and then use
those subskills with hierarchical reinforcement learning methods,
and you can explore the space of
possible behaviors to help you later on get out of those local optima.
So, uh, if you want
kind of a more concrete motivating example scenario, let's imagine that,
uh, you buy, uh, your, you know,
mark 1 household robot,
and this is your kitchen.
And you buy the robot in the morning,
you put it in the kitchen, and then you say,
"You know, do whatever it is that you need to do
to prepare yourself for an unknown future goal," and then
you're gonna go to work and the robot is going to go at it in
your kitchen trying to figure out what is it that it can do.
And then you're gonna come and they- come back in the evening and you gonna say,
"Well, tonight your job is going to be to do the dishes for me."
And the robot should have done whatever was necessary during
this unsupervised pre-training phase to prepare it as
well as it possibly co- could for
whatever unknown tasks you might give it down the line like doing the dishes.
That's sort of an example scenario that we can keep in our minds as
I discuss some of the techniques that we can think about for this.
And you could, uh, you know, if you're willing to, uh,
sort of stretch the analogy a little bit,
you could also imagine that the, uh,
the child that was playing around with toys on
the previous slide was unintentionally doing some variant of this,
essentially exploring the world with
the unintentional aim of preparing itself for whatever its future might demand of it.
Okay. Um, so that's maybe the high-level motivation,
but the concrete things that I'd like to discuss in today's lecture,
I'm going to start with some definitions and
concepts from information theory that we're going to utilize,
uh, repeatedly in the lecture to actually think of algorithms that can do this.
I'm going to discuss a distribution matching formulation of reinforcement learning
that deviates a little bit from the standard way
of formulating the reinforcement learning problem,
and provides us a way to connect it, uh,
with some of these probabilistic concepts.
Then I'll talk about our first, uh,
fully unsupervised or reinforce learning algorithm.
We're just going to learn without a reward function to reach diverse goals.
And then, uh, I'll- I'll discuss a way that we can
extend this idea to match desired state distributions.
Talk about whether coverage of state distributions is
actually a good exploration objective and some theoretical sense,
and then generalize the idea further from covering
the space of states to covering the space of skills.
And then I'll conclude by, uh,
briefly tying these ideas back to meta-learning,
uh with a- with a short discussion about how- some of
these unsupervised skill extraction techniques can actually be utilized together with,
uh, with meta-learning to perform essentially unsupervised meta-learning,
you know, uh, and that'll be at the end of the lecture.
I- I'm not sure if I'll get through all of this
but it sort of depends on how many questions you guys ask.
So feel free to ask questions.
Uh, if- if we're running out of time,
this is sort of intentionally designed so that the most important stuff comes first, and,
uh, sort of the fuzzier stuff comes later. All right.
Let's start with, uh, some definitions and concepts.
And many of you might already know a- a lot of these
but I do want to go through them to make sure we're all on the same page.
So first some useful identities.
Hopefully, all of you know what a distribution is.
Uh, but we need to know what a distribution
is because we're going to work a lot with distributions.
So if this is your variable x,
you can think of some getting some samples represented by those blue dots.
Using those samples, you can fit a distribution and just tells you how
dense are your points in, uh, different regions.
Now, a particular quantity, uh,
that we can compute on these distributions,
which is going to be very useful for us in this lecture,
is the entropy of the distribution.
So I'm going to use this script H to denote entropy.
Uh, entropy can be defined, uh,
like this and the- this equation might be a little bit cryptic.
But to give you a little bit of intuition about what's going on here,
it's the negative of the expectation of the log probabilities.
So intuitively, if your distribution is very large log probability somewhere,
then this expectation will be large,
which means that its negative will be,
uh, a very negative number.
If the distribution has very small log probabilities,
then this expectation will also be small,
so it'll be, uh,
a less negative number.
So larger entropies correspond to smaller log probabilities,
uh, which means that the entropy quantifies in a sense how broad the distribution is.
So if this distribution is very wide,
it has large entropy, meaning,
it has small log probabilities.
And if it's very peaked,
then it has a small entropy meaning large log probabilities.
And this will be very important, uh, in today's lecture.
Ah, the third very useful identity is mutual information.
Now again, mutual information can be defined as a somewhat cryptic way,
but then we can discuss the- sort of the intuition behind it,
and that intuition will be very useful for us.
So mutual information is a measure of how, ah,
intuitively how dependent two random variables are and its definitions,
the KL divergence between the joint distribution of those variables,
p of x, y, and the product of their marginals,
p of x times p of y,
which we can equivalently write like this using the definition of KL divergence.
But to provide a little bit of intuition for this,
let's say that you have two different distributions over x and y,
this top one here and this bottom one.
So the green dots that I've plotted are,
ah, samples from that distribution.
So now from looking at this definition, um, can you, ah,
raise your hand if you think that the top plot
shows a distribution with higher mutual information between x and y.
Now raise your hand if you think that the lower one has higher mutual information.
Okay. Right. So since mutual information is a measure of
the dep- of the degree to which the variables are dependent on each other,
ah, high mutual information means that x and y are highly dependent,
low mutual information means that they are largely independent.
And very conveniently, we can rewrite, ah,
mutual information as the difference of two entropies.
Ah, the entropy of the marginal of one of the variables,
so let's say the entropy of p of y,
minus the conditional entropy of that variable conditional on the other one.
And this has I think a very appealing intuitive interpretation.
The second term is very small.
It's a very small entropy.
If when you know x,
you can accurately guess y.
So in this top plot, you know,
y is roughly equal to x,
so if you know x, you can guess y pretty well.
In the bottom plot,
y is independent of x,
so knowing x doesn't help you guess y any better.
But it's not enough for x- for you to be able to guess y from knowing x,
you have to actually- it has to actually be difficult to guess y.
So if y is always equal to 0.5,
of course you can guess y very accurately if you know x,
but you can also guess it very accurately if you don't know x.
So this first term basically quantifies how hard it is to guess y a priori.
If it's hard to guess y a priori,
but much easier to guess y if you know x,
that means that x tells you a lot about
y and that means that x and y have high mutual information.
And the reason I'm somewhat belaboring
this definition is because this intuition will be very important
later on to understand what some of
these unsupervised reinforcing learning algorithms are doing.
So as a little warm-up,
let's go through, um, ah,
a sho- a short, ah,
kind of example of mutual information and reinforcement learning.
Let's say that we have a distribution.
Our distribution is going to be defined in terms of
quantities that we actually care about in RL,
it's going to be defined in terms of policies and states.
So we'll say that pi of s is the state marginal distribution of our policy pi.
So you run your policy a whole bunch,
count up which states it visits,
average together those counts and you get a distribution over states.
The entropy of pi of s quantifies in a sense the coverage of your policy.
So if your policy has a very large entropy of pi of s,
that means intuitively that it goes to a wide range of different states.
It's doing a good job of exploring.
Now let's think of an example of, ah,
how we can use mutual information to understand something about, ah, MDPs.
Here's an example of a mutual information quantity called empowerment.
I'm not actually gonna go through empowerment in detail,
I just wanna use it as a short example.
So, ah, classically, empowerment is defined as
the mutual information between the next state,
s_t plus 1, and the current action a_t.
On the surface, this might at first seem like a slightly cryptic concept,
but let's go back to those difference of entropies definition from the previous slide.
We know that we can write this mutual information as the entropy
of s_t plus 1 minus the entropy of s_t plus 1 given a_t.
So that means that if you know what action you took,
you can guess your next state very accurately;
if you don't know what action you took,
you cannot guess your next state very accurate.
What that means is that you go to a wide range of
different states and your actions are very predictive of those states.
So now let's think about that a little bit.
Why would we call something like this empowerment?
It sounds like a very lofty term.
Any guesses about why this is called empowerment?
Yes.
Measures to what degree you can cover your environment.
Exactly, yeah. It's- it measures the amount
of control authority you have over your world.
If your actions are highly predictive of your states,
that means your actions are influencing your states,
ah, in a fairly predictable way.
So that means that you have a lot of control about what goes on.
If- if these two entropies are equal,
that means that your actions have no effect on the state.
That means that you're sort of enfeebled,
you're not able to affect the world around you.
So that's why it's called, ah, empowerment.
[NOISE] So anyway, that's just a little exercise to help
us think about mutual information in the context of control problems.
Let's talk about, ah, the first, ah,
kind of algorithmic components in today's lecture,
which is how we can reformulate
classic reinforcement learning as a distribution matching problem.
In this case, there'll be a distribution matching problem on actions.
So I'll first motivate this a little bit with
a- with an actual challenge that we sometimes face in RL.
So this is the challenge that I showed on that first slide.
You take your favorite RL algorithm,
this particular one is TRPO,
you run it twice and you get two different answers,
and you think, what the hell just happened?
Um, well, you can, ah,
set up a little toy problem that captures sort of a microcosm of this issue,
ah, because it's a little hard to reason about these complicated humanoids.
So let's set up something much simpler.
Let's say we have four legs instead of two because that's clearly simpler,
and we're navigating a little maze.
And our goal is to reach this blue square.
Now when you're first starting out,
you can explore the upper passage or the lower passage,
and both at first seem like you're making progress towards the goal.
So if you make it a little further along the upper passage and you think, oh,
I'm getting closer, I'm getting closer,
let's just go there, [NOISE] but then you get stuck.
So intuitively, the right strategy for a problem like this ought to be to explore
both passages in roughly equal proportion
until you figure out that one of them is decisively better than the other.
And I would actually posit that the issue here is very much like
the issue here that reinforcement learning, it- it's self-reinforcing.
Once you see that you're making a little bit more progress along the upper passage,
classic reinforcing learning algorithms,
we'll want to commit all the probability mass
there because they're getting larger rewards there in
the same way that this little running guy
as soon as he figures out one type of running gait,
he'll want to go more to that place and ignore all other possible gaits
because that one running gait where he ran and made
a little more progress is the one that seems to be more rewarding.
And that's a local optima problem.
So how can we track both hypotheses or in general all hypotheses that are valuable to us?
So we're going to talk a lot about Q-functions.
Many of you might already know what-Q functions are, but a short primer.
Ah, in our reality of actions,
you have states and you have rewards.
Your goal is to maximize your expected reward on your policy.
So hopefully, we're all good with that.
And the Q function is defined as the expected value of
the rewards that you will accumulate if you start in state s,
take the action a, and then afterwards follow your policy pi.
And reward functions are very useful because if you ha- sorry, ah,
Q -functions are very useful because if you have a Q-function for a policy,
you can extract a better policy simply by being greedy with respect to your Q-function.
So if you take an action with probability 1,
if that action is the argmax of your Q-function,
you know that this policy will be at least as good
as the one that you use to train your Q-function.
And of course from this, you can get, ah,
a reinforcement learning algorithm, ah, the one that I outlined
here is policy iteration.
You can also directly enforce optimality on
the Q-function by forcing your- your Q-function to obey this equation.
So if you can enforce this equation everywhere,
then you'll just recover the optimal Q function directly. So that's Q learning.
Okay. So, um, yeah,
this is basically the- the only equation you need to know for the Q learning algorithm.
So we can do something like Q learning,
ah, for our little example with the two passengers.
And let's say that initially, ah,
we make a little bit more progress along this upper passage.
If we make a little bit more progress along the upper passage,
then the height of this,
ah, Q-function will be a little larger.
It'll- it'll take on slightly larger values for the- for going left here.
And that's a problem because if the way
that we're going to improve our policies is by taking the argmax,
then our policy will then put all this probability mass on this better option.
This option is not actually better,
it just looks better because we got lucky.
But as soon as we get lucky once,
we'll go there even more,
which means that we'll get even luckier until eventually we're completely
committing ourselves to exploring
this upper passage without even having seen what's going on down here.
So that's no good. What we can do
instead is we could imagine committing our probability mass in
proportion to some positive transformation of our Q-function
so that if the Q values for two options look about equally good,
we'll explore them about equally often.
And it turns out that
a very convenient positive transformation to use is the exponential.
We'll see why in a second.
Ah, but intuitively, if we choose our policy to not be
the greedy policy but to commit probability mass in
proportion to the exponential of the Q values,
then two options that have roughly equal Q values will
be explored to about equal degrees and we
won't necessarily have this problem of immediately
committing to the first option that accidentally looks a little bit better.
So why is it so convenient to use an exponential?
Well, because it allows us to bring in some of those concepts from information theory.
It turns out that if you write down the KL divergence between
your policy Pi and the exponential of the Q function,
so you can treat this as a probability distribution if you normalize it,
then by the definition of KL divergence,
this is equal to the expectation under Pi of Q minus log Pi.
And this is now starting to look an awful lot like regular reinforcement learning.
This first term is basically your expected reward,
because remember that Q is the expectation of future rewards.
So that means that this first term turns into
the expectation under Pi of your future rewards.
The second term, that's the new guy.
That's something that we didn't have in regular RL,
but the expectation under Pi of log Pi is just the entropy.
So all we've done,
if we change our objective to this KL divergence,
is we've taken our regular RL objective and added the entropy of Pi to it.
We call this the maximum entropy objective
because what it tells us to do is to maximize our,
our rewards while at the same time being as random as possible.
And this actually allows us to address this problem with the two passages, why?
Well, because if you're maximizing your rewards while being as random as possible,
that means that when you have two roughly equally good options,
you should take both options in equal proportion because it
will give you higher entropy than committing to one of the two.
Once you're sure that one of the two options is much,
much better, then it's fine to commit all your probability mass to that.
But if they look about equally good,
you should try both, and that will allow you to maximize your entropy.
So it's a distribution matching objective in the sense
that Pi is matching this exponential Q distribution,
and it allows us to begin addressing this local optima issue.
Now building on this idea, uh,
we can do, uh, lots of fancy math.
So for those of you that are, uh,
that like concepts like variational inference,
you can actually show that this kind of
objective emerges from a particular kind of inference problem.
So in optimal control or reinforcement learning,
we asked the question,
which actions lead to, to an optimal future?
Which action maximizes your reward?
That's a very reasonable question to ask.
In inference, you can imagine asking a slightly different question,
but ending u- ending up with a very similar answer,
which is, given that your future is optimal,
which action would you have taken?
It turns out that the answer to that second question
is basically exactly that maximum entropy objective.
So if we're going to talk about inference,
of course, we need to ask inference in what model.
So we're going to build up a graphical model,
a probabilistic graphical model,
that we can use for maximum entropy reinforcement learning.
In this graphical model, we're going to have states and actions,
that's- that should be familiar to all of you,
that's basically how a sequential decision-making process works.
But we also need to somehow introduce our rewards.
And the way that we will introduce our rewards at first looks a little peculiar,
but it actually is going to make a lot of sense.
So we're going to introduce these additional auxiliary variables
that I'm going to call O for optimality.
These variables are not equal to rewards,
they're actually binary variables,
and our evidence is that those variables are all true.
That is to say our evidence is that you are being optimal,
given that you are being optimal,
what action are you taking?
And we're going to define the probability of
those variables being true as the exponential of the reward.
In a slight regularity condition for this to make sense,
which is that your reward needs to always be negative because you can't have,
you know, probabilities that are larger than 1.
But except for that regularity condition,
this is a well-defined probabilistic graphical model.
So now the question that you want to answer is,
what's the probability of an action given
your current state and given that you are always being optimal?
So given that you are rational,
how likely is a certain action?
And if we set up this inference problem, uh,
we can actually do basically the usual thing that we do,
we can do some va- varia- variable elimination.
So I'll walk through this pretty quickly.,
there'll be lots of equations,
but I promise you that they're actually pretty simple equations.
So we need to write down the distribution which we're doing inference.
So it's a distribution over all of our states and actions
because we don't know what those are going to be given our evidence,
which is that these optimality variables are true.
We have three types of conditional probability distributions in this model,
the initial state, p of s_0,
our optimality variable probabilities exponential of r,
and the transition probabilities p of s_t plus 1, you have an s_t, a_t.
We're gonna take this joint,
and then we're gonna compute the conditional that we want,
which is p of a_t, given s_t comma our evidence.
So that's the thing that we want.
And the way that we're going to do this is going to be using variational inference,
because s's and a's are continuous,
huge, so doing this, uh,
inference exactly is intractable,
but variational inference gives us
a nice optimization-based way to solve this inference problem,
and it requires us to define a variational distribution.
Remember that a variational distribution is just the distribution over all of
our hidden variables that has some convenient form that makes inference easy.
Usually, we choose the variational distribution to be something really
simple like a product of independent marginals.
Here, I'm going to make a very peculiar choice for my very short distribution.
I'm going to choose it such that it looks a lot like my original distribution.
Usually, if you're doing variational inference,
this is not a very good idea because the whole point of doing
variational inference is to use a simpler variational family than your original model.
But it turns out that in this very, very special case,
this will lead to a- a very convenient solution.
So the way that I'm going to define
my variational distribution is by using the same initial state probabilities,
the same transition probabilities,
and then I'm going to- the only thing that I'm actually going to say
that's different is Pi of a given
s. So up here I have this exponential r, here I have the Pi.
So that's my, uh, variational family.
This is just, uh, a, a picture of the,
of the variational distribution.
So you have this additional edge from s to a,
and you no longer have the O's because that's your evidence,
and we don't put evidence in our variational distributions.
So now, at the variational lower bound,
which is what we optimize when we do variational inference,
it's simply the KL divergence between Q and p. Um,
if you, uh, don't believe me,
you can do a little bit of algebra.
I promise you this is, this is in
fact a correct way to write the variational lower bound.
And being a KL divergence,
so you can express it as simply the expectation under Q of log Q
minus log p. And this is where the cleverness is gonna come in.
The reason that we chose our Q in
this very peculiar way is that now we're gonna get lots and lots of cancellation.
If we actually substitute in our definition of Q and our definition of p into here,
we get a huge, huge equation,
almost all of which cancels.
I- if you're wondering what I did here,
is I just put the log of these in here and log of products as sum of logs,
so that's where all this stuff comes from.
And of course because I chose the p of s_0 here to match the one here, that cancels out,
the transition probabilities cancel out,
so the only terms that remain are the log Pi's and the rewards.
And this is all in expectation under Q, which is this thing.
So that's just equal to the sum over all my time steps of the expectation of r,
plus the entropy of Pi,
which is exactly what we had on the previous slide when we
talked about maximum entropy reinforcement learning.
So maximum entropy reinforcement learning,
its objective is exactly the variational
lower bound for this particular variational approximation,
which means that we are in fact doing approximate inference in this graphical model.
That's pretty neat. That means that, uh,
our reinforcement learning procedure now actually corresponds to probabilistic inference.
Uh, now, how do we actually optimize that variational lower bound?
Uh, well we have a few choices,
one choice is to just use policy gradient.
Policy gradient is great because you can just write down your objective,
calculate its derivatives, and then do gradient ascent, and it'll work.
It'll take a huge number of samples, but it will work.
Um, if you want to be a little more clever,
you can actually modify Q-learning to optimize this objective.
It turns out to look like a procedure that greatly resembles message passing.
Uh, so how do you modify Q-learning to optimize this objective?
I'm not gonna prove this,
I'm just gonna state the result but, uh,
you can look up the, uh, you know,
the explanation for why this is true in this tutorial.
So regular Q-learning performs updates like this.
So reward plus the max over the next Q,
that's the Q-learning algorithm that we all learn from, uh,
Richard Sutton's textbook or whatever first reinforcement learning class you took.
If you want to modify Q-learning to solve this inference problem,
all you do is you replace this max with
a seemingly cryptic log of the integral of an exponential.
This first- at first this seems like a really bizarre thing to do,
what's a log of an integral of an exponential.
But imagine for a second that your Q values are very, very big.
If your Q values are very, very big,
when you exponentiate them,
the biggest of them dominates all the rest.
When you sum them all together,
that sum is entirely dominated by the exponential of the biggest value.
So when you take the log,
you essentially get back to the biggest value.
So if your Q values are very big,
then the log of the integral of the exponential is very close to a max.
And that makes a lot of sense,
because if we're matching distributions,
if we're doing this max n thing,
and we have one choice that is extremely large,
and all other choices are extremely bad,
then we'll just put all our mass there,
and then we'll do a max.
But if all our choices are about equally good,
then we wanna put probability mass on all of them and then we
have a very soft, uh, max here.
That looks more like this one. [NOISE]
So all you do is your regular,
uh, Q-learning except that instead of a max,
you use the log of the integral of exponentials and then you will
actually optimize this maximum entropy RL problem.
Okay. And then you'll do this nice distribution match.
Um, a, a modern algorithm that instantiates
this basic idea in more of
a policy iteration framework rather than
a Q-learning framework is called soft actor-critic.
Um, some of you probably heard about soft actor-critic in,
uh, Kate Rakelly's lecture.
Uh, and actually soft actor-critic in fact solves that same inference problem.
So it consists of primarily two steps.
There's a Q-function update,
which updates the Q-function for a given policy.
So this is not a Q-star update,
this is Q-Pi update,
and the only modification there is to incorporate that entropy term, the log Pi.
So this will convert to Q-Pi for a given policy under the maximum entropy objective.
And then there's a policy update step which is basically the KL divergence.
The MaxEnt RL, the KL divergence objective.
KL divergence between Pi and this Q-function.
And of course in practice you do
the usual actor-critic thing which means you take one gradient step on this,
one gradient step on that, and then repeat.
But if it is a proper policy iteration procedure,
you would actually run this thing to convergence,
and then run this thing to convergence.
And then you interact with the world,
collect more data sort of usual RL thing.
It turns out that the Q-function update is
essentially doing message passing in your graphical model.
And the policy update is fitting that variational distribution.
So they actually correspond to parts of a well-defined inference procedure.
Um, this algorithm works decently well, um,
so it, it gets good results on a bunch of benchmark tests.
But maybe more interestingly, it actually, uh,
exhibits some interesting properties that might suggest that it
could alleviate some of these issues that I outlined on the first slide of the lecture.
For example, you can use soft actor-critic or in
general soft optimality algorithm like soft Q-learning
to pre-train policies that can then be fine tuned on more narrow objectives.
This is not fully unsupervised pre-training yet that will come later,
uh, but it's sort of starting to get at that idea.
So one of the things you can do is you can train a policy for this little, uh,
quadrupedal amp robot with the reward function that says run as fast as possible.
Now, let's think about this for a minute.
If you reward function is just the norm of
your velocity and you run regular reinforcement learning, what will it do?
Well, it'll pick a direction arbitrarily and will run in that direction.
If you're doing MaxEnt entropy reinforcement learning,
our objective is to maximize our reward while being as random as possible.
So let's, let's imagine for a second what that would do with
an objective that just says maximize the norm of your velocity.
Okay. So if you've all thought about it, uh,
let's see what it actually does.
So this is the soft Q-learning method.
There it goes. It's a Halloween special.
Uh, it's fright- it's this frightening explosion of spiders.
But once you get over the initial scare, you'll notice that,
uh, it's actually running in essentially a distribution of possible directions.
So it tries to run as- in as many directions as
possible because by doing that it can maximize entropy.
Now why is this useful other than,
uh, you know, uh, scaring people?
You can take this thing and you can fine tune it with a specific direction.
So you can for example put it into a hallway.
And then we're going to fine tune it to run down this hallway.
So you'll see this one is initialized randomly,
this one is initialized with deterministic RL,
and this one is initialized with maximum entropy RL.
The initialization from the deterministic RL knows how to run in
one specific arbitrary direction which is
almost certainly not the direction of this hallway because,
uh, you know, it's extremely unlikely that was the one I picked during pre-training.
The maximum entropy one knows how to run in all possible directions.
So all that has to do is just
unlearn all the wrong directions and just keep going in the right direction.
So that allows the MaxEnt initialization to learn a lot more quickly in practice.
And of course if we look at the plots, that's pretty much what we see.
So the blue lines show fine tuning for the MaxEnt initialization and,
ah, the green lines show fine tuning for the deterministic,
uh, reinforcement learning algorithm.
Um, okay, I'll probably skip over
the hierarchical policies since I do wanna get more to the,
uh, fun unsupervised stuff.
But maybe I'll briefly show some- the robot videos as a little intermission.
So, uh, besides, uh,
generating videos with exploding spiders,
we can use this to actually train robots to do interesting tasks in the real world.
Um, so this is, uh, a soft Q-learning experiment where we use soft
Q-learning to train this Sawyer robot to stack a Lego block.
We weren't so much interested as- in,
in whether or not it could stack the Lego block.
I mean we, we know how to do this part.
This is not that difficult, but we wanted to see is whether the injection of
that entropy maximization resulted in interesting behavior at test time.
So what we did at test time of course is that we went in and we messed with it.
We went in and we perturbed the robot
to try to see how well it could react to our perturbations.
And the reason that it can react to those perturbations is
because during training you're maximizing entropy,
which means that you are essentially teaching the policy to be
resilient to as much injected noise as possible.
You can do this on other kinds of robots too.
This is a- this is actually soft actor-critic
running on a quadrupedal robot that is trying to learn how to walk.
Uh, so initially, the robot doesn't know anything.
So it sort of wiggles around randomly.
Uh, and then after some time passes,
it begins making forward progress.
And this, uh, this basically learns in about two hours how to walk forward.
And once learning is concluded,
then you can run the robot and it has a pretty decent- pretty plausible gate.
And then again, you can start messing with it and see how much
the entropy injection during training actually helped it acquire robust strategies.
Of course, they've never seen slopes like this before.
So struggles a little bit and you can- if you give it a large enough perturbation,
it will eventually fail, but it has some degree of robustness,
so it can walk downstairs.
It can't actually walk upstairs,
that will fail, but it can go downstairs.
Uh, and, uh, it can very poorly play Jenga.
[LAUGHTER] So, um, and there we go.
So that's the- that's maximum entropy RL.
Now maximum entropy RL is not yet unsupervised reinforcement learning.
We still have to give it a reward function,
but subject to that reward function,
it was learning a breadth of different behaviors.
Next, we're gonna talk about how we can
actually learn without any reward function at all.
So let's think back to this scenario.
You buy your household robot,
you put it in your kitchen, you turn it on,
and you let it work for a whole day,
and then you come back in the evening and you give it a goal.
And it's supposed to do something during this day that best prepares
it for the goal that you're gonna give it. So what can it do?
Uh, well, before we talk about what it's gonna do during,
during it's day, let's actually
first fast-forward and talk about what's gonna happen in the evening.
In the evening, you need to give it an objective.
And that means that you need an interface.
Let's nail down a very simple interface.
Let's say that you're gonna show it a state that you want it to reach.
So if it's doing things using camera images,
then maybe what you're gonna show it is an image.
If it's doing things using, you know,
a vector of states of all the objects in the world,
then you're gonna, you're gonna give it
a desired vector of states of objects in the world.
So you're gonna give it a goal state.
This- I'm not necessarily gonna claim that this is a good interface.
It's just a simple one that allows us to get started.
It's a little weird to, you know,
task your robot by giving it a photograph of the desired outcome.
But at least it's simple and we can all understand it.
So if that's the interface we're gonna use,
then somewhere under the hood,
your friendly robot needs to be able to compare two states.
And it needs to look at this picture,
and it needs to look at this picture,
and tell you how similar they are to each other.
There are a variety of ways to do this, uh,
but the way that we're gonna try to do it is by building a latent variable model
whose latent variables are a little bit
more meaningful than the original observation space.
So if you compare the pixels in an image, the,
the resulting distance doesn't really
correspond to like the physical proximity of two states,
but maybe you can embed those images into
some lat- latent space where distances
are meaningful and you have a variety of choices for how to do this.
GANs will do this.
We're gonna use something called variational autoencoders.
Just another latent variable model,
and there are many choices.
And then we're gonna use the latent space of
that variational autoencoder to compare two states.
And then since we actually have this latent variable model,
the way that we're gonna get the robot to practice during the day
is by generating samples from that latent variable model.
So the robot has a model, it can sample from that model,
it can essentially hallucinate potential images,
set those as goals for itself and then go and practice reaching them.
That makes a lot of sense. I think, you know,
that this is a plausible way to practice goals.
But there's, um, a pretty substantial problem with this setup.
Does anyone have ideas about what problem you might have if we go and do this?
It might [inaudible] ?
Right. So one answer is that it might- it might set easy goals for itself.
Where's the latent variable model gonna come from?
We have to train it on data.
There's only one source of data, it's the robot,
so it's gonna use its own experience to train that latent variable model.
Maybe, it did some random useless stuff.
Maybe, it broke the dishes,
and now it has lots of data of breaking dishes.
So it'll generate more goals of breaking dishes,
and it'll just keep going in a loop,
essentially doing the thing that it's done successfully before
setting those easy goals. So that's no good.
What we need is, we need some way to encourage it to diversify.
So, um, let's, uh,
first sort of outline this algorithm a little bit more precisely.
We have our variational autoencoder.
That variational autoencoder has
a decoder distribution that I'm going to call p theta of x given z,
so x is your state or image and z is your latent variable.
It has a prior p of z.
So if you wanted to sample, your first sample z from p of z and then x from,
uh, p theta x given z,
and it also has an encoder q phi,
uh, z given g-,
uh, s, uh, sorry,
z given x, that should be z given x.
It's a little typo. Um, and the way that
your training is going to proceed is that you're going to propose a goal.
How do you propose a goal, where you sample from your VE?
So sample z from p of z sample x from p of x given z,
then you're going to attempt to reach that goal using your policy.
So your policy now is pi of a given x comma
x g. So it's conditioning your current state and the goal.
So you're going to learn this giant policy that-
that will attempt to reach all possible goals.
Of course, your policy, in general,
will not necessarily succeed,
it won't reach that goal perfectly,
it'll reach some other state,
x bar and if it's a very good policy then x bar would be very similar to x g.
If it is a very bad policy then x bar will be
some random thing, and then you're gonna learn.
You're going to use your data to update your policy,
and you're going to use your data to update your VE,
which means that you're going to change p theta and q phi.
Now, of course, in practice,
the way that we would implement this is we would,
uh, use a replay buffer.
So when you attempt to reach the goal,
you just add this to your buffer,
and then you use all of your data so far to update your policy and your VE, but for now,
let's- for simplicity let's think of this as an online algorithm,
that will just keep things a little simple, and then of course, you repeat.
So this easy goals problem happens because we use this data that we actually,
obtained from this policy to update our goal proposer which will then propose
more goals that looked like the states were reached and will
start going in a circle and not actually, do anything interesting.
So intuitively, what we'd like to do,
is we'd like to up weight those goals that seem more interesting to us.
So if your robot lives in a 2D environment,
and these little circles denote the states that it had visited.
If you have a big clump of stuff over here and you have
these really rare things that you reached out of the fringes.
Intuitively, what you'd like to do is to somehow up-weight these rare things,
and do those more often because those are probably the harder,
the more interesting goals.
So maybe, if you somehow up-weight them,
and then fit your model,
and then propose from that model,
when you then go to reach those goals you might actually,
end up putting more mass onto the tails of your distribution, and gradually, expand.
And we'll make this very vague notion a little bit more precise, later.
So in order to do something like that,
all we really have to do is we have to modify this last step because step four,
is the step that actually,
uses your data to change which goals we propose in the future,
and we need to modify step four,
so that if the state x bar that you reached was very interesting,
it should get a bigger weight and if it was very boring it should get smaller weight.
Now, interesting and boring here means,
is it a state that you've reached,
often or is it the state that you've reached, rarely?
[NOISE] So that's this step.
[NOISE] Here's how you can do it.
Standard maximum likelihood estimation will basically,
train your VAE to maximize some approximation to the log likelihood of x bar.
Now, if it's a VAE you're actually using a variational lower bound
to approximate this because you can't evaluate log p of x bar,
exactly but some approximation to log p of x bar.
Weighted maximum likelihood estimation exactly,
as the name might imply will just put a little multiplier on
this and this multiplier is some function of x bar,
and by choosing that multiplier carefully, we can actually,
create this effect where the interesting or rare outcomes are
up-weighted and the uninteresting or common outcomes are down-weighted.
All we do, is we use
our previous density estimator, our p theta x bar or some approximation thereof.
Raised to some power alpha,
where alpha is between negative 1 and 0,
so it's basically, gonna be,
uh, it's- it- the- the weight is smaller for
larger probabilities to weight our likelihood.
So that means that, if a particular x bar is extremely probable,
it's in this really dense region,
then when you raise it to the power,
let's say, negative 0.5,
it'll get a much smaller weight.
If it's very rare,
it's probabilities like 0.0001,
then you raise it to some negative power and you get a really big weight.
So that'll do exactly what we wanted,
we'll up-weight these things in the tails and the
interesting result that you can prove which is described in the second paper,
Dalal, Pong, Lin et al, is that for any alpha between negative 1 and 0,
following this procedure actually,
increases the entropy of p theta of x.
Now, if you increase the entropy every single step,
and you can prove that it doesn't asymptote to something
suboptimal and eventually, you get full entropy.
You get the highest entropy distribution that you can have,
and the highest entropy distribution that you can have is
the uniform distribution over all valid states. Question?
[inaudible]
Right. So the question was,
at each step why don't you just uniformly sample from the state space?
If you could uniformly sample from the state space that would be a better algorithm.
The trouble that we have is that our,
uh, observations or our states here.
These Xs they might be for example,
images from the robot's camera.
Uni-formally, sampling images, uh,
by itself doesn't actually produce
uniformly sampled valid states because
most random images are kind of static, they're white noise.
So what you want is you want uniformly sampled valid states,
and that can be a really complex manifold in the space of all possible x vectors.
So essentially, you can view this procedure as
a really fancy way of discovering that uniform distribution.
Any other questions? [BACKGROUND] Okay.
Um, so now we have,
uh, we've- we've defined how this step is going to work,
we just use this w which is- which comes from using our previous density estimator,
and now we can ask the question, what is the objective for this procedure?
So we kinda cooked up this recipe but what is this recipe doing?
Is it actually optimizing some well-defined objective?
And that's where things get a little bit interesting.
So if your entropy increases every step,
you can actually show that under a few additional conditions,
what you're actually optimizing is the entropy of your goal distribution.
So this p of g this is the distribution of your goals which are coming from your VE.
So you're increasing that entropy,
so you're maximizing that entropy,
and you can show that this is actually true.
But that's not all that you're doing,
you're not just training a VE,
you're also training a policy.
So your goals get higher entropy due to this skew fit procedure,
but what does RL do?
Well, your policy pi,
and I'm gonna rewrite it as pi of
a given S, G just to make it very explicit that there's a state and there is a goal.
It's being trained to reach goals,
and if it does a very good job of reaching those goals then it will reach a state,
its final state S that is close to the goal,
and in fact, that might even be equal to the goal.
So what that means is that as your policy gets better and better,
it becomes easier and easier to predict what
the goal was from looking at the state that it reached.
So if the policy is very good,
if it always reaches the goal then G is equal to S. So P of G given S,
is a very low entropy distribution.
Now, where have we seen that before?
We're making one entropy big and we're making some conditional entropy small.
[BACKGROUND] So if that's our objective,
that's actually, exactly the mutual information between S and G and that's kinda cool.
Somehow, we had this kind of cooked up procedure involving
VEs and maximizing entropies and- and up-weighting things,
and all that turns into is
just the mutual information between the last state that you reaching your trajectory,
and your goal, and if you maximize that mutual information,
then you end up successfully,
reaching a wide range of goals.
So maximize mutual information between S and G,
at least a good exploration because you're trying to make this first entropy term big,
and it leads to effective goal
reaching because you're trying to make that second entropy small,
and of course, there are other ways to approximate
that mutual information of which this algorithm is just one of them.
Any questions about this?
Okay.
Again, I- I'm contractually obligated to show you robot videos.
So, ah, we ran this thing on a robot and, ah,
therefore what we have to show this to you, uh,
at zero hours so that what,
what is this robot doing first of all?
It's, it's a robotic arm,
It's placed in front of a door,
ah, you might think that it's told to open the door,
it's not told to open the door, it's just placed in front
of the door but that's a very suggestive way to,
ah, set up your robot.
So of course initially,
what it's going to do is it's gonna wiggle its hand a little bit because that's
the most obvious thing to do for an untrained reinforcing learning algorithm,
but pretty quickly it figures out that hand wiggling results in
images that have a high probability under its VAE and it starts
proposing all sorts of other goals
that involve actually interacting with the door because
that allows it to achieve outcomes that have a low probability under its VAE,
and then that probability increases,
and it opens the door to all sorts of different angles.
And then at the end once it's all trained,
it can actually open this door to any angle that you want,
ah, without actually being told in
advance that door opening is what you're supposed to do.
So you can think of this as a very,
very limited version of that household robot,
where it's not placed in a messy kitchen,
it's placed in this admittedly rather pristine laboratory setting,
but maybe a step in the right direction.
And you can test it,
you can give it different goal states.
So here, um, a person is actually giving it pictures and
we're just testing to make sure that it can open the door to every desired angle.
All right. So that was learning with goals.
Now, the next thing that I'm going to talk about is kind of
a state distribution matching formulation of reinforcement
learning that generalizes these ideas a little bit further.
So remember before I talked about this MaxEnt RL thing,
which matches action distributions,
and I also talked about how we can maximize the entropy of possible goals.
So can we somehow combine these ideas to match state distribution?
So instead of just making your state distribution as broad as possible,
can you say match a distribution in state space rather than in action space?
It's potentially quite useful because then you can go and explore
diverse states under a prior over good states.
So if- so maybe your household robot,
uh, needs to be told in advance,
I don't care about states where my dishes are broken,
I don't care about states where,
you know, you threw my vase out the window. All the other states, those I care about.
So do everything except for breaking my dishes and throwing stuff out the window,
and then you're gonna have a much more coherent exploration strategy.
So before I describe how to do this,
as an aside, let me describe something that does not solve this problem.
It, it might seem like it solves us from but it doesn't,
and it's something called ex- intrinsic motivation.
So this is a common method for doing exploration in non-information theoretic settings.
You incentivize your policy Pi of a given s to
explore diverse states before it sees any reward.
And the way that you do it is by rewarding visiting states that are novel.
This first seems like it's very,
very similar to what I discussed before but it actually leads to very different outcomes.
So if a state is visited often, it's not novel,
if a state is visited rarely,
then it is novel, and the way that we do this,
this is kind of very standard, uh,
reward bonus exploration is by adding a little term to our reward function based on
the negative log p Pi of s. So you estimate how often
your policy visits each state and then subtract those log probabilities from your reward,
and that'll get your policy to receive
larger rewards for states that have visited rarely.
So you update your policy to maximize this expected,
the expected value of this modified reward,
update your p Pi,
update your estimate of the state marginal, and then repeat.
And what does this do? Well, let's say that initially your policy does this.
You fit your state distribution,
you get this green circle.
It's a little faint on the slide but there's
little old green circle that I actually drew around this thing.
So then anything inside that green circle
gets a lower reward because it has higher probability.
Everything outside that circle gets
a higher reward be- because there's lower probability.
So what does the policy do?
Well, it goes somewhere else and then your distribution changes,
and then it goes somewhere else, and so on.
So your policy does end up going to lots of
different places but at any given point in time,
your policy is not actually covering the space,
it's just going to some arbitrary location.
It'll go somewhere else next time.
So this distribution keeps chasi- chasing your policy and your policy keeps changing.
At no instant in time, do you actually have a policy that covers the space.
So it kind of seems like it's very close to the right answer,
but it's not quite the right answer. There's something missing.
So can we fill in this missing piece?
Can we somehow change this intrinsic motivation method
so that it actually matches a desired state distribution?
It turns out that it's not actually that hard.
So the state marginal matching problem is to learn a policy Pi so as to minimize
the KL divergence between that policy state marginal and a desired state marginal.
So you have a target density and our idea is to somehow
utilize this intrinsic motivation, uh, principle.
So we're going to define our reward function as log of, uh,
our desired probability minus the log of our current, uh, probability.
So this is starting to look a lot like, uh, uh,
KL divergence, it's also starting to look a lot like intrinsic motivation.
Now, this by itself doesn't yet perform
marginal matching because your policy keeps getting
chased around like this your policy never actually ends up,
uh, matching the distribution,
but it's very close.
So the procedure is you, you learn your Pi_k and
I'm going to actually gonna put the superscript k to denote
the policy iteration k to maximize the expected value of your reward r_k.
So r_k means that the p Pi is actually p Pi_k,
you update your distribution to fit the state marginal to
get p Pi_k plus 1 and then you repeat.
And let's say this orange circle is my desired distribution.
So by itself, my policy will sort of try to escape from my,
uh, distribution, it will keep getting chased around and never actually match the thing.
So what if instead,
we update our distribution p Pi_k to fit all the states seen so far?
Um, so that will take care of one side of the problem because now
we're actually- we're not just matching distribution from the current policy,
we're matching distribution from all policies.
And then what we return at the end is not the last Pi_k but
the mixture of all the policies seen so
far because remember these policies were running all over the state space.
It went here, it went there, it went there,
no one policy covers the space but the ensemble of
all policies together does actually cover the space.
And it's not actually just a vague intuition,
it turns out that, uh,
you can actually show that this ends up being, uh,
the Nash equilibrium for a two-player game,
where one player is trying to, uh,
basically optimize this objective and the other one is trying to match the distribution.
This is a, a technique called a
self-player or someone referred to as historical averaging,
where if you're optimizing these objectives
in alternating fashion the way that you actually recover at equilibrium,
which has to be a stochastic policy,
is by averaging together the iterates.
So this, this is just like a theorem that you can look up.
Very simple modification to intrinsic motivation,
turns out to actually match distributions.
So it's a game between the two players,
the players are Pi_k and p Pi_k.
And of course, there's a special case of this where if you
choose p_star to be a constant,
meaning the uniform distribution, uh,
then you're just maximizing state entropy,
and then, then you recover the algorithm from the previous section.
Okay. So does this work?
Um, the answer is that it basically does.
So this is a, a comparison between soft Actor-Critic,
which is a maximum entropy reinforcing learning algorithm that matches
action distributions versus the state marginal matching method
which matches state distributions.
And there's a heat map visualizing
this little quadrupedal robot running
around in this little funny world with three hallways.
And you can see that the heat map or say marginal matching is much more uniform.
This is trying to match a uniform distribution.
Um, I'll skip over this.
Here it's trying to match kind of a funny lopsided distribution.
So it's trying to move this little block to different places and we tell it we
want the left side to be twice as likely as the right side.
Now, of course it can't match it perfectly.
The red line is a state marginal matching
because that's contend with the dynamics of the real world.
So you can't go to the left side without
passing through at least a little bit of the right- right side.
But you can see that it's a little bit more skewed to the left,
so it's a lot more probability mass on the left side,
a lot less than the right side.
So kinda seems to be doing something.
All right. But maybe the next question that I wanna ask and I think that's
an important question is- is this actually
a good way to explore like intuitively kinda make sense.
You want to see lots of different things in the world so you wanna get good coverage.
So you wanna maximize your state entropy.
Is it actually a good idea to maximize state entropy.
So is state entropy really a good objective and if
it's a good objective what does a good objective for?
So, you know, just to sort of recap what we saw before,
the skew-fit does this,
you know mutual information between states and goals,
which corresponds to maximizing goal entropy and minimizing goal given state entropy.
State marginal matching in the special case where you choose
P star to be a constant also maximizes a state entropy.
So it seems like this is something we kind of know how to do.
They're more or less the same thing.
But when is this a good idea?
So one place where it's a good idea,
and I'm gonna quote Eysenbach's theorem.
It's not actually a theorem.
It's just Ben Eysenbach was the one who- who told me about it.
But it's, uh, it follows trivially from a classic maximum entropy modeling and result.
So don't take the name or the fact that I call it a theorem too seriously.
Um, It says that if at test time an adversary will choose the worst goal,
meaning that when you come home after letting your robot run all day,
you know what your robot is good at and you're gonna choose the worst possible thing.
You're like the world's worst robot owner.
If that's what you're going to do,
then the- the best goal distribution to use for
training is actually the one with the largest entropy.
So if the robot knows that you're gonna to be the worst possible robot owner,
then the optimal thing for that robot to do without any other knowledge
about what kind of goal you might propose is to maximize the entropy of goals.
And this follows directly from the equivalence between maximum entropy modeling
and the adversarial formulation where you maximize p and minimize q.
So if you read about maximum entropy models,
it's just basically a trivial corollary to that.
So if you wanna read a little bit more about this idea,
it's summarized in this paper called
Unsupervised Meta-Learning for Reinforcement Learning,
which I'll come back to later.
But the short version is that there is actually a kind of
a principled reason to suppose that maximizing state entropy is good,
uh, because it makes you robust in the face of the worst-case human so to speak.
And another actually excellent paper to read about some theoretical reasons
why high state might be a good idea is this paper called
Provably Efficient Maximum Entropy Exploration by Hazan et al.
So if you're interested in some of the theoretical foundations for this,
I highly recommend that paper.
Okay. So the next kind of algorithmic idea I wanna discuss a little bit is how we can go
a little bit beyond just covering
states and actually think about covering the space of possible skills.
So, so far, all of the unsupervised RL algorithms we've talked about,
they are attempting to reach goals or reach states,
maximize standard, reach that sort of thing.
But there's more to behavior than just reaching states.
So next, I'm gonna talk about learning diverse skills.
So a skill- you know,
I'm gonna define skill in a very peculiar way.
Don't take this kind of too seriously but
I just need a term for it so I'm going to call it a skill.
I'm gonna to have a policy pie that gives
a distribution over actions, conditioned on states,
and conditioned over the- on the Z before this
was a goal but now it's just gonna be a task index.
What's a task index it's- it's an integer.
It's gonna be like zero, one, two, three.
And intuitively, if you want diverse skills,
then what you want this pi to do is you want it to go to-
to go into different things for different values of Z.
So maybe, if you give it a z of 0,
you should you- should have it like go up here;
if you give it a z of 1, it should go over here;
a z of 2 should go there.
So basically different z's should do different things.
Okay. That kinda makes sense.
Um, now, like integers do this
with maximum entropy reinforcement learning or that goal reaching thing.
Well, so maximum entropy RL,
it just does action entropy.
Action entropy is not the same as covering the space of skills.
So you'll take different actions but you might actually end up in
very similar places from taking very different actions.
So that's not quite what we want.
You can take different actions and land on similar states.
Reaching diverse goals is not the same as performing
diverse tasks either and that- that's a little bit more subtle.
The reason for this is that not all behaviors can be captured by goal reaching.
Let me give you an example of a behavior that cannot be captured by goal
reaching. This is my- my favorite quadrupedal robot.
It needs to go to this green location and it needs to avoid this red thing at all costs.
So maybe the red thing is the- is the fire pit of death.
That's where you're not allowed to go.
The green thing is your- is your favorite object in the world.
If you just try to reach this green state,
then you'll go straight through the bad red thing.
So you cannot frame this as a pure goal reaching problem.
Now, this task can still be defined in terms of states is just that
now the state of distribution that you want is more complicated.
There are some states that you want to not land,
then in some other states, you want to land on as often as possible.
And then to go back to the MaxEnt stuff you know
the MaxEnt RL policies are stochastic but they're not always controllable.
So they- they'll do- they, you know, if you remember that, uh,
the ant running in all possible directions,
we couldn't really tell which direction to run,
it just randomly runs in random directions.
Here we want something very controllable.
We want different task indices to correspond to different coherent behaviors.
So the intuition behind the method that I'm gonna describe is that different skills,
different values of z should visit different state-space regions.
Not necessarily different goals but different regions.
And this generalizes the notion of goals.
So here's how we can do it.
We're going to use a diversity promoting reward function.
So when you train your policy,
you're always gonna train it the same way.
You're gonna to train it with RL.
Maybe maximum entropy RL but some kind
of a reward maximizing thing maybe with an entropy term.
And the thing that you get to choose is this reward, right?
So we're going to train our policy to be the arg max of some expected reward.
And of course, we have many different z's.
So we'll sum that over all possible Zs.
The choice that we have to make is the choice of R,
and our R will depend on Z.
So different tasks have different rewards but of course we can't design them manually.
We have to somehow acquire them automatically.
So we'll make a very simple choice.
We'll reward states that are unlikely for other Zs.
We'll say you get a big reward if you go to
a state that's very likely for the current Z and very unlikely for the other Zs.
And that'll encourage the Zs to specialize.
So the way that you do this if you want to,
uh, think about how this can be done mechanically,
is you train a classifier.
You train a classifier that predicts p of z given s. What is p of z given s?
Well, z is an integer, s is a state.
So you can treat z as a basic categorical labels p of z given s to
classify the classifies which integer was fed
into your policy that resulted in it visiting the state s.
So if I see you going somewhere because somebody gave you a command,
what I'm trying to guess is what command were you given that caused you to go over there.
So then you can imagine this whole system,
the training of the classifier,
the training of the policy and so on as
a loop where you have your policy or agent that takes actions.
Those go to the environment,
the environment responds with states.
The policy at the beginning of the first timestep of
every episode is given a skill, an integer,
a z, and the state goes to this discriminator- this classifier,
which attempts to guess what z was given to the policy.
And they are colluding,
they're cooperating with each other.
The policy's trying to do the thing that makes the classifier get the right answer and
the classifier's trying to look at what the policy
did and- and try to guess the right answer.
So now, let's think about what this interaction will
actually do to the behavior of the policy.
Let's say that I have only two values of C just for simplicity.
So I have a green value and a blue value.
Initially, you have a random policy
and when you condition on green and you condition on blue,
it kinda does the same random stuff.
But of course, just to random chance,
sometimes the green lines will be a little bit further this way,
the blue lines will be a little further this way.
So when you train your classifier,
the classifier will say I have no idea what's going on,
but if you really want me to draw a decision boundary, I'm gonna draw this one.
Because, you know, there's a little more green stuff over here,
a little more blue stuff over here.
It really has no idea but it's like it's saying,
like maybe this is like 51% versus 49%.
So there's just a little bit of tiebreaker through random chance.
When the policy gets to go,
now the policy is trying to help this classifier.
It's saying, oh you think that this is 51% green and this is 49% green.
Okay. I can do that. I can send more green stuff here and more blue stuff here.
And then the classifier gets to go again and
the classifier will draw the line a little more cleanly.
And then the policy gets to go and it will separate out even more and so on.
Eventually, this process will cause the policy and the classifier to,
uh, agree on a separation,
uh, of the state space.
And of course, this is an example of two skills
but now imagine you did this with like 2,000 skills.
Then you'd have policies that go to all- you have skills that go to
all the different places in the world and also avoid all the different places and
basically do all- all possible combinations of state space region coverage.
Um, okay. Any questions about this method?
Yes.
Is it possible to do this with like me- meaningful latent features of some kind?
That's a really interesting question.
Yes. So the question for those of you who didn't hear is,
is it possible to do this with some kind of meaningful, uh, latent features?
I think that could be possible, yes.
So you could imagine, for example, um,
doing some- something where you impose structure on the Z's.
You could, for example, instead of having the Z's be integers,
you can have them be vectors of categorical variables,
almost like vectors of attributes.
Uh, you could supervise some of the Z's.
You can say, well, these entries
correspond to like these semantically meaningful features,
the other interest could correspond to whatever you want.
So you could probably impose a lot more structure on the Z's.
Uh, we've tried a little bit of that but not very much.
I can, I can tell you a little bit after the lecture if you
want. Any other questions? Yeah.
Do Z's make difference goes- corresponding to different states?
Right.
[inaudible].
Yeah, yeah, yeah.
So the, so the question was- well,
I made a big deal out of how goal-reaching is not the same as, uh, as skills.
But now, I'm telling you that, that different skills correspond to different states.
So, so what's up?
Um, the- it's a very good question.
And the answer is a little subtle because
while this is trying to make different tasks correspond to different states,
it's no longer trying to make different task correspond to different goals.
So you could have, for example,
a state visitation profile for this blue line which is like
avoid this region and end up in this region.
So that's a little bit more elaborate than just reaching goals.
Now, there's, uh- there are other versions of this kind of,
uh, method that could be formulated.
For example, you could imagine training your classifier to predict Z
not conditional on a single state but conditional on an entire sequence of states.
Then you have non-Markovian rewards which is a little tough,
but then you would actually get this thing to
separate the policy into all sorts of different trajectories,
uh, which would maybe be a little closer to what your- to what you might want.
Uh, there's a subtlety here though which is that different trajectories might also not be
exactly what you want because you can have two trajectories that look very
different but achieve very similar outcomes.
So there's a choice that you have to make.
In fact, people have proposed variants on this objective.
There were a couple of other papers. I think I have a citation of that somewhere.
Let's see where I have that.
Okay. So here, there's a citation to Gregor et al, Variational Intrinsic Control.
So if you check out this paper,
Variational Intrinsic Control, uh,
it actually proposed a slightly different discriminator
which has conditioned on the current state and like the last state or something.
And you can do all sorts of combinations,
conditional trajectories, current state, future state,
whatever, and they all end up with slightly different definitions of what a task is.
Okay. Uh, any other questions?
Okay. Um, let's look at what this thing does.
So, um, we can, uh,
train it on everybody's favorite Half Cheetah task.
Uh, I'm not sure whether, uh,
you guys had to do any of the MuJoCo RL things in this course.
Do you know- raise your hand if you know what Half Cheetah is.
Does it mean anything- okay.
Okay. Good. [LAUGHTER] Um, so
that Half Cheetah is supposed to run, that's its goal in life.
But if you don't know what the reward function for Half Cheetah is and you
just run this unsupervised skill extraction method,
uh, sometimes you get skills that run in a very festive way,
uh, sometimes you get skills that actually run backwards.
It's very easy to tell whether you're running backwards or forwards.
So the discriminator really likes that.
Uh, but sometimes, it'll also do like a weird front flip,
because that's also very easy to distinguish from running forward and backward.
So basically, it tries to do the most extreme things that are the easiest to distinguish.
Um, here is the ant.
Uh, the ant kind of- you know,
the ant has a lot more different things that it could do.
So it ends up doing these kinda funny wiggles where sometimes it'll walk in like a,
a letter S-shaped, sometimes a letter J shape.
So it sort of walks in different wiggles.
Uh, if you run this on a really simple task like mountain car,
uh, mountain car is a pretty simple task.
There's only so many things you could do.
So usually, you get one of the skills extra corresponding to just outright solving the task.
So this, this skill is just doing the task.
It's basically reaching the goal,
the other skills are doing other weird stuff,
like this one appears to be trying to go as fast as possible.
So something is actually working and
the skills are actually distinguishable from one another.
Uh, you can also use this for hierarchical RL,
so you can train up some skills for the cheetah and then you can
make the cheetah do hurdles by treating the skills,
the different Z's as different actions for higher level policy.
The same with the ant. You can take the skills for the ant and then
train a higher level policy that directs the ants to different places.
[NOISE] So these are some of the learning curves for the hierarchical version.
Uh, but perhaps for the purpose of today's lecture,
one thing I do wanna make sure to cover is the connection between
this and all these other information theoretic ideas that I touched on.
So recall that our objective is to maximize the expected value of our reward and
our reward is log p of z given s. It turns
out that this is also maximizing mutual information.
But now instead of goals, you have Z's.
So it's maximizing mutual information between z and
s. Remember that mutual information is a difference of two entropies,
the entropy of Z minus the entropy of Z given S. This first term,
back when we had goals, it was very hard.
Now, it's very easy because these Z's are integers.
How do you maximize the entropy of Z?
We'll just pick your integers uniformly at random.
If you picked them uniformly at random,
that first term is maximized.
So that's really easy.
It's actually only the second term that's hard.
[NOISE] And the second term is of course minimized by
maximizing log p of z given s. If your classifier is perfect,
then that second term has zero- has, has no entropy.
So just make your classifier be really,
really good, conveniently enough,
that's exactly what the whole method is
doing because the classifier is training itself to be really
good and the policy is changing itself so
the classifiers had an easier time classifying its states.
So this method is also doing mutual information between z and s,
where z is basically the replacement for goals now.
Okay. So the last thing I'm gonna talk about,
and this is gonna be quite brief,
uh, is a kind of an application of this idea to meta-learning.
So, so far, we just talked about the unsupervised part,
like how do you discover these different skills?
But what if you actually want to use this,
uh, in a meta-learning setting?
So I don't think I need a quick summary of- well,
you should all know what Meta-RL is, right?
So I, I could just skip this.
I, I think if you don't- you know,
if you haven't been paying attention then maybe, maybe it's too late for that.
But let's talk about something else.
[NOISE] Let's talk about the bad stuff.
Let's talk about meta-overfitting,
uh, which is about as exciting as it sounds.
Uh, so meta-learning requires task distributions.
And where do the task distributions come from in meta-learning?
Well, they come from you? Right. You have to build them.
Uh, if you're doing reinforcement learning,
maybe you figured out that a good set of tasks to
meta-train on is tasks that require them to run in different directions.
And if you were smart enough to design those tasks,
define rewards for all of them,
and then use them for meta-training,
then you might be able to adapt quickly to new tasks.
So you might, you know,
run something like MAML,
then the ant does some cool stuff where it runs in place, ready to go.
And as soon as given even a tiny bit of reward for running in a certain direction,
immediately figures it out.
But, of course, that's only if you chose the right meta-training tasks.
When there are too few meta-training tasks,
you can meta-overfit which means that you're- you become
really good at recalling your training tasks,
but really bad at trying to figure out new tasks.
The same as overfitting and,
and supervised learning, you can have meta-overfitting and meta-learning.
So specifying task distributions can
actually be a very delicate choice and it can be quite difficult,
especially for reinforcement learning because their task
correspond to reward functions which you're gonna program by hand.
So it'd be very nice to be able to propose tasks automatically [NOISE] so that
you get as many tasks as you want and avoid meta-overfitting.
Of course, you have to propose them properly so
their distribution has something in common with the task that you want to test-time.
So you can imagine the kind of a pipeline for unsupervised meta-reinforcement learning,
where you're given a particular environment without a reward function,
you automatically acquire some tasks,
basically invent some things to do in that environment,
and then meta-learn on those tasks,
so that then at test-ti me when somebody comes along and gives you a reward function,
you can adapt very quickly to that reward function,
without having received any reward functions during meta-training.
So what does it mean to propose tasks?
Well, it means making up some rewards r of s,
z for different tasks to z,
so different Zs should have different rewards.
And Meta-RL just means train your model to maximize the expectation with
respect to all of your tasks of the reward of
the policy after the policy has been updated on that task.
So that's just the definition of Meta-RL.
So how can you propose tasks?
Well, you can do exactly the thing that I described before,
you can choose your r of s,
z to maximize the mutual information between z and s,
basically using any of the techniques covered in the lecture up to now.
And there's a crucial difference here,
which is bef- before the result was a policy conditioned on the task.
Before the result was always like Pi of a given s,
g or Pi of a given s, z.
But if we're doing Meta-learning,
that's not what we're after.
In Meta-learning, what we're after is a policy that can adapt to rewards.
So if we use those tasks for Meta-RL,
then what we get is not a Pi of a given s,
z, what we get is a policy that can learn from rewards.
And that's really nice, because now we can define
new tasks for this thing just by defining a reward.
We don't even have to know what the Zs mean.
Um, so this basic idea does actually, work.
So this is an experiment that uses the,
uh, the- the s- skill, you know,
mutual information between s, z method called the diversity is all you need or DIAYN,
uh, in an unsupervised Meta-RL pipeline.
And what these comparisons shows, uh,
the green one shows the performance add adaptation time,
so that's Meta testing,
where the Meta training was done using
task proposals using that Mutual Information method.
The red line shows RL from scratch,
and the blue line,
that's a pretty funny one that actually,
shows a setting where the Meta-learning was done by proposing random tasks.
The random tasks proposed by such- by initializing the discriminator with random weights.
So it so sort of a random shopping of the space.
The interesting thing about this is that
even the random proposals actually, work decently well,
in some case, like they work well for this simple 2D navigation task,
they kind of work decently, well for the ant.
They don't seem to work so well for the cheetah for some reason.
The DIAYN ones are a little more consistent, uh,
but the- the main point is that this kind of unsupervised proposals can actually,
get you some, uh,
decently meaningful Meta-learning models.
So this is kind of potentially a viable way to address the Meta-overfitting issue.
Okay. Uh, well, uh,
that's it for the lecture.
Uh, I'd be happy to take any questions from you guys, uh,
about either the last part,
the unsupervised Meta-RL or anything that preceded it.
Um, so if you have any questions now's the time to ask. Yeah?
Uh, in the [inaudible] [OVERLAPPING].
Yeah. [OVERLAPPING]
Any ideas for how to use information theory to make that adjustment? [OVERLAPPING]
Yeah, that's a really good question.
So the question was I pick the number of skills,
well, I didn't pick it, Ben Eysenbach picked it.
[LAUGHTER]. So that's my excuse.
Uh, somewhat arbitrarily, is there a better way to do it?
Um, I don't know.
Uh, we did try something that it was seem pretty obvious,
we just use a continuous space of skills.
That really, didn't work.
We're not sure why.
Uh, we also tried somewhat less obvious things like using a combinatorial space of skill.
So if you- instead of having an integer,
you might have a vector of binary values.
So if you have, you know, 32 binary values,
you can have 2 to the 32nd power of different skills,
still discrete set, but a very large number, that actually, works.
Um, but I don't know how to select that number in a principled way. Yeah?
So, um, around this idea of [inaudible] mutual information,
we can optimize in two ways, right?
We can in- introduce [inaudible] in variational distribution
[inaudible] introduce the forward model.
[OVERLAPPING] And I don't know if you've [inaudible].
And when you introduce the forward model,
we get this optimization with reward that depends on the forward, right?
[OVERLAPPING] Then you talk about his other idea
whether we use the probability of a particular goal we
reached as it increased motivation
[OVERLAPPING] which sounds like really
similar to this idea of curiosity
[OVERLAPPING] Um,
and one thing I've been trying to kinda reconcile about
these two methods of increasing motivation is that,
if you take mutual information with
foreign model has [inaudible] [OVERLAPPING] so
you can take curiosity and other stuff like,
[OVERLAPPING] using the novelty of the status as your reward.
Basically, [inaudible] are over the state distribution,
they optimize exactly the opposite thing,
[OVERLAPPING] but it seems that both of these things
[NOISE] create highly structured, um [OVERLAPPING]
Right. Yeah. [OVERLAPPING] Thi- this is
a- this is a very complex question that I ca- I can probably give you,
you know maybe like a 20-30 minute answer,
but I'll- I'll give you the short answer,
um, you can get- well there- there's a two-part short answer.
Uh, one part is,
you can get meaningful behavior out of seemingly contradictory objectives.
There are actually, many more examples
tha- that I could give you a- after the lecture if you like.
So that's a thing that happens,
uh, and this isn't the only case where it happens.
But um, in terms of trying to reconcile the-
the relationships between
these different techniques, uh, something that I might actually,
recommend is a- is a paper by Marc Bellemare,
Unifying Count-Based Exploration and Intrinsic Motivation,
which discusses this in a little bit more detail.
Uh, I don't think he covers the,
the Model Error version of intrinsic motivation in a lot of detail,
but he does talk a lot about the log P of S versus the counts and so on,
and that might be a starting point to try to understand how
these things can be reconciled. Yeah?
So the sort of diversity method seems like
[inaudible] RL where you're kind of partitioning the state-space,
but can't you apply it to
a more general sense of just something for data [OVERLAPPING] [inaudible]
Um, Right.
So the question was can som- can somebody like
this mutual information diversity method be applied to general unstructured data?
Um, so clustering does exactly this.
So if, if, if you imagine doing like ye-, uh, you know, uh,
Gaussian mixture model or k-means clustering it- that's exactly- exactly what it's doing.
It can there also in the sense,
to learn tasks to maximum of entropy on this data, as well?
Um, Right. Can it learn tasks?
That is a more complex question.
I mean, you could imagine a clustering as learning tasks,
like whe- when you cluster the spacing and say predict the cluster from the,
from the data points, so that's a task.
Um, in fact that will probably be the closest analogue actually,
because if you're classifiers doing you know,
p of z given s,
then if you do clustering into p of z given x tha- that's the closest analog.
I think, you can probably do more than that, but I don't know.
It's a good idea. Any other questions? Yeah.
[inaudible]
make sure you're
pretty good at everything,
um, as it goes to you're trying to match some target distribution, um,
and it seems like as your state space gets bigger and bigger,
the strategy is sort of,
being conservative and, you know,
preparing for the worst-case is less and less useful [OVERLAPPING] um,
and particularly, if you go back, in case,
your kitchen example, you know,
if your robot spends all day [LAUGHTER] preparing for your worst-case demand obviously,
that's going to include your command where you can say. [OVERLAPPING]
You might need a couple of days.
Sorry?
You might need a couple of days.
[LAUGHTER] [inaudible] Um, so I- I'm curious you know,
if- if- to make these algorithms,
useful in- in practice,
it seems like using this sort of you know,
pure maximum entropy effective procedures and revision probably
isn't going to be as useful as the ones where we come up.
We have [inaudible] some sort of prior about what are
useful type of [inaudible] actually [inaudible] question, um,
what is sort of your thought on how to go from,
how I want to prepare for, you know,
likely things I might ask you to do in [inaudible] work,
into an actual prior that we could inject into this algorithm.
Yeah. So um, the question was es- essentially, uh,
how can we do better than just
optimizing for the worst case in very complex environments,
where optimizing for the worst-case would take forever,
basically, where there are a gajillion things you could do.
Um, two part answer, uh, one is,
is a- a very lame answer which is that
this- the state- a state distribution matching formulation of reinforcement learning,
kind of aims to do exactly that.
It assumes that you're given some distribution.
I think my question was if you're- if you wanna do that?
How do you actually, go from in my brain,
I have this idea that things are useful [OVERLAPPING]
Yeah.
[inaudible] [OVERLAPPING]
Yeah.
[inaudible] [OVERLAPPING]
Yeah. Right. So how do you actually get that distribution?
That's a really good question.
And I don't really know.
And I think that question also gets at
something a bit deeper it's- it's really kind of a design question.
It's like what's the right way to- to communicate these things?
Or what's the right way to do- do, you know, should you use data
should you have some communication channel?
But wha- one other answer that I- that I- that I wanna give here which was maybe,
a little bit of lateral thinking on this topic is that,
perhaps it's not so unreasonable to try to learn all the things.
If you're do- doing a little bit better than just density estimation,
because what you can do is you can figure out that even though, uh,
you haven't broken every single dish yet,
having broken one of the dishes you can kinda
guess what's going to happen if your break all the others.
So maybe you don't have to break every single dish before deciding
that you've mastered the dish breaking skill and move on to something more productive.
So perhaps, uh, having this thing be somehow,
aware of your capability to generalize might actually,
cover the space without like physically covering the space.
Okay? Should we wrap up?
Yeah, thanks Sergey.
 Okay. Let's get started.
Uh, so for some logistics, uh,
the poster session is tomorrow, uh, 1:30 PM.
Print your posters well ahead of time.
Hopefully, you've got either already printed
it or you- you're going to print it very soon.
Um, your final project report is due Monday the 16th at midnight.
Uh, you're very welcome to submit it earlier, uh,
and this is a- a hard deadline like no late days for this
because grades are due very shortly after this deadline.
So try to get it done early.
Um, and then of course this is the last lecture.
Uh, and I'll leave some time for course evaluations at the end.
So you can fill those out and we're looking forward to
the feedback that you had throughout the course especially
since this is the first time that we're offering the course.
Okay. So, um, let's get to the main topic.
So I think today is actually, uh, in my mind,
one of the most interesting, uh,
and exciting lectures because we're gonna talk about,
what are the things that don't work very well,
and some of the things that we might do to resolve these challenges, uh,
and really the first in- part of
the lecture and most of the lecture will be covering some of the research that
we've been doing very recently to try to mitigate some of
the challenges in meta-learning and multi-task learning,
and, uh, the second part we'll get into
some more open and unsolved challenges in these topics
beyond all of the challenges that we've covered in the rest of the course.
Cool. So, um, what doesn't work very well?
Uh, the first thing that I'd like to talk about today is,
where do we construct tasks for meta-learning and for multitask learning?
Uh, in general, we- we saw reasonable ways to construct tasks.
For example, for few-shot image classification.
Uh, but in much more realistic situations,
it may not be clear how you're- how- how you might go about
getting this set of structured data from
the data sets that you have or the data sets that you have the means to collect.
Uh, and in particular, we'll- we'll talk
about some of the challenges that come up when you
try to construct tasks such as memorization problems,
such as how to use time-series data without task boundaries,
and how to, um,
how to actually have algorithms that can come up with the tasks
themselves from unstructured data sources.
Uh, and then beyond the problem of tasks, uh,
I'm gonna talk a little bit about how we might go about running
multi-task RL and meta-RL across very distinct task families.
So before, we were seeing, like,
different variations on a task like running forward or running backward or running at
different velocities or reaching to different points in space and,
can we think about actually running multi-task RL and meta-RL across,
uh, qualitatively distinct tasks?
Um, and we'll talk about challenges such as how do we
go about actually specifying the tasks to the- to the agent,
um, what sort of distinct tasks do we train on
and what are some of the challenges that arise when we actually try to do this?
Uh, and then lastly, we'll talk about open challenges.
Okay. So first, let's talk about how to construct tasks for meta-learning.
Um, and to briefly review,
uh, we were talking about, earlier in this course,
few-shot image classification and the way
that we - the kind of- the kind of problem that we want to
solve was something like this where you want to be able to learn
from a data set to classify new examples.
Uh, and the way that you would construct your task or your distribution of tasks would be,
uh, something like this.
This is what you did in your homework where you, uh,
separate the data in two sets of classes and you, uh,
randomly assigned labels to each of these, uh, images,
and then,uh, basically trained the network such that after entering the data on the left,
it could make classifications on the right.
Now, one of the things that you did here,
uh, is you essentially shuffle the labels.
Uh, you randomly selected, uh,
a label category for each of these five images, uh,
and that was kind of randomly permuted for every single task.
Now, one of the questions I'd like to ask you is,
what would happen if we didn't shuffle the labels?
Uh, and in particular,
what if we always kept the category corresponding to mushroom as,
uh, the second class label?
And what if we always kept the category corresponding to a singer as the fourth category?
And piano is the fifth category?
What would the meta- what would- what would the meta learning algorithm
do? Would it- would it work?
Would it- will everything be the same?
Would it be different? Does the question make sense?
Is this kind of what we did with our homework?
I mean, when we set up the initial categories,
it seems to change the performance.
Are we talking about the same thing?
Um, right. So this is actually something that is a little bit different.
So in your homework, one of the things that you- one of the things that you
found- that many people found to work better is, uh,
that you always, uh, like,
oh the thing corre- corresponding to class one was always for that first,
the thing corresponding class two is always for that second, etc.
What I'm talking about here is actually the assignment of
semantic class labels- the semantic classes to class labels.
Uh, and so in your homework, you would always- you'd
randomly sample class and assign that to a class label,
and then once you assign the class labels, uh,
you'd pass those class labels into a sequence.
Uh, but what if for example the,
the semantic class always corresponds to the same class label?
Like, uh, birds row is one,
mushrooms row is two,
that particular breed of dog was always number three,
singer was always number four.
We would use the class labels as always to do
classification which will make it more difficult to generalize with these new classes.
Yeah. Yeah. Can you elaborate on that a little bit?
Uh, information is contained within the class label if it keeps the same during training,
and as such when it goes to meta test time,
it will be looking for the class label as information and we'll be able to find it there.
Yes. That's- that's like very close.
So what would- what would it,
um, let's see, what would the,
um, would it like, uh,
would it be- if you give it a new task
corresponding to the same classes that it's had in the training
set would it be able to perform well?
Say it again. If it uses.
If it sees the same task as- as one of
the ones that saw before but just different images,
would it be able to perform well?
Yeah. Assuming the class label is the same.
Yeah. And then if you change the- change the order of the class label,
would it be able to do well?
And why? Why wouldn't it be able to do well? Sorry.
It's only learned the features in that one category.
Yeah. Is there a comment over here?
Yeah. I think [inaudible].
Yeah, so basically it can just memorize the ordering of the labels and say,
okay, bird is always the first label.
If I see a bird, I can just output 1, right?
And the mushroom is always second label,
I can always just output 2.
Uh, and so then if it ever sees a mushroom,
or a singer, or something, it'll always just know what to do.
It doesn't actually have to pay attention to
the training dataset to figure out the ordering, right?
And so it could actually- one,
it can learn to ignore the training data for each task because
it can just output the label, and two,
if it's given a new class and it doesn't know what the,
I- it- it is given a new semantic category,
it doesn't know kind of the- the label corresponding to that class that it can't,
um, it can't figure out what- what class label that corresponds to.
So let's look at another example that
might- may- might make this a little bit more clear.
So say we want to, um, learn a pose predictor.
Uh, and, uh, what we do is we, uh, if we really,
really wanna predict the orientation of an object
and we wanna be able to adapt to a new object that we've never seen before.
Um, so say we have some task training data, uh,
and we're given like four examples per
object labeled with the orientation of that object.
Uh, and our goal is given four images,
we wanna be able to predict new- um,
new orientations of that same exact object, right?
[NOISE] Um, now, to solve this task,
one of the things that the learner could do is just remember, uh,
what the canonical orientation of each object in the training set
is and then use that canonical orientation to output the pose of that object.
Uh, so we can- could actually, um,
learn to ignore the training data and just kinda
memorize the canonical orientation of the object and output,
uh, the actual orientation.
Um, is this a problem?
Well, this is fine for the objects in the training dataset and if it
sees a new couch that has the same canonical orientation, it's fine, right?
Uh, when is it bad?
So this is a problem when you see new objects, right.
So um, if you're- for example,
you are given a new, um,
class of object that you haven't seen before you
don't know the canonical pose of that object,
then because your meta-learner just memorized the canonical poses of all the-
all of the training objects and learned to ignore the data on the left.
It can- it can't generalize this new setting
because it- it doesn't know how to use the data
in order to figure out the canonical orientation.
Um, so this is kind of bad when we're given an
unseen object with an unseen canonical orientation.
Does this make sense?
Any questions about why this would fail?
[NOISE]
Okay. So then the question is,
like if this is a problem,
like basically we wanna be able to kind of train
our post predictor and it could just memorize the pose of all the objects,
of all the training objects, get perfect training error,
and then generalize poorly when it sees
new objects because it doesn't know the canonical orientation of that object.
Um, I'll refer to this as the memorization problem because it can memorize the aspect of
the training dataset in a way that doesn't allow it to- generalized to the test set.
Um, then the question is,
can we do anything about this?
Um, so let's try to actually formally think about this problem.
Um, so in the homeworks that you did,
uh, for fuchsia in image classification where you were changing the labels.
Uh, it was true that a single function couldn't solve all of the tasks.
Uh, and the reason for that is because you would change the kind
of class label corresponding to bird across different examples.
So you can't, you- your thing I- uh,
it would be like, uh,
you- you can't have a single function that will output
a class label of two for bird or a class label of one for bird.
Um, and this is due to label reshuffling or- or hiding information from the iterate,
like in the reinforced learning example,
you might hide the position of the object that you
want to reach and it has to learn through- that through reinforcement.
Now, if the tasks are what- are what called non-mutually exclusive,
that means that a single function actually can solve all the tasks.
Uh, and if a single function can solve for- solve all the task,
there's actually multiple solutions to the meta-learning problem.
And the way that you can see this is if you view meta-learning as
learning this function that takes this input training data and the input x.
Uh, what it could do is just choose to ignore that training data- uh,
choose to ignore the training data just learn the function that takes this input the-
the image or takes as input the input and makes a prediction based on that.
Um, so one solution is just memorize the canonical pose of the objects,
ignore the training data and output the pose of
the training objects with our- our function.
Uh, and another solution would be to not carry any information about
the canonical pose in your meta-parameters
Theta and acquire that information from the training data.
Um, and then if you see these two solutions,
then you can actually see an entire spectrum of solutions based on how information flows,
based on how much information about the canonical pose of the object you're
carrying in Theta versus carrying in- uh,
versus acquiring from the training data.
Okay. Does this makes sense?
Any questions? Yeah.
The other technical meaning of information flows?
So there is actually.
And so this is um,
you can actually measure these sorts of quantities through mutual information uh,
and other information theoretic quantities like uh,
in- in- information theory like entropy for example.
Uh, some of the quantities basically that Sergei mentioned in his lecture.
Uh, one kind of formal definition that you can think of, uh,
of when this is a problem is when
basically the mutual- the mutual information between, uh,
your prediction, uh, and your training data,
uh, given your input is equal to zero.
Uh, and what this means is that- and maybe this is
also given the parameters of your function Theta.
What this means is that, uh,
the training data isn't affecting your prediction in any way,
uh, and then so their mutual information is zero.
You're- you're basically- your prediction is only dependent on,
uh, Theta and x.
Okay. So this is, uh, this kind of characterizes when- when you might just
memorize this sort of quantity and now one of
the things that we could do about this if we take this view is that we could
potentially solve this problem by
trying to control the information flow in a certain way,
by basically trying to control whether or not the information comes from X,
whether or not the information is stored in Theta,
or whether or not it comes from the training data.
Okay. Um, so one of the things that we can do,
uh, is what we'll call meta-regularization, uh,
which is to minimize the meta-training loss which is
our- our performance on the meta-training set,
uh, but also the information that's stored in Theta.
And so for example, if we don't want it to memorize and
store information about the canonical pose in Theta,
um, then we can minimize, uh,
the meta-training loss and also minimize the amount of information in Theta
by using an information bottleneck by essentially constraining, uh,
a distribution over Theta which will be parameterized by Theta Mu and
Theta Sigma to be close to this prior distribution that doesn't- um,
that carries very little information.
Now, one of the things that's really important is
to actually include this first term, uh,
because if you only include the second term which is to minimize the information in Theta
then you wouldn't have any information in Theta
and would do terribly on your meta-training tasks.
Uh, and that's where that beta term comes in is that you need
to balance these two terms effectively.
Um, and- and what this- what this actually does is for- um, for larger Beta,
this is gonna place precedence on using information in
the training dataset over using information in your meta-parameters.
Okay. Uh, and then you can combine this with your favorite meta-learning algorithm, um,
and optimization-based algorithm or also like a black box,
uh, adaptation approach. Is there a question?
Yes, I don't understand this- this second part.
So how is the distribution over Theta obtained?
And is it that just like pushing
the entire networking user-generated random noise versus just like [inaudible] .
Yeah, so the way we do this is um,
you can for like an, uh,
optimization-based algorithm like MAML,
what we do is we add noise, um,
to the- to our weight vector that is, uh, uh,
has magnitude of, uh, Theta Sigma, uh,
basically and that's the- that's the variance of- of the noise.
Um, and you learn both the mean and the variance,
such that you do well on the train dataset and such that you,
um, try to stay close to this prior.
You can't achieve both of these objectives and so if you only
use the second objective than you'd just be adding, um,
you would just basically have random noise as parameters as Theta, uh,
and it's that first term that also encourages you to do well on the training dataset.
Does that answer your question?
Yeah, but there is an also Theta that,
um, define how we process the training set as well.
Yeah, so that's a good point.
So in this- in the case of MAML,
Theta is both the meta parameters and also kind of the update rule,
um, like the way that you actually go about learning from the training dataset.
Um, the update rule I guess it's- it's both from Theta and also the gradient
step and so disentangling those two is difficult to do in an approach like MAML,
um, in a black box approach.
Um, and so in that case you find
that it just works best to do it on on all of the parameters.
Um, in a black box approach where you have
separate pathways from the training dataset and X, um,
and then you might have uh, some,
some intermediate variable which we'll call Z,
um, and then you make a prediction.
We find that it makes sense to put the regularization only on this pathway,
and not on the pathway from here,
and that kind of allows you to do what you're suggesting which is
to only regularize and minimize the content from,
from the input and not the pathway from the training data.
But that's something that's a little bit more difficult to do in optimization-based,
like with like a MAML based approach.
[inaudible] you just mask the data-
Mask the data.
-to minimize performance when you're masking the data.
Right, so you could also try to minimize performance when you mask the data, um.
Which is kind of like the general [inaudible] gonna explain a bit.
Yeah, so this is the approach that does something somewhat like that, um,
called Task Agnostic Meta-Learning where you try to- what they tried to do is they,
um, say that, uh.
So in MAML you have Theta prime equals Theta minus Alpha grad,
uh, something like this and what they tried to do is they
say that F Theta should output a distribution that has maximum entropy.
Um, I think it's
I guess what they're doing is maybe a little bit different from what you're saying.
I'm not sure what they say is basically this should have
maximum entropy which corresponds to like minimum performance.
Basically, it should output a uniform distribution over the labels.
Uh, in practice one of the things that I think an approach like that can do is is sort of
cheat and pretend to like not have any information about the- the inputs.
But when you then take a gradient step it can real- it can kind of just flip a switch,
such that it- it knows it can then like, it'll pretend that it doesn't know
anything but then once you flip that switch it's accessible to the- to function.
Um, yeah and so if you do
something like Omniglot without label shuffling like I was describing before,
which we call non-mutually exclusive Omniglot.
Um, MAML does pretty terribly,
uh, on this in comparison to what happens when you shuffle the labels.
Uh, this- this approach that I just mentioned where
they try to maximize the entropy of the output,
also does quite poorly when you try to generalize to new,
um, settings because it's like basically,
secretly hiding the fact that it's memorizing things.
Um, whereas if you, um, regularize,
uh, the information in Theta like I mentioned before,
you can do substantially better.
All right, um, and then um,
likewise on the post prediction task the, um, uh,
if you compare MAML to the meta-regularized variant
and also conditional neural processes which is a black box approach, um,
to the meta-meta regularized version of that,
you can see significant improvements by, um,
incorporating this objective or this regularizer.
Uh, and this isn't just- just as simple as standard regularization,
so one of the things that we tried is adding this sort of standard regularization
on all of the parameters of conditional neural processes.
Uh, and we find that if you do it on all the parameters in a fairly naive way,
you also do, uh,
fairly poorly whereas if you just simply regularize,
uh, the pathway that,
um, that- that you care about,
then you're able to achieve good performance.
Okay. Any more questions on that before I move on?
Okay. So, um, the reason why I think this is important is that there are
many settings where we can't necessarily shuffle
the labels and make the task non-mutually exclusive.
And so I think that this work represents a step towards making
meta-learning algorithms more generally applicable to those kinds of settings.
Um, next what if we have data that doesn't have task boundaries?
So, uh, in that setting we had task boundaries but the, um,
we had these memorization problems but we had
these- we didn't know kind of what the underlying task structure was.
Um, instead what if we simply have like a time series of data and we want to be able to
meta-learn from that time series of data in a way that allows us to make predictions,
um, about new tasks very quickly.
And so some examples of this might be if we want to predict energy demand,
uh, or if we want to,
uh, predict the dynamics of a robot,
or of a car, um,
you want to predict transportation usage across time, uh,
if you want to make some money,
predict the stock market, um,
video analytics of reinforcement learning agent.
All of these, uh,
situations are examples where you have unsegmented time series of data,
but there is significant temporal structure.
So there's structure that, um,
that affects- that the nature of that data, uh, for example,
energy demand is going to be different if the weather is in different situate- is in,
um, if the weather is very hot versus very cold, um,
the dynamics of a car can be varied if you're on like a grass terrain versus,
uh, a very, uh,
slow- a very paved terrain,
transportation usage is going to vary based off of
different sports events or or whatever.
So there's significant temporal structure that, um,
that governs the- these processes and we should be able to
actually recognize that temporal structure in
those time series and latch onto it in order to make effective predictions.
Um, and so what we're gonna try and do is learn to segment
the time series into tasks and then meta- learn across those tasks.
Uh, so then the key question is how do you go about segmenting the data?
So, um, in this work which was led by,
uh, James Harrison and Apoorva Sharma, uh,
one thing they looked into is an approach called
Bayesian Online change point detection, um,
which assumes that you have some discrete task switches according to some probability.
Uh, so you might have data that looks like this where you have data
according to some process and then there's a discrete switch and then you see,
um, data from another distribution.
And then you maintain a belief over the duration of
your current task as well as
a posterior distribution for your model for each of those durations.
Uh, and then what you can do, um, so for example,
the duration will be something like this so you're kind of, it's being incremented by 1
until there's a discrete task switch, in which case the run-length will be back to 0.
And then what you can do with this
is when you have your belief over your task distribution you
can recursively update that belief using the performance of your model.
So if your perform- if your model's performing very well
that means that you're probably in the same segment,
whereas if your model is performing, um,
worse that it's more likely that you have switched to a different task.
Okay, so that's kind of a very brief and crude overview
of Bayesian Online Changepoint Detection.
One of the things that's neat about it is that
these recursive updates are completely differentiable.
Uh, and so what that means is that you can actually backprop through the update, um,
of your belief over the task duration to update your,
um, to kind of to train your model or or to meta-train your model.
So you can simultaneously learn these change points as well as the prior,
um, across these change points using meta-learning.
Um, and so, uh, the name that, uh,
that James and Apoorva came up for this is MOCA
or Meta-Learning with Online Changepoint Analysis, uh,
and what they found is that, uh,
the problem setting that they're considering is first during
meta-training you're given an unsegmented time series of data,
um, of offline data.
You want to be able to simultaneously learn how to recognize changepoints and
to learn priors to- for few shot learning within those segments of data.
And then at meta test-time you're given a streaming source of
online data and you want to
simultaneously learn from that data and make predictions from that data.
It's typical to the typical online learning problem setting.
All right, so what kind of an example of how it works is they first looked at a Sinusoid
regression setting where there is discrete shifts in this underlying sinusoid curve,
uh, and what this is showing is on the bottom is the belief over the,
uh, the current run length of the task segment.
And what the top is showing is that, uh,
the green is showing you the curves from the previous, um,
previous timesteps and the red is showing- well
the green and the red are showing that the points from the current- previous time steps,
um, and the red ones are the ones from the current task,
er, as labeled by an oracle.
And so we can see is that it can very rapidly recognize when there are Changepoints, uh,
based on its belief over the function and also very quickly adapt to that,
so even with just a couple of points it can figure out what the sinusoid
is corresponding to that,
um, corresponding to that data point.
Okay, um, and then they also developed a variant of Mini-ImageNet,
that is also kind of streaming, uh,
data where you don't know the task boundaries during meta training, uh,
and in comparison to,
um, this training on all the tasks,
uh, which, um, which is the kind of the black line on the bottom,
um, this is able to do significantly better.
So the x-axis here is showing the hazard rate
or how frequently the task and switching but basically,
the probability that the task will switch and the y-axis is showing accuracy.
Uh, and each of the approaches in red are basically using a sliding window
to basically meta-training across all of the data points within,
um, a fixed window size.
So they actually tried to predict where these Changepoints are,
it's able to do significantly better than just using this naive sliding window approach.
Okay. Cool. So this means that we can use data without
just kind of a time series of data without any task boundaries.
Now, can we take this one step further and just assume that we have, um,
not even time series data with temporal structure but just an
unlabeled data set of images, for example.
Can the algorithm come up with its own tasks in
a way that prepares it for future downstream tasks?
Um, so let's first look at the-
the unsupervised case where we have
images and then we'll look at the reinforcement learning case.
Um, so let's say that we have unlabeled images,
typically we assume that we have- we construct tasks with,
um, with labeled data.
Um, so the way that we're going to try to approach this problem,
is we're gonna try to construct the tasks,
uh, construct these kinds of tasks but with unlabeled data.
Um, and so to do that, uh,
we need to in some ways kind of group the data into
different images and then also label the images within those groupings.
Uh, so what we'll do is to group the data
we will- we can't group the data in pixel space.
We can't, like, run like K means in pixel space, that wouldn't work very well.
So what we'll do, is we'll first run unsupervised learning
to get a low dimensional embedding space.
Um, and that will allow us to take some images embed them
into some low dimensional space,
and then we'll propose tasks by clustering within this low dimensional latent space.
Uh, so for example one set of clusters might look like this,
another set of clusters might look like this,
so we can cluster the data multiple
times in order to create different groupings of the data.
And then to get a task we can sample these different groupings,
uh, and treat each of those groupings as a different class label.
So say, um, we wanted to create a, uh,
a two way classification task,
we can sample two classes maybe the red class and the blue class,
sample those two images uh, uh,
sample two images from those two groupings,
treat that as a training data set, uh,
sample two more images and treat that as the test set.
Uh, this would allow you to create a one shot,
two way classification task.
Uh, and you can repeat this process and for example sample the green cluster,
and the purple cluster to create more,
um, to create more tasks and so on.
Okay. Um, and so this gives us, uh,
a set of tasks and then once we have
that we can simply run our favorite meta learning algorithm.
Be it uh, uh, a black box approach,
an optimization based approach or a non parametric approach.
Okay. Um, and then the result of this process is, uh,
if you do something like MAML as a representation that's particularly well
suited for few shot learning. Was there a question?
Yes. So in this case they're a number of tasks are equal
to the number of clusters in the [inaudible] setting?
So in this case the total number of tasks corresponds to
the total number of clusters choose the number of ways.
So if you wanna, um, create five way classification tasks and you have,
um, 1,000 clusters, then you could create 1,000 choose five tasks.
And so in the case the number, uh,
sorry the ID of the tasks for the [inaudible] doesn't necessarily correspond to the
classification label how to deal with this uh, uh, [inaudible]?
So in this case the- like
each like cluster may not correspond to an actual semantic class label.
So for example, um,
we found clusters that correspond to pairs of objects or round objects for example,
um, and in general kind of the idea
behind this algorithm is that they don't actually need
unnecessarily correspond to exactly class labels.
What's important is that they-that they correspond to structured,
uh, structured aspects of the visual data,
such that the meta learner can figure out- can recog- learn to recognize
that structure very quickly from only a few examples and make predictions.
And so for example you could imagine, um,
basically practicing for few shot learning by practicing with
these other sorts of concepts like round and pairs of objects,
in a way that allows you to, um,
then at test time when you're given a very small data set,
be able to adapt very quickly to that data set.
So the down- after you do this, after you, uh,
provide meta learning you'll be given a downstream task a very small data
set for that downstream task and your goal is to quickly learn that task.
Uh, and that data will actually correspond to like ground truth,
um, ground truth semantic labels.
Okay. So, um, there's a few different design decisions in this approach,
um, for example we needed to choose an unsupervised learning algorithm,
um, there were a few options.
Two of the front runners that we used were BiGAN or, um,
or DeepCluster, uh, that we proposed tasks by- by clustering.
Um, and as with most machine learning papers
these days you need to come up with a fun, uh,
acronym for your methods and we call it clustering to automatically
construct tasks for unsupervised meta learning or CACTUs,
uh, and then we run our- run different meta algorithms on it.
Uh, and if you're curious what the combination of a CACTUs and MAML looks like you
get something like this and, uh, let's see how it does.
So, uh, we've looked at, um, Mini-ImageNet, uh,
and if you give Mini-ImageNet like full labels,
uh, with MAML as kind of a control.
Uh, it gets around 62% success.
Uh, this is with like a fully labeled meta training data set,
and then all the other methods will not use
any of the labels in the meta-training data set,
they just get to see the images.
They don't know anything about how those images correspond to classes.
Um, and then kind of the key question is
how does this sort of approach this sort of unsupervised meta learning approach,
compare to just using the representation of these approaches directly?
Um, so for example, we can take the BiGAN representation that's learned,
uh, and do k nearest neighbors in that representation with the downstream data.
Uh, and if we do that we get something around 31% test- meta test accuracy.
Uh, we can also do logistic regression on top of that representation,
and if we do that we get around 34% success.
Uh, we also tried using a neural network on top of
that representation and then using dropout to regularize it,
uh, and that actually did a little bit worse than using logistic regression.
Um, you can imagine an approach where you try to match the clusters,
we- if you try to basically want to use the clustering that you do here,
you could try to match the clusters, uh,
with that representation to the- the theta in your test task training set.
Uh, and if you do that you get around- also around 29% success.
Um, and in comparison if you do, uh,
if you do MAML on top of the-the clustered
BiGAN representation you get something like 51% accuracy.
So by explicitly training it for the ability to quickly adapt to tasks,
it's able to learn a prior that can effectively
transfer to downstream few-shot learning tasks.
Uh, and if you combine it with DeepCluster you do,
uh, a little bit better than that as well.
And one of the things that was interesting was that,
uh, this result didn't just hold for Mini-ImageNet, uh,
it also held for
four different embedding methods, four different unsupervised learning approaches,
uh, across four different data sets Omniglot,
CelebA, minilmageNet, and MNIST.
Um, two meta learning methods, MAML and
prototypical networks although ProtoNets under-performed in a couple of cases.
Uh, and it also worked well with test tasks that had larger data sets.
So for example, if you're in the 50 shot case rather than in the 5 shot case,
uh, this - this approach can also perform well. Yeah.
It seems as though like,
you know, [inaudible] the [inaudible] embedding space like the prototypes.
Would you [inaudible] use that to generate that [inaudible] .
Yeah, one thing that's interesting here is,
can you then kind of after you run meta-learning like
close the loop and use that to refine the tasks.
Um, we for something- we- we add
ma- we added prototypical networks as a somewhat last-minute addition to this paper.
Um, with MAML we found that if we re-use the MAML representation,
it actually wasn't good for proposing tasks which suggested to us that
one- basically these sort of embeddings are very good for proposing tasks,
and the meta-learning embeddings are very good for,
basically adapting to test tasks and
they- these two sorts of purposes aren't necessarily,
uh, like, uh, useful for the same sorts of things.
Um, for something like protonets, uh,
that's not an experiment that we ran but,
it can be pretty interesting to try.
And conceivably it does seem like,
you should be able to improve with the sort of iterative,
uh, iterative task proposal.
Okay. So now what about- what abou- what about,
um, unsupervised meta-reinforcement learning?
So this is a paper that, um,
are kind of an approach that Sergey very briefly touched on in his guest lecture.
I'll try to go on in a little bit more depth than that.
And what the general recipe will look like is very similar
to the- the previous approach which is,
we're going to be given an environment,
we want to be able to, uh,
propose tasks in an unsupervised way
and then run meta-reinforcement learning on those tasks.
Uh, and then the results of that will be, uh,
a reinforcement learning algorithm or reinforcement learning procedure that
is particularly tuned for a given environment.
Uh, and then for that environment if you're given a new reward function from a human,
you [NOISE] should be able to very quickly
maximize that reward function with a small amount of experience.
Uh, so then the key question is how do you go about proposing tasks?
Uh, one very, uh,
naive approach that you could imagine doing, uh,
but maybe still does something reasonable is to use Random Task Proposals.
Uh, so in particular what you could do is you can
just randomly initialize neural networks,
uh, that basically classify whether or not a state corresponds to a given task.
Uh, and so in particular what we'll do is we'll have, uh,
some discriminator that outputs a, uh,
logits over a discrete random variable, uh,
and if, uh, the probability of that corresponding
to task one is high then the reward function for that,
uh, task will also be high and likewise for- for other tasks.
So it's basically you'll try to classify which of
a discrete set of tasks each state belongs to.
Um, and- and in this case,
D is just gonna be a randomly initialized neural network
and one of the things you could imagine this
doing is essentially randomly partitioning the state space into different regions.
Such that one, one of these partitions corresponds to one task,
another partition corresponds to another task.
Okay. Uh, and one of the things that's really important to note here,
is this is random functions over the state-space not over the policy space.
Uh, and so, uh,
and the reason why this is important is that if there are
random functions over the policy space, uh,
then you'd kind of have an exponential search problem over
the policy that kind of visits, um, a set of states.
Whereas if it's a random reward function over a state-space,
uh, you instead just have a polynomial search problem.
Okay. Uh, so this is kind of the first naive thing that we could do,
uh, is there something that we can do that's a bit better than this?
Uh, and in particular one thing that we can imagine doing is trying to actually, uh,
make a set of, uh,
tasks that are more diverse from one another.
Uh, and in particular the way that we can do this is try to make
tasks that are discriminable from each other,
skills that look different from one another.
The way that you can go about doing this is actually very similar to the- approach, uh,
CACTUs that I mentioned before which is to try to actually cluster skills into,
uh, more discrete parts of your policy space.
Um, and now we can't, uh,
in the reinforcement learning setting when you actually generate the- the tasks,
uh, generate the kind of skills.
And so what we'll do, uh,
is we'll use an approach, uh,
called diversity is all you need or DIAYN.
Ah, this is an approach that Sergey covered in his guest lecture where you,
um, have an agent or policy that takes as input,
uh, a discrete skill
Z, it produces actions and you have a discriminator that takes as input, um,
a sequence a, a, a,
a state and tries to predict which skill was passed into the policy.
And then what you have is you have
a cooperative game where the policy tries to visit which states are
discriminable by the discriminator and
the discriminator tries to predict the skill from the state.
So the objective of both of these is to maximize the accuracy of the discriminator.
Uh, and then once you- once you have this discriminator, uh,
then you can have the reward function for unsupervised meta-learning to simply
be the likelihood of one of those skills given the state.
[NOISE] So here are some examples of the acquired skills that,
um, it acquires through this approach.
And so basically what this- what this approach
corresponds to is learning these skills with this sort of
discriminability metric or objective and then runs meta-learning across these skills.
With this- with each, with each of these skills as the meta-training tasks.
Okay. So does this work,
um, the short answer is ah, yes.
So if you run, if you run this algorithm and then give it
some downstream tasks specified by a human.
For example, navigating different 2D positions or running in different directions or uh,
running to certain points in space.
Uh, the kind of the- the result of the meta-learning algorithm is something that can
much more quickly solve these downstream test tasks then learning from scratch.
And one of the things that's particularly interesting
here is that even in training with
a random tasks which is shown in blue it's
able to do significantly better than training from scratch,
uh, in two of the three cases,
and also the, um,
if you use the diversity driven skills shown in green it's able to learn, uh, much, uh,
very quickly in all three cases.
Yeah.
[inaudible] Do you think you know,
how do you think they could be advised [inaudible]
Yeah, I think tha- that's a great question I'd love to see
more work on- on manipulation and in particular,
I think that exploration and manipulation is very challenging.
Uh, because in- in locomotion like
exploration is just like visiting different parts of the state space very clearly.
Whereas in something like manipulation, uh,
if you want to manipulate objects like just waving your hands around
randomly isn't actually a very good exploration strategy,
even though it is exploring a very large part of the state space for your arms.
And the things that do actually matter are
the objects and doing interesting things with the objects.
So I think that these kinds of approaches, um,
would struggle if like these kinds of exploration purchase,
if you apply them to manipulation settings may not do as well.
Uh, and we may need,
uh, basically different tactics for exploration in order to do that.
Uh, one of the things that I think is exciting
about meta-learning in the context of exploration is
it seems like you should be able to learn
exploration strategy that's particularly tuned to a domain.
So for example, if in locomotion versus in navigation versus
in manipulation it's probably
the different exploration strategies are actually better for different domains.
One is very object driven and the other is very kind of, uh, region driven.
Uh, it seems like you should be able to- instead of trying
to have a hand-coded algorithm that is good at all these different domains,
you should instead be able to learn an exploration approach
for that's particularly tailored towards each of these domains.
Okay. Um, so I guess for me the- one of the interesting things here is that it seems like
relatively simple mechanisms for proposing tasks work surprisingly well.
Um, but it- it's possible that these sorts of things may
not work well in- in- in a manipulation setting.
Uh, the other barrier I think for manipulation settings is that there aren't
a lot of good environments that are for manipulation,
that are kind of very well-grounded in the field and very well, um, widely used.
Okay. So um, some takeaways from this part of the lecture is that, uh,
you can learn priors for few-shot adaptation using a variety of
data sources including non-mutually exclusive tasks,
including unsegmented time-series data,
and using, um, unlabeled data and unlabeled experience.
And the way that we could do this is through meta-regularization, um,
with end-to-end change point detection and
using clustering of your skills or your data points.
Uh, and in general I think that all of these approaches should make it
significantly easier to deploy meta-learning algorithms.
Um, but at the same time, I don't think that these are
necessarily the final approaches for each of these problems.
Um, all of this work is- is extremely recent.
I think that there's there's really a lot more work to be done to
make it easy to- to deploy meta-learning algorithms.
Okay. Um, so now I want to talk about what does it
take to actually run these algorithms on very qualitatively distinct skills?
Uh, and the motivation here is that in, um,
in a lot of the kind of well-conditioned reinforcement learning,
uh, multi-task reinforcement learning and meta-reinforcement learning.
We see that the different tasks just correspond to
different variations of the same skill rather than completely distinct skills.
Um, and so if you think about whether or not things like MAML and
PEARL had actually accomplish our goal of making policy adaptation very fast.
Uh, I think that there's sort of accomplished this goal, uh,
to some degree and that we can kind of very quickly adapt to new situations,
but they haven't shown the ability to adapt to entirely new tasks,
Uh, and one of the challenges in doing this is that,
uh, if we want to adopt entirely new tasks, uh,
we also know that we need the kind of
the task distributions that meta-training meta-test time to be the same.
Um, and so what this means is that we need
a very broad distribution of tasks for meta-training.
Uh, so it seems like there's kind of two steps two things that we need to do here.
First is the ability to just meta-train over at
least like two task families rather than just within a single task family.
Uh, and the second step is to meta-train across a very diverse set of
tasks that allows for generalization to entirely new tasks.
Uh, so first I'd like to talk about the first point which is can we
perform meta-training across multiple task families?
Uh, and in particular what we'll consider is, uh,
the settings right here where we consider a space of
multiple manipulation tasks including things like grasping objects,
pressing buttons, sliding objects across the table,
and stacking two objects.
Uh, so I think it's,
it's pretty fair to say that these four things
are fairly distinct from one another and- and much more
distinct from one another than the task distributions that we saw earlier in this course.
And then our goal will be to learn- in this case,
our goal will not be the generalized for history an entirely new task.
It will simply just to be, to generalize within this broader task distribution.
Uh, and in particular, we're going, to learn a new variation of one of
these task families with a small number of trials
and sparse rewards for that task. Okay.
Now, one problem that comes up when you start to think about this problem is that,
if you have sparse rewards,
if you just have wa- like if the robot gets one
when it accomplishes the task and zero otherwise,
then the robot's gonna have to explore
every possible task to figure out which one is the one that it's supposed to be doing.
Um, so for example if you have like a robot right here,
you want it to be able to quickly learn a task,
you just want to be able to give it 1 when it did something good and 0 otherwise,
then we basically have to poke all the objects in the scene to
figure out which- which is the thing that you- it- you want it to accomplish.
And it seems a bit silly, I guess we're kind of hiding, we're
purposely hiding information from the robot,
uh, by not telling it what tasks we want it to do.
Um, so the way that we're gonna try to overcome this is,
in particular try to give it an indication of what the task is.
Uh, and- and then also given that indication how to figure out how to perform the task.
Uh, and in particular,
we'll see if we can learn from one demonstration which we'll use to convey the-
what the task is and a few
trials which will be used to figure out how to solve that task.
Okay. Um, and so kind of concretely what this might look like is that we're,
uh, it'll watch one demonstration as shown on the left.
It will then try the task itself in a new situation.
So in this case, it will try to push the- the teacup over to the right.
Um, in this case, this, uh, this wasn't quite the correct thing
to do and so it will learn from that- that a
reward of zero from that situation to figure out that
what it should have done in that situation is to push the cup towards the teacup.
Um, so in this case we're also introducing some amount of
ambiguity in the new situation since it has to
figure out if it should push the object on the left towards
the right or the object on the right towards the left.
Okay, um, then how can we train for this in
a scalable way if we want to learn from demonstrations and trials?
Uh, the way that we'll do this is,
we'll collect a few demonstrations for many different tasks.
Uh, we need this in order to learn from- from demonstrations.
We'll then t- first train
a one-shot imitation learning policy to be able to imitate the- the,
um, to imitate the- the demonstration, uh, for each task.
Uh, this is something that we can do completely
offline from the data that we collected, which is nice.
We'll then collect trials for each task
by running this one shot imitation learning policy,
uh, in the environment,
and we'll do this in a ver- in a batch off policy setting,
where we just collect a large batch of data once and then we're done collecting data.
Uh, this is pretty easy to do in- or easier to do in
real world settings so we don't have to be iteratively constantly collecting data.
Uh, then lastly, we'll train a policy that learns how to retry the task using,
uh, an imitation learning objective.
So what we'll do is, we'll give- I will train
kind of a meta-learning algorithm that
gets both the demonstration and one of the trials that it-
it collected in step three and train it such that
the resulting policy is able to perform the task,
uh, in a way that closely matches the demonstration in that situation.
Okay, so basically dtrain is gonna to correspond to a demo and,
uh, one or a couple of trials,
and then, uh, the outer objective will correspond to a new demonstration of that task.
Okay, um, so in our experiments,
one of the things we wanna look at is,
uh, this approach which we call watch, try,
learn, [NOISE] in the sense that it's watching a demonstration,
trying the task, and then,
uh, learn to do the task.
We also wanna compare this to meta- reinforcement learning that only uses trial data.
Uh, it isn't giving indication of the task through
demonstrations and this is gonna be very hard for the reasons that I mentioned before,
because you [NOISE] have to poke all the objects.
Um, and then also compare to meta-imitation learning that only uses demonstrations,
uh, and is only allowed- isn't allowed to kind of retry the task in a new situation.
Uh, and then lastly we'll also compare the approach that doesn't use any meta-learning,
that just behavior clones all the demonstrations.
Um, so quantitatively what we can see is that,
um, across the board, uh,
the approach that uses a- a trial and- a demonstration and
a trial is- is first able to learn decently well across the four task families.
Uh, it also significantly outperforms only using demonstrations which is shown in orange.
Um, and also- it also significantly outperforms using only trials.
Uh, in this case, using only trials performed, uh,
so poorly that we didn't actually plot it on this graph.
Um, so it's basically getting 0% performance,
um, because it was also a very challenging optimization problem.
Okay, um, and then we can look at some qualitative examples.
Uh, so this is similar to some of the qualitative examples we showed previously.
So here's the demonstration, uh,
then it gets one trial where it has to figure out how to solve all the tasks.
So it tried to- in this case it tried to push the object on the left towards the right.
Uh, and then that wasn't the correct thing and so it figured
out that it needed to push the object on the right towards the left.
Uh, and likewise for something like a grasping task, in this case,
it's given a demonstration,
um, which is to grasp one of the objects.
Uh, in this case, it- it wa- wasn't able to grasp on the first try,
uh, and then it figures out,
um, using that experience and it's ex- experiencing
in dynamics to figure out how to successfully grasp the cup.
Um, and then one last comparison we did is we took
the behavior cloning approach and fine-tuned that
with reinforcement learning on one of the test tasks
and we found that that required 900 trials,
um, of- of reinforcement learning in order to match
the performance of what WTL was able to get in just a single trial.
Um, so this is kind of emphasizing the- the efficiency that meta-learning can give you.
Okay, um, and one side note on memorization
is that one of the things that we specifically did in this problem
setting was to make it such that the demonstration only partially
specified the task and didn't kind of completely specify the task.
Uh, and one of the things that will happen is if the demonstration fully specifies the task,
then it will basically just resolve to a one-shot imitation learning
and will learn to ignore the trials during meta-learning.
Um, so this is basically sort of a variant
on the memorization problem that we mentioned before,
uh, and this might be- this is like resolving to one-shot imitation learning,
it is fine if you're good at the meta-test task.
So like if you're given a demonstration of a task, uh,
and you can like solve the task very well from that demonstration,
then you don't need trials and like life is good.
Uh, but if you're given a demonstration and then you can't quite solve the task,
you fail at the task,
then this is where memorization is a problem.
Because once you fail at the task,
that means that you don't have any mechanism to quickly adapt
from trials and then you may have to run reinflict,
fine tuning with reinforcement learning,
um, in order to actually learn a policy for the task.
Okay, so essentially this is just a variant on
the memorization problem where it's ignoring part of the training data, uh,
and then in principle, some of the algorithms that we mentioned
previously that actually regularize information flow,
could help solve this problem.
Okay, cool.
So, um, we talked about how we could train across task families.
Now, how will you go about generalizing to entirely new tasks, right?
Um, to do that we need a broad distribution of
tasks but we also need that broad distribution of tasks to be fairly dense.
We can't just take four tasks from
that task family and expect generalization to an entirely new task.
Um, so we need a broad and- and not sparse distribution of tasks.
Um, then the question comes is like- the question that comes up is,
where do you actually get that distribution of tasks?
Um, there's a few options for this,
uh, from kinda the reinforcement learning literature,
so there's things like OpenAI Gym,
uh, and like the Atari learning environment.
Both of these aren't very satisfying options
for meta-learning because there isn't necessarily
very clear underlying structure that you can
leverage to quickly learn new tasks from this distribution.
Um, there's also a- a suite of tasks called SURREAL or- or Robosuite.
Uh, and this suite of tasks has considerably more structure but it only has six,
I think around six tasks,
and so that also isn't something that we could use
for meta-learning to quickly learn new tasks.
And so we really want is,
um, many qualitatively distinct tasks.
So maybe 50 or more tasks, um,
and we want each of these tasks to have, uh,
reward functions and success metrics for- for meta-reinforcement learning.
Uh, we also- if we wanna be able to do meta-learning across this,
we want to be able to study transfer,
as we want all the tasks themselves to be individually solvable.
Um, and lastly, we want [NOISE] a unified state space and a unified action space,
and so motivated by this des- uh,
desiderata, we, uh, created a benchmark,
uh, that we call Meta-World that I think many of you probably are- are familiar with.
Uh, it was mentioned on, on Piazza,
uh, that has each of these traits.
So it has, uh, 50 distinct tasks,
shaped reward functions for each of the tasks, um,
and also a unified state space and unified action space,
such that we can hope, um,
hope to actually see some generalization to entirely new tasks shown on the right.
Okay, so, um, then the key question is how do current algorithms actually work,
uh, when you run them on this set of tasks?
So, uh, kind of the TL DR is that there are
some signs of life but there is significant room for improvement.
Uh, and in particular, uh, if you look at
the performance of meta-learning algorithms like MAML, RL squared,
and PEARL, uh, they get, uh,
meta-task performances in the 20s or- or low 30s.
Um, so this is a bit, uh,
perhaps disappointing but also, uh,
exciting because it means that there's a lot of room for
improvement for future algorithms.
Um, so then the question is why- why are they performing so poorly?
Uh, and if you dig into the details,
it turns out that the reason why they're
performing so poorly isn't because they're generalizing
poorly but because they're actually having
trouble doing well on the meta-training distribution.
Um, so they're performing poorly even on the 45-minute training tasks.
Uh, and so then you- okay,
the kind of natural thing to think is,
well, can we just do multi-task RL on the 45 tasks?
Uh, and if you then run multi-task RL algorithms on these 45 tasks,
they don't need to generalize to entirely new tasks,
they just need to be able to kind of do well on the training tasks.
Uh, these- these algorithms also struggled to do well on the 45 or in this case,
on the 50, uh,
tasks in the benchmark.
Okay. So what's the problem here?
Uh, we- we, developed these algorithms.
Um, and it seems like there is considerable structure among these tasks, right?
Um, so why the poor results?
So, um, you might think,
well, maybe it's an exploration challenge.
Uh, maybe it's having trouble exploring in
a way that allows it to figure out how to solve these tasks.
Um, but all the tasks are individually solvable with reinforcing learning algorithms.
So it isn't an exploration problem.
We know that, uh, if you try to solve the task individually,
you can- everything works just fine.
Then you might ask, well, maybe it's a data scarcity problem.
Like maybe we just aren't given enough data,
um, to perform these tasks well.
Um, but all of them are- are in the- in
these comparisons all the methods were given plenty of data,
plenty of samples within their budget.
They were also given plenty of model capacity,
uh, so that isn't the- the challenge.
Uh, and kind of the confusing thing here is that it
turns out that training models independently, uh,
from one another actually performed significantly better
than training a single model across all of the tasks.
Uh, so kind of going back to
some of the things that we talked about at the very beginning of the course,
it seems like the algorithms are having trouble,
um, kind of sharing information and sharing weights.
Okay. So our conclusion from this was that,
uh, it must be an optimization challenge.
So it's not a model capacity,
it's not a data problem, it's not an exploration challenge.
It seems like it must be an optimization challenge that's- that's
preventing these algorithms from solving the training tasks.
Um, so [NOISE] how would we go about solving this optimization challenge?
Um, we had a couple of hypotheses for what the underlying optimization problem might be.
Uh, the first hypothesis is that it
seems like maybe gradients from different tasks are conflicting with one another.
Uh, and this- this seemed like
a reasonable hypothesis because if they weren't conflicting with
each other then it seems like the optimization process would be fine.
Um, and so if there is conflict then we would see negative inner product,
um, between different gradients.
So we'd see gradients that are kind of pointing in- in opposite directions.
Uh, and our second hypothesis which is important is that when they do conflict,
they cause more damage than expected.
Uh, and the reason why this hypothesis is important is that,
if the- if the gradients are pointing in opposite directions,
um, just averaging the gradients should be the right thing to do.
Uh, and that's what standard multi-task reinforcement learning would do.
It's just to simply average the two gradients, uh,
and then the one that is higher in magnitude is the one that
will kind of dominate more, right?
Um, so it seems like averaging isn't the right thing to do, uh,
and we need to do something that tries to mitigate, um,
some of the damage that is being caused by these gradients pointing opposite directions.
Um, and in particular, our hypothesis here currently is
that the damage is being caused by high curvature.
Um, so for example,
imagine this is 1D example where you have
a one-dimensional parameter vector and a loss function that's shown on the y-axis.
Uh, if you have high positive curvature,
um, then you have,
uh, maybe a function that looks somewhat like this.
Uh, if you take one point on its curve,
maybe this is where you're currently at.
Uh, you could also measure the gradient of this function.
Now, um, this is the- the loss function corresponding to one of your tasks, uh,
and maybe it's the case that another task has
a larger gradient and it wants to move right on- right,
uh, in the right direction on the x-axis.
And so then what will happen is that if you want to move
right according to this gradient,
um, if you move right and take a certain step size, you would end up here.
Uh, but in reality,
you don't actually end up there, you end up up here.
Uh, and so even though your gradient was- had
a relatively low magnitude and said that if you go right,
your loss function wouldn't get that much worse.
In reality, because of the positive curvature,
you actually ended up in a much worse place in terms of
your loss function then your gradient suggested. Yeah.
Can't also a smaller learning rate fix that?
Um, so principally, yeah,
a smaller learning rate could certainly fix that, uh,
with the caveat that- then learning would be a bit slower. Yeah.
Sorry, just a quick question, first of all,
I would also like- how is MAML coming to this, uh, because, you know,
usually this is [inaudible] so how does that work with MAML
externally and I guess the other question is when you say
gradients in the domain of- of policy learning,
what gradients were you specifically referring to?
Yeah. So for the first question,
right now we're just looking at multi-task learning.
We're not looking at MAML at all,
um, because we just wanna solve the training tasks.
Uh, and so yeah,
so that's- although at the same time,
we're- we think the hypothesis about curvature but the solution won't end up
involving a curvature or won't end up involving any second order information.
And so that shouldn't in theory play nicely with MAML.
Um, for your- what was your second question again?
So when you talk about gradients [inaudible].
Yeah. Yeah, so in this case the gradient, um,
for an algorithm like SAC would be the gradient of your q function, of your critic,
and the gradient of your actor,
and we would be talking about the gradients of both of them.
It may be that one of them suffers from this challenge more so than the other.
Um, but in practice,
uh, when we look at this, we're looking at both of them.
Yeah. Okay. Um, oh,
and then with regard to the smaller step sizes again, um,
I- I kind of blew this up so that we can see it for visualization purposes.
Um, in practice, even with smaller step sizes,
you would also have this problem if you have very high curvature.
Um, so in- in practice you would never take it a step size probably that large but yeah.
Okay. Cool. So, um,
given these two hypotheses,
our approach to the problem was,
when taking the gradient step for one task,
try to avoid making the other task worse.
Uh, and this is motivated by the fact that maybe if we make the other task worse,
it will get a lot worse or more worse than we expected.
Uh, and so in particular the way that we could do this is very simple.
Um, our algorithm will simply be that if two gradients conflict with one another,
essentially if they- if they have negative inner product then
we'll project each of the gradients onto the normal plane of the other gradient.
And so in particular, we'll project
the gradient of one of the tasks onto the normal plane of the
other and will project the- the gradient of
the red task onto the normal plane of the blue task.
Uh, and as a result,
when you project this, um, this gradient,
this is essentially in some ways solving
a constrained optimization problem that's trying to change the gradient such that it
doesn't actually affect the- the loss function of
the other task doesn't actually get any worse according to your gradient.
It may actually get worse, uh,
if- if there's certain curvature, uh,
but it should get less worse according to
your gradient than if you were to take the original one.
Um, and if they don't conflict then don't do anything.
You can just leave them alone and they should interact in a positive way. Yeah.
Is that asymmetrical like if you were to project GJ on a GI normal plane
versus GI's on the the GJ's, does it vary?
Um, yeah. So if you only do one of these it is asymmetric.
And so what we do is we do both of them to make it symmetric.
Um, so we call this projecting conflicting gradients, uh,
or PC grad because that's what we're doing,
we're projecting the conflicting gradients.
Okay. Um, so how does this work?
Uh, so if you run multi-task RL on Meta-World on
both 10 tasks in Meta-World and 50 tasks in Meta-World, what we see is that,
um, this approach showed in purple is able to learn, um,
significantly faster and significantly
better than if you were to train independent networks.
And also significantly better and significantly faster than if
you try to do weight sharing without this, um,
without this sort of optimization trick to
make the- the landscape- or to make the optimization work,
um, work in a more clean way.
Okay. Um, so this helps a lot on- on reinforcement learning, uh,
and it's hopefully a step towards actually making
meta learning algorithms work well on this benchmark,
but that's not a step that we've done yet.
Um, and then another question is, well,
this helps for RL, but does it help for supervised learning?
Uh, so we also ran it on multitask supervised learning,
um, benchmarks that we've seen in the literature.
And we could also see that, uh,
in comparison to other approaches.
It is able to outperform a number of previous approaches and also, uh,
combines well with previous architectural solutions to, uh, to multitask learning.
So routing networks, for example,
is one architectural solution to multitask RL.
Uh, and if you combine that approach with the optimization, um,
solution of PCGrad,
it can perform, uh,
significantly better than all of the approaches.
Um, and then we also compare this on, uh,
the NYUv2 dataset which has three tasks predicting segmentation,
predicting depth, and predicting service normals.
Uh, and we're also able to see a significant improvement from,
um, from using PCGrad here as well.
Um, so kind of the takeaway here is that it also helps, uh,
multitask supervised learning compared to
a number of different multitask architectures that you could use.
Okay. Um, and then lastly one question you might ask is,
why does it work so well?
Um, and here, we wanted to kind of
confirm some of the intuitions that I mentioned earlier.
Uh, and in particular one of the things we tried is that, um,
when you project onto the, the normal plane,
you're changing both the magnitude of the gradient and the direction of the gradient.
And so what we tried doing it was an ablation where we
changed only the magnitude or changed only the direction of the gradient,
and left the other component of the gradient unchanged,
uh, compared to just averaging across the gradients.
And what we found here is that the,
um, first, both components were important,
but the most important component was changing the direction
of the gradient and not the magnitude of the gradient.
Uh, and to us, this provides some evidence of the kind of the effect of, um,
the kind of the- the intuition that I described previously which is where we,
where we don't want to affect the gradient of
the other task by changing the direction of the gradient.
Okay. So some takeaways,
um, first is scaling to broad task distributions is hard.
Uh, they can't be taken for granted from the algorithms that we have currently.
And, um, to do- uh, to kind of try to do this,
we can try to convey task information beyond the reward function,
for example, using a demonstration.
Um, we could try and train on very broad and dense task distributions such as Meta-World.
Although, there- this is kind of an initial approach towards a good benchmark and there,
definitely, is lots of room for improvement there.
Um, and also trying to avoid conflicting gradients
by projecting gradients when they conflict.
Okay. Um, so that's the last part about,
uh, kind of, very recent research.
Uh, now, I want to talk about some of
the more open challenges in multitask learning and meta learning.
Uh, and it's worth mentioning that I think
we've covered a large number of challenges and,
and downsides of approaches throughout this course.
And so I'm going to try to, just,
just discuss some of the things that we haven't previously covered in the course yet.
Um, so the first class of
challenges in my mind is just trying to address the fundamental assumptions
that we make when we set up the problem
of multitask learning or the problem of meta, uh, meta learning.
Um, in the first assumption is this assumption that the meta training,
the meta testing task distribution must be the same.
Uh, and this is, I think,
really important because there could be many situations where we have
out-of-distribution tasks or very long-tailed task distributions.
Um, so we may have a distribution that looks
like this where we have a lot of data in, kind of,
the head of the data distribution and small amounts of
data in a wide range of situations such as different objects encountered,
different interactions with people,
different words heard, different driving situations, [NOISE] etc.
Um, and in principle,
we know how to do few-shot learning, uh,
so we should be able to adapt to small amounts of data in the tail.
But the catch is that,
these few-shot tasks on the right are from
a different distribution than the ones where most of our data is.
Uh, and so in order to solve this problem,
we need to be thinking about settings or algorithms that can handle assumptions where
the meta training task distribution and
the meta testing task distribution aren't the same.
And some hints I think,
towards this problem might come from
the domain adaptation literature or the robustness literature where we're trying to be,
uh, do well on basically, uh,
on data points that are kind of,
from the tails of our distribution or from- or from a different distribution.
Um, a second challenge is multimodality.
So uh, a lot of the- a lot of
the motivation for meta learning is a leveraged previous experience.
Uh, the previous experience came in the form of
a set of tasks or kind of a set of datasets.
Uh, but in practice, in the real world,
we may have much more rich sources or previous experiences,
ranging from image data to,
to touch feedback, to language, to social cues.
Uh, and so I think it's important to think about how we
might learn priors across multiple data modalities that
can be compiled into some single prior about
the world or some kind of single source of common sense knowledge about the world.
Uh, and some of the challenges that come up here is that you
have varying dimensionalities of these,
of these, um, of these modalities,
different units of these modalities.
Uh, and they also carry different forms in different- uh,
actually complementary forms of information.
And some hints at this problem might come from the multimodal learning literature.
For example, but I think that this is really, uh,
an unsolved or really, um, unexplored area.
[NOISE] Um, and then,
the last question is, uh,
when should we actually use multitask learning or meta learning?
When will it help? Uh, when,
when will it give us benefits?
Uh, and this is, in many ways,
a problem with algorithm selection or model selection.
Okay. So that's telling you about the problem assumptions.
Uh, another challenge, ah,
that we've started to get at a little bit, uh,
but not really in a satisfying way is, is better benchmarks.
Uh, and in particular, I think we need, uh,
kind of- in this class,
you explo- you explored, um, Omniglot.
We also talked a lot about Mini-ImageNet,
um, and we talked about Meta-World today.
Um, I think that ultimately,
we need benchmarks that both reflect breadth of, of,
kind of the types of distributions that we wanna cover so that we can
generalize broadly as well as realism.
We want them to reflect the kinds of real-world problems that we want to solve.
Um, and so we have some steps towards
good benchmarks that we saw in this course such as Meta-Dataset,
uh, today, we saw Meta-World.
Um, there's also a new benchmark called the visual task adaptation benchmark.
Uh, and there's also the tasko- Taskonomy Dataset by Zamir et al.
Um, and I think that these are, are really, kind of,
important steps towards better,
uh, benchmarks for actually studying your algorithms.
Um, but ultimately, we need benchmarks that can really reflect the real-world problems.
Uh, it can have the appropriate level of difficulty for where we're at right now,
and are also very easy to use.
And I think that many of these days that don't capture,
kind of, all of these, uh, different properties.
Uh, and the reason why benchmarks are important is that without benchmarks,
we can't necessarily make any progress on the algorithm side of things.
We can't make progress on solving the problems.
[NOISE] Okay.
Um, and then lastly it's simply just improving core algorithms,
and this is something that we talked a lot about in the course.
So I'll just talk about it briefly here.
Um, things like handling computation and memory requirements, uh,
and making large-scale bi-level optimization actually practical to run,
um, on, on the computers that we have today.
Uh, trying to develop more of
a theoretical understanding of the performance of these algorithms.
So we talked a little bit about, um,
theoretical measures of ex- expressive power, um,
and different assumptions that our, our algorithms make,
but I think that there's a lot of work to be done to
under- better understand what these approaches are doing.
Um, and lastly, uh,
one thing that we didn't cover that much in this course is looking at problems.
Looking at multitask problems where you want to perform tasks in sequence.
Uh, and this sort of sequential decision-making problem,
I think is particularly challenging because one task, uh,
the starting distribution of one task will depend
on where the policy for the previous task takes you.
And actually along- a number of these directions,
there's been a number of final projects that
have tried to look at some of these challenges.
So in addition to these, uh,
there's also, kind of, the challenges that you hopefully, uh,
discovered in your homework and final projects. Um, great.
And so lastly, I'd like to talk, uh,
just very briefly about the bigger picture and,
and some of the motivation for this course.
So uh, one of the things that we talked about at the very beginning
is kind of how- if you look at a lot of existing machine learning systems in many ways,
they're, kind of, very specialized towards particular applications.
Uh, they can perform one very narrow thing in one,
kind of, narrow environment.
Uh, and hopefully, if we care
about building machines that can go into real-world environments,
then they can look, uh,
more like the, um- they can exhibit
more of the generality and flexibility that humans have.
And so in this course, uh, I think,
we covered some of the steps towards these sort of
generalist machine learning systems, uh,
such as systems that can learn multiple tasks,
that can leverage previous experience when learning new knowledge.
They can learn general-purpose, um,
models of the world, uh,
they can prepare for tasks before you know what they are, uh,
such as exploration, can perform tasks in sequence,
uh, and can learn continuously in like lifelong learning settings.
Uh, now, what's missing,
uh, towards systems that are, that are kind of generalizable.
Uh, I think that now you're equipped with
the tools of this task and that this is something that you are,
um, well-prepared to figure out.
Great. Um, so a couple more reminders,
uh, the poster session is tomorrow.
The final project report,
please submit it before the 16th.
Uh, and please fill out your course evaluations.
We love to hear, um,
any feedback that you have,
either in the course evaluations, or over e-mail,
or in person. Thank you.
 A few reminders for- just for the course in general.
So I'll be sending out the project guidelines very soon,
as well as different suggestions for
projects if you're still looking for different project topics.
Uh, a reminder that the homework is due- Homework 1 is due next Wednesday,
and then Homework 2 will be coming out on the same day.
Uh, for the presentations today we'll have around 20 minutes for each presentation, uh,
and people can ask questions and discuss the papers throughout the presentations, uh,
just like during lecture and we'll, uh,
we'll try to sharply cut off the presentations,
uh, at even increments so that we can get on to the next one.
Uh, also for those of you that are presenting,
pre- if, uh, if there's an audience question,
please remember to repeat the question.
Uh, this isn't necessarily for the purpose of the room,
but for people watching the recording,
then they'll be able to hear what the question was.
Um, either repeat the question or answer the question the way that, uh,
that the question is apparent from your answer.
Um, yeah.
So those are all the, uh, the reminders and information,
and we'll start with the first presentation
in a couple of minutes once we figure out the, uh, the Internet.
Yeah. Hi all.
So we're just starting the paper review session.
So the first one is meta-learning for a low resource neural machine translation.
It's by.
So over to you.
Sweet, thank you. [APPLAUSE] [NOISE] Hi, everyone.
Please use the microphone.
Okay, awesome. Hello.
[BACKGROUND] Hi, I'm Sabri.
Hi. I'm Ryan.
I'm Manurith.
Cool. We're going to be talking about meta-learning for
low resource neural machine translation.
Awesome. So to start off, um,
to give like a brief introduction on the history of like, um, NMT progress.
So historically, like neural machine translation, um,
was first based off like statistical methods for language translation,
um, and then over the last few years,
as deep learning has gotten a lot better, uh,
models have moved towards, um,
NMT basically using deep learning for NLP type tasks,
and those have outperformed statistical models on the vast majority of,
uh, like trans- multilingual translation pairs.
Uh, and then there's the issue that still exists that for low-resource language pairs,
uh, statistical models are still outperforming because deep learning is very data hungry.
So basically,
neural- neural machine translation first
started off as just targeting like large monolingual,
uh, corpora, so like,
targeting just like English to German or like,
just like English to French or something of that nature.
Um, slowly over time,
different papers came out basically treating,
uh, multiple languages at the same time, uh,
and the context of a single task learning prac- uh, learning, uh,
procedure, and then after that they applied like direct transfer learning methods.
So initially, like training a model on like English to German,
and then fine tuning on like
a different dataset for like English to French or something of that nature.
So the idea here was that we could use neural machine [NOISE]
translation methods and like meta-learning
for optimizing fine-tuning on low resource pairs,
so that classification learning would go much faster and much more efficient.
So the way the authors first set this up, uh,
as a meta-learning problem,
is that first they have to define the tasks.
Um, so they divided into two sets.
One is the high-resource language translation tasks
and then the low-resource language translation tasks.
So each task is essentially a translation between two languages,
whether it's like Spanish to English,
or French to English, or in the low-resource case, Turkish to English.
So the idea is during meta-training phase,
you sample your tasks from these high-resource language translation tasks, uh,
and then you train on these and during meta-test time,
you fine-tune on these lower resource,
uh, language trans- translation tasks.
Now, um, it's important to note that
these low-resource languages are lower resource than the high-resource languages,
but they're still not true low-resource languages.
So the authors had to sub-sample in order to mimic a- a low-resource setting.
So typically, um, if you are doing meta-training,
your- during the gradient updates,
would first occur, uh,
on your training dataset for your task, right?
So you're gonna update your parameters theta to theta prime using your training dataset,
and then you compute, uh,
the predictions on the test dataset and then you take the derivative of the loss
with respect to the original parameters theta to update the original theta.
But what this involves is a second derivative,
and because these guys are using transformers for NMT,
transformers are larger models,
um, so this is going to be computationally very expensive.
So instead, what they decided to do is a first-order approximation.
So instead of taking the derivative of the gradient of the loss with respect to theta,
they take the gradient of the loss with respect to theta prime,
and- but they still do the update on theta.
Now, the authors show that these are roughly equivalent,
and even in Dr. Finn's MAML paper,
she talks about how, um,
they are roughly equivalent, uh,
and the intuition behind this is that the ReLU neural networks locally are linear,
so the second order derivatives tend to be 0 in many cases.
Uh, so that most of the gains actually happened from these post update parameters,
which are the theta primes.
[BACKGROUND].
Cool. Okay, so we've got this cool idea.
We're gonna be using meta-learning for
neural machine translations that we get these low-resource languages,
in this case like Turkish and Finnish,
and we'll be able to kinda perform better by first
meta-learning on a bunch of high resource languages.
So just kinda to recap the- the setting is,
we're going meta-train on some language, right?
So for example, here we have a Spanish sentence,
we're gonna be translating that into English, right?
Um, and then we're going to also be then meta-testing
on a low-resource language like Turkish in this case.
Um, and kinda one of the main issues or kind of the central issue that the- the author has
kinda tried to overcome with applying meta-learning to this particular problem,
is that the, uh,
the meta-train and the meta-test input spaces,
aren't aligned or they are not in the same space, right?
And so that is to say,
if we have a sentence like Spanish, in Spanish, right,
and then we wanna take the word nombre and look up its Spanish word embedding and
that Spanish word embedding is gonna be trained on some monolingual corpora in Spanish.
If we look up that embedding, we'll get some embedding for nombre,
but then if we do the same thing in
another sentence in a low-resource language like Turkish,
we look up adim, right?
That means name roughly in Turkish, right?
But when we look it up, right,
we're gonna get another embedding and there's gonna be
no correspondence between those two different word embeddings, right?
And so we want the embeddings,
you know, in the meta-train, uh,
languages and the embeddings and the meta-test languages to be sort of in
the same space so that we can really get the most out of meta-training.
Cool. And so the idea that the- that the authors had to
kind of address this issue was what they call a universal lexical representation,
and the idea is that they're going to have sort of one set of word embeddings
that they're going to share across all of these different languages,
both the meta-train high-resource languages and the,
um, low-resource languages at meta-test time.
Okay, so the way that they kinda
go about constructing this universal lexical representation,
is they first train word embeddings independently for all the different languages.
So all of the low-resource languages and all of the high-resource languages,
they get a set of word embeddings and they can do this using
monolingual corpora and then like Word2vec or whatever they wanna use.
Cool. So we get word embeddings for all these different languages.
And what they're gonna do is they want to create this set of universal embeddings that
all the languages they're gonna share and so what they do
actually is they just take the English embeddings, right?
But then they also had this set of what they call universal embedding keys,
and the idea is that these are going to be like lookups to look
up values in the universal embedding values, right?
So if a word embedding is close to one of the keys,
that means that the corresponding value in
the universal embedding values probably like has a lot of weight.
So getting pointed that to put this in my- in my shirt.
Okay, cool, awesome.
So how do they actually go about using this?
So going back to this example with the word nombre,
we take the word nombre, right?
We look up the Spanish word embedding, just like before,
except now what we're gonna do is we're gonna pass
that Spanish word embedding through a linear transformation,
and then we're going to compare it to all of those different keys in
the universal word embeddings keys, right?
And what that's going to produce is going to produce this vector, right?
That gives us like a match value,
like or how well the Spanish word embedding match with
all the different keys and we're going to call that vector alpha, right?
So each of the keys now has this one scalar value telling us how much
it matched with the word embedding in Spanish.
Then what we can do is we can multiply that alpha by the universal embedding values.
Remember, those are the universal embeddings that we're
sharing across all the different languages,
and that's equivalent to doing sort of like
a convex combination or like a linear combination of
all of those different embeddings in the universal embedding values.
Cool. So the idea is that the words,
the word nombre in Spanish,
is going to now be a combination,
some weighted sum of all of
the different embeddings in the universal lexical representation,
and the cool thing about this is now we can do the same thing with adim, right?
But the advantage is that,
so while adim kinda means name in Turkish,
in Turkish we decline nouns and so that means that adim kinda means more
like my name and so maybe you could imagine that when we do this linear combination,
we're going to get a little bit more wor- weight to
also the word like I or the word my, right?
So this is kind of this flexible way of having this one universal representation,
and then at train time, they hold they-
they freeze the weights for the Spanish word embeddings.
They also freeze those keys which are just English word embeddings,
and then they allow you to train
those universal word- the universal word embeddings and they
also train that transformation matrix, um,
going from Spanish to the universal embedding keys.
Cool.
So one of the first experiments they ran was to show,
um, how does this perform versus a multi-task setting?
So in both the meta-learning case and the multitask case, they first, uh,
trained their model on all the high resource tasks,
um, and then they fine tune on the low-resource tasks.
Um, so what we- the first thing that we do see is that um, the meta- the
meta-learned ah, model outperforms the equivalent multitask model,
uh, across all categories.
So across all of the,
uh, low resource language tasks.
And now it's interesting note in their diagrams the way they split this out.
So they have all, uh,
embedding plus encoding and then just embedding.
So what's going on here is that they're not- in the all case,
they're fine tuning all the parameters while in embedding plus encoding,
uh, the encoder they're just fine tuning
that portion and then they also just fine tune the embedding.
So, uh, over here what's happening is that most the gains happen
when you fine-tune both the embedding plus the encoder as opposed to,
uh, fine-tuning the entire model.
And this is interesting to see because, um,
the decoder typically in an NMT model,
ah, you're going from one language to another language.
Here all the tasks are translating to English.
So the decoder is taking
the encoded representation of some language and then translating that to English.
So it's better not to fine tune
that portion because that's not- that shouldn't- that part shouldn't change.
It's mainly the encoder and then the embeddings.
Cool. Okay. I think I'm getting pointed at to use this.
Hello. I'm not sure.
Okay, uh, so- one of the- one of the- one other thing to mention here um also,
so they also- ah,
when you're doing meta-learning,
it can also be useful,
like, to have actually like a validation meta test task, right?
To determine when to- to early stop during your meta-training.
And so what they do is they actually just compare
different options of which meta lea- which of
these low-resource tasks that uses that validation meta-learning um, task.
And so they kinda showed that,
that choice is actually significant and that the performance varies depending on which,
uh, which validation and task you choose.
Yeah, and then the next experiment they do is they try and vary the amount of
training data they have in order to compare
between the multitask and the meta-setting- meta-learning setting.
Um, and as the hypothesis, um, suggests, um,
when you have lesser training examples,
like in the zero shots setting or a few shots setting,
um, the multitask models or the meta-learning model is gonna do a lot better.
Um, but as you keep increasing the size of your, uh, training examples, um,
slowly the gap between the multitask and the meta-learning model ah, the gap reduces.
So both perform equ- almost equivalent.
Okay, cool. And so then here they also kinda asked this question
of how does our performance vary after fine tuning,
uh, depending on sort of which languages we used during that meta-learning phase?
And the main takeaway of the idea was basically,
the more languages, the better, right?
So the more languages we used during meta-learning,
the better our performance after fine-tuning was,
but kind of an interesting study to do there.
The other kind of interesting takeaway from- from this ah,
from this table is that last row right there which is fully supervised.
So one thing that we mentioned at the beginning was that these
aren't real low-resource languages or I guess like Turkish for example,
is a lower resource language than French, maybe, right?
But it certainly isn't like the lowest resource language.
So compared to like real low-resource languages,
like Berber, or like Basque or something, right?
These are actually pretty high resource languages.
And so to simulate this setting of low-resource languages,
what they do is they sub-sample the tokens
in these low-resource languages like Turkish and Finnish, right?
And so when they actually do like fully supervised training on that full set,
they get blue scores that are way
higher than what they were getting with these sub-samplings,
which is to be expected but it also begs this question of like,
"Well, why didn't you also validate this method
on- on like real low-resource languages, right?"
Uh, I think it would have been be cool if they had,
for example, like, yes,
some case study for a really,
really low resource language-like Berber,
and then push the state of the art on
that particular language um, as opposed
to just like doing the simulated setting where they're,
uh, where they're kinda under-sampling
from- from these different medium resource languages.
Yeah, I mean, I think what Sabri is saying makes a lot of sense because, like,
if we were going to build a product to do language translations on low-resource tests, um,
at least for languages that they show,
it doesn't seem like the meta NMT model is what you would use,
um, because it's clearly not the best performing model.
Any questions or? Yeah.
[inaudible].
Got it. Okay, so I think the question was um,
how does the universe lexical representation get around the low resource problem?
And I think that the way that I see it's like
the universal lexical representation doesn't directly address the low-resource problem.
What it does is it creates the setting in which
all of these different languages that have totally different vocabularies,
often different orthography also, right?
What it allows you to do is, it allows you to, kind of,
embed all of these languages into a similar space,
which then makes meta learning more effective.
And the meta learning is what's really addressing the low-resource issue.
Sure. Well like- like are
the embeddings for the low resource languages, like, [inaudible]
Well, so the- the key thing here is that while
like for a low-resource language in neural machine translation,
we're usually talking about low resource in the sense that we
don't have a lot of parallel corpora, right?
So that is to say we don't have a lot of texts where we have like,
you know, Turkish texts and then also English text.
Um, but you might have like
a lot of modeling of corpora that you can train those word embeddings on.
So those word embeddings could actually be like kinda decent, I don't know.
I think that's kind of the idea here.
What makes you think that languages are linearly related?
So like, sorry, the- the idea that like oh,
like in the- in the- in this sense, sorry,
the question was, what makes me think that
that [LAUGHTER] languages are linearly related?
Um, I don't know if I think that languages are linearly related,
but I think that's kind of like a- a question about like sort of
this part of their- this part of their approach.
Um, yeah.
I mean, I think the idea is that basically like each one of
these rows in the universal lexical representation doesn't
represent like a specific word as much as it represents like some of,
like semantic notion, right?
And that different words could be combinations, like,
a linear combination of different semantic, like ideas.
Kind of like yeah, like- with like adim, it means like my name.
And so it's like, yeah- yeah also-
Um, yeah, so yeah,
we didn't mention this during the presentation,
but one of the critiques that we were thinking of while reading through this paper is,
this transformation matrix A,
is shared across all the different,
um, languages for the embedding keys.
And so there is an implicit assumption there that along these-
along the same sub spaces that these keys belong
to that they are undergoing the same transformation.
So that was like an implicit assumption that they could have
gone around by using different transformation matrices for the different keys look ups.
During the meta training, uh, with, uh,
the languages that have a lot of examples,
do they sub-sample those examples to sort of simulate,
um, these low-resource languages?
I remember- remember in like er, what we were
talking about last lecture, we would, you know,
simulate handing five pictures over to a
measuring algorithm and then teach it to then classify a small number.
So, like, is that this same idea being used here?
Yes. So yeah, um,
I- l think when we meant sub-sampling what that actually is during the fine-tuning phase.
So for example, like when they're comparing against the multitask model,
um, they don't fine tune on all the available data.
They only fine tune on a sub-sample of the data, uh,
in order to simulate the low-resource comparison.
Yeah. [NOISE]
[inaudible]
Sorry.
We need to switch laptops. So you guys, you can answer the question [OVERLAPPING]
Cool.
Sure, sure. Did you want to say something?
[inaudible].
Is a question of the meta test? [OVERLAPPING]
I still don't quite understand what
the test based
measures [inaudible]
So, yeah, if you're talking about
specifically during the- so both during meta train and meta test,
we're gonna be doing the language translation task.
Uh, so any of the embedding lookups that
happen during meta train will also happen during meta test.
Right.
Yeah. I think here maybe- I think maybe the-
We had one slide up there which was like we've kind of freeze, like froze some weights,
uh, maybe with that kind of gave the impression that
we were freezing them after meta-learning.
Actually what we're doing is we're freezing them for
both meta training and meta testing, right?
So the other weights were just like learn,
they were like learn in unsupervised way.
They were embeddings mostly.
And then we're learning all the other ones like in
both meta train and meta test and then getting the number, yeah.
Cool, cool, thank you.
[APPLAUSE]
Yeah. Ah, so the next talk is, uh,
on few-shot autoregressive density estimation and it's been presented by Rohan and Varun.
Hi everyone. I'm Rohan and this is Varun and we'll be
reviewing this paper on towards learning to learn distributions.
Uh, to get started,
uh, we wanted to talk a bit about the motivation.
Uh, so why would you want to learn distributions?
So generative models in general learn distributions and you could use it in many ways.
You could use it to enlarge your dataset,
maybe for some data augmentation.
You could- let say you have some missing pixels in that Obama image there.
You want to fill those pixels,
you could use a generative model for that,
or you have a specific application like
a capture thing and you want to generate certain images,
you might use this learned distribution for that.
And, um, I guess the main theme of this paper is that,
uh, we as humans are able to like,
uh, if you are given a few images,
we are able to generate a new image or
understand the latent structure of what is happening in that image.
Uh, and the hope,
like the idea in this paper is basically to do the same,
can you learn to learn these distributions in a few short setting?
So the learning setup would be this, uh,
a task would be defined as something like given a support set of few images,
can I generate a new image which looks similar to the support set?
And, uh, for that,
uh, like we will have some training tasks.
So for- in this case, you have an image of a bicycle or you have an image of,
uh, ima- a few images of cups.
Uh, and you have a test task where you are
given these few images of like a sewing machine.
And you- the hope is that you want to generate
a new image which looks something similar to the sewing machine.
Uh, and the authors in this paper rely on two major techniques,
one is the neural attention and one is meta-learning.
So, uh, we wanted to take a step back and talk a bit about
some prerequisites which will help us understand the models that the,
uh, authors talk about.
The first is the auto-regressive models.
These are types of generative models which
are basically use this assumption that there is
a sequential ordering when you factorize a joint distribution into a set of marginals.
And PixelCNN is, uh,
is an example of an auto-regressive model.
By, by that I mean that I will be generating these pixels in a sequential fashion.
So over here, if I am talking about generation of pixel xi,
I would use all the pixels that I have generated so far,
which are shown by the blue,
like all the blue boxes here.
And, uh, what would- I,
I have an architecture which is like, uh,
very big deep architecture of lots of
convolutions and some residual connections and so on.
But the idea is that I would do a feed-forward,
generate a new pixel,
and then do it all over again, and so on, so forth.
Uh, this is the third concept which we'll be using.
Uh, this is called- I mean, attention.
Most of you might be aware of this, but, uh,
basically there is, uh,
a query vector and a set of keys and values.
And in this case, you have a sequence to sequence architecture where you
are using the hidden states of the encoder, uh,
as keys and the hidden state of the decoder as values and you get,
uh, a weighted average as your attention vector.
Uh, so in order to solve this problem,
the baseline model that the authors talk about is called as conditional PixelCNN.
Now, what do you mean by- before going into conditional,
what do you mean by conditional PixelCNN?
Uh, the, the architecture that they use is called as
the gated PixelCNN instead of the normal PixelCNN. Why do they do that?
It's because, uh, PixelRNNs were shown to perform better on generation tasks.
And why that is the case is because they have
some multiplicative units which are modeling some complex interactions.
And the authors basically change the PixelCNN architecture to
a gated PixelCNN architecture where they're having these like,
uh, gating, uh, of mechanism by having that sigmoid which acts as a gate and the tanh,
which acts as an activation.
Uh, the conditioning here is basically,
let's say I'm given, uh,
some information about the image that I would like to generate,
maybe it's a description.
I can convert that into an encoding.
For example, something like h here,
and I condition on that.
So my generation is conditioned on that as an, uh, input.
In our case, uh, like going back to the our task which- where we had a set of
support images and we would like to
generate a new image which looks similar to the support set,
we could potentially say that the support set acts as a given information to me,
and I could encode it by this function,
f of s. Now,
f of s is some convolution function which does like some encoding.
And you apply the gated function and you condition based on these,
these set of equations.
So, uh, this acts as the baseline model for generation of images.
So, uh, given some support set,
I'll be able to use this model to generate an image.
Uh, but there is no some sort of like a concept of learning here, like, uh,
there is learning per task,
but then you are not- you won't be able to generalize that well.
So the authors proposed two models.
The first model is called the attention PixelCNN.
And, uh, the key idea here is that as in when I'm generating these pixels,
I want to focus on different aspects of my support set.
So my support set is let's say 10 images.
I want to- let say I am generating the 10th pixel,
I want to focus on different pixels from
the support set as compared to gene- like generation for the 11th pixel.
So how do they model that is via this CNN based architecture,
which is computing some keys and values,
and then they have an attention mechanism where
they do this transformation and learn their attention weights.
And there are two major important points to note here.
One is that the authors augmented
the support set with some additional features which represent positional data.
And the second is that you are learning these weights,
but you are not- like for a test task,
like a given new task of like, let's say 10 new images,
you will not learn the weights or update the weights,
you will use the weights that you have learned before,
so there is no gradient steps happening here.
Um, the next model that the authors proposed is called as the Meta PixelCNN,
uh, which is basically using the concept of, uh,
meta-learning in a slightly different fashion,
uh, where, uh, it- the framework remains the same,
but the key distinction is that the, the,
they had this notion in- in like meta-learning,
you have this notion of the outer loss and the inner loss.
The outer loss is modeled as the normal,
like negative log likelihood.
But the inner loss is something which is learned.
So, uh, the g function here represents the inner loss,
which is basically taking in the output of
the fixed PixelCNN and generating a scalar value,
and that scalar value acts as a loss.
Uh, and now I would let,
uh, Varun talk a bit about the results and the takeaways from the build.
Hey, everyone, uh, so yeah.
So some of the tasks, uh,
some in the experiment section,
they perform three different tasks.
In order of increasing difficulty, uh,
they first performed image inversion,
then character generation and then finally image generation.
The datasets that they used were ImageNet,
Omniglot, which is basically, uh,
50- characters from 50 different languages and Stanford online product dataset,
which basically for each product,
there's, uh, a variety of,
uh, images that correspond to it.
So some of the evaluation metrics that they used for,
uh, this paper were,
uh, qualitative and quantitative.
So qualitatively they would look at the output of
the model and determine if it looked good or not.
And quantitatively, they use nats per them.
So as a review, uh,
nats is a effectively a unit of
information or entropy based on natural logarithms in powers of
e. So one nat would correspond to the negative natural log of 1 over e. And,
uh, in this paper, um,
basically they looked at the net- the negative, uh,
log likelihood and, uh,
averaged it across all the pixels.
So, uh, for the first task,
they perf- performed one-shot image generation where the support set was this dog,
and basically, they try to invert the dog.
Um, as you can see,
the conditional PixelCNN and the Meta PixelCNN didn't perform
as well as their proposed attention, uh, PixelCNN.
And quantitatively, we can see the results reflect the same thing.
Um, one of the things that they noted from, uh,
the attention PixelCNNs performance was that
the attention had learned to effectively, uh,
move left to right while the output was writing- sorry,
moved to have learned right to left,
while the output was writing left to right,
which is a naive approach of inverting the image.
Um, the next, uh,
task that they did was a few shot character generation.
So if you look at the first two rows,
um, you can see that the support for,
for support images, uh,
they were able to generate characters using three different models.
Um, quali- qualitatively, we could see that the attention in Meta PixelCNN,
uh, outperformed PixelCNN.
These are some of the quantitative results.
Um, they tried it on different, uh,
support set sizes, um,
overall the attention PixelCNN outperformed the conditional PixelCNN
and even out- outperform the current state of the art- at the time.
Um, in addition, um,
they tried a different- a new model,
which they didn't really talk about in their,
um, in their, uh,
model section, which is the attention Meta PixelCNN,
which is, uh, unique and,
uh, but unfortunately, it didn't perform as well as just the attention of PixelCNN,
we'll discuss about it later.
Um, so in addition, they even perfo- uh,
they provided basically, um,
how the samples are generated and, uh, basically,
where the attention weights are, uh,
with respect to the support images as they're generating,
uh, each, uh, character.
Finally, the third test that they performed was a few shot image generation.
So given few images,
they wanted to understand latent- the latent structure of
the image and try to generate a similar product and substance.
So they compared with attention and without attention and the results,
they argued that the attention mechanism is better,
but it's questionable and they even- and if you look at it quantitatively,
it's like the difference is marginal.
They argued that maybe there needs to be
a better metric to do this sort of task or to measure this sort of task.
So some of the takeaways,
we will break it down into strengths and weaknesses.
The strengths, attention is great for flipping images,
especially in the one-shot generation case and this seems
to be one of the- this is one of the first papers that we've seen
that where -we can use- we can apply meta-learning.
Okay. So I'll just continue.
So the meta generative,
yeah, so the meta generative models.
Um, so basically some of- one of the,
another strength is basically
they can apply a meta-learning framework, [NOISE] awesome, er,
around generative models, um,
to generate unseen characters and then another cool thing was that they
were able to learn the inner loss function in the meta-learning framework.
[NOISE] Some of the weaknesses.
Um, basically, few shot image generation needs a new model.
Um, from just the result, it seems that, uh,
this approach probably doesn't work well and, um,
didn't provide an analysis of the inner loop gradient steps versus performance.
Um, it would've been nice if we had seen that.
Maybe it would have helped a meta-learning algorithm, er, perform better.
Um, and then finally, the -I'm sorry,
they added -they had some naive combination of meta-learning and attention.
Um, this is also a strength in some sense because this has never been done before,
um, but it would have been nice if they explored this idea further.
Maybe there's some- there's a model out there that could perform incredibly well.
And finally they had inconsistent experiments,
specifically with the meta-learning and attention-based model.
Um, they only applied it to one task.
It would've been nice to see it on all three tasks.
And, uh, with that, um,
I'd like to open the floor for discussion, um,
and any questions, if you have any questions,
please feel free to talk about them.
But, um, just to start,
I guess some discourse.
Uh, yeah, sure.
[BACKGROUND].
[inaudible]
Right, that's a very good point and like, uh, I mean,
we were also thinking about there's no- so, uh,
the question was that, uh, this loss that is learned,
is not, uh, constrained in any manner.
So it could just be learning to be emitting out a 0 but, uh,
uh, I mean, we were also thinking about the same and there's one of the points here,
the last point here about, uh,
whether this loss function should be constrained in some fashion, uh,
but we asked the authors and we'll talk about that a
bit later and it turns out that, uh, I mean,
the loss- the- the- the- the network G in itself is differentiable
and also you are minimizing over
that and because you have some bias terms and stuff like that,
you could prevent it to be 0.
So you- it, it might be possible to constrain it to be 0.
Yeah.
So, uh, but if,
if the inner loop actually become 0 then there is no update in the build.
The parameters don't get updated in the inner loop.
Yeah.
So that would mean that in the outer loop also you have
very high likelihood which will lead to high loss.
Right. But But then if you have some bias terms
and those bias terms are not 0 in the network,
[OVERLAPPING] then you wouldn't have a zero loss.
Yeah. So yeah, so basically that's the down, uh,
that's the disadvantage of having of learning a zero loss because
it would prevent your parameters from getting
updated in the inner loop [OVERLAPPING] which leads to a-
Right. Yeah, yeah, that is another point.
Yeah, I mean, the outer loop will not see
any updates and that's why I like, yeah [OVERLAPPING].
So that would force the- [OVERLAPPING]
Inner- inner loss not to be 0.
Not to be 0.
Right, yeah, that could be another intuition.
Yeah. We hadn't thought about it from that perspective.
It's a good point. Thank you.
Uh, any other questions? Yeah.
So in element sharing, it has generating a completely new image, uh,
what's preventing this network from just copying the example like this one?
Right, but, uh, I mean,
you could potentially- You wanna take that?
He is asking you. [BACKGROUND].
So the question was, uh,
what prevents the network from some just
copying this net- the images from the support set.
Um, because you have this notion of like performance across tasks,
uh, you wouldn't like get a,
a very good performance.
Or like the loss function would not go down if you are just copying.
I mean, copying for example,
uh, if we go back to the, uh, site of,
you know generating like bicycles from, from example,
a bicycle was made [OVERLAPPING] for- for the last ones. Yeah, yeah.
Right.
This slide. Right, so what was preventing it from just copying the source, that's what I mean?
Yeah, I mean- [OVERLAPPING]
For- for the inversion task,
it's, it's learning like an algorithm, right?
Right?
So why is that, you know,
the case it has stopped learning like to copy. [OVERLAPPING]
Right, but that's a good point.
But then if you see all the images,
all the images are slightly different from each other.
So if it just copies like one thing from one image,
another thing from another image,
the final generated image would not make sense at all.
So that's why it has to- it's learning- in- in our opinion,
I think it's learning a proper distribution of the objects that are sort of
important in that image because
they- all the images are slightly different from each other.
You can't just like pick and copy one of them.
And also, uh, an interesting,
uh, point was that, um,
that Ron made earlier was that in the, um,
in the attention-based PixelCNN, uh,
they augmented the dataset to include the positional information.
Um, and we're- and basically augmented for,
I guess the meta PixelCNN and also the,
uh, um, the conditional based PixelCNN.
So we think maybe the attention learned basically, uh,
how to flip the images by looking at that augmented component,
which is basically the positional, um,
dimensions, if that makes sense.
[BACKGROUND]
[APPLAUSE]
So this one is one-shot imitation learning.
It's by Sean and Dan.
Yeah, thanks, cool. Yeah. Okay.
Uh, hello, my name is Sean, uh, and this is Dan.
We're gonna present the paper,
One-Shot Imitation Learning, um,
to kinda describe the problem setup a little bit and motivation.
Um, Imitation Learning is pretty commonly applied to isolated tasks,
um, to kinda reproduce, um,
you know, demonstrations we see of different things.
Ah, but we have this desire to learn from a few demonstrations,
um, if we're gonna try and learn, uh,
a new task and instantly generalize to those new setups that we might see.
Um, in this specific paper, um,
the authors ask us to consider this case where there are infinite tasks to learn,
each with various instantiations.
However, in the paper,
they have this kind of like narrow scope of that, which is, um,
this like robot- robotic arm that,
um, is given like two pairs of demonstrations.
Um, and it- uh,
where- where each task is to stack blocks that are on this table, um,
in a different, um,
permutation and a different,
um, numbers of towers.
So here, it's trying to stack, um,
various blocks on top of each other in the policy, which is,
um, kind of like our test time to match what's going on in the demonstration.
Um, and- um, oh, sorry.
Cool. Um, to- to kinda like go over how it splits up into train and test.
Um, in the- the train state, you know,
we get this- we get a pair of demonstrations.
Ah, we have one full demonstration that we look at,
uh, with it- with attention.
And then from the other demonstration, um,
we sa- we have one state from it that we sample.
And we compute this optimal action for that state,
And we compare it, um,
to like the optimal action for that state,
um, for the- the unseen kind of policy. That's how we compute our loss.
And then during test time, we are also shown a new demonstration for a new task.
Um, we- it- I mean,
the-- the task will be something we haven't seen before.
So some new permutation of blocks that we have to stack in
some different order or in some different number of towers, um,
and we get some state, we compute some action for that state,
and then we'd run this loop multiple times until we achieve some terminal state.
Um, to talk about how we implement
the architecture or how the authors implemented the architecture of this.
There's three neural networks, um, that are,
you know, computing these- these different embeddings and taking different actions.
So there's the demonstration network,
the context network, and the manipulation network.
Um, to go through them one by one.
In the demonstration network, um,
it receives as input some,
um, trajectory, some demonstrations directory,
which is a sequence of frames taken from the camera of the robotic arm,
or is some embedding from the neural network that
analyzes the images coming from that robotic arm.
Um, its output is an embedding of the demonstration,
um, to be used by the subsequent neural network in this architecture.
Um, the size of this embedding depends on
the se- the number of time steps in the demonstration,
as well as the number of blocks on the table.
Um, so the way that,
um, we kind of, um,
capture info of- or the- the way we make this problem tractable, um,
in this neural network is we- we use
temporal dropout because we might have like thousands
of- of time steps within, um, our observation.
We throw away 95% of them, um,
randomly, and then, um,
we use a dilated temporal convolution to capture information,
um, from the past, but focusing more on the more recent time steps.
Um, we then use neighborhood attention to map these- um,
these like variable dimensional input into, um,
an output, um, representing the number of query hits, um,
in- in the context of attention,
um, that have the same number of outputs as we have, um, inputs.
So, um, it's really important, I guess.
Um, the next network is this context network,
which takes in, um, the current state,
the observation from our camera,
as well as the embedding produced by the demonstration network.
Um, and it produces a context embedding, um,
which kind of will tell our manipulation network,
um, like what context is to a form and what action we have to take.
Um, so it is important for the- the final network that its input is a fixed dimension.
So the- basically, the context network traps, um,
all of the contexts that we have in this, um,
variably sized dimension input,
um, into a fixed size output.
And the way we do that is temporal attention, um,
which produces a vector whose size is proportional,
or basically, it removes the dependency on time in our output vector.
So, um, we're only dependent on the number of blocks now, um,
and then when we apply attention over a current state and we produce
fixed dimensional vectors that are no longer
dependent upon the number of blocks in our state.
Um, and the intuition behind,
um, like why we're able to, uh,
remove kind of like a lot of
information in these like variably sized dimensional vectors,
um, is that the number of relevant objects is usually small and fixed.
For instance, um, if we have this robotic arm, we're trying to pick up blocks,
we probably only care about the block we're trying to pick
up and the block trying to put it on top of.
And finally, the man- the manipulation network is the simplest network of them all.
It's a simple multilayered perceptron.
Um, basically, it takes in some context embedding and produces
some n-dimensional output vector representing the action for this robotic arm to take.
Uh, one of the- the things that the authors leave
open is the potential for modular training here.
So if you have some, um, specialized, um,
task where you can effectively train this manipulation network, um,
it doesn't need to- necessarily need to be
trained in parallel with the rest of these networks.
It can be more highly specialized.
Um, and that's a solution for one of the problems they noticed later on,
which is where a lot of the errors they encounter in training
are just because there's a result in mis-manipulation.
So it will knock like some block off the table
and it'll be like some irre- irrecoverable failure.
Um, so you go through like just broadly,
um, we have some demonstration.
Um, we- it's a series of frames.
We've applied temporal dropout to it.
Um, we apply this temporal convolution to it, um,
to- to kind of attract the mos- uh,
to folks on most the recent steps.
Um, we apply neighborhood attention, um, such that,
ah, we can produce this like,
uh, variably sized, um, embedding.
Um, we apply our context network to this embedding to
remove dependencies on time steps and,
um, the number of blocks on the table.
And then finally, that context embedding is passed into our manipulation network,
which takes actions, um,
to move the robotic arm.
Um, to kind of stop here and briefly summarize and discuss.
Um, one of the questions I had about- once I read this paper,
was that like stacking blocks on top of each other,
um, to me might not necessarily be a Meta learning problem,
if the only like,
um, difference is, um,
like the order in which- or the permutation which you're
supposed to stack the blocks or the number of tabs you're trying to create.
Um, what do you guys think? Is this a- a Meta learning problem or not?
[NOISE]
So I guess I feel like some of the Meta learning problems that we,
you know, talked about, like for- for example, last, uh, April,
we talked about inverting an image,
which is basically like you- after you've learned an- an algorithm,
you're basically done, right?
So I guess that could come as Meta learning.
And also, if you had different number of blocks,
that's like a significant difference in the things you're observing.
And on the other hand, you have like potentially [inaudible]
restricting yourself to like two blocks at a time. So I guess I just [inaudible]
Yeah. Thank you. That's a super good answer.
Um, so yeah, I think there's a potential for- for that kind of thing here too.
Um, if you guys have any thoughts on, you know, whatever,
I look other kinds of tasks this could generalize too [NOISE] this problem setup,
I'd be, you know, curious to hear.
If not, we can go into the experiments and results.
[NOISE]
All right.
Cool.
All right. Um, so the question then becomes,
does- how does this model perform?
So the authors tried to devise experiments that
could answer the following questions, uh, listed here.
So the first thing they wanted to answer was,
what happens if they changed their training scheme?
So the two training schemes they take- that they tested was,
uh, behavioral cloning and DAGGER.
And for those of you who are not familiar with the terms, by behavioral cloning,
what we're doing is we're just literally directly learning-
trying to learn to policy using, uh, supervised learning.
But by DAGGER, what we're doing is we have
an expert that is demonstrating these trajectories,
which we initially tried to learn from.
And then after that, we also gathered trajectories that our agents,
that our learned policy is performing,
and then we check those states and ask the expert to
perform to like give the good actions,
to like label them.
And then we're gonna add that to our data so that we're
like interacting with the expert as we're learning.
So that's, uh, what's going on with DAGGER,
uh, repeatedly aggregating, uh, data.
So these are the two training schemes that we wanted- uh,
that the authors wanted to test.
The second one they wanted to answer is,
what is the effect of conditioning on different data?
So if you, uh, remember back to our network,
what we were doing was looking back to the whole demonstration and use that as the input.
So what happens if we just look at the very last frame or
what happens if we are allowed to look at only key frames,
which the, uh, authors manually selected?
So how does that affect our results?
The last thing was, of course,
how does this generalize, does this network learn to generalize?
So, um, this is the setup.
They had different training and test tasks,
each with 2 to 10 blocks of different layouts.
And then they collected about 1,000 trajectories per task using a hard-coded policy.
And they compared four models.
Uh, the first two were using the same architecture that we introduced,
but using two different training schemes,
uh, behavior cloning and DAGGER.
And, uh, the third and fourth one is- uses a slightly different input.
Um, the third one only looks at the final state of the trajectory,
while the fourth one, um,
looks at different, uh,
key, uh, snapshots of the whole trajectory.
So, um, I just wanna stop here a bit and ask, uh,
if anyone has any idea on like
expe- expectations on how these models would differ or like how would they perform?
Any guesses? If not,
[LAUGHTER] I can just proceed and, uh, report what happened.
But, uh, just looking at these definitions,
what we could guess is, for example, um,
because of the third one and fourth one,
we are looking at only a subset of the whole trajectory,
that's probably gonna be faster, right?
And also, um, we might expect that if we only look at the final state,
which is the final end image that we want to achieve as compared to the whole trajectory,
maybe the performance will not be as good.
On- on the other hand,
if we know what are the key steps that we have to follow,
maybe it's gonna be using, uh,
the data efficiently that the model actually performs better.
So those are some of the, uh,
guesses that the authors made before they proceeded with experiments.
And these are the actual results of the experiments.
There's kind of a lot of colorful bars here, um,
but the x-axis is the number of stages required to achieve the task at hand,
uh, the number of stages.
So for example, one is just- you just need to put a block on top of the other, and so on.
So it's- it corresponds to the difficulty of the problem.
The y-axis is the success rate.
So how successful the models were in performing these tasks.
And in terms of the policy,
from left to right,
we have the hard- hard-coded policy,
behavior cloning, DAGGER, uh,
only looking at the snapshots or the key frames,
and the last one is only looking at the final state.
Uh, yes, uh, you can make a few observations.
The first observation is that behavior cloning and
DAGGER actually seems to perform pretty much equally.
Sort of there was- hasn't been too much benefit
from human intervention or the hard policy intervention, uh,
with DAGGER, the authors assumed that this might be because,
um, we're already adding noise, uh, to the data.
Uh, they did not really specify what noise was,
but [LAUGHTER] they're adding noise to the trajectory.
So they assumed that this might be what's,
uh, what- why we are able to learn well.
And, uh, another point that we can see is that the snapshots and final states,
they're actually performing not too well, uh,
which is a bit surprising for snapshots.
And the authors, again, assumed because, uh,
re- uh, if you remember,
we were doing those temporal dropouts,
we were dropping 95%.
Uh, so we're doing some kind of regularization already when we're training,
so that, um, when we actually get the data,
we kind of know how to deal with it.
Um, so those are some possible explanations.
And the- this one was for the training task and this one is for the testing task.
As you obviously would imagine,
they perform much worse.
Um, but the story is about similar here, um,
the intuition that you get from the results is about the same.
And another fun chart here is how- looking at how attention,
um, differs across the blocks.
So on the x-axis, we have different blocks, A to J,
and from the y-axis, starting from the top,
we have the time steps across the policy.
So like as the policy carries on,
which blocks we are paying more attention to.
And the one- and the configuration that we want to achieve is ab, cde, fg, hij,
which means that we want block A on top of block B,
uh, block C on top of D,
which is on top of E, and so on.
So if you look at this plot,
it's actually kind of interesting because they're actually blocked into blocks.
Um, so, uh, in the beginning,
we're looking at A and B because we have- we know that
we have to put block A on top of block B and so on.
So these are actually kind of well-separated and kind of well
demonstrates intuitively that the model is looking at the right blocks at the right time,
that it knows what to do.
[NOISE] So this- um,
if you- uh, recall what we were talking about.
So they had two attentions.
One attention was looking at, uh, different blocks.
They had another attention looking at different time steps of the demonstration.
So this is what we're doing here.
So on the x-axis,
now, instead of blocks,
we have different time steps of the demonstration of, uh,
BI that was shown to the model to learn,
and our y-axis is the same.
So here we see that again,
we have the separations.
Um, in the beginning,
we have- we are focusing on this beginning time steps and that might be because
the model demonstration actually moved AB in the earlier time steps and so on.
So this is kinda interest- interesting in that they show what these attention values
are and it makes intuitive sense when you look at the- uh, the, uh, plots.
And they also broke down what kind of failures they had, uh,
in term- because none- none of the models are perfect,
including their hard-coded policy.
So they have three types of failure.
The wrong move, manipulation failure, and recoverable failure.
A wrong move is when the final result is just wrong.
The second one is manipulation, uh, failure,
when, uh, the- robot basically,
for example, drops the block on the ta- on- on- from the table,
so it just doesn't know what to do.
And the third one is recoverable failure,
which is they make a mistake, uh,
which they could have fixed if they were given time.
[NOISE] So one example was they have been building a tower and
they were trying to pick a block up and they just crashed the tower.
And over time, they would fix it,
but they did not finish it on time.
So those are some different, uh,
failures that they kind of ran into.
And you see, uh,
there are actually not that many wrong moves,
um, uh, it's- the plot is very hard to see.
But [LAUGHTER] the summary is, uh,
there weren't actually that many wrong moves,
except the case of when we conditioned the final state.
Um, and a lot of them were manipulation failures,
so they just need to manipulate the arm better.
So in terms of takeaways and strengths, um,
it seems like learning a family of skills
actually make learning/performing relevant tasks easier,
in this case, stacking the blocks.
And it had an interesting breakdown of modular structures.
If you remember, there were three modules.
Um, some results just like that attention one was very intuitive and clear.
And the neighbor attention, uh,
which they introduced allowed them to translate,
uh, input of variable size to weights of variable size as well.
And single-shot result is actually rather impressive of,
uh- you can look up the videos in your own time if you feel like it.
And while not presented in this paper,
the data was actually collected using simulations,
not real-life images, and the person was demonstrating using VRs.
So that was actually kind of cool as well.
Um, some weaknesses, um,
that we could think of was this is kind of the weakness of imitation learning itself,
is that we have to assume that there are
successful demonstrations that we- that the model can learn from,
and that is not necessarily always the case.
And if the demonstration is not optimal,
it- if it's just running around,
fooling around, then that's what our model might learn.
So that's some limitation just innate in imitation learning.
And as we've asked before,
the tasks are quite similar,
so it is a little bit questionable if you can say,
"Hey, this is meta-learning."
Um, it's definitely a- a step in it,
um, but it is still arguable.
And the algorithm just doesn't know what to do when
we have a block just under four and things like that.
Um, there were some other assumptions.
And in terms of the actual paper itself, um,
if you read through the paper,
there are actually not that many equations and descriptions.
So there are a lot of natural language descriptions that it's kinda hard to
comprehend what the network is actually doing unless you read the actual algorithm,
which is like a full page in the appendix.
Um, and they only discuss a single experiment,
which is stacking the blocks.
Uh, so it'd be nice to see
what other tasks could have been carried out using this scheme.
[NOISE] And also, they actually never really defined what action is,
uh, in this paper, at least we could- um, from what we could read.
So that was, uh, kind of sad.
Um, so these are some further questions that we could think of,
um, and we have some discussions as well, and,
uh, we could go into discussion questions,
but as we're almost done with time,
if there are any questions, we'll be happy to accept them right now.
So is this super clear? [LAUGHTER]
So how did the artist [LAUGHTER] take care of
the different task which has different blocks?
Sorry.
So how did the artist take care of different task each having different blocks?
So how is this case entered when the project has to have different number of blocks?
Right. Um, so [NOISE] that'll depend on,
uh, the- the network structure as well as the neighborhood attention mechanism.
Um, so that's what allowed us to have like variable length of inputs.
And so, for example,
like a- at the attention of a demonstration,
at that stage we'll have still variable length,
but after context network,
then we would end up in as an aggregated fixed size dimension that they
could like process and go with the action like- choose an action. Yeah.
Okay. Let's [inaudible]
Okay. [APPLAUSE] [NOISE]
Yeah. So the next presentation is massively multitask networks for
drug discovery and it's being presented by Andrew, Weston, and Justin.
Hi, I'm Andrew.
I'm Weston.
I'm Justin.
Er, we'll be presenting on the paper,
massively multitask networks for drug discovery.
Uh, so the first question that we have when reading this paper is what is drug discovery,
uh, as that we're not in, uh,
pharmaceutical engineering or anything like that.
Uh, so the goal for pharmaceutical companies is going to be something like, um,
they have different drugs that are testing out, um,
in an effort to find some attractive molecules for further optimization.
Um, but the problem with this, um,
is that there's a lot of
different compounds and their interactions are pretty complicated.
And so one of the ways that they try to go about doing
this is by automating the process using machine learning.
Um, and the goal there is to predict the interactions between the different targets and
the small molecules that are present within, um, their compounds.
So for example, in this dataset,
like the dude-inha, um,
the target might be to, um,
find which of the active molecules,
uh, might be targeting the enoyl reductase.
So, um, for studying this as a machine learning problem,
one of the motivations is that the datasets are extremely imbalanced.
Um, in fact, only 1-2% of the screen compounds were active against a given target.
Um, and we know that these kind of extremely skewed, um,
distributions can cause our machine learning algorithms to have problems.
Um, the second is that they're
very disparate sources of experimental data across multiple targets.
And so there's 259 datasets altogether,
uh, with 249 tasks.
Um, but as you can see here, there are different classes of tasks that we have.
Um, and there's not perfect overlap, um,
across these different datasets with the different tasks.
Um, and so it becomes an interesting, um,
case of like what kind of information is present in each dataset and each task,
uh, such that the algorithms are able to,
uh, profit from multitask learning.
And finally, uh, at least in the domain of drug discovery, uh,
prior work is unclear
whether as to multitask learning is actually beneficial in drug discovery.
Uh, so there was a Kaggle competition in 2012,
um, but wit- one of the criticisms, um,
to the model that was the winner in this competition was that they
had too small of a sample size and the gains in predictive accuracy were too small.
Uh, another paper by Unterthiner,
uh, was that the performance gains, uh, were positive.
But then there was a different paper in 2006 by Erhan,
which said that, uh,
multitasking networks did not consistently outperform single task networks.
And so this paper tries to add an extra data point in, um,
this literature of multitask learning, oh,
at whether as to it actually is better than single task networks.
So to give an overview of the method, uh,
the first thing that's necessary to do this task is to figure out a way
to featurize molecules as that's what our datasets are,
there are molecules with whether they're active with a specific target or not.
And it turns out prior work has done this before the paper.
And there is a way to featurize molecules by looking at different points on
the molecule and looking at what it is connected
to and have an embeddings for different molecule pieces.
And then, sort of hashing them together into a fixed length vector.
And the way the method works is it takes as an input layer,
this fixed length representation of molecules,
um, using the embedding we talked about in the previous slide.
And it feeds it through several hidden layers,
1-4, with a variable number of nodes.
The, um, the paper tries
several different architectures with
the different variable number of nodes in each hidden layer.
And the last layer,
it's a very simple multitask network.
It's just a different softmax node for each particular dataset.
So, um, most of the parameters are shared,
and it's only at the last layer where you have task specific parameters. All right.
So they tried to, uh,
understand how their models are
working through a number of experiments to answer a bunch of
questions which are: "How do you know
multitask neural nets perform relative to baselines?
How does adding more tasks affect the accuracy on a held in set of tasks?
Uh, would we rather have more tasks or more examples?
How does adding more tasks affect pre-training accuracy?
And then, when do datasets benefit from multitask training?"
Uh, so Experiment 1,
how do multitask neuron- neural nets perform relative to baselines?
Basically, they're just training a bunch of
simple machine learning models on each of their tasks and then compare their,
uh, multitask networks to those baselines.
So the first five lines here are four simple baselines,
logistic regression, random forest,
and two simpler neural nets.
Uh, and then the max here is just, uh,
the like task Y's max.
So taking the best model for each task and evaluating those.
And then, they train two
multitask networks and showed that these multitask networks do better.
Um, the first one just has a single,
uh, hidden layer of length 1,200.
And they note that this is not the best design
because if your final hidden layer is 1,200 long,
then each of your softmax nodes needs to have 1,200 parameters.
Whereas if you have a, uh,
two hidden layers where you have 2,000 in the first one and then 100 in the second one,
then you only need 100 parameters per softmax layer.
And so that allows you to avoid overfitting,
uh, to individual tasks in those softmax layers.
Um, so then, in Experiment 2, they ask,
"How does adding more tests- tasks, excuse me,
affect the accuracy on a set of held-in tasks?"
So they set this up as,
they trained a bunch of models on the same 10 tasks, and then, uh,
add a variable number of additional randomly sampled tasks,
and observe the accuracy as a function of the number of additional tasks.
So the things that can happen as you add, uh,
more tasks to your model are either you kind of increase your accuracy, uh,
continually as you add more tasks,
you plateau as you add more task and you don't really get any gains in accuracy,
or you go up for a while and then back down and some amount of tasks begins to hurt you.
Uh, so they run it on some held in set of 10 tasks and they find that on average,
uh, the accuracy continues to increase over time.
Uh, but notably, for many of them,
the accuracy initially decreases with addition of some tasks. Yeah.
Sir, I think you guys have said this before.
What- what's the definition of a task?
Great. So a task is,
uh, something like this.
So you have a- uh, basically,
you wanna know whether something [NOISE] will interact with some target.
Uh, so you have a bunch of different input mol- molecules and you're asking,
will it interact with this thing? [BACKGROUND] Um.
So yeah, so each of these colored lines is an- different individual, uh, interaction.
And then the black line with the confidence interval in the middle is the average.
So on average, it continues to increase over, uh,
with the addition of additional tasks,
but in many cases,
it actually hurts the accuracy at first and then goes up.
And in some cases,
it continues to hurt the accuracy and it doesn't really recover from that,
no matter how many tasks you add.
Uh, and they only really addressed the average case here.
They note that sometimes it decreases,
but they don't offer any explanation of that,
which I really would have liked.
Um, [NOISE] and they also choose these 10 tasks,
but don't analyze any of the other tasks, uh, which I,
I think that this result is probably sensitive to the task that they chose,
uh, and I would have liked to see it on all of the tasks,
kind of different types.
Uh, so then in Experiment 3, they asked,
would you rather have more tasks or more examples in our dataset?
Uh, and so here, uh,
the x-axis is the addition of new tasks,
the y-axis is accuracy,
and then these different colored lines are additional examples in the dataset.
Uh, and basically, they both help kind of orthogonally, uh,
and you can see there's trends in, uh,
in both directions as you add data points and as we add tasks,
which is what we'd expect.
Uh, unfortunately, the confidence intervals here are really wide,
and I think that's because they did the same setup as here where they
chose these just 10 held out, uh,
hel- held in, rather, uh,
tasks and evaluated on those rather than evaluating on all kinds of different tasks.
Um, and so I wish that they had done that, uh,
to have some more, uh,
general- generalizable results to all of their tasks.
Uh, and then finally, Experiment 4,
they asked how does adding more tasks affect pre-training accuracy?
So if you pre-train a model on a bunch of tasks and then, uh,
use those ways to initialize a model- a single task model for a new task,
how does that affect the accuracy?
Uh, and again, they said that on average,
it continues to increase a little bit,
but in some cases, it hurts a lot.
Uh, and this time they actually point that out and say,
"We hypothesize that the extent of this generalizability is determined by
the presence or absence of relevant data in the multi-task training set."
Uh, and this is the only analysis they give of this,
which, uh, I feel like this is kind of an obvious statement,
that if you're pre-trained on something that it
matters how relevant the data you're pre-training is,
uh, and really I'd,
I'd like the whole paper to kinda be addressing this point,
that how do we decide if data is relevant or not?
Uh, but they don't.
They just say that sometimes it'll hurt you,
uh, and that's it.
Um, cool.
You're gonna talk about the last one.
Yeah. So the last experiments, um, or the last set,
set of experiments they do is to try to figure out when
specifically do datasets benefit from multi-task training?
So uh, first way we going to test this is they
create this metric called active occurrence rates,
which is basically for a specific dataset and a specific molecule.
They measure the number of other datasets that that molecule is also active in,
and they sort of want to see if a molecule is active in a lot of datasets, um,
does multitask learning help that, um,
train better for a specific task than it would
if it's only active for like a small number of tasks?
And as you can see, um,
this graph sort of shows what the active occurrence rate is
versus how much it helps the specific tasks problem for doing multitask learning.
And, um, well, it's a- it's a very noisy graph.
You can see [NOISE] there's clearly a positive correlation between what
the active occurrence rate is and how much it helps to do multitask learning.
And in this vein,
another thing they want to figure out was,
is there a specific class of target molecules for which, uh,
multi-task training is more beneficial than any other task,
um, or any other category of tasks?
So they put them in these categories,
and ones that didn't fit an obvious category,
they just put in a miscellaneous category.
And, um, each black dot represents a specific target, um, dataset.
And it appears there's
like, not much difference in what kind of benefit you
can get from being in a different category,
although some of them do have like marginal variation,
uh, but in general, it appears this,
um, the benefit for multi-task learning
isn't really that dependent on which category it's in.
All right. And uh, to give an overview of
what we thought the strengths of the paper were.
Um, primarily, we thought one,
it's a very detailed empirical analysis on real-world data,
which is always a good thing in a machine-learning paper.
Um, they're able to tackle a challenging problem with
an extreme data skew where you're doing a binary classification,
but only 1 to 2% of the input data is actually active,
uh, which is a very difficult problem in general and they do it well here.
They use a simple network which enables for very detailed and simple analysis.
And they're exploring under what conditions multitask learning produces
positive and negative results and have
very thorough experiments on the specific real-world problem.
And they also achieve results using
a multi-task network that outperformed any other baseline that they could come up with,
which, um, is a positive showing that
this multitask learning is actually beneficial for this particular type of problem.
[NOISE] Um, but at the same time,
there were some weaknesses, um,
as are there in any other paper.
Um, so the first weakness is that there was
a huge correlation between the data size and the number of tasks.
Um, just because for any given dataset, uh,
you might find that it only has five tasks or something like that.
And so, you know, if you have 40 tasks but each dataset is only able to have,
uh, five tasks in each,
um, then there's not going to be as much overlap.
And so, um, that was a big limitation that we saw in one of the, uh,
quite a few of the experiments, uh,
where if you actually wanted to see, um,
what is the difference when you have 20 tasks versus 40 tasks, um,
the amount of data that you can actually have in the,
um, 20 task case is going to be very different and very much smaller.
Um, the second is, um,
while they gave us some good intuition, um,
there wasn't much, uh,
theoretical justification when or when to not use multitask learning.
Um, obviously they draw upon some, uh,
a level of, um,
like domain knowledge and saying that, uh,
perhaps these things are categorized as certain types of tasks, uh,
or maybe there is some relationship between the size of the dataset and then,
um, the variety of tasks that are employed.
Uh, but again, I think as an empirical paper, um,
it does give us intuition,
but not so much a strong theory to go upon.
Um, and lastly, uh,
whereas we did praise the simplicity of the network,
uh, for this analysis, uh,
we all do also wished that they could have explored all of the architectures,
uh, given that, uh,
this is a real-world problem.
It could be interesting to see, okay,
how far can we go with this drug discovery,
um, as an automated process.
Cool. And then potential improvements which are related to weaknesses.
Uh, so both more theoretical and more empirical results on task overlap,
uh, and how that is going to affect our accuracy, um,
maybe something like an analysis of how covariance between classes is going to, uh,
help classes learn from each other, uh,
or something simpler, just like saying,
"So we have these different categories of tasks.
If we train a single model that does all of the tasks in one category,
does that give us better accuracy than mixing all these tests together?"
Uh, and I think you probably could have gotten some more,
uh, fine grained results from doing that.
Uh, controlling the training set size versus the number of tasks.
So it would have been good to say like, uh,
control the number of like fixed X-Y pairs that are
going- being fed into the model during training time,
whether they are on different tasks or the same x value rather- the,
the same x-value on different tasks or the, the opposite.
Uh, also comparing different architectures like we said, um,
they only do this one very simple, uh,
embedding and the one,
uh, multi-layer perceptron, uh, setup.
And they do say that their results are kind of
dependent on using these smaller second hidden layers,
but they don't offer any analysis of how it changes,
uh, as they change their architecture.
Uh, and then finally,
they could have bench-marked against,
uh, models from related papers.
They say, basically, that,
uh, it's hard to do, so they didn't do it.
But I feel like there must have been some result that they could have
said compared to some other model or some other method that people are using.
Uh, takeaways.
Yeah. So, um, as, um,
machine learning researchers and not chemists,
we're interested in what can we take away from this in our own modeling.
Um, so first is, uh,
what seems to be the thesis of this paper that
multitask learning can yield superior results to single task learning.
Um, and that there- second,
there's limited transferability to task not contained in the training set.
Um, they say limited because they saw it in some cases and not others.
Um, third, multitask affects stronger for some datasets than others,
so it is pretty heterogeneous and, um,
it seems to require some level of
domain knowledge if you're going to employ multitask learning.
Um, fourth, the presence of
shared active compounds moderately correlated with multitask improvement.
Um, again, this, uh,
this was in relation to the fifth experiment,
um, where they saw that,
like if a molecule was, uh,
in 30 task versus only in 5 tasks,
then it would, uh, benefit more from multi- multi-task training.
Um, and lastly, the efficacy of multitask learning is directly,
uh, related to the availability of the relevant data.
Uh, again, pointing to the fifth experiment, um,
looking at just how many tasks each data point is associated with.
And that concludes our presentation.
Are there any questions?
[NOISE] Then thank you.
[APPLAUSE]
So hopefully, some of the limitations in those papers will
inspire some good final projects for the class,
uh, and I'll see everyone on Monday.
 All right. Let's get started.
So as a reminder for those presenting,
make sure to hook the microphone to your clothing
rather than holding it so that the audio is more consistent.
And we have two mikes. So you don't have to [NOISE] switch off.
Yeah. And I will start with this first paper.
[BACKGROUND]
Hi everyone. Uh, my name is Alex.
This is Sotta and this is Hendrick.
And today we'll be presenting a paper
called Meta-Learning of Unsupervised Learning Rules,
done by a team at Google Brain about a year ago.
So here's an outline of what we'll be covering in the presentation.
We'll keep it at the top right corner so you can stay oriented as to where we are.
So first let's, take, ah,
a look at what unsupervised learning really means.
Uh, so in a sense,
unsupervised learning allows us to learn a representation that
will be useful for further supervised tasks at a later time.
Um, and some of the most common examples of those are variational autoencoders,
which can learn a latent space representation of
a probability distribution so that we can later
use that for discriminative or generative tasks.
And also GANs, which do something similar but can be used to transfer,
for example, the style of- of a famous artist to any given input photo.
But two of the major deficits of both of
these approaches are that they tend to over-fit in
a sense to tha- the data that they are given so that
they really can't generalize well to other types of data,
and that they require enormous amount more data to learn to,
uh, solve another problem, another task.
So this basically brings up the question, uh,
can we find some way to learn
an unsupervised learning rule that can work well on a broad sense,
um, in a broad sense, ah,
on a lot of given tasks and independent of the specific data that they're given.
Um, so the approach that this paper takes to solve this is,
uh, something called Semi-Supervised Few-Shot Classification.
Which means it's not totally unsupervised,
but that it tries to solve
a supervised task using the most efficient representation of some- some unlabeled data.
Um, so what this looks like is,
we're basically given some unlabeled input as our train set,
and we tried to find some unsupervised rule to learn
an encoder that can then take this supervised problem and,
uh, encode the- this labeled train set in the most efficient way to solve it.
Um, so in the sense,
the supervised part actually consists of a small amount of data, uh,
but it's more used as a reference to make sure
that the rule we're learning generalizes well.
We'd actually use it, uh, solving a supervised problem.
So the question is, uh,
can we find, ah,
an unsupervised rule that can work well in a broad variety of supervised contexts,
supervised tasks, um, using meta-learning?
And before we do this,
let's just take a step back and look at
what an unsupervised rule would actually look like.
Um, so one supervised rule that we all know well is backprop.
And what that does is basically tries to learn
some model parameters by finding
the changes with those model parameters that will minimize some loss function.
But here, we don't really have a loss function.
So we're trying to find more of a general rule that just given some made-up parameters,
and the last layer of a neural network,
which is the architecture that's,
um, kind of constrained in this paper,
um, that we can kind of update those,
uh, weight rules automatically.
Um, so like a lot of meta-learning algorithms,
this approach consists of an outer loop and an inner loop,
and the outer loop is supervised.
We have this, um,
what is kind of a loss function.
It's called the meta objective,
but it's basically the sum of the losses
for each of these supervised tasks that it's trying to perform well on.
And the inner loop basically gives the model the chance
to go through lots of this unlabeled data and try to make sense of it,
uh, basically to find the most efficient way to encode
this data so that it can perform well on these later supervised tasks.
So, um, let's- let's take a look at this in a little bit more detail.
This is the high-level picture for how the algorithm is going to work.
It has sort of the components we're familiar with.
We'll start with the inner loop.
So the inner loop is what's modifying our base model.
The base model takes in raw data,
produces feature representations, which can then be used for classification.
And this model is only- so in this step,
in this inner loop step,
we're only using unlabeled data to generate these,
uh, uh, feature representations.
And the task is applying an unsupervised update algorithm
or a function to change the parameters inside the base model.
In the outer loop, now we're going to take this base model and
the feature representations that it has learned to apply it on labeled data,
generate a, uh, you know,
prediction for the classification of that labeled data, and then, uh,
compute the loss with respect to
the error between the labeled data that we do have and the,
and the predictions from the feature representations,
and then that'll be back propagated all the way through- through the unsupervised update.
So let's- let's think a little bit more about what this inner loop is doing.
What question are we trying to solve here?
Well, we're given some encoder function
which is parameterized by the task-specific parameters Phi.
And the challenge we're facing is to change
the parameters Phi such that given unlabeled data,
we're able to produce efficient,
compact representations of that data.
And, you know, it would be nice if we had some true loss function which
could tell us how well that
those feature representations are useful for a classification task,
but we don't have that.
So the idea that they propose is, well,
what if we use another neural network parameterized by
the meta parameters Theta to generate those error signals internally.
So a little bit more of the mechanics.
Let's see what's going on here.
We're gonna have this base model,
which is parameterized as a- as
a deep neural network with forward weights w for each of the layers.
And we're going to take our inputs,
pass it through the network,
generate all the intermediate activations,
and then at the final layer,
output the feature representation of that input.
We're going to hold onto those intermediate activations
and use them to generate the error signals.
So these err- these intermediate activations are passed through a neural network,
parameterized by those meta parameters Theta to produce error signals,
which will be used in a backward pass.
The way this process works is, you know,
at the top you initialize your error with the output of, you know,
the top layer is NLP and then we're going to
just backprop all the way through in the way that we're all familiar with,
where at each intermediate layer where getting the top-down error signal,
and we're also linearly combining that with the,
uh, output from the error producing network.
So we can think a little bit about
how these updates are made now that we have some way of producing,
internally, the error within the network.
So phi are the base model parameters.
Those are the task specific parameters that are going to essentially
learn how to build a representation per- uh,
yeah, build a representation of our raw data.
Those are parameterized by W,
V and all the biases.
And then, now that we've already computed those error function- error- error quantities,
we can make local updates using just almost linear functions of those errors.
There's some small qualifications,
in that the actual expression used to, uh,
take in the error quantity from the layer
above as well as your internally generated error, uh,
and impose some non-linear functions on top of it to basically normalize,
uh, as it propagates through the network.
So let's- let's think about what are
the key points to take away from this inner loop process.
The first is that the- the error generating network is what drives
the- the unsupervised learning within this base model,
and it's supposed to resemble sort of what we're familiar with, with backpropagation.
These iterative updates that we make using the backpropagation
like rule can be used to tune the model parameters,
spore some higher level objective.
And that objective is defined by the outer loop,
which changes the parameters of the error producing network in order to have
the unsupervised training explore different regions of its sort of,
uh, space of task specific parameters.
There's actually some interesting connections you can draw
between what's going on here and, say,
a reinforcement learning problem,
where your task specific parameters,
phi, are sort of like the states of your state space.
And the outer loop's role is to define a policy parameterized by theta,
which changes how your unsupervised learning algorithm
navigates its state-space and comes to a final,
sort of final state,
which is able to take in data and produce useful feature representations of that data.
So now let's look a little bit closer at what the outer loop is going to do.
Okay. So it's all gone,
come together here in the outer loop.
So now, where we're at is we had some unlabeled support.
This was an inner loop,
and we applied our unsupervised learning rule,
which was parameterized by theta.
Okay. And we'll hold off like we'll- how do you actually learn theta?
Well, we'll get to that. So we have now an encoder.
Now we move out to the outer loop,
and we're going to use this encoder.
So we have labeled support.
So support is the task specific training data,
and the labeled query,
which is the task specific test data.
And we apply this encoder to these vectors.
So you can imagine now we have a bunch of images,
X_1, X_2, X_3, and X_4.
That's for training images and we have two test images,
and we're doing few-shot uh, classification.
And we get compact vectors,
one for each image if we're doing this on images.
Now, we fit- wh- what they do specifically
is they fit a linear model for linear regression on those vectors,
those green vectors, eh,
because we have labels here.
Okay, so now we have fit a linear model in just a few examples.
And now we can apply this model on the label query and evaluate the model,
and we evaluate and get a mean squared error.
Okay. Now you have this whole computation graph,
and to update theta in the top left corner,
we can just backprop all the way through this computation graph.
Now, everything is not the opposite direction.
And now you may wonder,
okay, but that inner loop,
it's probably quite a few steps because we are applying this
unsuper- these unsupervised updates.
And yes, that's like thousands of thousands of steps many times.
So you- you- you can't backprop through that whole loop.
So what they do instead is truncate a backprop,
which I won't go into the details of,
but basically what's going on is
the unsupervised rule does ten steps of updating the encoder.
Then we apply the encoder, fit a linear model,
evaluate the model, get a mean squared error,
backprop, and then we continue.
So you do ten steps updates,
go through the whole process,
backprop, ten steps update.
So that- that's sort of the spirit of truncated backprop.
Okay. So how well does this do?
They meta-train using CIFAR data and ImageNet.
And then they evaluate how well does this generalize to other datasets?
So what that means is we learn an unsupervised learning rule,
we meta learn an unsupervised learning rule on CIFAR10 and ImageNet,
and then we go to another dataset and see how it does this job.
Then they explore how well does it do in other domains?
And that means taking this unsupervised learning rule that was
meta-trained on image data and now apply that to text data,
specifically in this case.
And then they also explore what if we apply
this unsupervised learning rule now to completely new network architectures.
That- that means that the encoder has a different architecture.
Okay. So what's going on here?
We're evaluating our unsupervised learning rule
on different datasets and compare it to other methods.
They compare it to three other methods,
but we'll only focus on one other method right now.
So that's a VAE, so that's in orange, the variational auto-encoder.
So in that case,
that you have your unlabeled data,
you apply your auto-encoder,
get a representation on which you do fit a linear model just like before.
Okay. And then compare that to their learned- unsupervised learning rule.
So VAE is an unsupervised learning rule.
Okay. And but-we're- we're comparing that to the learning rule that we have learned.
And it does better on these four image datasets,
TinyMnist, Mnist, TinyFashionMnist, and FashionMnist.
Now, they tried generalization over domains, okay?
So they're evaluating the rule on two-way text classification.
And if we focus our intention on the purple,
the 30 hours curve,
what that means is, we did 30 hours on meta-training,
and now, we have an unsupervised learning rule.
And now, we apply that to the case of two-way text classification.
And what you can see here is,
if I apply my unsupervised learning rule for 1,000 steps,
I get a quite good representation such that I can get this accuracy.
But if I apply my unsupervised learning rule for a bit longer,
for 2,000 steps, I can do even better,
but then it levels off.
Okay. So that seems to be- it seems to have learned useful representation for text data.
So this, this is pretty cool.
They- there was no text data involved in learning this unsupervised learning rule.
But then we can look at the 200 hours.
What this one represent is,
I meta-trained for 200 hours, got a rule.
Now, I'm applying this rule.
This rule doesn't work well at all.
And their explanation is something along the lines of- in this case,
we have meta over fitted to the specific- images basically.
Okay. So it's- this rule here in the red is not as general as the one in purple.
And then they explored generalization over networks.
Okay. So and- what that means is,
so let's focus our attention to the most blue one here.
Okay. So here's an unsupervised learning rule that was trained for 10,000 steps.
And then they explore how well does this rule do
for an encoder when the base model has different numbers of layers.
Basically what this says is, it generalizes.
Okay, it does well. You can apply this rule.
But this rule is, you apply- you,
you can have as many neurons as you want to or many layers as you want to because
this rule really operates on the neuron specific layer.
Okay. And this light green here,
it's just as a comparison.
Well, what if we- basically,
how you just- a random rule.
And they try with number of units.
So the first one was, was the depth,
this is width of the encoder.
And they also explore wha- what if I have different activation function in our encoder?
Okay. Now, let's finalize here with
some cri- critiques and limitations specifically that we identified.
Well, first of all, this is very computationally expensive.
This is not for you at home basically,
like thi- this is- this took a long time to train.
And this was after using many, many tricks.
So what one type of trick,
which is hyperparameter tuning,
and this is gradient clipping,
and specific learning rates.
But also, how the actual sup- unsupervised learning rule
was parameterized and structured was very tricky.
There were- we abstracted up way a bunch of details in this talk.
So that was- it's sort about also like a limitation that its- that's,
uh, it seems to be quite hard to get this working.
And allege of that is,
that there is no ablative analysis.
Meaning, we don't know- of all these tricks,
let- let's say there were 40 tricks in this paper probably.
They were like an appendix that they were- it's a long appendix [LAUGHTER].
It's unclear what, what was the secret sauce.
Were, were they all necessary or were,
were some tricks more important than the others?
Also, reproducibility, like if we went to reproduce these paper.
There are many things that we weren't able to actually get from the paper.
For instance, they're doing few-shot classification,
but how many label examples did they have?
Like what was the K in their K shot,
and what was their N in there anyway?
As well as how many unlabeled examples did they have on
each task, that we,
we were not able to read that out from the paper.
And that seems quite important to actually interpret the accuracy scores, for instance.
Now, given these limitations,
we have some suggestions.
First, we we could do an ablative analysis.
Uh, perhaps this, we were thinking that in this case they're sort
of doing, um, truncated backprop.
Potentially, you could do something like implicit MAML instead.
Let us know if that's true.
[LAUGHTER] Um, you could also,
it'd be interesting to investigate beyond,
just and then like more network architectures such as CNN or attention-based models,
how generalized it is to those?
Uh, is there a better way that like that enco- learning rule
was encoded in a very complex way.
And like, is there a simple way of doing this?
And also, where we're curious,
like is this learning rule fully expressive?
Like can this, uh,
approach learn any potential learning rule?
Uh, we don't know, but perhaps it can.
Um, and I think that was it. Thank you.
[APPLAUSE]
So if you- how much time do we have?
Negative two minutes.
[LAUGHTER] That's right. So if you have any questions,
you can't ask them.
[LAUGHTER] Right they can't, or can they ask?
No. [LAUGHTER] They can ask it offline.
One more time.
They can ask you offline now.
Ah, perfect, offline apparently, you're allowed.
Thanks. So I'm Katherine and this is Saelig.
And today, we'll be talking about, um, this, uh,
Bachman, Sordoni and Trischler paper, Learning Algorithms for Active Learning.
So this is basically ah,
an extension of matching networks to an active learning case.
So I'll start by briefly reviewing matching networks.
Um, so as Chelsea,
uh, discussed in a recent lecture,
one way to think about matching networks is as a differentiable k-nearest neighbors,
um, over a learned embedding space.
Um, so in a very simple setup like we have here, um,
we can imagine that we are going to embed each of
our example items as well as our probe item,
um, using, uh, for example, a CNN.
Uh, and then we want to compute the label for the probe item.
So to do that, we have, um,
either an attention or a distance function that basically compares the similarity
of examples or embedded examples with the embedded probe item.
Um, and from that,
we can take an average of the example labels weighted by their similarity with the probe.
Okay. So, um, in the previous case, we had, um,
uh, basically shape as the distinguishing feature among the examples.
But in this task,
um, color distinguishes them.
So this motivates the idea that we might wanna be using a context sensitive encoder,
where the embedding of a given example
depends on the other examples that are present for that task.
Um, so one way we might do this is using a contextual encoder like a bidirectional LSTM,
um, where, ah, again, each, um,
example embedding depends on the other examples that are present.
Uh, similarly, we might want the embedding of
our probe item to depend on the examples that are present.
Um, so the way that they do that in matching networks is they have an attention LSTM,
um, which, uh, in addition to conditioning on the examples,
also allows the, um,
probe item encoder to do things like ignore some of
the examples if they're like redundant or there are outliers present,
um, and also, uh,
builds in variance to the order in which the examples come in.
Okay. So, um, that's like a bird's-eye view of matching networks.
Um, but now, imagine that we're operating in
a case where we only have labels for some of our examples.
So, um, the area of active learning deals with
this because there are like lots of real-world, um,
settings where we have plenty of unlabeled data,
and then just a little bit of labeled data, um,
or it's very expensive to acquire.
Um, so one example is like an- in medical imaging case,
it might be that you could acquire labels for scans through expert annotation,
but it would be costly to get them.
So the idea, um,
in the paper we're going to be describing today is that they use meta-learning to,
um, active learn strategies for a given task.
Um, uh, so in other words, the,
the model is going to learn which labels it makes sense to request.
Okay. So Saelig will go into the details in a minute,
but just a high-level overview of the model,
um, the idea is that we have a,
um, controller, an LSTM controller,
um, that requests labels for, um,
what come in as a completely unlabeled set of example items,
um, over a series of time steps.
So, um, this is the, kind of,
active learning phase, and that means that at a given time step,
the controller, you know,
sees its unlabelled set,
it requests the label for one of the as-yet unlabeled items.
It reads in that label along with the encoding of the example item.
Um, and, uh, then it uses this information to
basically make predictions for the labels
of the other currently unlabeled items in the set.
Um, it gets a reward for how well it does, um,
and then proceeds to the next time separate can request another label.
Um, at the end of,
um, T time steps,
it takes all of the labeled items that it's acquired, um,
and uses a modified matching network to classify held-out probe item.
Um, so, ah, because it's making
discrete choices as it requests labels as this is trained with reinforcement learning,
and the total reward is a combination of how well it does during that, kind of,
intermediate rewards of the act of learning, um,
training part and how,
how well it does pacifying the held-out probe item.
Um, one other thing to mention here is,
you might ask like, well,
why aren't you using the matching network,
um, at each time step?
And the reason is that the kind of
intermediate predictor that it uses is less computationally expensive,
so it doesn't use the full matching network on the probe item until the very end.
[NOISE]
Oh, thanks Katherine. So now we'll dive a little bit deeper into
the individual modules used in the diagram you saw on the previous slide,
and we also just had a picture up on the corner just for, ah, easy reference.
So we start with the context free, uh, encoding.
So these you can think of like, for example,
image data, you'd have a CNN, um,
generate this, um, encoding which is kind of
independent of the context of the support set.
And then using these,
um, context free encodings,
we generate these context-sensitive encodings
by essentially concatenating- concatenating,
ah, these encodings together and then run a bi-directional LSTM.
That way they have an idea, what else is in the- on support set.
Then we have the selection module.
So at each time step t,
we essentially get a probability distribution,
um, over all the unlabeled items and we have to figure out which one we wanna pick.
And so the way we do this is we use,
um, a bunch of different features, the max, min,
and mean cosine similarity features between, ah,
the item in- the controller-item and the item-item similarities.
We concatenate these all into a single vector and
then use a gated linear combination of these features to figure out,
um, which- which is like the maximum logit we want to select for the unlabeled label.
Um, then in the reading module it takes
this label and the embedding for the item and simply concatenates them together, um,
and then applies a linear transformation, ah,
which the controller then takes as input, um,
and then applies a simple LSTM update like you can see on it right there.
So and then in terms of the prediction rewards- so on the top,
you can kinda see, ah, what is the reward you're trying to optimize over?
So essentially, we want to maximize the log probability that the,
um, label for the held-out item,
um, is given, ah, given that the, ah,
from our held-out test item x, um,
and then the current hidden state,
and the current support, um,
vector at time t. Um,
and so the objective is to minimize, uh,
maximize this at each time step.
And the idea we wanna maximize this at each time step,
is to promote this, uh,
anytime behavior so that even if you're gonna kind of stop midway,
you perform as optimally as you can.
Um, but this is quite computationally intensive to calculate.
So the authors actually suggest,
um, calculating what they have on the right,
which is an approximation.
And so now you can see there is two reward terms,
the left being the reward for, um,
the unlabeled items and then the right being
the reward for at the- at the very end across
the whole episode using the terminal and support in a terminal hidden state.
So on the left,
to calculate the left reward term,
we can use this mechanism,
which they call fast prediction.
And essentially, the idea is that this is
attention based prediction that uses the similarities between,
um, the unlabeled and labeled items.
Um, and then they actually used this sharpening score,
which they empirically found to work much better and get better performance.
Um, and then the idea is that the similarities,
since they don't change with each time step,
you can precompute them and therefore,
it's much faster, um,
to use this computation.
Then the slow prediction,
which they use to calculate the right reward term,
um, they do once at the very end.
And this is using a modified matching network which has
to pass through the full LSTM, um,
and its condition on the active learning control state, um,
like h of t, um,
at the terminal one at the very end as well.
And so this is, um,
because you have to go through the full LSTM,
this is why they call it the slow prediction.
So just to reiterate like the full algorithm as Katherine was mentioning.
Um, so for each time step,
we essentially select the next instance using our selection module.
Then we read this label instance concatenated with the label,
and then apply an LSTM update.
Um, then based on the- now the- the labeled item pair we have,
ah, we update our known and unknown, uh,
set and then perform the fa- fast prediction to
calculate the reward term as you saw in the previous slide at every time step,
and then finally use the slow prediction to calculate the right reward term.
So in terms of task,
so they wanna to basically test out their strategy on a couple of different tasks.
So they used the Omniglot,
which we're all familiar with from our homework, and, uh,
and so this is a bunch of these characters from 50 different alphabets.
Um, but they also wanted to test in a more practical setting.
So they tested on the MovieLens dataset, uh,
which is basically a dataset composed of many ratings by users given to movies.
And so kind of like the idea of this is,
when a new user signs up, ah,
for a movie, you don't really know their preferences.
So how- how can you get this cold-start, ah,
kind of, um, thing where how can you figure out what their preferences are?
So one example is you can just show them random movies or popular movies and
see how they rate and get their preference from that
but that's probably not the most optimal way.
Um, so this is why they use this active learning approach in that case.
So in terms of the Omniglot, um, Baseline Models,
they use two slightly different versions of,
uh, the matching network.
And so in the first one,
they used this random set, where essentially, uh,
to generate the support,
they randomly sample across the images in classes.
So this means that there probably isn't gonna be an even distribution.
Um, that even some classes may not even have
any images in the dataset for them to train on.
Um, then the matching- ah,
matching net on the balanced case is they actually ensure an even class distribution.
So say five images per class or something like that.
And this is kind of like, you know,
the optimistic, um, performance
that you would- you would hope that the active learner, ah,
could kind of figure out to get as many uniquely levels to from each class as
possible and make sure that no class is under represented or not represented at all,
which may be the case in the random.
Ah, and then the last baseline,
they used is kind of a heuristic,
where it's like this Minimum-Maximum Cosine Similarity.
So basically, this is effectively choosing items that are most
different from the items that you currently have. So how did this do?
So you can see that the,
um, their active, um,
MN network, um, beat the random one,
which you kind of hope it would do.
Um, and then it came quite close to performance into the matching net balance,
which is kind of like I was saying,
you know, something that you hope the active learner can learn.
So it's more of an optimistic baseline.
Um, you can see that there is a 2% degradation on the 1-shot
10-way compared to the Active MN and the MN in the balanced case.
And this is kind of expected because in a 1-shot case,
if you only have one image and you're trying to distinguish with,
like in a large N,
I guess, with many different classes,
that can be quite hard for the active learner to figure out it needs to
pick like a unique label from each class.
And then as I was mentioning before,
we wanted to optimize both the performance and data efficiency,
and so in terms of data efficiency on the top,
we can see the Omniglot performance and so the- the left plot
is basically how is it generalizing beyond a 1-shot setting?
And so you can see that, like,
as you get more and more labels requested,
um, the policy in purple, um,
actually does much better than
the MN balanced case in the 1-shot and approaches, um, in the 2-shot.
And as we get more labels,
it does do better, which is,
ah, what we'd expect.
Um, and then on the right,
you can kinda see how does this generalize to situations where
there is more classes in the test set than what was trained on.
So they do a comparison with 5,
10, 15, and 20 classes.
And you can see even in the five class setting,
ah, it kind of, ah,
it- it does reasonably well.
It kind of approaches this, um,
oracle policy where basically requests a unique label at every single timestamp,
which you'd hope is like what the active learner is trying to do.
And we see that, you know,
if it is in fact trained with 20 classes and that's when you test on,
um, it almost, uh, matches Oracle policy.
Um, on the bottom, we see the MovieLens performance.
So on the y-axis,
we have the root mean squared error.
And so we're trying to minimize this.
And so in this case,
the active MN- MN actually outperforms all the baselines.
And so the way the- they use this baseline, so for example,
you have the popular entropy.
So this is kinda what I was describing the heuristic like earlier.
So basically, you just pick the popular movies
in your dataset and kind of show those to the users.
And so these kind of baselines were, um, basically, ah,
replaced the selection module, ah,
in the network and then they run the full thing
end-to-end. And so this is kind of nice to see.
It kind of gives you an idea of the importance of
the selection model- the selection module.
So you see the importance of, you know,
how you're selecting things in the policy to do that.
Um, and yeah, so as you can see,
um, it does outperform all these baselines.
So in conclusion, um,
they proposed a model that basically active- actively learns,
ah, a system end-to-end.
Um, they showed that we pretty much approach optimistic performance on the Omniglot.
And we actually outperform baselines on the MovieLens dataset,
which is a more practical setting.
So maybe it shows promise for this work
actually being used in more real-world applications.
And now on to Katherine,
we'll talk a little bit more about their critique and discussion points.
Yes, so, um, we thought one of the kind of most apparent issues, ah,
with the model is that the controller doesn't
actually condition its label requests on the probe item at all,
which is kinda strange if you think about it.
So, um, in this kind of contrived example we've created,
and we have a marmot probe item,
and then one of the other examples is a highly similar image.
Um, and you might expect that, you know,
you want to just request the label for the other marmot image,
given that the other items are not at all visually similar.
Ah, but on this model, there is, um,
no way for this to happen since the controller hasn't actually seen the probe item,
um, and, uh, that doesn't actually come in until the- the very end of the process.
Um, there is a related point in Matching Networks, ah,
which is that the embeddings of the,
ah, examples don't depend on the probe item.
So in Matching Networks,
the examples depend on each other and the probe item depends on the examples,
but not vice versa.
Okay, um, kind of stepping back though, ah,
we were thinking about the fact that active learning is useful in
a case where data is really expensive to collect, but this model,
of course to meta-learn active learning policies, um,
has to have a lot of labeled data, ah, to learn.
So we were trying to think about domains where,
um, this would or wouldn't be a realistic scenario and we're curious if people had ideas.
[NOISE] So I think, um,
one positive use case might be like
the medical imaging example that we gave at the beginning,
where it is possible to collect,
ah, labels for scans.
And maybe you already have a big database of those,
but you don't wanna to have like a label- a
labeler having to continue labeling scans in the future.
So in that case, you'd have the data to train
the model and then you could use it going forward.
Yeah. We have a few more discussion points,
but do we have time to [BACKGROUND] -
You've got 20 minutes.
Okay.
One thing I would like to say is that large datasets in
the medical is usually- so in the case of radiology is from radiology reports.
So that they're like letting those labels,
so you could imagine a scenario where you have error-prone label to this,
but then you can request good labels.
Right? Oh I understand.
Yeah, and- and I guess, um,
kinda going forward that like, you know,
say you, um, have like these NLP tasks, right?
Um, so do- do you guys think that there is,
um, this approach could help in some NLP tasks?
I guess in choosing, you know,
maybe more relevant data or something that's,
you know, I guess more useful to the task?
So I guess like, uh, one, you know,
one idea that we had was in basically choosing the more relevant sentences.
So maybe like a machine translation task,
you have a bunch of different, uh,
sentences in your data set and maybe some are more representative of,
you know, what the usual language is like,
maybe some are more slang.
And, you know, depending on the task,
maybe you want to pick the ones that are more, um,
I guess more representative or I guess would better help in the translation.
Um, and yeah, I guess like the last discussion point we had was,
um, so they actually did a couple of ablations, at least for the Omniglot.
And so they found that the context-sensitive encoder that we were talking about,
where we have the LSTM run over all these contexts
independent embeddings actually didn't have much of an effect.
Um, are they- and, I guess,
like our reasoning for that is probably because in the Omniglot,
like all- all the images that, you know,
CNN would probably do pretty well at getting the context and looking at what the other,
um, I guess characters are in the support probably isn't too irrelevant.
Um, but maybe there are other applications like, you know,
say there were different colors then that would be really,
uh, useful to know the context of other ones.
Um, maybe there's other applications where this could be essential.
Yeah. So you like introduce more variability,
like across tasks in example items,
you should expect to need context more.
Yeah. I guess we are all right then.
[APPLAUSE].
Hi everyone, is this working?
So I'm Megumi.
Ayush.
I'm Kamil.
And we're going to be presenting on one-shot imitation
from observing humans via domain adaptive meta-learning.
Um, the authors of the paper are here today,
[LAUGHTER] which is great.
Um, so let's get started.
So let's start with the general problem of imitation learning, um,
where a robot must learn to do a task by observing human demonstrations.
So traditionally, this is done either through kinesthetic teaching,
which is where a human directly manipulates the robot's arm,
for example, or through teleoperation where this manipulation is done remotely.
And in either of these cases,
it requires a lot of grad student time and effort to be put
into this tedious task of carefully manipulating the robot.
Uh, humans, on the other hand, um,
are able to- are also able to learn,
ah, from demonstrations by others.
But we can do this through visual input.
So the question is,
what can robots do then?
There are several different challenges,
um, with regards to this task.
So the first one is that if we are trying to learn from raw visual input,
um, that requires a lot of data in the first place.
And second, ah, there's this fundamental problem [BACKGROUND] of a domain shift.
So for one, um,
it's very hard to learn the morphological correspondence between,
ah, a robot's body parts and human's body parts,
a robotic arm and a human arm.
Um, and this correspondence might not even exist in the first place for some robots.
Ah, and then on top of that,
the robot must be able to generalize between different demonstrators, different objects,
and different backgrounds, um, and learn, ah,
task specific parameters regardless of these other changing factors.
So the authors of this paper were motivated by
these challenges to develop domain adoptive meta-learning.
And here's the idea.
So the final goal is for the robot to be able to learn to do
a certain task just by observing one single human demonstration in the form of a video.
So this is the end goal.
And in order to do that,
we can provide the robot with, ah,
human demonstrations and robot demonstrations during meta-training,
where a human demonstration, d_h,
is a sequence of observations, ah,
and the robot demonstration d_r,
is a sequence of not only observations,
but the robot state.
So for example, the robots joint angles or some body configuration information,
as well as the robot actions.
And so the idea here is that during meta-training,
we can learn how to infer a task specific policy, ah,
from a human demonstration by validating on the robot demonstrations,
which is much easier to do because you have access to the robots actions.
And this is just an overview and we'll get into the details later.
Um, and so, um,
just really quickly, this is very similar
to- this is an extension of the MAML algorithms.
So you can see that the,
um, thing inside the blue box,
ah, is the inner- is what we would call the inner loop,
and the red is the outer loop.
Ah, and you can see that the loss inside the inner loop,
ah, is computed on just the human demonstrations.
Whereas for the meta-training part,
ah, in the outer loop,
we can compute, ah,
behavior cloning loss on the robot demonstrations.
So kind of getting more into,
ah, what the algorithm is doing.
Um, so during meta-training,
we first sample a task from the task distribution,
and then we sample a human demonstration specifically of that task.
And then what we can do is, ah,
compute policy parameters, ah,
by using this loss L Psi on the human demonstration and during meta-training,
since we do have access to the robot demonstrations,
we can sample a robot demonstration of that same task and we can
update our meta parameters using the loss calculated on the robot demonstrations.
And as mentioned- as I mentioned, ah,
in the previous slide here we can use LBC,
which is the behavioral cloning loss,
since we do have access to the robot's, ah, actions.
In the output here, um,
are two, um, meta parameters.
So there's Theta and there's Psi.
Uh, Theta is something that we are probably all familiar with.
So during meta-test time,
Theta acts as the initial policy parameters that you're doing,
the gradient step from.
Um, and then something that might be new and I think there was
a group that presented on something that fits similar to this,
um, is we meta learn that inner loss.
So there's L Psi,
ah, and we're learning Psi, ah,
which parametizes this loss, ah, during meta-training.
And, uh, Kamil will talk more about that later.
And then of course, ah, in the input to the meta-test algorithm,
we have our single video of a human demonstration,
which we want to learn from.
And the output is the policy parameters, ah,
which we can compute from this learn loss.
[BACKGROUND]
Okay. Cool. So we talked about the architecture in detail.
So we talked about the training step,
the meta-training, and then also the meta-testing.
So we'll go over it in regards to this policy architecture.
So as we talked about initially,
what we have is this initial parameter Theta and we wanna update
Theta to be more specific to a new task that we're trying to generalize too.
So during meta-training initially,
what we do is we feed in this human demonstration,
which is of a new task that we're trying to update our task parameters,
uh, to be able to accomplish.
And so one of the things that we think about is that as Megumi mentioned earlier,
we don't have access to human actions and they
might not directly correspond to robot actions.
So it's not enough to enforce a loss on- on this action like the human demonstration,
because that wouldn't give us enough- like we- first of all,
we don't have access to those actions and number 2,
since those- since those actions might not have direct correspondence,
it's not actually gonna inform us to make
any updates or supply any gradients to update these task parameters.
So what they do instead is that they have this idea of a learned adaptation objective,
and we'll treat that as a black box for
now and then we'll get into it later in the next slide.
So what happens initially is we feed in
this human demonstrations through some perception layers.
So a few convolutional layers of stride 2 and then two of stride 1.
And this outputs a feature vector which corresponds to, ah, the perception.
Like it's- it's part of the perception output.
And then those features are concatenated with
the last hiddens- hidden layer of the control layers.
So these are the ones that are used to predict the action and we
concatenate these two vectors and we feed them into this learned adaptation objective.
And what this learned adaptation objective has to
accomplish is that it has to understand based on this new task,
what the behavior- task specific behavior is,
and as well as the objects that are necessary to execute that task.
And so one of the things that they theorized is that to-
to understand what this learned adaptation objective should look like,
we have to have a temporal view of the human demonstration.
This is because actions take- take place across
multiple frames and having a view of all the frames at once
allows you to understand what motions needs to happen and
what needs to- what needs to be updated is not only the perceptual,
ah, components of this network,
but also the control components,
because it's both the perception and the control
that influence the behavior for the new task.
And so this learn adaptation objective,
which we'll talk about later,
is the goal is to supply gradients back to
both the control layers and the perception components of this network.
Such that we can generalize and we can go from this initial Theta to Phi,
which is the new parameters for this new task.
So after we update these parameters,
we feed in a robot demonstration,
and this robot demonstration goes through
the same perceptual layers and then we have the robot configuration,
which is the states, so the joints of the robots and the configuration of those joints.
We take those along with the perception features to
predict the gripper pose and the gripper pose is
essentially whether the gripper should be open or closed.
And so we have a squared error loss on this
to enforce that this- that this is done correctly.
And then because actions are more complex than simply just opening a gripper,
we have to have- we have a few fully connected layers which go on to predict the action.
And then we have a behavioral cloning loss based on
the robot actions that we also have access to during meta-training.
So the combination of these two things,
does this one-shot imitation learning such that we feed in this human demonstration.
We have one example of this.
And then we update the layers such that we have this new- we update
the parameters such that we have this new Phi parameter which is specific to that task,
such that the robot demonstration can fulfill that task.
So we'll go into this learned adaptation objective now.
So the way the learned adaptation objective works it takes into account
the perceptual features that have been extracted from those earlier convolutional layers,
as well as the control, ah,
the last hidden layers of the control layers.
And so both of these control the perception and
the control of the robot and what we do is we feed these through temporal convolutions.
So we- we have 1D convolutions across time and then we convolve this down, ah,
using 10 1D convolutions and then lastly,
six 1 by 1 convolutions.
And then we do a matrix normalization to get a scalar value.
And the goal is that when we update this loss,
when we- when we back prop on this loss,
we compute this inner gradient step.
It updates the parameters such that we're specific to this new task.
And then if you look back at this behavioral cloning loss that we talked about.
Since the behavioral loss- behavior cloning loss has
this implicit loss that we're learning embedded inside of this as an inner loss,
when we do this final gradient step on the behavioral cloning loss,
it also updates the weights of- of the Psi weights which are corresponding to this loss.
So we learn this loss through our meta learning procedure.
Yes. And now, let's talk about some of the- uh, here we go.
Let's talk about some of the [NOISE] experiments that are
being done that we're being compared to.
Um, so the first one is the contextual policy,
where all you're feeding in is the last, um,
image from a human demonstration,
um, to identify the task and the robot state.
Uh, the DA-LSTM policy was,
uh, talked about earlier, um, but it's,
um, it's basically you're getting, uh,
the full human video demonstration,
as well as the current robot stage break.
That current robot action and doing that over time steps, unrolling it.
Uh, DAML with the linear loss is the same thing as what we presented,
except instead of using the temporal loss, they just do, uh,
a single la- linear layer over the policy activations.
And then DAML temporal loss is the full network that we showed you before
with a full temporal learned adaptation loss, um, there.
So just some, uh,
I think it's great if we can visualize some of the results really quickly.
But the experiments that they were working with- um,
I'm gonna change its speed.
Um, so the first is, like, placing and you can compare,
like, as you can see,
they're using different, um,
objects and configurations at,
uh, meta-train and meta-test time.
And you can see that models like the LSTM and
contextual model were not able to perform nearly as well.
They misidentified the object,
or they were not able to control- even if they identified the object correctly,
they were not able to control the object they're replacing into the right spot.
Um, and they work with tasks like pushing, uh,
pick and place and just place where the object is already in the gripper or the hand.
But qualitatively, you can see a lot more struggling with contextual and LSTM.
And linear loss is pretty well,
but not as well as inputting the temporal loss. Uh, let's maybe look at it.
Brief, yes, so pick and place is when you actually have to pick up
the object and also then place it into the container.
Um, so moving on to the first experiment and truly understanding qualita- quantitatively.
So the first one is placing, pushing,
and pick and place using all three different, um, tasks.
Uh, but with all- comparing the four layers,
I mean, sorry, the four networks,
and you can see that, clearly,
the temporal loss and the meta-learned,
uh, adaptive objective was very useful,
and that there's sig- an extreme increase in accuracies in all three of these tasks.
So temporal formation across the human demonstrations is
obviously going to be very important as according to this table you can see.
Uh, the second experiment was the pushing task.
So we're just looking at one task of pushing,
but exploring like the different backgrounds that could be placed,
that you could use it, train it at
meta-train, and meta-test time.
So in all these experiments,
like they're varying the object from meta-train to meta-test time.
The objects that they're using, they have different sets of objects.
But now, here, we're really focusing on backgrounds.
So using the temporal loss,
the full model that we were talking about earlier,
when it operates on a scene background in meta-test time,
it has an 81.8% accuracy.
But on new backgrounds,
you can see that it's significantly lower, uh,
more than 10% difference approximately.
And actually, what's more interesting is if you analyze where the failures of,
er, the DAML occurs on the scene in novel backgrounds.
You can see that some of the failures will occur
from not being able to identify what the task is,
like not knowing which, uh,
like the visual features, like which,
um, object to place it in.
Uh, and some of the failures are- the second group of failures are even
if you know which object to place it in and you understand all the visual cues,
you can't control the object that you are placing
or pushing to move it to the right spot.
So- and in a background that's seen before,
you see that you have only 1 out of 33 failures in task identification
which makes sense because you've seen that background before.
But when you've seen new backgrounds,
then you see a lot more failure percentages in task identification,
which is probably because of the,
the different visual cues that you aren't used to.
And overall, the failures from control seem relatively straight,
uh, relatively constant across these three background experiments.
Um, and that probably makes sense because just controlling an object,
um, in different scenarios, um, it's difficult overall.
So it should not change much with the,
uh, introduction of a new background.
Um, and our third experiment is, uh,
the third experiment presented in the paper was placing- uh,
doing the placing task but using the Sawyer arm instead
of the other robotic configuration they had.
And the whole point of this was to test on a different,
um, completely different framework.
So this was like using
kinesthetic teaching instead of teleoperation for the outer loss.
Um, and you use complete different- they use
completely different robot here and a different form of robotic demonstration.
So you can see that there is still
a pretty good- success rate of 77.8% in placing.
So basically, what this shows is that you can generalize the,
uh, algorithm to different,
um, robotic configurations and different forms of supervision,
whether that be teleoperational or kinesthetic.
And the final experiment was really comparing the-
really understanding the role of the, um, temporal loss.
And here in this experiment,
they didn't do any domain shift.
It was on a simulation,
um, robotics simulation, and, uh,
this simulated the pushing task with, again,
like no domain shifts, so just really focusing on the loss.
And you can see that just using, um,
the temporal loss and the meta imitation learning versus a linear loss,
there was a large improvement of over,
again, like 13-14%.
So the temporal loss is really important to use, uh,
to really identify the task and visual cu- cues well.
[NOISE]
Yeah. Okay. So some of the strengths of the paper.
So we'll go over the strengths and then we'll do
a critique also on some of the limitations that we saw.
So the success is that we can do- we can learn new tasks
using this one shot imitation from a visual input of just a human demonstration.
And so like we talked about,
this as an extension of MAML such that you can, uh,
define, there's like this learned- like with
the addition of this learned temporal adaptation objective.
And so what they show is that they're able to update these parameters
for new tasks and they're able to do this with just one demonstration of a human,
of a human demonstrating that new task.
And that's actually pretty impressive and we found that to be a really good strength.
This idea of a learned temporal adaptation to incorporate temporal information to
understand behaviors and be able to translate
those to update not only the perceptual components,
but also the control components was also something we
found very interesting and a unique contribution to this paper.
And one of the things that we also mentioned as a strength,
but we'll also mention later again as a limitation,
is this idea that we can perform well even though we have,
uh, the amount of data per task is low.
So one of the things that happens is that they have thousands of
demonstrations that they use during meta training,
but the amount of data that they have per task is actually quite low.
And what this shows is the fact that they can perform well
even with having less amount of data per task,
is that they can actually generalize and adapt to a diverse range of tasks,
which is also something we counted as a strength.
So to discuss some of the limitations,
so one of the things that we saw is that they
have not demonstrated the ability to learn entirely new motions.
So one of the things you'll see is that during meta-training and meta-test time,
so the behaviors that you will encounter during meta-training are
structurally similar to the ones you'll account encounter during meta te- meta-test time.
And so it's not as if you're encountering
completely new tasks that have- that are structurally,
uh, completely different from those you'll encounter in meta-training.
And so one of the things that we, we thought would be very interesting to see is how,
how I would generalize to entirely new sorts of motions.
Uh, one of the other things is that we need more data during
meta-training time to enable- and we're
wondering if that could maybe lead to better results.
So one of the things that we mentioned as a strength can also be seen as
a limitation in the sense that getting more data per task might
also give us insight into how- maybe even how
the learned adapt- adaptative loss can improve on like motions that are more complex,
and we'll talk about that as well.
And then lastly, we still require robot demos.
So you'll see in meta-training time,
we paired the human demonstrations with robot demonstrations, uh,
in order to- in order to update these parameters and to,
and to learn from new tasks.
So one of the things that we were curious about is since
domain adaptation involves generalizing from different viewpoints,
different lighting conditions, and all sorts of domain shifts that might occur,
can we do this simply with just human demonstrations?
Is it possible to have, for example,
maybe a discriminator that can discriminate between varying lighting conditions,
uh, various domain shifts,
and maybe send back like have a gradient reversal layer such
that you can send back gradients that encourage an agnostic,
uh, representation of some of these things?
So it's just something to think about is, uh,
do we need these robot demos or can we do this even with just the human demonstrations?
Uh, so some of the discussions that we thought about, uh,
when we were going through this paper was,
how do we interpret this meta learned temporal adaptation objective?
So one of the things that they show is they show like obviously,
this meta learn- this meta-learned temporal adaptation objective
is beneficial and it learns important- important,
uh, semantic information regarding how to
update the perceptual components and the control components.
But it'd be really- it would be really interesting to see if we can
visualize this loss in terms of what it's actually learning.
If there's any- and one of the things that we leave open to
discussion is how could we make this loss more interpretable?
And then maybe how could this loss be used
in maybe other tasks that aren't robotic space?
Uh, lastly, can this approach be extended to tasks with more complex actions?
So we saw pushing,
we- we saw placing.
These are very simple tasks, uh, obviously complex.
But like can we, can we, can we increase this, uh, this approach?
Can we use this for tasks that might be more complex in nature,
such that it involve a myriad of things to happen before the task is completed.
So is, is this idea of a temporal adaptation loss
sufficient to be able to generalize to actions that are very complex.
So that's something we were also considering and so
we'll leave that open to discussion as well.
Thank you. [APPLAUSE]
Does anyone have questions? [BACKGROUND]
[inaudible] and so is a- what's the relationship between,
uh, the inner loss function and, uh,
to, uh, for example,
the reward function right from inward reinforcement learning?
Is there a connection between those?
You mean between this and just substituting this with-
With, uh, uh, so is,
is L5 somet- something similar to, uh,
a reward function, uh,
learned from reinforcement learning?
So I think what, what this learned adaptation objective is trying to
do is this idea of like looking at a human demonstration,
looking at like the- like the actions that are occurring across
multiple frames and trying to understand not only perceptually,
like which objects in the scene are important for a particular task,
so this would be like identifying the objects in the scene,
but then also thinking about how to like, uh,
update the control layers of the network to predict
actions that can maneuver those for the new task.
So I think this learned adaptation objective is,
is trying to look at that
temporally and then update these different components of the layers.
I can, I can see where like you could
update this with like some sort of reward function as well.
But I think at least since we don't have human actions or any sort of, uh,
any other type of supervision,
this is something that's learned through some of
the gradients that get passed through this outer objective as well.
So update data, so.
And you can't discretize like the human actions so that's why the loss to be learned.
That's the main difference a standard represent- I mean, for reinforcement learning.
Yeah. So this is like implicitly learned as well.
So this loss is learned during your meta-training.
So since you're updating these layers across
time and then when you do this behavioral cloning loss,
it also updates psi as the parameters of psi as well.
So this loss is being learned over time as a way to improve,
uh, improve this generalization to new tasks.
All right. Um, okay.
My name's Josh Melander, [NOISE] Uh,
I'm a third year neuroscience PhD student,
um, [NOISE] and yeah.
I'm going to be presenting on this paper, uh,
from Richard Zemel's lab.
Uh, it's called Meta-learning for Semi-supervi- uh,
Supervised few shot classification.
And, sorry, remind me how long do I have?
Like [OVERLAPPING] Fifteen minutes.
Fifteen minutes? Okay. [NOISE] Um, okay, all right.
So we all know that, uh,
recent years have sort of ushered in this great success,
um, of supervised class- classification problems.
Um, this is, you know, sort of uh,
engendered by this ImageNet data set,
uh, which is this massive labeled data set.
Um, but, uh, you know,
we know that in the real world and,
you know, the way that humans get data, it's,
it's, it's not this bolus of labeled data, right?
We have, um, maybe a few examples of, uh, labeled data,
maybe your mom tells you like that's a tree or something,
and then [NOISE] you have a bunch of unlabeled examples.
So you see a bunch of trees when you're walking through the forest.
Um, [NOISE] and so this is, this is sort of the,
the problem of semi-supervised learning.
Um, [NOISE] and uh, I guess,
you know as, as someone who's studying neuroscience,
I- I think it's a little bit more interesting than
pure supervised learning because, you know this is what,
what biological systems are sort of uh,
optimized to, to perform, [NOISE] right?
So, um, [NOISE] this paper,
basically applies this new domain of semi-supervised learning to, you know,
this familiar problem [NOISE] of, of uh,
multitask learning that we've been talking about and uh, right.
So to start off with the familiar problem that we all implemented, uh,
solution to in our homework, um,
you're presented with uh, support sets.
Uh, here we have a goldfish and a shark,
and then we get a new image and we have to basically say,
which one is it?, right?
And the whole point is that, uh, you know,
we need to come up with a solution that generalizes the unseen classes.
Um, [NOISE] now the,
the new twist on this problem is in the support set,
what if we're also given a bunch of unlabeled data, right?
So in addition to uh,
the labeled shark and goldfish,
we have a bunch of images of uh,
a bunch of other animals um, and that,
that could include or not include images of sharks and goldfish, right?
But the, the question is,
can we leverage uh,
er, can we leverage this sort of um,
[NOISE] massive amount of unlabeled data uh,
in order to improve our,
our, uh, um, multi-task learning, right?
So um, I- yeah,
I guess just to underscore this,
we would hope that because we're seeing more examples of images that we,
that we would be able to uh,
learn something from that.
[NOISE] Okay.
So what does this actually look like in terms of the, the task?
Um, so at the top here we have
our support set which everyone's familiar with and on the right is the query set.
Uh, so three image- three classes and we,
we see new examples of those three classes.
Um, but in addition,
we're also gonna present to our model uh,
this unlabeled data set.
And here the, the green plusses represents uh,
unlabeled data that uh,
that come from classes that are also part of the support test.
Um, so right, so we have like a little finch and this,
this white dog and a piano.
But then you can see there's also these uh,
these distractor classes which are labeled here with a red minus sign, all right?
[NOISE] Um, yeah. And so,
so we have training and testing and,
and those classes are disparate.
Um, [NOISE] and we- you know,
one thing that these authors wanted to get out is, you know,
not only can we leverage this unlabeled data but the presence of distractors,
[NOISE] presents kind of a particular challenge for
these networks and they're gonna try to present the way to overcome that challenge.
[NOISE] Um, yeah.
[NOISE] Okay.
So uh, this is just the setup of the problem, right?
And you can imagine that there are a lot of
ways that you could go about trying to tackle it.
Um, we've seen a lot of,
a lot of uh,
multitask learning and meta-learning solutions.
[NOISE] And um, I think any of these,
Siamese networks, matching networks,
prototypical networks, um, any of these would be fine starting points.
Um, they- the authors of this paper sort of choose to uh,
start from prototypical networks,
which I think were presented on last week, and uh,
extend those models to a regime that would work for semi supervised [NOISE] learning.
Um, okay. So just a,
a quick overview of the prototypical networks, um, right.
So the- these are, you know,
very relatively simple models um, where,
you know, each class basically gets embedded into some space.
And then new examples are then uh,
basically matched via some distance metric
to the embedding of previously seen classes, right?
So in this case,
you know, you have um,
a number- let's say we have five classes here in,
in purple and we're gonna use the seen classes to generate an embedding
and then new examples can be embedded with that same embedding and uh,
sort of classified based on its distance to this prototype.
So this, this vector here in this prototype vector is sort of generated from,
from labeled data in this sort of vanilla protoNets, right?
And uh, in the original protoNet paper and in this paper,
um, the embedding is actually performed with a pretty simple uh,
convolution neural network, um,
with three by three filters,
Batchnorm, ReLU, Maxpool and it results in a 64D vector.
So that's the sort of embedding space, [NOISE] right.
Okay. So you pass in a couple of images with known labels,
you compute uh, you compute your prototype.
[NOISE] Um, and then in the vanilla sort of prototypical network uh, examples, you um,
pass in your, your sort of uh,
your query set and just softmax over the distributions to the prototypes.
And then you can back-propagate the loss through
that embedding network and that's really the only thing that's trained,
um, is that embedding.
[NOISE] Um, right.
And so they,
they make a lot of points in this in the papers that it's,
it's a nice model because it has very,
very little assumptions and very simple inductive bias.
And it's so simple that actually in
the case of Euclidean distance which they use in this paper,
it, it, it reduces down to a linear model.
[NOISE] So very simple.
Okay. So now how do we extend this to the case of
unlabeled data and semi-supervised learning?
Um, so what they're gonna do is here,
here you have uh,
an example of the same kind of embedding plot as before.
Um, the, the dotted examples are unlabeled data,
and the white examples are your testing data.
So basically what they plan to do and what they will do in this paper is uh,
start out with the vanilla protoNet, embed um,
embed the, uh, embed the labeled data in and get the sort of labeled prototypes.
And then using the unlabeled data,
they're gonna refine those original prototypes to sort
of create new uh, classification boundaries.
And of course, this um,
[NOISE] this will be uh,
sort of more and less effective in the cases where you have
distractors and so we'll talk a little bit about how they deal with that.
Um, okay. So, so you start with
the labeled data that you're passing in and you're gonna generate the labeled prototypes,
um, but via the embedding which is just the mean of,
of, of your embedding for all of,
all of your examples.
Um, and then for each of the unlabeled inputs,
you're going to give them a partial assignment to each cluster um,
using some distance metric and then incorporate
those unlabeled data into the original prototype.
And now you have a new prototype that has incorporated the unlabeled data.
[NOISE] Um, and this is very simple [NOISE] and done using- uh,
sort of soft K-means, which is differentiable uh, K-means, right?
So um, basically if we have
this unlabeled support set that we're also passing in with the labeled uh, uh,
the labeled support set,
then what we can do is sort of assign each of these unlabeled data sets- uh,
each of these unlabeled data points to one of the classes.
This is just sort of a Softmax on the,
on the distance, um,
of the unlabeled data to each of
the original prototypes and then incorporate that into our new prototype.
So this is- it's just the prototypical networks but
their performing K-means on the unlabeled data and uh,
uh, using that to update their new prototypes.
[NOISE] Right? So um, yeah.
As I mentioned, this sort of breaks when
you think through the logic when you incorporate a distractor class,
and uh, that's because you don't wanna be
incorporating distractor classes into your [NOISE] updated prototypes.
So they offer two solutions for this.
The first is to just add another prototype at the origin, uh,
so 000 up to 64,
[NOISE] and this is just kind of like a buffer to capture all the distractions.
Um, but [NOISE] this sort of relies on this uh,
assumption that all of these distractors are coming from one class.
Which [NOISE] we know is not gonna be the case, right?
Um, you see lots of different classes that are confounding your inferences,
not just, just one, [NOISE] right?
So uh, to, to deal with this,
they come up with er,
also pretty simple strategy where they,
they do their soft means uh,
prototype update on the unlabeled data and then they also have a small uh, 20 neuron uh,
20 neuron wide uh,
single layer [NOISE] neural network that basically takes in the distance- s- some,
some sort of computed distance statistics.
So the min-max variance scheme part of each image to each class.
And it's gonna take these in and basically give a threshold to each prototype.
So um, yeah.
So, so now they have a differentiable way to,
to sort of uh,
set um, boundaries on how
aggressively the prototypes should be taking in the unlabeled data.
[NOISE] Uh, right and it's differentiable.
So they can just train this end to end. [NOISE] Um,-
And I- I'll come back to this in a bit,
uh, but it- it turns out to be quite useful.
Um, okay, so- so tha- that's the general strategy.
They have this- this original Protonet with K-means and, uh,
in order to deal with the distractor classes,
they have a little neural network that will
basically tell you how aggressively to incorporate the unlabeled data.
Um, now they- they test these results on
the Omniglot dataset as well as the miniImageNet dataset.
Uh, however, they also generate this new,
uh, split of ImageNet,
which they call tieredImageNet and,
uh, they- they seem to make a big deal about it in the paper.
But the idea is that it's sort of more organized based on
higher nodes in the sort of semantic classification of the network.
So whereas in miniImageNet you have
electric guitars in testing and acoustic guitars in training,
in the tieredImageNet, uh,
it's sort of more uh, diverse.
So- so your testing has musical instruments,
and your training has farming equipment.
So you're actually seeing completely unseen,
uh, [NOISE] classes of images.
Um, and basically across the board,
they're gonna take 10% of- of their, uh,
training split for labeled data and 90% goes to unlabeled classes.
And so it's important [NOISE] to point out that
because they're only labeling 10% of the data,
they're actually getting uh,
they're actually using much,
much less labels than sort of all of the previous work that we've been talking about.
[NOISE] Um, and all the results that I'm gonna show you, uh,
they are using- it's basically a five way classification,
and they are including five,
uh, five unlabeled, uh, examples as well.
Um, and in the case of distraction,
they're gonna be including five, uh,
unlabeled images, uh, from distractor classes as well.
[NOISE] Okay. So, uh,
a couple of baselines before I show you the results.
So first, they just have the Vanilla Protonet.
So ignore the- the unlabeled classes and just- just,
uh, do the Protonet thing.
And then the second is a sort of semi-supervised inference.
So this is the Vanilla Protonet,
the embedding generated from the Vanilla Protonet.
But now they're actually doing the K-means during training.
Uh, so this is a way to test how much
the embedding is- is changing with the unlabeled data.
And they find that, uh,
they- they're able to get sort of uns- unsurpassed accuracy with, um,
their, uh, K-means clustering on the Protonet as well as the, sort of,
this masked neural network, uh,
version of- of the K-means Protonets.
And on the- on the left is just with unlabeled data from the same classes.
On the right, you have unlabeled data, um, with distractors.
And what they find is that, across the board,
the- the sort of masked K-means Protonets do the best, um,
and especially they do the best across all three datasets in the distractor cases.
So, um, yeah.
[NOISE] There's a few other baselines they look at, uh,
just like a linear regression and sort of a, uh,
a one-way K-means with just the pixels and- but- I- I'm running out of time.
So I guess the last sort of novel thing that they found is that, um, you know,
if you- if during the training if you now- so- so you've trained this network,
um, to embed, and now during testing,
you're going to increase the amount of unlabeled data.
And what they find is that,
as you start increasing- so you've frozen the network,
you're no longer doing gradient updates.
But as you start increasing the amount of unlabeled data, um,
the model starts to sort of out-perform the classic Protonet example.
So what- what they're claiming here is that
these models learn to generalize from unlabeled data.
Um, in other words, uh,
you- basically, you would imagine that their performance would, uh,
increase, uh, as you took the number of
unlabeled examples and distractors out to infinity.
Um, and that's what they show here.
And then they also sort of make a point about the fact that they have this- this-
you can really start to separate the models in
the distractor case with the generalization.
Okay. So right.
They achieve state-of-the-art performance over
a couple logically proposed baselines on the three datasets.
Um, this K-means masked model performs best with the distractors, uh, and, uh,
they also show that their models are able to extrapolate
to sort of make- make more efficient use of new unlabeled data.
Um, and not that interesting,
but they- they also propose this sort of new benchmark dataset of this tieredImageNet.
[NOISE] Um, a- a few critiques.
Uh, I- I didn't have a lot of,
uh, sort of nitpicky critiques.
I- I think it was a relatively straightforward application
of this previous work from Protonets and,
uh, you know, the beloved K-means clustering.
Um, so they, it- it makes sense to use them and- and it worked, um.
I- I think, you know, they- they chose this Protonet model and, uh,
they claim that the reason they wanted to do this was because it
had very simple inductive bias, um,
but it's not really clear exactly what they gained from that because they didn't actually
do any- anything that required having a very interpretable model.
Um, and also I- I, you know,
compared to some of the other methods that everyone's presented,
I- I think this- this particular approach doesn't really generalize
very well beyond classification [NOISE] problems, um, yeah.
So, uh, I guess future directions,
I would be pretty interested to take this case to
the next extreme which is now you don't have any, um, labeled data.
And [NOISE] , uh,
so you're- you're just presented with images without labels.
And you have to sort of figure out which images- w- what- which new, uh,
query images correspond to which unlabeled, uh,
support images and, uh, right.
And so then, the model would need to not only learn how many classes there were,
but also, um, to correctly classify them.
And I- I think that there's
some pretty straightforward applications of some- some work done here in the Bay Area,
uh, sort of looking at unsupervised representations between images.
Um, so yeah, ah,
it'd be cool to apply that here.
Um, yeah. That's it.
[NOISE] Pretty simple paper. [APPLAUSE] [NOISE]
Questions? No? Okay.
[inaudible].
Yeah. Ca- can you just to expand, uh,
on the last thing you talked about like the future directions.
Yeah. So, uh, I- in- so this- Stella Yu,
uh, this- this paper is- is pretty cool and,
uh, you should check it out if you're not familiar.
But they basically, uh,
the idea is like you can pass in ImageNet with- without labels.
And they have this sort of embedding strategy where you
can embed each image into this 128D Unit Sphere.
And basically from that, without labels, um,
learn the- learn how many classes there are based on
the sort of di- differences between images and between classes, right?
So, um, it's- it's like a way of creating
an embedding space that separates out
different classes based on the sort of variability between,
um, between examples of the same class,
um, in a way that's completely unsupervised.
So no labels, and,
um, [NOISE] yeah. And-
And they come up with some- w- what I don't understand there is,
let's say, I've 10 classes [inaudible].
Yeah.
Like I- I could easily just for each class,
come up with the five subclass that- I- I don't understand how.
How can you learn anything general without having any classes?
Um.
[inaudible] separate things but-
Yeah.
Yeah, did you understand my question, there?
Um, how can you learn anything without having-
[inaudible] ImageNet now.
Le- let's say you have a dataset, there are 10 classes.
Yeah.
The model managed to pick out those 10 classes unsupervised.
Yeah.
But I could go in now and say, ''No,
actually this dataset has 10- 20 classes.''
Like, number of classes that you sort of as a human,
um, in a way like we- we decide how many classes are there.
Yes.
[inaudible] something objective really about-
Yeah.
-how many classes are in here.
Yeah. No, I- I think that's a- I think that's a- a good point.
Other than the fact that there is something objective, uh, you know.
Yeah.
There- there is something objective in that these
are examples of specific classes of things.
And, like, each class has its own set
of- like its own distribution of relationships within a class.
So I agree with that. I'm just thinking about then a class you could say,
''Oh, within this class you have 10 classes of cats.''
Yeah.
[inaudible]. [NOISE]
Yeah. It's a- yeah,
I think that's- that's- that's sort of more of a critique on- on this work-
Yeah- yeah- yeah. that [OVERLAPPING].
But- but, yeah, it's a- it's a good point. Yeah.
Thank you.
Yeah, thanks.
