 Welcome to CS229 Machine Learning.
Uh, some of you know that this class has been taught at Stanford for a long time.
And this is often the course that, um,
I most look forward to teaching each year because this is where we've helped I think,
several generations of Stanford students become experts in machine learning, go on to build
many of their products and services and startups that I'm
sure many of you are pre- or all of you are using, uh, uh, today.
Um, so what I want to do today was spend some time talking over, uh, logistics,
and then, uh, spend some time, you know,
giving you a beginning of a intro,
talk a little bit about machine learning.
So about 229.
Um, you know,
all of you have been reading about AI in the news,
uh, about machine learning in the news.
Um, and you probably heard me or others say, AI is the new electricity.
Uh, the emergence and rise of electricity about 100 years ago,
it transformed every major industry.
I think AI already we call machine learning for the rest of the world seems to call AI.
[NOISE]
Um, machine learning and,
and AI and deep learning will change the world.
And I hope that through 229,
we'll give you the tools you need so that you can be many of
these future titans of industries that you can be one to go out and build,
you know, help the large tech companies do the amazing things they do,
or build your own start-up,
or go into some other industry.
Go, go transform healthcare or go
transform transportation or go build a self-driving car.
Um, and do all of these things that,
um, after this class,
I think you'll be able to do.
You know, um, the majority of students supplying- the,
the demand for AI skills- the demand for machine learning skills is so vast.
I think you all know that.
Um, and I think it's because machine learning has advanced
so rapidly in the last few years that there are so many opportunities,
um, to apply the learning algorithms, right?
Both in industry as well as in academia.
I think today, we have, um,
the English department professors trying to
apply learning algorithms to understand history better.
Uh, we have lawyers trying to apply machine learning into
process legal documents and off-campus, every company,
both the tech companies as well as lots of
other companies that you wouldn't consider tech companies,
everything from manufacturing companies, to healthcare companies,
to logistics companies are also trying to apply machine learning.
So I think that, um, uh, uh,
if you look at it on a- on a factual basis,
the number of people doing
very valuable machine learning projects today is much greater than it was six months ago.
And six months ago is much greater than it was 12 months ago.
And the amount of value,
the amount of exciting meaningful work being done in machine learning is,
is, is very strongly going up.
Um, and I think that given the rise of, you know, the,
the amounts of data we have as well as the new machine learning tools that we have,
um, it will be a long time before we run out of opportunities.
You know, before, before society as
a whole has enough people with the machine learning skill set.
Um, so just as maybe, I don't know,
20 years ago was a good time to start working on this Internet thing and
all people that started working on the Internet like 20 years ago have fantastic careers.
I think today is a wonderful time to jump into machine learning, uh, and, and,
and the number of- and the opportunities for you to
do unique things that no one has- no one else is doing, right?
The opportunity for you to go to a logistics company and
find that exciting way to apply machine learning, uh,
will be very high because chances are
that logistic company has no one else even working on this.
Because, you know, they probably can't- they,
they may not be able to hire a fantastic Stanford student that's a graduate of CS229, right?
Because there just aren't a lot of CS229 graduates around.
Um, so what I want to do today is,
um, do a quick intro talking about logistics.
Um, and then uh, we'll,
we'll spend the second half of the day, you know,
giving an overview and,
and talk a little bit more about machine learning.
Okay? And uh- oh, and I apologize.
I- I think that, uh, this room,
according to that sign there, seats,
what, 300 and something students.
Uh, I think- we have, uh,
uh, like not quite 800 people enrolled in this class.
[LAUGHTER] Uh, so there are people outside, and,
and all of the classes,
uh, are recorded and broadcast in SCPD.
Uh, they usually- the videos are usually made available the same day.
So for those who they can't get into the room, my apologies.
Um, the- there were some years, um,
where even I had trouble getting into the room but I'm glad [LAUGHTER] they let me in.
But, but I'm- but, but hopefully, you can watch.
You, you can watch all of these things online shortly.
[inaudible].
Oh, I see. Yes. Yeah. [LAUGHTER] I don't know,
it's a bit complicated.
[LAUGHTER] Yeah. Thank you.
I think it's okay. Yeah. I- I- okay, yeah.
Yeah. Maybe for the next few classes you can squeeze in and use the NTC.
So for now, it might be too complicated.
Okay. So quick intros, um,
uh, I'm sorry, I should have introduced myself. My name is Andrew Ng.
[LAUGHTER] Uh, uh, uh, and I wanted to introduce
some of the rest of the teaching team as well.
There's a class coordinator.
Um, she has been playing this role for many years now and helps
keep the trains run on time and make sure that everything in
class happens when it's supposed to.
Uh, uh, so, so, so should be uh- and then, uh,
we're thrilled to have-
Do you guys want to stand up?
Uh, we have the co-head TAs,
uh, respectively are PhD students working with me.
Uh, and so bringing a lot of, um, uh,
technical experience, uh, technical experience in
machine learning as well as practical know-how on how to actually make these things work.
And with the large class that we have,
we have a large TA team.
Um, maybe I won't introduce all of the TAs here today
but you'll meet many of them throughout this course here.
But the TAs expertise span everything from computer vision and natural language processing,
to computational biology, to robotics.
And so, um, through this quarter,
as you work on your class projects,
I hope that you get a lot of, uh,
help and advice and mentoring from the TAs, uh,
all of which- all of whom have deep expertise not just in
machine learning but often in a specific vertical application area,
um, of machine learning.
So depending on what your projects,
we tried to match you to a TA that can give you advice, um, eh,
the most relevant, uh,
whatever project you end up working on.
Um, so yeah, goal of this class,
I hope that after the next 10 weeks,
uh, you will be an expert in machine learning.
Um, it turns out that, uh,
uh, you know, um,
and- and I hope that after this class,
you'll be able to go out and build very meaningful machine learning applications,
uh, either in an academic setting where, uh,
hopefully you can apply it to your problems in mechanical engineering,
electrical engineering, and, uh, English,
and law and, um, uh,
and- and- and education and all of this wonderful work that happens on campus,
uh, as well as after you graduate from Stanford
to be able to apply it to whatever jobs you find.
Um, one of the things I find very exciting about machine learning is that it's no
longer a sort of pure tech company only kind of thing, right?
I think that many years ago,
um, machine learning, it was like a thing that, you know,
the computer science department would do and that
the elite AI companies like Google and Facebook and Baidu and Microsoft would do.
Uh, but now, it is so pervasive that even companies that are
not traditional because there are tech companies see a huge need to apply these tools,
and I find a lot of the most exciting work, uh, these days.
Um, and, and, and maybe some of you guys know my history some would be biased, right?
I- I led the Google Brain team which helped Google transform
from what was already a great company 10 years ago to today which is,
you know, a great AI company.
And then I also led the AI group at Baidu,
and, you know, led the company's technology strategy to help Baidu.
Also, it transformed from what was already a great company
many years ago to today arguably China's greatest AI company.
So having led the, you know,
built the teams that led the AI transformations of two large tech companies,
I, I, I feel like that's a great thing to do.
Uh, but even beyond tech, I think that, um,
there's a lot of exciting work to do as well to help other industries,
to help other sectors, uh,
embrace machine learning and use these tools effectively.
Um, but after this class,
I hope that each one of you will be well qualified to
get a job at a shiny tech company and do machine learning there,
or go into one of these other industries and do
very valuable machine learning projects there.
Um, and in addition,
if any of you, um,
are taking this class with the primary goal of, uh,
being able to do research,
uh, in machine learning,
so, so, actua- so some of you I know are PhD students.
Um, I hope that this class will also leave you well-equipped to,
um, be able to read and understand research papers, uh,
as well as, uh, you know,
be qualified to start pushing forward,
um, the state of the art, right.
Um, so let's see.
Um, so today, uh,
so, so just as machine learning is evolving rapidly, um,
the whole teaching team,
we've been constantly updating CS229 as well.
So, um, it's actually very interesting.
I feel like the pace of progress in machine learning has accelerated,
so it, it actually feels like that,
uh, the amounts we changed the class year over year has been increasing over time.
So- so for your friends who took the class last year, you know,
things are a little bit different this year because
we're, we're constantly updating the class
to keep up with what feels like still
accelerating progress in the whole field of machine learning.
Um, so, so, so, so there are some logistical changes.
For example, uh, uh, we've gone from- uh,
what we used to hand out paper copies of handouts,
uh, that we're, we're trying to make this class digital only.
Uh, but let me talk a little bit about, uh,
prerequisites as well as in case your friends have taken this class before,
some of the differences for this year, right?
Um, so prerequisites.
Um, we are going to assume that,
um, all of you have a knowledge of basic computer skills and principles.
Uh, so, you know, Big O notation,
queues, stacks, binary trees.
Hopefully, you understand what all of those concepts are.
And, uh, assume that all of you have a basic familiarity with,
um, uh, probability, right?
Hopefully, you know what's a random variable,
what's the expected value of a random variable,
what's the variance of a random variable.
Um, and if- for some of you,
maybe especially the SCPD students taking this remotely,
it has been, you know,
some number of years since you last had a probability and statistics class.
Uh, we will have review sessions, uh,
on, on, on Fridays, uh,
where we'll go over some of this prerequisite material as well. But it's okay.
Hopefully, you know what a random variable is,
what expected value is.
But if you're a little bit fuzzy on those concepts,
we'll go over them again,
um, at a- at a discussion section, uh, on Friday.
Also, seem to be familiar with basic linear algebra.
So hopefully that you know what's a matrix,
what's a vector, how to multiply two matrices and multiplying matrices and a vector.
Um, if you know what an eigenvector then that's even better.
Uh, if you're not quite sure what an eigenvector is,
we'll go over it that- that you, you, you better-
uh, yeah, we'll, we'll, we'll go over it I guess.
And then, um, a large part of this class, uh, uh, is,
um, having you practice these ideas through the homeworks,
uh, as well as I mention later a, uh, open-ended project.
And so, um, one,
uh, there are- we've actually, uh,
until now we used to use uh MATLAB,
uh and Octave for the programming assignments, uh,
but this year, we're trying to shift the programming assignments to, uh,
Python, um, and so,
um, I think for a long time, uh,
even today, you know,
I sometimes use Octave to prototype because the syntax in Octave is so nice and just run,
you know, very simple experiments very quickly.
But I think the machine learning world, um, is,
you know, really migrating I think from a
MATLAB Python world to increasing- excuse me,
MATLAB Octave world to increasingly a Python
maybe and- and then eventually for production Java or C++, kind of world.
And so, uh, we're rewriting a lot of the assignments
for this class this school year.
Having, having driving that process,
uh, so that- so that this course,
uh, you could do more of the assignments.
Uh, uh, maybe most- maybe all of the assignments in,
um, Python, uh, NumPy instead.
Um, now, a note on the honor codes,
um, we ask that, you know,
we, we actually encourage you to form study groups.
Uh, so, so you know I've been um, fascinated by education, a long time.
I spent a long time studying education and pedagogy and how instructors
like us can help support youth to learn more efficiently.
And one of the lessons I've learned from
the educational research literature is that the highly technical classes like this,
if you form study groups, uh,
you will probably have an easier time, right?
So, so CS229, we go for the highly technical material.
There's a lot of math, some of the problems are hard and if you have
a group of friends to study with, uh,
you probably have an easier time uh, uh,
because you can now ask each other questions and work together and help each other.
Um, where we ask you to draw the line or what we ask
you to, to, to do relative to Stanford's,
uh, Honor Code is, um,
we ask that you do the homework problems by yourself, right?
Uh, and, and, and most specifically, um,
it's okay to discuss the homework problems with friends,
but if you, um,
but after discussing homework problems with friends,
we ask you to go back and write out the solutions by yourself, uh,
without referring to notes that,
you know, you and your friends had developed together, okay?
Um, the class's honor code is written clearly
on the class handouts posted digitally on the website.
So if you ever have any questions about what
is allowed collaboration and what isn't allowed, uh,
please refer to that written document on
the course website where we describe this more clearly, but,
um, out of respect for the Stanford honor code as well as for,
uh, uh, you know,
for, for, for students kind of doing their own work,
we asked you to basically do your own work, uh,
for the- it's okay to discuss it,
but after discussing homework problems with friends, ultimately,
we ask you to write up your problems by yourself so that
the homework submissions reflect your own work, right?
Um, and I care about this because it turns out that having CS 229,
you know, CS 229 is one of those classes that employers recognize.
Uh- uh, I don't know if you guys know,
but there have been, um,
companies that have put up job ads that say stuff like,
"So long as you've got- so long as you completed
CS 229 we guarantee you get an interview," right?
[LAUGHTER] I've- I've seen stuff like that.
And so I think you know in order to, to maintain
that sanctity of what it means to be a CS 229 completer,
I think, um, and I'll ask all of you so that- really do your homework.
Um, or stay within the bounds of acceptable,
acceptable collaboration relative to the honor code.
Um, let's see.
And I think that um, uh,
if- uh, you know what?
This is, um, [NOISE] yeah.
And I think that, uh,
one of the best parts of CS 229,
it turns out is, um, excuse me.
So I'm trying, sorry, I'm going to try looking for my mouse cursor.
Uh, all right. Sorry about that.
My- my- my displays are not mirrored.
So this is a little bit awkward.
Um, so one of the best parts of the class is- oh,
shoot. Sorry about that.
[LAUGHTER]
All right, never mind. I won't do this.
Um, you could do, you could do it yourself online later.
Um, yeah, I started
using- I started using Firefox recently in addition to Chrome here. It's a mix up.
Um, one of the best parts of, um,
the class is, um, the class project.
Um, and so, you know,
one of the goals of the class is to leave you
well-qualified to do a meaningful machine learning project.
And so, uh, one of the best ways to make sure you have that skill set
is through this class and hopefully with the help of some of our TAs.
Uh, we wanna support you to work on
a small group to complete a meaningful machine learning project.
Um, and so one thing I hope you start doing, you know,
later today, uh, is to start brainstorming maybe with your friends.
Um, some of the- some of the class projects you might work on.
Uh, and the most common class project that, you know,
people do in CS 229 is to pick an area or pick an application that excites you and
to apply machine learning to it and see if you can
build a good machine learning system for some application area.
And so, um, if you go to the course website, you know,
cs229.stanford.edu and look at previous year's projects,
you ha- you, you see machine learning projects applied to pretty much,
you know, pretty much every imaginable application under the sun.
Everything from I don't know,
diagnosing cancer to creating art to, uh,
lots of, um, uh, projects applied to other areas of engineering, uh,
applying to application areas in EE,
or Mechanical engineering, or Civil engineering,
or Earthquake engineering, and so on, uh,
to applying it to understand literature,
to applying it to um, uh, I don't know.
And, and, and, and, and so, uh,
if you look at the previous year's projects
of many of which are posted on the course website.
You could use that as inspiration to see the types of projects students complete,
completing this class are able to do and also encourage you to, um, uh,
you can look at that for inspiration to get
a sense of what you'll be able to do at the end-
conclusion of this class and also see if looking at
previous year's projects gives you inspiration for what,
um, you might do yourself.
Uh, so we asked you to- we, we invite you I guess to
do class projects in small groups and so, um,
after class today, also encourage you to start
making friends in the class both for the purpose of forming
study groups as well as with the purpose of maybe finding
a small group to do a class project with.
Um, uh, we asked you to form project groups of,
um, up to size three.
Uh, uh, most project groups end up being size two or three.
Um, if you insist on doing it by yourself,
right without any partners that's actually okay too.
You're welcome to do that.
But, uh, but- but I think often, you know,
having one or two others to work with may give you an easier time.
And for projects of exceptional scope,
if you have a very very large project,
that just cannot be done by three people.
Um, uh, sometimes, you know,
let us know and we're open to- with, with to some project groups of size four,
but our expectation- but we do hold projects, you know,
with a group of four to a higher standard than projects with size one to three, okay.
So- so what that means is that if your project team size is one,
two or three persons,
the grading is one criteria.
If your project group is bigger than three persons,
we use a stricter criteria when it comes to grading class projects.
Okay. Um, and that, that reminds me um,
uh, I know that uh- let's see.
So for most of you since this- since this
started 9:30 AM on the first day of the quarter,
uh, for many of you,
this may be- this may be your very first class at Stanford.
How many of you, this is your very first class at Stanford?
Wow. Cool. Okay. Awesome. Great. Welcome to Stanford.
[LAUGHTER]
Uh, and if someone next to
you just raise their hand- uh actually, rai- raise your hand again.
So I hope that, you know, maybe after class today,
if someone next to you raised a hand, uh,
help welcome them to Stanford, and
then, say hi and introduce yourself and make friends on the way.
Yeah. Cool. Nice, nice to see so many of you here.
Um.
[NOISE]
All right. So um,
just a bit more on logistics, uh-
So, um, let's see,
in addition to the main lectures that we'll have here,
uh, on Mondays and Wednesdays,
um, CS229 also has discussion sections, uh,
on- held on Fridays that are- and everything we do including the- all the,
all the lectures and discussion sections are recorded and broadcast through SCPD,
uh, through the online websites.
Um, and one of- and,
uh, discussion sections are taught,
uh, usually by the TAs on Fridays and attendance at discussion sections is optional.
Uh, and what I mean is that, um, you- you know,
you- 100% promise,
there won't be material on the midterm that will sneak in from this kind of section.
So it's 100% optional.
Uh, and you will be able to do all the homework and
the projects without attending the discussion section.
But what we'll use the discussion section for,
uh, for the first three discussion sections.
So, you know, this week, next week, uh, the week after that,
we'll use the discussion sections to go over prerequisite material in greater depth.
So, uh, go over, uh,
linear algebra, basic probability statistics,
teach a little bit about Python NumPy in
case you're less familiar with those frameworks.
Uh, so we'll do that for the first few weeks.
And then for the discussion sections that are held later this quarter,
we'll usually use them to go over more advanced optional material.
Uh, for example, um,
CS229, most of the learning algorithms,
you- you hear about in a class rely on convex optimization algorithms,
but we want to focus the class on
the learning algorithms and spend less time on convex optimization.
So you want to come and hear about more advanced concepts in convex optimization.
We'll defer that to the discussion section.
Uh, and then, there, there are few other advanced topics,
uh, Hidden Markov Models, time series,
uh, that we're planning to defer to the,
um, Friday discussion sections.
Okay. Um, so, uh, let's see.
Um, cool, and, uh, and,
um, a final bit of logistics, um, uh,
for- there are digital tools that some of you have seen, but, um,
for this class, we'll drive a lot of the discussion through the,
uh, online website Piazza.
How- how many of you have used Piazza before? Okay, cool, mostly.
Wow, all of you?
That's very amazing. Uh, good.
So, so, uh, online discussion board
for those of you that haven't seen it before, but, um,
I definitely encourage you to participate actively
on Piazza and also to answer other student's questions.
I think that one of the best ways to learn as well as contribute, you know,
back to the class as a whole is if you see someone else ask a question on Piazza,
if you jump in and help answer that, uh,
that, that often helps you and helps your classmates.
I strongly encourage you to do that.
For those of you that have a private question,
you know, sometimes we have students, um, uh,
reaching out to us to- with a personal matter or something that, you know,
is not appropriate to share in a public forum in which case you're welcome
to email us at the class email address as well.
Uh, and we answer in, in the class email address- the cla-
teaching staff's email address on the course website,
you can find it there and contact us.
But for anything technical,
anything reasonable to share with the class, uh,
which includes most technical questions and most logistical questions, right?
Questions like, you know,
can you confirm what date is midterm,
or, or, you know, what happens?
Uh, can you confirm when is the handout for this going out and so on?
For questions that are not personal or private in nature,
I strongly encourage you to post on Piazza rather than emailing us because statistically,
you actually get a faster answer, uh,
posting it on- post- posting on Piazza than- than, you know,
if you wait for one of us to respond to you, um,
and we'll be using Gradescope as well,
um, to- for, for online grading.
And then, if, if you don't know what Gradescope is, don't worry about it.
We'll, we'll, we'll send you links and show you how to use it later.
Um, oh, and, uh, again,
relative to- one last logistical thing to plan for, um,
unlike previous, um, uh,
years where we taught CS229,
uh, so we're constantly updating the syllabus, right?
The technical content to try to show you the latest machine learning algorithms, uh,
and the two big logistical changes we're making this year,
I guess one is, uh,
Python instead of MATLAB,
and the other one is, um,
instead of having a midterm exam, you know,
there's a timed midterm, uh,
we're planning to have a take-home midterm, uh, this course, instead.
So I, I know some people just breathed in sharply when I said that.
[LAUGHTER] I don't know what that means.
[LAUGHTER] Was that shock or happiness? I don't know.
Okay. Don't worry, midterms are fun.
You- you'll, you'll love it.
[LAUGHTER]
All right.
So that's it for the- that's it for the logistical aspects.
Um, let me check with the- so let- let me check if there are any questions.
Oh, yeah, go ahead.
On campus, are those courses offered every quarter [inaudible].
Yeah. So that's interesting. Uh, let's see.
I think it's offered in spring.
And one other person. Oh, yes, is teaching it.
So someone else is teaching it in spring quarter.
Um, uh, I actually did not know it was gonna be offered in winter.
[inaudible]
Yeah.
[inaudible].
Yeah, right, yeah. So- so I think a free guide and teaching it in- sorry,
in their [inaudible] and you are right, are teaching it in, uh,
spring, uh, and I don't think it is offered in winter.
[inaudible].
Will the session be recorded? Yes, they will be.
Oh, and by the way, if, if, if you wonder
why I'm recording that I'm repeating the question,
I know it feels weird, I'm recording with a microphone,
so that- so that people watching this at home can hear the question.
But, uh, both the lectures and the discussion sections,
uh, will be- will be recorded and put on the website.
Uh, maybe the one thing we do that's not
recorded and broadcast are the office hours. Great. Isn't that right?
[LAUGHTER]
Oh, oh, but,
uh, I think, uh, this year,
uh, we have a 60-hour, how many hour?
Well, 60 office hours.
Uh, 60 office hours per week. Right, yeah.
[LAUGHTER] So- so- so hopefully,
I- I just again,
we- we're constantly trying to improve the course.
In previous years, one of the feedback we got
was that the office hours were really crowded.
So- so we have 60 hour- 60 hours,
about 60 office hour slots per week this year. That- that seems like a lot.
So hopefully, if you need to track down one of us,
track down a TA to get help, hopefully,
that- that'll make it easier for you to do so. Go ahead.
[inaudible].
Say that again. Well-
[inaudible].
Oh, well logistical things like when homeworks are due, would be covered in lectures.
Uh, we have uh, yes,
so we have uh, four planned homeworks. Oh sorry.
[inaudible]
Yeah, and if you go to the- if you go to
the course website and you click on the syllabus link uh,
that has a calendar with when each homework assignments go out and when they'll be due.
Uh, so four homeworks and uh,
project proposals due a few weeks from now and uh,
final projects due at the end of the quarter.
But all the, all the exact days are listed on the course website I think.
[inaudible]
Uh, sure yes, difference between this class and 229a.
Um, let me think how to answer that.
Yes. Uh, so yeah I know,
I was debating earlier this morning how to
answer that because I've been asked that a few times.
Um, so I think that what has happened at Stanford is that
the volume of demand for machine learning education is just,
right skyrocketing because anything everyone sees,
everyone wants to learn this stuff and so um,
uh, so within- so
the computer science department has been trying to grow
the number of machine learning offerings we have.
Um, uh, we actually kept the enrollments to
CS229a at a relatively low number at 100 students.
So I actually don't want to encourage too many of you to sign up because uh,
I think we might be hitting the enrollment cap already so,
so please don't all sign up for CS229a because um,
we- CS229a, does not have the capacity this quarter but since CS229a is uh, um,
much less mathematical and much more applied,
uh, uh, a relatively more applied version of machine learning and uh,
so I, I guess I'm teaching CS229a and CS230 and CS229, this quarter.
Of the three, CS229,
is the most mathematical.
Um, it is a little bit less applied than
CS229a which is more applied machine learning and CS230 which is deep learning.
My advice to students is that um,
CS229, uh, CS229a,
excuse me, let me write this down.
I think I'm-
so CS229a, uh,
is taught in a flipped classroom format which means that, uh,
since taking it, we'll mainly watch videos um,
on the Coursera website and do a lot of uh,
programming exercises and then, meet for weekly discussion sections.
Uh, but there's a smaller class with [inaudible] .
Um, I, I would advise you that um,
if you feel ready for CS229 and CS230 to do those uh, but CS229,
you know, because of the math we do, this is a,
this is a very heavy workload and pretty challenging class and so,
if you're not sure you're ready for CS229 and CS229a,
it may be a good thing to,
to, to take first, uh,
and then uh, CS229, CS229a
cover a broader range of machine learning algorithms uh,
and CS230 is more focused on deep learning algorithms specifically, right.
Which is a much narrow set of algorithms but it is,
you know, one of the hardest areas of deep learning.
Uh, there is not that much overlap in content between the three classes.
So if you actually take all three,
you'll learn relatively different things from all of them uh, in the past,
we've had students simultaneously take 229 and 229a and there is a little bit of overlap.
You know, they, they do kind of cover
related algorithms but from different points of view.
So, so some people actually take multiple of these courses at the same time.
Uh, but 229a is more applied, a bit more,
you know practical know-how hands-on and so on and,
and uh, much less mathematical.
Uh, and, and CS230 is also less
mathematical more applied more about kind of getting it to work where CS229a,
um, we do much more mathematical derivations in CS229.
Cool, any questions? Yes, someone had their hand up.
[inaudible]
So uh, once you say that what- I would
generally prefer students not do that in the interest of time but what, what do you want?
[inaudible]
Oh, I see, sure go for it.
Who is enrolled in 229 and 230?
Oh not that many of you, interesting.
Oh, that's actually interesting.
Cool. Yeah. Thank you, yeah, I just didn't want to set
the presence of students using this as a forum to run surveys.
[LAUGHTER]
That was, that was, that was,
that that was an interesting question.
So thank you.
[LAUGHTER]
Um, cool.
All right, and, and by the way I think uh, you know,
just one thing about Stanford is the
AI world and machine learning world,
AI is bigger than machine learning right
and machine learning is bigger than deep learning.
Um, one of the great things about being a Stanford student is,
you can and I think should take multiple classes, right.
I think that your CS229,
has for many years been the core of the machine learning world at Stanford.
Uh, but even beyond CS229,
it's worth your while to take multiple classes and getting multiple perspectives.
So, so if you want to uh,
be really effective, you know,
after you graduate from Stanford,
you do wanna be an expert in machine learning.
You do wanna be an expert in deep learning.
Uh, and you probably wanna know probability statistics.
Maybe you wanna know a bit of convex optimization, and maybe you
wanna know a bit more about reinforcement learning,
know a ittle bit about planning,
know a bit about lots of things.
So, so I actually encourage you to take multiple classes I guess.
Cool. All right.
Good. Um, if there are no more questions,
let's go on to talk a bit about some machine learning.
So um, all right,
so the remainder of this class,
what I'd like to do is um,
give a quick overview of uh,
you know, the major uh,
areas of machine learning and also um,
and, and also give you a sort of overview of the things you learn uh,
in the next 10 weeks.
So, you know, what is machine learning?
Right. It seems to be everywhere these days and it's useful for so many spaces,
and, and I think that um,
and uh, you know, and uh, uh,
and I, I feel like they uh- just to share with you my personal bias, right.
You, you read the news about these people who are making
so much money building learning algorithms. I think that's great.
I hope, I hope all of you go make a lot of money
but the thing I find even more exciting is,
is the meaningful work we could do.
I think that, you know,
I think that every time there's a major technological disruption which there is now,
through machine learning um,
it gives us an opportunity to remake large parts of the world and if we behave ethically
in a principled way and use the superpowers of machine learning to do things that,
you know, helps people's lives, right.
Maybe we could um, uh,
maybe you can improve the healthcare system,
maybe you can improve give every child a personalized tutor.
Uh, maybe we can make our democracy run better rather than make it run worse.
But I think that um,
the meaning I find in machine learning is that there's so many people that
are so eager for us to go in and help them with these tools that um,
if, if you become good at these tools,
it gives you an opportunity to really remake some piece,
some meaningful piece of the world.
Uh, hopefully in a way that helps other people and makes the world kind of,
makes the world a better place is very cliche in Silicon Valley.
But, but I think, you know, with these tools,
you actually have the power to do
that and if you go make a ton of money, that's great too.
But I find uh, much greater meaning of the work we could do.
Um, it gives us a unique opportunity to do these things, right.
But um, despite all the excitement of machine learning.
What is machine learning?
So let me give you a couple um,
definitions of machine learning.
Um, Arthur Samuel whose claim to fame was uh,
building a checkers playing program,
uh, defined it as follows.
So field of study gives computers the ability to learn without being explicitly programmed.
Um, and you know interesting- when,
when Arthur Samuel many,
many decades ago, built
the checkers playing program.
Uh, the debates of the day was can a computer ever do something
that it wasn't explicitly told to do? And Arthur Samuel uh, wrote a
checkers playing program, that through self play learns whether the patterns of uh,
the checkerboard that are more likely to lead to win
versus more likely to lead to a loss and learned uh,
to be even better than Arthur Samuel the author himself at playing checkers.
So back then, this was viewed as a remarkable result that a computer programmer,
you know that could write a piece of
software to do something that the computer program himself
could not do, right, because this program became better than Arthur Samuel um,
at, uh, uh, at,
at, at the task of playing checkers.
Um, and I think today we um, are
used to computers or machine learning algorithms outperforming humans on so many tasks.
Uh, but it turns out that when you choose a narrow task like,
speech recognition on a certain type of task,
you can maybe surpass human level performance.
If you choose a narrow task like,
playing the game of Go,
than by throwing really,
tons of computational power at it and self play.
Uh, uh, uh you can have a computer,
you know become very good at,
at these narrow tasks.
But this is maybe one of the first such examples in the history of computing. Um.
Uh, and I think this is the one of the most widely cited, um, definitions right.
Gives computers the ability learn without being explicitly programmed.
Um, my friend Tom Mitchell in his textbook,
defined this as a Well-posed Learning Problem.
Uh, a program is said to learn from
experience E with respect to task T and some performance measure P,
if its performance on T, as measured by P,
improves with experience E. And I- I asked Tom this.
I asked Tom if, um,
he wrote this definition just because he wanted it to
rhyme and [LAUGHTER] he, he, he, he, he did not say yes,
but I, I, I don't know.
Um, but in this definition,
the experienced E. For- for the case of playing checkers,
the experience E would be the experience of having
a checkers play- program played tons of games against itself.
Uh, so computers lots of patients and
sit there for days playing games or checkers against itself.
So that's experience E.
The task T is the task of playing checkers,
the performance measure P maybe,
um, what's the chance of this program
winning the next game of checkers it plays against the next opponent.
Right. So- so we say that, ah,
this is a well-posed learning problem, learning the game of checkers.
Now, within this, um,
set of ideas with machine learning,
there are many different tools we use in machine learning.
And so in the next 10 weeks,
you'll learn about a variety of these different tools.
Um, and so the first of them and the most widely used one is supervised learning.
Um, let's see.
I wanna switch to the white board.
Do you guys know how to raise the screen?
[NOISE]
So what I wanna do today is really go over some of the major categories of,
uh, Machine Learning Tools, and, uh,
and so what you learn in the next,
um, ah, by the end of this quarter.
So the most widely used machine learning tool is,
uh, today is supervised learning.
Actually, let me check, how, how many of you know what supervised learning is?
Ah, like two-thirds, half of you maybe.
Okay cool. Let me, let me just briefly define it.
Um, here's one example.
Let's say, you have a database of
housing prices and so I'm gonna plot your dataset where on the horizontal axis,
I'm- I'm gonna plot the size of the house in square feet.
And on the vertical axis,
we'll plot the price of the house.
Right. And, um, maybe your dataset looks like that.
Um, and so horizontal axis,
I guess we'd call this X and vertical axis we'll call that Y.
So, um, the supervised learning problem is given a dataset
like this to find the relationship mapping from X to Y.
And so, um, for example,
let's say- let's say- let's say you have- let's
say you're fortunate enough to own a house in Palo Alto.
Right. Ah, and you're trying to sell it,
and you want to know how the price of the house.
So maybe your house has a size,
you know, of that amount on the horizontal axis.
I don't know, maybe this is 500 square feet,
1,000 square feet, 1,500 square feet.
So your house is, ah, 1,250 square feet.
Right. And you want to know,
you know, how do you price this house.
So given this dataset,
one thing you can do is,
um, fit a straight line to it.
Right. And then you could estimate or predicts the price
to be whatever value you read off on the, um, vertical axis.
So in supervised learning,
you are given a dataset with, ah,
inputs X and labels Y,
and your goal is to learn a mapping from X to Y.
Right. Now, um, fitting a straight line to data is maybe the simplest possible.
Maybe the simplest possible learning algorithm,
maybe one of the simplest poss- learning algorithms.
Um, given a dataset like this,
there are many possible ways to learn a mapping,
to learn the function mapping from the input size to the estimated price.
And so, um, maybe you wanna fit a quadratic function instead,
maybe that actually fits the data a little bit better.
And so how do you choose among different models will be, ah,
either automatically or manual intervention will be-
will be something we'll spend a lot time talking about.
Now to give a little bit more.
Um, to define a few more things.
This example is a problem called a regression problem.
And the term regression refers to
that the value y you're trying to predict is continuous.
Right. Um, in contrast,
here's a- here's a different type of problem.
Um, so problem that some of my friends were working on,
and- and I'll simplify it was- was a healthcare problem, where, ah,
they were looking at, uh,
breast cancer or breast tumors,
um, and trying to decide if a tumor is benign or malignant.
Right. So a tumor is a lump in a- in a woman's breast, um,
is- can be ma- malign,
or cancerous, um, or benign,
meaning you know, roughly it's not that harmful.
And so if on the horizontal axis,
you plot the size of a tumor.
Um, and on the vertical axis,
you plot is it malignant or not.
Malignant means harmful, right.
Um, and some tumors are harmful some are not.
And so whether it is malignant or not,
takes only two values, 1 or 0.
And so you may have a dataset, um, like that.
Right. Ah, and given this,
can you learn a mapping from X to Y,
so that if a new patient walks into your office, uh,
walks in the doctor's office and the tumor size is,
you know say, this,
can the learning algorithm figure out from this data that it was probably,
well, based on this dataset,
looks like there's- there's a high chance that that tumor is, um, malignant.
Um, so, ah, so this is an example of
a classification problem and
the term classification refers to that Y here takes on a discrete number of variables.
So for a regression problem,
Y is a real number.
I guess technically prices can be rounded off to the nearest dollar and cents,
so prices aren't really real numbers.
Um, you know that- because you'd probably not price it,
how's it like Pi times 1 million or whatever.
Ah, but, so, so- but for all practical purposes prices are continuous
so we call them housing price prediction to be a regression problem,
whereas if you have, ah, two values of possible output,
0 and 1, call it a classification problem.
Um, if you have K discrete outputs so,
uh, if the tumor can be, uh,
malignant or if there are five types of cancer, right,
so you have one of five possible outputs,
then that's also a classification problem.
If the output is discrete.
Now, um, I wanna find a different way to visualize this dataset which is,
um, let me draw a line on top.
And I'm just going to, you know,
map all this data on the horizontal axis upward onto a line.
But let me show you what I'm gonna do.
I'm going to use a symbol O to denote.
Right. Um, I hope what I did was clear.
So I took the two sets of examples,
uh, the positive and negative examples.
Positive example was this 1,
negative example was 0.
And I took all of these examples and- and kinda pushed them up onto a straight line,
and I use two symbols,
I use O's to denote negative examples and I use crosses to denote positive examples.
Okay. So this is just a different way of visualizing the same data, um,
by drawing it on the line and using, you know,
two symbols to denote the two discrete values 0 and 1, right?
So, um, it turns out that, uh, uh,
in both of these examples,
the input X was one-dimensional,
it was a single real number.
For most of the, um,
machine learning applications you work with,
the input X will be multi-dimensional.
You won't be given just one number and asked to predict another number.
Instead, you'll often be given, uh,
multiple features and multiple numbers to predict another number.
So for example, instead of just using a
tumor size to predict- to estimate malignancy- malignant versus benign tumors, um,
you may instead have two features where one is
tumor size and the second is age of the patient,
and be given a dataset, [NOISE]
right? And be given a dataset that looks like that, right?
Where now your task is, um,
given two input features,
so X is tumor size and age,
you know, like a two-dimensional vector,
um, and your task is given, uh,
these two input features,
um, to predict whether a given tumor is malignant or benign.
So if a new patient walks in a doctor's office
and that the tumor size is here and the age is here,
so that point there,
then hopefully you can conclude that, you know,
this patient's tumor is probably benign, right?
Corresponding the O, that negative example.
Um, and so what thing- one thing you'll learn, uh,
next week is a learning algorithm that can fit a straight line to the data as follows,
kinda like that, to separate out the positive and negative examples.
Separate out the O's and the crosses.
And so next week, you'll learn about the logistic regression algorithm which,
um, which can do that.
Okay? So, um, one of the most interesting things you'll learn about is, uh, let's see.
So in this example,
I drew a dataset with two input features, um,
when- so I have friends that actually worked on
the breast cancer, uh, prediction problem,
and in practice you usually have a lot more than one or two features,
and usually you have so many features you can't plot them on the board, right?
And so for an actual breast cancer prediction problem,
my friends are working on this were- were using many other features such as,
don't worry about what these me- mean,
I guess clump thickness,
uh, you know, uniformity of cell size,
uniformity of cell shape, right?
Um, uh, adhesion, how well the cells stick together.
Don't worry about what this means but, uh,
if you are actually doing this in a- in a actual medical application,
there's a good chance that you'll be using a lot more features than just two.
Uh, and this means that you actually can't plot this data,
right? It's two high-dimensional.
You can't plot things higher than 3-dimensional or maybe 4-dimensional,
or something like that and so when we have lots of
features it's actually difficult to plot this data.
I'll come back to this in a second in learning theory.
Um, and, uh, one of the things you'll learn about- so as we develop learning algorithms,
you'll learn how to build, um,
regression algorithms or classification algorithms that can deal
with these relatively large number of features.
One of the, uh,
most fascinating results you learn is that, um,
[NOISE] you'll also learn about an algorithm called the Support Vector Machine which uses
not one or two or three or 10 or 100 or a million input features,
but uses an infinite number of input features, right?
And so, so, so just to be clear, if in
this example the state of a patient were represents as one number,
you know, tumor size, uh,
in this example we had two features.
So the state of a patient were represented using two numbers,
the tumor size and the age.
If you use this list of features maybe
a patient that's represented with five or six numbers.
Uh, but there's an algorithm called the support vector machine that
allows you to use an infinite-dimensional vector,
um, to represent a patient.
And, um, how do you deal with that
and how can the computer even store an infinite-dimensional vector, right?
I mean, you know, computer memory,
you can store one row number, two row numbers,
but you can't store an infinite number of row numbers in
a computer without running out of memory or processor speed or whatever.
So so how do you do that?
Uh, so when we talk about support vector machines
and specifically the technical method called kernels,
you'll learn how to build learning algorithms that work with, uh,
so that the infinitely long lists of features,
infinitely long list of feature of- for for- which which- and you can
imagine that if you have an infinitely long list of numbers to represent a patient,
that might give you a lot of information about that patient and so that
is one of the relatively effective learning algorithms to solve problems, okay?
Um, so that's supervised learning.
And, you know, let me just,
um, uh, play a video,
um, show you a fun- slightly older example
of supervised learning to give you a sense of what this means.
[NOISE] But at the heart of supervised learning is the idea that during training, uh,
you are given inputs X together with the labels Y and you give it both at the same time,
and the job of your learning algorithm is to, uh,
find a mapping so that given a new X,
you can map it to the most appropriate output Y.
Um, so this is a very old video, uh, made by, um,
DA Pomerleau and we've known him for a long time as well, uh,
using supervised learning for autonomous driving.
Uh, this is not state of the art for autonomous driving anymore,
but it actually does remarkably well.
Oh, and, uh, um, as you, uh,
you hear a few technical terms like back-propagation,
you'll learn all those techniques in this class, uh,
and by the end of class, you'll either build
a learning algorithm much more effective than what you see here.
But let's- let's- let's see this application.
Uh, could you turn up the volume maybe have that?
Are you guys getting volume audio?
[BACKGROUND]
Oh, I see.
All right, I'll narrate this.
[LAUGHTER]
So I'll be using artificial neural network to drive this vehicle that,
uh, was built at Carnegie Mellon University, uh, many years ago.
And what happens is, uh,
during training, it watches the human, um,
drive the vehicle and I think 10 times per second,
uh, it digitizes the image in front of the vehicle.
And, um, so that's a picture taken by a front-facing camera.
Um, and what it does is in order to collect labeled data,
the car while the human is driving it,
records both the image such as it's seeing here,
as well as, the steering direction that was chosen by human.
So at the bottom here is the image turned to grayscale and lower res,
and, uh, on top,
let me pause this for a second.
Um, this is the driver direction,
the font's kinda blurry but this text says driver direction.
So this is the Y label,
the label Y that the human driver chose.
Um, and so the position of this white bar of
this white blob shows how the human is choosing to steer the car.
So in this, in this image,
the white blob is a little bit to the left of center so the human is,
you know, steering just a little bit to the left.
Um, this second line here is the output of the neural network and initially,
the neural network doesn't know how to drive,
and so it's just outputting this white smear everywhere and it's saying,
"No, I don't know, do I drive left, right, center?
I don't know." So it's outputting this gray blur everywhere.
Um, and as the algorithm learns
using the back-propagation learning algorithm
or gradient descents which you'll learn about,
uh, you'll actually learn about gradient descent this Wednesday.
Um, you see that
the neural network's outputs becomes less and less of this white smear,
this white blur but starts to, uh,
become sharper, um, and starts to mimic more
accurately the human selected driving direction.
Right. So this, um,
is an example of supervised learning because
the human driver demonstrates inputs X and outputs Y,
uh, meaning, uh, if you see this in front of the car steer like that so that's X and Y.
And, uh, after the learning algorithm has learned,
um, you can then,
uh, well, he pushes a button,
takes the hands off the steering wheel, um,
[NOISE] and then it's using this neural network to drive itself, right?
Digitizing the image in front of the road,
taking this image and passing it through the learning algorithm,
through the trained neural network,
letting the neural networks select a steering direction,
uh, and then using a little motor to turn the wheel.
Um, this is slightly more advanced version which has trained two separate models;
one for, I think, a two-lane road,
one for a four-lane road.
Uh, so that's the, um,
uh, so the second and third lines this is for a two-lane road,
this is a four-lane road.
And the arbitrator is, is another algorithm that tries to decide whether
the two-lane or the four-lane road model is
the more, more appropriate one for a particular given situation.
Um, and so as Alvin is, excuse me a one-lane road or,
uh, a two-lane road.
So, so, so it's driving from a one-lane road here,
uh, to another intersection,
um, the, uh the the algorithm realizes it should swi- switch over from,
um, I think I forget,
I think the one-lane neural network to the- to
the two-lane neural network [NOISE] one of these, right?
All right. Um.
Okay. Oh, oh, right.
Fine. We'll just see the final dramatic moment of
switching from a one-way road to a two-lane road.
[LAUGHTER]
All right.
Um, uh, and I think,
you know, so this is just using supervised learning to- take as input,
what's in front of the car to decide on the steering direction.
This is not state of the art for how self-driving cars are built today,
but you know, you could do some things in some limited contexts.
Uh, uh, and I think,
uh, in, in several weeks,
you'll actually be able to build something that is more sophisticated than this.
Right. Um, so after supervised learning,
uh, we wi- will- in this class we'll spend
a bit of time talking about machine learning strategy.
Also, well, I think on the class notes we annotate this as a learning theory.
But what that means is, um,
when I give you the tools to go out and apply learning algorithms effectively.
And I think I've been fortunate to have,
uh, you know, to know a lot of, uh,
uh, I, I think that,
um, I've been fortunate to have,
you know, over the years constantly visited lots of great tech companies.
Uh, uh, more than once that I've that- that I've been probably associated with, right?
But often, just to help friends out,
I visit various tech companies,
uh, whose products I'm sure are installed on your cell phone.
Uh, but I often visit tech companies and you know,
talk to the machine learning teams and see what they're doing,
and see if I can help them out.
And what I see is that there's a huge difference in
the effectiveness of how
two different teams could apply the exact same learning algorithm.
All right? Uh, and I think that, um,
what I've seen sadly is that
sometimes there will be a team or even in some of the best tech companies, right?
The, the, the, the EV, AI companies, right?
And, and, and multiple of them,
where you go talk to a team and they'll
tell you about something that they've been working on for six months.
And then, you can quickly take a look at the data and,
and hear that they're not- the algorithm isn't
quite working and sometimes you can look at what they're doing,
and go yeah, you know,
I could have told you six months ago that this approach is never gonna work, right?
Um, and, um, what I find is that
the most skilled machine learning practitioners are very strategic.
By which I mean that your skill at deciding- um,
when you work on a machine learning project,
you have- you- you have a lot of decisions to make.
Right? Do you collect more data?
Do you try a different learning algorithm?
Uh, do you rent faster GPUs to train your learning algorithm for longer?
Or if you collect more data,
what type of data do you collect?
Or for- all of these architectural choices,
using neural networks for reference machine which is regression,
which ones do you pick?
Um, but there are a lot of decisions you need to
make when building these learning algorithms.
So one thing that's quite unique to the way we teach is, uh,
we want to help you become more systematic in driving machine learning as a,
as a systematic engineering discipline,
so that when one day when you are working on as machine learning project,
you can efficiently figure out what to do next.
Right? Um, and I sometimes make an analogy to how,
um, to, uh, uh,
to, to software engineering.
Um, you know, like many years ago,
I had a friend, um,
that would debug code by compiling it and then,
um, uh, this friend would look for all of the syntax errors, right?
That, you know, C++ compiler outputs.
And they thought that the best way to eliminate the errors
is to delete all the lines of code with
syntax errors and that was [LAUGHTER] their first heuristic.
So that did not go well, right?
Um, uh, it took me a while to persuade them to stop doing that.
Uh, but, but, but so it turns out that, um,
when you run a learning algorithm,
you know, it almost never works the first time.
All right? It's this just life.
Uh, uh, and, and the way you go about debugging the learning algorithm will
have a huge impact on your efficiency o- on,
on how quickly you can build effective learning systems.
And I think until now,
too much of the- of this process of, uh,
making your learning algorithms work well has been a black magic kind of process where,
you know, has worked on this for decades.
So when you run something,
you don't know why it does not recognize it like, "Hey,
what do I do and it says, Oh, yeah, do that."
And then, and then, because he's so experienced it works,
but I think, um,
what we're trying to do with the discipline of machine learning
is to evolve it from a black magic,
tribal knowledge, experience-based thing to a systematic engineering process.
All right. And so um, later this quarter,
as we talk about machine learning strategy,
we'll talk about learning theory.
We'll try to systematically give you tools on how to,
um, uh, go about strategizing.
Uh, so- so it can be very efficient in, um,
how you- how you yourself,
how you can lead a team to build an effective learning system,
because I don't want you to be one of those people that, you know,
wastes six months on some direction that
maybe could have relatively quickly figured out it was not promising.
Or maybe one last analogy,
if you- um, if you're used to optimizing code, right?
Making code run faster,
I'm not sure if you have done that.
Uh, uh, uh, less experienced software engineers,
who'll just dive in and optimize the code,
they try to make it run faster, right?
Let's take the C++ and code in assembly or something.
But more experienced people will run a profiler to try to figure
out what part of code is actually the bottleneck and then just focus on changing on that.
So, uh, one of the things we hope to do this quarter is, uh,
uh, convey to you some of these more systematic engineering principles.
All right. And yeah.
Oh, and actually this is very interesting.
This is a, uh, uh, yeah.
Actually, I've been- I've been invited,
so actually- so how many of you have heard the machine learning journey?
Oh, just a few of you, interesting.
Oh, so actually, so,
so- if any of you are interested,
um, just in my,
uh, spare time, uh,
I've been writing a book,
um, uh, to try to codify
systematic engineering principles for machine learning and so if you are,
uh, and so uh,
if you want the, you know,
free draft copy of the book,
sign up for a mailing list here.
I tend to just write stuff and put it on the Internet for free, yeah.
So if you want a free draft copy of the book,
uh, uh, you know,
go to this website, uh,
enter your e-mail address and the website will send you a copy of that book.
And I'll talk a little bit about these engineering principles as well.
Okay. All right. So, uh,
so first subject, machine learning.
Second subject, learning theory.
Um, and, uh, the third major subject we'll talk about is, uh, deep learning, right?
And so you have a lot of tools in machine learning and many of them are
worth learning about and I use many different tools in machine learning,
you know, for many different applications.
There's one subset of machine learning that's really hot right now
because it's just advancing very rapidly, which is deep learning.
And so we'll spend a little time talking about deep learning so that you can
understand the basics of how to train a neural network as well.
But I think that's where CS229
covers a much broader set of algorithms which are all useful.
CS230, more narrowly covers just deep learning, right? Um.
So, uh, other than deep learning slash
after- after deep learning slash neu- neural networks the other,
the four, four of the five major topics we'll cover will be on unsupervised learning.
Um, so what is unsupervised learning?
[NOISE]
So you saw me draw a picture like this just now, right?
And this would be a classification problem like the tumor,
malignant, benign problems, this is a classification problem.
And that was a supervised learning problem because you have to learn
a function mapping from X to Y.
Um, unsupervised learning would be if I give you a dataset like this with no labels.
So you're just given inputs X and no Y,
and you're asked to find me something interesting in this data,
figure out, you know, interesting structure in this data.
Um, and so in this dataset,
it looks like there are two clusters,
and then unsupervised learning algorithm
which we learned about called K-means clustering,
will discover this, um,
this structure in the data.
Um, other examples as well as learning, you know,
if- if you actually,
Google News is a very interesting website.
Sometimes I use it to look up, right?
Latest news, just this old example.
But Google News everyday crawls or reads,
uh, uh, I don't know, uh, uh, many many thousands or
tens of thousands of news articles on the Internet and groups them together, right?
For example, there's a set of articles on the BP Oil Well spill, and it has,
uh, taken a lot of the articles written by different reporters and grouped them together.
So you can, you know,
figure out that what BP,
uh, Macondo oil well, right?
That this is a CNN article about the oil well spill,
there's a Guardian article about oil well spill and this is
an example of a clustering algorithm whereas taking
these different news sources and figuring out that these are
all stories kind of about the same thing, right?
Um, and other examples of clustering,
just getting data and figuring out what groups belong together.
Um, a lot of work on, um, genetic data.
This is a visualization of- of genetic microarray data.
Where given data like this,
you can group individuals into different types
of- into individuals of different, uh, characteristics, um,
or clustering algorithms grouping this type of data together is used to, um,
organize computing clusters, you know,
figure out what machines workflows are more related
to each other and organize computing clusters appropriately.
So take a social network like LinkedIn or Facebook or
other social networks and figure out which are the groups of
friends and which are the cohesive communities within a social network,
um, or market segmentation.
Um, actually many companies I've worked with look at
the customer database and cluster the users together.
So you can say that looks like we're four types of users, you know,
looks like that, um,
there are the, uh,
young professionals looking to develop themselves,
there are the, you know,
soccer moms and soccer dads, there are this category and these categories.
You can then market to the different market segments, um, separately.
Uh, and- and actually,
many years ago my friend Andrew Moore, uh, uh,
was using this type of data for
astronomical data analysis to group together galaxies. You have a question?
[inaudible].
Is unsupervised learning the same as learning clustering? No it's not.
So unsupervised learning broadly is the concept of using unlabeled data.
So just X and finding interesting things about it, right?
Um, so, um, for example,
uh, actually here's- shoot.
This won't work with all of you will do this later in the cla- in the class I guess.
Um, maybe I say we'll do this later.
Cocktail party problem, um, uh,
is another unsupervised learning problem.
Reading the audio for this to explain this though,
um, let me think how to explain this.
Um, the cocktail party problem and I'll try to
do the demo when we can get all your work on this laptop.
Is a problem where, um,
if you have a noisy room and you stick
mult- multiple microphones in the room and record overlapping voices,
um, so that no labels reaches multiple microphones,
an array of microphones,
in a room of lots of people talking.
Uh, how can you have the algorithm separate out the people's voices.
So that's an unsupervised learning problem because,
um, there are no labels.
So you just stick microphones in the room and have it
record different people's voices, overlapping voices,
you have multiple users at the same time and then
have it try to separate out people's voices.
And one of the programming exercises you do later is,
if we have, you know, five people talking.
So each microphone records five people's overlapping voices, right?
Because, you know, each microphone hears five people at the same time.
How can you have an algorithm separate out
these voices so you get clean recordings of just one voice at a time.
So that's called the cocktail party problem and the algorithm you
use to do this is called ICA, Independent Components Analysis.
And that's something you implement in one of these later homework exercises, right?
Um, and there are other examples of unsupervised learning as well.
Uh, the Internet has tons of unlabeled text data.
You just suck down data from the Internet.
There are no labels necessarily but can you learn interesting things about language,
figure out what- figure out on,
I don't know, one of the best cited results recently was learning analogies,
like yeah, man is to woman as king is to queen, right?
Or a- what's a Tokyo is to Japan as Washington DC is to the United States, right?
To learn analogies like that.
It turns out you can learn analogies like that from unlabeled data,
just from texts on the Internet.
So that's also unsupervised learning.
Okay? Um, so after unsupervised learning,
oh, and unsupervised learning.
So you know machine learning is very useful today.
It turns out that most of the recent wave of
economic value created by machine learning is through supervised learning.
Uh, but there are important use cases for unsupervised learning as well.
So I use them in my work occasionally.
Uh, and is also a bleeding edge for a lot of exciting research.
And then the final topic.
Finally, the five topics we covered.
So talk about supervised learning, machine learning strategy,
deep learning, unsupervised learning,
and then the fifth one is reinforcement learning, is this.
Which is, um, let's say, I give you the keys to Stanford autonomous helicopter.
This helicopter is actually sitting in my office,
and I'm trying to figure out how to get rid of it.
Um, and I'll ask you to write a program to- to make it fly, right?
So how do you do that?
Um, so this is a video of a helicopter flying.
The audio is just a lot of helicopter noise. So that's not important.
But we'll zoom out the video.
You see she's found in the sky, right?
There. So, um, you can use learning alg- that's kinda cool, right?
[LAUGHTER] I was- I was the camera man that day.
Um, but so you can use learning algorithms to get,
you know, robots to do pretty interesting things like this.
Um, and it turns out that a good way to do this is through reinforcement learning.
So what's reinforcement learning?
Um, it turns out that no one knows what's the optimal way to fly a helicopter, right?
If you fly a helicopter,
you have two control sticks that you're moving.
Um, but no one knows what's the optimal way to move the control stick.
So the way you can get a helicopter fly itself is, um,
let the helicopter do whatever- think of this as training a dog, right?
You can't teach a dog the optimal way to behave,
but- actually, how many of you have had a pet dog or pet cat before?
Oh, not that many of you. This is fascinating.
Okay. So I had a pet dog when I was a kid
and my family made it my job to train the dog. So how do you train a dog?
You let the dog do whatever it wants,
and then whenever it behaves well,
you go, "Oh, good dog".
And when it misbehaves you go, "bad dog".
[LAUGHTER] Um, and then over time,
the dog learns to do more of the good dog things and fewer of the bad dog things,
and so reinforcement learning is a bit like that, right?
I don't know what's the optimal way to fly a helicopter.
So you let the helicopter do whatever it wants and then whenever it flies well, you know,
does some maneuver you want, or flies accurately without jetting around too much,
you go, "Oh, good helicopter".
[LAUGHTER] And when it crashes you go,
"bad helicopter" and it's the job of
the reinforcement learning algorithms to figure out how to
control it over time so as to get more of
the good helicopter things and fewer of the bad helicopter things.
Um, and I think,
um, well, just one more video.
Um, oh, yeah, that's interesting. All right.
And so again given a robot like this,
I actually don't know how to program a- actually
a robot like this has a lot of joints, right?
So how do you get a robot like this to climb over obstacles?
So well, this is actually a robot dog,
so you can actually say,
"Good dog" or "Bad dog".
[LAUGHTER] By giving those signals,
called a reward signal, uh,
you can have a learning algorithm figure out by itself,
how to optimize the reward,
and therefore, [LAUGHTER] climb over these types of obstacles.
Um, and I think recently,
the most famous applications of reinforcement learning happened for game-playing,
playing Atari games or playing,
you know, Game of Go, like AlphaGo.
I think that's a- I think that is a- game playing has made some remarkable stunts a
remarkable PR but I'm also equally excited or maybe
even more excited about the integrals and reinforcement learning is making
into robotics applications, right?
So I think, um, I think- yeah,
reinforcement learning has been proven to be fantastic at playing
games, it's also getting- making real traction
in optimizing robots and optimizing sort of logistic system and things like that.
Um, so you learned about all these things.
Um, last thing for today, uh,
I hope that you will start to, to meet people in the class,
make friends, find project partners and study groups,
and if you have any questions, [NOISE] you know,
dive on the Piazza, asking questions as you help others answer their questions.
So let's break for today, and I look forward to seeing you on Wednesday.
Welcome to 229.
 Morning and welcome back.
So what we'll see today in class
is the first in-depth discussion of a learning algorithm,
linear regression, and in particular,
over the next, what,
hour and a bit you'll see linear regression,
batch and stochastic gradient descent is
an algorithm for fitting linear regression models,
and then the normal equations, um, uh,
as a way of- as a very efficient way to let you fit linear models.
Um, and we're going to define notation,
and a few concepts today that will lay the foundation
for a lot of the work that we'll see the rest of this quarter.
Um, so to- to motivate linear regression, it's gonna be, uh,
maybe the- maybe the simplest,
one of the simplest learning algorithms.
Um, you remember the ALVINN video,
the autonomous driving video that I had shown in class on Monday,
um, for the self-driving car video,
that was a supervised learning problem.
And the term supervised learning [NOISE] meant
that you were given Xs which was a picture of what's in front of the car,
and the algorithm [NOISE] had to map that to
an output Y which was the steering direction.
And that was a regression problem,
[NOISE] because the output Y that you want is a continuous value, right?
As opposed to a classification problem where Y is the speed.
And we'll talk about classification, um,
next Monday, but supervised learning regression.
So I think the simplest,
maybe the simplest possible learning algorithm,
a supervised learning regression problem, is linear regression.
And to motivate that,
rather than using a self-driving car example which is quite complicated,
we'll- we'll build up a supervised learning algorithm using a simpler example.
Um, so let's say you want to predict or estimate the prices of houses.
So [NOISE] the way you build a learning algorithm is
start by collecting a data-set of houses, and their prices.
Um, so this is a data-set that we collected off Craigslist a little bit back.
This is data from Portland, Oregon.
[NOISE] But so there's the size of a house in square feet, [NOISE] um,
and there's the price of a house in thousands of dollars, [NOISE] right?
And so there's a house that is 2,104 square feet whose asking price was $400,000.
Um, [NOISE] house with, uh,
that size, with that price,
[NOISE] and so on.
Okay? Um, and maybe more conventionally if you plot this data,
there's a size, there's a price.
So you have some dataset like that.
And what we'll end up doing today is fit a straight line to this data, right?
[NOISE] And go through how to do that.
So in supervised learning, um,
the [NOISE] process of supervised learning is that you have
a training set such as the data-set that I drew on the left,
and you feed this to a learning algorithm, [NOISE] right?
And the job of the learning algorithm is to output a function,
uh, to make predictions about housing prices.
And by convention, um,
I'm gonna call this a function that it outputs a hypothesis, [NOISE] right?
And the job of the hypothesis is, [NOISE] you know,
it will- it can input the size of a new house,
or the size of a different house that you haven't seen yet,
[NOISE] and will output the estimated [NOISE] price.
Okay? Um, so the job of the learning algorithm is to input a training set,
and output a hypothesis.
The job of the hypothesis is to take as input,
any size of a house,
and try to tell you what it thinks should be the price of that house.
Now, when designing a learning algorithm,
um, and- and, you know,
even though linear regression, right?
You may have seen it in a linear algebra class before,
or some other class before, um,
the way you go about structuring a machine learning algorithm is important.
And design choices of,
you know, what is the workflow?
What is the data-set? What is the hypothesis?
How does this represent the hypothesis?
These are the key decisions you have to make in pretty much every supervised learning,
every machine learning algorithm's design.
So, uh, as we go through linear regression,
I will try to describe the concepts clearly as well
because they'll lay the foundation for the rest of the algorithms.
Sometimes it's much more complicated with the algorithms you'll see later this quarter.
So when designing a learning algorithm the first thing we need to ask is, um,
[NOISE] how- how do you represent the hypothesis, H, right?
And in linear regression,
for the purpose of this lecture,
[NOISE] we're going to say that, um,
the hypothesis is going to be [NOISE] that.
Right? That the input, uh, size X,
and output a number as a- as a linear function,
um, of the size X, okay?
And then, the mathematicians in the room,
you'll say technically this is an affine function.
It was a linear function, there's no theta 0, technically, you know,
but- but the machine learning sometimes just calls this a linear function,
but technically it's an affine function. Doesn't- doesn't matter.
Um, so more generally in- in this example we have just one input feature X.
More generally, if you have multiple input features,
so if you have more data,
more information about these houses,
such as number of bedrooms [NOISE] Excuse me,
my handwriting is not big.
That's the word bedrooms, [NOISE] right?
Then, I guess- [NOISE] All right.
Yeah. Cool. My- my- my father-in-law lives a little bit outside Portland,
uh, and he's actually really into real estate.
So this is actually a real data-set from Portland.
[LAUGHTER] Um, so more generally,
uh, if you know the size,
as well as the number of bedrooms in these houses,
then you may have two input features [NOISE] where X1 is the size,
and X2 is the number of bedrooms.
[NOISE] Um, I'm using the pound sign bedrooms to denote number of bedrooms,
and you might say that you estimate the size of a house as,
um, h of x equals,
theta 0 plus theta 1,
[NOISE] X1, plus theta 2, X2,
where X1 is the size of the house,
and X2 is- [NOISE] is the number of bedrooms.
Okay? Um, so in order to-
[NOISE] so in order to simplify the notation,
[NOISE] um, [NOISE] in order to make that notation a little bit more compact,
um, I'm also gonna introduce this other notation where,
um, we want to write a hypothesis,
as sum from J equals 0-2 of theta JXJ,
so this is the summation,
where for conciseness we define X0 to be equal to 1, okay?
See we define- if we define X0 to be
a dummy feature that always takes on the value of 1,
then you can write the hypothesis h of x this way,
sum from J equals 0-2,
or just theta JXJ, okay?
It's the same with that equation that you saw to the upper right.
And so here theta becomes a three-dimensional parameter,
theta 0, theta 1, theta 2.
This index starting from 0,
and the features become a three dimensional feature vector X0, X1,
X2, where X0 is always 1,
X1 is the size of the house,
and X2 is the number of bedrooms of the house, okay?
So, um, to introduce a bit more terminology.
Theta [NOISE] is called the parameters, um,
of the learning algorithm,
and the job [NOISE] of the learning algorithm is to choose parameters theta,
that allows you to make good predictions about your prices of houses, right?
Um, and just to lay out some more notation that we're gonna use throughout this quarter.
We're gonna use a standard that, uh, M,
we'll define as the number of training examples.
So M is going to be the number of rows,
[NOISE] right, in the table above, um,
where, you know, each house you have in your training set.
This one training example.
Um, you've already seen [NOISE] me use X to denote the inputs,
um, and often the inputs are called features.
Um, you know, I think, I don't know,
as- as- as a- as a emerging discipline grows up, right,
notation kind of emerges depending on what
different scientists use for the first time when you write a paper.
So I think that, I don't know,
I think that the fact that we call these things hypotheses,
frankly, I don't think that's a great name.
But- but I think someone many decades ago wrote a few papers calling it a hypothesis,
and then others followed, and we're kind of stuck with some of this terminology.
But X is called input features,
or sometimes input attributes, um,
and Y [NOISE] is the output, right?
And sometimes we call this the target variable.
[NOISE] Okay.
Uh, so x, y is,
uh, one training example.
[NOISE] Um, and, uh,
I'm going to use this notation, um,
x_i, y_i in parentheses to denote
the i_th training example.
Okay. So the superscript parentheses i, that's not exponentiation.
Uh, I think that as suppo- uh, this is- um,
this notation x_i, y_ i,
this is just a way of,
uh, writing an index into the table of training examples above.
Okay. So, so maybe, for example,
if the first training example is, uh,
the size- the house of size 2104,
so x_1_1 would be equal to 2104,
right, because this is the size of the first house in the training example.
And I guess, uh, x, um,
second example, feature one would be 1416 with our example above.
So the superscript in parentheses is just some,
uh, uh, yes, it's, it's just the, um,
index into the different training examples
where i- superscript i here would run from 1 through m,
1 through the number of training examples you have.
Um, and then one last bit of notation, um,
I'm going to use n to
denote the number of features you have for the supervised learning problem.
So in this example, uh,
n is equal to 2, right?
Because we have two features which is,
um, the size of house and the number of bedrooms, so two features.
Which is why you can take this and,
and write this, um,
as the sum from j equals 0 to n. Um, and so here,
x and Theta are n plus 1 dimensional because we added the extra,
um, x_0 and Theta_0.
Okay. So- so we have two features then these are three-dimensional vectors.
And more generally, if you have n features, uh, you,
you end up with x and Theta being n plus 1 dimensional features. All right.
And, you know, uh,
you see this notation in multiple times,
in multiple algorithms throughout this quarter.
So if you, you know,
don't manage to memorize all these symbols right now, don't worry about it.
You'll see them over and over and they become familiar. All right.
So, um, given the data set and given that this is the way you define the hypothesis,
how do you choose the parameters, right?
So you- the learning algorithm's job is to choose values for the parameters
Theta so that it can output a hypothesis.
So how do you choose parameters Theta?
Well, what we'll do, um,
is let's choose Theta
such that h of x is close to y,
uh, for the training examples.
Okay. So, um, and I think the final bit of notation, um,
I've been writing h of x as a function of the features of the house,
as a function of the size and number of bedrooms of the house.
[NOISE] Um, sometimes we emphasize that h depends both
on the parameters Theta and on the input features x. Um,
we're going to use h_Theta
of x to emphasize that the hypothesis depends both on the parameters and on the,
you know, input features x, right?
But, uh, sometimes for notational convenience,
I just write this as h of x,
sometimes I include the Theta there, and they mean the same thing.
It's just, uh, maybe a abbreviation in notation.
Okay. But so in order to,
um, learn a set of parameters,
what we'll want to do is choose a parameters Theta
so that at least for the houses whose prices you know, that, you know,
the learning algorithm outputs prices that are close to
what you know where the correct price is for that set of houses,
what the correct asking price is for those houses.
And so more formally, um,
in the linear regression algorithm,
also called ordinary least squares.
With linear regression, um,
we will want to minimize,
I'm gonna build out this equation one piece at a time, okay?
Minimize the square difference between what the hypothesis outputs,
h_Theta of x minus y squared, right?
So let's say we wanna minimize the squared difference between the prediction,
which is h of x and y,
which is the correct price.
Um, and so what we want to do is choose values of Theta that minimizes that.
Um, to fill this out, you know,
you have m training examples.
So I'm going to sum from i equals 1 through m of that square difference.
So this is sum over i equals 1 through all,
say, 50 examples you have, right?
Um, of the square difference between what
your algorithm predicts and what the true price of the house is.
Um, and then finally, by convention,
we put a one-half there- put a one-half constant there because, uh,
when we take derivatives to minimize this later,
putting a one-half there would make some of the math a little bit simpler.
So, you know, changing one- adding a one-half.
Minimizing that formula should give you the same as minimizing one-half of that but we
often put a one-half there so to make the math a little bit simpler later, okay?
And so in linear regression,
we're gonna define the cost function J of Theta to be equal to that.
And, uh, [NOISE] we'll find parameters
Theta that minimizes the cost function J of Theta, okay?
Um, and, the questions I've often gotten is,
you know, why squared error?
Why not absolute error, or this error to the power of 4?
We'll talk more about that when we talk about, um, uh,
when, when, when we talk about the generalization of, uh, linear regression.
Um, when we talk about generalized linear models,
which we'll do next week, you'll see that, um,
uh, linear regression is a special case
of a bigger family of algorithms called generalizing your models.
And that, uh, using squared error corresponds to a Gaussian, but the- we, we,
we'll justify maybe a little bit more why squared error
rather than absolute error or errors to the power of 4, uh, next week.
So, um, let me just check,
see if any questions,
[NOISE] at this point. No, okay. Cool.
All right. So, um,
so let's- next let's see how you can implement
an algorithm to find the value of Theta that minimizes J of Theta.
That- that minimizes the cost function J of Theta.
Um, we're going to use an algorithm called gradient descent.
And, um, you know,
this is my first time teaching in this classroom,
so trying to figure out logistics like this.
All right. Let's get rid of the chair.
Cool, um, all right.
And so with, uh,
gradient descent we are going to start with some value of Theta,
um, and it could be, you know,
Theta equals the vector of all zeros would be a reasonable default.
We can initialize it randomly, the count doesn't really matter.
But, uh, Theta is this three-dimensional vector.
And I'm writing 0 with an arrow on top to denote the vector of all 0s.
So 0 with an arrow on top that's a vector that says 0,
0, 0, everywhere, right.
So, um, uh, so sought to some, you know,
initial value of Theta and we're going to keep changing Theta,
um, to reduce J of Theta, okay?
So let me show you a,
um- vi- vis- let me show you a visualization of gradient descent,
and then- and then we'll write out the math.
[NOISE] Um, so- all right.
Let's say you want to minimize some function J of Theta and,
uh, it's important to get the axes right in this diagram, right?
So in this diagram the horizontal axes are Theta 0 and Theta 1.
And what you want to do is find values for Theta 0 and Theta 1.
In our- I- I- In our example it's actually Theta 0, Theta 1,
Theta 2 because Theta's 3-dimensional but I can't plot that.
So I'm just using Theta 0 and Theta 1.
But what you want to do is find values for Theta 0 and Theta 1, right?
That's the, um, uh, right,
you wanna find values of Theta 0 and Theta
1 that minimizes the height of the surface j of Theta.
So maybe this- this looks like a good- pretty good point or something, okay?
Um, and so in gradient descent you, you know,
start off at some point on this surface and you do that by initializing
Theta 0 and Theta 1 either randomly or to the value
of all zeros or something doesn't- doesn't matter too much.
And, um, what you do is, uh,
im- imagine that you are standing on this lower hill, right
standing at the point of that little x or that little cross.
Um, what you do in gradient descent is, uh,
turn on- turn around all 360 degrees and look all around
you and see if you were to take a tiny little step, you know,
take a tiny little baby step,
in what direction should you take a little step to go downhill as fast
as possible because you're trying to go downhill which
is- goes to the lowest possible elevation,
goes to the lowest possible point of J of Theta, okay?
So what gradient descent will do is, uh,
stand at that point look around,
look all- all around you and say, well,
what direction should I take a little step in to go downhill as
quickly as possible because you want to minimize, uh, J of Theta.
You wanna minim- reduce the value of J of Theta,
you know, go to the lowest possible elevation on this hill.
Um, and so gradient descent will take that little baby step, right?
And then- and then repeat.
Uh, now you're a little bit lower on the surface.
So you again take a look all around you and say oh it looks like that hill,
that- that little direction is the steepest direction or the steepest gradient downhill.
So you take another little step,
take another step- another step and so on,
until, um, uh, until you- until you get to a hopefully a local optimum.
Now one property of gradient descent is that, um, uh,
depend on where you initialize parameters,
you can get to local diff- different points, right?
So previously, you had started it at that lower point x.
But imagine if, uh,
you had started it, you know,
just a few steps over to the right, right?
At that- at that new x instead of the one on the left.
If you had run gradient descent from that new point then, uh,
that would have been the first step,
that would be the second step and so on.
And you would have gotten to
a different local optimum- to a different local minima, okay?
Um, it turns out that when you run gradient descents on linear regression,
it turns out that, uh, uh, uh,
there will not be local optimum but we'll talk about that in a little bit, okay?
So let's formalize the [NOISE] gradient descent algorithm.
In gradient descent, um,
each step of gradient descent,
uh, is implemented as follows.
So- so remember, in- in this example,
the training set is fixed, right?
You- You know you've collected the data set of housing prices from Portland,
Oregon so you just have that stored in your computer memory.
And so the data set is fixed.
The cost function J is a fixed function there's function of parameters Theta,
and the only thing you're gonna do is tweak or modify the parameters Theta.
One step of gradient descent,
um, can be implemented as follows,
which is Theta j gets updated as Theta j minus,
I'll just write this out, okay?
Um, so bit more notation,
I'm gonna use colon equals,
I'm gonna use this notation to denote assignment.
So what this means is,
we're gonna take the value on the right and assign it to Theta on the left, right?
And so, um, so in other words,
in the notation we'll use this quarter, you know,
a colon equals a plus 1.
This means increment the value of a by 1.
Um, whereas, you know, a equals b,
if I write a equals b I'm asserting a statement of fact, right?
I'm asserting that the value of a is equal to the value of b. Um, and hopefully,
I won't ever write a equals a plus 1,
right because- cos that is rarely true, okay?
Um, all right. So, uh,
in each step of gradient descent,
you're going to- for each value of j,
so you're gonna do this for j equals 0, 1 ,2 or 0, 1,
up to n, where n is the number of features.
For each value of j takes either j and update it according to Theta j minus Alpha.
Um, which is called the learning rate.
Um, Alpha the learning rate times this formula.
And this formula is the partial derivative of the cost function J
of Theta with respect to the parameter,
um, Theta j, okay?
In- and this partial derivative notation.
Uh, for those of you that, um,
haven't seen calculus for a while or haven't seen,
you know, some of their prerequisites for a while.
We'll- we'll- we'll go over some more of
this in a little bit greater detail in discussion section,
but I'll- I'll- I'll do this, um, quickly now.
But, um, I don't know. If, if you've taken a calculus class a while back,
you may remember that the derivative of a function is,
you know, defines the direction of steepest descent.
So it defines the direction that allows you to go downhill as steeply as possible,
uh, on the, on the hill like that. There's a question.
How do you determine the learning rate?
How do you determine the learning rate? Ah, let me get back to that. It's a good question.
Uh, for now, um,
uh, you know, there's a theory and there's a practice.
Uh, in practice, you set to 0.01.
[LAUGHTER].
[LAUGHTER] Let me say a bit more about that later.
[NOISE].
Uh, if- if you actually- if- if you scale all the features between 0 and 1,
you know, minus 1 and plus 1 or something like that, then, then, yeah.
Then, then try- you can try
a few values and see what lets you minimize the function best,
but if the feature is scaled to plus minus 1,
I usually start with 0.01 and then,
and then try increasing and decreasing it.
Say, say a little bit more about that.
[NOISE] Um, uh, all right, cool.
So, um, let's see.
Let me just quickly [NOISE] show how the derivative calculation is done.
Um, and you know, I'm,
I'm gonna do a few more equations in this lecture,
uh, and then, and then over time I think.
Um, all, all of these,
all of these definitions and derivations are
written out in full detail in the lecture notes,
uh, posted on the course website.
So sometimes, I'll do more math in class when, um,
we want you to see the steps of the derivation and sometimes to save time in class,
we'll gloss over the mathematical details and leave you to read over,
the full details in the lecture notes on the CS229
you know, course website.
Um, so partial derivative with respect to J of Theta,
that's the partial derivative with respect to that
of one-half H of Theta of X minus Y squared.
Uh, and so I'm going to do
a slightly simpler version assuming we have just one training example, right?
The, the actual deriva- definition of J of Theta has
a sum over I from 1 to M over all the training examples.
So I'm just forgetting that sum for now.
So if you have only one training example.
Um, and so from calculus,
if you take the derivative of a square, you know,
the 2 comes down and so that cancels out with the half.
So 2 times 1.5 times, um,
uh, the thing inside, right?
Uh, and then by the, uh,
chain rule of, uh, derivatives.
Uh, that's times the partial derivative of Theta J of X Theta X minus Y, right?
So if you take the derivative of a square,
the two comes down and then you take
the derivative of what's inside and multiply that, right?
[NOISE] Um, and so the two and one-half cancel out.
So this leaves you with H minus Y times partial derivative respect to Theta J of
Theta 0X0 plus Theta 1X1
plus th- th- that plus Theta NXN minus Y, right?
Where I just took the definition of H of X and expanded it out to that, um, sum, right?
Because, uh, H of X is just equal to that.
So if you look at the partial derivative of each of these terms with respect to Theta J,
the partial derivative of every one of these terms with respect to
Theta J is going to z- be 0 except for,
uh, the term corresponding to J, right?
Because, uh, if J was equal to 1, say, right?
Then this term doesn't depend on Theta 1.
Uh, this term, this term,
all of them do not even depend on Theta 1.
The only term that depends on Theta 1 is this term over there.
Um, and the partial derivative of this term with respect to Theta 1 will be just X1, right?
And so, um, when you take the partial derivative of
this big sum with respect to say the J, uh,
in- in- in- instead of just J equals 1 and with respect to Theta J in general,
then the only term that even depends on Theta J is the term Theta JXJ.
And so the partial derivative of all the other terms end up being 0 and
partial derivative of this term with respect to Theta J is equal to XJ, okay?
And so this ends up being H Theta X minus Y times XJ, okay?
Um, and again, listen, if you haven't,
if you haven't played with calculus for awhile, if you- you know,
don't quite remember what a partial derivative is,
or don't quite get what we just said. Don't worry too much about it.
We'll go over a bit more in the section and we- and,
and then also read through the lecture notes which kind of goes over this in,
in, in, um, in more detail and more slowly than,
than, uh, we might do in class, okay?
[NOISE]
So, um, so plugging this- let's see.
So we've just calculated that this partial derivative,
right, is equal to this,
and so plugging it back into that formula,
one step of gradient descent is,
um, is the following,
which is that we will- that Theta J be updated according to Theta J minus the learning
rate times H of X minus
Y times XJ, okay?
Now, I'm, I'm gonna just add a few more things in this equation.
Um, so I did this with one training example, but, uh,
this was- I kind of used definition of the cost function J of
Theta defined using just one single training example,
but you actually have M training examples.
And so, um, the,
the correct formula for the derivative is actually if you
take this thing and sum it over all M training examples,
um, the derivative of- you know,
the derivative of a sum is the sum of the derivatives, right?
So, um, so you actually- If, if,
if you redo this derivation, you know,
summing with the correct definition of J of
Theta which sums over all M training examples.
If you just redo that little derivation,
you end up with, uh,
sum equals I through M of that, right?
Where remember XI is the Ith training examples input features,
YI is the target label, is the, uh,
price in the Ith training example, okay?
Um, and so this is the actual correct formula for the partial derivative with
respect to that of the cost function J of Theta when it's defined using,
um, uh, all of the,
um, [NOISE] uh, on- when it's defined using all of the training examples, okay?
And so the gradient descent algorithm is to- [NOISE]
Repeat until convergence, carry out this update,
and in each iteration of gradient descent, uh,
you do this update for j equals,
uh, 0, 1 up to n. Uh,
where n is the number of features.
So n was 2 in our example.
Okay. Um, and if you do this then,
uh, uh, you know, actually let me see.
Then what will happen is,
um, [NOISE] well, I'll show you the animation.
As you fit- hopefully,
you find a pretty good value of the parameters Theta.
Okay. So, um, it turns out that when
you plot the cost function j of Theta for a linear regression model,
um, it turns out that,
unlike the earlier diagram I had shown which has local optima,
it turns out that if j of Theta is defined the way that, you know,
we just defined it for linear regression,
is the sum of squared terms, um,
then j of Theta turns out to be a quadratic function, right?
It's a sum of these squares of terms, and so,
j of Theta will always look like,
look like a big bowl like this.
Okay. Um, another way to look at this, uh, uh,
and so and so j of Theta does not have local optima,
um, or the only local optima is also the global optimum.
The other way to look at the function like this is
to look at the contours of this plot, right?
So you plot the contours by looking at the big bowl
and taking horizontal slices and plotting where the,
where the curves where, where the edges of the horizontal slice is.
So the contours of a big bowl or I guess a formal is,
uh, of a bigger,
uh, of of this quadratic function will be ellipsis,
um, like these or these ovals or these ellipses like this.
And so if you run gradient descent on this algorithm, um,
let's say I initialize, uh,
my parameters at that little x,
uh, shown over here, right.
And usually you initialize Theta degree with a 0,
but but, you know, but it doesn't matter too much.
So let's reinitialize over there.
Then, um, with one step of gradient descent,
the algorithm will take that step downhill,
uh, and then with a second step,
it'll take that step downhill whereby we, fun fact, uh,
if you- if you think about the contours of the function,
it turns out that the direction of steepest descent is always at 90 degrees,
is always orthogonal, uh,
to the contour direction, right.
So, I don't know, yeah.
I seem to remember that from my high-school something, I think it's true. All right.
And so as you,
as you take steps downhill, uh, uh,
because there's only one global minimum,
um, this algorithm will eventually converge to the global minimum.
Okay. And so the question just now about the choice of the learning rate Alpha.
Um, if you set Alpha to be very very large,
to be too large then they can overshoot, right.
The steps you take can be too large and you can run past the minimum.
Uh, if you set to be too small,
then you need a lot of iterations and the algorithm will be slow.
And so what happens in practice is, uh,
usually you try a few values and and and see
what value of the learning rate allows you to most efficiently,
you know, drive down the value of j of Theta.
Um, and if you see j of Theta increasing rather than decreasing,
you see the cost function increasing rather than decreasing, then,
there's a very strong sign that the learning rate is,
uh, too large, and so, um.
[NOISE] Actually what what I often do is actually try out multiple values of,
um, the learning rate Alpha, and, uh,
uh, and and usually try them on an exponential scale.
So you try open O1,
open O2, open O4,
open O8 kinda like a doubling scale or some- uh, uh,
or doubling or tripling scale and try a few values and see
what value allows you to drive down to the learning rate fastest.
Okay. Um, let me just.
So I just want to visualize this in one other way,
um, which is with the data.
So, uh, this is this is the actual dataset.
Uh, they're, um, there are actually 49 points in this dataset.
So m the number of training examples is 49,
and so if you initialize the parameters to 0,
that means, initializing your hypothesis or
initializing your straight line fit to the data to be that horizontal line, right?
So, if you initialize Theta 0 equals 0, Theta 1 equals 0,
then your hypothesis is, you know,
for any input size of house or price,
the estimated price is 0, right?
And so your hypothesis starts off with a horizontal line,
that is whatever the input x the output y is 0.
And what you're doing, um,
as you run gradient descent is you're changing the parameters Theta, right?
So the parameters went from this value to
this value to this value to this value and so on.
And so, the other way of visualizing gradient descent is,
if gradient descent starts off with this hypothesis,
with each iteration of gradient descent,
you are trying to find different values of the parameters Theta,
uh, that allows this straight line to fit the data better.
So after one iteration of gradient descent,
this is the new hypothesis,
you now have different values of Theta 0 and
Theta 1 that fits the data a little bit better.
Um, after two iterations,
you end up with that hypothesis, uh,
and with each iteration of gradient descent it's trying to minimize j of Theta.
Is trying to minimize one half of the sum of squares errors of
the hypothesis or predictions on the different examples, right?
With three iterations of gradient descent, um,
uh, four iterations and so on.
And then and then a bunch more iterations, uh,
and eventually it converges to that hypothesis,
which is a pretty, pretty decent straight line fit to the data.
Okay. Is there a question? Yeah, go for it.
[inaudible]
Uh, sure.
Maybe, uh, just to repeat the question.
Why is the- why are you subtracting Alpha
times the gradient rather than adding Alpha times the gradient?
Um, let me suggest,
actually let me raise the screen.
Um, [NOISE] so let me suggest you work through one example.
Um, uh, it turns out that if you add multiple times in a gradient,
you'll be going uphill rather than going downhill,
and maybe one way to see that would be if, um,
you know, take a quadratic function, um, excuse me.
Right. If you are here,
the gradient is a positive direction and you want to reduce,
so this would be Theta and this will be j I guess.
So you want Theta to decrease,
so the gradient is positive.
You wanna decrease Theta,
so you want to subtract a multiple times the gradient.
Um, I think maybe the best way to see that would be the work through an example yourself.
Uh, uh, set j of Theta equals Theta squared and set Theta equals 1.
So here at the quadratic function of the derivative is equal to 1.
So you want to subtract the value from Theta rather than add.
Okay? Cool. Um.
All right. Great. So you've now seen your first learning algorithm,
um, and, you know,
gradient descent and linear regression is
definitely still one of the most widely used learning algorithms in the world today,
and if you implement this- if you,
if you, if you implement this today,
right, you could use this for,
for some actually pretty, pretty decent purposes.
Um, now, I wanna I give this algorithm one other name.
Uh, so our gradient descent algorithm here, um,
calculates this derivative by summing over
your entire training set m. And so sometimes this version of gradient descent,
has another name, which is batch gradient descent.
Oops. All right
and the term batch,
um, you know- and again- I think in machine learning, uh,
our whole committee, we just make up names and stuff and sometimes the names aren't great.
But the- the term batch gradient descent refers to that,
you look at the entire training set,
all 49 examples in the example I just had on, uh, PowerPoint.
You know, you- you think of all for 49 examples as one batch of data,
I'm gonna process all the data as a batch,
so hence the name batch gradient descent.
The disadvantage of batch gradient descent is that if you have a giant dataset,
if you have, um,
and- and in the era of big data we're really,
moving to larger and larger datasets,
so I've used, you know,
we train machine learning models of like hundreds of millions of examples.
And- and if you are trying to- if you have, uh,
if you download the US census database,
if your data, the United States census,
that's a very large dataset.
And you wanna predict housing prices,
from all across the United States,
um, that- that- that may have a dataset with many- many millions of examples.
And the disadvantage of batch gradient descent is that,
in order to make one update,
to your parameters, in order to even take a single step of gradient descent,
you need to calculate, this sum.
And if m is say a million or 10 million or 100 million,
you need to scan through your entire database,
scan through your entire dataset and calculate this for,
you know, 100 million examples and sum it up.
And so every single step of gradient descent
becomes very slow because you're scanning over,
you're reading over, right,
like 100 million training examples, uh, uh,
and uh, uh, before you can even, you know,
make one tiny little step of gradient descent.
Okay, um, yeah, and by the way,
I think- I feel like in today's era of
big data people start to lose intuitions about what's a big data-set.
I think even by today's standards,
like a hundred million examples is still very big, right,
I- I rarely- only rarely use a hundred million examples.
Um, I don't know,
maybe in a few years we'll look back on
a hundred million examples and say that was really small,
but at least today. Uh, yeah.
So the main disadvantage of batch gradient descent is,
every single step of gradient descent requires that you read through, you know,
your entire data-set, maybe terabytes of data-sets maybe- maybe- maybe, uh,
tens or hundreds of terabytes of data,
uh, before you can even update the parameters just once.
And if gradient descent needs, you know,
hundreds of iterations to converge,
then you'll be scanning through your entire data-set hundreds of times.
Right, or-or and then sometimes we train,
our algorithms with thousands or tens of thousands of iterations.
And so- so this- this gets expensive.
So there's an alternative to batch gradient descent. Um, and
let me just write out the algorithm here that we can talk about it,
which is going to repeatedly do this.
[NOISE] Oops, okay. Um, so this algorithm,
which is called stochastic gradient descent.
[NOISE] Um, instead of scanning through
all million examples before you update the parameters theta even a little bit,
in stochastic gradient descent,
instead, in the inner loop of the algorithm,
you loop through j equals 1 through m of taking a gradient descent step
using, the derivative of just one single example of just that,
uh, one example, ah,
oh, excuse me it's through i, right.
Yeah, so let i go from 1 to m,
and update theta j for every j.
So you update this for j equals 1 through n, update theta j,
using this derivative that when now this derivative
is taken just with respect to one training example- example I.
Okay, um, I'll- I'll just- alright and I guess you update this for every j.
Okay, and so, let me just draw a picture of what this algorithm is doing.
If um, this is the contour,
like the one you saw just now.
So the axes are, uh,
theta 0 and theta 1,
and the height of the surface right, denote the contours as j of theta.
With stochastic gradient descent,
what you do is you initialize the parameters somewhere.
And then you will look at your first training example.
Hey, lets just look at one house,
and see if you can predict that house as better,
and you modify the parameters to
increase the accuracy where you predict the price of that one house.
And because you're fitting the data just for one house, um, you know,
maybe you end up improving the parameters a little bit,
but not quite going in the most direct direction downhill.
And you go look at the second house and say,
hey, let's try to fit that house better.
And then you update the parameters.
And you look at third house, fourth house.
Right, and so as you run stochastic gradient descent,
it takes a slightly noisy, slightly random path.
Uh, but on average,
it's headed toward the global minimum, okay.
So as you run stochastic gradient descent-
stochastic gradient descent will actually, never quite converge.
In- with- with batch gradient descent,
it kind of went to the global minimum and stopped right,
uh, with stochastic gradient descent even as you won't run it,
the parameters will oscillate and won't ever quite converge because you're always
running around looking at different houses and trying to do better than
just that one house- and that one house- and that one house.
Uh, but when you have a very large data-set,
stochastic gradient descent, allows
your implementation- allows you algorithm to make much faster progress.
Uh, and so, um, uh,
uh- and so when you have very large data-sets,
stochastic gradient descent is used much more in practice than batch gradient descent.
[BACKGROUND]
Uh, yeah, is it possible to start with stochastic gradient descent and then switch over to batch gradient descent?
Yes, it is.
So, uh, boy, something that wasn't talked about in this class,
it's talked about in CS230 is Mini-batch gradient descent, where, um,
you don't- where you use say
a hundred examples at a time rather than one example at a time.
And so- uh, so that's another algorithm that I should use more often in practice.
I think people rarely- actually,
so- so in practice, you know,
when your dataset is large, we rarely,
ever switch to batch gradient descent,
because batch gradient descent is just so slow, right.
So, I-I know I'm thinking through concrete examples of problems I've worked on.
And I think that what- maybe actually maybe- I think that uh,
for a lot of- for- for modern machine learning,
where you have- if you have very- very large data sets, right so you know,
whether- if you're building a speech recognition system,
you might have like a terabyte of data,
right, and so, um,
it's so expensive to scan through a terabyte of data just reading it from disk,
right it's so expensive that you would
probably never even run one iteration of batch gradient descent.
Uh, and it turns out the- the-
there's one- one huge saving grace of stochastic gradient descent is,
um, let's say you run stochastic gradient descent, right,
and, you know, you end up with this parameter and that's the parameter you use,
for your machine learning system,
rather than the global optimum.
It turns out that parameter is actually not that bad, right,
you- you probably make perfectly fine predictions even if you don't
quite get to the like the global- global minimum.
So, uh, what you said I think it's a fine thing to do, no harm trying it.
Although in practice uh,
uh, in practice we
don't bother, I think in practice we usually use stochastic gradient descent.
The thing that actually is more common,
is to slowly decrease the learning rate.
So just keep using stochastic gradient descent,
but reduce the learning rate over time.
So it takes smaller and smaller steps.
So if you do that, then what happens is the size of the oscillations would decrease.
Uh, and so you end up oscillating or bouncing around the smaller region.
So wherever you end up,
may not be the global- global minimum,
but at least it'll- be it'll be closer to the global minimum.
Yeah, so decreasing the learning rate is used much more often.
Cool. Question. Yeah. [BACKGROUND]
Oh sure, when do you stop with stochastic gradient descent?
Uh, uh, plot to j of theta, uh, over time.
So j of theta is a cost function that you're trying to drag down.
So monitor j of theta as,
you know, is going down over time,
and then if it looks like it stopped going down,
then you can say, "Oh,
it looks like it looks like it stopped going down," then you stop training.
Although- and then- ah, uh, you know,
one nice thing about linear regression is that it has no local optimum and so,
um, uh, it- you run into these convergence debugging types of issues less often.
Where you're training highly non-linear things like neural networks,
we should talk about later in CS229 as well.
Uh, these issues become more acute.
Cool. Okay, great.
So, um, uh, yeah.
[BACKGROUND].
Oh, would your learning rate be 1 over n times linear regressions then? Not really,
it's usually much bigger than that.
Uh, uh, yeah, because if your learning rate was 1 over n times
that of what you'd use with batch gradient descent
then it would end up being as slow as batch gradient descent,
so it's usually much bigger.
Okay. So, um, so that's stochastic gradient descent and- and- so I'll tell you what I do.
If- if you have a relatively small dataset,
you know, if you have- if you have, I don't know,
like hundreds of examples maybe thousands of examples where,
uh, it's computationally efficient to do batch gradient descent.
If batch gradient descent doesn't cost too much,
I would almost always just use
batch gradient descent because it's one less thing to fiddle with, right?
It's just one less thing to have to worry about,
uh, the parameters oscillating,
but your dataset is too large that
batch gradient descent becomes prohibit- prohibitively slow,
then almost everyone will use, you know,
stochastic gradient descent or whatever more like stochastic gradient descent, okay?
All right, so, um, gradient descent,
both batch gradient descent and stochastic gradient descent is
an iterative algorithm meaning that you have to take multiple steps to get to,
you know, get near hopefully the global optimum.
It turns out there is another algorithm,
uh, and- and, um,
for many other algorithms we'll talk about in this course
including generalized linear models and neural networks and a few other algorithms, uh,
you will have to use gradient descent and so- and so we'll see gradient descent,
you know, as we develop multiple different algorithms later this quarter.
It turns out that for the special case of linear regression, uh, uh,
and I mean linear regression but not the algorithm we'll talk about next Monday,
not the algorithm we'll talk about next Wednesday,
but if the algorithm you're using is linear regression and exactly linear regression.
It turns out that there's a way to, uh,
solve for the optimal value of the parameters theta to just jump in
one step to the global optimum without needing to use an iterative algorithm,
right, and this- this one I'm gonna present next is called the normal equation.
It works only for linear regression,
doesn't work for any of the other algorithms I talk about later this quarter.
But [NOISE] um, uh,
let me quickly show you the derivation of that.
And, um, what I want to do is, uh,
give you a flavor of how to derive
the normal equation and where you end up with is you know,
wha- what- what I hope to do is end up with a formula that lets you
say theta equals some stuff where you just
set theta equals to that and in one step with a few matrix multiplications you
end up with the optimal value of theta that lands you right at the global optimum,
right, just like that, just in one step.
Okay. Um, and if you've taken, you know,
advanced linear algebra courses before or something,
you may have seen, um,
this formula for linear regression.
Wha- what a lot of linear algebra classes do
is, what some linear algebra classes do is cover the board with,
you know, pages and pages of matrix derivatives.
Um, what I wanna do is describe to you
a matrix derivative notation that allows you to derive
the normal equation in roughly four lines of linear algebra,
uh, rather than some pages and pages of
linear algebra and in the work I've done in machine learning you know,
sometimes notation really matters, right.
If you have the right notation you can solve some problems
much more easily and what I wanna do is,
um, uh, define this uh,
matrix linear algebra notation and then I don't wanna do all the steps of the derivation,
I wanna give you- give you a sense of the flavor of what it looks like and then,
um, I'll ask you to,
uh, uh, get a lot of details yourself, um,
in the- in the lecture notes where we
work out everything in more detail than I want to do algebra in class.
And, um, in problem set one you'll get to practice using this yourself to- to- to-,
you know, derive some additional things.
I've- I've found this notation really convenient,
right, for deriving learning algorithms.
Okay. So, um, I'm going to use the following notation.
Um, so J, right.
There's a function mapping from parameters to the real numbers.
So I'm going to define this- this is the derivative of J of theta with respect to theta,
where- remember theta is a three-dimensional vector says R3,
or actually it's R n+1, right.
If you have, uh, two features to the house if n=2,
then theta was 3 dimensional,
it's n+1 dimensional so it's a vector.
And so I'm gonna define the derivative with respect to theta of J of theta as follows.
Um, this is going to be itself a
3 by 1 vector
[NOISE].
Okay, so I hope this notation is clear.
So this is a three-dimensional vector with, uh, three components.
Alright so that's what I guess I'm.
So that's the first component is a vector,
there's a second and there's a third.
It's the partial derivative of J with respect to each of the three elements.
Um, and more generally,
in the notation we'll use,
um, let me give you an example.
Um, uh, let's say that a is a matrix.
So let's say that a is a two-by-two matrix.
Then, um, you can have a function,
right, so let's say a is, you know,
A1-1, A1-2, A2-1 and A2-2, right.
So A is a two-by-two matrix.
Then you might have some function um,
of a matrix A right,
then that's a real number.
So maybe f maps from A 2-by-2 to,
uh, excuse me, R 2-by-2, it's a real number.
So, um, uh, and so for example,
if f of A equals A11 plus A12 squared,
then f of, you know, 5,
6, 7, 8 would be equal to I guess 5 plus 6 squared, right.
So as we derive this,
we'll be working a little bit with functions that map from
matrices to real numbers and this is just one made up
example of a function that
inputs a matrix and maps the matrix, maps the values of matrix to a real number.
And when you have a matrix function like this,
I'm going to define the derivative with respect to A of f of A to be
equal to itself a matrix where the derivative of f of A with respect to the matrix A.
Uh, this itself will be a matrix with the same dimension of a and the elements of
this are the derivative with respect to the individual elements.
Actually, let me just write it like this.
[NOISE]
Okay. So if A was a 2-by-2 matrix
then the derivative of F of A with respect to A is itself
a 2-by-2 matrix and you compute this 2-by-2 matrix just by looking at F and taking,
uh, derivatives with respect to
the different elements and plugging them into the different,
the different elements of this matrix.
Okay. Um, and so in this particular example,
I guess the derivative respect to A of F of A.
This would be, um, [NOISE] right,
it would be- it would be that.
Ah and I got these four numbers by taking, um,
the definition of F and taking
the derivative with respect to A_1, 1 and plugging that here.
Ah, taking the derivative with respect to A_1,2 and
plugging that here and taking the derivative with respect to
the remaining elements and plugging them here which- which was 0.
Okay? So that's the definition of a matrix derivative. Yeah?
[inaudible].
Oh, yes. We're just using the definition for a vector.
Ah, N by 1 or N by 1 matrix.
Yes. And in fact that definition and
this definition for the derivative of J with respect to Theta these are consistent.
So if you apply that definition to a column vector,
treating a column vector as an N by 1 matrix or N,
I guess it would be N plus 1 by 1 matrix
then that- that specializes to what we described here.
[NOISE]
All right. So, um, let's see.
Okay. So, um, I want to leave the details of the lecture notes because there's
more lines of algebra which I won't do but it'll give you an overview
[NOISE] of what the derivation of the normal equation looks like.
Um, so onto this definition of a derivative of a- of a matrix,
um, the broad outline of what we're going to do is we're going to take J of Theta.
Right. That's the cost function.
Um, take the derivative with respect to Theta.
Right. Ah, since Theta is a vector so you want to take
the derivative with respect to Theta and you know well,
how do you minimize a function?
You take derivatives with [NOISE] respect to Theta and set it equal to 0.
And then you solve for the value of Theta so that the derivative is 0.
Right. The- the minimum, you know,
the maximum and minimum of a function is where the derivative is equal to 0.
So- so how you derive the normal equation is take this vector.
Ah, so J of Theta maps from a vector to a real number.
So we'll, take the derivatives respect to Theta set it to 0,0 and solve
for Theta and then we end up with a formula for Theta that lets you just,
um, ah, you know,
immediately go to the global minimum of the- of the cost function J of Theta.
And, and a lot of the build up,
a lot of this notation is, you know,
is there- what does this mean and is there
an easy way to compute the derivative of J of Theta?
Okay? So, um, ah,
so I hope you understand the lecture notes when hopefully you take a look at them,
ah, just a couple other derivations.
Um, if A [NOISE] is a square matrix.
So let's say A is a [NOISE] N,
N by N matrix.
So number of rows equals number of columns.
Um, I'm going to denote the trace of A
[NOISE] to be equal to [NOISE] the sum of the diagonal entries.
[NOISE] So sum of i of A_ii.
And this is pronounced the trace of A, um, and, ah,
and- and you can- you can also write this as
trace operator like the trace function applied to A
but by convention we often write trace of A without the parentheses.
And so this is called the trace of A.
[NOISE] So trace just means sum of diagonal entries and,
um, some facts about the trace of a matrix.
You know, trace of A is equal to
the trace of A transpose because if you transpose a matrix,
right, you're just flipping it along the- the 45 degree axis.
And so the the diagonal entries actually stay the same when you transpose the matrix.
So the trace of A is equal to trace of A transpose, um,
and then, ah, there-there are some other useful properties of,
um, the trace operator.
Um, here's one that I don't want to prove but that you could go home and prove
yourself with a-with a few with- with a little bit of work, maybe not,
not too much which is,
ah, if you define, um,
F of A [NOISE] equals trace of A times B.
So here if B is some fixed matrix, right, ah,
and what F of A does is it multiplies A and
B and then it takes the sum of diagonal entries.
Then it turns out that the derivative with respect to A of F of A is equal to,
um, B transpose [NOISE].
Um, and this is, ah, you could prove this yourself.
For any matrix B, if F of A is defined this way,
the de- the derivative is equal to B transpose.
Um, the trace function or the trace operator has other interesting properties.
The trace of AB is equal to the trace of BA.
Ah, um, you could- you could prove this from past principles,
it's a little bit of work to prove,
ah, ah, that- that you,
if you expand out the definition of A and B it should prove
that [NOISE] and the trace of A times B times
C is equal to [NOISE] the trace of C times A times B. Ah,
this is a cyclic permutation property.
If you have a multiply, you know,
multiply several matrices together you can always take one from
the end and move it to the front and the trace will remain the same.
[NOISE] And,
um, another one that is a little bit harder to prove is that the trace,
excuse me, the derivative of A trans- of
AA transpose C is [NOISE] Okay.
Yeah. So I think just as- just as for you know, ordinary,
um, calculus we know the derivative of X squared is 2_X.
Right. And so we all figured out that very well.
We just use it too much without- without having to re-derive it every time.
Ah, this is a little bit like that.
The trace of A squared C is,
you know, two times CA.
Right. It's a little bit like that but- but with- with matrix notation as there.
So think of this as analogous to D,
DA of A squared C equals 2AC.
Right. But that is like the matrix version of that.
[NOISE]
All right. So finally, um,
what I'd like to do is take J of Theta and express it in this,
uh, you know, matrix vector notation.
So we can take derivatives with respect to Theta,
and set the derivatives equal to 0,
and just solve for the value of Theta, right?
And so, um, let me just write out the definition of J of Theta.
So J of Theta was one-half sum from i equals 1 through m
of h of x i minus y i squared.
Um, and it turns out
that, um, all right.
It turns out that, um, if it is,
if you define a matrix capital X as follows.
Which is, I'm going to take the matrix capital X and take the training examples we have,
you know, and stack them up in rows.
So we have m training examples, right?
So so the X's were column vectors.
So I'm taking transpose to just stack up the training examples in,
uh, in rows here.
So let me call this the design matrix.
So the capital X called the design matrix.
And, uh, it turns out that if you define X this way,
then X times Theta,
there's this thing times Theta.
And the way a matrix vector multiplication
works is your Theta is now a column vector, right?
So Theta is, you know, Theta_0, Theta_1, Theta_2.
So the way that, um,
matrix-vector multiplication works is you multiply
this column vector with each of these in in turn.
And so this ends up being X1 transpose Theta, X2 transpose Theta,
down to X_m transpose Theta,
which is of course just the vector of all of the predictions of the algorithm.
And so if, um,
now let me also define a vector y to be taking all of the,
uh, labels from your training example,
and stacking them up into a big column vector, right.
Let me define y that way.
Um, it turns out that, um,
J of Theta can then be written as one-half of
X Theta minus y transpose X Theta minus y.
Okay. Um, and let me see.
Yeah. Let me just, uh, uh,
outline the proof, but I won't do this in great detail.
So X Theta minus y is going to be,
right, so this is X Theta, this is y.
So, you know, X Theta minus y is going to be this vector of h of x1 minus
y1 down to h of xm minus ym, right.
So it's just all the errors your learning algorithm is making on the m examples.
It's the difference between predictions and the actual labels.
And if you you remember,
so Z transpose Z is equal to sum over i Z squared, right.
A vector transpose itself is a sum of squares of elements.
And so this vector transpose itself is the sum of squares of the elements, right.
So so which is why, uh,
so so the cost function J of Theta is computed
by taking the sum of squares of all of these elements,
of all of these errors,
and and the way you do that is to take this vector,
your X Theta minus y transpose itself,
is the sum of squares of these,
which is exactly the error.
So that's why you end up with a,
this as the sum of squares of the, those error terms.
Okay. And, um, if some of the steps don't quite make sense,
really don't worry about it.
All this is written out more slowly and carefully in the lecture notes.
But I wanted you to have a sense of the, uh,
broad arc of the of the big picture of
their derivation before you go through them
yourself in greater detail in the lecture notes elsewhere.
So finally, what we want to do is take the derivative with respect to Theta of J of Theta,
and set that to 0.
And so this is going to be equal to the derivative of
one-half X Theta minus y transpose X Theta minus y. Um, and so I'm gonna,
I'm gonna do the steps really quickly, right.
So the steps require some of the little properties of
traces and matrix derivatives I wrote down briefly just now.
But so I'm gonna do these very quickly without getting into the details, but, uh,
so this is equal to one-half derivative of Theta of, um.
So take transposes of these things.
So this becomes Theta transpose X transpose minus y transpose.
Right. Um, and then, uh,
kind of like expanding out a quadratic function, right.
This is, you know, A minus B times C minus D. So you can just AC minus AD this and so on.
So I'll just write this out.
All right. And so, uh,
what I just did here this is similar to how, you know,
ax minus b times ax minus b,
is equal to a squared x squared minus axb minus bax plus b squared.
Is it's kind of, it's just expanding out a quadratic function.
Um, and then the final step
is, yeah, go ahead.
[BACKGROUND] Oh, is that right?
Oh yes, thank you. Thank you.
Um, and then the final step is,
you know, for each of these four terms; first,
second, third, and fourth terms,
to take the derivative with respect to Theta.
And if you use some of the formulas I was alluding to over there,
you find that the derivative, um,
which which I don't want to show the derivation of,
but it turns out that the derivative is, um,
X transpose X Theta plus X transpose X Theta minus,
um, X transpose y minus X transpose y,
um, and we're going to set this derivative.
Actually not, let me just do this.
And so this simplifies to X transpose X Theta minus X transpose y.
And so as as described earlier,
I'm gonna set this derivative to 0.
And how to go from this step to that step is using the matrix derivatives,
uh, explained in more detail in the lecture notes.
And so the final step is,
you know, having set this to 0,
this implies that X transpose X Theta equals X transpose y.
Uh, so this is called the normal equations.
And the optimum value for Theta is Theta equals
X transpose X inverse, X transpose y.
Okay. Um, and if you implement this, um,
then, you know, you can in basically one step,
get the value of Theta that corresponds to the global minimum.
Okay. Um, and and and again,
common question I get is one is, well,
what if X is non-invertible?
Uh, what that usually means is you have have redundant features,
uh, that your features are linearly dependent.
Uh, but if you use something called the pseudo inverse,
you kind of get the right answer if that's the case.
Although I think the, uh, even more right
answer is if you have linearly dependent features,
probably means you have the same feature repeated twice,
and I will usually go and figure out what features are actually repeated,
leading to this problem.
Okay. All right.
Uh, any last questions before- so that so that's the normal equations.
Hope you read through the detailed derivations in the lecture notes.
Any last questions before we break?
Okay.
[BACKGROUND]
Oh, yeah.
How do you choose a learning rate?
It's, it's, it's quite empirical, I think.
So most people would try different values,
and then just pick one. All right.
I think let's let's break.
If, if people have more questions,
when the TAs come up, we're going to keep taking questions.
Well, let's break for the day. Thanks everyone.
 What I'd like to do today is continue our discussion of supervised learning.
So last Wednesday, you saw the linear regression algorithm, uh,
including both gradient descent, how to formulate the problem,
then gradient descent, and then the normal equations.
What I'd like to do today is, um,
talk about locally weighted regression which is a,
a way to modify linear regressions and make it
fit very non-linear functions so you aren't just fitting straight lines.
And then I'll talk about
a probabilistic interpretation of linear regression and that will
lead us into the first classification algorithm
you see in this class called logistic regression,
and we'll talk about an algorithm called Newton's method for logistic regression.
And so the dependency of ideas in this class is that,
um, locally weighted regression will depend on what you learned in linear regression.
And then, um, we're actually gonna just
cover the key ideas of locally weighted regression,
and let you play with some of the ideas yourself in the,
um, problem set 1 which we'll release later this week.
And then, um, I guess,
give a probabilistic interpretation of linear regression,
logistic regression depend on that, um,
and Newton's method is for logistic regression, okay?
To recap the notation you saw on Wednesday,
we use this notation x_i, i- y_i to denote
a single training example where x_i was n + 1 dimensional.
So if you have two features,
the size of the house and the number of bedrooms,
then x_i would be 2 + 1, it would 3-dimensional because we have introduced a new,
uh, sort of fake feature x_0 which was always set to the value of 1.
Uh, and then yi,
in the case of regression is always a real number and what's
the number of training examples and what's the number of features
and, uh, this was the hypothesis, right?
It's a linear function of the features x, um,
including this feature x_0 which is always set to 1, and, uh,
j was the cost function you would minimize,
you minimize this as function of j to find the parameters
Theta for your straight line fit to the data, okay?
So that's what you saw last Wednesday.
Um, now if you have a dataset, that looks like that,
where this is the size of a house and this is the price of a house.
What you saw on Wednesday- last Wednesday,
was an algorithm to fit a straight line,
right to this data so the hypothesis was of the form
Theta 0 + Theta 1 x_0- x_0- Theta 1 x_1 to be very specific.
But with this dataset maybe it actually looks, you know,
maybe the data looks a little bit like that and so
one question that you have to address when, uh,
fitting models to data is what are the features you want?
Do you want to fit a straight line to this problem or do you
want to fit a hypothesis of the form, um,
Theta 1x + Theta 2x squared since this may be a quadratic function, right?
Now the problem with quadratic functions is that quadratic functions eventually start,
you know, curving back down so that will be a quadratic function.
This arc starts curving back down.
So maybe you don't want to fit a quadratic function.
Uh, instead maybe you want, uh, to fit something like that.
If- if housing prices sort of curved down a little bit but you don't want it
to eventually curve back down the way a quadratic function would, right?
Um, so- oh, and, and if you want to do this the way you would implement is
is you define the first feature x_1 = x
and the second feature x_2 = x squared or you
define x_1 to be equal to x and x_2 = square root of x,
right and by defining a new feature x_2 which would be the square root of x and square root of x.
Then the machinery that you saw from Wednesday of
linear regression applies to fit these types of,
um, these types of functions of the data.
So later this quarter you'll hear about feature selection algorithms,
which is a type of algorithm for automatically deciding,
do you want x squared as a feature or
square root of x as a feature or maybe you want,
um, log of x as a feature, right.
But what set of features, um,
does the best job fitting the data that you have
if it's not fit well by a perfectly straight line.
Um, what I would like to do today is- so,
so you'll hear about feature selection later this quarter.
What I want to share with you today is a different way
of addressing this out- this problem of
whether the data isn't just fit well by
a straight line and in particular I wanna share with you an idea called,
uh, locally weighted regression or locally weighted linear regression.
So let me use a slightly different, um, example to illustrate this.
Um, which is, uh, which is that, you know,
if you have a dataset that looks like that.
[NOISE] So it's pretty clear what the shape of this data is.
Um, but how do you fit a curve that,
you know, kind of looks like that, right?
And it's, it's actually quite difficult to find features, is it square root of x,
log of x, x cube, like third root of x, x the power of 2/3.
But what is the set of features that lets you do this? So
we'll sidestep all those problems with an algorithm called,
uh, locally weighted regression.
Um, okay, and to introduce a bit more machine learning terminology.
Um, in machine learning we sometimes distinguish between
parametric learning algorithms and non-parametric learning algorithms.
But in a parametric learning algorithm does, uh, uh,
you fit some fixed set of parameters such as Theta
i to data and so linear regression as you saw
last Wednesday is a parametric learning algorithm
because there's a fixed set of parameters, the Theta i's,
so you fit to data and then you're done, right.
Locally weighted regression will be our first exposure to
a non-parametric learning algorithm.
Um, and what that means is that the amount of data/parameters, uh,
you need to keep grows and in
this case it grows linearly with the size of the data,
with size of training set, okay?
So with the parametric learning algorithm,
no matter how big your training, uh,
your training set is, you fit the parameters Theta i.
Then you could erase a training set from
your computer memory and make predictions just using the parameters
Theta i and in a non-parametric learning algorithm which we'll see in a second,
the amount of stuff you need to keep around in
computer memory or the amount of stuff you
need to store around grows linearly as a function of the training set size.
Uh, and so this type of algorithm is your- may,
may, may not be great if you have a really,
really massive dataset because you need to keep all of the data around
your- in computer memory or on disk just to make predictions, okay?
So- but we'll see an example of this
and one of the effects of this is that with that, it'll,
it'll be able to fit that data that I drew up there, uh,
quite well without you needing to fiddle manually with features.
Um, and again you get to practice implementing locally weighted regression in the homework.
So I'm gonna go over the height of ideas relatively quickly and then let you,
uh, uh, gain practice, uh, in the problem set. All right.
So let me redraw that dataset, it'd be something like this.
[NOISE] All right.
So- so say you have a dataset like this.
Um, now for linear regression if you want to evaluate
h at a certain value of the input, right?
So to make a prediction at a certain value of x what you- for
linear regression what you do is you fit theta,
you know, to minimize this cost function.
[NOISE]
And then you return Theta transpose x, right?
So you fit a straight line and then, you know,
if you want to make a prediction at this value x you then return say the transpose x.
For locally weighted regression,
um, you do something slightly different.
Which is if this is the value of x
and you want to make a prediction around that value of x.
What you do is you look in a lo- local neighborhood
at the training examples close to that point x where you want to make a prediction.
And then, um, I'll describe this informally for now
but we'll- we'll formalize this in math for the second.
Um, but focusing mainly on these examples and,
you know, looking a little bit at further all the examples.
But really focusing mainly on these examples,
you try to fit a straight line like that,
focusing on the training examples that are close to where you want to make a prediction.
And by close I mean the values are similar, uh, on the x axis.
The x values are similar.
And then to actually make a prediction, you will, uh,
use this green line that you just fit to make a prediction at that value of x, okay?
Now if you want to make a prediction at a different point.
Um, let's say that, you know,
the user now says, "Hey, make a prediction for this point."
Then what you would do is you focus on this local area,
kinda look at those points.
Um, and when I say focus say, you know,
put most of the weights on these points but you
kinda take a glance at the points further away,
but mostly the attention is on these for the straight line to that,
and then you use that straight line to make a prediction, okay.
Um, and so to formalize this in locally weighted regression, um,
you will fit Theta to minimize a modified cost function
[NOISE]
Where wi is a weight function.
Um, and so a good- well the default choice,
a common choice for wi will be this.
[NOISE] Right, um, I'm gonna add something to this equation a little bit later.
But, uh, wi is a weighting function where,
notice that this, this formula has a defining property, right?
If xi - x is small, then the weight will be close to 1.
Because, uh, if xi x- so x is the location where you want to make
a prediction and xi is the input x for your ith training example.
So wi is a weighting function, um,
that's a value between 0 and 1 that tells you how much should you pay attention to
the values of xi, yi when fitting say this green line or that red line.
And so if xi - x is small
so that's a training example that is
close to where you want to make the prediction for x.
Then this is about e to the 0, right,
e to the -0 if the- if the numerator here is
small and e to the 0 is close to 1.
Right, um, and conversely if xi - x is large,
then wi is close to 0.
And so if xi is very far away so let's see if it's fitting this green line.
And this is your example, xi yi then it's saying, give this
example all the way out there if you're fitting the green line where you look at
this first x saying that example should have weight fairly close to 0, okay?
Um, and so if you, um, look at the cost function,
the main modification to the cost function we've made is that
we've added this weighting term, right?
And so what locally weighted regression does is the same.
If an example xi is far from where you wanna make
a prediction multiply that error term by 0 or by a constant very close to 0.
Um, whereas if it's close to where you wanna make
a prediction multiply that error term by 1.
And so the net effect of this is that this is summing if, if,
you know, the terms multiplied by 0 disappear, right?
So the net effect of this is that the sums over essentially only the terms, uh,
for the squared error for the examples that are close to
the value, close to the value of x where you want to make a prediction, okay?
Um, and that's why when you fit Theta to minimize this,
you end up paying attention only to the points,
only to the examples close to where you wanna make a prediction and
fitting a line like a green line over there, okay?
Um, so let me draw a couple more pictures to- to- to illustrate this.
Um, so if- let me draw a slightly smaller data set just to make this easier to illustrate.
Um, so that's your training set.
So there's your examples x1, x2, x3, x4.
And if you want to make a prediction here, right,
at that point x, then, um,
this curve here looks the- the- the shape of this curve is actually like this, right?
Um, and this is the shape of a Gaussian bell curve.
But this has nothing to do with a Gaussian density,
right, so this thing does not integrate to 1.
So- so it's just sometimes you ask well, is
this- is this using a Gaussian density? The answer is no.
Uh, this is just a function that, um,
is shaped a lot like a Gaussian but, you know,
Gaussian densities, probability density functions
have to integrate to 1 and this does not.
So there's nothing to do with a Gaussian probability density. Question?
So how- how do you choose the width of the-
Oh, so how do you choose the width, lemmi get back to that.
Yeah. Um, and so for this example this height
here says give this example a weight equal to the height of that thing.
Give this example a weight to the height of this,
height of this, height of that, right?
Which is why if you actually- if you have an example this way out there,
you know, is given a weight that's essentially 0.
Which is why it's weighting only the nearby examples when trying to fit a straight line,
right, uh, for the- for making predictions close to this, okay?
Um, now so one last thing that I wanna mention which is,
um, the- the- the question just now which is
how do you choose the width of this Gaussian density, right?
How fat it is or how thin should it be?
Um, and this decides how big a neighborhood should you look in
order to decide what's the neighborhood of points you should use to fit this,
you know, local straight line.
And so, um, for Gaussian function like this, uh,
this- I'm gonna call this the, um, bandwidth parameter tau, right?
And this is a parameter or a hyper-parameter of the algorithm.
And, uh, depending on the choice of tau, um,
uh, you can choose a fatter or a thinner bell-shaped curve,
which causes you to look in a bigger or a narrower window in order to decide,
um, you know, how many nearby examples to use in order to fit the straight line, okay?
And it turns out that, um, and I wanna leave-
I wanna leave you to discover this yourself in the problem set.
Um, if- if you've taken a little bit of machine learning elsewhere I've
heard of the terms [inaudible] Test. It's on?
Okay, good. It was on. Good.
It turns out that, um,
the choice of the bandwidth tau has an effect on,
uh, overfitting and underfitting.
If you don't know what those terms mean don't worry about it,
we'll define them later in this course.
But, uh, what you get to do in the problem set is, uh,
play with tau yourself and see why, um,
if tau is too broad, you end up fitting, um,
you end up over-smoothing the data and if tau is too
thin you end up fitting a very jagged fit to the data.
And if any of these things don't make sense
yet don't worry about it they'll make sense after you
play a bit in the- in the problem set, okay?
Um, so yeah, since- since you- you play with the varying tau in the problem
set and see for yourself the net impact of that, okay? Question?
Is tau raised to power there or is that just a- just a- [NOISE]
Thank you, uh, this is tau squared.
Yeah. Yeah.
So- so what happens if you need to invert, uh, the [inaudible]
What happens if you need to infer the value of h outside the scope of the dataset?
It turns out that you can still use this algorithm.
It's just that, um, its results may not be very good.
Yeah. It- it- it depends I guess.
Um, locally linear regression is usually not greater than extrapolation,
but then most- many learning algorithms are not good at extrapolation.
So all- all the formulas still work, you can still implement this.
But, um, yeah.
You can also try- you can also try a linear problem set and
see what happens. Yeah. One last question.
Is it possible to have like a vertical tau depending on whether some parts of your data have lots of- Yeah.
Yes, this is mostly for the variable tau depending-
Uh, uh, yes, it is, uh,
and there are quite complicated ways to choose
tau based on how many points there on the local region and so on.
Yes. There's a huge literature on different formulas actually for example
instead of this Gaussian bump thing, uh, there's,
uh, sometimes people use that triangle shape function.
So it actually goes to zero outside some small rings.
So there are, there are many versions of this algorithm.
Um, so I tend to use, uh,
a locally weighted linear regression when,
uh, you have a relatively low dimensional data set.
So when the number of features is not too big, right?
So when n is quite small like 2 or 3 or something and we have a lot of data.
And you don't wanna think about what features to use, right.
So- so that's the scenario.
So if, if you actually have a data set that looks like these up in drawing, you know,
locally weighted linear regression is,
is a, is a pretty good algorithm.
Um, just one last question. Then we're moving on.
When you have a lot of data like this, does it usually complicate the question, since you're [BACKGROUND]
Oh, sure. Yes, if you have a lot of data, that wants to be computationally expensive,
yes, it would be.
Uh, I guess a lot of data is relative.
Uh, yes we have, you know, 2, 3, 4 dimensional data
and hundreds of examples, I mean, thousands of examples.
Uh, it turns out the computation needed to fit the minimization is, uh,
similar to the normal equations,
and so you- it involves solving a linear system of
equations of dimension equal to the number of training examples you have.
So, if that's, you know, like a thousand or a few thousands, that's not too bad.
If you have millions of examples then,
then there are also multiple scaling algorithms like KD trees and
much more complicated algorithms to do this when you
have millions or hun- tens of millions of examples.
Yeah. Okay. So you get a better sense of this algorithm when you play with it,
um, in the problem set.
Now, the second topic-one of- so I'm gonna put aside locally weighted regression.
We won't talk about that set of ideas anymore, uh, today.
But, but what I wanna do today is, uh,
on last Wednesday I had said that- I had promised last Wednesday that
today I'll give a justification for why we use the squared error, right.
Why the squared error and why not to the fourth power or absolute value?
Um, and so, um, what I want to show you today- now is
a probabilistic interpretation of linear regression and
this probabilistic interpretation will put us in
good standing as we go on to logistic regression today,
uh, and then generalize linear models later this week.
We're going to keep up to-keep the notation there so we could continue to refer to it.
So why these squares? Why squared error?
Um, I'm gonna present a set of assumptions
under which these squares using squared error falls out very naturally.
Which is let's say for housing price prediction.
Let's assume that there's a true price of every house y i which is x transpose,
um, say there i, plus epsilon i.
Where epsilon i is an error term.
That includes, um, unmodeled effects,
you know, and just random noise.
Okay. So let's assume that the way, you know,
housing prices truly work is that every house's price
is a linear function of the size of the house and number of bedrooms,
plus an error term that captures unmodeled effects such as maybe one day
that seller is in an unusually good mood or
an unusually bad mood and so that makes the price go higher or lower.
We just don't model that, um, as well as random noise, right.
Or, or maybe the model will skew this street, you know,
preset to persistent capture, that's one of the features,
but other things have an impact on housing prices.
Um, and we're going to assume that, uh,
epsilon i is distributed Gaussian would mean 0 and co-variance sigma squared.
So I'm going to use this notation to mean- so the way you read
this notation is epsilon i this twiddle you pronounce as, it's distributed.
And then stripped n parens 0, sigma squared.
This is a normal distribution also called the Gaussian Distribution, same thing.
Normal distribution and Gaussian distribution mean the same thing.
The normal distribution would mean 0 and,
um, a variance sigma squared.
Okay. Um, and what this means is that the probability density of epsilon i is- this is
the Gaussian density, 1 over root 2 pi sigma e to
the negative epsilon i squared over 2 sigma squared.
Okay. And unlike the Bell state-the bell-shaped curve
I used earlier for locally weighted linear regression,
this thing does integrate to 1, right.
This-this function integrates to 1.
Uh, and so this is a Gaussian density,
this is a prob-prob-probability density function.
Um, and this is the familiar, you know,
Gaussian bell-shaped curve with mean 0 and co-variance- and variance,
uh, uh, sigma squared where sigma kinda controls the width of this Gaussian.
Okay? Uh, and if you haven't seen Gaussian's for a while we'll go over some of the, er,
probability, probability pre-reqs as well in the classes, Friday discussion sections.
So, in other words, um,
we assume that the way housing prices are determined is that,
first is a true price theta transpose x.
And then, you know, some random force of nature.
Right, the mood of the seller or, I-I-I don't know-I don't have other factors, right.
Perturbs it from this true value, theta transpose xi.
Um, and the huge assumption we're gonna make is that the epsilon I's
these error terms are IID. And IID
from statistics stands for Independently and Identically Distributed.
And what that means is that the error term for one house is independent,
uh, as the error term for a different house.
Which is actually not a true assumption.
Right. Because, you know, if,
if one house is priced on one street is
unusually high, probably a price on
a different house on the same street will also be unusually high.
And so- but, uh, this assumption that these epsilon I's are
IID since they're independently and identically distributed.
Um, is one of those assumptions that,
that, you know, is probably not absolutely true,
but may be good enough that if you make this assumption,
you get a pretty good model.
Okay. Um, and so let's see.
Under these set of assumptions this implies that [NOISE] the density or
the probability of y i given x i and theta this is going to be this.
Um, and I'll, I'll take this and write it in another way.
In other words, given x and theta,
what's the density- what's the probability of a particular house's price?
Well, it's going to be Gaussian with mean given
by theta transpose xi or theta transpose x,
and the variance is, um, given by sigma squared.
Okay. Um, and so, uh,
because the way that the price of a house is determined
is by taking theta transpose x with the, you know,
quote true price of the house and then adding
noise or adding error of variance sigma squared to it.
And so, um, the,
the assumptions on the left imply that given x and theta,
the density of y, you know, has this distribution.
Which is- really this is the random variable y,
and that's the mean, right,
and that's the variance of the Gaussian density.
Okay. Now, um, two pieces of notation.
Um, I want to, one that you should get familiar with.
Um, the reason I wrote the semicolon here is, uh,
that- the way you read this equation is the semicolon should be read as parameterized as.
Right, um, and so because, uh, uh, the,
the alternative way to write this would be to say P of xi given yi,
excuse me, P of y given xi comma theta.
But if you were to write this notation this way,
this would be conditioning on theta,
but theta is not a random variable.
So you shouldn't condition on theta,
which is why I'm gonna write a semicolon.
And so the way you read this is,
the probability of yi given xi and parameterize, oh,
excuse me, parameterized by theta is equal to that formula, okay?
Um, if, if, if you don't understand this distinction,
again, don't worry too much about it.
In, in statistics there are multiple schools of statistics called Bayesian statistics,
frequentist statistics, this is a frequentist interpretation.
Uh, for the purposes of machine learning, don't worry about it,
but I find that being more consistent with terminology
prevents some of our statistician friends from getting really upset, but, but,
but, you know, I'll try to follow statistics convention.
Uh, so- because just only unnecessary flack I guess,
um, but for the per- for practical purposes this is not that important.
If you forget this notation on your homework.
don't worry about it we won't penalize you,
but I'll try to be consistent.
Um, but this just means that theta in this view is not a random variable,
it's just theta is a set of parameters that parameterizes this probability distribution.
Okay? Um, and the way to read the second equation is, um,
when you write these equations usually don't write them with parentheses,
but the way to parse this equation is to say that this thing is a random variable.
The random variable y given x and parameterized by theta.
This thing that I just drew in
green parentheses is just a distributed Gaussian with that distribution, okay?
All right. Um, any questions about this?
Okay. So it turns out that
[NOISE] if you are willing to make those assumptions,
then linear regression, um,
falls out almost naturally of the assumptions we just made.
And in particular, under the assumptions we just made, um,
the likelihood of the parameters theta,
so this is pronounced the likelihood of the parameters theta,
uh, L of theta which is defined as the probability of the data.
Right? So this is probability of all the values of y of y1
up to ym given all the xs and given,
uh, the parameters theta parameterized by theta.
Um, this is equal to the product from I equals
1 through m of p of yi given xi parameterized by theta.
Um, because we assumed the examples were- because we assume the errors are IID, right,
that the error terms are independently and identically distributed to each other,
so the probability of all of the observations,
of all the values of y in your training set is equal to the product of the probabilities,
because of the independence assumption we made.
And so plugging in the definition of p of
y given x parameterized by theta that we had up there,
this is equal to product of that.
Okay? Now, um, again, one more piece of terminology.
Uh, you know, another question I've always been asked if you say, hey, Andrew,
what's the difference between likelihood and probability, right?
And so the likelihood of the parameters
is exactly the same thing as the probability of the data,
uh, but the reason we sometimes talk about likelihood,
and sometimes talk of probability is, um, we think of likelihood.
So this, this is some function, right?
This thing is a function of the data as well as a function of the parameters theta.
And if you view this number, whatever this number is,
if you view this thing as a function of the parameters holding the data fixed,
then we call that the likelihood.
So if you think of the training set the data as a fixed thing,
and then varying parameters theta,
then I'm going to use the term likelihood.
Whereas if you view the parameters theta as fixed and maybe varying the data,
I'm gonna say probability, right?
So, so you hear me use- well,
I'll, I'll try to be consistent.
I find I'm, I'm pretty good at being consistent but not perfect,
but I'm going to try to say likelihood of the parameters,
and probability of the data even though
those evaluate to the same thing as just, you know,
for this function, this function is a function of theta and
the parameters which one are you viewing as
fixed and which one are you viewing as, as variables.
So when you view this as a function of theta,
I'm gonna use this term likelihood.
Uh, but- so, so hopefully you hear me say likelihood of the parameters.
Hopefully you won't hear me say likelihood of the data, right?
And, and similarly, hopefully you hear me say probability of
the data and not the probability of the parameters, okay? Yeah.
[inaudible].
Like other parameters.
[inaudible].
Uh, okay.
So probability of the data.
No. Uh, uh, theta, I got it sorry, yes.
Likelihood of theta. Got it.
Yes. Sorry. Yes. Likelihood of theta.
That's right.
[inaudible]. Oh, uh, no. So- no.
Uh, uh, so theta is a set of parameters,
it's not a random variable.
So we- likelihood of theta doesn't mean theta is a random variable.
Right. Cool. Yeah. Thank you.
Um, by the way, the, the,
the stuff about what's a random variable and what's not,
the semicolon versus comma thing.
We explained this in more detail in the lecture notes.
To me this is part of, um, uh, you know,
a little bit paying homage to the- to the religion of Bayesian frequencies versus Bayesian,
uh, frequentist versus Bayesians in statistics.
From a- from a machine- from an applied machine learning
operational what you write code point of view,
it doesn't matter that much.
Uh, yeah. But theta is not a random variable,
we have likelihood of parameters which are not a variable. Yeah. Go ahead.
[inaudible].
Oh, what's the rationale for choosing,
uh, oh, sure, why is epsilon i Gaussian?
So, uh, uh, turns out because of central limit theorem,
uh, from statistics, uh, most error distributions are Gaussian, right?
If something is- if there's an era that's made up of
lots of little noise sources which are not too correlated,
then by central limit theorem it will be Gaussian.
So if you think that, most perturbations are,
the mood of the seller,
what's the school district, you know,
what's the weather like, or access to transportation,
and all of these sources are not too correlated,
and you add them up then the distribution will be Gaussian.
Um, and, and I think- well, yeah.
So you can use the central limit theorem,
I think the Gaussian has become a default noise distribution.
But for things where the true noise distribution is very far from Gaussian,
uh, this model does do that as well.
And in fact, for when you see generalized linear models on Wednesday,
you see when- how to generalize all of
these algorithms to very different distributions like Poisson, and so on.
All right. So, um, so we've seen the likelihood of the parameters theta.
Um, so I'm gonna use lower case l to denote the log-likelihood.
And the log-likelihood is just the log of the likelihood.
Um, and so- well, just- right.
And so, um, log of a product is equal to the sum of the logs.
Uh, and so this is equal to-
and so this is m log 1 over root.
Okay? Um. And so, um, one of the, uh, you know,
well-tested letters in statistics estimating parameters is to
use maximum likelihood estimation or MLE
which means you choose theta
to maximize the likelihood, right?
So given the data set,
how would you like to estimate theta?
Well, one natural way to choose theta is to choose
whatever value of theta has a highest likelihood.
Or in other words, choose a value of theta so that that value of
theta maximizes the probability of the data, right?
And so, um, for- to simplify the algebra rather than
maximizing the likelihood capital L is actually easier to maximize the log likelihood.
But the log is a strictly monotonically increasing function.
So the value of theta that maximizes
the log likelihood should be the same as
the value of theta that maximizes the likelihood.
And if you divide the log likelihood, um,
we conclude that if you're using maximum likelihood estimation,
what you'd like to do is choose a value of theta that maximizes this thing, right?
But, uh, this first term is just a constant,
theta doesn't even appear in this first term.
And so what you'd like to do is choose the value of
theta that maximizes this second term.
Ah, notice there's a minus sign there.
And so what you'd like to do is,
uh, uh, i.e, you know,
choose theta to minimize this term.
Right. Also, sigma squared is just a constant.
Right. No matter what sigma squared is,
you know, so, so, uh,
so if you want to minimize this term, excuse me,
if you want to maximize this term,
negative of this thing,
that's the same as minimizing this term.
Uh, but this is just J of theta.
The cost function you saw earlier for linear regression.
Okay? So this little proof shows that,
um, choosing the value of theta to minimize the least squares errors,
like you saw last Wednesday,
that's just finding the maximum likelihood estimate
for the parameters theta under this set of assumptions we made,
that the error terms are Gaussian and IID.
Okay, go ahead. Oh, thank you.
Yes. Great. Thanks. Go ahead.
[inaudible].
Oh, is there a situation where using this formula
instead of least squares cost function will be a good idea?
No. So this- I think this derivation shows that
this- this is completely equivalent to least squares.
Right. That if- if you want- if you're willing
to assume that the error terms are Gaussian and
IID and if you want to use
Maximum Likelihood Estimation which is a very natural procedure in statistics,
then, you know, then you should use least squares. Right. So yeah.
If you know for some reason that the errors are not IID, like, is there a better way to figure out a better cost function?
If you know for some reason errors are not IID, could you figure out a better cost function? Yes and no.
I think that, um, you know, when building learners algorithms,
ah, often we make model- we make assumptions about the world that we just know
are not 100% true because it leads to algorithms that are computationally efficient.
Um, and so if you knew that
your- if you knew that your training set was very very non IID,
there are- there're more sophisticated models you could build.
But, um, ah, ah, yeah.
But- but very often we wouldn't bother I think.
Yeah. More often than not we might not bother.
Ah, I can think of a few special cases where you
would bother there but only if you think the assumption is really really bad.
Ah, if you don't have enough data or something- something. Quite- quite rare.
All right. Um, lemme think why, all right.
I want to move on to make sure we get through the rest of things.
Any burning questions? Yeah, okay, cool.
All right. Um, so out of this machinery.
Right. So- so- so what did we do here?
Was we set up a set of probabilistic assumptions,
we made certain assumptions about P of Y given X,
where the key assumption was Gaussian errors in IID.
And then through maximum likelihood estimation,
we derived an algorithm which turns out to be exactly the least squares algorithm.
Right? Um, what I'd like to do is take this framework,
ah, and apply it to our first classification problem.
Right. And so the- the key steps are, you know, one,
make an assumption about P of Y given X,
P of Y given X parameters theta,
and then second is figure out maximum likelihood estimation.
So I'd like to take this framework and apply it to a different type of problem,
where the value of Y is now either 0 or 1.
So is a classification problem.
Okay? So, um, let's see.
So the classification problem.
In our first classification problem,
we're going to start with binary classification.
So the value of Y is either 0 or 1.
And sometimes we call this binary classification because there are two clauses.
Classification. Right. Um, and so right- so that's
a data set where I guess this is X and this is Y. Um, so
something that's not a good idea is to apply linear regression to this data set.
Some- sometimes you will do it and maybe you'll get away
with it but I wouldn't do it and here's.
Which is, um, is- is tempting to just fit a straight line to
this data and then take the straight line and threshold it at 0.5,
and then say, oh, if it's above 0.5 round off to 1,
if it's below 0.5 round it off to 0.
But it turns out that this, um, is not a good idea,
uh, for classification problems.
And- and here's why?
Which is- for this data set it's really obvious what the- what the pattern is.
Right? Everything to the left of this point predict 0.
Everything to the right of that point predict 1.
But let's say we now change the data set to just add one more example there.
Right. And the pattern is still really obvious.
It says everything to the left of this predict 0,
everything to the right of that predict 1.
But if you fit a straight line to this data set with this extra one point there,
and just not even the outlier it's really
obvious at this point way out there should be labeled one.
But with this extra example, um,
if we fit a straight line to the data,
you end up with maybe something like that.
Um, and somehow adding this one example,
it really didn't change anything, right?
But somehow the straight line fit moved from the green line to the, uh,
moved from the blue line to the green line.
And if you now threshold it at 0.5,
you end up with a very different decision boundary.
And so linear regression is just not a good algorithm for classification.
Some people use it and sometimes they get lucky and it's not too bad but
I- I- I personally never use linear regression for classification algorithms.
Right. Because you just don't know if you end up with
a really bad fit to the data like this, okay?
Um, so oh and- and- and the other unnatural thing
about using linear regression for a classification problem is that,
um, you know for a classification problem that the values are,
you know, 0 or 1.
Right. And so it outputs negative values or values even
greater than 1 seems- seems strange, um.
So what I'd like to share with you now is really,
probably by far the most commonly used
classification algorithm ah, called logistic regression.
Now let's say the two learning algorithms
I probably use the most often are linear regression and logistic regression.
Yeah, probably these two, actually. Um, and, uh, this is the algorithm.
So, um, as- as we designed a logistic regression algorithm,
one of the things we might naturally want is for
the hypothesis to output values between 0 and 1.
Right. And this is mathematical notation for the values for H of X or H prime,
H subscript theta of X, uh, lies in the set from 0 to 1.
Right? This 0 to 1 square bracket is the set of all real numbers from 0 to 1.
So this says, we want the hypothesis output values in you know
between 0 and 1, so that in the set of all numbers between z- from 0 to 1.
Um, and so we're going to choose the following form of the hypothesis.
Um, so. Okay. So we're gonna define a function,
g of z, that looks like this.
And this is called the sigmoid, uh,
or the logistic function.
Uh, these are synonyms,
they mean exactly the same thing.
So, uh, it can be called the sigmoid function,
or the logistic function,
it means exactly the same thing.
But we're gonna choose a function, g of z.
Uh, and this function is shaped as follows.
If you plot this function,
you find that it looks like this.
Um, where if the horizontal axis is z,
then this is g of z.
And so it crosses x intercept at 0,
um, and it, you know,
starts off, well, really,
really close to 0,
rises, and then asymptotes towards 1.
Okay? And so g of z output values are between 0 and 1.
And, um, what logistic regression does is instead of- let's see.
So previously, for linear regression,
we had chosen this form for the hypothesis, right?
We just made a choice that we'll say
the housing prices are a linear function of the features x.
And what logistic regression does is theta transpose x could be bigger than 1,
it can be less than 0, which is not very natural.
But instead, it's going to take theta transpose x and pass it
through this sigmoid function g. So this force,
the output values only between 0 and 1.
Okay? Um, so you know,
when designing a learning algorithm, uh,
sometimes you just have to choose the form of the hypothesis.
How are you gonna represent the function h,
or h of- h subscript theta.
And so we're making that choice here today.
And if you're wondering,
you know, there are lots of functions that we could have chosen, right?
There are lots of why, why not, why not this function?
Or why not, you know, there are lots of functions with vaguely this shape,
they go between 0 and 1.
So why are we choosing this specifically?
It turns out that there's a broader class of algorithms called generalized linear models.
You'll hear about on Wednesday, uh,
of which this is a special case.
So we've seen linear regression,
you'll see logistic regression in a second, and on Wednesday,
you'll see that both of these examples of a much bigger set
of algorithms derived using a broader set of principles.
So, so for now, just, you know,
take my word for it tha- that we want to use the logistic function.
Uh, uh, it'll turn out- you'll see on Wednesday that there's
a way to derive even this function from,
uh, from more basic principles,
rather than just putting all this, this all out.
But for now, let me just pull this out of a hat and say,
that's the one we want to use.
Okay.
[NOISE]
So, um, let's make some assumptions
about the distribution of y given x parameterized by theta.
So I'm going to assume that the data has the following distribution.
The probability of y being 1, uh, again,
from the breast cancer prediction that we had,
from, uh, the first lecture.
Right? It will be the chance of a tumor being cancerous,
or being, um, um, malignant.
Chance of y being 1, given the size of the tumor,
that's the feature x parameterized by theta.
That this is equal to the output of your hypothesis.
So in other words, we're gonna assume that, um,
what you want your learning algorithm to do is input
the features and tell me what's the chance that this tumor is malignant.
Right? What's the chance that y is equal to 1?
Um, and by logic, I guess,
because y can be only 1 or 0,
the chance of y being equal to 0,
this has got to be 1 minus that.
Right? Because if a tumor has a 10% chance of being malignant,
that means it has a 1 minus that.
It means it must have a 90% chance of being benign.
Right? Since these two probabilities must add up to 1. Okay? Yeah.
[inaudible]
Say that again.
[inaudible].
Oh, can we change the parameters here? Yes, you can,
but I'm not- yeah.
But I think just to stick with convention in logistic regression. You, you- yeah.
Sure. We can assume that p of y equals 1 was this,
and p of y equals 1 was that, but I think either way.
It's just one you call positive example,
one you call a negative example.
Right. So, so, uh, use this convention.
Okay. Um, and now,
bearing in mind that y, right?
By definition, because it is a binary classification problem.
But bear in mind that y can only take on two values, 0 or 1.
Um, there's a nifty,
sort of little algebra way to take these two equations and write them in one equation,
and this will make some of the math a little bit easier.
When I take these two equations,
take these two assumptions and take these two facts,
and compress it into one equation, which is this.
[NOISE] [BACKGROUND] Okay?
Oh, and I dropped the theta subscript just to simplify the notation of it.
But I'm, I'm gonna be a little bit sloppy sometimes.
Well, a little less formal,
whether I write the theta there or not.
Okay? Um, but these two definitions of p of y given x parameterized by theta,
bearing in mind that y is either 0 or 1,
can be compressed into one equation like this.
Uh, and, and let me just say why.
Right? It's because if y- use a different color.
Right. If y is equal to 1,
then this becomes h of x to the power of 1 times this thing to the power of 0.
Right? If y is equal to 1, then,
um, 1 - y is 0.
And, you know, anything to the power of 0 is just equal to 1.
[NOISE] And so if y is equal to 1,
you end up with p of y given x parameterized by theta equals h of x.
Right? Which is just what we had there.
And conversely, if y is equal to 0,
then, um, this thing will be 0, and this thing will be 1.
And so you end up with p of y given x parameterized theta is equal to 1 minus h of x,
which is just equal to that second equation.
Okay? Right. Um, and so this is a nifty way
to take these two equations and compress them into one line,
because depending on whether y is 0 or 1,
one of these two terms switches off,
because it's exponentiated to the power of 0.
Um, and anything to the power of 0 is just equal to 1.
Right? So one of these terms is just, you know, 1.
Just leaving the other term, and just selecting the,
the appropriate equation, depending on whether y is 0 or 1.
Okay? So with that, um, uh, so with this little, uh,
on a notational trick,
it will make the data derivations simpler.
Okay? Um, yeah. So let me use a new board.
[NOISE] I want that.
All right. Actually we can reuse along with this.
All right. So, uh,
we're gonna use maximum likelihood estimation again.
So let's write down the likelihood of the parameters.
Um, so well, it's actually p of all the y's given all the
x's parameterized by theta was equal to this, uh,
which is now equal to product from i equals 1 through m,
h of x_i to the power of y_i,
times 1 minus h of x_i to the power of 1 minus y_i.
Okay. Where all I did was take this definition of p of
y given x parameterized by theta, uh, you know, from that,
after we did that little exponentiation trick and wrote it in here.
Okay. Um. [NOISE]
And then, uh, with maximum likelihood estimation
we'll want to find the value of theta that maximizes the likelihood,
maximizes the likelihood of the parameters.
And so, um, same as what we did for linear regression to make the algebra,
you have to, to, to make the algebra a bit more simple,
we're going to take the log of the likelihood and so compute the log likelihood.
And so that's equal to, um, [NOISE] let's see, right.
And so if you take the log of that,
um, you end up with- you end up with that.
Okay? And, um, it- so, so, in other words, uh,
the last thing you want to do is,
try to choose the value of theta to try to
maximize L of theta.
Okay. Now, so, so just,
just to summarize where we are, right.
Uh, if you're trying to predict,
your malignancy and benign, uh,
tumors, you'd have a training set with XI YI.
You define the likelihood, define the log-likelihood.
And then what you need to do is have an algorithm
such as gradient descent, or gradient descent, talk about that in
a sec to try to find the value of theta that maximizes the log-likelihood.
And then having chosen the value of theta when a new patient
walks into the doctor's office you would take the features of the new tumor
and then use H of theta to estimate the chance of this new tumor in the new patient
that walks in tomorrow to estimate the chance that
this new thing is ah is- is malignant or benign.
Okay? So the algorithm we're going to use to
choose theta to try to maximize the log-likelihood is
a gradient ascent or batch gradient ascent.
And what that means is we will update the parameters theta J according to theta J
plus the partial derivative with respect to the log-likelihood.
Okay? Um, and the differences from what you saw
that linear regression from last time is the following.
Just two differences I guess.
For linear regression. Last week,
I have written this down,
theta J gets updated as theta J minus
partial with respect to theta J of J of theta, right?
So you saw this on Wednesday.
So the two differences between that is well,
first instead of J of theta you're now
trying to optimize the log-likelihood instead of this squared cost function.
And the second change is, previously you were trying to minimize the squared error.
That's why we had the minus.
And today you're trying to maximize the log-likelihood which is why there's a plus sign.
Okay? And so, um, so gradient descent you know,
is trying to climb down this hill whereas gradient ascent has a,
um, uh, has a- has a concave function like this.
And it's trying to, like,
climb up the hill rather than climb down the hill.
So that's why there's a plus symbol here instead of
a minus symbol because we are trying to maximize
the function rather than minimize the function.
So the last thing to really flesh out this algorithm which is done in the lecture notes,
but I don't want to do it here today is to plug in
the definition of H of theta into this equation and then take this thing.
So that's the log-likelihood of theta and then through
calculus and algebra you can take derivatives of this whole thing with respect to theta.
This is done in detail in the lecture notes.
I don't want to use this in class,
but go ahead and take derivatives of this big formula with respect to
the parameters theta in order to figure out what is that thing, right?
What is this thing that I just circled?
And it turns out that if you do
so you will find that batch gradient ascent is the following.
You update theta J according to- oh,
actually I'm sorry, I forgot the learning rate.
Yeah, it's your learning rate Alpha.
Okay. Learning rate Alpha times this.
Okay? Because this term here is
the partial derivative respect to Theta J after log-likelihood.
Okay? And the full calculus and so on derivations given the lecture notes.
Okay? Um, yeah.
[inaudible].
Is there a chance of local maximum in this case?
No. There isn't. It turns out that this function that the log-likelihood
function L of Theta full logistic regression it always looks like that.
Uh, so this is a concave function.
So there are no local op.
The only maximum is a global maxima.
There's actually another reason why we chose the logistic function because if you
choose a logistic function rather than some other function that will give you 0 to 1,
you're guaranteed that the likelihood function has only one global maximum.
And this, there's actually a big class about, actually what you'll see on Wednesday,
this is a big class of algorithms of which linear regression is one example,
logistic regression is another example and for all of the algorithms in
this class there are no local optima problems when you- when you derive them this way.
So you see that on Wednesday when we talk about generalized linear models.
Okay? Um, so actually, but now that I think about,
there's just one question for you to think about.
This looks exactly the same as what we've figured out for linear regression, right?
That when actually the difference for linear regression was I had
a minus sign here and I reversed these two terms.
I think I had H theta of XI minus YI.
If you put the minus sign there and reverse these two terms,
so take the minus minus,
this is actually exactly the same as what we had come up with for linear regression.
So why, why, why is this different, right?
I started off saying, don't use linear regression for classification problems
because of ah because of that problem that a single example could
really you know- I started off with an example assuming that linear regression is
really bad for classification and we did
all this work and I came back to the same algorithm.
So what happened? Just, yeah go ahead.
[BACKGROUND].
Yeah. All right, cool. Awesome. Right? So what happened is
the definition of H of theta is now different than
before but the surface level of the equation turns out to be the same.
Okay? And again it turns out that for every algorithm in
this class of algorithms you'll see you on Wednesday you end up with the same thing.
Actually this is a general property of a much bigger class of algorithms
called generalized linear models.
Although, yeah, i- i- interesting historical diverge, because of the confusion
between these two algorithms in the early history of machine
learning there was some debate about you know between academics saying,
no, I invented that, no, I invented that.
And then he goes, no, it's actually different algorithms.
[LAUGHTER] Alright, any questions? Oh go ahead.
[BACKGROUND].
Oh, great question. Is there a equivalent of normal equations to logistic regression?
Um, short answer is no.
So for linear regression the normal equations
gives you like a one shot way to just find the best value of theta.
There is no known way to just have a close form equation
unless you find the best value of theta which is why you always have to use an algorithm,
an iterative optimization algorithm such as
gradient ascent or ah and we'll see in a second Newton's method.
All right, cool. So, um, there's a great lead in to, um,
the last topic for today which is Newton's method.
[NOISE]
Um, you know, gradient ascent right is a good algorithm.
I use gradient ascent all the time but it takes a baby step, takes a baby step,
take a baby step, it takes a lot of iterations for gradient assent to converge.
Um, there's another algorithm called Newton's method
which allows you to take much bigger jumps so that's theta,
you know, so- so, uh,
there are problems where you might need you know,say
100 iterations or 1000 iterations of gradient ascent.
That if you run this algorithm called Newton's method you might need
only 10 iterations to get a very good value of theta.
But each iteration will be more expensive.
We'll talk about pros and cons in a second.
But, um, let's see how- let's- let's describe this algorithm which is sometimes much
faster for gradient than gradient ascent for optimizing the value of theta.
Okay? So what we'd like to do is, uh, all right,
so let me- let me use
this simplified one-dimensional problem to describe Newton's method.
So I'm going to solve a slightly different problem with Newton's method which
is say you have some function f, right,
and you want to find a theta such that f of theta is equal to 0.
Okay? So this is a problem that Newton's method solves.
And the way we're going to use this later is what you
really want is to maximize L of theta,
right, and well at the maximum the first derivative must be 0.
So i.e. you want to value where the derivative L prime of theta is equal to 0, right?
And L prime is the derivative of theta because this
is where L prime is another notation for the first derivative of theta.
So you want to maximize a function or minimize a function.
What that really means is you want to find a point where the derivative is equal to 0.
So the way we're going to use Newton's method is we're going to set F of theta equal to
the derivative and then try to find the point where the derivative is equal to 0.
Okay? But to explain Newton's method I'm gonna, you
know, work on this other problem where you have a function F and you just
want to find the value of theta where F of
theta is equal to 0 and then- and we'll set F
equal to L prime theta and that's how we'll we'll apply this to um, logistic regression.
So, let me draw in pictures how this algorithm works.
Uh. [NOISE] [BACKGROUND] All right.
So let's say that's the function f, and, you know,
to make this drawable on a whiteboard,
I'm gonna assume theta is just a real number for now.
So theta is just a single,
you know, like a scalar, a real number.
Um, so this is how Newton's method works.
Um, oh, and the goal is to find this point.
Right? The goal is to find the value of theta where f of theta is equal to 0.
Okay? So let's say you start off, um, right.
Let's say you start off at this point.
Right? At the first iteration,
you have randomly initialized data,
and actually theta is zero or something.
But let's say you start off at that point.
This is how one iteration of Newton's method will work,
which is- let me use a different color.
Right. Start off with theta 0,
that's just a first value consideration.
What we're going to do is look at the function f,
and then find a line that is just tangent to f. So take the derivative of f and
find a line that is just tangent to f. So take that red line.
It just touches the function f. And we're gonna use, if you will,
use a straight line approximation to f,
and solve for where f touches the horizontal axis.
So we're gonna solve for the point where this straight line touches the horizontal axis.
Okay? And then we're going to set this,
and that's one iteration of Newton's method.
So we're gonna move from this value to this value, right?
And then in the second iteration of Newton's method,
we're gonna look at this point.
And again, you know,
take a line that is just tangent to it,
and then solve for where this touches the horizontal axis,
and then that's after two iterations of Newton's method.
Right. And then you repeat.
Take this, sometimes you can overshoot a little bit, but that's okay.
Right? And then that's,
um, there's a cycle back to red.
Let's take the three,
then you take this, let's take the four.
[NOISE] Excuse me.
So you can tell that Newton's method is actually a pretty fast algorithm.
Right? When in just one,
two, three, four iterations,
we've gotten really really close to the point where f of theta is equal to 0.
So let's write out the math for how you do this.
So um, let's see.
I'm going to- so let me just write out the,
the derive, um, you know,
how you go from theta 0 to theta 1.
So I'm going to use this horizontal distance.
I'm gonna denote this as, uh, delta.
This triangle is uppercase Greek alphabet delta.
Right? This is lowercase delta, that's uppercase delta.
Right? Uh, and then the height here,
well that's just f of theta 0.
Right? This is the height of- it's just f of theta 0.
And so, um, let's see.
Right.
So, uh, what we'd like to do is solve for the value of delta,
because one iteration of Newton's method is a set, you know,
of theta 1 is set to theta 0 minus delta.
Right? So how do you solve for delta?
Well, from, uh, calculus we know that
the slope of the function f is the height over the run.
Well, height over the width.
And so we know that the derivative of del- f prime,
that's the derivative of f at the point theta 0,
that's equal to the height, that's f of theta,
divided by the horizontal. Right? So the derivative,
meaning the slope of the red line is by definition the derivative is
this ratio between this height over this width.
Um, and so delta is equal to f of theta 0 over f prime of theta 0.
And if you plug that in,
then you find that a single iteration of Newton's method is
the following rule of theta t plus 1 gets updated as
theta t minus f of theta t over f prime of theta t. Okay.
Where instead of 0 and 1 I replaced them with t and t plus 1.
Right? Um, and finally to, to- you know,
the very first thing we did was let's let f of theta be equal to say L prime of theta.
Right? Because we wanna find the place where the first derivative of L is 0.
Then this becomes theta t plus 1,
gets updated as theta t minus L prime of
theta t over L double prime of theta t. So it's really,
uh, the first derivative divided by the second derivative.
Okay?
So Newton's method
is a very fast algorithm,
and, uh, it has, um,
Newton's method enjoys a property called quadratic convergence.
Not a great name. Don't worry- don't worry too much about what it means.
But informally, what it means is that, um,
if on one iteration Newton's method has 0.01 error,
so on the X axis,
you're 0.01 away from the,
from the value, from the true minimum,
or the true value of f is equal to 0.
Um, after one iteration,
the error could go to 0.0001 error,
and after two iterations it goes 0.00000001.
But roughly Newton's method,
um, under certain assumptions, uh, uh,
that functions move not too far from quadratic,
the number of significant digits that you have
converged, the minimum doubles on a single iteration.
So this is called quadratic convergence.
Um, and so when you get near the minimum,
Newton's method converges extremely rapidly.
Right? So, so after a single iteration, it becomes much more accurate,
after another iteration it becomes way, way, way more accurate,
which is why Newton's method requires relatively few iterations.
Um, and, uh, let's see.
I have written out Newton's method for when theta is a real number.
Um, when theta is a vector, right?
Then the generalization of the rule I wrote above is the following,
theta t plus 1 gets updated as theta t plus H that,
where H is the Hessian matrix.
So these details are written in the lecture notes.
Um, but to give you a sense,
it- when theta is a vector,
this is the vector of derivatives.
All right, so I guess this R_n plus 1 dimensional.
If theta is in R_n plus 1,
then this derivative respect to theta
of the log-likelihood becomes a vector of derivatives,
and the Hessian matrix,
this becomes a matrix as R_n plus 1 by n plus 1.
So it becomes a squared matrix with the dimension equal to the parameter vector theta.
And the Hessian matrix is defined as the matrix of partial derivatives.
Right? So um, [NOISE] and so
the disadvantage of Newton's method is that in high-dimensional problems,
if theta is a vector,
then each step of Newton's method is much more expensive,
because, um, you're, you're either solving a linear system equations,
or having to invert a pretty big matrix.
So if theta is ten-dimensional,
you know, this involves inverting a 10 by 10 matrix, which is fine.
But if theta was 10,000 or 100,000,
then each iteration requires computing like a
100,000 by a 100,000 matrix and inverting that, which is very hard.
Right? It's actually very difficult to do that in very high-dimensional problems.
Um, so, you know, some rules of thumb,
um, if the number of parameters you have
for- if the number of parameters in your iteration is not too big,
if you have 10 parameters, or 50 parameters,
I would almost certainly- I would very likely use Newton's method,
uh, because then you probably get convergence in maybe 10 iterations,
or, you know, 15 iterations, or even less than 10 iterations.
But if you have a very large number of parameters,
if you have, you know, 10,000 parameters,
then rather than dealing with a 10,000 by 10,000 matrix, or even bigger,
the 50 by 1000 by 50,000 matrix,
and you have 50,000 parameters,
I will use, uh, gradient descent then.
Okay? But if the number of parameters is not too big,
so that the computational cost per iteration is manageable,
then Newton's method converges in a very small number of iterations,
and, and could be much faster algorithm than gradient descent.
All right. So, um, that's it for, uh, Newton's method.
Um, on Wednesday, I guess we are running out of time.
On Wednesday, you'll hear about generalized linear models.
Um, I think unfortunately I- I promised to be in Washington DC,
uh, uh, tonight, I guess through Wednesday.
So, uh, you'll hear from some- I think Anand will give the lecture on Wednesday,
uh, but I will be back next week.
So un- unfortunately was trying to do this,
but because of his health things, he can't lecture.
So Anand will do this Wednesday.
Thanks everyone. See you on Wednesday.
 Couple of announcements, uh,
before we get started.
So, uh, first of all, PS1 is out.
Uh, problem set 1, um,
it is due on the 17th.
That's two weeks from today.
You have, um, exactly two weeks to work on it.
You can take up to,
um, two or three late days.
I think you can take up to, uh,
three late days, um.
There is, uh, there's a good amount
of programming and a good amount of math you, uh, you need to do.
So PS1 needs to be uploaded.
Uh, the solutions need to be uploaded to Gradescope.
Um, you'll have to make two submissions.
One submission will be a PDF file,
uh, which you can either, uh,
which you can either use a LaTeX template that we provide or you can
handwrite it as well but you're strongly encouraged to use the- the LaTeX template.
Um, and there is a separate coding assignment, uh,
for which you'll have to submit code as a separate,
uh, Gradescope assignment.
So they're gonna- you're gonna see two assignments in Gradescope.
One is for the written part.
The other is for the, uh,
is for the programming part.
Uh, with that, let's- let's jump right into today's topics.
So, uh, today, we're gonna cover,
uh- briefly we're gonna cover, uh,
the perceptron, uh, algorithm.
Um, and then, you know,
a good chunk of today is gonna be exponential family and,
uh, generalized linear models.
And, uh, we'll- we'll end it with, uh,
softmax regression for multi-class classification.
So, uh, perceptron, um,
we saw in logistic regression, um.
So first of all, the perceptron algorithm, um,
I should mention is not something that is widely used in practice.
Uh, we study it mostly for, um, historical reasons.
And also because it is- it's nice and simple and, you know,
it's easy to analyze and,
uh, we also have homework questions on it.
So, uh, logistic regression.
Uh, we saw logistic regression uses,
uh, the sigmoid function.
Right. So, uh, the logistic regression,
uh, using the sigmoid function which, uh,
which essentially squeezes the entire real line
from minus infinity to infinity between 0 and 1.
Um, and - and the 0 and 1 kind of represents,
uh, the probability right?
Um, you could also think of,
uh, a variant of that, uh,
which will be, um,
like the perceptron where, um.
So in- in- in the sigmoid function at, um,
at z equals 0- at z equals 0- g of z is a half.
And as z tends to minus infinity,
g tends to 0 and as z tends to plus infinity,
g tends to 1.
The perceptron, um, algorithm uses,
uh, uh, a somewhat similar but, uh,
different, uh, function which,
uh, let's say this is z.
Right. So, uh, g of z in this case
is 1 if z is greater than or equal to 0 and 0 if z is less than 0, right?
So you ca- you can think of this as the hard version of-
of- of the sigmoid function, right?
And this leads to, um,
um, this leads to the hypothesis function, uh,
here being, uh, h Theta of x is equal to,
um, g of Theta transpose x.
So, uh, Theta transpose x,
um, your Theta has the parameter,
x is the, um,
x is the input and h Theta of x will be 0 or 1,
depending on whether Theta transpose x was less than 0 or- or,
uh, greater than 0.
And you tol- and,
um, similarly in, uh,
logistic regression we had a state of x is equal to,
um, 1 over 1 plus e to the minus Theta transpose x. Yeah.
That's essentially, uh, g of- g of z where g is s,
uh, the sigma- sigmoid function.
Um, both of them have a common update rule,
uh, which, you know,
on the surface looks similar.
So Theta j equal to Theta j plus Alpha times y_i
minus h Theta of- of
x_i times x_ij, right?
So the update rules for,
um, the perceptron and logistic regression,
they look the same except h Theta of x means
different things in- in- in- in the two different, um, uh, scenarios.
We also saw that it was similar for linear regression as well.
And we're gonna see why this- this is, um, you know,
that this is actually a- a more common- common theme.
So, uh, what's happening here?
So, uh, if you inspect this equation, um,
to get a better sense of what's happening in- in the perceptron algorithm,
this quantity over here is a scalar, right?
It's the difference between y_i which can be either 0 and 1
and h Theta of x_i which can either be 0 or 1, right?
So when the algorithm makes a prediction of h Theta of- h Theta of x_i for a given x_i,
this quantity will either be zero if- if, uh,
the algorithm got it right already, right?
And it would be either plus 1 or minus 1 if- if y_i- if- if the actual, uh,
if the ground truth was plus 1 and the algorithm predicted 0,
then it, uh, uh,
this will evaluate to 1 if
wrong and y_i equals 1 and similarly it is,
uh, minus 1 if
wrong and y_i is 0.
So what's happening here? Um, to see what's- what's- what's happening,
uh, it's useful to see this picture, right?
So this is the input space, right?
And, uh, let's imagine there are two, uh,
two classes, boxes and,
let's say, circles, right?
And you want to learn,
I wanna learn an algorithm that can separate these two classes, right?
And, uh, if you imagine that the, uh, uh,
what- what the algorithm has learned so far is a Theta that
represents this decision boundary, right?
So this represents, uh,
Theta transpose x equals 0.
And, uh, anything above is Theta transpose,
uh, x is greater than 0.
And anything below is Theta transpose x less than 0, all right?
And let's say, um,
the algorithm is learning one example at a time,
and a new example comes in.
Uh, and this time it happens to be- the new example happens to be a square, uh, or a box.
And, uh, but the algorithm has mis- misclassified it, right?
Now, um, this line, the separating boundary,
um, if- if- if the vector equivalent of that would be a vector that's normal to the line.
So, uh, this was- would be Theta, all right?
And this is our new x,
right? This is the new x.
So this got misclassified, this, uh, uh,
this is lying to, you know,
lying on the bottom of the decision boundary.
So what- what- what's gonna happen here?
Um, y_i, let's call this the one class and this is- this is the zero class, right?
So y_i minus- h state of i will be plus 1, right?
And what the algorithm is doing is, uh,
it sets Theta to be Theta plus Alpha times x, right?
So this is the old Theta,
this is x. Alpha is some small learning rate.
So it adds- let me use a different color here.
It adds, right, Alpha times x to
Theta and now say this is- let's call it Theta prime, is the new vector.
That's- that's the updated value, right?
And the- and the separating, um, uh,
hyperplane corresponding to this is something that is normal to it, right?
Yeah. So- so it updated the, um,
decision boundary such that x is now included in the positive class, right?
The- the, um, idea here- here is that, um, Theta,
we want Theta to be similar to x in general,
where such- where y is 1.
And we want Theta to be not similar to x when y equals 0.
The reason is, uh,
when two vectors are similar,
the dot product is positive and they are not similar,
the dot product is negative.
Uh, what does that mean?
If, uh, let's say this is,
um, x and let's say you have Theta.
If they're kind of, um,
pointed outwards, their dot product would be, um, negative.
And when- and if you have a Theta that looks like this,
we call it Theta prime, then the dot product will be
positive if the angle is- is less than r.
So, um, this essentially means that as Theta is rotating,
the, um, decision boundary is kind of perpendicular to Theta.
And you wanna get all the positive x's on one side of the decision boundary.
And what's the- what's the, uh,
most naive way of- of- of taking Theta and given x,
try to make Theta more kind of closer to x?
A simple thing is to just add a component of x in that direction.
You know, add it here and kind of make Theta.
And so this- this is a very common technique used in lots of
algorithms where if you add a vector to another vector,
you make the second one kind of closer to the first one, essentially.
So this is- this is,
uh, the perceptron algorithm.
Um, you go example by example in an online manner,
and if the al- if the,
um, example is already classified, you do nothing.
You get a 0 over here.
If it is misclassified,
you either add the- add a small component of, uh,
as, uh, you add the vector itself,
the example itself to your Theta or you subtract it,
depending on the class of the vector.
This is about it. Any- any- any questions about the perceptron?
Cool. So let's move on to the next topic, um, exponential families.
Um, so, um, exponential family is essentially a class of- yeah.
Why don't we use them in practice?
Um, it's, um, not used in practice because,
um, it- it does not have a probabilistic interpretation of what's- what's happening.
You kinda have a geometrical feel of what's happening with- with
the hyperplane but it- it doesn't have a probabilistic interpretation.
Also, um, it's, um,
it- it was- and I think the perceptron was,
uh, pretty famous in, I think,
the 1950s or the '60s where people thought this is a good model of how the brain works.
And, uh, I think it was,
uh, Marvin Minsky who wrote a paper saying, you know,
the perceptron is- is kind of limited because it- it could never classify,
uh, points like this.
And there is no possible separating boundary that can,
you know, do- do something as simple as this.
And kind of people lost interest in it, but, um, yeah.
And in fact, what- what we see is- is,
uh, in logistic regression,
it's like a software version of,
uh, the perceptron itself in a way. Yeah.
[inaudible]
It's- it's, uh, it's up to,
you know, it's- it's a design choice that you make.
What you could do is you can- you can kind of,
um, anneal your learning rate with every step, every time, uh,
you see a new example decrease your learning rate until something,
um, um, until you stop changing, uh, Theta by a lot.
You can- you're not guaranteed that you'll- you'll be able to get every example right.
For example here, no matter how long you learn you're- you're never gonna,
you know, um, uh, find,
uh, a learning boundary.
So it's- it's up to you when you wanna stop training.
Uh, a common thing is to just decrease the learning rate,
uh, with every time step until you stop making changes.
All right.
Let's move on to exponential families.
So, uh, exponential families is, uh,
is a class of probability distributions,
which are somewhat nice mathematically, right?
Um, they're also very closely related to GLMs,
which we will be going over next, right?
But first we kind of take a deeper look at, uh,
exponential families and, uh,
and- and what they're about.
So, uh, an exponential family is one, um, whose PDF,
right? So whose PDF can be written in the
form- by PDF I mean probability density function,
but for a discrete, uh, distribution,
then it would be the probability mass function, right?
Whose PDF can be written in the form, um.
All right. This looks pretty scary.
Let's- let's- let's kind of, uh,
break it down into, you know,
what- what- what they actually mean.
So y over here is the data, right?
And there's a reason why we call it y because- yeah.
Can you write a bit larger.
A bit larger, sure.
Is this better?
Yeah. So y is the data.
And the reason- there's a reason why we call it y and not x.
And that- and that's because we're gonna use exponential families
to model the output of your- of- of your data,
you know, in a, uh, in a supervised learning setting.
Um, and- and you're gonna see x when we move on to GLMs.
Until, you know, until then we're just gonna deal with y's for now.
Uh, so y is the data.
Um, Eta is- is called the natural parameter.
T of y is called a sufficient statistic.
If you have a statistics background and you've learn- if
you come across the word sufficient statistic before, it's the exact same thing.
But you don't need to know much about this because
for all the distributions that we're gonna be seeing today,
uh, or in this class,
t of y will be equal to just y.
So you can, you can just replace t of y with y for,
um, for all the examples today and in the rest of the calcu- of the class.
Uh, b of y,
is called a base measure.
Right, and finally a of Eta,
is called the log-partition function.
And we're gonna be seeing a lot of this function, log-partition function.
Right, so, um, again, y is the data that,
uh, this probability distribution is trying to model.
Eta is the parameter of the distribution.
Um, t of y,
which will mostly be just y, um,
but technically you know, t of y is more, more correct.
Um, um, b of y,
which means it is a function of only y.
This function cannot involve Eta. All right.
And similarly t of y cannot involve Eta.
It should be purely a function of y. Um,
b of y is called the base measure,
and a of Eta,
which has to be a function of only Eta and, and constants.
No, no y can,
can, uh, can be part of a of, uh, Eta.
This is called the log-partition function.
Right. And, uh, the reason why this is called the log-partition function
is pretty easy to see because this can be written as b of y,
ex of Eta, times t of y over.
So these two are exactly the same.
Um, just take this out and, um, um.
Sorry, this should be the log.
I think it's fine. These two are exactly the same.
And, uh.
It should be the [inaudible] and that should be positive.
Oh, yeah, you're right. This should be positive, um. Thank you.
So, uh, this is, um,
you can think of this as a normalizing constant of the distribution such that the,
um, the whole thing integrates to 1, right?
Um, and, uh, therefore the log of this will be a of Eta,
that's why it's just called the log of the partition function.
So the partition function is a technical term to indicate
the normalizing constant of, uh, probability distributions.
Now, um, you can plug-in any definition of b,
a, and t. Yeah.
Sure. So why is your y,
and for most of, uh, most of our example is going to be a scalar.
Eta can be a vector.
But we will also be focusing, uh,
except maybe in Softmax,
um, this would be, uh, a scalar.
T of y has to match,
so these- the dimension of these two has to match [NOISE].
And these are scalars, right?
So for any choice of a,
b and t, that you've- that,
that, that can be your choice completely.
As long as the expression integrates to 1,
you have a family in the exponential family, right?
Uh, what does that mean?
For a specific choice of, say, for,
for, for some choice of a,
b, and t. This can actually- this will be equal to say the, uh,
PDF of the Gaussian, in which case you,
you got for that choice of t, a, and,
and b, you got the Gaussian distribution.
A family of Gaussian distribution such that for any value of the parameter,
you get a member of the Gaussian family. All right.
And this is mostly,
uh, to show that, uh,
a distribution is in the exponential family.
Um, the most straightforward way to do it
is to write out the PDF of the distribution in a form that you know,
and just do some algebraic massaging to bring it into this form, right?
And then you do a pattern match to, to and,
and, you know, conclude that it's a member of the exponential family.
So let's do it for a couple of examples.
So, uh, we have
[NOISE].
So, uh, a Bernoulli distribution is one you use to,
uh, model binary data.
Right. And it has a parameter, uh,
let's call it Phi, which is,
you know, the probability of the event happening or not.
Right, right. Now, the,
uh, what's the PDF of a Bernoulli distribution?
One way to, um,
write this is Phi of y,
times 1 minus Phi,
1 minus y. I think this makes sense.
This, this pattern is like, uh, uh,
a way of writing a programming- programmatic if else in,
in, in math. All right.
So whenever y is 1,
this term cancels out,
so the answer would be Phi.
And whenever y is 0 this term cancels out and the answer is 1 minus Phi.
So this is just a mathematical way to,
to represent an if else that you would do in programming, right.
So this is the PDF of, um, a Bernoulli.
And our goal is to take this form and massage it into that form, right,
and see what, what the individual t,
b, and a turn out to be, right.
So, uh, whenever you, you,
uh, see your distribution in this form, a common, um,
technique is to wrap
this with a log and then Exp.
Right, um, because these two cancel out so, uh,
this is actually exactly equal to this [NOISE].
And, uh, if you,
uh, do some more algebra and this, uh, we will see that,
this turns out to be Exp of log Phi over 1 minus Phi times y,
plus log of 1 minus Phi, right?
It's pretty straightforward to go from here to here.
Um, I'll, I'll let you guys,uh,
uh, verify it yourself.
But once we have it in this form, um,
it's easy to kind of start doing some pattern matching,
from this expression to, uh, that expression.
So what, what we see, um,
here is, uh, the base measure b of y is equal to.
If you match this with that,
b of y will be just 1.
Uh, because there's no b of y term here. All right.
And, um, so this would be b of y.
This would be Eta.
This would be t of y.
This would be a of Eta, right?
So that could be, uh, um,
you can see that the kind of matching pattern.
So b of y would be 1.
T of y is just y,
as, um, as expected.
Um, so Eta is equal to log Phi over 1 minus Phi.
And, uh, this is an equivalent statement is to invert this operation and say
Phi is equal to 1 over 1 plus e to the minus Eta.
I'm just flipping the operation from,
uh, this went from Phi to Eta here.
It's, it's, it's the equivalent.
Now, here it goes from Eta to Phi, right?
And a of Eta is going to be, um,
so here we have it as a function of Phi,
but we got an expression for Phi in terms of eta,
so you can plug this expression in here,
and that, uh, change of minus sign.
So, so, let, let me work out this,
minus log of 1 minus Phi.
This is, uh, just,
uh, the pattern matching there.
And minus log 1 minus,
this thing over, 1 over 1 plus Eta to the minus Eta.
The reason is because we want an expression in terms of Eta.
Here we got it in terms of Phi, but we need to,
uh, plug in, um, plug in Eta over here.
Uh, Eta, and this will just be,
uh, log of 1 plus e to the Eta.
Right. So there you go.
So this, this kind of, uh,
verifies that the Bernoulli distribution is a member of the exponential family.
Any questions here? So note that this may look familiar.
It looks like the, uh,
sigmoid function, somewhat like the sigmoid function,
and there's actually no accident.
We'll see, uh, why, why it's, uh,
actually the sigmoid- how,
how it kind of relates to,
uh, logistic regression in a minute.
So another example, um
[NOISE].
So, uh, a Gaussian with fixed variance.
Right, so, um, a Gaussian distribution,
um, has two parameters the mean and the variance, uh,
for our purposes we're gonna assume a constant variance, um,
you-you can, uh, have,
um, you can also consider Gaussians with,
with where the variance is also a variable,
but for-for, uh, our course we are go- we are only interested in, um,
Gaussians with fixed variance and we are going to assume,
assume that variance is equal to 1.
So, this gives the PDF of a Gaussian to look like this,
p of y parameterized as mu. So note here,
when we start writing out,
we start with the, uh,
parameters that we are, um,
commonly used to, and we- they are also called like the canonical parameters.
And then we set up a link between the canonical parameters and the natural parameters,
that's part of the massaging exercise that we do.
So we're going to start with the canonical parameters, um,
is equal to 1 over root 2
pi, minus over 2.
So this is the Gaussian PDF with,
um, with- with a variance equal to 1, right,
and this can be rewritten as- again,
I'm skipping a few algebra steps, you know,
straightforward no tricks there,
uh, any question? Yep?
[BACKGROUND].
Fixed variance. E to the minus y squared over 2, times EX.
Again, we go to the same exercise,
you know, pattern match, this is b of y,
this is eta, this is t of y,
and this would be a of eta, right?
So, uh, we have, uh,
b of y equals 1 over root 2
pi minus y squared by 2.
Note that this is a function of only y,
there's no eta here,
um, t of y is just y, and in this case,
the natural parameter is-is mu, eta is mu,
and the log partition function is equal to mu square by 2,
and when we-and we repeat the same exercise we did here,
we start with a log partition function that is parameterized by the canonical parameters,
and we use the,
the link between the canonical and, and,
uh, the natural parameters, invert it and,
um, um, so in this case it's- it's the- it's the same sets, eta over 2.
So, a of eta is a function of only eta,
again here a of eta was a function of only eta,
and, um, p of y is a function of only y,
and b of y is a function of only,
um, y as well.
Any questions on this? Yeah.
If the variance is unknown [inaudible].
Yeah, you- if, if the variance is unknown you can write it as
an exponential family in which case eta will now be a vector,
it won't be a scalar anymore,
it'll be- it'll have two, uh,
like eta1 and eta2,
and you will also have, um,
you will have a mapping between each of
the canonical parameters and each of the natural parameters,
you, you can do it, uh, you know, it's pretty straightforward.
Right, so this is- this is exponential- these are exponential families, right?
Uh, the reason why we are, uh,
why we use exponential families is because it has
some nice mathematical properties, right?
So, uh, so one property is now,
uh, if we perform maximum likelihood on,
um, on the exponential family,
um, as, as, uh,
when, when the exponential family is parameterized in the natural parameters,
then, uh, the optimization problem is concave.
So MLE with respect to eta is concave.
Similarly, if you, uh,
flip this sign and use the, the, uh,
what's called the negative log-likelihood,
so you take the log of the expression negate it and in this case,
the negative log-likelihood is like
the cost function equivalent of doing maximum likelihood,
so you're just flipping the sign, instead of maximizing,
you minimize the negative log likelihood,
so-and, and you know, uh,
the NLL is therefore convex, okay.
Um, the expectation of y.
What does this mean? Um, each of the distribution,
uh, we start with, uh, a of eta,
differentiate this with respect to eta,
the log partition function with respect to eta,
and you get another function with respect to eta,
and that function will- is,
is the mean of the distribution as parameterized by eta,
and similarly the variance of y parameterized by eta,
is just the second derivative,
this was the first derivative,
this is the second derivative, this is eta.
So, um, the reason why
this is nice is because in general for
probability distributions to calculate the mean and the variance,
you generally need to integrate something,
but over here you just need to differentiate,
which is a lot easier operation, all right?
And, um, and you
will be proving these properties in your first homework.
You're provided hints so it should be [LAUGHTER].
All right, so, um,
now we're going to move on to,
uh, generalized linear models, uh,
this- this is all we wanna talk about exponential families, any questions? Yep.
[inaudible].
Exactly, so, ah, if you're-if you're, um,
if you're- if it's a multi-variate Gaussian,
then this eta would be a vector,
and this would be the Hessian.
All right, let's move on to, uh, GLM's.
So the GLM is, is, um,
somewhat like a natural extension of the exponential families to include,
um, include covariates or include your input features in some way, right.
So over here, uh,
we are only dealing with,
uh, in, in the exponential families,
you're only dealing with like the y, uh, which in,
in our case, it- it'll kind of map to the outputs, um.
But, um, we can actually build a lot of many powerful models by,
by choosing, uh, an appropriate, um, um,
family in the exponential family and kind of plugging it onto a, a linear model.
So, so the, uh,
assumptions we are going to make for GLM is that one, um,
so these are the assumptions or
design choices that are gonna take us from exponential families to,
uh, generalized linear models.
So the most important assumption is that, uh, well, yeah.
Assumption is that y given x parameterized
by Theta is a member of an exponential family.
Right. By exponential family of Theta,
I mean that form.
It could, it could, uh, in,
in the particular, uh, uh, uh,
scenario that you have, it could take on any one of these, um, uh, distributions.
Um, we only, we only,
uh, talked about the Bernoullian Gaussian.
There are also, um,
other distributions that are- those are part of the, uh, exponential family.
For example, um, I forgot to mention this.
So if you have, uh,
real value data, you use a Gaussian.
If you have binary, a Bernoulli.
If you have count,
uh, like, counts here.
And so this is a real value.
It can take any value between zero and infinity by count.
That means just non-negative integers,
uh, but not anything between it.
So if you have counts, you can use a Poisson.
If you have uh, positive real value integers like say,
the volume of some object or a time to an event which,
you know, um, that you are only predicting into the future.
So here, you can use, uh,
like Gamma or exponential.
So, um, so there is the exponential family,
and there is also a distribution called the exponential distribution,
which are, you know, two distinct things.
The exponential distribution happens to be a member of the exponential family as well,
but no, they're not the same thing.
Um, the exponential and, um, yeah,
and you can also have, um,
you can also have probability distributions over probability distributions.
Like, uh, the Beta, the Dirichlet.
These mostly show up in Bayesian machine learning or Bayesian statistics.
Right. So depending on the kind of data that you have,
if your y-variable is, is,
is if you're trying to do a regression,
then your y is going to be say, say a Gaussian.
If you're trying to do a classification, then your y is,
and if it's a binary classification,
then the exponential family would be Bernoulli.
So depending on the problem that you have,
you can choose any member of the exponential family,
um, as, as parameterized by Eta.
And so that's the first assumption.
That y conditioned on y given x is a member of the exponential family.
And the, uh, second, the design choice that we are making here is
that Eta is equal to Theta transpose x.
So this is where your x now comes into the picture.
Right. So Theta is, um,
is in Rn, and x is also in Rn.
Now, this n has nothing to do with anything in the exponential family.
It's purely, um, a dimensions of your of,
of your data that you have,
of the x's of your inputs,
and, and this does not show up anywhere else. And that, that- that's, um.
And, and, uh, Eta is, is, uh, we,
we make a design choice that Eta will be Theta transpose- transpose x. Um, and
another kind of assumption is that at test time, um, right.
When we want an output for a new x,
given a new x, we want to make an output, right.
So the output will be, right.
So given an x and,
um, given an x,
we get, uh, an exponential family distribution, right.
And the mean of that distribution will be the prediction that we make for a given,
for a given x. Um, this may sound a little abstract, but, you know,
uh, we're going to make this, uh, uh, more clear.
So this- what this essentially means is that the hypothesis function
is actually just, right.
This is our hypothesis function.
And we will see that, you know, what we do over here,
if you plug in the,
uh, um, exponential family,
uh, as, as Gaussian,
then the hypothesis will be the same, you know,
Gaussian hypothesis that we saw in linear regression.
If we plug in a Bernoulli,
then this will turn out to be the same hypothesis that we saw in logistic regression,
and so on, right.
So, uh, one way to kind of,
um, visualize this is,
right. So one way to think of is,
of- if this is, there is a model and there is a distribution, right.
So the model we are assuming it to be a linear model, right.
Given x, there is a learnable parameter Theta,
and Theta transpose x will give you a parameter, right.
This is the model,
and here is the distribution.
Now, the distribution, um,
is a member of the exponential family.
And the parameter for this distribution is the output of the linear model, right.
This, this is the picture you want to have in your mind.
And the exponential family,
we make, uh, depending on the data that we have.
Whether it's a, you know, whether it's, uh,
a classification problem or a regression problem or a time to vent problem,
you would choose an appropriate b,
a and t, uh,
based on the distribution of your choice, right.
So this entire thing, uh,
a-and from this, you can say, uh,
get the, uh, expectation of y given Eta.
And this is the same as expectation of y given Theta transpose x, right.
And this is essentially our hypothesis function.
Right.
Yep. [BACKGROUND] That's exactly right.
Uh, so, uh, so the question is,
um, are we training Theta to, uh, uh, um,
to predict the parameter of the, um,
exponential family distribution whose mean is,
um, the, uh, uh,
uh, prediction that we're gonna make for y.
That's, that's correct, right.
And, um, so this is what we do at test time, right.
And during training time,
how do we train this model?
So in this model,
the parameter that we are learning by doing gradient descent,
are these parameters, right.
So you're not learning any the parameters in the,
uh, in the, uh, uh, exponential family.
We're not learning Mu or Sigma square or, or Eta.
We are not learning those. We're learning Theta that's part of the model,
and not part of, uh, the distribution.
And the output of this will become the,
um, the distributions parameter.
It's unfortunate that we use the word parameter for this and that, but, uh, there,
there are- it's important to understand what,
what is being learned during training phase and, and, and what's not.
So this parameter is the output of a function.
It's not, it's not a variable that we,
that we, uh, do gradient descent on.
So during learning, what we do is maximum likelihood.
Maximize with respect to Theta of P of
y i given, right.
So you're doing gradient ascent on the log probability of,
of y where, um, the, the, um,
natural parameter was re-parameterized, uh,
with the linear model, right.
And we are doing gradient ascent by taking gradients on Theta, right.
Thi-this is like the big picture of what's happening with GLMs,
and how they kind of,
yeah, are an extension of exponential families.
You re-parameterize the parameters with the linear model,
and you get a GLM.
[NOISE].
So let's, let's look at, uh,
some more detail on what happens at train time.
[NOISE]
So another, um,
kind of incidental benefit of using, uh, uh,
GLMs is
that at train time,
we saw that you wanna do, um,
maximum likelihood on the log prob- using
the log probability with respect to Thetas, right?
Now, um, at first it may appear that,
you know, we need to do some more algebra, uh,
figure out what the expression for, you know,
P is, um, represented in the- in-
in- as a function of Theta transpose x and take the derivatives and,
you know, come up with a gradient update rule and so on.
But it turns out that,
uh, no matter which- uh,
what kind of GLM you're doing,
no matter which choice of distribution that you make,
the learning update rule is the same.
[NOISE] The learning update rule is Theta equals
Theta j plus Alpha times y_i
minus h Theta of x_i.
You guys have seen this so many times by now.
So this is- you can,
you can straight away just apply this learning rule without ever having to,
um, do any more algebra to figure out what
the gradients are or what the- what, what the loss is.
You can go straight to the update rule and do your learning.
You plug in the appropriate h Theta of x,
you plug in the appropriate h Theta of x, uh,
depending on the choice of distribution that you make and you can start learning.
Initialize your Theta to some random values and,
and, and you can start learning.
So um, any question on this? Yeah.
[inaudible]
You can do, uh- if you wanna do it for batch gradient descent,
then you just, um,
sum over all your examples.
[inaudible]
Yeah. So, um, the uh,
Newton method is, is, uh,
is probably the most common you would use with GLMs, uh,
and that again comes with the assumption that you're- the
dimensionality of your data is not extremely high.
As long as the number of features is less than a few thousand,
then you can do Newton's method.
Any other questions? Good. So, um,
so this is the same update rule for any, any,
um, any specific type of GLM based on the choice of distribution that you have.
Whether you are modeling, uh,
you know, um, you're doing classification,
whether you're doing regression, whether you're doing- you know,
a Poisson regression, the update rule is the same.
You just plug in a different h Theta of x and you get your learning rule.
Another, um, some more terminology.
So Eta is what we call the natural parameter.
[NOISE] So Eta is
the natural parameter and the function that
links the natural parameter
to the mean of the distribution and this has a name,
it's called the canonical response function.
Right. And, um, similarly,
you can also- let's call it Mu.
It's like the mean of the distribution.
Uh, similarly you can go from Mu back to Eta with the inverse of this,
and this is also called the canonical link function.
There's some, uh, terminology.
We also already saw that g of Eta is also equal to the,
the, the gradient of the log partition function with respect to Eta.
So a side-note g is equal to- [NOISE]
right. And it's also helpful to
make- explicit the distinction between
the three different kinds of parameterizations we have.
So we have three parameterizations.
So we have the model parameters, that's Theta,
the natural parameters, that's Eta,
and we have the canonical parameters.
And this is a Phi for Bernoulli,
Mu and Sigma square for Gaussian, Lambda for Poisson.
Right. So these are three different ways we are- we can parameterize,
um, either the exponential family or,
or, or the G- uh, GLM.
And whenever we are learning a GLM,
it is only this thing that we learn.
Right. That is the Theta in the linear model.
This is the Theta that is, that is learned.
Right. And, uh, the connection between these two is, is linear.
So Theta transpose x will give you a natural parameter.
Uh, and this is the design choice that we're making.
Right. We choose to reparameterize Eta by a linear model,
uh, a linear of- linear in your data.
And, um, between these two,
you have g to go this way and g inverse to come back this way where g is also the,
uh, uh, uh, derivative of the log partition.
So yeah. So it's important to,
to kind of realize.
It can get pretty confusing when you're seeing this for the first time because you
have so many parameters that are being swapped around and,
you know, getting reparameterized.
There are three kind of
spaces in which- three different ways in which we are parameterizing,
uh, uh, generalized linear models.
Uh, the model parameters,
the ones that we learn and the output of this is
the natural parameter for the exponential family and you can, you know,
do some algebraic manipulations and get the canonical parameters for, uh,
the distribution, uh, that we are choosing, uh,
depending on the task where there's classification or regression.
[NOISE]
Any questions on this?
[NOISE]
So no- now it's actually pretty,
you know, um, you can- you can see that, you know,
when you are doing logistic regression, [NOISE] right?
So h theta of X,
um, so h theta of X, um,
is the expected value of- of,
um, of Y, uh,
conditioned on X theta,
[NOISE] and this is equal to phi, right?
Because, um, here the choice of distribution is a Bernoulli.
And the mean of a Bernoulli distribution is
just phi the- in- in the canonical parameter space.
And if we, um,
write that as, um,
in terms of the, um,
h minus eta and this is
equal to 1 over minus theta transpose X, right?
So, ah, the logistic function which when we introduced,
ah, linear reg-, uh,
logistic regression we just, you know,
pulled out the logistic function out of thin air, and said, hey,
this is something that can squash minus infinity to infinity,
between 0 and 1,
seems like a good choice.
Bu-but now we see that it is- it is a natural outcome.
It just pops out from
this more elegant generalized linear model where if you choose Bernoulli to be, uh,
uh, to be the distribution of your, uh, output, then,
you know, the logistic regression just- just pops out naturally.
[NOISE] So,um, [NOISE] any questions? Yeah.
Maybe you speak a little bit more about choosing a distribution to be the output.
Yeah. So the, uh,
the choice of what distribution you are going to
choose is really dependent on the task that you have.
So if your task is regression,
where you want to output real valued numbers like,
you know, price of the house, or- or something,
uh, then you choose a distribution over the real va- real- real numbers like a Gaussian.
If your task is classification,
where your output is binary 0, or 1,
you choose a distribution that models binary data.
Right? So the task in a way influences you to pick the distribution.
And, you know, uh, most of the times that choice is pretty obvious.
[NOISE] If you want to model the number of visitors to a website which is like a count,
you know, you want to use a Poisson distribution,
because Poisson distribution is a distribution over integers. So the task deci-,
you know, pretty much tells you what distribution you want to choose,
and then you- you do the- you know, uh,
um, you do this, you know,
all- you- you go through this machinery of- of- of figuring out what are the, uh,
what h state of X is,
and you plug in h state of X over there and you have your learning rule.
Any more questions? So, uh, it-,
so we made some assumptions.
Uh, these assumptions.
Now it- it- it's also helpful to kind of get,
uh, a visualization of what these assumptions actually mean, right?
[NOISE]
So to expand upon your point, um, um.
You know if you think of the question,
"Are GLMs used for classification,
or are they used for regression,
or are they used for,
you know, um, something else?"
The answer really depends on what is
the choice of distribution that you're gonna choose, you know.
GLMs are just a general way to model data,
and that data could be, you know,
um, binary, it could be real value.
And- and, uh, as long as you have a distribution that can model,
ah, that kind of data,
and falls in the exponential family,
it can be just plugged into a GLM and everything just, uh,
uh, uh works out nicely.
Right. So, uh, [NOISE] so the assumptions that we made.
Let, uh, let's start with regression, [NOISE] right?
So for regression, we assume there is some X.
Uh, to simplify I'm, um,
I'm drawing X as one dimension but,
you know, X could be multi-dimensional.
And there exists a theta, right?
And theta transpose X would- would be some linear, um,
um, some linear, uh, uh, uh, hyperplane.
And this, we assume is Eta, right?
And in case of regression Eta was also Mu.
So Eta was also Mu, right?
Um, and then we are assuming that the Y,
for any given X,
is distributed as a Gaussian with Mu as the mean.
So which means, for every X,
every possible X, you have the appropriate, uh, um, um, Eta.
And with this as the mean,
let's- let's think of this as Y.
So that is, uh, a Gaussian distribution at
every possible- we assume a variance of 1.
So this is like, uh, a Gaussian with standard deviation or variance equal to 1, right?
So for every possible X,
there is a Y given X, um,
which is parameterized by- by- by theta transpose X as- as the mean, right?
And you assume that your data is generated from this process, right?
So what does it mean?
It means, um, you're given X,
and let's- let's say this is Y.
So you would have examples in your training set that- that may look like this, right?
The assumption here is that,
for every X there is, um,
um- let's say for this particular value of X,
um, there was a Gaussian distribution that started from the mean over here.
And from this Gaussian distribution this value was sampled, right?
You're - you're- you're- you're just sampling it from- from the distribution.
Now, the, um- this is how your data is generated.
Again, this is our assumption, [NOISE] right?
Now that- now based on
these assumptions what we are doing with the GLM is we start with the data.
We don't know anything else.
We make an assumption that there is
some linear model from which the data was-was- was- was generated in this format.
And we want to work backwards, right,
to find theta that will give us this line, right?
So for different choice of theta we get a different line, right?
We assume that, you know,
if -if that line represents the- the Mu's,
or the means of the Y's for that particular X, uh,
from which it's sampled from,
we are trying to find a line, [NOISE] ah,
which is- which will be like
your theta transpose X from which these Y's are most likely to have sampled.
That's- that's essentially what's happening when you do
maximum likelihood with- with -with the GLM, right?
Ah, similarly, um, [NOISE]
Similarly for, um, classification,
again let's assume there's an x, right?
And there are some Theta transpose x, right?
And, uh, and this Theta transpose x is equal- is Eta.
We assign this to be Eta, right?
And this Eta is,
uh, from this Eta,
we- we run this through the sigmoid function, uh,
1 over 1 plus e to the minus Eta to get Phi, right?
So if these are the Etas for each, um,
for each Eta we run it through the sigmoid and we get something like this, right?
So this tends to, uh, 1.
This tends to 0.
And, um, when- at this point when Eta is 0,
the sigmoid is- is 0.5.
This is 0.5, right?
And now, um, at each point- at- at- at any given choice of x,
we have a probability distribution.
In this case, it's- it's a- it's a binary.
So let's assume probability of y is the height to the sigmoid line and here it is low.
Um, right. Every x we have a different, uh,
Bernoulli distribution essentially, um, that's obtained where,
you know, the- the probability of y is- is the height to the,
uh, uh, sigmoid through the natural parameter.
And from this, you have a data generating distribution that would look like this.
So x and, uh,
you have a few xs in your training set.
And for those xs, you calc- you- you figure out what your,
you know, y distribution is and sample from it.
So let's say- right.
And now, um, again our goal is to stop- given- given this data,
so- so over here this is the x and this is y.
So this is- these are points for which y is 0.
These are points for which y is 1.
And so given- given this data,
we wanna work backwards to find out,
uh, what Theta was.
What's the Theta that would have resulted in a sigmoid like
curve from which these- these y's were most likely to have been sampled?
That's- and- and figuring out that y is- is- is essentially doing logistic regression.
Any questions?
All right.
So in the last 10 minutes or so,
we will, uh, go over softmax regression.
So softmax regression is,
um, so in the lecture notes,
softmax regression is, uh,
explained as, uh, as yet another member of the GLM family.
Uh, however, in- in- in today's lecture we'll be taking
a non-GLM approach and kind of, um,
seeing- and- and see how softmax is- is essentially doing,
uh, what's also called as cross entropy minimization.
We'll end up with the same- same formulas and equations.
You can- you can go through the GLM interpretation in the notes.
It's a little messy to kind of do it on the whiteboard.
So, um, whereas this has- has- has a nicer, um, um, interpretation.
Um, and it's good to kind of get this cross entropy interpretation as well.
So, uh, let's assume- so here we are talking about multiclass classification.
So let's assume we have three cat- three,
uh, classes of data.
Let's call them circles, um,
squares, and say triangles.
Now, uh, if- here and this is x1 and x2.
We're just- we're just visualizing your input space and the output space, y
is kind of implicit in the shape of this, so, um, um.
So, um, in- in,
um, in multicl- multiclass classification,
our goal is to start from this data and learn a model that can,
given a new data point, you know,
make a prediction of whether this point is a circle,
square or a triangle, right?
Uh, you're just looking at three because it's
easy to visualize but this can work over thousands of classes.
And, um, so what we have is
so you have x_is in R_n.
All right. So the label y
is, uh, is 0, 1_k.
So k is the number of classes, right?
So the labels y is- is- is a one-hot vector.
What would you call it as a one-hot vector?
Where it's a vector which indicates which class the,
uh, x corresponds to.
So each- each element in the vector,
uh, corresponds to one of the classes.
So this may correspond to the triangle class,
circle class, square class or maybe something else.
Uh, so the labels are, uh,
in this one-hot vector where we have a vector that's filled with 0s
except with a 1 in one of the places, right?
And- and- and- and the way we're gonna- the way we're gonna,
uh, um, think of softmax regression is that
each class has its- its own set of parameters.
So we have, uh,
Theta class, right, in R_n.
And there are k such things where class is in here,
triangle, circle, square, etc, right?
So in logistic regression,
we had just one Theta,
which would do a binary, you know, yes versus no.
Uh, in softmax, we have one such vector of Theta per class, right?
So you could also optionally represent them as a matrix.
There's an n by k matrix where, you know,
you have a Theta class- Theta class, right?
Uh, so in softmax, uh, regression, um,
it's- it's- it's a generalization of logistic regression where you have,
um, a set of parameters per class, right?
And we're gonna do something, um,
something similar to, uh, so, uh,
[NOISE] so corresponding
to each- each class,
uh, uh, of- of, uh, parameters that exists, um [NOISE]
So there's- there exists this line which represents say,
Theta triangle transpose x equals 0,
and anything to the left,
will be Theta triangle transpose x is greater than 0,
and over here it'll be less than 0, right?
So if, if- for, for- uh, uh,
the- Theta triangle class,
um, there is- uh,
there is this line, um,
which- which corresponds to,
uh, uh, Theta transpose x equals 0.
Anything to the left, uh will give you
a value greater than on- zero, anything to the right.
Similarly, there is also-.
Uh, so this corresponds to Theta,
uh, square transpose x equals 0.
Anything below will be greater than 0,
anything above will be less than 0.
Similarly you have another one for,
um, this corresponds to Theta circle transpose x equals 0.
And, and, and, and this half plane, we have,
uh, to be greater than 0,
and to the left, it is less than 0,
right? So we have, um,
a different set of parameters per class which,
um, which, which, which hopefully satisfies this property, um,
and now, um, our goal is to take these parameters and let's see what happens when,
when we field a new example.
So given an example x,
we get a set of- given x,
um, and over here we have classes, right?
So we have the circle class,
the triangle class, the square class, right?
So, um, over here,
we plot Theta class transpose x.
So we may get something that looks like this.
So let's say for a new point x over here,
uh, if that's our new x,
we would have Theta transpose,
um, Theta trans- Theta square transpose x to be positive.
So we- all right.
And maybe for, um, for the others,
we may have some negative and maybe something like this for this, right?
So- th- this space is,
is also called the logic space, right?
So these are real numbers, right?
Thi- this will, this will, uh,
this is not a value between 0 and 1,
this is between plus infinity and minus infinity, right?
And, and our goal is to get,
uh, a probability distribution over the classes.
Uh, and in order to do that,
we perform a few steps.
So we exponentiate the logics which would give us- so now it is x above Theta class
transpose x and this will make everything positive so
it should be a small one.
Squares, triangles and circles, right?
Now we've got a set of positive numbers.
And next, we normalize this.
By normalize, I mean, um,
divide everything by the sum of all of them.
So here we have Theta e to the Theta class
transpose x over the sum of i in triangle,
square, circle, e to the Theta i transpose x.
So n- once we do this operation,
we now get a probability distribution
where the sum of the heights will add up to 1, right?
So, uh- so given- so- if, if,
if- given a new point x and we run through this pipeline,
we get a probability output over the classes for which
class that example is most likely to belong to, right?
And this whole process,
so let's call this p hat of,
of, of, of y for the given x, right?
So this is like our hypothesis.
The output of the hypothesis function will output this probability distribution.
In the other cases, the output of the hypothesis function,
generally, output a scalar or a probability.
In this case, it's outputting a probability di- distribution over all the classes.
And now, the true y would look something like this, right?
Let's say, the point over there was- le- let's say it was a triangle,
for, for whatever reason, right?
If that was the triangle,
then the p of y which is also called the label,
you can think of that as a probability distribution which is 1 over
the correct class and 0 elsewhere, right?
So p of y. This is essentially
representing the one-hot representation as a probability distribution, right?
Now the goal or, or, um,
the learning approach that we're going to do is in a way minimize
the distance between these two distributions, right?
This is one distribution,
this is another distribution.
We want to change this distribution to look like that distribution, right?
Uh, and, and, uh, technically,
that- the term for that is minimize the cross entropy between the two distributions.
So the cross entropy
between p and p hat is equal to,
for y in circle, triangle, square,
p of y times log p hat of
y. I don't think
we'll have time to go over the interpretation of cross entropy but you can look that up.
So here we see that p of y will be
one for just one of the classes and zero for the others.
So let's say in this, this example,
p of- so y was say a triangle.
So this will essentially boil down to- there's a little min-
minus log p hat of y triangle, right?
And what we saw that this- the hypothesis is essentially that expression.
So that's equal to minus log x e x of
Theta triangle transpose x over sum of class in triangle,
square, circle, e to the triangle.
Right. And on this, you, you, you,
you treat this as the loss and do gradient descent.
Gradient descent with respect to the parameters.
Right, um, yeah.
With, with, with that I think, uh,
uh, any, any questions on softmax?
Okay. So we'll, we'll break for today in that case. Thanks.
 Hey, morning everyone. Welcome back.
Um, so last week you heard about uh,
logistic regression and um,
uh, generalized linear models.
And it turns out all of the learning algorithms we've been learning about
so far are called discriminative learning algorithms,
which is one big bucket of learning algorithms.
And today um, what I'd like to do is share with
you how generative learning algorithms work.
Um, and in particular you learned about
Gaussian discriminant analysis so by the end of the day,
you will know how to implement this.
And it turns out that uh,
compared to say logistic regression for classification,
GDA is actually a um,
simpler and maybe more computationally
efficient algorithm to implement ah, in some cases.
So um, and it sometimes works better if you have uh,
very small data sets sometimes with some caveats.
Um, and there was a helpful comparison between generative learning algorithms,
which is a new class of algorithms you hear about today,
versus discriminative learn- learning algorithms.
And then we'll talk about naive Bayes and how you can use that to uh,
build a spam filter, for example.
Okay? So um, we'll use binary classification as the motivating example for today.
And um, if you have a data set that looks like this with two classes,
then what a discriminative learning algorithm,
like logistic regression would do,
is use gradient descent to search for
a line that separates the positive-negative examples, right?
So if you randomish - randomly initialize parameters,
maybe starts with some digital boundary
like that and over the course of gradient descent, you know,
the line migrates or evolves until you get maybe a line like that,
that separates the positive and negative examples.
And um, logistic regression is really searching for a line,
searching for a decision boundary that separates the positive and negative examples.
Um, and so if this was the uh,
malignant tumors [NOISE] and the benign tumors example,
right, that's - that's what logistic regression would do.
Now, there's a different class of algorithm which isn't searching for this separation,
which isn't trying to maximize the likelihood that you -
the way you saw last week, which is um,
here's an alternative, just call it generative learning algorithm;
which is rather than looking at two classes and trying to find the separation.
Instead, the algorithm is going to look at the classes one at a time.
First, we'll look at all of the malignant tumors, right?
In the cancer example and try to build a model for what malignant tumors look like.
So you might say, ah, it looks like all the malignant tumors um,
roughly [NOISE] all the malignant tumors roughly live in that ellipse.
And then you look at all the benign tumors in isolation and say,
ah, it looks like all the benign tumors roughly live in that ellipse.
And then at classification time,
if there's a new patient in your office with those features, uh,
it would then look at this new patient and compare it
to the malignant tumor model compared to the benign tumor model and then say,
in this case, ah, it looks like this one.
Looks a lot more like the benign tumors I had previously seen,
so we're gonna classify that as a benign tumor.
Okay? So um, rather
than looking at both classes simultaneously and searching for a way to separate them,
a generative learning algorithm, uh,
instead builds a model of what each of the classes looks like,
kind of almost in isolation,
with some details we'll learn about later.
And then at test time uh,
it evaluates a new example against the benign model,
evaluates against the malignant model and tries to see which of
the two models it matches more closely against.
So let's formalize this.
Um, a discriminative learning [NOISE] algorithm
learns P of y given x, right?
Um, or uh, what it learns um,
[NOISE] right?
Some mapping [NOISE] from x to y directly.
You know, as I learn- Or it can learn,
I think Annan briefly talked about the Perceptron algorithm,
it's helpful to support vector machines later.
But learns a function mapping from x to the labels directly.
So that's a discriminative learning algorithm.
We're trying to discriminate between positive and negative classes.
[NOISE] In contrast, a generative learning algorithm,
[NOISE] it learns P
of um, x given y.
So this says, what are the features like,
[NOISE] given the class, right?
So um, instead P of y given x,
we're gonna learn p of x given y.
So in other words, given that a tumor is malignant,
what are the features likely gonna be like?
Or given the tumor's benign,
what are the features x gonna be like?
Okay? And then as- and then they'll also- generative learning algorithm,
will also learn P of y.
So this is a- this is also called the class prior to be this probability, I guess.
It's called a class prior.
It's just- when the patient walks into your office,
before you've even examined them,
before you've even seen them,
what are the odds that their tumor is malignant versus benign, right?
Before you see any features, okay?
And so using Bayes' Rule,
[NOISE] if you can build a model for P of x given y and for P of y,
um, if- you know,
if you can calculate numbers for both of these quantities then using Bayes' rule,
when you have a new test example [NOISE] with features x,
you can then calculate the chance of y being equal to
1 as this,
[NOISE] right?
Where P of x
by the - [NOISE] okay?
[NOISE] Um, and so
if you learn this term, P of x given y,
then you can plug that in here, right?
And if you've also learned this term P of y,
you can plug that in here.
Right. Um, and so P of x in the denominators,
goes into denominator, okay?
So if you've learned both- both of those terms
in the red square and in the orange square,
you could plug it into all of those terms and therefore use
Bayes' rule to calculate P of y equals 1, given x.
So given the new patient with features x,
you could use this formula to calculate what's the chance that a tumor is malignant.
If you've estimated you know
these - these two quantities in the red and in the orange circles.
Okay? So um, [NOISE]
that's the framework we'll use to build generative learning algorithms.
And in fact, today you see two examples of generative learning algorithms.
One for continuous value features,
which is used for things like the tumor classification and one for discrete features,
which uh, you can use for building,
like in email spam, for example, right?
Or - or I don't know. Or If you want to download
Twitter things and see how positive or negative a sentiment on Twitter is or something.
right? Well we'll have a natural language processing example later.
So um, let's talk about Gaussian discriminant analysis.
[NOISE] GDA.
Um, so uh,
let's develop this model,
assuming that the features x are continuous values.
And when we develop um,
generative learning algorithms, I'm gonna use x and Rn.
So you know, I'm gonna drop the x 0 equals 1, convention.
So I'm not gonna- we're not gonna need that extra x equals 1.
So x is now Rn rather than Rn plus 1.
And the key assumption in Gaussian discriminant analysis is,
we're going to assume that P of x given y
[NOISE] is distributed Gaussian, right?
In other words conditioned on the tumors being malignant,
the distribution of the features is Gaussian.
The other features like uh,
size of the- size of the tumor,
the- the cell adhesion or whatever features you use to measure a tumor um,
and condition on it being benign,
the distribution is also Gaussian.
So um, actually, how many of you are familiar with the multivariate Gaussian?
Raise your hand if you are. Like half of you?
One-third? No. Two-fifths? Okay. Cool. Alright. How many
of you are familiar about a uni-variate,
like a single dimensional Gaussian?
Okay. Cool. Almost everyone. All right.
Cool. So let me- let me just go through what is a multivariate Gaussian distribution.
So the Gaussian is this familiar bell-shaped curve.
A multivariate Gaussian is the generalization of
this familiar bell-shaped curve over a 1-dimensional random
variable to multiple random variables at the same time to- to- to
vector value random variables rather than a uni-variate random variable.
So um, if z,
[NOISE] this is due to Gaussian,
with some mean vector mu and some covariance matrix sigma um,
so if z is in Rn then mu would be Rn as well.
And sigma, the covariance matrix,
will be n by n. So z is two-dimensional,
mu is two-dimensional and sigma is two-dimensional.
And the expected value of z is equal to um, the mean.
And the um, covariance of z,
[NOISE] if you're familiar with multivariate co-variances,
uh, this is the formula.
Right. Um, and this simplifies,
we show in the lecture notes.
You can get this in the lecture notes.
[NOISE] So you- and uh,
following sometimes semi-standard convention,
I'm sometimes gonna omit the square brackets.
So instead of writing the expected value of z,
meaning the mean of z, sometimes I just write to this e, z right?
And omit- omit the square brackets to simplify the notation a bit.
Okay? And the derivation from this step to this step is given in the lecture notes.
Um, and so, well,
[NOISE] the probability density function for a Gaussian looks like this.
[NOISE]
And this is one of those formulas that, I don't know.
When you're implementing these algorithms you use it over and over.
But what I've seen for a lot of people is al- almost no one- well,
very few people start their machine learning and memorize this formula.
Just look at it every time you need it.
I've used it so many times I seem to have it seared in my brain by now,
but most people don't even- when you've used it enough,
you- you- you end up memorizing it.
But let me show you some pictures of what this looks like since I think that would,
um, that might be more useful.
So the Multivariate Gaussian density has two parameters; Mu and Sigma.
They control the mean and the variance of this density.
Okay? So this is a picture of the Gaussian density.
Um, this is a two-dimensional Gaussian bump.
And for now, I've set the mean parameter to 0.
So Mu is a two dimensional parameter,
it's uh, it's 0, 0,
which is why this Gaussian bump is centered at 0.
Um, and the Co-variance matrix Sigma is the identity,
um, i- i- i - is the identity matrix.
So uh, so you know, well,
so- so you've have this standard- this is also called the standard Gaussian
distribution which means 0 and covariance equals to the identity.
Now, I'm gonna take the covariance matrix and shrink it, right?
So take a covariance matrix and multiply it by a number less than 1.
That should shrink the variance- reduce the variability of distributions.
If I do that, the density um,
the p- probability density function becomes taller.
Uh, this- this is a probability density function.
So it always integrates to 1, right?
The area under the curve,
you know, is- is 1.
And so by reducing the covariance from the identity to 0.6 times the identity,
it reduces the spread of the Gaussian density, um,
but it also makes it tall as a result, because,
you know, the area under the curve must integrate to 1.
Now let's make it fatter.
Let's make the covariance two times the identity.
Then you end up with a wider distribution where the values of
um- I guess the axes here, this would be the z1 and the z2 axis;
the two-dimensional Gaussian density, right?
Increases the variance of the density.
So let's go back to a standard Gaussian,
uh, covariance equal 1, 1.
Now, let's try fooling around with the off-diagonal entries.
Um, I'm gonna- So right now,
the off diagonal entries are 0, right?
So in this Gaussian density,
the off-diagonal elements are 0, 0.
Let's increase that to 0.5 and see what happens.
So if you do that,
then the Gaussian density,
uh, hope you can see see the change, right?
It goes from this round shape to this slightly narrower thing.
Let's increase that further to 0.8, 0.8.
Then the density ends up looking like that, um,
where now, it's more likely that z1 now- z1 and z2 are positively correlated.
Okay? So let's go through all of these plots.
But now looking at contours of these Gaussian densities instead of these 3-D bumps.
So uh, this is the contours of
the Gaussian density when the covariance matrix
is the identity matrix and I apologize the aspect ratio.
These are supposed to be perfectly round circles
but the aspect ratio makes this look a little bit fatter,
but this is supposed to be perfectly round circles.
Um, and so, uh, when, uh,
the covariance matrix is the identity matrix,
you know, z1 and z2 are uncorrelated.
Um, uh, and the contours of the Gaussian bump,
of the Gaussian density look like brown circles.
And if you increase the off-diagonal,
excuse me, then it looks like that.
If you increase it further to 0.8,
0.8, it looks like that, okay?
Uh, where now, most of
the probability mass- probability ma- most probably density function places value on,
um, z1 and z2 being positively correlated.
Um, next, let's look at, uh,
what happens if we set the off-diagonal elements to negative values, right?
So, um, actually what do you think will happen?
Let's set the off-diagonals to negative 0.5, 0.5.
Right. Oh well. People are seeing,
fewer making that hand gesture. Okay, cool.
Right. [LAUGHTER] Right.
So- so- so as you- you endow the two random variables with negative correlation,
so you end up with, um,
this type of probability density function, right?
Uh, and the contours, it looks like this.
Okay? Whe- whereas now slanted the other way.
So now z1 and z2 have a negative correlation.
And that's 0.8, 0.8.
Okay? All right.
So- so far we've been keeping the mean vector as
0 and just varying the covariance matrix.
Um, oh good. Yeah?
[inaudible].
Uh, yes. Every covariance matrix is symmetric. Yeah.
[inaudible]
Uh, the true thing about
the covariance matrix has interesting column vectors,
that point in interesting directions.
Not really.
Um, let me think.
Maybe you should- yeah- yeah- uh,
no I- I- I think the covariance matrix is always symmetric.
And so I would usually not look at single columns of the covariance matrix in isolation.
Uh, when we talk about Principal components analysis,
we talk about the Eigenvectors of the covariance matrix,
which are the principle directions in which it points but,
uh, yeah we- we- we- we'll get to that later.
[inaudible]
Uh, yeah.
So the Eigenvectors are a covariance matrix,
points in the principal axes of the ellipse.
That's defined by the contents.
Yeah. Cool. Okay. Um, so this standard Gaussian would mean 0.
So the Gaussian bump is centered at 0,
0 because mu is 0, 0.
Uh, let's move Mu around.
So I'm going to move, you know, Mu to 0,
1.5.
So that moves the Gaussian, uh,
the position of the Gaussian density right.
Now let's move it to a different location.
Move it to minus 1.5, minus 1.
And so by varying the value of Mu,
you could also shift the center of the Gaussian density around.
Okay? So I hope this gives you a sense of,
um, as you vary the parameters,
the mean and the covariance matrix of the 2D Gaussian density, um,
those are probably- probably density functions you can get as
a result of changing Mu and Sigma.
Okay? Um, any other questions about this?
Raise the screen. [NOISE] All right, cool.
Here is a GDA, right, model.
Um, and- and, uh, let's see.
So, um, remember for GDA,
we need to model P of x given y,
right? It's up here, y given x.
So I'm gonna write this separately in two separate equations P of x given y equals 0.
So what's the chance- what's the, uh,
probability density of the features if it is a benign tumor?
Um, I'm going to assume it's Gaussian.
So I'm just going to write down the formula for Gaussian.
[NOISE]
And then similarly, I'm going to assume
that if is a malignant tumor as if y is equal to 1,
that the density of the features is also Gaussian, okay?
And, um, I wanna point out a couple of things,
so the parameters of the GDA model are
mu0, mu1, and sigma.
Um, and for reasons,
we'll go into a little bit,
we'll use the same sigma for both class.
Um, but we use different means, 0 and 1, okay?
Uh, and we can come back to this later.
If you want, you could use separate parameters, you know,
sigma 0 and sigma 1,
but that's not usually done.
So we're going to assume that the two Gaussians,
for the positive and negative classes,
have the same covariance matrix but they,
they have different means.
Uh, you don't have to make this assumption,
but this is the way it's most commonly done.
And then we can talk about the reason why we tend to do that in a second.
Um, so this is a model for P of y given x.
The other thing we need to do is model P of y.
Uh, so y is just a Bernoulli random variable, right.
It takes on, you know, the values 0 or 1.
And so, I'm going to write it like this,
phi to the y times 1 minus phi to the 1 minus y, okay?
Um, and you saw this kind of notation when we talked about logistic regression,
but all this means is that, um, you know,
probability of y being equal to 1 is equal to phi, right.
Because y is either 0 or 1.
And so, um, this is the way of writing,
uh, uh, probability of y equals 1 is equal to phi, okay?
And, uh, you saw a similar explanation,
it's a notation when we're talking about,
um, logistic regression, right,
one week ago, last Monday.
And so, the last parameter is phi.
So this is Rn,
this is also Rn,
this is Rn by n and that's just a real number between 0 and 1, okay?
So, um, for any- let's see.
So if you can fit mu0,
mu1, sigma, and phi to your data,
then these parameters will define P of x given y and P of y.
And so, if at test time you have a new patient walk into your office,
and you need to compute this,
then you can compute,
right, these things in the red and the orange boxes.
Each of these is a number,
and by plugging all these numbers in the formula,
you get a number alpha P of y equals 1 given x and you can then predict,
you know, malignant or benign tumor.
Right. So let's talk about how to fit the parameters.
So you have a training set, um, as usual,
I'm gonna write the tre- well,
I'm go- let me write the training set like this xi,
yi, for i equals 1 through m, right?
This is a usual training set.
Um, and what we're going to do,
in order to fit these parameters is maximize the joint likelihood.
And in particular, um,
let me define the likelihood of
the parameters to be
equal to the product from i equals 1 through m,
up here, xi, yi, you know,
parameterized by the, um, the parameters, okay?
Um, and I'm, I'm just like dropped the parameters here, right?
To simplify the notation a little bit, okay?
And the big difference between, um,
a generative learning algorithm like this,
compared to a discriminative learning algorithm,
is that the cost function you maximize is this joint likelihood which is p of x, y.
Whereas for a discriminative learning algorithm,
we were maximizing, um,
this other thing, right.
Uh, which is sometimes also called the conditional likelihood, okay?
So the big difference between the- these two cost functions,
is that for logistic regression or linear regression and generalized linear models,
you were trying to choose parameters theta,
that maximize p of y given x.
But for generative learning algorithms,
we're gonna try to choose parameters that maximize p of x and y or p of x, y, right.
Okay?
So all right.
So if you use,
um, maximum likelihood estimation.
Um, so you choose the parameters phi, mu0, mu1,
and sigma they maximize the log likelihood, right.
Where this you define as, you know,
log of the likelihood that we defined up there.
Um, and so, uh, th- we,
we actually ask you to do this as a problem set in the next homework.
But so the way you maximize this is,
um, look at that formula for the likelihood,
take logs, take derivatives of this thing,
set the derivative equal to 0 and then solve for
the values of the parameters that maximize this whole thing.
And I'll, I'll, I'll just tell you the answers you are supposed to get.
[LAUGHTER].
But you still have to do the derivation.
Right. um, the value of phi that maximizes this is,
you know, not that surprisingly.
So, so phi is the estimate of probability of y being equal to 1, right?
So what's the chance when the next patient walks into your, uh,
doctor's office that they have a, a malignant tumor?
And so the maximum likelihood estimate for phi is, um,
it's just of all of your training examples,
what's the fraction with label y equals 1, right.
So it's the, the maximum likelihood of the, uh,
bias of a coin toss is just, well, count up the fraction of heads you got,
okay? So this, this is it.
um, and one other way to write this is,
um, sum from i equals 1 through m indicator.
Okay. Right. Um, let's see.
So as you saw the indicator notation on Wednesday, did you?
No.
Uh, did you so- do, did we talk about the indicator notation on Wednesday?
No.
Okay. Um, so, um, uh,
this notation is an indicator function, uh, where,
um, indicator yi, equals 1 is,
uh, uh, return 0 or 1 depending on whether the thing inside is true, right?
So there's an indicator notation in which an indicator of
a true statement is equal to 1 and indicator of a false statement is equal to 0.
So that's another way of writing,
writing this formula, right.
Um, and then the maximum likelihood estimate for mu0 is this, um,
I'll just write out.
Okay.
Ah, so, well, it- it actually if you,
ah, put aside the math for now,
what do you think is the maximum likelihood estimate of the mean of all of the,
ah, features for the benign tumors, right?
Well, what you do is you take
all the benign tumors in your training set and just take the average,
that seems like a very reasonable way.
Just look- look at your training set.
Look at all of the- look at all of the benign tumors, all the Os,
I guess, and you just take the mean of these, and that,
you know, seems like a pretty reasonable way to estimate Mu 0, right?
Look all of your negative examples and average their features.
So this is a way of writing out that intuition.
Um, So the denominator is sum from i equals 1 through m indicates a y_i equals, 0,
and so the denominator will count up the number of
examples that have benign tumors, right?
Because every time y_i equals 0,
you get an extra 1 in this sum,
um, ah, and so the denominator ends up being
the total number of benign tumors in your training set.
Okay? Um, and the numerator, ah,
sum for m equals 1 through m indicator is a benign tumor times x_i.
So the effect of that is, um,
whenever, a tumor is benign is 1 times the features,
whenever an example is malignant is
0 times the features and so the numerator is summing up all the features,
all the feature vectors for all of the examples that are benign.
Does that make sense?
I- I just write this out,
so this is the sum of feature vectors for,
um, for all the examples
with y equals 0 and the denominator is a number of the examples,
where y equals 0, okay?
And then if you take this ratio,
if you take this fraction,
then you're summing up all of the feature vectors for
the benign tumors divide by the total number of benign tumors in the training set,
and so that's just the mean of the feature vectors of all of the benign examples.
Okay? Um, and then,
right, maximum likelihood for Mu 1,
no surprises, is sort of kind of what you'd expect,
sum up all of the positive examples and
divide by the total number of positive examples and get the means.
So that's maximum likelihood for Mu_1,
um, and then I just write this out.
If you are familiar with covariance matrices,
this formula may not surprise you.
But if you're less familiar,
then I guess you can see the details in the homework.
Okay. Don't worry too much about that.
Ah, you can unpack the details in the lecture notes.
So we'll know how it works, okay?
But the covariance matrix, basically tries to,
you know, fit contours to the ellipse, right?
Like we saw, ah, so- so try to fill the Gaussian to both of these with
these corresponding means but you want one covariance matrix to both of these.
Okay? Um, So these are the- so- so- so the way- so the way I motivated this was,
you know, I said, well,
if you want to estimate the mean of a coin toss,
just count up the fraction of coin tosses, they came up heads,
ah, and then it seems that the mean for Mu_0 and Mu_1,
you just look at these examples and pick the mean, right?
So that- that was the intuitive explanation for how you get these formulas.
But the mathematically sound way to get
these formulas is not by this intuitive argument that I just gave,
it's instead to look at the likelihood, ah,
take logs, get the log likelihood,
take derivatives, set derivatives equal to 0,
solve for all these values and prove more formally that these
are the actual values that maximize this thing, right?
By- by the same theories as you solved,
so you can see that for yourself,
um, in the problem sets.
Okay? So- All right.
Um, finally, having fit these parameters,
um, if you want to make a prediction, right?
So given the new patient, ah,
how do you make a prediction for whether their tumor is malignant or benign?
Um, so if you want to predict the most likely class label,
ah, you choose max over y,
of p of y, given x, right?
Um, and by Bayes' rule,
this is max over y of p of x given y,
p of y divided by p of x.
Okay? Now, um, I wanna introduce one esh- well,
one- one more piece of notation which is,
ah, I wanna introduce,
actually, how- how many of you are familiar with the arg max notation?
Most of you? Like two- two-thirds? Okay, cool.
I- I- I'll go over this quickly.
So, um, this is just an example.
So the, um, let's see.
Ah, boy. All right.
So, you know, the Min over z of, uh,
z minus 5 squared is equal to 0
because the smallest possible value of z by a 5 squared is 0, right?
and the arg min over z of z minus 5 squared is equal to 5.
Okay? So the min is the smallest possible value attained by the thing inside
and the arg min is the value you need to
plug in to achieve that smallest possible value, right?
So ah, the prediction you actually want to make,
if you want to output a value for y,
you don't wanna output a probability, right?
You know what I'm saying? Well, what do I think is the value of y?
So you might choose a value of y that maximizes this,
and- so- so there's the arg max of this and this would be either 0 or 1, right?
Um, so that's equal to arg max of that,
and you notice that,
ah, this denominator is just a constant, right?
It doesn't- it doesn't- it's a p of x,
it's- y doesn't even appear in there?
It's just some positive number.
And so this is equal to,
just arg max over y,
p of x given y times p of y, okay?
So when implementing, um, ah,
when- when making predictions with Gaussian disc-
in a- with the generative learning algorithms,
sometimes to save on computation,
you don't bother to calculate the denominator,
if all you care about is to make a prediction,
but if you'd actually need a probability,
then you'd have to normalize the probability, okay?
Okay. So let's examine
what the algorithm is doing.
[NOISE].
All right. So let's look at the same dataset and compare and contrast what
a discriminative learning algorithm versus
a generative learning algorithm will do on this dataset.
Right. Um, here's example with two features X1 and X2 and positive and negative examples.
So let's start with a discriminative learning algorithm.
Um, let say you initialize the parameters randomly.
Typically, when you run a logistic regression,
I almost always initialize the parameters as 0 but- but this just, you know,
it's more interesting to start off for the purposes of visualization,
with a random line I guess.
And then if you run one iteration of gradient descent on the conditional likelihood,
um, one iteration of logistic regression moves the line there.
There's two iterations, three iterations, um,
four iterations and so on and after
about 20 iterations it will converge to that pretty decent discriminative boundary.
So that's logistic regression,
really searching for a line that separates positive and negative examples.
How about the generative learning algorithm?
What it does is the following,
which is fit with Gaussian discriminant analysis.
What we'll do, is fit Gaussians to the positive and negative examples.
Right, and just one- one technical detail, um,
I described this as if we look at the two classes separately
because we use the same covariance matrix sigma for the positive and negative classes.
We actually don't quite look at them totally separately but we do
fit two Gaussian densities to the positive and negative examples.
And then what we do is,
for each point try to decide whether this is class label using Bayes' rule,
using that formula and it turns out that this implies the following decision boundary.
Right. So points to the upper right of this decision boundary,
to that straight line I just drew,
you are closer to the negative class.
You end up classifying them as negative examples and
points to the lower left of that line,
you end there classifying as- as a positive examples.
And I've- I've also drawn in green here the decision boundary for logistic regression.
So- so- so these two algorithms actually come up
with slightly different decision boundaries.
Okay, but the way you arrive at these two decision boundaries are a little bit different.
So, um. All right,
let's go back to the- Any questions about this? Yeah.
[NOISE]
[inaudible].
Oh, sure yes, good question.
So why- why- why do we use two separate means,
mu 0 and mu 1 and a single covariance matrix sigma?
It turns out that, um-.
It turns out that if you choose to build the model this way,
the decision boundary ends up being linear and so
for a lot of problems if you want to linear decision boundary,
um, uh, um, yeah.
And it turns out you could choose to use two separate,
um, covariance matrix sigma 0 and sigma 1,
and they'll actually work okay.
Right. There's- it is actually very reasonable to do so as well,
but you double the number of parameters roughly and
you end up with a decision boundary that isn't linear anymore.
But it is actually not an unreasonable algorithm to do that as well.
Um, now, there's one-
[BACKGROUND].
Now, there's one very interesting property, um,
about Gaussian discriminant analysis and it turns out that's- ah.
Well, let's- let's compare
GDA to logistic regression and,
um, for a fixed set of parameters.
Right. So let's say you've learned some set of parameters.
Um, I'm going to do an exercise where we're going to plot,
P of Y equals 1 given X,
you're parameterized by all these things,
right, as a function of x.
So I'm gonna do this little exercise in a second,
but what this means is,
um, well, this formula,
this is equal to P of X given Y equals 1, you know,
which is parameterized by- right well,
the various parameters times p of y equals 1,
is parameterized by phi divided by P of X which depends on all the parameters, I guess.
Right. So by Bayes rule,
you know this formula is equal to
this little thing and just as we saw earlier, I guess right.
Once you have fixed all the parameters that's
just a number you compute by evaluating the Gaussian density.
Um, this is the Bernoulli probability,
so actually P of Y equals 1 parameterized by phi is just equal to phi
is that second term and you similarly calculate the denominator.
But so for every value of x,
you can compute this ratio and thus get a number for
the chance of Y being 1 given X. So I'm gonna go
through one example of
what function you'd get for P of Y equals 1 given X,
for what function you get for this if you actually plot this for,
um, different values of X.
Okay. So, um, let's see.
Let's say you have just one feature X,
so X is a- a- and let's say that you have
a few negative examples there and a few positive examples there.
Right. So it's a simple dataset.
Okay, and let's see what Gaussian discriminant analysis will do on this dataset.
Um, with just one feature so that's why all the data is parsing on 1D.
So let me map all this data to an x-axis.
I just filled this data and mapped it down.
And if you fit a Gaussian to each of these two data sets then you end up with, you know,
Gaussians as follows where this bump on the left is P of X given Y equals 0 and
this bump on the right is P of X given Y equals 1.
Right, and- and again just to check on all details that
we set the same variance to the two Gaussians,
but you know, you kinda model the Gaussian densities of what does this class 0 look like?
What does class 1 look like with two Gaussian bumps like this?
Then because the dataset is split 50-50 P of Y equals 1 is 0.5.
Right, so one half prior.
Okay. Now, let's go through that exercise I described on the left of trying to
plot P of Y equals 1 given X for different values of X.
So the vertical axis here as P of Y equals 1 given different values of X.
So, um, let's pick a point far to the left here.
Right. With this model you- if you actually
calculate this ratio you find that if you have a point here,
it almost certainly came from this Gaussian on the left.
If- if you have an unlabeled example here,
you're almost certain it came from the class 0 Gaussian
because the chance of
this Gaussian generating example all the way to left is almost 0.
Right, and so chance of P- P of Y equals 1 given X is very small.
So for a point-like that,
you end up with a point you know,
very close to 0, right.
Um, let's pick another point.
Right, how about this point, the midpoint.
Well, if you're getting example right at the midpoint,
you- you really have no idea. You really can't tell.
Did this come from the negative or the positive Gaussian?
Can't tell. Right. So this is really 50-50.
So I guess if this is 0.5 for that midpoint you
would have P of Y equals 1 given X is 0.5.
Um, then if you go to a point away to the variance,
if you get an example way here,
then you'd be pretty sure this came from the positive examples and so,
you know, you get a point like that.
Right. Now, it turns out that if you repeat this exercise
sweeping from left to right for many many points on the X axis you find that,
for points far to the left,
the chance of this coming from, um,
the Y equals 1 class is very small and as you approach this midpoint,
it increases to 0.5 and it surpasses 0.5.
And then beyond a certain point,
it becomes very very close to 1.
Right, and you do this exercise and actually just for every point, you know,
for a dense grid on the x-axis evaluate
this formula which will give you a number between 0 and 1.
Is the probability and go ahead and plot,
you know, the values you get a curve like this.
It turns out that if you connect up the dots,
um, then this is exactly a sigmoid function.
The shape of that turns out to be
exactly a shaped sigmoid function and you prove this in the problem sets as well.
Right. Um, so, um,
both logistic regression and Gaussian discriminant analysis actually end up using
a sigmoid function to calculate P of Y equals 1 given X or- or the,
the outcome ends up being a sigmoid function.
I guess the mechanics is,
you actually use this calculation rather than compute a sigmoid function.
Right. But, um, the specific choice of the parameters they end up
choosing are quite different and you saw when I was
projecting the results on the display just now in PowerPoint,
that the two algorithms actually come up with two different decision boundaries.
Right. So, um, let's discuss when
a genitive algorithm like GDA is superior and when
a distributed algorithm like logistic regression is superior.
Um, let's see if I can get rid of this.
[BACKGROUND]
All right. So GDA, Gaussian Distributed Analysis.
So the generative approach.
This assumes that x given y equals 0,
this is Gaussian, with mean Mu_0 and co variance Sigma.
It assumes x given y equals 1,
this is Gaussian with mean Mu_1 and covariance Sigma,
and y is Bernoulli with, um, parenthesis Phi.
Right. And what logistic regression does.
[NOISE] This is a discriminative algorithm,
uh, there is some [LAUGHTER] strange wind at the back, is it?
Yeah.
I see.
Okay. Cool. All right.
Yeah. Why? You know
the-there's just a scary UN report on global [LAUGHTER] warming over the weekend.
I hope we don't already have storms here, um.
Okay. It's okay. Did you guys see the UN report?
It's slightly scary actually wa- the-
the UN report on global warming but hopefully- all right.
Good. Hurricane stopped.
[LAUGHTER] Um, let's see.
Uh, so what logistic regression assumes is p of y equals 1 given x.
You know, that this is, uh, governed by logistic function.
Right. So this is really 1 over 1 plus e is a negative Theta transpose x.
We-where some details about x_0 equals 1 and so on.
Right. So just- just- okay.
So- so in other words,
uh, it's assumed that this is,
um, p of y equals 1 given x is logistic.
Okay. And the argument that I just described just now, uh,
plotting you know p of y equals 1 given x
point-by-point to really the sigmoid curve I drew on the other board.
What that illustrates.
Um, it doesn't prove it.
You prove it yourself in a homework problem.
But what that illustrates is that,
this set of assumptions
implies that p of y equals 1 given x is governed by a logistic function.
Right. But it turns out that the implication in the opposite direction is not true.
Right. So if you assume p of y equals 1
given x is governed by logistic function by- by this shape,
this does not in any way shape or form assume that x given y is Gaussian,
uh, uh, x given y equals 0 is Gaussian x given y equals 1 is Gaussian.
Right. So what this means is that GDA,
the generative learning algorithm in this case,
this makes a stronger set of assumptions and which this regression makes
a weaker set of
assumptions because you can prove these assumptions from these assumptions.
Okay. Um, and by the way as- as- uh,
as- as- uh, let's see.
And so what you see in a lot of learning algorithms is that, um,
if you make strongly modeling assumptions
and if your modeling assumptions are roughly correct,
then your model will do better because you're telling more information to the algorithm.
So if indeed x given y is Gaussian,
then GDA will do better because you're telling
the algorithm x given y is Gaussian and so it can be more efficient.
And so even if a very small dataset, um,
if these assumptions are roughly correct,
then GA will do better.
And the problem with GDA is,
if these assumptions turn out to be wrong.
So if x given y is not at all Gaussian,
then this might be a very bad set of assumptions to make.
You might be trying to fit a Gaussian density to data that is not
at all Gaussian and then GDA would do more poorly.
Okay. So here's one fun fact.
Here's another example, get to your question in a second,
which is let's say the following are true;
let's say that x given y equals 1 is Poisson with, uh,
parameter Lambda_1 and x given y equals 0 is Poisson with mean,
uh, Lambda_0, or lambda_1 not 0 and y,
as before, is Bernoulli 5x.
Right. It turns out that this set of assumptions also imply that p of y equals 1 given x.
This is logistic, okay, and you can prove this.
And this is actually true for, um,
any generalized linear model, actually where, uh,
where- where, uh, the difference between
these two distributions varies only
according to the natural parameter as a generalized name.
Excuse me, of the exponential family distribution.
Right. And so what this means is that, um,
if you don't know if your data is Gaussian or Poisson,
um, if you're using logistic regression you don't need to worry about it.
It'll work fine either way.
Right. So- so, you know, maybe, um,
you are fitting data to s- maybe a fitting, uh,
uh, a model, binary classification model to some data.
And you don't know, is a data Gaussian?
Is it Poisson? Is this some other exponential family model? Maybe you just don't know.
But if you're fitting logistic regression,
it- it'll do fine under all of those scenarios.
Right. But if your data was actually Poisson but you assumed it was Gaussian,
then your model might do quite poorly.
Okay. So the key high level principles when you take away from this is, um, uh, uh,
if you make weaker assumptions as in logistic regression,
then your algorithm will be more robust to modeling
assumptions such as accidentally assuming the data is Gaussian and it is not.
Uh, but on the flip side,
if you have a very small dataset, then, um,
using a model that makes more assumptions will actually allow you to do better
because by making more assumptions you're just
telling the algorithm more truth about the world which is,
you know, "Hey, algorithm, the world is Gaussian," and if it is Gaussian,
then it will actually do- do- do better.
Okay. Your question at the back or a few questions. Go ahead.
Just from that, is there a point do you know like what sort
of data it usually has a Gaussian problem?
Oh, oh, yeah.
Practical sample without data is a Gaussian probably,
you know, it's, uh,
uh- yeah, you know,
it's a matter of degree.
Right. Most data on this universe is Gaussian [LAUGHTER] uh,
uh, uh, except at this feed data, I guess.
Yeah, but- but, um- I think it's actually a- a matter of degree.
Right. If- if you plot- actually if you take
continuous value data- no, ther- ther- there are exceptions.
You could plot it and most data that you plot, you know,
will not really be Gaussian but a lot of
it you can convince yourself is vaguely Gaussian.
So I think a lot of it is amount of degree.
I- I- I actually tell you the way I choose to use,
um, these two algorithms.
So I think that the whole world has moved toward using bigger than three datasets.
Right. Digital Civil Society which is a lot of data
and so for a lot of problems we have a lot of data,
I would probably use logistic regression.
Because with more data,
you could overcome telling the algorithm less about the world.
Right. So- so the algorithm has two sources of knowledge.
Uh, one source of knowledge is what did you tell it,
what are the assumptions you told it to make?
And the second source of knowledge is learned from
the data and in this era of big data,
we have a lot of data, you know,
there is a strong trend to use logistic regression which makes
less assumptions and just lets the algorithm
figure out whether it wants to figure out from the data.
Right. Now, one practical reason why I still use algorithms like the GDA,
general discriminant analysis, so algorithms like this,
um, uh, is that,
it's actually quite computationally efficient and so
the- there's actually one use case at Landing. AI that we're working
on where we just need to fit a ton of models and
don't have the patience to run the GC progression over and over.
And it turns out computing mean and variances of, um,
covariance matrices is very efficient and so
there's actually apart from the assumptions type of benefit,
uh, which is a general philosophical point.
We'll see again later in this course.
Right. Th- this idea about do you make strong or weak assumptions?
This is a general principle in machine learning that we'll see again in other places.
But the very concrete- the other reason I tend to use GDA these days is
less that I think I perform better from
an accuracy point of view but there's actually a very efficient algorithm.
We just compute the mean covar- covariance and we are
done and there's no iterative process needed.
So these days when I use these models, um,
is more motivated by computation and less by performance.
But this general principle is one that we'll come back to again later
when we develop more sophisticated learning algorithms. Yeah.
Uh, if the data is generated from a Gaussian but
my program synthesis are different with the assumption
that we just use the same program for performance-?
Oh, right, ah, so what happens when the co-variance matrices are different?
It turns out that, uh,
uh, trying to remember, it still ends up being a logistic function but with
a bunch of quadratic terms in the logistic function.
So it's not a linear decision boundary anymore.
You can end up with a decision boundary,
you know, that- that- that looks like this, right?
With positive and negative examples separated by
some- by some other shape from a linear decision boundary.
Uh, you- you could- you could- you could fig- actually,
I- if you're curious, I encourage you to, you know, uh, uh,
fire up Python NumPy and- and
play around their parameters and plot this for yourself, uh, questions?
Is it recommended that we use some kind of statistical global test to make
sure that the plot distribution results have a equal variance before we do GDA?
Yeah. It's recommended that you do some cyclical tests to see if it's Gaussian,
um, I can tell you what's done in practice.
I think in practice,
if you have enough data to do a cyclical test and gain conviction,
you probably have enough data to just use logistic regression,
um, uh, the- the- I- I don't know.
[LAUGHTER] Well no, that's not really fair. I don't know.
If they're very high dimensional data,
I- I think what often happens more,
is people just plot the data,
and if it looks clearly non-Gaussian,
then, you know, there will be reasons not to use GDA.
But what happens often is that um,
uh, uh, yeah sometimes you just have a very small training
set and it's just a matter of judgment, right?
Like if you have, you- if you have, uh, uh, uh,
you know, I don't know, 50 examples of healthcare records,
then you just have to ask some doctors and ask, "Well,
do you think the distribution is rath- rath-
relatively Gaussian," and use domain knowledge like that.
Right? I think- by the way a- another philosophical point,
um, I think that,
uh, the machine learning world has,
frank- you know, a little bit overhyped big data, right?
And- and yes it's true that when we have more data,
it's great and I love data and a- um,
having more data pretty much never hurts and
usually the more data the better, so all that is true.
And I think we did a good job telling people that high-level message,
you know, more data almost always helps.
But, um, uh, I think a lot of the skill in machine learning
these days is getting your algorithms to work
even when you don't have a million in examples,
even you don't have a hundred million examples.
So there are lots of machine learning applications
where you just don't have a million examples, uh,
you have a hundred examples and, um,
it's then the skill in designing your learning algorithm matters much more.
Um, so if you take something like ImageNet,
mi- million in- in- images,
there are now dozens of teams,
maybe hundreds of teams, I don't know.
They can get great results.
They give a million examples, right?
and so the performance difference between teams, you know,
there are now dozens of teams that get great performance,
if a million examples, uh,
for- for- for image classification, for ImageNet.
But if you have only a hundred examples,
then the high-skilled teams will actually do much,
much, much, much better than the low skilled teams,
whereas the performance gap is smaller when you have giant data sets I think,
so and I think that it's these types of intuitions,
you know, what assumptions you use,
generative or discriminative, that actually
distinguishes the high-skilled teams and, uh, and, uh,
and the less experienced teams and drives a lot of
the performance differences when you have small data.
Oh, and if someone goes to you and says,
"Oh you only have a hundred examples,
you'll never do anything."
Uh, then I don't know,
if- if there's a competitor saying that,
then I'll say, "Great, you know,
don't do it because I can make it work."
Uh, well, I don't know.
Uh, but- but I think there are a lot of applications where
your skill at designing a machine learning system,
really makes a bigger difference when you have a- make- makes
a- it makes a difference from big data and small data,
but it just- this is a very clear where you don't have much data,
is the assumptions you code into the algorithm like,
is it Gaussian, is it Poisson?
That- that skill allows you to drive
much bigger performance than a lower-skill team would be able to.
All right. This is- uh- uh-
coul- could- I should still take questions from all of you. Yeah, go ahead.
Um, what's the implication when [inaudible].
Oh, sure. So does this, uh,
yes, so what's the general statement of this?
Yes, so if, uh,
x given y equals 1,
uh, it comes from an exponential family distribution,
x given y equals 0 comes to an exponential family distribution,
it's the same exponential family distribution and if they
vary only by the natural parameter of the exponential family distribution,
then this will be logistic.
Yeah. Um, I think this was once a midterm homework problem to prove this actually?
But, yeah. All right,
uh, actually let's take one last question then we move on, go ahead.
Uh, if performance [inaudible]
Oh, uh, does performance
improvement happen even as you increase the number of classes?
Uh, ye- I think so yes,
uh, and the generalization of this would be
the Softmax Regression which I didn't talk about. But yes.
I think it's a similar thing holds true for, um,
GDA for multiple- and we have so far we're going to talk about Binary Classification,
whether you have more than two classes.
But, uh, but yes, similar- similar things holds true for,
uh, like a GDA with three classes and Softmax.
Yeah. Oh yes, right. You saw Softmax the other day.
Cool. Um, and this- this theme
that when you have less data the algorithm needs to rely more on assumptions you code in.
This is a recurring theme that we'll come back to it as well.
This is one of the important principles of machine learning,
that when you have less data your skill at coding and your knowledge matters much more.
Uh, this is a theme we'll come back to you when we
talk about much more complicated learning algorithms
as well. All right.
So, uh, I want a fresh board for this.
So you've seen GDA in the context of, um,
continuous valued, uh, features x.
The last thing I want to do today, um,
is talk about one more generative learning algorithm called Naive Bayes,
um, and I'm gonna use as most of the example;
e-mail spam classification, but this- this is- this-
I guess this is our first foray into natural language processing, right?
But given in a piece of text, like given a piece of email,
can you classify this as spam or not spam?
Or, uh, other examples, uh, uh, actually several years ago, Ebay,
used to have a problem of, you know,
the- if someone's trying to sell something and you write a text description, right?
"Hey, I have a secondhand, you know,
Roomba, I'm trying to sell it on Ebay."
How do you take that text that someone wrote over the description and
categorize it, is it an electronic thing or are they trying to sell a TV?
Are they trying to sell clothing?
Uh, but these- these examples of text classification problems,
we have a piece of text and you want to classify into one of
two categories for spam or not spam or one of maybe thousands of categories,
and they're trying to take a product description and classify it into one of the classes.
Um, and so the first question we will have is, um, uh,
given the e-mail problem, uh,
given the e-mail classification problem,
how do you represent it as a feature vector?
And so, um, in Naive Bayes what we're going to do is take your e-mail,
take a piece of e-mail and first map it to a feature vector X.
And we'll do so as follows,
which is first, um, let's start with a- let's start with
the English dictionary and make a list of all the words in the English dictionary, right?
So first of all there's the English dictionary as A,
second word in the English edition is aardvark.
Third word is aardwolf.
[BACKGROUND]
No, it's easy, look it up.
[NOISE] [LAUGHTER]
Um, and then, you know, uh, uh,
e- e-mail spam lot of people asking to buy stuff so that they would buy, right?
And then, um, uh,
and then the last word in my dictionary is zymurgy,
which is the technological chemistry that refers to the fermentation process in brewing.
Um, So- so- I think it is a useful way to think about it, in- in- in- practice,
what you do is not, uh, uh,
actually look at the dictionary but look at the top 10,000 words,
you know, in your training set.
Right? So maybe you have 10,000,
it's easier to think about it as if it was a dictionary but, you know,
in practice, well, you- the other thing that's- dictionary has too many words,
but where- the other way to do this is to look through your own e-mail
co-pairs and just find the top 10,000 occurring words and use that as a feature set,
and so I don't know.
Right? And your e-mails, I guess you're getting a bunch of
e-mail about- from us or maybe others about CS229.
So CS229 might appear in
your dictionary of building your e-mail spam filter for yourself,
even if it doesn't appear in the- in the official, uh,
was it like the Oxford dictionary,
just yet just- just- just- you wait,
we'll- we- we'll get CS229 there someday.
All right. Um, and so given an e-mail,
what we would like to do is then, um,
take this piece of text and represent it as a feature vector.
And so one way to do this is,
um, you can create a binary feature vector,
that puts a 1,
if a word appears in the e-mail and puts a 0 if it doesn't.
Right? So if you've gotten an e-mail, um, uh,
that asks you to, you know,
buy some stuff and then the word A appears in e-mail, you put a 1 there.
Did not try to sell aardvark or aardwolf,
so 0 there, buy and so on.
Right? So you take a- take an e-mail and turn it into a binary feature vector.
Um, and so here the feature vector is 0, 1 to the n,
because there's a n-dimensional binary feature vector,
where- where for the purpose of illustration, let's say,
n is 10,000 because you're using,
you know, take the top 10,000 words, uh,
that appear in your e-mail training set as the dictionary that you will use.
So, um.
So in other words, X_i is
indicator word i appears in the e-mail, right?
So it's either 0 or 1 depending on whether or not that word
i from this list appears in your e-mail.
Now, um, in the Naive Bayes algorithm,
we're going to build a generative learning algorithm.
Um, and so we want to
model P of x given y, right?
As well as P of y, okay?
But there are, uh,
2 to the 10,000 possible values of x, right?
Because x is a binary vector of this 10,000 dimensional.
So we try to model P of x in the straightforward way as a multinomial distribution over,
you know, 2 to the 10,000 possible outcomes.
Then you need, right, uh, uh, you need,
you know 2 to the 10,000 parameters, right?
Which is a lot, or technically,
you need 2 to 10,000 minus
1 parameter because that adds up to 1, and you can see one parameter.
But so, modeling this without additional assumptions won't- won't work,
right, because of the excessive number of parameters.
So in the Naive Bayes algorithm,
we're going to assume that X_i's are
conditionally independent given y, okay?
Uh, let me just write out what this means,
but so P of x_1 up to x_10,000 given y by the chain rule of probability,
this is equal to P of x_1 given y times P of x_2 given,
um, x_1 and y times p of x_3 given x_1,
x_2 Y up to your p of x_10,000 given, and so on, right?
So I haven't made any assumptions yet.
This is just a true statement of fact as always
true by the- by the chain rule of probability.
Um, and what we're going to assume which is what this assumption is,
is that this is equal to this first term no change the
x_2 given y p of x_3 given y and so on,
p of X_10,000 given y, okay?
So this assumption is called a
conditional independence assumption it's also sometimes called the Naive Bayes assumption.
But you're assuming that, um,
so long as you know why the chance of seeing the words,
um, aardvark in your e-mail does not
depend on whether the word "A" appears in your e-mail, right?
Um, and this is one of those assumptions that is definitely not
a true assumption and that is just not mathematically true assumption.
Just that sometimes your data isn't perfectly Gaussian,
but if it was Gaussian you can kind of get away with it.
So this assumption is not true,
um, in a mathematical sense,
but it may be not so horrible that you can't get away with it, right?
Um, and so- so- so it's like,
if you- if any of you are familiar with probabilistic graphical models,
if you've taken CS-228, uh,
this assumption is summarizing this picture,
and if you haven't taken CS-228 this picture won't make sense, but don't worry about it.
Um, right, that, uh,
once you know the class label is a spam or not spam whether
or not each word appears or does not appear is independent, okay?
So this is called conditional.
So the mechanics of this assumption is really just captured by this equation,
um, and you just use this equation,
that's all you need to derive Naive Bayes.
But intuition is that if I tell you whether
this piece- if I tell you that this piece of e-mail is spam then
whether the word by appears in it doesn't affect you believes that what- whether the word
mortgage or discount or whatever spammy words appear, right?
So just to summarize,
this is product from i equals 1 through n of p of X_i given y.
All right, so the parameters of this model,
um, are, I'm going to write it, Phi subscript,
um, j given y equals
1 as the probability that x_j equals 1 given y equals 1,
phi subscript j given y equals
0, and then Phi.
And just to distinguish all these Phi's from each other,
we can just call this Phi subscript y, okay?
So this parameter says,
if a spam e-mail,
if y equals 1 is spam and y equals 0 is not spam.
If it's a spam e-mail, what's the chance of word j appearing in the e-mail?
If it's not spam e-mail what's the chance of word j appearing in the e-mail.
Then also, what's the cost prior,
what's the prior probability that the next e-mail you receive in your,
uh, in your- in your inbox is spam e-mail?
And so to fit the parameters of this model,
you would s- similar to Gaussian discriminant analysis,
write down the John- joint likelihood.
So the joint likelihood of these parameters, right?
Is a product, you know,
given these parameters, right?
Similar to what we had for Gaussian discriminant analysis.
And the maximum likelihood estimates, um, if you take this,
take logs, take derivatives,
set derivatives to 0, solve for the values that maximize this,
you find that the maximum likelihood estimates of the parameters are, Phi_y,
this is pretty much what you'd expect, right?
It's just a fraction of spam e-mails and, uh,
Phi of j given y equals 1 is, um, well,
I'll write this out in indicator
function notation.
Oh, shoot, sorry.
Okay. So that's the indicator function notation of writing notes.
Look through your, uh, training set,
find all the spam e-mails and of all the spam e-mails,
i.e., examples of y equals 1 count up what fraction of them had word j in it, right?
So you estimate that the chance of word j appearing- you estimate the chance of
the word by appearing in a spam e-mail is
just we have all the spam e-mails in your training set,
what fraction of them contain the word by?
What- what fraction of them had, you know,
x_j equals 1 for say,
the word by, okay?
Um, and so it turns out that if you implement this algorithm,
it will- it will nearly work,
I guess, uh, uh,
but this is Naive Bayes for,
um, for e-mail spam classification, right?
And I mentioned, uh, one reason this,
uh, and it turns out that what one fixed to this algorithm,
which we'll talk about on Wednesday, um,
this is actually, it's actually a not too horrible spam classifier.
It turns out that if you used logistic regression
for spam classification you do better than this almost all the time.
But this is a very efficient algorithm,
because estimating these parameters is just counting,
and then computing probabilities is just multiplying a bunch of numbers.
So there's nothing iterative about this.
So you can fit this model very efficiently and
also keep on updating this model even as you get new data,
even as you get new- new- new uses hits mark or spam or whatever,
even as you get new data, you can update this model very efficiently.
Um, but it turns out that,
uh, actually, the biggest problem with this algorithm is,
what happens if, uh,
this is zero or if- if you get zeros in some of these equations, right?
But we'll come back to that when we talk about Laplace moving on Wednesday, okay?
All right, any quick questions before we wrap up? Okay, okay good.
So now you've learned about generative learning algorithms, um,
we'll come back on Wednesday and learn about
some more fine details how to make this work even better.
So let's break, I'll see you on Wednesday.
 All right. Hey, everyone.
Morning and welcome back.
Um, so what I'd like to do today
is continue our discussion of Naive Bayes and in particular,
um, we've described how to use Naive Bayes in a generative learning algorithm,
to build a spam classifier that will almost work, right?
And, and, and so today you see how Laplace smoothing is one other idea, uh,
you need to add to the Naive Bayes algorithm we described on Monday,
to really make it work, um, for, say,
email spam classification, or,
or for text classification.
Uh, and then we'll talk about the different version of
Naive Bayes that's even better than the one we've been discussing so far.
Um, talk a little bit about,
ah, advice for applying machine-learning algorithms.
So this would be useful to you as you get started on your,
ah, CS229 class projects as well.
This is a strategy of how to choose an algorithm and what to do first, what to do second,
uh, and then we'll start with,
um, intro to support vector machines.
Okay? Um, so to recap, uh,
the Naive Bayes Algorithm is
a generative learning algorithm in which given a piece of email,
or Twitter message or some piece of text, um,
take a dictionary and put in zeros and ones depending on
whether different words appear in
a particular email and so this becomes your feature representation for,
say, an email that you're trying to classify as spam or not spam.
Um, so using the indicator function notation, um, X_j-, uh,
X_j- I've been trying to use the subscript J not consistently
to denote the indexes and the features
and ith index in the training examples and you'll see I'm not being consistent with that.
So X_j is whether or not the indicator for whether words j appears in an email.
And so, um, to build a generative model for this,
uh, we need to model these two terms p of x given y and p of y.
Uh, so Gaussian distribution analysis models these two terms
with a Gaussian and the Bernoulli respectively and Naive Bayes uses a different model.
And with Naive Bayes in particular p of x given y is modeled as a, um,
product of the conditional probabilities of the individual features given the class label y.
And so the parameters that Naive Bayes model are,
um, phi subscript y is the class prior.
What's the chance that y is equal to 1,
before you've seen any features?
As well as phi subscript J given y equals 0,
which is a chance of that word appearing in a non-spam,
as well as phi subscript J given y equals 1 which is a chance of
that word appearing in spam email.
Okay? Um, and so if you derive the maximum likelihood estimates,
you will find that the maximum likelihood estimates of,
you know, phi y is this.
Right? Just a fraction of training examples, um,
that was equal to spam and maximum likelihood estimates of this-
and this is just an indicator function notation,
way of writing, um,
look, at all of your,
uh, emails with label y equals 0 and contact y fraction of them,
did this feature X_j appear?
Did this word X_j appear?
Right? Um, and then finally at prediction time,
um, let's see,
you will calculate p of y equals 1 given X.
This is kinda according to Bayes rule.
Okay?
Um, all right.
So it turns out this algorithm will almost work and here's where it breaks down,
which is, um, you know,
so actually eve- every year,
there are some CS229 students and some machine learning students,
they will do a class project and some of you will
end up submitting this to an academic conference.
Right? Some, some- actually some,
some of CS229 class projects get submitted,
you know, as conference papers pretty much every year.
One of the top machine learning conferences,
is the conference NIPS.
NIPS stands for Neural Information Processing Systems, um,
ah, and let's say that in your dictionary,
you know, you have 10,000 words in your dictionary.
Let's say that the NIPS conference,
the word NIPS corresponds to word number 6017, right?
In your, in your 10,000 word dictionary.
But up until now,
presumably you've not had a lot of emails from your friends asking,
"Hey, do you want to submit the paper to the NIPS Conference or not."
Um, and so if you use your current, you know,
email, set of emails to find these maximum likelihood estimates of parameters,
you will probably estimate that, um,
probability of seeing this word given that it's spam email, is probably zero.
Right? Zero over the number of, ah,
examples that you've labeled as spam in your email.
So if, if you train up this model using your personal email,
probably none of the emails you've received for
the last few ones had the word NIPS in it, um, maybe.
Uh, and so if you plug in this formula for maximum likelihood estimate,
the numerator is 0 and so your estimate of this is probably 0.
Um, and then similarly,
this is also 0 over,
you know, the number of non-spam emails I guess.
Right. So that's what this is,
is just this formula.
Right? And, um, statistically it's just a bad idea to say that
the chance of something is 0 just because you haven't seen it
yet and where this will cause the Naive Bayes algorithm to break down is,
if you use these as estimates of the, of the parameters,
so this is your estimates parameter phi subscript 6017 given y equals 1.
This is phi subscript 6017 given y equals 0.
Yes? And if you ever calculate this probability,
that is equal to a product from I equals 1 through n. Let's say you
have 10,000 words appear of X_i equals 1,
p of X_i given y, right?
And so if, um,
you train your spam classifier on the emails you've gotten up until today,
and then after CS229, your project teammates sen- starts sending you emails saying,
hey, you know, we like the class project.
Shall we consider submitting this class project to the NIPS conference?
The NIPS conference deadline is usually in,
um, sort of May or June most years so, you know,
finish your class project this December,
work on it some more by January, February,
March, April next year and then maybe submit it to the conference May or June of 2019.
When you start getting emails from your friends saying,
let's submit our papers to NIPS conference,
then when you start to see the word NIPS in your email maybe in March of next year,
um, this product of probabilities will have a 0 in it, right?
And so this thing that I've just circled will evaluate to
0 because you multiply a lot numbers, one of which is 0.
Um, and in the same way this,
well, this is also 0, right?
And this is also 0 because there'll be that one term in that product over there.
And so what that means is if you train a spam classifier
today using all the data you have in your email inbox so far,
and if tomorrow or- or,
you know, or two months from now, whenever.
The first time you get an email from your teammates that has the word NIPS in it,
your spam classifier will estimate this probability as 0 over 0 plus 0, okay?
Now, apart from the divide by 0 error, uh,
it turns out that, um,
statistically, it's just a bad idea, right?
To estimate the probability of something as 0 just
because you have not seen it once yet, right?
Um, so [NOISE] what I want to do is describe to you Laplace smoothing,
which is a technique that helps,
um, address this problem.
Okay? And, um, let's- let's- In order to motivate Laplace smoothing, let me, um,
use a- a- a- Yeah,
Let me use a different example for now. Right? Um.
Let's see. All right.
So, you know, several years ago,
this is- this is all the data,
but several years ago- so- so let me put aside Naive Bayes,
I want to talk about Laplace smoothing.
We will come back to apply Laplace smoothing in Naive Bayes.
So several years ago,
I was tracking the progress of the Stanford football team,
um, just a few years ago now.
But that year on 9/12, um,
our football team played to Wake Forest and,
you know, actually these are all the, uh,
all the stay games we played that year, right?
And, um, uh, we did not win that game.
Then on 10/10, we played Oregon State and we did not win that game.
Arizona, we did not win that game.
We played Caltech, we did not win that game.
[LAUGHTER].
And the question is,
these are all the away games- almost all the out of state games we played that year.
And so you're, you know, Stanford football team's biggest fan.
You followed them to every single out of state game and watched all these games.
The question is, after this unfortunate streak,
when you go on- there's actually a game on New Year's Eve,
you follow them to their over home game,
what's your estimate of the chances of their winning or losing?
Right? Now, if you use maximum likelihood,
so let's say this is the variable x,
you would estimate the probability of their winning.
Well, maximum likelihood is really count up the number of wins, right,
and divide that by the number of wins plus the number of losses.
And so in this case, um,
you estimate this as 0 divided by number of wins with 0,
number of losses was 4, right?
Which is equal to 0, okay?
Um, that's kinda mean, right?
[LAUGHTER].
They lost 4 games, but you say, no, the chances of their winning is 0.
Absolute certainty.
And- and- and just statistically, this is not,
um, this is not a good idea.
Um, and so what Laplace smoothing,
what we're going to do is, uh,
imagine that we saw the positive outcomes,
the number of wins, you know,
just add 1 to the number of wins we actually saw and also the number of losses add 1.
Right? So if you actually saw 0 wins, pretend you saw one and if you saw 4 losses,
pretend you saw 1 more than you actually saw.
And so Laplace smoothing,
you're gonna end up adding 1 to the numerator and adding 2 to the denominator.
And so this ends up being 1 over 6, right?
And that's actually a more reasonable may- maybe
it is a more reasonable estimate for the chance of,
uh, them winning or losing the next game.
Um, uh, and the- there's actually
a cert- certain- certain set of circumstances under which there's more estimates.
I didn't just make this up in thin air.
Uh, Laplace, um, uh, you know, uh,
it's an ancient that -- well known,
uh, very influential mathematician.
He actually tried to estimated the chance of the sun rising the next day.
And the reasoning was, well, we've seen the sunrise all times and so, uh,
but tha- that doesn't mean we should be absolutely certain
the sun will still rise tomorrow, right?
And so his reasoning was, well,
we've seen the sunrise 10,000 times, you know,
we can be really certain the sun will rise again tomorrow but maybe not absolutely
certain because maybe something will go wrong
or who- who knows what will happen in this galaxy?
Um , uh, uh, and so his reasoning
was- he derived the optimal estimate- way of estimating,
you know, really the chance the sun will rise tomorrow.
And this is actually an optimal estimate under I'll say- I'll say the same assumptions,
we don't need to worry about it.
But it turns out that if you assume that you are Bayesian,
where the uniform Bayesian prior on the chance of the sun rising tomorrow.
So if the chance the sun rising tomorrow is uniformly distributed,
you know, in the unit interval anywhere from 0 to 1,
then after a set of observations of this coin toss of whether the sun rises,
this is actually a Bayesian optimal estimate of the chances of the sun rising tomorrow, okay?
If you don't understand what I just said in the last 30 seconds, don't worry about it.
Um, uh, it's taught in sort of
a Bayesian statistics- advanced Bayesian statistics classes.
But mechanically, what you should do is, uh,
take this formula and add 1 to the number of
counts you actually saw for each of the possible outcomes.
Um, and more generally,
uh, if y, er, excuse me.
If- if you're estimating probabilities for a k way random variable, um,
then you estimate the chance that X being i to be equal to,
um, so- so that's
the maximum likelihood estimate.
And for the fast-moving,
you'd add one to the numerator and,
um, you add k to the denominator.
Okay? So for Naive Bayes,
the way this mod- modifies your parameter estimates is this.
Um, I'm just gonna copy over the formula from above.
Right?
Um, so that's the maximum likely estimate.
And with Laplace smoothing,
you add one to the numerator and add two to the denominator and this
means that your estimates are probably- these probabilities they're never
exactly 0 or exactly 1,
which takes away that problem of,
you know, the 0 over 0.
Okay. Um, and so if you implement this algorithm,
it's not- it's not like a great spam classifier but it's not terrible either.
And one nice thing about this algorithm is is so simple, right?
Estimated parameters is just counting ,um,
uh, uh can be done, you know,
very efficiently, right, just- just by counting,
uh, and then- and classification time is just multiplying a bunch probabilities together.
Uh, this is very confusing first algorithm. All right.
Any questions about this? Yeah.
[inaudible]?
Oh sorry. This is y. Er, oh yes.
Thank you. Er, yes thank you.
All right. Oh, by the way,
I- I was actually following the Stanford football team that year so,
you know, they lost.
[LAUGHTER].
Because, okay, I love our football team.
They're doing much better right now. That was a few years ago.
[LAUGHTER].
[NOISE].
All right.
Um,
[NOISE]
So, um, in- in the- examples we've talked about so far,
the features were binary valued.
Um, and so, um,
actually one quick generalization, uh,
when the features are multinomial valued,
um, then the generalization- actually here's one example.
We talked about predicting housing prices, right?
That was our very first world meaning example.
Let's say you have a classification problem instead,
which is you're listing a house you want to sell,
what is the chance of this house to be sold within the next 30 days?
So it's a classification problem.
Um, so if one of the features is the size of the house x, right,
then one way to turn the feature into
a discrete feature would be to choose a few buckets,
assert the size is less than 400 square feet, uh, versus,
you know, 400 to 800 or 800 to 1200 or greater than 1200 square feet.
Then you can set the feature XI to one of four values, right?
So that is how you discretize a continuous valued feature to a discrete value feature.
Um, and if you want to apply Naive Bayes to this problem,
then probability of x given y,
this is just the same as before.
Product from i equals 1 through n of p of
xj given Y where now this can be a multinomial probability.
Right? Where if- if X now takes on one of four values there then,
um, this can be a,
uh, estimators and multinomial problem.
So instead of a Bernoulli distribution over two possible outcomes,
this can be a probably, uh,
probability mass function probably over four possible outcomes if you
discretize the size of a house into four values.
Um, and if you ever discretized variables, a
typical rule of thumb in machine learning often we
discretize variables into 10 values, into 10 buckets.
Uh, just as a- it often seems to work well enough.
I- I drew 4 here so I don't have to write all 10 buckets.
But if you ever discretize var- variables,
you know, most people will start off with discretizing things into 10 values.
All right. Now, uh, right.
And so this is how you can apply Naive Bayes on other problems as well including cost line,
for example, if a house is likely to be sold in the next 30 days.
Now, um, there's, uh,
there's a different variation on Naive Bayes that I want to describe to
you that is actually much better for the specific problem of text classification.
Uh and so our feature representation for x so far was the following, right?
With a dictionary a, aardvark, buy,
So let's say you get an email that's,
you know, a very spammy email that's "Drugs, buy drugs now",
[LAUGHTER] This is meant as an illustrative example,
I'm not telling any of you to buy drugs.
[LAUGHTER] Um, so if,
uh, if you have a dictionary of 10,000 words,
then I guess- let's say a is worth 1,
aardvark is worth 2,
uh, just to, you know, make this example concrete.
Let's say the word buy is word 800,
drugs is word 1,600,
and let's say now is the word- is the 6,200th word in your,
uh, 10,000 words in the sorted dictionary.
Um, then the representation for x will be,
you know, 0, 0, [NOISE] right?
And they put a 1 there, and a 1 there, and a 1 there.
Okay? Now, one, one- so, um,
one interesting thing about Naive Bayes is that it throws away
the fact that the word drugs has appeared twice, right?
So that's losing a little bit of information, um, uh, and,
and in this feature representation, um,
you know, each feature is either 0 or 1, right?
And that's part of why it throws away the information that's, uh,
where the one-word drugs appear twice,
and maybe should be given more weight for your- in your classifier.
Um, [NOISE] there's a different representation,
uh, which is specific to text.
And I think text data has a,
has a property that they can be very long or very short.
You can have a five-word email,
or a 1,000-word email, um,
and somehow you're taking very short or very long
emails and just mapping them to a feature vector that's always the same length.
Just a different representation [NOISE] for, um,
this email, which is, uh,
for that email that says,
"Drugs, buy drugs now",
we're gonna represent it as a four-dimensional feature vector, [NOISE] right?
And so this is going to be, um,
n-dimensional for an email of length n.
So rather than a 10,000-dimensional feature vector,
we now have a four-dimensional feature vector,
but now xj is,
um, an index from 1 to 10,000 instead of just being 0 or 1.
Okay? And, uh, n is- and I guess n varies by training example.
So ni is the, uh,
length of email i.
So the longer email,
this vector, the feature vector x will be longer,
and the shorter email,
this feature vector will be shorter, okay?
So, um, let's see.
Uh, just to give names to the algorithms we're gonna develop,
these are- these are really very confusing, very horrible names.
But this is what the community calls them.
That the, the model we've talked about so far is sometimes
called the Multivariate Bernoulli.
And that model, uh,
so Bernoulli means coin tosses,
so multivariate means, you know,
there are 10,000 Bernoulli random variables in
this model whereas as a Multivariate Bernoulli event model.
An event comes with statistics I guess.
Um, and the new representation we're gonna talk about is called
the [NOISE] Multinomial Event Model.
Uh, these two names are- are- are- frankly,
these two names are quite confusing.
But these are the names that, uh, I think- actually,
one of my friends Andrew McCallum, uh, as far as I know,
wrote the paper that named these two algorithms.
But- but I think these are- these are the names we seem to use.
Um, and so, with this new model,
um, we're gonna build a generative model,
and because it's a generative model,
or model p of x,
y which can be factored as follows and using the Naive Bayes assumption,
we're going to assume that p of x given y is product from i equals 1 through n,
of j equals 1 through n,
of p of xj,
given y, and then times, you know, p of y.
Is that second term, right?
Now, one of the, uh, uh, one,
one of the reasons these two models were very-
were frankly actually very confusing to the machine learning community,
is because this is exactly the equation [NOISE] that,
you know, you saw on Monday,
when we described Naive Bayes for the first time, um,
that, you know, this, you know,
p of x given y is part of probabilities.
Right? So this is exactly, uh, so this,
this equation looks cosmetically identical,
but with this new model, the second model,
the confusingly named Multinomial Event Model, um,
the definition of xj and the definition of n is very different, right?
So instead of a product from 1 through 10,000,
there's a product from 1 through the number of words in the email,
and this is now instead a multinomial probability.
Rather than a binary or Bernoulli probability.
Okay? Um, and it turns out that, uh, well,
[NOISE] with this model,
the parameters are same as before.
Phi y is probability of y equals 1,
and also, um, the other parameters of this model, phi k,
given y equals 0,
is a chance of xj equals k,
given y equals 0.
Right? And- and just to make sure you understand the notation.
See if this makes sense.
So this probability is the chance of word
blank being blank if label y equals 0.
So what goes into those two blanks?
Actually, what goes in the second blank?
Uh, let's see.
Well- well, yeah?
[inaudible].
Yes. Right. So it's the chance of the third word in the email,
being the word drugs,
or the chance of the second in the email being buy, or whatever.
And one part of, um,
why we implicitly assume,
mainly why this is tricky,
is that, uh, we assume that this probability doesn't depend on j, right?
That for every position in the email,
for the- the chance that the first word being
drugs is same as chance of the second word being drugs,
is same as the third word being drugs,
which is why, um,
on the left-hand side j doesn't actually appear on the left-hand side, right.
Makes sense? Any questions about this?
No?
Okay. All right.
Um, and so the way you calculate the probability, the way you would,
um, uh, and, and so the way that,
uh, given a new email,
a test email, um, uh,
you would calculate this probability is by, you know,
plugging these parameters that you estimate from the data into this formula.
Okay?
[NOISE]
Um, oh, and then, um, I wrote down, uh, [NOISE] right.
And then, and then the other set of the parameters is this.
[NOISE] Right.
Kind of just with y equals 1,
is that y equals 0.
And then for the maximum likelihood estimate of the parameters,
I'll just write out one of them.
[NOISE] Your estimate of, uh,
the chance of a given word is really anywhere in any position,
being word k. What's the chance of some word in
a non-spam email being the word drugs, let's say?
Um, the chance of that is equal to [NOISE]
I find that- well, this indicates a function notation. It looks complex.
I'll just say in a second,
uh, what this actually means.
So the denominator, um,
so this space means- so- and so if you figure out what
the English meaning of this complicated formula is, this basically says,
"Look at all the words in all of your non-spam emails,
all the emails of y equals 0,
and look at all of the words in all of the emails,
and so all of those words,
what fraction of those words is the word drugs?"
And that's, uh, your estimate of the chance of the word drugs
appearing in the non-spam email in some position in that email, right?
And so, um, in math,
the denominator is sum of your training set, indicator is not spam,
times the number of words in that email.
So the denominator ends up being
the total number of words in all of your non-spam emails in your training set,
um, and the numerator as some of your training set,
sum from i equals 1 through m,
indicates a y equals 0.
So, you know, count up only the things for non-spam email,
and for the non-spam email j equals 1 through ni,
go over the words in that email and see how many words are that word k. Right.
And so, uh, uh, if in your training set you have,
um, uh, ah, you know,
100,000 words in your non-spam emails and 200 of them are the word drugs,
that occurs, uh, you know,
200 times, then this ratio will be 200 over 100,000.
Okay? Oh, and then lastly, um,
[NOISE] to implement Laplace smoothing with this, you would,
um, add 1 to the numerator as usual,
and then, um, let's see.
Actually, what- what- what- what would you add to the denominator? Uh-
Uh, wait. But what is k?
Not k, right? k is a variable.
So k indexes into, ah,
the words? What do you have?
About 10,000.
10,000. Cool. How come? Why 10,000?
[inaudible].
Cool. Yeah. Yeah. All right.
Yeah, Right.
Oh, I think I just realized why you say k I think, uh, overloading notation.
When defining the possibility,
I think I used k as the number of possible outcomes.
Yeah, but here k is an index.
Yeah. Right? So, um, uh,
see I want a numerator and add to number of the
possible outcomes in the denominator which in this case was there 10,000.
So, um, uh, so this is the probability of, um,
X being equal to the value of k,
where k ranges from 1-10,000 if you have a dictionary size.
If you have a list of 10,000 words you're modeling.
And so the number of possible values for X is 10,000,
so you add 10,000 to the denominator.
Makes sense? Cool. Yeah. Question?
[inaudible].
Oh, what do you do if the word's not in your dictionary?
So, um, uh, there are two approaches to that.
One is, um, just throw it away.
Just ignore it, disregard it, that's one.
Uh, second approach, is to take the rare words and map them to
a special token which traditionally is denoted UNK for unknown words.
So, um, if in your training set, uh,
you decide to take just the top 10,000 words in- into your dictionary,
then everything that's not in the top 10,000 words can map to
your unknown word token or the unknown words special symbol. Yeah.
[inaudible].
Oh, why did I write the run before?
Oh, this is an indicator function notation.
Uh, uh, so indicator function uh,
boy- so if- if,
um, and so this is- this notation, right?
Means uh- well, so indicator of, you know,
2 equals 1 plus 1. This is true.
An indicator of, you know,
3 equals 5 is- is 0, is false.
So that's the- yeah, um, cool.
Yes, uh, but this is a- this is a little formula
that's either true or false depending on whether y-i is 0.
Uh, I guess if y-i is 01 this- this is the same as not y-i I guess,
so 1 minus y-i will give us 0- yeah.
Cool. Okay great.
Um, all right.
So I think both of the models, ah, ah,
including the details that maximum likelihood estimate are written out in,
um, more detail in the lecture notes.
Um, so, you know,
when would you use the Naive Bayes algorithm.
It turns out Naive Bayes algorithm is actually not
very competitive with other learning algorithms.
Uh, so for most problems you find that logistic regression,um,
will work better in terms of delivering a higher accuracy than Naive Bayes.
But the- the- the advantages of Naive Bayes is, uh,
first it's computationally very efficient,
and second it's relatively quick to implement, right?
And it also doesn't require an iterative gradient descent thing,
and the number of lines of code needed to implement Naive Bayes is relatively small.
So if you are, uh, facing a problem,
way you go is to implement something quick and dirty,
then Naive Bayes is- is maybe a reasonable choice.
Um, and I think,
um, you know as you work on your class projects,
I think some of you
probably a minority will try to invent a new machine learning algorithm,
and write a research paper.
Um, and I think, you know,
inventing the machine learning algorithm is a great thing to do.
It helps a lot of people on a lot different applications so that's one.
Um, the majority of class projects in CS229 will try
to apply a learning algorithm to a project that you care about.
Apply to a research project you're working on somewhere in
Stanford or apply to a fun application you wanna
build or apply to a business application for some of you
taking this on SCPD, taking this remotely.
And if your goal is not to invent a brand new learning algorithm,
but to take the existing algorithms and apply them,
then rule of thumb that's suggested here is, um, ah,
when you get started on a machine learning project,
start by implementing something quick and dirty.
That's been implemented in most complicated possible learning algorithms.
Start by implementing something quickly,
and, uh, train the algorithm,
look at how it performs,
and then use that to deep out the algorithm,
and keep iterating on- on that.
So I think, you know,
we're- we're- that's at Stanford.
So we're very good at coming up with very complicated algorithms.
But if your goal is to make something,
um, work for an application,
rather than inventing a new learning algorithm and publishing a paper
on a new technical, you know, contribution.
If you- if your main goal is, uh,
you're working on an application on- on
understanding news better or improving the environment or estimating prices or whatever.
Uh, and your primary objective is just make an algorithm work.
Then rather than, uh,
building a very complicated algorithm at the onset,
um, I would recommend implementing something quickly,
uh, so that you can then better understand how it's performing,
and then do error analysis which we'll talk about later,
and use that to drive your development.
Um, you know one- one- one analogy I sometimes make is that,
um, if you are,
uh, uh, let's see.
So if you're writing a new computer program with 10,000 lines of code, right?
One approach is to write all 10,000 lines of code first,
and then to try compiling it for the first time, right.
And that's clearly a bad idea, right?
And it's a, you know, you should write small modules, run it,
it test it- unit testing,
and then build up a program incrementally.
Rather than write 10,000 lines of code,
and then start to see what syntax errors you're getting for the first time.
Um, and I think it's similar for machine learning.
Uh, instead of building a very complicated algorithm from the get-go, um,
you build a simpler algorithm, test it,
and then- and then use the- see what it's doing wrong,
see what it's doing wrong to improve from there.
You often end up, um, uh,
getting to a better performing algorithm faster.
Um, so here's- here's- here's one example.
This is actually something I used to work on.
I- I actually started a conference on email and anti-spam.
My student worked on spam classification many years ago.
And, um, it turns out that when your'e starting out on a new application problem,
um, it's hard to know what's the hardest part of the problem, right.
So if you want to build an anti-spam classifier,
there are lots of you could work on.
For example, spammers will deliberately misspell words.
Uh, you know, a lot of mortgage spam, right,
refinance your mortgage or whatever.
But instead of writing th- the words uh,
mortgage spammers will write M-0-R-T-G-A-G-E.
Right. Or instead of G-A-G-E, maybe,
uh, slash slash, right.
But all of us as people have no trouble reading this as a word mortgage but uh,
this will trip up a spam filter.
This might map the word to- to an unknown word.
There it was off by just a letter and it hasn't seen this before,
and that's the lightest way to slip by this spam filter.
So that's one idea for improving, um,
spam or- actually one of our PhD students [inaudible]
actually wrote a paper mapping this back to words like that.
So the spam filter can see the words the way that humans see them, right.
So- so that's one idea.
Um, another idea might be a lot of spam email spoofs email headers.
[NOISE] You know, uh,
spam has often tried to hide where the email truly came from, uh,
by spoofing the email header that,
you know, address and other information.
Um, ah, an- an- another thing you might do is, ah,
try to fetch the URLs that are referred to in the email,
and then analyze the web pages that you get to.
Right, there are a lot of things that you could do to improve a spam filter.
And any one of these topics could easily be three months or six months of research.
But when you are building say a new spam filter for the first time,
how do you actually know which of these is the best investments of your time.
So my advice to, ah,
those who work on projects,
if your primary goal is to just get this thing to work,
is to not so-somewhat arbitrarily dive in,
and spend six months on improving this or spend,
you know, six months on trying to analyze email headers.
But you instead implement a more basic algorithm.
Almost implement something quick and dirty.
And then look at the examples that your learning algorithm is still misclassifying.
And you'll find that, if after you've implemented a quick and dirty algorithm,
you find that your sp- anti-spam algorithm is
misclassifying a lot of examples with these deliberately misspelled words.
It's only then that you have more evidence that it's worth
spending a bunch of time solving the misspelled words,
the deliberately misspelled words problem.
Right. When you implement a spam filter,
and you see that it's not misclassifying a lot of examples of these misspelled words,
then I would say don't bother.
Go work on something else instead or at least- at least treat that as a low priority.
Okay. So one of the uses of, um,
GDA Gaussian discriminant analysis as well as Naive Bayes is that- is,
uh, they're not going to be the most accurate algorithms.
If you want the highest classification accuracy,
their are other algorithms like logistic regression or SVM
which we talked about, or neural networks we'll talk about later,
which will almost always give you higher classification accuracy than these algorithms.
But the advantage of Gaussian discriminant analysis,
and Naive Bayes is that, um,
they are very quick to train or it's non-iterative.
Uh, uh, this is just counting,
and GDA is just computing means and co-variances, right.
So it's very competition efficient,
and also they are- they are simple to implement.
So it can help you implement that quick and dirty thing that helps you,
um, get going more quickly.
And so I think for your project as well,
I would advise most of you to uh, uh,
you know, as you start working on your project,
I would advise most of you to, um,
don't spend weeks designing exactly what you're going to do.
Uh, if you have an applicant- if- if you- if you- if you have an applied project,
but instead get a data set,
uh, and apply something simple.
Start with logistic regression not- not
a neural network or not- not something more complicated.
Or start with Naive Bayes,
and then see how that performs,
and then- and then go from there.
Okay? All right.
So that's it for,
uh, Naive Bayes, um,
and generative learning algorithms.
The next thing I wanna do is move on to a different cla- type of classifier, ah,
which is a support vector machine.
Um, let me just check any questions about this before I move on. Yeah.
[inaudible].
Sorry you can use logistic regression with.
[OVERLAPPING] Discrete variables [inaudible]
Oh I see yeah right yes so yes, uh, right.
So one of the weaknesses of
the Naive Bayes Algorithm is that it treats all of the words as completely,
you know, separate from each other.
And so the words one and two are quite similar and the words,
you know, like mother and father are quite similar.
Uh, and so wi- wi-with this, uh,
feature representation, it doesn't know the relationship between these words.
So, um, in machine learning there are other ways of representing words,
uh, there's a technique called word embeddings,
um-[NOISE] In which you
choose the feature representation that encodes
the fact that the words one and two are quite similar to each other.
Uh, the words mother and father are quite similar to each other.
Yeah the words, um,
whatever London and Tokyo are
quite similar to each other because they are both city names.
Uh, and so, uh,
this is a technique that I was not planning to teach here but that is taught in CS 230.
So in- in- in neural networks [NOISE] , right,
but you can also read up on word embeddings or look at some of
the videos and resources from CS 230 if you want to learn about that.
Uh, so the word embeddings techniques.
These are techniques from neural networks really.
Will reduce the number of training examples you need so they are
a good text classifier because it comes in with
more knowledge baked in, right. Cool. Anything else?
[NOISE] Cool.
By the way I do this in the other classes too.
In some of the other classes,
somebody's got a question they go,
no we don't do that we just covered that in
CS 229 so
[LAUGHTER].
Actually CS224N I think also covers this.
Yeah, The NLP class, yeah, pretty sure, actually I am sure they do.
Okay so,
[NOISE] su-support vector machines, SVMs.
Um, let's say the classification problem,
[NOISE].
Right, where the data set looks like this, uh,
and so you want an algorithm to find, you know,
like a nonlinear decision boundary, right?
So the support vector machine will be an algorithm to help us
find potentially very very non-linear decision boundaries like this.
Now one way to build a classifier like this would be to use logistic regression.
But if this is X 1, this is X 2, right,
so logistic regression will fit the three lines of data,
Gaussian discriminant analysis will end up with a straight line decision boundary.
So one way to apply logistic regression like this would be to take
your feature vector X 1 X 2 and map it to a high dimensional feature vector with,
you know, X 1, X 2, X 1 squared,
X 2 squared X 1, X 2 maybe X 1 cubed,
X 2 cubed and so on.
And have a new feature vector which we would call phi of
x. That- that has these high-dimensional features right, now, um,
it turns out if you do this and then apply
logistic regression to this augmented feature vector, uh,
then logistic regression can learn non-linear decision boundaries.
Uh, with these other features it's just regression
and you actually learn the decision boundary.
This is- there's a- there's a shape of an ellipse, right.
Um, but randomly choosing
these features is little bit of a pain right. I- I- I don't know.
What I- I- I actually don't know what,
you know, type of a,
uh, set of features could get you a decision boundary like that right.
Rather than just an ellipse and more complex as your boundary.
Um, and what we will see with support vector machines is that we
will be able to derive an algorithm that can take say input features X 1, X 2,
map them to a much higher dimensional set of features.
Uh, and then apply a linear classifier,
uh, in a way similar to logistic regression.
But different in details that allows you to learn very non-linear decision boundaries.
Okay. Um, and I think,
uh, you know, a support vector machine,
one of the- actually one of the reasons, uh,
support vector machines are used today is- is a relatively turn-key algorithm.
And what I mean by that is it doesn't have too many parameters to fiddle with.
Uh, even for logistic regression or for linear regression.
You know you might have to tune the gradient descent parameter,
uh, tune the learning rate sorry, tune the learning rate alpha.
And that's just another thing to fit in with.
We`ll try a few values and hope you didn't mess up how you set that value.
Um, support vector machine today has a very, uh,
robust, very mature software packages that you
can just download to train the support vector machine on- on any on,
you know, on a problem and you just run it and the algorithm will,
kind of, converge without you having to worry too much about the details.
Um, so I think in the grand scheme of things today I would say
support vector machines are not as effective as neural networks for many problems.
But, um, uh, but one great property of support vector machines is- is- is turn key.
You kind of just turn the key and it works and there isn't as
many parameters like the learning rate and other things that you had to fiddle with.
Okay, um so the road map is,
uh, we're going to develop the following set of ideas.
We talked about the optimal [NOISE] margin classifier today, and, uh,
we'll start with the separable case
and what that means is going to start off with datasets,
um, that we assume look like this and that are linearly separable.
Right, and so the optimal margin classifier is
the basic building block for the support vector machine,
and, uh, we'll first derive an algorithm, uh,
that' ll be- that will have some similarities to
logistic regression but that allows us to scale, uh,
in important ways that to find a linear classifier
for training sets like this that we assume for now can be linearly separated.
Um, so we'll do that today.
And then what you'll see on Wednesday is, um,
excuse me, next Monday,
which is next Monday is an idea called kernels.
And the kernel idea is one of the most powerful ideas in machine learning.
Is, um, how do you take a feature vector x, maybe this is R 2,
right, and map it to a much higher dimensional set of features.
In our example there that was R 5,
right, and then train an algorithm on this high dimensional set of features.
And- and the cool thing about kernels is that
this high dimensional set of features may not be R 5.
It might be R100,000 or it might even be R infinite.
Um, and so with the kernel formulation we're gonna take
our original set of features that you are given for the houses you're trying to sell.
For, uh, you know,
medical conditions you're trying to predict and map
this two-dimensional feature vector space
into maybe infinite dimensional set of features.
And, um, what this does is it relieves us
from a lot of the burden of manually picking features,
right, like do you want to have square root of X 1
or maybe X 1, X 2 to the power of two thirds.
So you just don't have to fiddle with these features too much
because the kernels will allow you to choose an infinitely large set of features.
Okay, um, and then finally,
uh, we'll talk about the inseparable case.
[NOISE] So we're gonna do this today and then
this next, uh, Monday okay.
So [NOISE] and by the way I,
you know, th-the machine learning world has become a little bit funny.
I think that if you read in the news
the media talks a lot about machine learning, the media just talks about,
you know, neural networks all the time, right?
And you'll hear about neural networks and deep learning a little bit later in this class.
But if you look at what actually happens in practice in machine learning.
Uh, the set of algorithms is actually used in practice,
is actually much wider than neural networks and deep learning.
So- so we do not live in a neural networks only world.
We actually use many, many tools in machine learning.
It's just that deep learning attracts the attention of the media in
some way there's quite disproportionate to what I find useful, you know,
I knew that's like- I loved that,
you know but- but they're not- they're not the only thing in the world,
uh, and so yeah and then late last night I was talking to an engineer, uh,
about factor analysis which we'll learn about later in CS229
right, there's an unsupervised learning algorithm and there's an application,
uh, that one of my teams is working on in manufacturing.
Where we're gonna use factor analysis or something very similar to it.
Which- which is totally not a neural network technique.
Right. But still there, there are all these other techniques that including
support vector machines and Naive Bayes I think do get used and are important.
All right so let's start developing the optimal margin classifier.
[NOISE]
So, um, first, let me define the functional margin,
which is, uh, informally,
the functional margin of the classifier is how well- how,
how confidently and accurately do you classify an example.
Um, so here's what I mean.
Uh, we're gonna go to binary classification,
and we're gonna use logistic regression, right?
So, so let's, let's start by motivating this with logistic regression [NOISE].
So this, this is a classifier H of theta
equals the logistic function of pi to theta transpose x.
And so, um, if you turn this into a binary classification, if, if,
if you have this algorithm predict not a probability but predict 0 or 1,
then what this classifier will do is, uh, predict 1.
If theta transpose x is greater than 0, right?
Um, and predict 0 otherwise.
Okay. Because theta transpose x greater than 0, this means that,
um, g of theta transpose x is greater than 0.5 [NOISE],
and you can have greater than or greater than equal to, it doesn't matter.
It is, it's exactly 0.5,
it doesn't really matter what you do.
Um, and so you predict 1 if theta transpose x is greater than equal to 0,
meaning that the upper probability- the estimated probability of
a class being 1 is greater than 50/50, and so you predict 1.
And if theta transpose x is less than 0,
then you predict that this class is 0.
Okay. So this is what will happen if you have, um,
logistic regression output 1 or 0 rather than output a probability, right.
So in other words,
this means that if y_i is equal to 1, right?
Then hope or we want that
theta transpose x_i is much greater than 0.
Uh, this double greater than sign,
it means much greater, right?
Um, uh, because if the true label is 1,
then if the algorithm is doing well,
hopefully theta transpose x, right?
Will be faster there, right?
So the output probability is very,
very close to 1.
And if indeed theta transpose x is much greater than 0,
then g of theta transpose x will be very close to 1 which means that is,
it's giving a very good, very accurate prediction.
Very correct and confident prediction, right?
That, that equals 1.
Um, and if y_i is equal to 0,
then what we want or what we hope,
is that theta transpose xi is much less than 0, right?
Because, uh, if this is true,
then the algorithm is doing very well on this example.
Okay.
So, um.
So the functional margin which we'll define in a second,
uh, captures [NOISE] this idea that if a classifier has a large functional margin,
it means that these two statements are true, right?
Um, so looking ahead a little bit,
there's a different thing we'll define in a second
which is called the geometric margin and that's the following.
And for now, let's assume the data is,
is linearly separable.
Okay. Um, right.
So let's say that's the data set.
[NOISE]
Now,
[NOISE]
that seems like a pretty good decision boundary
for separating the positive [NOISE] and negative examples.
[NOISE] Um, that's another decision boundary in red,
that also separates the positive negative examples.
But somehow the green line looks much better than the red line, okay?
So, uh, why is that?
Well, the red line comes really close to a few of the training examples,
whereas the green line,
you know, has a much bigger separation, right?
Just has a much bigger distance from the positive and negative examples.
So even though the red line and the [NOISE] green line both, you know,
perfectly separate the positive and negative examples,
the green line has a much bigger separation,
uh, which is called the geometric margin.
But there's a much bigger geometric margin meaning a physical separation
from the trained examples even as it separates them.
Okay. Um, and so what I'd like to do in the,
uh, next several, I guess in the next, next,
next 20 minutes is formalize definite functional margin,
formalize definition geometric margin,
and it will pose the, the,
I guess the optimal margin classifier which based in
the algorithm that tries to maximize the geometric margin.
So what the rudimentary SVM does,
what the SVM and low-dimensional spaces will do,
also called the optimal margin classifier,
is pose an optimization problem to try to find
the green line to classify these examples, okay?
So, um, [NOISE] now,
um, in order to develop SVMs,
I'm going to change the notation a little bit again.
You know, because these algorithms have different properties, um,
using slightly different notation to describe them,
makes then the math a little bit easier.
So when developing SVMs,
we're going to use, um,
minus 1 and plus 1 to denote the class labels.
And, um, we're going to have a H output.
So rather than having a hypothesis output
a probability like you saw in logistic regression,
the support vector machine will output either minus 1 or plus 1.
And so, uh, g of z becomes minus 1 or 1, um, actually.
So output 1 if z is greater than equal to 0,
and minus 1 otherwise, okay.
So instead of a smooth transition from 0 to 1,
we have a hard transition,
an abrupt transition from negative 1 to, um, plus 1.
[NOISE]
And finally, where previously we had for logistic regression, right?
Where, uh, this was R N plus 1 with x_0 equals 1.
For the SVM, we will have h of, I'll just write this out.
Okay. Um, so for the SVM,
the parameters of the SVM will be the parameters w and b.
And hypothesis applied to x will be g of this,
and where dropping the x_0 equals 1 [NOISE] constraint.
So separate out w and b as follows.
So this is a standard notation used to develop support vector machines.
Um, and one way to think about this,
is if the parameters are,
you know, theta 0, theta 1,
theta 2, theta 3,
then this is a new b,
and this is a new w. Okay?
So you just separate out the, the, the, uh,
theta 0 which was previously multiplying to x_o, right?
And so um, uh, yeah, right.
And so this term here becomes sum from i equals 1 through N,
uh, w_i x_i plus b, right?
Since we've gotten rid of [NOISE] x_0.
[NOISE].
All right. So let me formalize the definition of a functional margin.
[NOISE] So um, ah,
so the parameters w and b are defined as linear classifier,
right, so you know,
wh- what- the formulas we just wrote down the parameters w and b,
defines the a- a, uh, uh,
re- really defines a hyperplane.
Ah, but defines a line,
or in high dimensions it'd be a plane or a hyperplane that defines a straight line,
ah, ah, separating out the positive and negative examples.
And so we're gonna say the functional margin of the,
actually my hyperplane [NOISE]
Okay, so the functional margin of
a hyperplane defined by this with respect to one training example.
We're going to write as this,
um, and hyperplane just means straight line,
right, but in high dimension.
So this is linear classifiers, so its just, you know,
functional margin of this classifier with respect to one training example,
we're going to define as this.
And so if you compare this with the equations we had up there,
um, you know, if y equals 1 we hope for that, if y equals 0, we hope for that.
So really what we hope for is for
our classifier to achieve a large functional margin, right?
And so, um, so if y_i equals 1 then what we want or what we hope for,
um, is that w transpose x_i plus b is greater than,
much greater than 0,
and that the label is equal to minus 1.
[NOISE] Then we want or we hope that [NOISE] this is much smaller than 0.
Um, and if you, kind of,
combine these two statements,
if you take y_i,
right, and multiply it with,
er, that, [NOISE] then, you know,
these two statements together is basically saying that you hope that
Gamma hat i is much greater than 0, right,
because y_i now is plus 1 or minus 1 and,
uh, uh, and so y is equal to 1 you want this to be very, very large.
If y_i is negative 1,
you want this to be a very,
very large negative number.
Um, and so either way it's just saying that you hope this would be very large, okay?
So we just hope that.
[NOISE] And- and as an aside,
ah, one property of this as well is that, um,
so long as Gamma hat i is greater than 0,
that means the algorithm,
um, right, is equal to y_i.
[NOISE] Ah, so- so- so long as the, um, functional margin,
so long as this Gamma hat i is greater than 0, it means that,
ah, either this is bigger than 0,
this is less than 0 depending on the sign of the label.
And it means that the algorithm gets
this one example correct at least, right?
And if- if much greater than 0 then it means, you know,
so if it is greater than 0 it means in- in the logistic regression case it means that,
the prediction is at least a little bit above 0.5,
a little bit below 0.5, probably 0 so that at least gets it, right?
And if it is much greater than 0 much less than 0,
then that means it, you know,
the probability of output in
the logistic regression case is either very close to 1 or very close to 0.
So one other definition,
[NOISE]
I'm gonna define
the functional margin with respect to the training set to be Gamma hat,
equals min over i of Gamma hat i,
where here i [NOISE] equals ranges over your training examples.
Okay. So, um, this is a worst-case notion,
but so this definition of a function margin,
on the left we defined functional margins with respect to a single training example,
which is how well are you doing on that one training example?
And we'll define the function margin with respect to the entire training set as,
how well are you doing on the worst example in your training set?
Okay, ah, this is a little bit of a plateau notion and we're for now,
for today, we're assuming that the training set is linearly separable.
So we're gonna assume that the training set,
you know, it looks like this.
[NOISE] And that you can separate
it on a straight line [NOISE] that will relax this later,
but because we're assuming, just for today,
that the training set is, um, linearly separable,
we'll use this kind of worst-case notion and define
the functional margin to be the functional margin of the worst training example.
Okay? [NOISE]. Now, one thing
about the definition of the functional margin is, it's actually
really easy to cheat and increase the functional margin, right?
And one thing you can do, um,
in regards to this formula is if you take w,
you know, and multiply it by 2 and take b and multiply it by 2.
[NOISE] then, um, everything
here just multiplies by two and you've doubled the functional margin,
right, but you haven't actually changed anything meaningful.
Okay, so- so one,
one way to cheat on the functional margin is just by scaling
the parameters by 2 or instead of 2 maybe you
can [NOISE] multiply all your parameters by 10 and then you've actually
increased the functional margin of your training examples as 10x,
but, ah, this doesn't actually change the decision boundary, right?
It doesn't actually change any classification,
just to multiply all of your parameters by a factor of 10.
Um, so one thing you could do is, ah,
replace, one thing you could do,
um, would be to normalize the length for your parameters.
So for example, hypothetically you could impose a constraint,
the normal w is equal to 1,
or another way to do that would be to take w and b and replace it with w
over normal b and replace b with, [NOISE] right,
just the value of parameters through by the magnitude,
by the- by the Euclidean length of the parameter vector w,
and this doesn't change any classification,
It's just rescaling the parameters.
Ah, but, ah, but,
but that it prevents, you know,
display of cheating on the functional margin.
Okay. Um, and in fact,
more generally you could actually scale w and b by
any other values you want and- and it doesn't- doesn't matter, right?
You can choose to replace this by w over 17 and b over 17 or any other number or any,
right, and the classification stays the same.
Okay. So we'll come back and use this property,
in a little bit. Okay. [NOISE]
All right. So to find the functional margin,
let's define the geometric margin.
An- and you'll see in a second how
the geometric and functional margin relate to each other.
Um, so les- let's,
let's define the, uh,
geometric margin with respect to a single example.
Which is, um- so let's see- let's say you have a classifier.
All right, so given parameters w and b that
defines a linear classifier and the equation,
wx plus b equals 0 defines the equation of a straight line.
Uh, so the axes here are x_1 and x_2,
and then half of this plane you know,
in this half of the plane,
you'll have w transpose x plus b is greater than 0.
And in this half, you'll have w transpose x plus b is less than 0.
And in between this- the straight line given by
this equation w transpose x plus b equals 0, right.
And so given parameters w and b, the upper right,
that's where your cost high will predict y equals 1 and the lower left
is where it'll predict y is equal to negative 1, okay.
Now, let's say you have one training example here, right?
So that's a training example, x_i, y_i.
And, uh, let's say it's a positive example, okay?
And so, um, your classifiers classify this example correctly, right?
Because in the upper right half- half plane-
here in this half plane w transpose x plus b is greater than 0.
And so in the- in this upper-right region, uh,
your classifier is predicting plus 1, right?
Whereas in this lower half region would be predicting h of x equals negative 1.
Right, and that's why this straight line where it switches
from predicting negative to positive is the decision boundary.
So what we're going to do is define this distance, um,
to be that geometric margin of this training example,
is the Euc- the Euclidean distance is what will define to be the geometric margin.
So let me just write down what that is.
[NOISE]
So the geometric margin of,
you know, the classifier of the hyperplane defined by w,
b with respect to one example x_i, y_i.
This is going to be gamma i equals
w transpose x plus b over [NOISE] the normal w. [NOISE]
Um, and let's see I'm not proving why this is the case,
the proof is given in the lecture notes but, uh,
the lecture notes shows why this is the right formula for
measuring the Euclidean distance that I just drew [NOISE] in the picture up there, okay.
Uh, but, and then, I'm not proving this
here but the proof is given in the lecture notes but this turns out
to be the way you compute the Euclidean distance between that example and uh,
and the decision boundary, okay?
Um, and uh, a- and this is [NOISE] for the positive example I guess.
Uh, more generally, um,
I'm going to define the geometric margin to be equal to this,
uh, and this definition applies to positive examples and the negative examples, okay?
And so the relationship between the geometric margin and the functional margin is
that the geometric margin is equal to the functional margin divided by
the norm of w. [NOISE]
Finally, um, the geometric margin with respect to the training set is, um,
where again uses worst-case notion of,
uh- look through all your training examples and pick the worst possible training example,
um, and that is your geometric margin on the training set.
Uh, an- and so I hope the- sorry,
I hope the notation is clear, right.
So gamma hat was the functional margin and
gamma is the geometric margin,
okay? And so, um,
what the optimal margin classifier
does is [NOISE] ,
um, choose the parameters w and
b to maximize the geometric margin, okay?
Um, so in other words,
thi- this- the optimal margin classifiers is the baby SVM, you know, it's like,
a SVM for linearly separable data, uh, at least for today.
[NOISE] And so the optimal margin classifier will choose that straight line,
because that straight line maximizes the distance or maximizes the geometric margin
to all of these examples, okay?
Now, uh, how you pose this mathematically,
there are a few steps of this derivations I don't want to do but I'll,
I'll just describe the beginning step and
the last step and leave the in bet- in between steps to the lecture notes.
But it turns out that, um,
one way to pose this problem is to maximize gamma w and b of gamma.
So you want to maximize the geometric margin subject to that.
Subject to the every training example, um, uh,
must have geometric margin,
uh, uh, greater than or equal to gamma, right?
So you want gamma to be as big as possible subject to
that every single training example must have at least that geometric margin.
[NOISE] This causes you to maximize the worst-case geometric margin.
And it turns out this is,
um- not in this form,
this is in a convex optimization problems.
So it's difficult to solve this without
a gradient descent and initially known local optima and so on.
But it turns out that via a few steps of V writing,
you can reformulate this problem as, um,
into the equivalent problem which is a minimizing norm of w subject
to the geometric margin, right.
Um, so it turns out- so I hope this problem makes sense, right?
So this problem is just you know solve for w and b to make sure that
every example has a geometric margin greater or
equal to gamma and you want gamma to be as big as possible.
So this is the way to formulate optimization problem that says,
''Maximize the geometric margin.''
And what we show in the lecture notes is that,
uh, through a few steps, uh,
you can rewrite this optimization problem into the following equivalent form
which is to try to minimize the norm of w, uh, subject to this.
And maybe one piece of intuition to take away is,
um, uh, the smaller w is the bigger, right?
Th- th- the, the less of a normalization division effect you have, right?
Uh, but the details I gave you in the lecture notes, okay?
Um, but this turns out to be a convex optimization problem and if you optimize this,
then you will have the optimal margin classifier and they're
very good numerical optimization packages to solve this optimization problem.
And if you give this a dataset then, you know,
assuming your data's separable [NOISE] and we'll fix that assumption,
uh, when we convene next week,
then you have the optimal management classifier
which is really a baby SVM and we add kernels to it,
then you have the full complexity of the SVM norm, okay?
All right, let's break for today,
uh, see, see you guys next Monday.
 All right. Good morning.
Um, let's get started.
So, ah, today you'll see the Support Vector Machine Algorithm.
Um, and this is one of my favorite algorithms because it's very turnkey, right?
If you have a classification problem, um,
you just, kind of, run it and it more or less works.
So in particular, I'll talk a bit more about
the optimization problem that you have to solve for the support vector machine,
then talk about something called the representer theorem,
and this will be a key idea to how we'll work in potentially very high-dimensional,
like 100,000 dimensional,
or a million dimensional,
or 100 billion dimensional,
or even infinite-dimensional feature spaces.
And just to teach you how to represent
feature vectors and how to represent parameters that may be,
you know, 100 billion dimensional,
or 100 trillion dimensional, or infinite dimensional.
Um, and based on this we derived kernels which is
the mechanism for work on these incredibly high dimensional fea- feature spaces,
and then hopefully, time permitting wrap up with
a few examples of concrete implementations of these ideas.
So to recap, on last Wednesday,
we had started to talk about the optimal margin classifier, which said that,
if you have a dataset that looks like this,
then you want to find
the decision boundary with the greatest possible geometric margin, right?
So the geometric margin, um,
can be calculated by this formula,
and this is just the- the- the derivations in the lecture notes.
It's just, you know, measuring the distance,
uh, to the nearest point, right?
Um, and for now let's assume the data can be separated by a straight line.
Um, and so Gamma i is- this is sort of geometry,
I guess, derivation in the lecture notes.
This is the formula for co- computing the distance from the example x_i, y_i,
to the decision boundary governed by the parameters w and b.
Um, and Gamma is the worst case geometric margin, right?
You will make- so- right.
Of all of your M training examples,
which one has the least or has the worst possible geometric margin?
And, the support vector, the optimal margin classifier,
we tried to make this as big as possible.
And by the way, what we'll- what you see later on
is that the optimal margin classifier is basically this algorithm.
And optimal margin classifier plus kernels meaning basically take
this idea of pi in a 100 billion dimensional feature space
that's a support vector machine, okay?
So I saw- one thing I didn't have time to talk about, uh,
on Wednesday was the derivation of
this classification problem, so where does this optimization objective come from?
So let me- let me just go over that very briefly.
Um, so, the way I motivated these definitions we said that given a training set,
you want to find the decision boundary parameterized by w and b,
um, that maximizes the geometric margin, right?
And so again, as recap, your classifier will
output g equals w transpose x plus b.
Um, and so you want to find premises w and b.
They'll define the decision boundary where
your classifications switch from positive to negative,
that maximizes the geometric module.
And so one way to pose this as an optimization problem is- um, let's see,
is to try to find the biggest possible value of Gamma
subject to that- subject to that the,
um, geometric margin must be greater than or equal to Gamma, right?
So, um, so, in this optimization problem,
the parameters you get to fiddle with are,
Gamma, w and b.
And if you solve this optimization problem,
then you are finding the values of w and b that defines a straight line,
that defines a decision boundary, um,
so that- so, so this constraint says that every example, right?
So this constraint says every example has geometric margin greater than or equal to Gamma.
This is- this is what they are saying.
And you wanna set Gamma as big as possible,
which means that you're maximizing the worst-case geometric margin.
This makes sense, right?
So- so if- if I- so the only way to make Gamma say 17,
or 20, or whatever,
is if every training example has geometric margin bigger than 17, right?
And so this optimization problem was trying to find w and b to drive up
Gamma as big as possible and have every example
have geometric margin even bigger than Gamma.
So this optimization problem maximizes the Geom- causes, um,
causes you to find w and b with as big a geometric margin as
poss- so as big as the worst-case geometric margin as possible, okay?
Um, and so, does this make sense actually, right?
Okay. Actually rai- raise your hand if this makes sense.
Uh, oh, good.
Okay. Well, many of you. All right.
Let me see if I can explain this in a slightly different way.
So let's say you have a few training examples, you know,
the training examples geometric margins are,
17, 2, and 5, right?
Then the geometric margin in this case is a worst-case value 2, right?
And so if you are solving an optimization problem
where I want every example- where I want the- the- the, uh, uh,
where I want the min of i- of Gamma i to be as big as possible,
one way to enforce this is to say that Gamma i must be bigger than or equal to Gamma,
for every possible value of i.
And then I'm going to lift Gamma up as much as possible, right?
Because the only way to lift Gamma up
subject to this is if every va- value of Gamma i is bigger than that.
And so, lifting Gamma up,
maximizing Gamma has effective maximizing the worst-case examples geometric margin,
which is, which is, which is how we define this optimization problem, okay?
Um, and then the last one step to turn this problem into this one on the left,
is this interesting observation that, um,
you might remember when we talked about the functional margin,
which is the numerator here, that,
you know, the functional margin you can scale w and b
by any number and the decision boundary stays the same, right?
And so, you know,
if- if your classifier is y,
so this is g of w transpose x plus b, right?
So if- let's see the example I want to use, uh, 2, 1.
If w was the vector 2, 1- [NOISE]
Let's say that's the classifier, right?
Then you can take W and B,
and multiply it by any number you want.
I can multiply this by 10,
[NOISE] and this defines the same straight line, right?
Um, so in particular, I think,
uh, let's see with this 2 1x.
[NOISE] This actually defines the decision boundary that looks like that.
Uh, if this is X1 and this is X2,
then this is the equation of the straight line where W transpose X plus
B equals 0, right?
Uh, that's uh, one, and two.
Uh, you can- you can verify it for yourself.
You plug in this point,
then W transpose X plus B equals 0.
We plug in this point, W transpose X equals 0,
um and so that's the decision boundary where the,
uh- as yet we'll predict positive [NOISE]
everywhere here and we'll predict [NOISE] negative everywhere to the lower left,
and this straight line, you know,
stays the same even when you multiply these parameters by any constant, okay?
Um, and so, um,
to simplify this, uh,
notice that you could choose anything you want for the normal W, right?
Just by scaling this by a factor of 10,
you can increase it, or scaling it by a factor of 1 over 10, you can decrease it.
But you have the flexibility to scale the parameters W and B, you know,
up or down by any fixed constant without changing the decision boundary,
and so the trick to simplify this equation into that one is if you choose
[NOISE] to scale the normal W to be equal to 1 over gamma.
Um, uh because if you do that,
then this optimization objective
[NOISE] becomes- [NOISE] Um,
maximize 1 over norm of W subject to-
[NOISE]
right?
Uh, so it substitutes norm of W equals 1 of gamma,
and so that cancels out,
and so you end up with this optimization problem instead of maximizing 1 over norm W,
you can minimize one half the norm of W squared subject to this.
[NOISE] Right?
Okay, and so that's a rough- I know I did this relatively quickly.
Again- as usual the full derivation is written on
your lecture notes but hopefully this gives you a flavor for why.
If you solve this optimization problem and you're minimizing over W and B
that you are solving for the parameters W and B that
give you the optimal margin classifier. Okay.
Now, delta margin classifier,
we've been deriving this algorithm as if you know the features X I um,
let's see, we've been deriving this algorithm as if the features X I are
some reasonable dimensional feature X equals R2,
X equals 100 or something.
Um, what we will talk about
later is a case where the features X I become you know, 100 trillion dimensional right?
Or infinite dimensional.
And um, what's- uh,
what we will assume is that W,
can be represented [NOISE] as a sum- as a linear combination of the training examples.
Okay? So um, in order to derive the support vector machine,
we're gonna make an additional restriction that the parameters W
can be expressed as a linear combination of the training examples.
Right? So um, and it turns out that when X I is you know, 100 trillion dimensional,
doing this will let us derive algorithms that work
even in these 100 trillion or these infinite-dimensional feature spaces.
Now, I'm just deriving this uh,
just as an assumption.
It turns out that there's a theorem called the representer theorem that
shows that you can make this assumption without losing any performance.
Uh, the proof that represents the theorem is quite complicated.
I don't wanna do this in this class, uh,
it is actually written out, the proof for why you can
make this assumption is also written in the lecture notes,
it's a pretty long and involved proof involving primal dual optimization.
Um, I don't wanna present the whole proof here but let me give you
a flavor for why this is a reasonable assumption to make.
Okay? And when- just to- just to make things complicated later on uh,
we actually do this.
Right? So Y I is always plus minus 1.
So- so we're actually by- by convention,
we're actually going to assume that W I can be written right?
So in- in this example this is plus minus 1 right?
So um, this makes some of the math
a little bit downstream, come out easier but it is- but it's still
saying that W is- can be represented as a linear combination of the training examples.
Okay? So um [NOISE] let me just
describe less formally why this is a reasonable assumption,
but it's actually not an assumption.
The representer theorem proves that you know,
this is just true at the optimal value of W. But let me convey a couple ways why um,
this is a reasonable thing to do, or assume I guess.
So um, maybe here's intuition number one.
And I'm going to refer to logistic regression.
[NOISE] Right? Where uh,
suppose that you run logistic regression with uh, gradient descent,
say stochastic gradient descent, then you initialize the parameters to be equal to 0 at first.
And then for each iteration of stochastic gradient descent,
right [NOISE] you update theta it gets updated as theta minus
the learning rate times [NOISE] you know,
[NOISE] times X and Y okay?
And so- sorry here alpha is the learning rate, uh,
nothing, this is overloaded notation,
this alpha has nothing to do with that alpha.
But so this is saying that on every iteration,
you're updating the parameters theta as- uh,
by- by adding or subtracting some constant times some training example.
And so kind of proof by induction,
right if theta starts out at 0,
and if- if on every iteration of
gradient descent you're adding a multiple of some training example,
then no matter how many iterations you run gradient descent,
theta is still a linear combination of your training examples.
Okay. And- and again I did this with theta- the- the- it was really
theta 0 theta 1 up to theta n. Right?
Whereas here we have uh,
B and then W1 down to WN.
Wow, this pen is really bad.
[NOISE] I feel like- alright um,
I feel like we should throw these away so they don't keep haunting us in the future.
Okay. Right, so but- but um,
if you- but uh, uh,
so I did this a theta rather than W, but it turns out if you work through
the algebra this is the proof by induction that, you know,
as you run a logistic regression after every iteration the parameters theta or
the parameters W are always a linear combination of the training examples.
Um, and this is also true if you use batch gradient descent.
[NOISE] If you use batch gradient descent [NOISE] then the update rule is this.
Um, Yeah, right, [NOISE] okay, alright.
And so it turns out you can derive
gradient descent for the support vector machine learning algorithm as well.
You can derive gradient descent optimized W subject to
this and you can have a proof by induction.
You know that no matter how many iterations you run during descent,
it will always be a linear combination of the training examples.
So that's one intuition for how
[NOISE] you might see that assuming W is a linear combination of the training examples,
you know is a- is a reasonable assumption.
[NOISE] I wanna present a second set of
intuitions and this one will be easier if you're
good at visualizing high dimensional spaces I guess.
But uh, let me just give intuition number two which is um let's see.
So um, so first of all let's take our example just now right?
Let's say that the classifier uses this,
2, 1 [NOISE] X minus 2, right?
So this is W and this is B.
Then it turns out that the decision boundary is this where this is 1 and this is uh,
2 and it turns out that
the vector W is always at 90 degrees to the decision boundary right?
This is a factor of I guess geometry or something or linear algebra, right?
Where as the vector W 2, 1.
So the vector W, you know,
is sort of 2 to the right side and 1 up is always at- well, alright.
The vector w is always at 90 degrees um,
to the decision boundary and the decision boundary separates where
you predict positive from where you predict negative.
Okay? And so it it turns out that uh,
if you have uh,
to take a simple example,
let's say you have um,
two training examples, a positive example and a negative example.
Right? Then by illus- X2 right?
The linear algebra way of saying this is that
the vector W lies in the span of the training examples.
Okay? Oh and- and- and um,
the way to picture this is that W sets the direction of
the decision boundary and as you vary B then the position so you- the relative position,
you know setting different values of B
will move that decision boundary back and forth like this.
And W uh, pins the direction of the decision boundary.
Okay? Um, and just one last example for- for why this might be true um,
is uh- so we're going to be working in very very high dimensional feature spaces.
For this example, let's say you have uh,
[NOISE] three features X1, X2, X3 right?
And_ and later we'll get to where this is like 100 trillion right?
Um, and let's say for the sake of illustration
that all of your examples lie in the plane of X1 and X2.
So let's say X3 is equal to 0.
Okay, so let's say if all your training examples x equals 0, um,
then the decision boundary,
you know, will be- will be some sort of vertical plane that looks like this, right?
So this is going to be the plane specifying,
um, w transpose x plus b equals 0 when now w and x are three-dimensional.
Um, and so the vector w, uh,
will have a- should have W_3 equals 0 right.
If- if one of the features is always 0,
is always fixed then you know,
W_3 should be equal to 0 and that's another way of saying that the vector w,
you know, should be, um,
represented as a- as a- in- in the span of just the features x1,
x2, as a span of the training examples [NOISE] okay.
All right, I'm not sure if- if either intuition 1 or intuition 2 convinces you,
I think hopefully that's good enough.
But this- the second intuition would be easier if you're used to
thinking about vectors in high-dimensional feature spaces.
Um, and again the formal proof of this result which is called
the representation theorem is given in the lecture notes, but it's a very bizarre
I don't know, it's actually- it's actually one of the most complicated- it's one- it's
definitely the high end in terms of complexity of the- of the full derivation,
of the formal derivation of this result.
Um, so.
[NOISE] All right,
so let's assume that W can be written as follows.
Um, so optimization problem was this,
you wanna solve for w and b so that the norm of w squared is as small as
possible and so that the a-this is bigger than the other one, right?
Um, for every value of i.
[NOISE] So let's see,
norm of w squared.
This is just equal to w transpose w,
um, and so if you plug in this definition of W,
you know, into these equations you have as
the optimization objective min of one half, um,
sum from i equals 1 through m. [NOISE]
So this is w transpose W,
um, which is equal to I guess sum of i's sum over j,
alpha i, alpha j, y_i y_j.
And then, um, X_i transpose X_j right?
And, um, I'm going to take this.
So this is an inner product between X_I and X_J.
And I'm gonna use- I'm just gonna write it as this.
Right, x_i this notation so x comma z, uh,
equals x transpose z, uh,
is the inner product between two vectors.
This is maybe another alternative notation for writing
inner products and when we derive kernels you see that, uh,
expressing your algorithm in terms of inner products between features
X is-is the key mathematical step needed to derive kernels and we'll
use this slightly different sort of open-angle brackets
close-angle brackets notation to denote
the-the inner product between two different feature vectors.
So that is the optimization objective,
um, oh, and then this constraint it becomes something else i guess, this becomes, uh, uh,
what is it, um, y_i times W which is,
um, transpose x plus b is greater than 1.
And again this simplifies or if you just multiply this out.
[NOISE].
So just to make sure that mapping is clear,
um- uh, all these pens are dying.
All right I'll not [NOISE]. All right.
So that becomes this and this becomes that, okay.
Um, and the key property we're going to use is that,
if you look at these two equations in terms of how we pose the optimization problem,
the only place that the feature vectors appears is in this inner product.
Right, um, and it turns
out when we talked about the Kernel Trick and we talked with the application of kernels,
it turns out that, um,
if you can compute this very efficiently,
that's when you can get away with manipulating even infinite dimensional feature vectors.
We- we'll get to this in a second.
But the reason we want to write the whole algorithm in terms of inner products is, uh,
there'll be important cases where the feature vectors
are 100 trillion dimensional but you
can compute the- or even infinite dimensional but you can
compute the inner product very efficiently without needing to loop over,
you know, the other 100 trillion elements in an array, right?
And- and we'll see exactly how to do that,
um, later in- in- very shortly.
Okay?
[NOISE]
So. All right,
um, now it turns out that,
uh, we've now expressed the whole,
um, optimization algorithm in terms of these parameters Alpha, right?
Defined here, uh, and b.
So now the parameters Theta,
now- now the parameter z is optimized for our Alpha, um,
it turns out that by convention
in the way that you see support vector machines referred to,
you know, in research papers or in textbooks.
It turns out there's a further simplification of
that optimization problem which is that you can simplify to this,
[NOISE] um, and the derivation to go from that to this is again relatively complicated.
[NOISE] But it turns out you can
further simplify the optimization problem I wrote there to this.
Okay? And again, uh,
you- you can copy this down if you want but this is also written in the lecture notes.
And by convention this slightly simplified version optimization problem
is called the dual optimization problem.
Um, the way to simplify that optimization problem to this one that's actually done by,
um, using convex optimization theory, uh,
and- and- and again the derivation
is written in the lecture notes but I don't want to do that here.
If- if you want think of it as doing
a bunch more algebra to simplify that problem to this one
and consequently, you cancel out B along
the way, it's a little more complicated than that but-but right,
the full derivation is given in the lecture notes.
Um, and so, um,
finally, you know, the way you train for-the way you make a prediction, right,
as you saw for the alpha i's and maybe for b, right, since you solve
this optimization problem or that optimization problem for
the Alpha i's and then to make a prediction,
um, you need to compute
h of W b of x for a new test example which is g of w transpose x plus b.
Right. But because of the definition of w- w this is equal to g of,
um, that's W transpose X plus b because this is
w and so that's equal to g of sum over
i Alpha_i y_i inner product between X_i and X plus b.
And so once again, you know,
once you have stored the Alphas in your computer memory,
um, you can make predictions using just inner products again, right?
And so the entire algorithm both
the optimization objective you need to deal with during training.
As well as how you make predictions is, um-uh,
is expressed only in terms of inner products, okay?
So we're now ready to
apply kernels and sometimes in
machine learning people sometimes we call this a kernel trick and let me
just the other recipe for what this means,
uh, step 1 is write your whole algorithm, [NOISE] um.
[NOISE]
In terms of X_i,
X_j, in terms of inner products.
Uh, and instead of carrying the superscript, you know X_i,
X_j, I'm sometimes gonna write inner product between X and Z, right?
Where X and Z are supposed to be proxies for
two different training examples X_i and X_j but it
simplifies the notation, uh, right a little bit.
Two, um, let there be some mapping,
um, from your original input features
X to some high dimensional set of features Phi.
Um, and so one example would be,
let's say you try to predict the housing prices
or predicting a house will be sold in the next month.
So maybe X in this case is the size of the house,
uh, or maybe is, uh,
size and yeah, let write.
Maybe X is the size of a house,
and so you could, um,
take this 1D feature and expand it to a high dimensional feature vector with X,
X squared, X cubed,
X to the 4th, right?
So this would be one way of defining a high dimensional feature mapping.
Or another one could be, if you have two features X_1 and X_2,
uh, corresponding to the size of the house and number of bedrooms,
now you can map this to different Phi X,
which may be X_1, X_2,
X_1 times X_2, X_1 squared X_2,
uh, X_1 X_2 squared, and so on.
They are kind of polynomials, set of features,
or maybe another set of features as well, okay?
And what we'll be able to do is,
work with, um, feature mappings, Phi of X,
where the original input X may be 1D or 2D or, or whatever,
and Phi of X could be,
you know, 100,000 dimensional or infinite dimensional.
That we'll be able to do this very efficiently right.
Or even infinite dimensional, okay?
So I guess we will get some concrete examples of this later,
but I want to give you the overall recipe.
And then, what we're going to do is to find a way to compute K of X comma Z,
equals Phi of X transpose Phi of Z.
So this is called the kernel function.
And what we're gonna do is,
we'll see that there are clever tricks so
that you can compute the inner product between X
and Z even when Phi of X and Phi of Z are incredibly high dimensional, right?
We'll see an example of this in a- in- in very very soon.
And step four is, um,
replace X, Z in algorithm
with K of X, Z, okay?
Um, because if you could do this then what you're doing is,
you're running the whole learning algorithm on this high dimensional set of features,
um, and the problem with swapping out X for Phi of X,
right, is that, it can be very computationally expensive if you're
working with 100,000 dimensional feature vectors, right.
I,I- even by to this standards, you know, 100,000, it's.
it's not the biggest I've seen, I've seen, actually,
biggest I've seen that you have a billion features, uh,
but even by today's standards,
100,000 features is actually quite a lot.
Um, uh, and- and if you're launching I said,
just 100,000 is, is- this is a lot- lot of large number of features, I guess.
Um, and the problem of using this is it's quite computationally expensive,
to carry around these 100,000 or million
dimensional or 100 million dimensional feature vectors or whatever.
Um, but that's what you would do if you were to swap in Phi of X,
you know in the naive straightforward way for X, but what we'll see is that,
if you can compute K of X, Z then you could,
because you've written your whole algorithm just in terms of inner products,
then you don't ever need to explicitly compute Phi of X,
you can always just compute these kernels. Yeah.
[inaudible]
Let me get to that later,
you know, I will go for some kernels and I will talk about uh,
bias-variance probably on Wednesday.
Yeah. I think the no free lunch theorem is
a fascinating theoretical concept but I think that it has been,
I don't know, it's been less useful actually because I think we
have inductive biases that turn out to be useful.
There's a famous theorem in learning theory called no free lunch.
It was like 20 years ago.
That basically says that, in the worst case,
learning algorithms do not work [NOISE].
For any learning algorithm,
I can come up with some data distribution so that your learning algorithm sucks.
That, that's roughly the no free lunch theorem,
proved about like 20 years ago.
But it turns out most of the world- most of the time,
the universe is not that hostile toward us.
So- so, yeah, so as the learning algorithms turned out okay [LAUGHTER].
Um, all right, let's go through one example of kernels.
Um, so for this example,
let's say that your offer is not input features was three-dimensional X_1, X_2, X_3.
And let's say I'm gonna choose the feature mapping,
Phi of X to be,
um, o- so pair-wise, um, monomial terms.
So I'm gonna choose X_1 times X_1, X_1 X_2,
X_1 X_3, X_2 X_1, all.
Okay. And there are a couple of duplicates so
X_1 X_3 is equal to X_3 X_1 but I'll just write it out this way.
And so notice that, er,
if you have- if X is in R_n, right?
Then Phi of X is in R_n squared, right.
So got the three-dimensional features to nine dimensional.
And I'm using small numbers for illustration.
In practice, think of X as 1,000 dimensional and so this is now a million.
Or think of this as maybe 10,000 and this is now like 100 million, okay.
So n squared features is much bigger [NOISE].
Um, and then similarly, Phi of Z is going to be Z_1 Z_1, Z_1 Z_2,
okay? So we've gone from n features like 10,000 features,
to n squared features which,
in this case, 100 million features.
Um, so because there are n squared elements, right?
You will need order n squared time to compute Phi of
X or to compute
phi X transpose Phi of Z explicitly, right?
So if you wanna compute the inner product between Phi of X and Phi of
Z and they do it explicitly, in the obvious way,
it'll take n squared time to just compute all of
these inner products and then do the- and,
and then they'll compute this,
er, com- compute this, right.
And it- it's actually n squared over 2,
because a lot of these things are duplicated but that's the order n-squared.
But let's see if we can find a better way to do that.
So what we want is to write out the kernel of x, z.
So this phi of x transpose phi of z, right?
And, uh, what I'm gonna prove is that this can be computed
as x transpose z squared, right?
And the cool thing is that remember x is n-dimensional, z is n-dimensional.
So x transpose z squared,
this is an order n time computation, right?
Because taking x transpose z, you know,
that's just in a product of two n-dimensional vectors and then you take that number,
x transpose z is a real number,
and you just square that number.
So that's the order n time computation.
Um, and so let me just prove that x transpose z is equal to,
well, le- le- le- let me, let me,
let me prove this step, right?
Um, and so x transpose z squared that's equal to,
um, right.
So this is x transpose z, right?
And then times this is also x transpose z.
So this formula is z transpose z squared,
it's x transpose z times itself.
Um, and then if I rearranged sums,
this is equal to sum from i equals 1 through n,
sum from j equals 1 through n, um,
x_i z_i, x_j z_j.
Um, and this in turn is,
you know, sum over i, sum over j,
of x_i x_j times z_i z_j, right?.
And so what this is doing,
is it's marching through all possible pairs of i and j and multiplying x_i x_j,
with the corresponding z_i z_j and adding that up.
But of course; if you were to compute phi of x transpose phi of z,
what you do is you take this and multiply with that and then add it to the sum,
then take this and multiply with that and add it to the sum,
and so on until you end up taking this and multiplying
that and adding it to your sum, right?
So that's why, um- so that's why this formula is just,
you know, marching down these two lists this, and multiplying, multiplying,
multiplying and add it up,
which is exactly, um, phi transpose.
Which is exactly phi of x transpose phi of z.
Okay? So this proves that, um,
you've turned what was previously an order n square time calculation,
into an order n time calculation.
Which means that, um,
if n was 10,000,
instead of needing to manipulate 100,000
dimensional [NOISE] vectors to come up with these.
Sorry. That's my phone buzzing. This is really loud.
Okay. Instead of needing to manipulate sort of 100,000 dimensional vectors,
you could do so manipulating only 10,000 dimensional vectors, okay?.
Now, um, a few other examples of kernels.
It turns out that,
um, if you choose this kernel- so let's see.
We had k of x comma z equals x transpose z squared, um,
if we now add a plus c there where c is a constant, um,
so c is just some fixed real number,
that corresponds to modifying your features as follows.
Um, where instead of just this- you know,
binomial terms of pairs of these things,
if we add plus c there,
it corresponds to adding x_1,
x_2, x_3, uh, to this- to your set of features.
Ah, technically, there's actually weighting on this.
There's your root 2c, root 2c,
root 2c and then as a constant c there as well.
And you can prove this yourself,
and it turns out that if this is your new definition for phi of x,
and make the same change to phi of z.
You know, so root 2c z_1 and so on.
Then if you can take the inner product of these,
then it can be computed as this.
Right? And so that's- and, and,
and so the role of the, um,
constant c it trades off the relative weighting between the binomial terms the- you know,
x_i x_j, compared to the,
to the single- to the first-degree terms the x_1 or, x_2 x_3.
Um, other examples, uh,
if you choose this to the power of d, right?
Um, notice that this still is an order n time computation, right?
X transpose z takes order n time,
you add a number to it and you take this the power of
d. So you can compute this in order n time.
But this corresponds to now phi of x has all- um,
the number of terms turns out to be n plus d choose d but it doesn't matter.
Uh, it turns out this contains all features of, uh,
monomials up to, uh,
order d. So by which I mean, um,
i- i- if, let's say d is equal to 5, right?
Then this contains- then phi of x contains all the features of the form
x_1 x_2 x_5 x_17 x_29, right?
This is a fifth degree thing, uh, or x,
or x_1 x_2 squared x_3 x, you know, 18.
This is also a fifth order polynomial-
a fifth order monomial it's called and so if you, um,
choose this as your kernel,
this corresponds to constructing phi of x to
contain all of these features and there are not exponentially many of them, right?
There a lot of these features.
Any or all the, um,
all, all the- these are called monomials.
Basically all the polynomial terms, all the monomial terms,
up to a fifth degree polynomial,
up to a fifth order monomial term.
So- and there are- it turns out there are n plus z choose ds which is,
uh, roughly n plus d to the power of d very roughly.
So this is a very, very large number of features, um,
but your computation doesn't blow up exponentially even as d increases.
Okay? So, um, what a support vector machine is, is, um,
taking the optimal margin classifier that we derived earlier,
and applying the kernel trick to it,
uh, in which we already had the- so well.
So optimal margin classifier plus the kernel trick,
right, that is the support vector machine.
Okay? And so if you choose some of these kernels for example,
then you could run an SVM in these very,
very high-dimensional feature spaces, uh,
in these, you know, 100 trillion dimensional feature spaces.
But your computational time,
scales only linearly, um, as order n,
as the numb- as a dimension of your input features x rather
than as a function of this 100 trillion dimensional feature space,
you're actually building a linear classifier.
Okay? So, um, why is this a good idea?
Let me just, sheesh.
Let's show a quick video to give you intuition for what this is doing.
Um, let's see.
Okay. I think the projector takes a while to warm up, does it?
[NOISE] All right. Any questions while we're- Yeah?
[inaudible]
Uh, yes. So, uh,
this kernel function appears- applies only to this visual mapping.
So each kernel function of, um, uh,
uh, yes, after trivial differences, right?
If you have a feature mapping where the features that
could- are permuted or something,
then the Kernel function stays the same.
Uh, uh, so there are trivial chunk function- transformations like that but, uh,
if we have a totally different feature mapping,
you would expect to need a totally different kernel function.
Cool.
So I wanted to- let's see.
Ah, cool, awesome. Uh, I want to give you a visual picture [NOISE]
of what this um,
[NOISE].
All right, um, this is a YouTube video that, uh,
Kian Katanforoosh who teaches CS230 found and suggested I use.
So I don't- I don't know who Udi Aharoni is but
this is a nice visualization of what a support vector machine is doing.
So um, let's see how the uh,
uh, learning algorithm where you're trying to separate the blue dots from the red dots.
Right? So um, the blue and the red dots can't be separated by a straight line,
but you put them on the plane and you use a feature mapping
phi to throw these points into much higher-dimensional space.
So there's now three of these points in the three-dimensional space.
In the three-dimensional space,
you can then find w. So w is now three-dimensional because it applied
the optimal margin classifier in
this three-dimensional space that separates the blue dots and the red dots.
Uh, and if you now you know examine what this is doing back in the original space,
then your linear classifier actually defines that elliptical decision boundary.
That makes sense right? So you're taking the data- all right um,
so you're taking the data, uh,
mapping it to a much higher dimensional feature space,
three-dimensional visualization that in practice can be
100 trillion dimensions and then finding
a linear decision boundary in that 100 trillion-dimensional space uh,
which is going to be a hyperplane like a- like a straight, you know,
like a plane or a straight line or a plane and then when you look at what you just did in
the original feature space you found a very non-linear decision boundary, okay?
Um, so this is why uh- and again here you can only
visualize relatively low dimensional feature spaces even, even on a display like that.
But you find that if you use an
SVM kernel you know, um, right,
you could learn very non-linear decision boundaries like that.
But that is a linear decision boundary in a very high-dimensional space.
But when you project it back down to you know,
2D you end up with a very non-linear decision boundary like that okay? All right.
So.
Yeah.
[inaudible] digital words [inaudible]
Oh sure, yes. So uh, in this high dimensional space
represented by the feature mapping phi of X
does the data always have to be linearly separable?
So far we're pretending that it does,
I'll come here back and fix that assumption later today.
Yeah.
Okay, so um now,
how do you make kernels?
Right? Um, so here's here's some- so here's some intuition you might have about kernels.
Um if X and Z are
similar you know if two if two- and for
the examples X and Z are close to each other or similar to each other then K of x z,
which is the inner product between X and Z, right?
Presumably this should be large.
Um and conversely if X and Z are dissimilar then K of x z,
you know this maybe should be small, right?
Because uh the inner product of
two very similar vectors that are pointing the same direction
should be large and the inner product of two dissimilar vectors should be small.
Right? So this is one uh guiding principle behind,
you know, what you see in a lot of kernels.
Just if- if this is phi of x and this is phi of z,
the inner product is large but then they kinda point off in random directions,
the inner product will be small right?
That's how vector inner product works.
Um and so- well what if we just pull
a function out of these three here, out of the air um, which is K
of xz equals e to the negative x minus z squared over 2 sigma squared.
Right? So this is one example of a similarity
sim sim sim sim- if you think of kernels as a similarity measure of a function,
this you know let's just make up
another similarity measure of a function and this does have the property that if
X and Z are very close to each other then this would be e to the 0 which is about 1.
But if X and Z are very far apart then this would be small, right?
So this function it- it actually satisfies this criteria.
It satisfies those criteria and the question is uh,
is it okay to use this as a kernel function?
Right? So it turns out that um a function like that K of x z,
you can use it as a kernel function.
Only if there exists
some phi such that K of x z equals phi of X transpose phi Z right?
So we derived the whole algorithm assuming this to be true and it
turns out if you plug in the kernel function for which this isn't true,
then all of the derivation we wrote down breaks down and the optimization
problem you know um, uh, can have very strange solutions, right?
That don't correspond to good classification though a good classifier at all.
Um and so this puts some constraints on
what kernel functions we could or for example,
one thing it must satisfy is K of X X which is phi X transpose phi of Z.
This would better be greater than equal to 0, right?
Sorry right?
Because inner product of a vector with itself had better be non-negative.
So K of X X is ever 0 or less than 0,
then this is not a valid kernel function, okay?
Um, more generally, there's
a theorem that uh proves when is something a valid kernel.
Um, somebody just outlined that that proof very briefly which is uh,
less than X_1 up to X_d you know be any d points, right?
And let's let K- sorry about overloading of
notation um this is a- so K represents
a kernel function and I'm gonna use K to represent the kernel matrix as well.
Sometimes it's also called the gram matrix uh but it's called the kernel matrix.
So that K_ ij is equal to the kernel function
applied to two of those points um X_i and X_j, right?
So you have d points.
So just apply the Kernel function to every pair of
those points and put them in a matrix, in a big d by d matrix like that.
So it turns out that uh,
given any vector Z- I think you've seen something similar to this in problem set one,
but given any vector z,
z transpose K z which is sum over i sum over
j z i k i j z j, right?
Um if K is a valid kernel function so if there is some feature mapping phi,
then this should equal to sum of i sum of j Z_i phi of X_i transpose phi
of z X_j times Z_j and by a couple other steps.
Um let's see.
This phi of X_i transpose phi of X_j.
I'm gonna to expand out that inner product.
So sum over k, phi of X_ i,
element k times phi of X_j element k times Z_ j,
um and then we are arranging
sums is sum- sum over
K oh sorry I'm running out of whiteboard let me just do it on the next board.
So we arrange sums, sum of k,
sum of i, sum of j,
z i phi of x i subscript k,
times phi of x [NOISE] j subscript k times z j.
Which is sum of the k [NOISE]
squared and therefore this must be greater than or equal to 0.
Right. And so this proves that the matrix K,
ah, the kernel matrix k is positive semi-definite.
Okay. Um, and so more generally,
it turns out that this is also a sufficient condition, um,
for a kernel function to- for our function k to be a valid kernel function.
So let me just write this out.
This is called a Mercer's Theorem, M-E-R-C-E-R.
Wait, um, so K is a valid kernel.
[NOISE] So K is
a valid kernel function i.e there exists phi such that K of x z,
equals phi of x,
transpose phi of z if and only
if for any d points,
you know, x one up to x z,
on the corresponding kernel matrix
[NOISE] is a positive semi-definite.
So if you write this K greater equals 0.
Okay. Um, and I proved just one-dimens- one- one direction of this implication.
Right. This proof outline here shows that if it is a valid kernel function,
ah, then this is positive semi-definite.
Um, this outline didn't prove the opposite direction.
You see if and only if.
Right. Shows both directions.
So this, ah, algebra we did just now
proves that dimension of the proof I didn't prove the reverse dimension.
But this turns out to be
an if and only if condition.
And so this gives maybe one test for,
um, whether or not something is a valid kernel function.
Okay. Um, and it turns out that- the kernel I wrote up there, um,
that one, K of x z, uh.
Right. And it turns out this is a valid kernel.
This is called the Gaussian kernel.
This is, uh, probably the most widely used kernel.
Um, well a- actually well,
uh, let me [NOISE].
Well, but the actually the most widely used kernel is-is maybe the linear kernel, um,
which just uses K of x z equals x transpose z,
ah, and so this is using you know phi of x equals x.
Right. So no- no- no high dimensional features.
So sometimes you call it the linear kernel.
It just means you're not using a high dimensional feature mapping
or the feature mapping is just equal to the original features.
Ah, this is this is actually a pretty commonly used kernel function,
ah, you- you're not taking advantage of kernels in other words.
Ah,but after the linear kernel
the Gaussian kernel is probably the most widely used kernel, uh,
the one I wrote up there and this corresponds to
a feature dimensional space that is um, infinite dimensional.
Right. And, ah, this is actually- this particular kernel function,
corresponds to using all monomial features.
So if you have, ah, you know,
X one and also X 1,
X 2 and X 1 squared X 2 and X 1 squared X 5 to the
10 and so on up to X 1 to the 10,000 and X 2 to the 17.
Right. Whatever. Um, ah,
so this particular kernel corresponds to using
all these polynomial features without end going to arbitrarily high dimensional um,
by giving a smaller weighting to the very very high dimensional ones.
Which is why it's wide.
Yeah.
Okay.
Um, great.
So the, ah, kernel to end- toward the end,
I'll give some other examples of kernels.
Um, so it turns out that the kernel trick
is more general than the support vector machine.
Um, it was really popularized by
the support vector machine where you know researchers, ah,
because Vladimir Vapnik and
Corinna Cortes found that applying these kernel tricks to a support vector machine,
makes for a very effective learning algorithm.
But the kernel trick is actually more general and if you have
any learning algorithm that you can write in terms of inner products like this,
then you can apply the kernel trick to it.
Ah and so you- you play with this for a different learning algorithm in the ah,
in the programming assignments as well.
And the way to apply the kernel trick is,
take a learning algorithm write the whole thing in terms of
inner products and then replace it
with K of x z for some appropriately chosen kernel function K of x z.
And all of the discriminative learning algorithms we've learned so far,
um, ah, can be written in this way so that you can apply the kernel trick.
So linear regression, logistic regression,
ah, everything of the generalized linear model family,
the perceptron algorithm, all of the- all of those algorithms,
um, you can actually apply the kernel trick to.
Which means that you could um,
apply linear regression in an infinite dimensional feature space if you wish.
Right. Um, and later in this class we'll talk about principal components analysis,
which you've heard of but when we talk about principal components analysis,
turns out that's yet another algorithm that can be written only in terms of
linear products and so there's an algorithm called kernel PCA,
kernel principal component analysis.
If you don't know what PCA is, don't worry about it we'll get to it later.
But a lot of algorithms can be,
ah, married with the kernel trick.
So implicitly apply the algorithm even in an infinite dimensional feature space,
but without needing your computer to have
an infinite amount of memory or using infinite amounts of computation.
Ah, for this- actually
the single place this is most powerfully applied is the- is the support vector machine.
In practice I don't- in practice the kernel trick is
applied all the time for support vector machines and less often in other algorithms.
[NOISE] All right.
Um, [NOISE] any questions, before
we move on. No. Okay. [NOISE]
All right. So last two things I wanna do today, Um,
one is fix the assumption that we had made that the data is linearly separable, right?
Um, so, you know, uh,
sometimes you don't want your learning algorithm to have, uh,
um, zero errors on the training set, right?
So when- when you take this
low dimensional data and map it to a very high dimensional feature space,
the data does become much more separable.
Uh, but it turns out that if your data set is a little bit noisy, [NOISE]
right, if your data looks like this,
you've, maybe you wanted to find a decision boundary like that,
uh, and you don't want it to try so hard to separate every little example,
right, that's defined a really complicated decision boundary like that, right?
So sometimes either the low-dimensional space or in the high dimensional space Phi, um,
you don't actually want the algorithms to separate out your data
perfectly and- and then sometimes even in high dimensional feature space,
your data may not be linearly separable.
You don't want the algorithm to, you know,
have zero error on the training set.
And so, um, there's an algorithm called the L_1
norm [NOISE] soft margin SVM,
which is a, um,
modification to the basic algorithm.
So the basic algorithm was min over this, right,
subject to, [NOISE] okay.
Um, [NOISE] and so what the L_1 norm sub margin does is the following;
It says, um, you know,
previously this is saying that remember this is the geometric margin.
[NOISE] Right.
If you normalize this by the norm of w becomes- excuse me,
this is the functional margin.
Um, if you divide this by the norm of w it becomes the geometric margin.
Um, so this optimization problem was
saying let's make sure each example has functional margin greater or equal to 1.
And in the L_1 soft margin SVM we're going to relax this.
We're gonna say that this needs to be bigger than 1 minus
c. There's a Greek alphabet C. Um,
and then we're gonna modify the cost function as follows.
[NOISE] Where these c I's are greater than or equal to 0.
Okay. So remember, um,
if the function margin is greater or equal to 0,
it means the algorithm is classifying that example correctly, right?
So long as this thing is getting 0, then, you know,
y and this thing will have the same sign either both positive or both negative.
Uh, that's what it means for a product of two things to be greater than zero,
both things have to have the same sign, right,
and so if this is if- if, um,
so as long as this is bigger than 0,
it means it's classifying that example correctly.
Um, and the SVM is asking for it to not just classify correctly,
but classify correctly with the- with the functional margin of the- at least 1.
Um, and if you allow CI to be positive,
then that's relaxing that constraint.
Okay. Um, but you don't want the CIs to be too big which is why
you add to the optimization cost function,
a cost for making CI too big.
[NOISE]
And so you optimize this as function of W.
[NOISE]
And these are Greek alphabets c.
[NOISE]
Um, and if- if you draw a picture,
it turns out that, um,
in this example with that being the optimal decision boundary, um,
it turns out that these examples- [NOISE]
these three examples would be equidistant from this straight line, right?
Because if they weren't, then you can fiddle the straight line
to improve the margin even a little bit more.
It turns out that these few examples have,
um, functional margin exactly equal to 1.
And this example over there,
we have functional margin equal to 2,
and the further away examples of even bigger functional margins.
And what this optimization objective is
saying is that it is okay if you have an example here,
where functional margin so everything right so everything here has functional margin one.
If an example here I have functional margin a little bit less than one.
And this by having- by setting Ci to 0.5 say is
letting me [NOISE] get away with having function module lower than, less than 1.
[NOISE] Um, er, one other reason why,
um, you might want to use the L_1 norm soft margin SVM is the following,
which is, um, [NOISE] let's say you have a data set that looks like this.
[NOISE] You know,
seems like- it seems like that would be a pretty good decision boundary, right?
But, um, if we add just,
you know, measure a lot of examples, a lot of evidence.
But if you have just one outlier,
say over here, then technically the data set is still linearly separable, right?
[NOISE] If you really want to separate this data set,
um, sorry, I seem to be killing these pens myself as well.
[NOISE] All right.
If you want to separate out this data set,
you can actually, you know,
choose that decision boundary.
But the basic optimal margin classifier will allow the presence of one training example
[NOISE] to cause you to have
this dramatic swing in the position of the decision boundaries.
So they are, because
the original optimal margin classifier it optimizes for the worst-case margin,
the concept of optimizing for the worst-case margin allows one example by being
the worst case training examples have a huge impact on
your decision boundary and so the L_1 soft margin SVM,
um, allows the SVM to still keep the decision boundary closer to the blue line,
even when there's one outlier.
And it makes it, um,
much more robust outliers.
Okay. Um, [NOISE] and then if you go through the representer theorem derivation, uh,
you know, represent w as a function of the Alphas and so on, um,
It turns out that the problem then simplifies to the following;
So this is- I'm just [NOISE] right,
after some- some- after, you know,
the whole representing the calc- the whole represents a calculation, [NOISE] derivation.
[NOISE] This is just what we had previously.
[NOISE] I've not changed anything so far.
[NOISE] Right.
This is just exactly what we had.
Um, all right, and, uh- [NOISE]
And it turns out that, um,
the only change to this is we end up with an additional condition on the authorize.
So if- if you go for that simplification, uh,
now that you've changed the algorithm to have this extra term, uh,
then the- the- the new form- this is called the dual form with the optimization problem.
The only change is that you end up with this additional condition, right?
The, the constraints between Alpha are between 0 and C. Um,
and it turns out that, uh,
today there are very good, you know,
packages, uh, software packages which are solving that for you.
I- I- I- I think once upon a time we were doing machine learning,
you need to worry about whether your code for inverting matrices was good enough, right?
And when- when code for inverting
matrices was less mature there's just one thing you had to think about.
But today uh, linear algebra, you know,
packages have gotten good enough that
when you invert the matrix you just invert the matrix.
You don't have to worry too much- when you're
solving you don't have to worry too much about it.
So in the early days of SVM solving this problem was really hard.
You had to worry if your optimization packages were optimizing it.
But I think today there are very good numerical optimization packages.
They just solve this problem for you and you can just code without worrying
about the- the details that much. All right.
So this L1 norm soft margin SVM and, uh,
oh and so, um,
and so this parameter C is something you need to choose.
We'll talk on Wednesday about how to choose this parameter.
But it trades off um- how much you want to insist
on getting the training examples right versus you know,
saying it's okay if you label a few terms out of this one.
[NOISE] We'll- we'll discuss on Wednesday when we discuss bias and variance.
How they choose a parameter like c. All right.
So the last thing I want to- last thing I'd like you to see
today is uh just a few examples of um, SVM kernels.
Uh, let me just give um- all right.
So, uh, it turns out the SVM with the polynomial kernel,
uh, works quite well.
So this is, uh, you know k of x,
z equals x transpose z to the d. This thing is called a polynomial kernel,
um, and this is called a Gaussian kernel which is really
uh- the most widely used one is the Gaussian kernel.
Right. And it turns out that I guess early days of SVMs,
you know, one of the proof points of SVMs was, um,
the field of machine learning was doing a lot of work on
handwritten digit classification so that's
uh- so a- a digit is a matrix of pixels with values that are,
you know, 0 or 1 or maybe grayscale values, right?
And so if you take a list of pixel intensity values and list them,
so this is 0, 0, 0, 1, 1,
0, 0, 0, 0, 0, 1,
0 and just- this is all the pixel intensity values,
then this can be your feature X and you feed it
to an SVM using either of these kernels um,
it'll do not too badly, uh,
as a handwritten digit classification, right?
So there's a classic data set, um, called MNIST,
which is a classic benchmark, uh, in computing- uh,
in- in history of machine learning and, um,
it was a very surprising result many years ago that, you know,
support vector machine with a kernel like this
does very well on handwritten digit classification.
Uh, in the past several years we've found that deep learning algorithms,
specific convolutional neural networks do even better than the SVM.
But for some time, um, SVMs were the best algorithm uh,
and- and they're very easy to use in turnkey.
There aren't a lot of parameters to filter with.
So that's the one very nice property about them.
Um, but more generally,
uh, a lot of the most innovative work in SVMs has been into design of kernels.
So here's one example.
Um, let's say you want a protein sequence classifier, right?
So uh, uh protein sequences are made up of ami- of- of amino acids so,
you know, I guess a lot of our
bodies are made of proteins and proteins are just sequences of
amino acids and there are 20 amino acids, um, but, uh,
in order to simplify the description and really not worry too much of biology,
I hope the biologists don't get mad at me,
I'm gonna pretend there are 26 amino acids
even though there aren't because there are 26 alphabets.
So I'm gonna use the alphabets A through Z to denote amino acids
even though I know there's supposed to be only 20 but it's
just easier to talk with- with 26 alphabets.
And so a protein is a sequence of alphabets.
Right? Because a protein in your body is a sequence
that's made up of a sequence of amino acids and,
uh, amino acids can be very- variable length,
some can be very, very long, some can be very, very short.
So the question is,
how do you represent the feature X?
So it turns out- uh, and so, um,
the goal is to get an input x and make a prediction about this particular protein.
Like, what is the function of this protein, right?
And so- well, here's one way to design a feature vector which is, uh,
I'm going to list out all combinations of four amino acids.
You can tell this will take a while.
Right. Go down to AAAZ and then AABA and so on.
Uh, and eventually, you know,
there'll be a BAJT,
TSTA down to ZZZZ.
Right. Um, and then I'm gonna construct phi of
x according to the number of times I see this sequence in the amino acids.
So for example, BAJT appears twice.
So I'm gonna put 2 there um,
uh, you know, TSTA, oh whatever.
Right. Appears once so I'm gonna put a 1 there and there are no AAAAs,
no AAABs, no AAACs and so on.
Okay? So this is a- uh,
a 20 to the 4, you know,
26 to the 4, 20 to the 4-dimensional feature vector.
So this is a very, very high dimensional feature vector.
And it turns out that, um,
using some statistic as 20 to the 4 is 160,000.
That's pretty high dimensional.
Quite expensive to compute.
And it turns out that using dynamic programming,
given two amino acid sequences you can compute phi of x transpose phi of z,
that's K of x,z.
And there's a- there's a- there's a dynamic programming algorithm for doing this.
Uh, the details aren't important for first-year students,
uh, if any of you, um,
have taken an advanced CS algorithms course and learned
about the Knuth-Morris-Pratt algorithm,
uh, it's- it's- it's quite similar to that.
Uh, so it's Don Knuth, right, Stanford- Stanford professor, emeritus professor here.
So the DP algorithm is quite similar to that and um,
uh using this is actually quite um,
this is actually a pretty decent algorithm for inputting a sequence of, say,
amino acids and training a supervised learning algorithm to
make a call- binary classification on amino acid sequences.
Okay? So as you apply support vector machines one of the things you
see is that depending on the input data you have,
there can be innovative kernels to use, uh,
in order to measure the similarity of two amino acid sequences or
the similarity of two of whatever else and then to use that to um,
build a classifier even on very strange shaped object which,
you now, do not come,
um, as a feature.
Okay? So um, uh,
and- and I think actually- another example- or if the input x is a histogram,
you know, maybe of two different countries.
You have histograms of people's demographics it
turns out that there is a kernel that's taking the min of
the two histograms and then summing up to compute
a kernel function that inputs two histograms that measures how similar they are.
So there are many different kernel functions for
many different unique types of inputs you might want to classify.
Okay? So that's it for SVMs uh,
very useful algorithm and what we'll do on Wednesday is
continue with more advice on now the- all of these learning algorithms.
We'll talk about bias and variance to give you more advice on how to actually apply them.
So let's break and then I'll look forward to seeing you on Wednesday.
 Hey guys. Um, let's get started.
So over the last several weeks,
you've learned a lot about many different learning algorithms from linear regression,
to logistic regression, to generalizing models,
generative algorithms like GDA and Naive Bayes to most recently support-vector machines.
Um, what I'd like to do today is to start talking
about advice for applying learning algorithms.
To teach a little bit about the theory behind, um,
how to make good decisions of what to do,
how to actually apply these algorithms.
And so today, um,
I wanna discuss bias and variance.
Um, and it turns out, you know, I've,
I've built quite a lot of machine learning systems, um,
and it turns out that bias and variance is one of those concepts.
It's, sort of, easy to understand,
but hard to master.
Uh, uh, what does it- lots of those,
was it all these board games or sometimes, uh, uh,
smartphone games, say easy to learn, hard to master or something like that?
So bias and variance is actually one of those things,
where I've had PhD students that worked with me for several years and then graduated,
and worked in the industry for a couple years after that.
And, and they actually tell me that, you know,
when they took, um,
machine learning at Stanford,
they learned bias and variance,
but as they progressed for many years
their understanding of bias and variance continues to deepen.
So I'm gonna try to accelerate your learning,
um, uh, uh, of,
of bias and variance because I find that people that understand this concept,
um, are much more efficient in
terms of how you develop learning algorithms and make your algorithms work.
So we'll talk about this today,
and it'll be a recurring theme that'll come up again a
few times in the next several weeks as well.
Um, then we'll discuss regularization, um,
uh, and talk about, um,
how to reduce variance in learning algorithms,
talk about train, dev, test splits, uh,
and then also talk about a few model selection and cross-validation algorithms.
Um, oh, let's see, reminders for today.
Uh, Problem Set 1 is due tonight,
uh, uh, 11:59 P.M.
Uh, and, uh, if you are not yet ready to submit it today,
uh, late submissions are accepted until Saturday evening.
Saturday 11:59 P.M, with the details of late submissions, uh,
written according to the late day policy written on the course website. So,
so I definitely encourage you to submit your homework on time today.
If for some reason you're not able to the late submission,
which we don't encourage anyone to take advantage of,
but it is written, uh, on the course website.
And Problem Set 2 will be released, uh shortly.
Actually I think, uh, it was already posted online,
um, uh, and is due two weeks from now.
Um, yeah.
Right. And so, uh, okay.
So, um, and, and what I'm going to do today is talk about the conceptual aspects of this.
Uh, and if you want to see even more math between these so the conceptual concepts,
uh, at this Friday's discussion section,
we'll be covering, um,
some of the the, uh,
mathematical aspects of learning theories such as error decomposition,
uniform convergence, and VC dimension.
You know, one, one interesting thing I've learned is, um,
really watching the evolution of machine learning over many years is that,
that machine learning as a discipline has actually
become less mathematical over the years, right?
Um, uh, so I remember when, um, you know,
machine learning people used to worry about, uh,
computing the normal equations,
like x transpose x inverse equals x transpose y.
How numerically stable is your numerical solver for solving
the normal equations of inverting a matrix for solving linear equations.
But because, um, numerical linear algebra has made tremendous rise,
now we just call linear- linear algebra routine.
To invert a matrix to solve linear system equations
not worry about whether it's numerically stable or not.
But once upon a time a lot of my friends in
machine learning were reading text books on, uh,
numerical optimization to figure out if your formula
for inverting a matrix or really solving linear system equations was numerically stable.
And so one of the trends I've seen is that,
uh, I think, um,
three or four years ago,
to understand bias and variance,
there was a certain mathematical theory that was crucial to understanding that.
And so I used to teach that in CS229,
but we decided, um,
that we're constantly trying to improve this class, right?
But I decided that, uh, that, uh,
mathematical theory is actually less crucial today.
If your main goal is to make these algorithms work.
So we still teach it.
But we're doing it in the Friday discussion section,
and that leaves more time for the main lecture here to talk
more about the conceptual thing that I think will help you build learning algorithms,
as well as for the newer topics like, um,
what- we'll talk about random forest,
decision trees of random forests in neural networks next week.
So here we go.
Okay. So let's talk about bias and variance.
Um, let's say you
have this dataset. [NOISE]
Um, I'm gonna draw the same dataset three times.
[NOISE]
Okay. So, um, let's say
you have a housing price prediction problem where
this is the size of the house and this is the price of the house.
Um, it looks like if you fit a straight line to this data,
maybe it's not too bad, right?
But it looks like this dataset seems to go
up and then curve downward a little bit, right?
And so [NOISE] maybe this is a slightly better model if you fit a, let me see.
So this if you fit a linear function,
um, Theta 0 plus Theta 1x.
Uh, but if you fit a quadratic model,
maybe this actually fits to the dataset a little bit better.
Um, or you could actually fit a high order polynomial.
This is one, two, three, four, five, six examples.
So if you fit a fifth order polynomial,
let's say the 5x to the 5th,
then, um, you can actually fit a function that passes through all the points perfectly.
But that doesn't seem like a great model for this data either.
And so, um, to name
this phenomenon, the function assuming the one in the middle is what we like,
um, fitting a quadratic function is maybe pretty good.
Let's call it just right.
Whereas, um, this, uh,
example on the left,
it underfits the data,
um, as in, it is not capturing the trend that is maybe semi-evident in the data.
And we say this algorithm has high bias.
And the term bias, um,
the term bias has,
has actually multiple meanings in the English language.
We, as a society,
want to to avoid racial bias,
and gender bias, and discrimination against people's orientation, and things like that.
So, uh, the term bias in machine learning has a completely separate meaning.
Um, and it just means that, uh, and,
and it just means that, um, uh,
this learning algorithm had
very strong preconceptions that the data could be fit by linear functions.
This album had a very strong bias or
the very strong preconception that
the relationship between pricing and house- size of house is linear,
and this bias turns out not to be true.
So this is actually a different sense of bias than, than the,
than the other types of undesirable bias we want to avoid in society or which,
which interestingly comes up in machine learning as well in other contexts, right?
We want our learning algorithms to avoid those different biases, there's a different use of the term.
And in contrast and just cut off on the right,
we say that this is overfitting, um, the data.
And this algorithm has high variance.
Um, and the term high variance comes from this intuition that,
um, you happen to get these five examples,
but if, you know,
a friend of yours was to collect data from, uh, see here, six,
six examples and a friend of yours was to
collect a slightly different set of six examples, right?
So if a friend of yours were to rerun the collected slightly different, um, uh,
set of housings- houses, you know, right?
Then this algorithm will fit some totally other varying function
on this and so the- your predictions will have very high variance.
If you think of this as averaging over different random draws of the data.
So, so the, the variations if,
if a friend of yours does the same experiment and they just
get a slightly different dataset just due to random noise,
then this algorithm fitting a fifth-order
polynomial results in a totally different result.
So that's-, uh, so we say that this algorithm has
a very high variance, there's a lot of variability in the predictions this algorithm will make, okay?
Um, so one of the things we'll need to do is,
um, identify if your learning algorithm.
Oh, so when you train a learning algorithm.
It almost never works the first time.
And so when I'm developing learning algorithms,
my standard work flow is often to train an algorithm-,
uh, often train up something quick and dirty,
and then try to understand if the algorithm has a problem of high bias or high variance,
if it's underfitting it or overfitting the data,
and I use that insight to decide how to improve the learning algorithm.
And I will say a lot more about,
um, how to improve the learning algorithm.
We have a menu of tools that we'll talk about in the next couple of weeks,
about how to reduce bias or reduce variance of,
uh, of, of your learning algorithms.
Um, I should have mentioned that the problems of bias and variance,
um, also hold true for classification problems.
Uh, so, [NOISE], right.
So let's say that's a binary classification problem.
Um, if you fit a, uh,
logistic regression model to this,
you know, straight line fit to the data.
Maybe that's not great, right?
Um, if you fit a logistic regression model, um,
with a few nonlinear features;
so you have features x_1 and x_2.
Um, if instead of using x_1 and x_2 as features,
you use additional features x_1 squared,
x_2 squared, x_1 times x_2, x_1,
qx_2 and this is Phi of x, right?
And you can have a small set of features you choose by hand.
excuse me, probably more features in this or using SVM kernel and using SVM for this problem.
Then, um, if you, let's see,
if you have too many features,
then you might actually have a learning algorithm that
fits a decision boundary that looks like that.
Right? And this learning algorithm actually gets
perfect performance on the training set but this overfits.
Um, excuse me, I meant to make the colors consistent,
sorry I meant to use red.
But you- you get what I mean.
Um, and there's only if you choose somewhere in-between,
you know, that you get something that,
that seems to be a much better fit to the data.
The green line seems to be a pretty good way of separating
the positive and negative examples that they're sort of just right.
So, uh, similar to,
I guess I messed up the colors slightly before,
kind of but similar to these colors here,
the blue line underfits because it's not
capturing trends that are pretty apparently in the data.
The orange line overfits.
It's just much too complicated a hypothesis whereas the green line,
um, is just right, okay?
So it turns out that, um,
in the error of GPU computing ability to train models with a lot of features,
um, one of the- by building a big enough model,
uh, so take a support vector machine.
If you add enough features to it,
if you have a high enough dimensional feature space,
um, or if you, um,
take a linear regression model, logistic
regression model and just add enough features to it,
you can often, um, overfit the data.
And it turns out that, um,
one of the most effective ways to prevent overfitting, um, is regularization.
So let me describe what that is and,
um, excuse me, just finding my notes, reworking today's lecture.
So this is new things I have not presented.
Um, so all that. Okay, cool.
And, um, regularization is the- it'll be one of those techniques that,
um, won't take that long to explain.
It'll sound deceptively simple but is one of the techniques that I use most often.
I, I, I feel like I use regularization in many, many models.
So, so just because it doesn't that sound that
complicated or maybe won't even take that long to explain today,
don't underestimate how widely used it is.
It's used in- it's not used in
every single machine learning model but it's used very, very often.
Um, so here's the idea,
um, which is- let's take linear regression.
Right. So that's the optimization objective for linear regression.
Um, if you want to add regularization, uh,
you just add one extra term here,
uh, Lambda, uh, times norm of,
uh, Theta squared, right?
Sometimes you write Lambda over two to make some of the derivations come out easier.
And what this does is it takes your cost function for logistic regression,
uh, which you try to minimize, try to minimize the square
error fit to the data and you're creating
an incentive term for the algorithm to make the parameter's Thetas, uh, smaller, okay?
So this is called a regularization term.
And it turns out that, um,
let's take the linear regression overfitting example, right.
So you know if you set Lambda equal to0,
then it's just linear regression over the fifth order polynomial features.
Uh, it turns out that as you increase Lambda,
you know, Lambda to some intermediate value,
uh, depending on the scales of data.
Let's say you said Lambda equals 1.
Then, when you solve for this minimization problem,
or this augmented problem for the value of Theta, um,
this term penalizes the parameters being too big and it turns out that you
end up with a fit that looks a little bit better, right?
It maybe it looks like that, okay?
Um, and by preventing the parameters Theta from being too big,
you make it harder for the learning algorithm to overfit the data and it turns out
fitting a very high order polynomial like that may result in
value of states that is very large, right?
Um, and, and then if you set Lambda to be too large,
then you actually end up,
um, in an underfitting regime, okay?
So there'll usually be some optimal value of Lambda where if Lambda equals 0,
you're not using any regularization.
You're so- maybe overfitting.
Um, if Lambda is way too big,
then you're forcing all the parameters to be too close to 0.
Um, in fact actually, if you think about it,
if Lambda was equal to 10 to the 100 or some ridiculously large number,
then you're really forcing all the Thetas to be 0, right?
If all the Thetas is 0, then you know then you're kinda fitting the straight line, right?
So that's if Lambda equals, uh, 10 to the 100.
And so- and this is a very simple function which is the function 0, right?
And, and this function h of Theta,
x equals 0, right, approximately 0.
It is a very simple function which you get if you set Lambda very large.
And by doubling Lambda between, you know,
a far too large value like
10 to the 100 compared to a far too small value like Lambda 0, you, you,
you smooth the interpolate between this much too simple function
of h equals 0 and a much too complex function, okay?
Um, so there is,
um- so that's pretty, uh,
it, it, it, um,
it- so that's pretty much it for regularization in
terms of what you need to implement but you feel
like your learning algorithm may be overfitting,
um, add this to your model and solve this optimization problem,
um, and it will help relieve overfitting.
Um, more generally, if you are, um, let's see.
More generally if you have a, uh,
say logistic regression problem where this is your cost function.
Then to add regularization,
I guess instead of min this is a max, right?
If you're applying logistic regression, uh,
then this was the original cost function, um,
then you can have minus [NOISE] Lambda or Lambda over 2,
right, it just depends on scaling of Lambda times
the norm of Theta squared and there's a minus here because for logistic regression,
we're maximizing rather than minimizing.
Or this could be argmax at any of the generalized linear model family as well.
But by subtracting Lambda times the norm of Theta squared,
this allows you to also regularize
the classification algorithm such as logistic regression.
Okay? [NOISE] Um, it turns out that, uh,
and- and I- I- I- wan- I make an analogy that, uh,
where all the math details are true,
but we don't wanna talk about all the math details.
It turns out that, um,
one of the reasons the support vector machine doesn't
overfit too badly even though it has,
you know, been working in infinite like,
you know, infinite dimensional feature space, right?
So- so why- why doesn't a support vector machine just overfit like crazy?
We showed, uh, on Monday that by using kernels,
it's sort of using infinite dimensional feature space, right?
So why doesn't it always fit these crazy complicated functions,
it just overfits the dataset like crazy?
It turns out and the theory is complicated.
It turns out that, um,
[NOISE] you know, the optimization
objective of the support vector machine was to minimize the norm of w squared.
Uh, this turns out to, uh,
correspond to maximizing the margin,
the geometric margin SVM,
and it's actually possible to prove that, um,
this has a similar effect as that, right?
That this is why the support vector machine despite working in
infinite dimensional feature space sometimes, um,
by forcing the parameters to be small is difficult for
the support vector machine to overfit the data too much.
Okay? The theory to actually show this is quite complicated.
Uh, um, uh, yeah, er,
uh, it's actually very- yeah,
is to show that the cost of cost Phi is where this is- where norm of
w small cannot be too complicated, complicating can overfit basically.
Um, but that's why, uh, SVMs can
work in- can work in infinite dimensional feature spaces. Yeah?
[inaudible]
Oh, sure. Do you ever regularized per elements of parameters?
Um, not really.
Uh, and the problem with that is, um,
you know, let me give one- let me give one more specific example,
then come back to that, right?
So it turns out that, um,
uh, so we talked about Naive Bayes as a text classification algorithm.
[NOISE] It turns out that,
um, let's see if the text classification algorithm problem,
you know, classify spam, non-spam,
or classified it to a sentiment,
possible negative sentiment of a tweet or something.
[NOISE] Let's say you have 100 examples,
but you have [NOISE] 10,000 dimensional features, right?
So let's say your features are these, you know,
take the dictionary A, aardvark and so on.
So 101, right.
So let's say you construct a feature like this, um,
it turns out that if you fit logistic regression to this type of data,
where you have 10,000 parameters and 100 examples,
this will badly- this will probably overfit the data,
um, because you have, uh,
uh, but it turns out that if you use logistic regression with regularization,
this is actually a pretty good algorithm for text classification, um,
and this will usually i- in terms of performance accuracy, um, yeah,
because this is logistic regression,
you need to implement gradient descent or something to solve local value parameters.
But logistic regression with regularization for text classification,
will usually perform- outperform Naive Bayes
o- on a- on a classification accuracy standpoint.
Uh, without regularization, logistic regression will badly overfit this data, right?
Um, and- and to- to explain a bit more, um, you know,
imagine that you have a three-dimensional subspace where you have two examples.
Then all you can do is fit a straight line,
right, for the hyper-plane to separate these two examples.
But so one rule of thumb for,
um, logistic regression is that,
if you do not use regularization,
it's nice if the number of examples is at least on
the order of the number of parameters you want to fit, right?
So this is if you're not using regularization.
It's nice if- in fact,
I- I personally think that, uh,
I tend to use the duration only if the number of
examples can be maybe 10x bigger than the number of examples,
uh, because that's what you need to have enough information
to fit good choices for all these parameters,
um, but that's if you're not using regularization.
But if you are using regularization, then, um,
you can fit, you know,
even 10,000 parameters, right?
Even with only 100 examples,
and this will be a pretty decent,
um, text classification algorithm.
Okay? Um, the question you have just now: why don't we regularize per parameter, right?
So why don't we, uh, let's see.
I guess instead of Lambda [NOISE] norm of Theta squared,
it would be a sum over j Lambda j,
you know, Theta j squared, right?
Um, the reason we don't do this is because you then end up with,
if you have 10,000 parameters here,
you end up with another 10,000 parameters here,
and so choosing all these 10,000 Lambdas is
as difficult as just choosing all these parameters in the first place.
So we don't have good weights to do this.
Whereas, when you talk about cross-validation, multiple regression a little bit,
we'll talk about how to choose maybe one parameter Lambda,
but that- those techniques won't work for choosing from
10,000 parameters Lambda j. You've got a question?
[inaudible]
You're absolutely right. Yes. Thank you. Um, yes.
So in order to make sure that the different Lambdas on the similar scale, uh,
a common pre-processing step we're using learning algorithms is, uh,
take your different features, um,
so for text classification of all the features is 01,
you can just leave the features alone.
But if a housing classification,
if feature one is the size of house which I guess ranges from,
I don't know, 100 to,
uh, how big are the biggest houses?
I don't know, like whatever.
Let's say houses go from, I don't know,
five inches square feet to 10,000 square feet.
Ten thousand square feet is really really big for a house, I guess.
But the numb- feature x2 is the number of
bedrooms which probably ranges from like, I don't know,
one to- I guess there's some houses with a ton of bedrooms,
but I would say most houses have at most five bedrooms,
I don't know, right?
Then these features are on very different scales and, uh,
normalizing them to all be on a similar scale,
so subtract out the mean and divide it by the standard deviation.
So scale all of these things to be between,
you know, 01 over 2 minus 1,
um, to 1, would- would- would be
a good pre-processing step before applying these methods.
Um, it turns out that this will make gradient descent run faster as well,
as a common pre-processing step to scale
each individual feature to be on a similar range of values.
All right. Yeah? At the back?
Uh, can we quickly go back to some more support vector machine model like NLG?
So it's actually both, so just to
repeat it, why-why don't support vector machines suffer too badly,
is it because it's small numbers for vectors or is it because of
minimizing the penalty W. Um,
I would say the formal argument relies more on the latter.
So it turns out that if you look all the class- if you're looking
at all the class of functions separate the data with a large margin, ah,
that class has low complexity formalized by
low VC dimension which you'll learn about in
Friday's discussion section if you want to come to that.
And so, it turns out that the class of all functions that separate the data of
a large margin is
a relatively simple class of functions by- and by simple class functions,
I mean, it has low VC dimension.
We should talk about this Friday.
Um, and thus any function within that class of functions,
uh, is not too likely to over-fit.
So, um, it is convenient
the support vector machine has a relatively low number of support vectors.
But, um, uh, you could imagine
other algorithms of a very large number of support vectors,
uh, but smallest to large margin is still a low complexity class that will move with it.
Alright, next one question.
I'm sorry say that again.
[inaudible]
Oh, sure yes. So is it possible that so yes.
So one of the- so yes.
So in general, models that have
high bias tend to underfit and models have high variance tend to overfit.
Um, we use these terms over-fit high variance,
underfit high bias not quite and they have very similar meanings.
Right, at their first approximation assume they, they mean the same thing.
One thing we'll see later,
uh, two weeks from now is,
uh, we'll talk about algorithms with high bias and high variance.
So, uh, this is, uh,
and actually one way to think of high bias and high variance,
we will talk about this later,
is if you have a dataset that looks like this,
uh, and if somehow your classifier has very high complexity,
there is a very, very complicated function.
But for some reason it's still not fitting your data well right,
so that would be one way to have high bias and high variance which does
happen. All right. Cool.
[NOISE].
So to wrap up the discussion on regularization, um,
there's one- so mechanically the way you implement
regularization is by adding that penalty on the norm of the parameters,
uh, so that's what you actually implement.
It turns out that, um,
there's another way to think about regularization.
So you remember when we talked about the new, uh,
linear regression we talked about minimizing squared error and then later on we saw that
linear regression was maximum likelihood estimation
on a certain generalized linear model using,
uh, using, using, using a Gaussian distribution as
the choice for the exponential family as a member of the exponential family.
It turns out that, um,
there's a similar point of view you can
take on the regularization algorithm that we just saw.
Which is, let's say S is the training set.
[NOISE].
Right. So, um, given a training set,
um, you want to find the most likely value of Theta, right?
Um, and so by Bayes rule P of Theta given S is P of S given Theta
times P of Theta divided by P of S. And
so if you want to pick the value of
Theta that's the most likely value of Theta given the data you saw,
then because the denominator is just a constant,
this is arg max over Theta of P of S given Theta times P of Theta.
Um, and so if you're using
logistic regression then the first term is this.
Right and in the second term is P of Theta,
um, where this is the,
you know, logistic regression model say,
right or any generalized linear model.
And it turns out that, um,
if you assume P of Theta is Gaussian.
So if you assume P of Theta is follow Theta.
The prior probability on Theta is Gaussian with mean 0 and,
uh, some variance tau squared i.
So in other words P of Theta is,
you know, 1 over root 2 pi, um,
I guess this would be the determinant of tau squared i, right,
e to the negative, um,
Theta transpose, uh, tau squared i inverse.
Right. So the Gaussian probability as follows.
It turns out that if, um,
this is your prior distribution for Theta and you plug this in
here and you take logs computer maps and so on,
then you end up with exactly the regularization technique that we found just now.
Okay. Um, and so in everything we've been doing so
far we've been taking a, um, frequentist interpretation.
I guess the two main schools of statistics are
the frequentist school of statistic and the Bayesian school of statistic,
um, and there used to be some titanic academic debates about which is the right one,
but I think, uh,
statisticians have gotten together and kind of made
peace and then go freely between these two more and more these days.
Maybe not now all the time but, uh,
but then the frequency score statistic.
We say that there is some data and we want to find, um,
the value of Theta that makes the data
as likely as possible and that's where we got maximum likelihood estimation right.
And in the frequentist school of statistics,
we view them as being some true value of Theta out in the world that is unknown.
Um, and so there is some true value of Theta that generated
all these housing prices and our goal is to estimate this true parameter.
In the Bayesian school of statistics we say that Theta is unknown.
But before you see even any data you already have some prior beliefs about how
housing prices are generated out in the world and
your prior beliefs are captured in a probability distribution,
uh, denoted by P of Theta,
so this is called the Gaussian prior.
And, um, we say that,
um, and- and if you look at this Gaussian prior.
[NOISE].
Excuse me. It's quite reasonable.
It's saying that before you've seen any data
on average I think the parameters of theta have mean
0 because I don't know if each Theta is positive or
negative so giving the mean 0 seems reasonable.
And most things in the world are Gaussians and we just
assume that my prior on Theta is Gaussian.
So you know, we could debate that this is, uh,
the right assumption but it's not totally unreasonable, right?
But they say well, for actually I think, you know,
for the next linear regression problem I'm
gonna work on next week and I have no idea what I'm going to work on,
where I'm going to apply linear regression that next week.
It is actually not too bad an assumption to say,
you know, my prior is Gaussian.
And in the Bayesian view of the world,
our goal is, um,
to find the value of Theta that is most likely after,
um, we have seen the data.
Okay. And so this is called map estimation.
Which stands for the maximum a posteriori estimation.
So this is actually the map estimator I guess the arg max of this, right?
Uh, as the map or the maximum a posteriori estimates of Theta which means,
look at the data, compute
the Bayesian posterior distribution of Theta
and pick the value of Theta that's most likely.
Okay. And so one of the things you do in the problem set that was just released, um,
is-is actually show this equivalence as well
as plugged in a different prior
for theta other than the Gaussian prior you experiment with,
uh, whether P of Theta is the Laplace prior and to
find a derive a different map as mean algorithm.
Okay. Um, all right, good. Yeah, question?
[inaudible].
Sorry can you say that again?
[inaudible]
Uh, yes.
[inaudible]
Oh I see, yes, can difference between
these two be seen as regularized versus non-regularized?
Yes. So, so, um,
MLU here corresponds to the origin of regularization,
uh, and this procedure here corresponds to adding regularization.
Um, it turns out that frequency statistic- statisticians can also use
regularization it just that they don't try to
justify it through a Bayesian prior they just say,
so if you're a frequentist statistic.
If you're a frequentist statistician your job is to wake up and come up with
an algorithm to estimate this you know true value of theta that exists out in the world,
and you can come up any procedure you want and to inspire your procedure,
you can add a regularization term.
I think there's a lot of these debates between frequentists and Bayesians are more philosophical.
I think there's a machine learning person, as an engineer.
I don't really you know,
I think the philosophical debates are lovely but I just- I,
I just like my stuff to work.
So, so, so frequentists can also infer regularization.
It just that they say this is part of the algorithm
they invented rather than derived from a Bayesian prior.
All right, cool. So, um,
[NOISE] all right.
Let's talk about um, so in,
in our discussion on regularization and choosing the degree of polynomial,
um, uh, all right.
So let's see, let's say I plot a chart where on the horizontal axis I plot,
um, [NOISE] model complexity.
So how complicated is your model?
So for example, uh, to the right of this curve could be a very high degree polynomial.
[NOISE] Right.
Um, and what you find is that as you
increase model complexity your training error- if you do not regularize, right?
So if, if you fit a
linear function, cosine function,
cubic function and so on.
You find that the higher the degree of
your polynomial the better your training error because you know,
a fifth-order polynomial always fits the data better than a fourth-order polynomial.
If you, if you do not regularize.
But what we saw with the original picture was that the ability of the algorithm
[NOISE] to generalize kind of goes down and then starts to go back up, right?
And so if you were to have a separate test set and evaluate
your classifier on a set of data that the algorithm hasn't seen so far,
so measure how well the algorithm generalizes to a different novel set of data,
then if you fit a linear function then this underfits [NOISE].
If you fit a fifth-order polynomial this overfits,
[NOISE] and there is somewhere in
between right, that is just right.
Okay? And um, this curve is true for regularization as well.
So say you apply linear regression
with 10,000 features to a very small training example.
If lambda was much too big then they will um, underfit.
If [NOISE] lambda was 0 so,
you're not regularizing at all then they will overfit,
and there will be some intermediate value of
lambda that is not too big and not too small that you know,
balances overfitting and underfitting.
Okay. So, um, what I'd like to do next is describe uh, a mechanistic.
A few different mechanistic procedures for trying to find this point in the middle,
right? And so [NOISE]
um, so given a data set
[NOISE]
what we'll often do is um,
take your data set and split it into different subsets,
uh, and a, a,
a good hygiene is to take a data in the trained set- train, dev and test sets, um,
So if you have say,
10,000 examples, all right,
and you're trying to carry out this model selection problem.
So for example, let's say you're trying to decide what
order polynomial you want to fit, [NOISE] right.
Or you're trying to choose the value of lambda, um,
or you're trying to choose the value of tau,
that was the bandwidth parameter in uh,
locally weighted regression that you saw in the problem set- that we saw with,
uh, locally weighted regression, all right?
So, um, or you're trying to choose a value C in a support vector machine.
So remember, the SVM objective was actually this, right.
With the you know, subject to some other things but for
the O unknown soft margin that we talked about on Wednesday- uh, talked about on Monday.
You're trying to minimize the normal W and then there was this additional parameter C
that trains off how much you insist on
classifying every training example perfectly. All right.
So whether you're trying to make- which of these decisions you are trying to make,
um, how do you,
uh, you know, choose a polynomial size or choose lambda or choose
tau or choose parameter C which also has this bias-variance trade-off.
There'll be some values of C that are too large and some values of C that are too small.
[NOISE]
So here's one thing you can do which is um,
uh, let's see, so
split your training data S into a subset which I'm gonna call the uh,
raw training set as subscript train,
um, and then some subset which we wanna call S subscript dev.
And dev stands for uh, development,
[NOISE] um, and then later we'll talk about [NOISE] a separate test set.
And so what you can do is train each model,
and by model I mean, um,
option [NOISE] for the degree of polynomial
[NOISE] on S train.
Um, so you're evaluating a menu of models, right?
So let's say, this is model 1,
model 2, and so on up to model 5, up to some number.
They can train each of these models, uh,
on the first subset of your data [NOISE] and then get some hypothesis.
Let's call it h_i, [NOISE] um, and then,
[NOISE] measure the error
on S dev which is a second subset of your data called the development set.
And pick the one- [NOISE]
Okay. So rather than- and- and- uh,
I wanna contrast this with an alternative procedure, right?
So the two sets of the da- two subsets of the data,
some test set data, training set, and development set.
And uh, after training, uh,
first order polynomial, second order polynomial,
third order polynomial on the training set,
we evaluate all of these different models on
the separate held-out developments sets and then pick
the one with the lowest error on the development set.
Okay, um, but the one thing to not do would be to evaluate
all these algorithms instead on the training set and then pick
the one with the lowest error on the training set, right.
Why not- wha- what goes wrong when you do that? Yeah.
[BACKGROUND] [inaudible]
Yeah, right, you just over-fit.
How- why- why will you over-fit?
[BACKGROUND] Parts of the error,
what you want to remain so don't want- [inaudible]
Yeah. Yep, cool, right. So if you use this procedure,
you'll always end up picking the fifth order polynomial, right.
Because the more complex algorithm will always do better on the training set.
So if you do this, this will always cause you to say,
let's use the fifth order polynomial or the- or the highest possible order polynomial.
So this won't help you realize in the housing price prediction example to
the second order polynomial is the benefit of the data, right. Does it make sense?
Um, and that's why for this procedure,
um, if you evaluate your, uh,
model's error on a separate development set
that the algorithm did not see during training,
this allows you to hopefully pick a model that neither over-fits nor underfits.
And in this example, hopefully,
you find that uh, there will be the second-order polynomial,
right, that the one that's just right in between that actually does
best on your development set, okay.
Um, all right.
Now, uh, and then, um,
you know if- if you are, uh,
if- if you are publishing an academic paper on machine learning um,
then, this procedure has looked at
the training set as well as the development set, right.
So this- this procedure,
this piece of code is,
you know, is two in these decisions.
Uh, it's two in the parameters, the training set,
and it's two in the decision on the degree of polynomial to the dev set.
And so if you want to know,
if you want to publish a paper that say, oh,
my algorithm achieves 90% accuracy on this dataset um,
it's not valid to report the results on
the dev set because the algorithm has already been optimized to the dev set.
In particular, information about what's the most- um,
what's the best uh, degree of polynomial to choose
was derived from the dev set from the development set.
And so if you're publishing a paper or you want to report an unbiased result, um,
evaluate the algorithm on a separate test set,
S test and report that error, okay.
And so if you're publishing a paper,
it's considered good hygiene to um, uh,
report the error on a completely separate test set that you did
not in any way shape or form look at during the development of your model,
during the training procedure, okay.
Clear with things? Oh, yeah.
Are dev and test [inaudible] is uh,
generally different by much?
Um, a dev and test set's error isn't strictly different by much.
It depends on the size of- it depends on the size of the dataset.
Um, uh, and so it turns out that um,
actu- let- let- actually let me- let me give an example, actually.
So let's say you're trying to fit a degree of polynomial, right.
Um, and you want to choose uh,
uh, write the dev error.
So we can fill the first, second,
third, fourth, fifth degree polynomial.
And so um, after fitting all of these,
lets say that the square error right,
to use round numbers is 10, um,
5.1, 5.0, 4.9, you know,
um, 7, 10, and so on, okay.
Just to- just to use round numbers for illustrative purposes.
If you're using the dev error to pick the best hypothesis,
to pick the best hot spot,
you would say that uh,
using the fifth order polynomial gets you 4.9 squared error, right.
But did you really earn that 4.9 square error or did you just get lucky?
Because there is some noise and so maybe
all of these actually have error that close to 5.0.
But some are just higher, some are just lower,
and you just got a little bit lucky that on the dev set this did better.
Which is why, if you look at your dev set error,
your dev set error is a biased estimate, right.
And so where's your very large test set?
If it's a very large test set,
maybe the true numbers are 10, 5, 5,
5, 7, 10 are your actual expected squared errors.
It's just that um,
because of a little bit of noise you got lucky and reported 4.9.
And so this would be a bad thing to do in an academic paper, right.
Because it's uh, what you earned was an error of 5.0 you didn't earn an error of 4.9.
It's just that- because you're over-fitting a little bit in the dev set.
Um, you chose the thing that looked best for the dev set,
but your algorithm didn't actually achieve that error, it's just because of noise, okay.
So- so um, now in- in so- so it's considered a good practice to report um,
uh, uh, so reporting on the dev error isn't- isn't-
isn't really a valid unbiased procedure.
And- and uh, um, yeah. Do you have a question?
[BACKGROUND]
[inaudible]
Yeah. Ye- so- so one of the just to we say,
I- I yes, you're right.
One of the problems with some of the machine learning benchmarks that people worked on
for a long time is this is unavoidable mental over-fitting.
The people'd gotten to use the dataset and everyone's working the
same trying to publish the best numbers from the same test set.
So the academic committee on machine learning does have some amount of over-fitting uh,
to the standard benchmarks that people have worked on for a long time.
And this is an unfortunate result uh,
when the test is very- very large,
the amounts of over-fitting is probably smaller,
but when the test set is not big enough then the over-fitting result can cause um,
sometimes even research papers to uh,
to publish results that are uh,
probably over-fit to the data set, right.
um, uh, and so I think there is actually uh,
one standard academic benchmark because there's a dataset called CIFAR, it's quite small.
It's actually the very same research paper uh, uh,
analyzing um, results on CIFAR uh,
arguing that some fraction of their progress that was made was actually perhaps uh,
researchers uni- unintentionally over-fitting to this dataset.
Okay. Oh and by the way um,
one thing I do when I'm building you know,
production machine learning systems.
So when I'm- when I'm shipping a product, right.
I just don't build a speech recognition system and just make it work.
I just wanna, and not- and if I'm
not trying to publish a paper, I'm not trying to make some claim.
Sometimes I don't bother to have a test set, right.
So and uh, and it means I don't know the true error of the system sometimes uh,
but I'm very conscious of that.
If I don't have a lot of data,
sometimes I'm may decide to just not have
a test set and it means I just don't try to report the test set number.
I can report that dev set number which I know is
biased and I just don't report the test set number.
Don't do this when you're publishing your academic paper, right.
This is not good if you're publishing a paper or making claims on
the outside but all we're doing is building a product and not writing a paper out,
this is- this is actually okay. Uh, yeah.
[inaudible]
Yeah. Okay, good. Uh, that's- lemme, lemme get to that.
Good. So, um, the next topic about setting up the train dev test split is,
how do you decide how much data should go into each of these three subsets?
Um, so uh, uh,
I can tell- so, so let me just tell you
the historical perspective and then a modern perspective.
Um, historically, the rule of thumb was you take a training set, right?
Take your training set S and then you would send- here,
one rule of thumb that you see a lot of people referring to is, uh,
70% training, right?
30% test.
[NOISE] This is one common rule of thumb that you just hear a lot.
Uh, or maybe you have- if you- if you don't have a dev set,
if- if- if you're not doing model selection,
if you just- if you've already picked the model and now you're revising.
Or maybe you have people use,
you know, 60% train,
20% dev, 20% test.
Right? So these are rules of thumb that people use to give.
Um, and these are decent rules of thumb when you don't have a massive dataset.
So you may have 100- 100 examples,
maybe you have 1,000 examples,
maybe several thousand examples,
I think these rules of thumb are perfectly fine.
[NOISE] Um, what I'm seeing is that as you move to machine learning problems with really,
really giant datasets, the percentage of data you send to dev and test are shrinking.
Right? And, and so, here's what I mean.
Um, let's say you have 10 million examples.
Um, you know, yeah,
decent size, not giant but like a reasonable size.
Um, so le- let's,
let's take this- this is actually
a pretty good rule of thumb if you have a small dataset.
If you have a thou- if you have, you know, 5 million examples,
this is a perfectly fine rule of thumb to use.
Um, but if you have 10 million examples,
then, you know, you have 6 million,
[NOISE] 2 million, [NOISE] 2 million,
right, train, dev, test.
[NOISE] And the question is,
do you really need 2 million examples to
estimate the performance of your final classifier?
Uh, sometimes you do if you're working on online advertising,
you know, which I have done,
and you're trying to increase your ad click-through rate by 0.1%,
because it turns out increasing ad click-through rate by 0.1%,
which I've done multiple times,
uh, turns out to be very lucrative.
[LAUGHTER] Uh, uh, then you actually need a very large dataset to measure these very,
very small improvements because to- to- to increasing ad click-through rate by 0.1,
you might have actually a lot of projects.
You might have 10 projects,
each of which increases ad click-through rate by 0.01%, right?
And so to measure these very different- small differences in,
algorithm one does 0.01% better than algorithm b
by- so you need a lot of data to tease out that very small difference.
So if you're in the business of teasing out these very small differences,
you actually need very large test sets.
But if you're comparing different algorithms and one algorithm is, you know,
2% better or even 1% better than the other algorithm,
then with 1,000 examples maybe, right,
1,000 examples may be enough for you to distinguish
between these much larger differences.
Um, so my recommendation for choosing
the dev and test sets is choose them to be big enough,
um, that you have
enough data to make meaningful comparisons between different algorithms.
Uh, and if you suspect your algorithms will vary in performance by 0.01%,
you just need a lot of data to distinguish that, right?
So, so if you have 100 examples,
then, you know, if,
if one algorithm has 90% accuracy and one algorithm has 90.01% accuracy,
then unless you have at least 1,000 examples and maybe 10,000 or more,
you just can't see this very small difference, right?
If you have 100 examples,
you just can't measure this very small difference.
So my, my advice is, uh,
choose your dev and test sets to be big enough that, um,
uh, you could see the differences in the performance of algorithms that you,
uh, tha- that you roughly expect to see.
Um, and then you don't need to make your dev and test sets much larger than that.
And I would usually then just put the data.
You don't need the dev and sets back in the training set.
So when you're working with very large datasets, say, you know,
a million or 10 million or 100 million examples,
what you see is that the percentage of data
that goes into dev and test tends to be much smaller.
So it might be, um,
uh, so you see for example,
maybe 90% train,
you know, 5% dev,
and 5% test, right?
Or, or, or even smaller,
or even 1%, 1% depending on how much data you really need.
To measure to the level of accuracy you
need the differences in the performance of your algorithms.
Okay? Cool. All right,
um, just to give this whole procedure a name,
um, what we just did here between the train and dev set,
this procedure that we have is called hold-out cross validation.
[NOISE] And sometimes,
to distinguish this from other cross validation procedures we'll talk about in a minute,
sometimes this is called simple hold-out cross validation.
We'll talk about some other hold-out cross validation procedures in a second.
Um, and, uh, uh,
and the dev set,
um, is sometimes also called the cross validation set.
Okay, right? Uh, so sometimes you- people use- sometimes,
you hear people say, you know,
we're gonna use a cross validation set.
That means roughly the same thing as a, as a dev set.
Okay? So in the normal workflow of developing a learning algorithm,
uh, when you're given the dataset,
I would split it into a training set and a dev set.
Oh, and I used to say cross-validation set,
but cross-validation is just a mouthful.
So I think just motivated by the reducing number of syllables,
because you're using this classifier so often,
more and more people just call it the dev set,
but it means roughly the same thing.
Right? So, so when I'm,
uh, building a machine learning system,
I'll often take the dataset,
split into train and dev,
and if you need a test set,
then also a test set,
um, and then, uh,
keep on fitting the parameters to the training set and, uh,
evaluating the performance of your algorithm on
the dev set and using that to come up with new features,
choose the model size,
choose the regularization parameter Lambda, um,
really try out lots of different things and spend, you know,
several days or weeks, uh,
to optimize the performance on the dev set.
Um, and then, uh,
when you want to know how well is your algorithm performing,
to then evaluate the model on the test set.
Right? And- and the thing to be careful not to do is to make
any decisions about your model using the test set, right?
Because then- then your scientific data to the test set is no longer an unbiased estimate.
Uh, one- so- and- and o- one thing that is actually okay to do is, um,
if you have a team that's working on the problem,
if every week they measure the performance on
the test set and report out on a chart, right?
You know, uh, the, the performance on the test set, that's actually okay.
You can evaluate the model multiple times on the test set.
You can actually give out a weekly report, saying, this week,
for our online advertising system,
we have this result on the test set.
One week later, we have this result on test set,
one week later, this result on the test set.
It's actually okay to evaluate your algorithm repeatedly on the test set.
Uh, what's not okay is to use
those evaluations to make any decisions about your learning algorithm.
So for example, if one day you notice that your model is
doing worse this week than last week on the test set,
if you use that to revert back to an older model,
then you've just made a decision that's based on the test set,
and- and your test set is no longer biased.
But if all you do is report out the result but not make
any decisions based on the test set
performance such as whether to revert to an earlier model,
then you can- I- I- it's actually legitimate it's actually okay to keep on,
you know, use, uh,
use the same test set to track your- your team's performance over time.
Okay. All right.
Good. Um, so when you have very large data sets,
this is the procedure if you're developing for defining the train dev
and test sets and this procedure can be used to choose the model of polynomial.
It can also be used to choose the regularization parameter Lambda or or
the parameter C or- or- or the parameter tau from now locally weighted regression.
Um, now, whenever you have a very small data-set, right?
Um, [NOISE] So it turns out that,
uh, so I'm gonna leave out the test set for now.
Le- let's just assume there is some separate test set.
I'm not gonna worry about that for now.
Um, but let's say you have 100 examples, right?
Um, if you're going to split this into, you know,
70 in the training set in S subscript train and 30 in S dev.
Then you're training your algorithm on 70 examples instead of 100 examples.
And so I've actually worked on a few healthcare problems.
Oh, actually, mo- most of my PhD students, uh,
including Annan, work,
doing a lot of work on, uh,
machine learning applied to health care.
And so we actually worked on a few data-sets in healthcare where, you know,
every training example corresponded to some patient that sometimes that, uh, you know,
unfortunate disease or- or- or if every- if you're working- or if, um,
every example corresponded to injecting a patient with a drug and seeing what happens to
the patient right sometimes there's literally a lot
of blood and pain that goes into collecting every example.
And if you have 100 examples to hold out 30 of them,
um, for the purpose of model selection using only 70 examples and 100 examples.
It seems like you're wasting a lot of data that was collected through a lot of,
you know, literal pain, right?
Um, so is there a way to say do
model selection such as choose the degree of polynomial without,
"Slightly wasting so much of the data."
There is a procedure that you should use only if you have a small data-set,
only if you're worried about the size of- oh,
and the other disadvantage of this is,
you evaluate your model only on 30 examples,
and that seems really small.
Right? You know can you- can you just find more data to evaluate your models as well.
So there's a procedure that you should use [NOISE] only if you have a small data-set,
uh, called k-fold cross-validation, or k-fold CV.
And this is, uh, in contrast to simple cross validation.
Um, but this is
the idea which is- let's say this is your training set S, so you have, you know,
X 1, Y 1 down to X say 100, Y 100.
[NOISE] What we're going to do is take the training set and,
uh, divide it into k pieces.
Um, so for the purpose of illustration, I'm gonna use k equals 5.
When I'm, just to make the- the writing on the board sane.
Uh, k equals 10, uh, is typical.
I guess, uh, for illustration.
[NOISE] But so what you do is, um,
take your data-set and divide it into five different subsets of- in this example,
you would have 20 examples.
100- 100 examples divided into five subsets,
so there are 20 examples in each subset.
And, um, what you do is,
for i equals 1 to k train i.e,
fit parameters on k minus 1 pieces.
And then test on the
remaining [NOISE] one piece,
and then you average.
Right? So in other words, uh,
when k is equals 5, we're going to loop through five times.
In the first iteration,
we're going to train on these and test on the last one fifth of the data. Um,
so we'll hold out the last one fifth of the data,
train on the rest and test on that.
And then in the second iteration, through this for loop,
we'll train on pieces 1 ,2, 3 and 5 and test on piece number 4,
and we get the number.
Um, and then you hold out this third piece,
train on the others, test on this, and so on.
So you do it five times,
where on each time,
you leave out one fifth of the data,
train on the remaining four-fifths and you evaluate the model on that final one fifth.
Okay? And so, um,
if you're trying to choose a degree of polynomial,
what you would do is- I guess for,
you know, D equals 1 through 5.
Right? So you do this procedure for a first order polynomial, uh,
fit- you fit a linear regression model five times,
each time on four-fifths of the model and test on the remaining one fifth,
and you repeat this whole procedure for the quadratic function.
Repeat this whole procedure for the cubic function, and so on.
And after doing this for every order polynomial from 1, 2, 3, 4, 5 you would then, uh,
pick the degree of polynomial that, um,
sorry and then for each of these models,
you then average the five S's you have for- for S error.
Okay? And then after doing this,
you would pick the degree of polynomial that did
best according to this- according to this metric.
Right? And then maybe you'll find that the second-order polynomial does best.
Um, and now you actually end up with, uh, five classifiers.
Right? Because you have five classifiers,
each one fits on four-fifths of the data,
uh, and then, uh,
and- and there's a- there's a final optional step,
which is to refit the model on all 100% of the data.
Right? So if you want,
you can keep five classifiers around and output their predictions,
but then you're keeping five classifiers around this.
Uh- uh maybe a bit more common to- now that you've chosen to use
a second-order polynomial to just refit the model once on all 100% of the data.
Okay? Um, and so the advantage of,
uh, k-fold cross validation is that,
instead of leaving out 30% of your data for your dev set on each iteration,
you're only leaving out 1 over k of your data.
I use k equals 5 for illustration, but in practice,
k equals 10 is by far the most common choice that we use.
I've sometimes seen people use k equals 20,
but quite rarely, but,
um, uh, if you use k equals 10,
then on each iteration,
you're leaving out just one tenth of the data.
10% of the data rather than 30% of the data.
Okay? Um, and so this procedure compared to simple cross-validation,
it makes more efficient use of the data,
because you're holding out you know only 10% of the data on each iteration.
Uh, the disadvantage of this is computationally very expensive,
that you're now fitting each model 10 times instead of just once.
Okay? But- but- but when you have a small data-set,
this- this is actually a better procedure than simple cross validation.
If you don't mind the computational expense of fitting each model 10 times.
This- this- this actually lets you get away with holding on this data.
[NOISE]
And then, um,
there's one even more extreme version of this,
which you should use, if you have very very small datasets.
So sometimes you might have an even smaller dataset.
You know, if you're doing a class project with 20 examples this-
that's- that's small even by today's machine learning standards.
So, uh, there's- there's an extreme version of k-fold cross-validation,
called leave-one-out cross validation,
which is if you set k equals m. Right?
So in other words, here's your training set, maybe 20 examples.
So you're gonna divide this into as many pieces as you have training examples.
And what you do is leave out one example, train on the other 19,
and test on the one example you held out.
And then leave out the second example,
train on the other 19 and test to the one example you held out,
and do that 20 times,
and then you average this over the 20 outcomes to
evaluate how good different orders of polynomial are.
Um, the huge downside of this is just is completely very- very expensive,
because now you need to change your algorithm m times.
So you, kind of,
never do this unless m is really small.
Uh, I personally have- I pretty much never use this procedure unless m is 100 or less.
I guess, you- your- if your model isn't too complicated,
you can afford to fit a linear regression model 100 times, like it's not too bad.
Right? So- so if- if- if m is,
uh, less than 100,
you could consider this procedure.
But- but if m is 1000,
fitting a linear model- fitting a model 1000 times,
it seems like a lot of work, then you usually use k-fold cross-validation instead.
Uh, but if you do have 20 examples,
then you know, I- I would then- then- if you have 20 examples,
I would probably use this procedure and somewhere between 20 and
50s maybe when I switch over from leave-one-out to k-fold cross-validation. Okay? Yeah.
In 10- fold cross validation should we use [inaudible] k times to go.
Yeah. So, um, right.
So since you have k estimates,
say 10- 10 estimates,
we're using 10-fold cross-validation.
Can you measure the variance on those 10 estimates?
Um, it turns out that those 10 estimates are
correlated because each of the 10 classifiers,
eight-eight-eight-eight out of nine of the sets of data they trained on overlap.
So, um, there were some very interesting results,
uh-uh there's some research papers written by Michael Kearns,
actually, um, it's like a long time ago, uh,
trying to understand how correlated are these 10 estimates.
And from a theoretical point of view,
the- we- the- as far as I know,
the latest error result shows that this is not a worse estimate in training error,
but note- but- but maybe it's showing us in
practice is not- you could measure it, but, uh,
we don't really trust that estimate of variance,
because we think all 10 estimates are highly correlated,
or- or at least somewhat correlated.
Yeah. Go ahead
[inaudible]
Whether we're gonna find using k-fold cross-validation in deep learning?
Um, if you have a very small training set, then maybe yes.
But deep learning algorithms depend on the details.
Right? Sometimes it takes so long to train,
that training- training- training on a neural network 20 times,
you know, seems like a pain unless- unless you have enough data.
Unless, um, your neural network was quite small.
Right? Um, so it's rarely done with a deep learning algorithm.
But if you have so- frank- frankly if you have so little data,
if you have 20 training examples,
uh - uh, you know there are
other techniques that you probably need to use to boost performance.
Such as transfer learning,
or just more heterogeneity of input features,
or something else. Right? Um, yeah.
[inaudible]
Sorry say again.
[inaudible].
Sorry thank you for asking that,
uh, this average set no.
Um, I meant, um,
averaging the test errors.
So, uh, here, you will have trained 10 classifiers and,
you know, when you evaluate it on the left at 110 for the data,
you get it wrong- you get a number.
Right? So you're looping 10 times, hold at one part, train on the others.
Test on this part you left out.
And so that will give you a number,
and they go say oh when you test on test on this part you left out,
the squared error was 5.0 and then you do it again,
squared error was 5.7, squared error was 2.8.
So by average I meant average those numbers.
And the average of those numbers is your estimate of the error of a,
you know, third order polynomial for this problem.
So this is an averaging the set of real numbers that you got from
this- so- so this loop gives you k real numbers, uh,
and so this is averaging those k real numbers to estimate for this outer loop,
how good a classifier with that degree polynomial is.
Okay? Wow, actually, a lot of questions,
there's one thing I want to cover, go ahead and this last two go ahead.
[inaudible]
I see. Sure. Yes, using something other than
F1 score would it just mean other than average?
Uh, yes, it would.
Having F1 score is complicated.
Yes. Uh, I think,
I think we'll talk- actually, um,
so this week, Friday, we'll talk about learning theory.
Next week- next Friday we're talking more about performance evaluation metrics.
So actually we will talk about F1 score?
Uh, Mitch, one last question?
How do you sample the data in the, in these sets?
Oh, sure. How do you sample the data in these sets?
Um, so for the purposes of this class,
assuming all your data comes through the same distribution,
uh I, I, I,
I would usually randomly shuffle.
Uh, again, in the era of machine learning and big data,
there's one other interesting trend is which,
which wasn't true 10 years ago which is we're
increasingly trying to train and test on different sets.
Uh, uh, we're trying to,
you know, train on data, uh,
collect it in one context and apply it to a totally different context.
Uh, such as, um,
we're trying to, uh,
uh, you know, train on,
on speech collected on your cellphone because you have all that data,
and trying to apply it to a, um, uh,
uh to a smart speaker where it was collected on a different microphone,
in your cellphone or something.
So, uh, if you are doing that and the way
you set your train dev test split is a bit more complicated.
Um, I wasn't going to talk about it in this class.
If you want to learn more, uh, ah,
I think at the start of the class,
I mentioned I was working on this book, Machine Learning Yearning.
So that book is finished.
And if you go to this website,
you can get a copy of it for free.
Uh, uh, that talks about that.
Uh, and I also talk about this more in CS230 which,
which goes more into the big data.
But you can, you can go,
go and learn machine- you can also read all about it in,
in Machine Learning Yearning.
Um, if the train and test sets are a different distribution.
Uh, yeah, but random shuffling would be a good default if you think you're training
dev test on two different, right? All right.
Just one last thing I want to cover real quick which is, um, feature selection.
And so, um, so let me just describe what- so sometimes you have a lot of features.
Um, so, so actually let's take text classification.
You might have 10,000 features corresponding to 10,000 words,
but you might suspect that a lot of the features are not important, right?
You know the word the,
whether the word the is called a stop word,
whether the word the appears in e-mail or not,
doesn't really tell you if it's spam or not spam because the word the,
a, of, you know,
these are called stop words.
They don't tell you much about the content of the email.
Um, but so if a lot of features, uh,
sometimes one way to reduce overfitting is to try to
find a small subset of the features that are most useful for your task, right?
And so, um, this takes judgment.
There are some problems like computer vision where you have
a lot of features corresponding to there being a lot of pixels in every image.
But probably, every pixel is somewhat relevant.
So you don't want to select a subset of pixels for most computer vision tasks.
But there are some other problems where you might have lot of features when you suspect
the way to prevent overfitting is to find
a small subset of the most relevant features for your task.
Um, so feature selection is a special case of model selection
that applies to when you suspect that even though you have 10,000 features,
maybe only 50 of them are highly relevant, right?
And so, um, uh one example,
if you are measuring a lot of things going on in a truck,
uh, in order to figure out if the truck is about to break down, right.
You, you might, uh, for, for preventive maintenance,
you might measure hundreds of variables or many hundreds of variables,
but you might secretly suspect that there are only a few things that,
you know, predict when this truck is about to go down,
so you can do preventive maintenance.
So if you suspect that's the case,
then feature selection would be a reasonable approach to try, right?
And so, um, here's the- I'll,
I'll just write out one algorithm, uh,
which is start with- this is script f equals the empty set of features.
And then you repeatedly try adding each feature i
to f and see
which single feature addition
most improves the dev set performance, right?
And then Step 2 is go ahead and connect to add
that feature to f, okay?
So let me illustrate this with pictures.
So let's say you have, um, five features,
x1 through x5, and in practice it's usually more like x1 through x500 or x1 through 10,000,
but I'll just use 5.
So start off with an empty set of features and,
you know, train a linear classifier with no feature.
So the model is, um,
h of x equals theta 0, right?
With no features.
Uh, so this won't be a very good model.
But see how well this does on your dev set.
Uh, so this way you average the ys, right?
So it's not for your model.
Next- so this is step one.
In the second iteration,
you would then take each of these features and add it to the empty sets.
You can try the empty set plus x1,
empty set plus x2,
empty set plus x5.
And for each of these, you would fit a corresponding model.
So for this one you fit h of x equals theta 0 plus theta 1 x5.
So try adding one feature to your model,
and see which model best improves your performance on the dev set, right?
And let's say you find that adding feature two is the best choice.
So now, what we'll do is set the set of features to be x2.
For the next step, you would then consider starting of x2 and adding x1,
or x3, or x4, or x5.
So if your model is already using the feature x2,
what's the other feature,
what additional feature most helps your algorithm?
Um, and let's say it is x4, right?
So you fit three or four models, see which one does best.
And now you would commit to using the features x2 and x4.
Um, and you kind of keep on doing this,
keep on adding features greedily,
keep on adding features one at a time to see which single feature addition,
um, helps improve your algorithm the most.
Um, and, and, and you can keep iterating
until adding more features now hurts performance.
Uh, and then pick what- whichever feature subset
allows you to have the best possible performance of dev set, okay?
So this is a special case of model selection called forward search.
It's called forward search because we started with a empty set of features,
and adding features one at a time.
There's a procedure called backwards search which we'll read about that.
We install all the features and remove features one at a time.
But this would be a reasonable,
uh, uh feature selection algorithm.
The disadvantage of this is it is quite computationally expensive, uh,
but this can help you select a decent set of features, okay?
Um, so we're running a little bit late, uh, let's break.
Oh, so I think, uh,
I was meant to be on the road next week but, uh,
because [inaudible] is still unable to teach,
I think we will have, uh,
Rafael, uh, uh, teach, uh,
decision trees next week,
and then also Kian will talk about neural networks next week, okay?
So let's break for today, um, and,
and maybe we'll see some of you at the Friday discussion session.
 Okay. Welcome everyone. So, um,
today we'll be going over learning theory.
Um, this is, um,
this used to be taught in the main lectures in- and in previous offerings.
Ah, this year we're gonna cover it as,
ah, as a Friday section.
Um, however, some of the concepts here are,
ah, we gonna be covering today are- are,
um, important in the sense that they kind of deepen
your understanding of how machine learning kind of works under the covers.
What are the assumptions that we're making and you know,
um, why do things generalize,
um, and- and so forth.
So here's the rough agenda for today.
So, ah, we're going to quickly start off with, ah,
framing the learning problem and, ah,
we'll go deep into bias-variance, um, trade off.
We'll go- we'll spend some time over there and we look at, uh,
some other ways where you can kind of, ah,
decompose the error, ah,
as approximation error and estimation error.
Um, we'll see what empirical, ah,
risk minimization is and then we'll spend some time
on uniform convergence and, um, VC dimensions.
So, ah, let's jump right in.
Right. So the, um,
so the assumptions under which we are going to be operating, um,
for- for this lecture and in fact for most of- most of
the- the algorithms that we'll be covering in this course,
um, is that there are two main assumptions.
One is that there exists a data distribution,
distribution D from which x y pairs are sampled.
So this is, ah, this makes sense in the supervised learning setting where,
um, you're expected to learn a mapping from x to y.
But, ah, the assumption also actually holds
more generally even in the unsupervised, ah, setting case.
The- the main assumption is that there is
a ge- data-generating distribution and the examples that we have in our training set,
and the ones we will be encountering when we test it,
ah, are all coming from the same distribution.
Right. That's- that's like the core assumption.
Um, without this, um,
coming up with any theory is- is- is gonna be much harder.
So the assumption here is that you know, um,
there is some kind of a data ge-, ah, generating process.
And we have a few samples from the data generating
process that becomes our training set and that is a finite number.
Um, you can get an infinite number of samples from this data generating process,
and the examples that we're gonna encounter,
ah, at test-time are also samples from the same process.
Right. That's- that's the assumption.
And there is a second assumption.
Um, which is that all the samples are sampled independently.
Um, so, um, with these two assumptions, ah,
we can imagine a learning,
ah, the process of learning to look something like this.
So, we have a set of x y pairs which we call as s. Um,
these are just x 1, y 1,
x m y m. So we have m samples from- from- sample from the data
generating process and we feed this into
a learning algorithm and
the output of the learning algorithm is what we call as a hypothesis.
Hypothesis, ah, is- is a function, um,
which accepts an input- a new input x and makes a prediction about- about y for that x.
So, ah, this hypothesis is sometimes also in the form of Theta hat.
So if we- if we restrict ourselves to a class of hypothesis.
For example, ah, all possible logistic regression models of,
ah, of dimension n, for example,
then, um, it's, you know, um,
obtaining those parameters is equivalent to obtaining the hypothesis function itself.
So a key thing to note here is that this s is a random variable.
All right.
This is a random variable.
This is a deterministic function.
And what happens when you feed a random variable through
a deterministic function you get a? Random variable.
Exactly. So, um, the hypothesis that we get is also a random variable.
Right. So all random variables have a distribution associated with them.
The distribution associated with the data is the distribution of- of capital D. Um,
this just a fixed, ah, deterministic function.
And there is a distribution associated with the,
um, um, with the- with the parameters that we obtain.
That has a certain distribution as well.
In, um, in the sta- in- in a more statistical setting,
um, we call this an estimator.
So if you take some advanced statistics courses you will call,
ah, what you will come across as an estimator.
Here we call it a learning algorithm.
Right, and the distribution of Theta,
um, is also called the sampling distribution.
And the, um, what's implied in this process is that there exists some Theta star,
ah, or in A star.
However you want to view it which is in a sense a true parameter.
A true parameter that we wish,
ah, to be the output of the learning algorithm, ah,
but of course, we never know- we never know what, ah,
Theta star is, um, and when, um,
what we get out of the learning algorithm, um,
is- is going to be just a- a sample from a random, um, random variable.
Now, a thing to note is that this the Theta star or A star is not random.
It's just an unknown constant.
Not a- when we say it's not random it means there is
no probability distribution associated with it.
It's just a constant which we don't know,
that- that's- that's the assumption under which you operate.
Right. Now, um, let- let's see what's,
ah, let's see what's- what's- what are some properties about this Theta- Theta-hat.
So all the, um,
all- all the- all the entities that we estimate are generally,
um, decorated with a hat on top,
which- which indicates that it's- it's something that we estimated.
Um, and anything with a star is like, you know,
the true or the right answer which we don't have access to it generally.
So any questions with this so far?
Yeah. [BACKGROUND]
Yeah. So, yeah, this could be, uh, um,
in case of like, uh, uh,
linear or, or logi- logistic regression
or linear regression generally happens to be a vector.
It could be a scalar,
it could be, you know,
a matrix, it could be anything.
Right. Uh, it's just an entity that we estimate.
Um, and sometimes, uh,
H star can also be so generic that it,
it need not even be parameterized.
It's just some function that you estimate.
So, uh, yeah, so it could,
it could be a vector or a scalar or,
or a matrix, it could be anything.
Right? So, uh, let's see what happens when we- so in the lecture,
we saw, uh, this diagram for in,
in the - when we were talking about bias-variance.
So in case of, uh, regression,
[NOISE] and, um, we saw that this was one fit,
this was just, uh,
let me use a different color,
straight line, and, right?
And we saw this as, uh,
the concepts of [NOISE] sorry,
underfitting and this is overfit and this is like just right.
Right, so the concept of underfitting and overfitting are,
kind of, closely related to bias and variance.
Uh, so this is how you would view it from the data.
So this is from the data view, right?
Cause this is x, this is y.
You know this is your data.
Um, and if, if you look at, you know, um,
look at it from a data point of view,
these are the kind of, uh,
different algorithms that you might get, right?
However, uh, to get a more formal sense,
uh, formal view into what's,
what's bias and variance,
it's more useful to see it from the parameter view.
[NOISE].
So let's imagine we have four different learning algorithms, right?
I'm just going to plot four different.
And here this is the parameter space,
let's say theta 1, theta 2.
Let's imagine, you know,
uh, we have just two parameters.
It's easier to visualize theta 1 and theta 2, right.
And this corresponds to algorithm A,
algorithm B, C, and D. Right.
There is, there is a true theta star.
Let's, let's- which is unknown, right?
Now, let's imagine we run through this,
this process of sampling m examples running it through the algorithm,
obtain a theta hat, right?
And then we start with a new sample- sample from
D run it through the algorithm we get a different theta hat, right?
And theta hat is going to be different for different learning algorithms.
So, so let's imagine first we,
we sample some data that's our training set,
run it through algorithm A and let's
say this is the parameter we got and then we run it through
Algorithm B and let's say this is the parameter we got and through
C here and through D over here.
And we're gonna repeat this, you know,
second one maybe here,
maybe here, here, here and so on and you repeat this process over and over and over.
The, the key is that the number of samples per input is m,
that is fixed, right?
But we're gonna repeat this process and over and over and for every time we repeat it,
we get a different point over here.
[NOISE]
Right? So, uh,
each point each dot corresponds to a sample of size M, right?
The number of points is basically the number of times we repeated the experiment, right?
And what we see is that
these dots are basically samples from the sampling distribution, right?
Now, the concept of,
of bias and variance is kind of visible over here.
So if we were to classify this,
now we would call this as
bias and variance, right?
So these two are algorithms that have low bias,
these two are- have high variance,
these two have low varia- I'm so- these two have low bias,
high bias low variance, high variance.
So what does this mean?
Uh, so bias is basically, um,
checking are the- is,
is the sampling distribution kind of centered around the true parameter,
the true unknown parameter?
Is it centered around the true parameter?
Right? And variance is, um, is,
is measuring basically how dispersed the,
the sampling distribution is, right?
So, so formally speaking,
this is bias and variance and it becomes, uh,
you know pretty clear when we see it in the parameter view instead of in,
uh, uh, uh, the data view.
And essentially bias and variance are basically
just properties of the first and second moments of your sampling distribution.
So you're asking the first moment that's the mean, is it centered around the true parameter
and the second moment that variance - that's
literally variance of the bias-variance trade-off. Yeah.
[inaudible].
Yeah.
[inaudible].
Um, so this is, a,
a diagram where I am using only two thetas just to fit,
you know write on a whiteboard.
So you, you would imagine something that has high variance, for example,
this one to probably be of a much,
much higher dimension, not just two,
but it would still be spread out.
It would still have like high variance.
There would be points in a higher-dimensional space,
you know but more spread out.
Right, so, so the question was,
um, the question was,
um, in over here we, uh,
we actually had more number of thetas but, uh,
here with the higher variance,
um, uh, plots we are having the same number of thetas.
So, uh, yeah so you could imagine this to be higher-dimensional.
And also, different algorithms could have different,
uh, bias and variance even though they have the same number of parameters.
For example, if you had regularization,
the variance would come down, for example.
Let me go over that, um, um, um,
a few observations that we want to make, uh,
is that as we increase the size of the data,
every time we feed in, so if this were to,
to be made bigger,
if you take a bigger sample for every, um,
every time we learn, uh,
the variance of theta hat would become small, right?
So if we repeat the same thing but with,
with larger number of examples,
this would be more- all of these would be more,
um, tightly concentrated, right?
So, so the spread is, uh, uh,
so the spread is a function of how many examples we have in each,
um, in each, uh, uh, iteration.
Right? So, uh, as m tends to infinity, right?
The variance tends to zero, right?
If you were to collect an infinite number of samples,
run it through the algorithm,
you would get some particular, um, um, theta-hat.
And if you were to repeat that with an infinite number of examples
we'll always keep getting the same, um, uh, theta hat.
Now the rate at which the variance goes,
goes to 0 as you increase m,
is you can think of it as what's also,
uh, called the statistical efficiency.
It's basically a measure of how efficient your algorithm
is in squeezing out information from a given amount of data.
And if theta hat tends
to theta star as m tends to infinity,
you call such algorithms as consistent.
So, um, consistent and if
the expected value of your theta hat is equal to theta star for all m, right?
So no matter how big your, um, sample size is,
if you always end up with
a sampling distribution that's centered around the true parameter,
then your estimator is called an unbiased estimator. Yes.
[inaudible].
So efficiency is, is, uh,
basically the rate at which, uh,
the variance drops to 0 as m tends to 0.
So for example, you may have one algorithm which, uh,
which, which, where the variance is a function of 1 over M square.
Another algorithm where the variance is a function of e to the,
uh, uh minus m. You,
you can have- the variance can, uh,
drive down at different rates, uh,
relative to m. So that's kind of captures, um, uh,
what- what's efficiency here.
[NOISE] Right? Yeah.
[inaudible]
Yeah. So uh, theta-theta hat approaches um,
so um, this is a random variable here so so here's one thing to be clear about here.
This is ah, a number, a constant,
and this is a constant but here this is a random variable, right?
So what we're seeing is that as m tends to infinity, theta hat,
that is the distribution,
converges towards being a constant and that constant is going to be a theta star.
Which means at smaller values of m,
your algorithm might be centered elsewhere,
but as you get more and more data,
your sampling distribution variance reduces and also gets
centered around the true theta star eventually.
Okay. So um, informally speaking,
if your algorithm has high bias,
it essentially means no matter how much data or evidence you provided,
it kind of always keeps away from from theta star, right?
You cannot change its mind no matter how much data you feed it,
it's never going to center itself around theta star.
That's like a high biased algorithm,
it's biased away from the true parameter.
And variance is, you can think of it as your algorithm
that's kind of highly distracted by
the noise in the data and kind of easily get swayed away,
you know, far away depending on the noise in your data.
So uh, these algorithms you would call them as those having high variance,
because they can easily get swayed by noise in the data.
And as we are seeing here,
bias and variance are kind of independent of each other.
You can have algorithms that have,
you know, an independent amount of bias and variance in them,
you know, there is there is no um,
um correlation between ah,
ah bias and variance.
And one way- so the- how do we how- do we kind of fight variance?
So first let's look at how we can address variance. Yes.
[BACKGROUND].
So bias and variance are properties of the algorithm at a given size m. Right?
So these plots were from um,
were from a fixed size m and for that fixed size data,
this algorithm has high bias, low variance,
this algorithm has high variance and high bias and so on.
Yeah. Yeah. You can you can um,
you can think of it as yeah, it,
you- you assume like a fixed data size.
Right? So uh, fighting variance.
Okay. So uh, one way to kind of ah,
address if you're in a high variance situation,
this will just increase the amount of data that you have,
and that would naturally just reduce the variance in your algorithm. Yes.
[BACKGROUND].
That is true. So you don't know upfront what uh,
whether you're you're uh,
in a in a high bias or high variance um, um, scenario.
One way to kind of um- one way to kind of uh, uh,
test that is by looking at your training performance versus test performance uh,
we'll go- we'll go over that um.
In fact we're gonna go into um, you know,
much more detail in the main lectures of how do you identify bias and variance,
here we're just going over the concepts of what are bias and what are variance.
So one way to um,
address variance is you just get more data, right?
As you as you get more data,
the- your sampling distributions kind of tend to get more concentrated.
Um, the other way is what's called as regularization.
So when you- when you um,
add regularization like L2 regularization or L1 regularization um,
what we're effectively doing is let's say we have
an algorithm with high variance maybe low bias,
low bias, high variance and you add regularization, right?
What you end up with is an algorithm that
has maybe a small bias,
you increase the bias by adding regularization but low variance.
So if what you care about is your predictive accuracy,
you're probably better off trading off
high variance to some bias and getting down- reducing your your um,
variance ah, to a large extent. Yeah.
[BACKGROUND].
Yeah. We'll- we- we- we're gonna uh,
uh, look into that next.
Right. So in order to kind of um,
get a better understanding of this uh, let's imagine um.
So think of this as the space of hypothesis, space of, right?
So um, let's assume there is a true- there exists, this hypothesis.
Let's call it g, right?
Which is like the best possible hypothesis you can think of.
By best possible hypothesis,
I mean if you were to kind of take this uh, um, um,
take this hypothesis and take the expected value of the loss
with respect to the data generating distribution across an infinite amount of data,
you kind of have the lowest error with this.
So this is, you know, um,
you know the best possible hypothesis.
And then, there is this class of hypotheses.
Let's call this classes h, right?
So this, for example,
can be the set of all logistic regression ah,
hypotheses, or the set of all ah, SVMs you know.
So this is a class of hypotheses and what we,
what we end up with when we ah,
take a finite amount of data,
is some member over here, right?
So let me call h star.
Okay. There is also some hypothesis in this class,
let me call it kind of h star,
which is the best in-class hypotheses.
So within the set of all logistic regression functions,
there exists some, you know,
some model which would give you the lowest um,
lowest error if you were to ah,
test it on the full data distribution, right?
Um, the best possible hypothesis may not be inside ah, your ah, um,
inside your hypothesis class,
it's just some, you know,
some hypothesis that that's um, um,
that's conceptually something outside the class, right?
Now g is not the best possible hypothesis,
h star is best in-class h,
and h hat is one you learned from finite data, right?
So, uh, we also introduce some new notation.
Um, so epsilon of H is,
you will call this the risk or generalization error.
[NOISE] Right?
And it is defined to be equal to the expectation of xy sampled from
E of indicator of h of x not equal to y.
Right? So you sample examples from the data-generating process,
run it through the hypothesis,
check whether it matches with,
uh, with your output and if it matches,
you get a 1, if it does, uh,
if it- if it, uh,
doesn't match you get a 1,
if it matches you get a 0.
So on average, this is, you know,
roughly speaking the fraction of all examples on which you make a mistake.
And here we are kind of thinking about this, um,
from a classification point of view to check if, you know,
the class of your output matches the true class or not.
But you can also extend this to,
uh, the regression setting.
Uh, but that's a little harder to analyze but, you know,
the generalization holds to, um uh,
the regression setting as well but we'll stick to classification for now.
And we have an epsilon hat,
s of h and this is called the empirical risk.
This is the empirical risk or empirical error.
And this over here is 1 over m,
i equal to 1 to m,
indicator of h of x_i not equal to y_i, right?
The difference here is that here this is like an infinite process.
You're- you're- you're, um,
sampling from D forever and calculating like the long-term average.
Whereas this is you have a finite number that's given to you
and what's the fraction of examples on which you make - you make an error.
Right. All right, uh,
before we go further, uh,
there was a question of how,
um, adding regularization reduces your variance.
So what you can see,
um, or actually let me- let me get back to that,
um - um, in a- in a bit.
Uh, so E of g and this is called the Bayes error.
[NOISE] So this essentially means if you take the best possible hypothesis,
what's the fraction, uh,
what's - what's the rate at which you make errors?
You know, uh, and that can be non-zero, right?
Even if you take the best possible hypothesis ever and that can still -
still make some - some mistakes and - and this is also called irreducible error.
[NOISE] For example if your data-generating process you know, uh,
spits out examples where for the same x you have different y's, uh,
in two different examples then no - no learning algorithm can,
you know, uh - uh,
do well in such cases.
That's just one- one kind of irreducible error,
they can be other kinds of irreducible, uh, errors as well.
And epsilon of h_*,
epsilon of g is called the approximation error.
[NOISE] So this essentially
means what is the price that we are paying for limiting ourselves to some class, right?
So it's the - it's the error between - it's the difference between
the best possible error that you can get and
the best possible error you can get from h_*.
Right, so this is, um,
this is an attribute of the class.
So what's the cost we are paying for restricting yourself to a class?
Then you have, uh,
epsilon of h_i minus epsilon h_* and this you call it the estimation error.
[NOISE] The estimation error is,
given the data that we got,
the m examples that we got and we estimated,
you know, using our estimator sum h - h - h_i.
What's the - what's
the - what's the error due to estimation and this is like approximation.
All right. So, this - this,
uh, the error on G is the Bayes error.
The gap between this error and the best in class is the approximation error and the gap
between the best in class and the hypothesis that you end
up with is called the estimation error, right?
And, uh, it's easy to see that, um,
h hat is actually equal to
estimation error
plus approximation error plus irreducible error.
Right? It's pretty easy.
You know, if you just add them up all these cancel out and you're just left with,
uh um, epsilon of H hat.
Um, so it's - it's kind of useful to think about
your generalization error as different components.
Um, some error which you just cannot,
you know, uh - uh, reduce it no matter what - no matter
what hypothesis you pick no matter how much of training data you have.
There's no way you can get rid of the irreducible error.
And then you make some - some decisions about
- that you're going to limit yourself to neural networks or
Logistic regression or whatever and thereby you're defining a class of
all possible models and that has a cost itself and that's your approximation error.
And then you are working with limited data.
And this is generally due to data, right?
And with the limited data that you have and
possibly due to some nuances of your algorithm,
you also have an estimation error, right?
We can further see that the estimation error can be broken down into
estimation variance and the estimation bias, right?
Um, and, uh, you can not, therefore,
write this as approximation error plus irreducible error.
And what we commonly call as bias and variance are - this we call it as variance
and this we call it as bias and this is just irreducible.
So sometimes you see
the bias-variance decomposition and
sometimes you see the estimation approximation error decomposition.
There are somewhat related, they're not exactly the same.
So, uh, the bias is basically why is,
you know, bias is basically trying to capture why is H hat far from a - from G, right?
Why is it staying away from G? You know.
Why did our hypothesis stay away from the true hypotheses?
And that could be because your classes, uh,
is- is kind of too small or it could be due to other reasons,
uh, such as, you know,
um - um, as we'll see
maybe regularization that kind of keeps you away from a certain- certain,
uh - uh, hypothesis, right?
And the variance is generally due to it like
- it's almost always due to having small data.
It could be due to other,
uh - uh, reasons as well.
But these are two different ways of,
uh, of decomposing your, um, your error.
So now, um, if you have high bias,
how do you fight high bias?
Fight high bias.
So how would you fight high bias?
Any guesses. [inaudible] Yeah exactly.
So one way is to just,
you know make your h bigger, right.
Make your h bigger. And also you can - you can try,
you know different algorithms,
um - um uh, after making your h bigger.
And what this generally means is what we saw there was regularization kind of,
you know reduces your - your, um,
variance by paying a small cost in bias and over here,
you know, um. [NOISE]
So let's say your algorithm has some bias, right.
So it has a high bias and some variance, right,
and you make H bigger, your,
your class bigger right and this generally results in
something which reduces your bias but also increases your variance, right?
So, with, with this picture you can,
you can also see, you know,
what's the effect of, um,
how, how does variance come into the picture?
Now just by having a bigger class,
there is a higher probability that
the hypothesis that you estimate can vary a lot, right,
if you reduce your- the space of hypothesis,
you may be increasing your bias because you may be moving away from g,
but you're also effectively reducing your variance, right.
So that's, that's the,
the one of the, you know,
trade off that you observe that any step,
you- a step that you take for example in,
um, reducing bias by making it
bigger also makes it possible for your h hat to land at much,
you know, a- at a wider space and increases your variance.
And if you take a step to reducing your variance by maybe making your,
um, your, your class smaller,
you may end up making it smaller by being away from the end thereby increase your,
your, um, um, increase your bias.
So, when you, when you add regularization,
you know, th- the question, uh, uh,
somebody asked before of how does, um, in,
how does adding regularization decrease the variance?
By adding regularization, you are effectively,
kind of shrinking the class of hypothesis that you have.
You start penalizing those hypotheses whose Theta is very,
is very large, and in a way you're kind of,
you know, shrinking the class of hypothesis that you have.
So, if you shrink the class of hypothesis your,
your variance is kind of reduced because, you know,
there's much smaller wiggles room for your estimator to place your h hat.
And, you know, if you shrink it by going away from,
from, uh, from g, you,
you also introduce bias.
That's like, you know, the bias variance,
uh, um, trade off.
Any questions on this so far?
Yeah. [BACKGROUND].Yeah, you, you probably wanna think of each of these,
you probably wanna think of this as a generalized version of this,
right, so here we have, like, fixed Theta 1,
Theta 2, but you know,
uh, because you could parameterize them into,
uh, uh, a few parameters you can kind of plot it in
a metric space but that's like a more general, um,
um, like a bag of hypotheses, and, you know,
but in any case in both of- both those diagrams,
a point here is one hypothesis,
a point there is one hypothesis.
Here it's parameterized, here it's not parameterized.
Yes. [BACKGROUND]. The thing
is we differ,
d, um, so the question is,
how- what if we,
we shrink it towards h star, right.
The thing is, uh,
we don't know where h star is, right.
If we knew it, we didn't even need to learn anything.
We could just go straight there,
right. So, um, yeah.
[BACKGROUND].
With regularization? So the question is,
when we add regularization,
are we sure that the bias is going up?
No, we, we don't know and,
and this is a common scenario what happens, right.
You, when you add regularization, you, you,
you reduce the variance for sure but you're
very likely gonna introduce some bias in that process.
[BACKGROUND].
So if you add regularization,
you're shrinking your hypothesis space in some ways.
So you're kind of moving away from 2g. So you're kind of adding a little bit of bias.
You're very likely to add some bias in that process.
Yes, so, it's, uh, so I, I,
I would encourage you to, you know, kind of,
after this lecture to think about this a little more slowly, it's, it's,
it takes a while to kind of internalize this,
the concept of bias and variance and, and, um,
uh, It's not very intuitive but,
but, uh thinking about it more definitely helps.
All right, an- any other questions before we move on?
[BACKGROUND].
So an example for a hypothesis class, right?
So the- an example would be, um,
the set of all logistic regression models, right?
And, uh, when you do gradient descent on your,
you know, logistic regression class,
you're kind of implicitly restricting yourself to set
up possible logistic regression models, that's kind of implicit.
[BACKGROUND].
So, the h is the output of the learning algorithm, right?
So you feed and input your algorithm.
Like this is not the model.
This the learning algorithm like, this is,
like gradient descent for example.
And the output of that is the parameters that you learned that converge to.
Right. So d- so, yeah, you,
you probably don't wanna think about this as the model that you learned but this as the,
like the training process and the output of the training process is a model that you learn.
And that is a point in your,
in the class of hypotheses.
[BACKGROUND]. Yes, so, so,
you fix, um, that, uh,
th- the class of learning models, you, you,
say I'm gonna only gonna learn logistic regression models, right?
For different, different samples of data that you feed it as your training set,
you're gonna get, learn a different Theta hat.
[BACKGROUND].
Yes, the- they have to be within the class of hypotheses.
All right, so let's move on.
Next, we come across this concept called empirical risk minimization.
[NOISE].
ERM. So this is the Empirical Risk Minimizer.
Right. So, so the empirical risk minimizer is a learning algorithm.
Right. It is one of those kind of boxes that we drew.
It is, you know, ah- so in the box
that we drew earlier as learning algorithm, right.
So the- the- the diagram that we drew earlier based on which we- we ah,
reasoned everything so far,
didn't actually tell you what actually happens inside.
It could be doing gradient descent, it could just do something else.
It could be, you know,
some- some, you know,
smart programmer who's written a whole bunch of if,
else and just returns a theta, it could be anything.
Right. Uh, and no matter what kind of algorithm was used,
the- the bias-variance theory still holds.
Right. Now we're going to look at, ah,
a very specific type of learning algorithms called the empirical risk minimizer.
Right. So, um, and this was feed into your algorithm and
you get h star,
h hat ERM.
Right? Now, h, um,
h hat ERM is equal to- what is ERM, empirical risk minimization?
It's what we've been doing so far in the course.
Right? We, we tried to find
a minimizer in a class of hypotheses that minimizes the average training error.
Right. Um, so for example, um,
this is trying to minimize the training error from a classification, ah, ah, perspective.
This is kind of minimizing the- or increasing the training accuracy,
which is different from what actually logistic regression did with,
where we were doing the maximum likelihood or minimizing the negative log-likelihood.
It can be shown that, ah,
losses like the logistic loss are - are can be well approximated by,
um, by the ERM.
And, and, and this theory should- should, ah, ah,
hold nonetheless. Um, All right.
So if- if we are limiting
ourselves to do that class of algorithms which,
which worked by minimizing the training loss, right, um,
as opposed to something that say returns a
constant all the time or- or- or does something else.
If we limit ourselves to, um,
empirical risk minimizers, then we can come up with more theoretical results.
For example, uniform convergence,
which we are gonna look at right now.
[NOISE].
Right. So, so we're limiting ourselves to
empirical risk minimizers and starting off, er, uniform convergence.
Right. So there are two central questions that we are kind of interested in.
So, ah, one question is,
if we do empirical risk minimization,
that is if we just reduce the training loss, right,
what- what does that say about the generalization of an effect?
So that is basically, um,
e hat of h versus h. So for,
you know, consider some hypotheses.
Right. And that gives you some amount of training error.
Right. What does that say about its generalization error?
And that's one central question we wanna, um, um, consider.
And the second one is,
how does the generalization error of our learned hypothesis
compare to the best possible generalization error in that class?
Right. Note we're- you know, we're only talking about h star and not, g um, there.
So h star is- is- is the best in class um, um.
So these are- these are two central questions that we wanna- we wanna um, explore.
And for this, we're gonna use our two tools.
Right. So one is called the union bound.
[NOISE] Right.
What's the union bound?
Um, if we have um,
k different events A_2, A_K.
Then, ah, these need not be independent.
Independent. Then the probability of A_1 union A_2 union A_k,
is less than equal to the sum of-
If this looks trivial, it is trivial.
It's- it's um, it's probably one of the axioms in- in- in your,
ah, undergrad probability class.
But the, the probability of any one of these events happening
is less than or equal to the sum of the probabilities of,
ah, each of them, ah, happening.
Right. And then we have a second tool.
Right. It's called the Hoeffding's inequality.
[NOISE].
We're only going to state the- the inequality here,
ah, there is ah, um,
a supplemental notes on the website that actually proves the Hoeffding inequality.
You can, ah, go through that,
um, but here we're only going to state the result.
In fact, throughout this session, we are going to state results.
We're not gonna prove anything.
Um, so, ah, let Z_1, Z_2, Z_m,
be sampled from some Bernoulli distribution of parameter phi.
And let's call well,
phi hat to be the average of them,
m of z_i,
and let there be a Gamma greater than zero,
which we call it as the margin.
So the Hoeffding Inequality basically says,
the probability that the absolute difference between
the estimated phi parameter and the true phi parameter is greater than some margin,
can be bounded by 2 times the exponential
of minus 2 gamma square m. Right?
Not very obvious but you know,
you can, you can show this.
What, what it's basically saying,
is there is some- there is some- some ber- ah,
parameter between 0 and 1 of a Bernoulli distribution.
The fact that it is between 0 and 1 means it's- it's bounded.
And- and that's a key requirement for,
ah, the Hoeffding's inequality.
And now, we take samples from this Bernoulli distribution,
and the estimator for this is basically- and these are just 0s or 1s.
Z- Z- each of the Z is either a 0 or 1.
The sample of 0 or a 1 with probability, um, um,
Phi, and the estimator is basically just the averages of your samples.
Right. And, um, the absolute difference between the estimated value and the true value,
the probability that this difference becomes greater
than some margin Gamma is bounded by this expression.
Right. So there are a lot of things happening here.
So you probably want to, um, um,
you know, slowly think through this.
So this is a margin.
All right.
And this is like- basically like the deviation of the error.
[NOISE] Right.
Um, the absolute value of how- how- how far away
your estimated values from- from the true.
And you'd like it to be small- closer.
So you- you- you probably want, ah,
your -your Phi hat and phi,
to be not more than,
I don't know, 0.001.
Right. So in which case,
if the absolute value between,
ah, ah, the estimated and,
um, the true parameter is greater than 0.01,
if that's the margin your- that you're interested in.
Then this, ah, the Hoeffding's inequality proves
that if you were to repeat this process over and over and over,
the number of times phi hat is going to be
great- is going to be farther than 0.001 from the true parameter,
it's going to be less than this expression,
which is a function of m. Right.
And that is- you- you- you can kind of, ah,
believe it because as m increases, this becomes smaller,
which means the probability of, um,
your estimate deviating more than a certain margin only reduces as you increase m. Right.
So this is Hoeffding's inequality and we're gonna use this.
[inaudible].
Oh, yeah. Questions?
[inaudible].
Not, so, so the question is,
is h star, uh,
the limit of h_r as M goes to infinity?
Uh, it is h star in,
in the limit as M goes to infinity,
if it is a consistent estimator, right?
So we, we, we went over the concept of consistency.
Given infinite data, will you eventually get to the right answer?
And if your estimator is not consistent,
then it will- it need not be.
So, uh, in general h hat
need not converge to h star as you get an infinite amount of data.
[NOISE] All right?
So, uh, now we wanna use, um, uh,
these tools, tool 1 and tool 2 to answer our- like the central questions.
[NOISE] Any other questions?
Yeah.
[BACKGROUND]
This is a more limited version of Hoeffding's inequality and yes,
uh, if we limit ourselves to a Bernoulli variable, uh,
ba- um, which has some parameter phi and you take samples from it.
And you construct an estimator which
is the average of th- the samples of the 0s and 1s,
then, um, this inequality holds.
That's- thi- this inequality is called the Hoeffding's inequality.
Yes.
[BACKGROUND] So if you're, um,
in general, there, there are- there are- there
is this class of algorithms called maximum likelihood algorithms,
maximum likelihood estimators and a pure
maximum likelihood estimator is generally consistent.
If you include regularization,
then it need not be- it need not be,
uh, uh, uh, consistent though,
uh, I'm not very sure about that.
I'm not very sure about that. [NOISE] Yeah, sure.
Yeah. So basically like- if you think about
a neural net where you have something that's completely
[inaudible] neural net is not always consistent.
Yeah. So the- basically, um, um,
um, I know for the mic, uh,
wha- what, what, what he responded was, um,
if you have an algorithm like a neural net which is, um,
which is non-convex, you may actually not end up with the same,
uh, uh, result even if you, uh, um,
increase, um, increase like,
uh, the number of, um,
uh- though I would probably call the,
uh, uh, the fact- I,
I would probably think of the non-convexity to be part of an estimation bias,
um, because you could in theory always
find like the global minima of a neural network.
It's just that there's some bias in our estimator that
we are using gradient descent and we cannot solve it.
Okay. So now, uh,
let's- let's use these two tools, uh,
and for that, uh, we're gonna start [NOISE] how do we look at this diagram, right?
So, [NOISE] so over here,
um, we have hypotheses.
[NOISE] Here we have error,
[NOISE] and let's think of this.
[NOISE] There's actually one,
one curve which I'm trying to make it thick
and probably make it look like multiple curves,
this is just one curve and this we will call it as.
[NOISE] So this is the generalization risk or the,
uh, uh, the generalization error of every possible hypothesis,
uh, in our class, right?
So pick one hypothesis that's gonna be somewhere on this axis,
calculate the generalization error,
not the empirical, the generalization error
and- no that's the height of that curve, right?
And we also have something like this.
[NOISE] Right?
So this dotted line now corresponds to sum each of s_h.
Now let's, let's sample a set of m examples and calculate
the empirical error of all our hypotheses in our class and plot it as a curve, right?
Any questions on what, what, what these two are?
Yeah. [BACKGROUND] It need not meet.
I'm, I'm just, uh, uh, in fact,
thi- this is very likely not even a straight line,
you're just thinking of all, all possible hypotheses.
It may not be convex.
Um, this just to, to, um,
get some ideas, um,
um, get, get better intuitions on some of these ideas.
Yes. [BACKGROUND] So, uh, the black line,
the thick black line is the generalization error of all your hypotheses, right?
And let's say you sample some,
some, some data, right?
Let's call it S. On that sample,
you have training error for all possible hypotheses, right?
[NOISE] We haven't not learned anything, right?
It's, it's, uh, uh,
this is the generalization error and this is the empirical error for the given S, right?
Now, uh, in order to app- apply
Hoeffding's Inequality here, right?
So let's consider some h_i, right?
This is some hypothesis. We- we don't know.
So we start with some random hypotheses, right?
And- so by starting
with some hypotheses like think of this as you start with some parameter, [NOISE] right?
And, uh, let's- right.
So the height of this line up to the,
the thick black curve is basically,
um, the generalization error of h_i is the height to the thick black curve.
So let me call this Epsilon of h_i, right?
And the height to the dotted curve until here.
And this is Epsilon hat of h_i.
I'm gonna ignore the S for now, right?
And this corresponds to like the,
the sample that we obtain.
Now one thing, ah, you can,
you can check is that the expected value of- [NOISE]
where the expectation is with respect to the data's sample.
So what this means is that, ah,
for one particular sample you,
ah, -this is the generalization error you got.
Take another set of samples,
that curve might look som- some,
you know, some other way, and,
you know, the height of the dotted line would be there.
So in general on average,
if you sum average across all possible training samples that you can get, ah, the,
the expected value of the height to the dotted line is gonna be the height to the,
the, the thick line.
Right? That's, that's justified.
Now here if you apply Hoeffding's inequality,
you basically get probability of absolute difference between the empirical error versus
the generalization error to be greater than Gamma is
less than equal to 2 minus 2 Gamma square.
And- this is basically,
you know, Hoeffding's inequality.
We have right here except in place of phi and phi hat,
we have the true generalization error,
and the empirical error.
Any questions on this so far?
So what we are saying is essentially
the gap between the generalization error and the empirical error. All right.
Right. The gap being greater than
some margin Gamma is gonna be bounded by this expression.
Right? So loosely speaking what this means is,
as we increase the size M,
if our training is up- if we plot the set of all dotted lines for a larger M,
they are gonna be more concentrated around the black line.
Does that make sense? So, so take a moment and think about it.
This dotted line corresponds to S of some particular size
M. We could take another sample of,
you know, a fixed set of examples,
and that might look kinda something like this.
Right. And take another sample of size M,
and that might look something like [NOISE] this.
Now- and now, consider the set of
all deviations from the black line
to every possible dotted line along the vertical line of HI.
Right? Now this gap is greater than some margin
Gamma with probability less than this term over here.
Right? So, so it essentially means that if
you start plotting dotted lines with a bigger M, right,
where the set of all those dotted lines correspond to a bigger M,
they are gonna be much more tightly
concentrated around the true generalization of that, of that edge.
That make sense? And you're basically applying
Hoeffding's inequality to this gap over here instead of some phi.
That's basically what you're doing.
Right? Now, that's good.
But there's a problem here.
The problem here is that,
we started with some hypotheses,
and then averaged across all possible data that you could sample.
But in practice, this is useless.
Because in practice we start with some data,
and run the empirical risk minimizer to find the lowest H for that particular data.
Right? And when you, when,
when- which means that H,
and the data that you have are not really independent.
Right? You, you chose the H to minimize, ah,
minimize the risk for the empirical risk for
th- the particular data that you are given in the first place.
Right? So to, to fix this,
what we wanna do is basically extend this result that we got
to account for all H. Right.
Now if we want to get a bound on the gap between the probabilistic bound,
and the gap between the generalization error,
and the empirical error for all H. You know what's that bound gonna look like.
Right. And this is basically called uniform, uniform convergence.
This is- I'll just call uniform convergence because we are trying to, ah,
we are trying to see how the risk curve converges uniformly to the generalization risk curve,
or how the empirical risk curve uniformly converges to the generalization risk curve.
And, ah, it's, ah,
that's called uniform convergence which you can apply to functions in general,
but here we are applying to the risk curves across our hypotheses.
And we can show- I'm gonna, ah,
just, um, skip the math.
So, um, this we showed using Hoeffding's inequality,
and you can apply the union bound for unioning across all H.
Except we can- first we're going to limit ourselves to, um- all right.
So let me start over.
So we got this bound for a fixed H. Right?
But we are interested in getting the bound for any possible H. Right?
So that's our next step. Right? And the way we're gonna,
gonna extend this pointwise result to across all of them,
is gonna look different for two possible cases.
One is a case of a finite hypothesis class,
and the other case is gonna be the case for infinite hypothesis classes.
So what does it look like?
So, [NOISE]
so let's first consider finite hypothesis classes.
So first we're gonna assume that the class of H has a finite number of hypotheses.
The result by itself is not very useful,
but it's gonna be like a building block for,
for the, for the other case.
So let's assume that the number of hypotheses in this class is some number K. Right?
Ah, we can show that- I'm not gonna go over the,
the derivation, but I'm just gonna,
um, write out the result.
It's, it's pretty intuitive.
So basically what we do is, ah,
we apply the union bound for all K hypotheses,
and we end up just multiplying that by a factor of K. Right?
So what we get is the probability that there exists some hypotheses
in H such that the empirical error
minus generalization error is greater than Gamma,
is less than equal to K times,
K times the probability of any 1 which is equal to K times,
ah, 2 minus xp 2 Gamma square M. And this we flip it over,
we negate it, and we get the probability that for all hypotheses in our class,
the empirical risk minus generalization risk is less than Gamma,
is gonna be greater than equal to 1 minus 2K,
minus 2 Gamma square.
Okay. So with probability at least 1 minus,
you know this expression,
which we can, we can call this Delta with probability at least so much.
For all hypotheses, our margin is gonna be less than some Gamma.
Right? This is, this is just, um,
Hoeffding's inequality plus union bound,
and just negate the two sides, you get this.
And you, you can go with this slowly, um, um,
you know later from the notes,
the notes goes over this,
um, in more detail.
Right? Now, basically now what we have is, you know,
now let's let Delta equals to
K exp minus 2 Gamma squared M. So we basically now have,
um, a relation between Delta which is like the probability of error.
By here, um, ah, by error I mean that the,
um, empirical risk, and the generalization risk are farther than some, some margin.
And Gamma is called the margin of error.
And M is your sample size.
So, so what this basically tells us, um,
if your algorithm is the empirical risk minimizer,
it could have been any kind of algorithm.
But if it is the kind that minimizes the training error,
then you can get by, by,
by just changing the sample size,
you can get a relation between the margin of error and
the probability of error and relate it to the sample size, right?
So, um, what we can do with this relation is basically
fix any two and solve for the third,
and that gives us,
nope, some actionable results.
For example, you can fix any two and solve for the third from this relationship, right?
And what, what, what that could, uh,
mean is for example,
so you, you can choose any two and solve for the third.
Um, I'm only gonna go over one, one of those.
So let, let's fix,
fix uh, Gamma and Delta to be greater than zero.
And we solve for m, and we get m [NOISE] weighted to, equal 1 over 2 Gamma square,
log 2K over Delta.
So what this means is with probability at least 1 minus
Delta which means probably at least 99% or 99.9%.
For example, with probability at least,
uh, 1 minus delta,
the margin of error between
the empirical risk and the true generalization risk is gonna be less
than Gamma as long as your training size is bigger than this expression, right.
That's something actionable for us, right.
Now, theory can be useful.
So this is also called the sample complexity dessert.
[NOISE] right? And uh, basically,
what this means is as you increase m and you,
you sample different [NOISE] sets of, uh, uh, data-sets,
your dotted lines are gonna get closer and closer to, to, uh,
the thick line which means,
minimizing your- minimizing on
the dotted line will also get you closer to the generalization error.
So this, this is basically telling you how minimizing on, on, um,
minimizing on the empirical risk gets you closer to,
uh, gen- generalization, right?
Okay, so that- so we started off with two questions,
relating the empirical risk to generalization risk.
Now, let's, let's explore the second question.
What about, uh, the generalization error [NOISE] of [NOISE] our minimizer with the,
uh, um, best possible in class?
So let's look at this diagram again.
Let's say we started with this dotted curve, right.
And the minimizer of that would be h-star.
This is h-star. Sorry the diagram is a little, uh,
[NOISE] let me erase the previous one [NOISE] right?
So this is h-hat.
Sorry, this is h-hat.
And this has a particular generalization error, right?
That is the point of, of- uh,
let- let- let's assume we got this data-set.
We ran the empirical [NOISE] risk minimizer and we obtained this hypothesis.
And when we deploy this in the world- in the real world,
its error is gonna be so much, right?
Now, how does this compare [NOISE] to the performance of the minimizer of the, the,
the best possible [NOISE] ,
so this is h-star,
best in class, right?
Now, we want to get a relation between this error level and this error level.
We got one bound that relates this to this,
and now we want something that relates this to this.
Now, how do we do that?
It's pretty straightforward.
Um, so the em- generalization error of h-hat,
that's this dot over here,
is less than equal to the empirical risk of h-hat plus Gamma.
So we got the result, um,
using a Hoeffding and union bound that the gap between the dotted line and the,
the thick black line is always less than Gamma, right?
And it's the absolute value.
So we can, we can, uh, um,
write it this way as well.
And this, right?
So basically, we, we start it from the,
the thick black line, drop down to the dotted line.
And this is gonna be less than the empirical error of h-star plus Gamma. Why is that?
Because em- empirical error,
um, the empirical error, uh, uh,
of h-hat by definition,
is less than or equal to the empirical error on any other hypotheses,
including the best in class.
Because this is the training error,
not, not, not the generalization error, right?
So which means, um- and,
and this is less than or equal to.
So we, we dropped from the generalization to the test.
And we said, this test is,
thi- this training error is always gonna to be less
than the empirical error of the best-in-class.
You see that the best-in-class is higher for the trade to be empirical error.
And this again, is now- this gap is also bounded because we,
we proved uniform convergence.
That the gap between the dotted line and thick line is bounded by Gamma for any h, right?
And this is therefore h-star plus 2 Gamma,
because we added the extra margin.
So we wanted the relation between the, uh,
the- our, our hypothesis
generalization error to the generalization error of the best in class hypotheses.
So we dropped from the generalization error to the empirical error of our hypotheses,
related that to the empirical one of the best in
class and again bounded by the gap between these two.
So we- we've got a gap between the generalization bound,
the generalized error for hypothesis to the best in
class generalization. Any questions on this?
So the result basically says,
with probability, 1 minus Delta,
and for training size m,
[NOISE] the generalization error of [NOISE] the hypothesis from
the empirical risk minimizer is going to be within the
best in class generalization error plus 2 times 1 over,
1 over 2m plus log 2K over Delta.
So this was basically uh,
so you can get this, uh, when you,
when you- so in this expression,
if you set this equal to Delta and solve for Gamma, you will get this.
Any questions? [NOISE] I think we're already over time.
So, uh, the case for infinite classes is an extension to this.
Maybe I'll just write the results.
So there is a concept called VC dimension,
which is a pretty simple concept but [NOISE] we won't be going over it today.
VC dimension basically says,
um, what is the- so VC dimension is,
you can think of it as trying to assign a size to an infinitely,
uh, to an infinite size hypothesis class.
For a fixed size hypothesis class,
we had like, you know, K to be the size of the hypothesis class.
So VC [NOISE] of some hypothesis class is gonna be some number, right?
Some number which, which kind of,
um, which is like the size of the hypothesis.
It's basically, telling you how,
how expressive it is um, and, and, uh,
on using, using the VC dimension,
uh, there are very nice uh,
geometrical meanings of VC dimension.
You can, you can get a bound, similar bound.
But now, it's not for, uh, uh, um,
it's not for uh,
uh, finite classes anymore.
Some big O of [NOISE]
right? So in place of this margin, we ended up with,
uh, a different margin that is, uh,
a function of the, the VC dimension.
And the, the key takeaway from this is that uh,
the number of data examples,
that the sample complexity that you want is generally,
uh, an order of the VC dimension to get good results.
That's basically, the, uh, uh,
main result from that, right?
From, uh- with that,
I guess we'll, we'll, uh,
we'll break for the day and,
uh, we'll take more questions.
 Hello everyone. Uh, so my name is Raphael Townshend.
I'm one of the head TAs for this class.
This week Andrew is traveling and my advisor is still dealing with medical issues.
So I'm going to be giving today's lecture.
Um, you heard from my wonderful co-head TA Anand a couple of weeks ago.
And so today, we're gonna be going over decision trees and various ensemble methods.
Uh, so these might seem a bit like disparate topics at first,
but really decision trees are,
sort of, a classical example model class to use with various ensembling methods.
We're gonna get into a little bit why in a bit,
but just to give you guys an overview of what the outlines can be.
We're first gonna go over decision trees,
then we're gonna go over general ensembling methods and then
go specifically into bagging random forests and boosting.
Okay. So let's get started.
So first let's cover some decision trees.
Okay. So last week,
Andrew was covering SVNs that are, sort of,
one of the classical linear models and,
sort of, brought to a close a lot of discussion of those linear models.
And so today we gonna be getting to decision trees which is really
one of our first examples of a non-linear model.
And so to motivate these guys let me give you guys an example.
Okay. So I'm Canadian,
I really like to ski.
So I'm gonna motivate it using that.
So pretend you have a classifier that given
a time and a location tells you whether or not you can ski,
so it's a binary classifier saying yes or no.
And so you have, you can imagine,
a graph like this,
and on the x-axis we're gonna have time in months,
so counting from the start.
So starting at 1 for January to 12 for December,
and then on the y-axis we're gonna use latitude in degrees, okay?
And so for those of you who might have forgotten what latitude is,
it's basically at positive 90 degrees you're at the North Pole,
at negative 90 degrees you're at the South Pole.
So positive 90, negative 90,
0 being the Equator and it's, sort of,
your location along the north-south axis.
Okay. So given this,
if you might recall,
the winter in the Northern Hemisphere generally happens in the early months of the year.
So you might see that you can ski in
these early months over here and it has some positive data points,
and then in the later months,
right, and then in the middle, you can't really ski.
Versus in the Southern Hemisphere,
it's basically flipped, where you can not ski in the early months.
You can ski during the May,
June, July, August time period,
and then you can't ski in the earlier months,
and then the equator in general is just not
great for skiing that's the reason I don't live there,
and so you just have a bunch of negatives here.
Okay. And so when you look at a data set like this,
you've sort of got these separate regions that you're looking at, right?
And you sort of want to isolate out those regions of positive examples.
If you had a linear classifier, you'd sort of
be hard-pressed to come up with any sort of
decision boundary that would separate this reasonably.
Now you could think okay, maybe you have an SVM or something,
you've come up with a kernel that could perh-
perhaps project this into a higher feature space that would make it linearly separable,
but it turns out that with decision trees,
you have a very natural way to do this.
So to sort of make clear exactly what we want to do with decision trees,
is we wanna sort of partition the space into individual regions.
So we sort of wanna isolate out these,
like, positive examples, for example.
In general this problem is fairly intractable just coming up with the optimal regions.
But how we do with decision trees is we do it in
this basically greedy,
top-down, recursive manner,
and this is going to be recursive partitioning.
Okay? And so it's- basically it's top-down because
we're starting with the overall region and we wanna slowly partition it up, okay?
And then it's greedy because at each step we wanna pick the best partition possible.
Okay. So let's actually try and work out intuitively what a decision tree would do, okay?
So what we do is we start with the overall space and the tree
is basically gonna play 20 Questions with this space.
Okay. So like for example,
one question it might ask is,
if we have the data coming in like this, is,
is the latitude greater than 30 degrees, okay?
And that would involve, sort of,
cutting the space like this, for example, okay?
And then we'd have a yes or a no.
And so starting from, like,
the most general space now we have partitioned
the overall space into two separate spaces using this, this question.
Okay. And this is where the recursive part comes in now,
because now that you've sort of
split the space into two,
you can then sort of
treat each individual space as a new problem to ask a new question about.
So for example now that you've asked this latitude greater than 30 question,
you could then ask something like,
month less than like March or something like that.
All right, and that would give you a yes or no.
And what that works out to effectively,
is that now you've taken this upper space here and divided it
up into these two separate regions like this.
And so you could imagine how through asking these recursive questions over and over again,
you could start splitting up the entire space into your individual regions like this.
Okay, and so to make this a little bit more formal,
what we're looking for is,
we're looking for, sort of,
the split function, okay?
So you can, sort of, define a region.
So you have a region and let's call that region R_p in this case for R parent, okay?
And we're looking for,
looking for a split S_p,
such that you have an S_p, you can, sort of,
write out this S_p function as a function of j,t.
Okay, where we saw you- j is which feature number and T is the threshold you're using.
And so you can, sort of, write this out formally as,
sort of, you're outputting a tuple,
where on the one hand you have a set X where you
have the x j- the jth feature of x is less than the threshold,
and you have Xs element of R-p,
since we're only partitioning that parent region.
And then the second set is literally the same thing,
except it's just those that are greater than t. And so we can refer
to each one of these as R_1 and
R_2.
Any questions so far?
No? Okay. So we,
sort of, define now how we would, sort of, do this.
We're trying to, like, greedily pick these peaks that are
partitioning our input space and the splits are,
sort of, defined by which feature you're looking at and
the threshold that you're applying to that feature.
Uh, sort of a natural question to ask now is,
is how do you choose these splits,
right? And so I sort of
gave this intuitive explanation,
that really what you're trying to do is you're trying to isolate out
the space of positives and negatives in this case.
And so what is useful to define is a loss on a region, okay?
So define your loss L on R,
loss on R. And
so for now let's define our loss as something fairly obvious,
is your misclassification loss.
It's how many examples in your region you get wrong.
And so assuming that you have, uh,
given C classes total,
you can define P hat c
to be the proportion
of examples in R
that are of class c. [NOISE]
And so now that we've got this definition,
where we had this p hat c of telling us
the proportion of examples that we've gotten that case,
you can try to define the loss of any region as loss,
let's call it misclassification,
it's just 1 minus max over c of p hat c, okay?
And so the reasoning behind this is
basically you can say that for any region that you've subdivided generally,
what you'll want to do is predict the most common class there,
which is just the maximum p hat c, right?
And so then all the remaining probability just gets thrown
onto misclassification errors, okay?
And so then once we do this,
we want to basically pick- now that we have a loss defined,
we want to, um,
pick a split that decreases the loss as much as possible.
So you recall I've defined this region,
R_parent, and then these two children regions R_1 and R_2.
And you basically want to reduce that loss as much as possible.
So you want to, um,
basically minimize loss,
R_parent minus loss of R_1 plus loss of R_2.
And so this is sort of your parent loss,
this is your children loss.
Okay. And since you're picking- and basically what you're
minimizing over in some case is this j, t that we
defined over here since this split is really what is gonna
define our two children regions, right.
And what you'll notice is that the loss of the parent doesn't really
matter in this case because that's already defined.
So really all you're trying to do is minimize this negative
sum of losses of your children, okay?
So let's move to the next board here.
[NOISE]
So I started to find this misclassification loss.
Let's get a little bit into actually why misclassification loss
isn't actually the right loss to use for this problem, so,
okay? And so for a simple example,
let's pretend- So I've sort of drawn out a tree like this,
Let's pretend that instead we have
another setup here where we're coming into a decision node.
And at this point we have 900 positives and 100 negatives, okay?
So this is sort of a misclassification loss,
of 100 in this case because you'd predict
the most common class and end up with 100 misclassified examples.
All right, and so this would be your region R_p right now, right?
And so then you can split it into these two other regions, right?
Say R_1 and R_2.
And say that what you've achieved now is you have the 700 positive,
100 negatives on this side versus, uh,
200 positives and 0 negatives on this side, okay?
Now, this seems like a pretty good split since you're getting out some more examples.
But what you can see is that, if you just drew the same thing again,
right, R_p with 900 and 100,
split split, and say in this case, instead,
you've got 400 positives over here,
100 negatives, and 500 positives and 0 negatives.
So most people would argue that this bright decision boundary is better than
the left one because you're basically isolating out even more positives in this case.
However, if you're just looking at your misclassification loss,
it turns out that on this left one here,
let's call this R_1 and R_2 versus this right one,
Let's call this R_1 prime,
R_2 prime, okay?
So your loss of R_1 plus R_2,
on this left case it's just 100 plus 0.
All right, so it's just 100.
And then on the right side here,
it's actually still just the same, right?
And in fact, if you'd look at the original loss of
your parent it's also just 100, right?
So you haven't really according to this loss metric changed anything at all.
And so that sort of brings up one problem with the misclassification loss is that,
it's not really sensitive enough, okay?
So like instead what we can do is we can define this cross-entropy loss, okay?
So which we'll define as L_cross.
Let me just write this out here.
And so really what you're doing is you're just summing over
the classes and it's the probability-
that the proportion of elements in that class
times the log of the proportion in that class.
And how you can think of this is,
It's sort of this concept that we borrow from information theory,
which is sort of like the number of bits you need to
communicate to tell someone who already
knows what the probabilities are what class you are looking at.
And so that sounds like a mouthful but really you can sort of think of it intuitively as,
if someone already knows the probabilities,
like say it's a 100% chance that it is of one class,
then you don't need to communicate anything to tell
them exactly which class it is because it's obvious that it is that
one class versus if you have a fairly even split then you'd need to communicate
a lot more information to tell someone exactly what class you were in.
Any questions so far? Yeah?
[inaudible].
The R_1, R_2 for the parent class?
[inaudible].
For this case here?
Yeah, yeah, so, um,
for that case there so you see that, er,
I'll try and reach up there, but so it's like say like R_p was your start region, right?
You could say it's the overall region, right?
And then R_1 would be all the points above this latitude 30 line.
And R_2 would be all the points below the latitude 30 line.
Yeah, yeah?
[inaudible].
Yeah. So the question is,
when you're trying to minimize this loss here,
is it the same as maximizing the,
the children loss, and since,
er - no, uh,
ah, let's see, of maximizing the children loss.
And yeah, it turns out it doesn't really matter,
which, um, which way you put it.
It just- basically, you're trying to either minimize the loss of
the children or maximize the gain in information, basically.
[inaudible].
Yeah. Let's see. Yeah, you're right.
That should actually be a max.
Let me fix that really quick.
Because you start with your parent loss,
and then you're subtracting out your children's loss,
and so the amount left,
let's see, the higher this loss is- yeah.
So you really want to maximize this guy.
Makes sense, everyone? Thanks for that.
Okay, so I've sort of given this like, hand-wavy- Oh, sure, what's up?
[inaudible].
So that would be log-based.
The question is, for the cross-entropy loss,
is it log base 2 or log base c?
It's log base 2.
Okay, here, I can write that out. Yep.
[inaudible].
Oh, sorry, I didn't quite hear that.
[inaudible].
Okay. Um, so the question is can- uh,
what is the proportion that are correct versus incorrect
for these two examples we've worked through here?
Um, and so, yeah- basically, what we're starting with is,
we're starting with we have 900/100, 900 positives and 100 negatives.
All right, so you can imagine that if you just stopped at this point, right,
you would just cla- classify everything as positive,
right, and so you get 100 negatives incorrect.
Does that make sense? Because this is 900 positives and 100 negatives.
So if you just stopped here and just tried to classify,
given this whole region R_p,
you would end up getting 10% of your examples wrong, right?
In this case, we're sort of talking- we're not talking about percentages,
we're talking about absolute number of examples that we've gotten wrong,
but you can also definitely talk in terms of percentages instead.
And then down here, once you've split it,
right, now you've got these two subregions, right?
And for- on this, on this left one here,
you still have more positives than negatives, right?
So you're still gonna classify positive in this leaf, right?
And you're still gonna classify positive in this leaf, too,
because they- they're both majority class,
or the positives are still the majority class there.
And in this case, since you have 0 negatives,
you're not gonna make any errors in your classification,
whereas in this case here,
it's still going to make 100 errors.
And so what I'm saying is that,
at this level, so if we just look above this line at R_p,
right, you're making 100 mistakes,
and then below this line you're still making 100 mistakes.
So what I'm saying is that, that the loss in this case is not very informative.
[inaudible].
Um, so this, this p-hat- okay,
I'm being a little bit loose with terminology with the notation here,
but the p-hat in this case is a proportion, okay?
But you can also easily- basically,
it's like whether you're normalizing the whole thing or not.
Yeah. Okay. So I've
sort of given this a bit handwavy explanation as to
why misclassification loss versus cross-entropy loss might be better or worse.
Um, we can actually get a fairly good intuition for why this is
the case by looking at it from a sort of geometric perspective.
So pretend now that you have this, this plot, okay?
And what you're plotting here is- pretend you have a binary classification problem, okay?
So you have just- is it positive class or negative class, okay?
And so you can sort of represent, say p-hat,
as like the proportion of positives in your set, okay?
And what you've got plotted up here is your loss.
Okay. For cross-entropy loss,
where your curve is gonna end up looking like is,
is gonna end up looking like this strictly concave curve like this, okay?
And what you can do is you can sort of look at where
your children versus your parent would fall on this curve.
So say that you have two children, okay?
You have one up here, so like let's call this LR1.
And you have one down here, LR2, okay?
And say that you have an equal number of examples in both R1 and R2,
so they're equally weighted.
If you take- when you're looking at the overall loss between the two,
right, that's really just the average of the two.
So you can draw a line between these two,
and the midpoint turns out to be the average of your two losses.
So this is LR1 plus LR2 divided by 2.
That's for this guy, okay?
And what you can notice is that, in fact,
the loss of the parent node is actually just this point projected upwards here,
so this would be your LR parent.
And this difference right here,
this difference, is sort of your change in loss.
Does this makes sense? Any questions?
Okay. So we have this- just to recap, okay.
So we have- say,
we have two children regions, right?
And they have different probabilities of positive examples occurring, right?
They sort of would fall- one would fall on this point on the curve, and say,
the other one falls on this point on the curve,
then the average of the two losses sort of falls on
the midpoint between these two original losses.
And if you look at the parent,
it's really just halfway between on the x-axis,
and you can project up towards for that as well,
and you end up with the loss of R_parent. What's up?
[inaudible].
Okay. So what we're looking at here is we're looking at the cross entropy loss.
So you've got this function here,
this L cross entropy right,
and that's in terms of p-hat c's, right?
In this case here,
we're just assuming that we have two classes, okay?
So what we're doing is we're just modifying the p-hat c, we're,
we're changing that on the x-axis and then we're looking at what
the response of the overall loss function is on the y-axis.
And so what I just did here is for any- this curve just represents for any p-hat c,
what the cross entropy loss would look like.
Okay. And so we can come back to this, for example, right?
And if we look at this parent here right,
this guy has a 10%, right?
It's sort of like p-hat,
p-hat for this guy is 0.1,
it's 10% basically or,
or I guess no, in this case, would be 0.9 sorry.
And then versus here, in these two cases, right,
your p-hat, in this case,
is 1 since you've got them all right,
all right, and then, in this case,
it's 0.8, okay?
So you can sort of see since these are equal,
there's the same number of examples in both of these,
the p-hat of the parent is just the average of the p-hat's of the children.
Okay. And so that's how we can sort of take this LR_parent,
this LR_parent is just halfway, if we projected this down, all right.
Let me just erase this little bit here.
If we projected this down like this,
we'd see that this- that this point here is the midpoint.
Okay. Um, but then when you're
actually averaging the two losses after you've done the split,
then you can basically just,
you're just taking the average loss right?
You're just summing LR1 plus
LR2 and if you're taking the average then you're dividing by two,
and what you can do is you can just draw the line and take
the midpoint of this line instead. Yeah.
[inaudible].
Yeah.
[inaudible]
Yeah. Exactly. So yeah really any- if there- it's a good point.
The question was if you have an uneven split,
uh what would that look like on this curve, right?
And so at this point, I've been making the math easy by
just saying there's an even split but really if there was
a slightly uneven split you- the average would just be
any point along this line that you've drawn.
As you can see the whole thing is strictly concave so any point along
that line is going to lie below the original loss curve for the parent.
So you're basically, as long as you're not picking
the exact same points on
the probability curve and not making any gain at all in your split,
you're gonna gain some amount of information through this split.
Okay. Now, this was the cross-entropy loss, right?
If instead, we look at the misclassification loss over here,
let's draw this one instead.
What we can see, in this case,
if you draw it is that it's in fact really this pyramid kind of shape where
it's just linear and then flips over once you start classifying the other side.
And if you did the same argument here where we had LR1 and LR2,
and then you drew a line between them, all right,
that's basically just still the loss curve, and so,
in this case, like your midpoint would be the same point as your parent.
So your loss of R_parent, in this case,
would equal your loss of R1 plus loss of R2 divided by 2.
All right. And so in this case,
you can- there's even now according to the cross entropy formulation,
you do have a gain in information and intuitively
we do see a gain in information over here.
For the misclassification loss, since it's not very sensitive,
if you end up with points on the same side of the curve,
then you actually don't see any sort of
information gain based on this kind of representation.
And so there's actually a couple, I,
I presented the cross entropy loss here.
There's also the Gini loss which is another one,
which people just write out as,
as the sum over your classes p-hat c times one minus p-hat c,
okay, and it turns out that this curve also looks very
similar to this original cross entropy curve.
And what you'll see is that actually most curves that
are successfully used for de- decision splits,
look basically like the strictly concave function.
Okay. So that's where it covers a lot of the criteria we use for splits.
Um, let's look at some extensions for decision trees.
Actually, I'm going to keep this guy.
Okay. So, so far I've been talking about decision trees for classification.
You could also imagine having decision trees for regression,
and people generally call these regression trees, okay.
So taking the ski example again let's
pretend that instead of now predicting whether or not you can ski,
you're predicting the amount of snowfall you would expect in that area around that time.
Um, so like let's- I'm just gonna say it's like inches of snowfall I guess or something,
per like day or something and just like maybe have some values up here.
Some high value because you're- it's winter over there,
it's mostly 0s over here because there's summer,
and then you have some more high values over here,
and then you have 0s along the equator again.
0s, southern hemisphere over our winter,
and then more 0s like this.
And you can sort of see how you would do just the exact same thing.
You still want to isolate out regions and sort of
increase like the purity of those regions.
So you could still create like your trees like this,
all right, and split out like this for example.
And what you do when you get to one of your leaves
is instead of just predicting a majority class,
what you can do is predict the mean of the values left.
So you're predicting,
predict y hat where, well for Rm.
So pretend you have a region Rm,
you're predicting y hat of m which is the sum of all the indices in Rm,
Y i minus y hat m,
and you want the squared loss and then you can sort of I guess,
in this case, you want to normalize by
the overall cardinality of Rm or how many points you have in Rm.
And so in this case, basically all you've done is you've switched
your loss function or, no sorry that's wrong.
[LAUGHTER] This is actually- I got a little bit ahead of myself.
This is actually just- the,
the mean value would just be this, in this case, right?
It's just your summing all the values within your region.
So in this case, 7, 9, 8,
10, and then just taking the average of that.
Um, but so then what you do,
what I was starting to write out there was actually really the,
the loss that you would use, in this case,
right, which is your squared loss, okay?
So like we'll just call that L squared which, in this case,
would be equal to Y i minus
y hat m squared over R m. That's what I started to write over there.
But in this case, right,
you have your mean prediction and then your loss in this case,
is how far off your mean prediction is from the overall predictions,
in this case. Yep.
So in terms of [inaudible].
So that's a really good question. The question was uh,
how do you actually search for your splits,
how do you actually solve the optimization problem of finding these splits?
And it turns out that you can actually basically brute force it very efficiently.
I'm going to get into sot of the details of how you do that shortly,
but it turns out that you can just go through everything
fairly quickly. Um, I'll get into that.
I think that's in a couple of sections from now,
yeah. Any other questions?
Okay. So this is,
uh, for regression trees, right?
It turns out that,
um, another useful extension that,
that you don't really get for other learning algorithms is that you can also deal with,
uh, categorical variables fairly easily.
And basically, for this case,
you could imagine that instead of having your latitude in degrees,
you could just have three categories right?
You could have something like, uh,
this is the northern hemisphere,
this is the equator,
and this is the southern hemisphere, okay?
And then you could ask questions instead of the sort,
like that initial question we had before,
where it was latitude greater than 30.
Your question could instead be is,
is- I guess this would be,
is location in northern hemisphere?
Right. And you can have basically any sort of subset- you could ask me
question about any sort of subset of the categories you're looking at.
Right? So in this case Northern,
you would still- this question would still split out
this top part from these bottom pieces here.
One thing to be careful about though is that if you have q categories,
then you have- I mean,
you basically are considering every single possible subset of these categories.
So that's 2 to the q possible splits.
And so in general, you don't want to deal with too many categories because this will
become quickly intractable to look through that many possible examples.
It turns out that in certain very specific cases,
you can still deal with a lot of categories.
One such case is for binary classification where then you can just- the math is a little
bit complicated for this one but you can basically sort
your categories by how many positive examples are in each category,
and then just take that as like a sorted order then search through that linearly,
and it turns out that that yields to an optimal solution.
So decision trees, we can use them for regression,
we can also use them for categorical variables.
Um, one thing that I've not gotten into is that,
you can imagine that in the limit if you grew your tree without ever stopping,
you could end up just having a separate region for every single data point that you have.
Um, so that's really- you could consider that probably
over fitting if you ran it all the way to that completion, right?
So you can sort of see that decision trees are fairly high variance models.
So one thing that we're interested in doing is regularizing these high variance models.
And generally, how people have solved this problem is through a number of heuristics, okay?
So one such heuristic is that if you hit a certain minimum leaf size,
you stop splitting that leaf, okay?
So for example in this case if you've hit like you only have
four examples left in this leaf, then you just stop.
Another one is you can enforce a maximum depth,
and sort of a related one in this case is a max number of nodes.
And then a fourth very tempting one I've got to say to use is you say,
a minimum decrease in loss, right?
I say this one's tempting because it's generally not
actually a good idea to use this minimum decrease in loss 1.
You can think about that,
by thinking that if you have any sort of
higher-order interactions between your variables, um,
you might have to ask one question that is not very optimal,
or doesn't give you that much of an increase in loss,
and then your follow-up question combined with
that first question might give you a much better increase.
And you can sort of see that in this case,
where the initial latitude questions doesn't really give us that much of a gain.
We sort of split some positive and negatives,
but the combination of the latitude question
plus the time question really nails down what we want.
And if we were looking at it purely from the minimum decrease in loss perspective,
we might stop too early and miss that entirely.
And so a better way to do this kind of loss decrease is instead you grow out your full tree,
and then you prune it backwards instead.
So you grow out the whole thing and then you check which nodes to prune out.
Pruning. And how you generally do this, is you,
you take it- you have a validation set that you use this with,
and you evaluate what your misclassification error is on your validation set.
If for each example that you might remove for each leaf that you might remove.
So you would use misclassification in
this case with a validation set.
Any questions? Yeah?
The minimum decrease in loss.
The minimum decrease in loss?
So, um, yeah of course.
Uh, so you'll recall that before I was talking about sort of this RP,
this loss of R_parent versus loss of R_1 plus loss of R_2.
All right, so when we're- I had written out a maximization basically,
um, oh to be clear, the question is,
can you explain a little bit more clearly what this minimum decrease in loss means?
And so you have your loss of R_1 and R_2 versus your loss of R_parent, right?
So the split before the split, right,
you have your loss before split.
You have the loss of R_parent,
and then after split,
you have loss of R_1 plus loss of R_2.
Yeah. And if, if this decrease
between your loss of R_parent to your loss of your children is not great enough,
you might be tempted to say, "Okay,
that question didn't really gain us anything,
and so therefore we will not actually use that question."
But what I'm saying is that sometimes you have to ask multiple questions, right?
You have to ask sort of sub-optimal questions first to get to the really good questions,
especially if you have sort of interaction between your variables,
if there is some amount of correlation between your variables.
Okay. So we talked about regularization.
I said that we would get to run time,
let's actually just go up here again.
[NOISE] So let's cover that really quickly.
[NOISE]
Okay. So it'll be useful to define a couple of numbers at this point.
So say you have n examples.
[NOISE] You have f features,
and finally, you have, uh,
d- let's say the depth of your tree is d, okay.
All right. So you've gra- you, you have n examples that you trained on you- with the each
of f features and your resulting tree has depth d. So at test time,
your run-time is basically just your depth d, right?
[NOISE] It's just o of d,
right? Which is your depth.
And typically, though not in all cases, um,
d is sort of about- is less than the log of your number of examples.
And you can sort of think about this as if you have a fairly balanced tree right,
you'll end up sort of evenly splitting out all the examples and sort of recursively like
doing these binary splits and so you'll be splitting it at the log of that n. Okay.
So at test-time you've generally got it pretty quick.
Uh, at train time,
um, you have each point.
So if you return back to this example,
you'll see that each point, right,
once you've done a split only belongs to
the left or right of that split afterwards. All right.
So it's sort of like, like this point right here,
once you've split here will only ever be part of this region,
will never be considered on the other side,
on the right-hand side of that split. All right.
So if your, if your tree is of depth d, each point,
each point is part
of Od nodes.
Okay. And then at each node,
you can actually work out that the cost of evaluating that point
for- at training time is actually just proportional to the number of features f.
I won't get too much into the details of why this is,
but you can consider that if you're doing binary features, for example,
where each feature is just yes or no of some sort,
then you only have to consider,
if you have f features total,
you only have to consider,
um, f possible splits.
So that's why the cost in that case would be f,
and then if it was instead a, uh, quantitative feature,
I mentioned briefly that you could sort
the overall features and then scan through them linearly,
um, and that also ends up being asymptotically O of f to do that.
Okay. So each point is at most O of d nodes,
and then the cost of point at each node is O of f and you have n points total.
So the total cost is really just,
is just O of nfd, like this.
It turns out that this is actually surprisingly fast, uh,
especially if you consider that n
times f is just the size of your original design matrix,
right or your data matrix, all right.
Your data matrix is
of size n times f,
right, and then you're only- your,
your runtime is going through the data matrix that most depth times,
and since depth is log of n,
that turns out to be or generally bounded by log of n,
you have generally, a fairly,
fast training time as well.
Any questions about runtime?
[NOISE] Okay.
So I've been talking a lot about the good sides of decision trees right,
they seem pretty nice so far.
However, there are a number of downsides too.
Um, and one big one is that it doesn't have additive structure to it.
And so let me explain a little bit what that means.
Okay. So let's say now we have an example and you have just two features again,
so x1 and x2,
and you ca- say you define a line, okay,
just running through the middle defined by x1 equals x2.
And all the points above this line are positive,
and all the points below it are negative.
Now, if you have a simple linear model like logistic regression,
you'll have no issue with this kind of setup.
But for a decision tree, [LAUGHTER] basically,
you'd have to ask a lot of questions that even somewhat approximate this line.
Like, what you can try is you're going to say okay let's split it this way,
and maybe we can do a split this way and then now I split here,
maybe something like this,
and basically something like that, right?
Even here you- so you've asked a lot of questions and you've only gotten
a very rough approximation of the actual line that you've drawn in this case.
And so decision trees do have a lot of issues with these kind of structures
where the v- the features are interacting additively with one another.
Okay. So to recap so far,
since we've covered a number of different things about decision trees,
there's a number of pu- pluses and minuses to decision trees.
Okay. So on the plus side, they're actually,
I think this is an important point is that they're actually
pretty easy to explain, right?
If you're explaining what a decision tree is to like a non-technical person,
it's fairly obvious you're like okay you have this tree,
you're just playing 20 Questions with your data
and letting it co- come up with one question at a time.
There are also interpretable,
you can just draw out the tree especially for
shorter trees to see exactly what it's doing.
It can deal with categorical variables,
and it's generally pretty fast.
However, on the negative side,
one that I alluded to is that they're
fairly high variance models and so are oftentimes prone to overfitting your data.
They're bad at additive structure.
And then finally they have,
because in large part because of these first two,
they generally have fairly low predictive accuracy.
[NOISE] I know what you guys are thinking,
I just spent all this time talking about
decision trees and then I tell you guys they actually sort of suck.
So why did I actually cover decision trees?
And the answer is that in fact you can make decision trees a lot better through ensembling.
And a lot of the methods,
for example at the leading methods in Kaggle these days are
actually built on ensembles of decision trees,
and they really provide an ideal sort of model framework to look at,
through which we can examine a lot of these different ensembling methods.
Any questions about decision trees before I move on?
[NOISE] Yeah?
[inaudible].
I don't think that's strictly- Okay.
So the question is for the cross-entropy loss,
does the log need to be base 2?
And the answer is I'm pretty sure that it's not very relevant in this case,
I'm not 100% sure about that but I'm
pretty sure that the base of the log of that makes,
it's cross entropy loss actually initially came out of like information theory,
we have like computer bits and you're transmitting bits.
So it's useful to think in terms of bits of information that you can transmit,
which is why it came up as log base 2 in the initial formulation.
[NOISE]
Okay. So now let's talk about ensembling.
Okay. So why does ensembling help?
At some level, you can sort of think back to your basic, uh, statistics.
So say you have, um, you have XIs,
XIs, which are random variables.
I'll sometimes write this as just RV, um,
that are independent and identically distributed.
And so probably a lot of you are familiar with this
already or you can call this IID, okay.
Now say that your variance of one of these variables is Sigma squared.
Then what you can show is that the variance of the mean of many of these variables.
So let's- of many of these random variables or written alternatively,
1 over N sum over I to the XI is equal to Sigma
squared over N. And so
each independent variable you factor in is
decreasing the variance of your model, all right?
And so the thought is that if you can factor in a number of independent sources,
you can slowly decrease your variance.
Okay, so, uh, so that- though this is
a little bit simplistic of a way of looking at this,
because really all these different things are factoring
together have some amount of correlation with each other.
And so this independence assumption is oftentimes not correct.
So if instead,
you drop the independence assumption.
So now your variables are just ID, right?
Okay. And say we can characterize what the correlation between
any two XIs is and we can write that down as Rho. So Xi.
Then you can actually write out the variance of
your mean as
Rho Sigma squared- Sigma squared,
plus 1 minus Rho over M or- no,
N Sigma squared, okay?
And so you can sort of see that if your correlation- if they're fully correlated,
then your- this term will drop to 0 and
that you'll just have Sigma squared again because adding
a bunch of fully correlated variables is just gonna give you
the original variable's variance versus if they're
completely decorrelated then this term drops to 0 and you
just end up with Sigma squared over N which gives you the initial,
uh, independent identically distributed equation.
And so in this case, really what you wanna do- the name of the game is,
you wanna have as many different models that you're
factoring as possible to increase this N which drives this term down.
And then on the other hand, you also want to make sure
those models are as decorrelated as possible so
that your Rho goes down and this first term goes down as well, okay?
And so this gives rise to a number of different ways to ensemble.
And one way you could think about doing this is you just use different algorithms, right?
This is actually what a lot of people in Kaggle, for example, will do,
is they'll just take a neural network or Random Forest, an SVM,
average them all together and generally that
actually works pretty well but- then you sort of have to
spend your time implementing all these separate algorithms
which is oftentimes not the most efficient use of your time.
Another one that people would like to do is just use different training sets.
Okay. And again, in this case,
like you probably spent a lot of effort collecting your initial training set,
you don't want your- like machine learning person to just come and recommend to you that,
just go collect a whole second training set
or something like that to improve your performance.
Like that's generally not the most helpful recommendation, okay?
And so then, what we're gonna cover now are
these two other methods that we use to do ensembling.
And one of them is called bagging,
which is sort of trying to approximate having different training sets.
We'll get into that quickly.
And then you also have boosting.
And just so that you guys will have a little bit of context,
we're gonna be using decision trees to talk a lot about these models;
and so bagging, you might have heard of random forests,
that's a variant of bagging for decision trees.
And then for boosting,
you might have heard of things like AdaBoost,
or XGBoost, which are variants of boosting for decision trees.
Okay, so that sort of covers at a high level what we would wanna do.
These first two are very nice because they're sort of would give us a much
more like independently correlated- or less correlated variables.
But generally, we're- we end up doing these latter two
because we don't want to collect new training sets or train entirely new algorithms.
Okay, so let's cover bagging first.
Okay, so bagging really stands for this thing.
It's called bootstrap aggregation, okay?
Um, and so- first,
let's just break down this term.
So bootstrap, what that is,
is it's typically this method used in statistics to measure
the uncertainty of your estimate, okay?
And so what- what is useful to define in this case for when you're talking about bagging
is you can say that you have
a true population P, okay?
And your training set- training
set S is sampled from P, right?
So you just start drawing a bunch of examples from
P and that's what forms your training set at some level.
And so ideally, like for example,
this different training set's approach.
What you do is, you just draw S1, S2,
S3, S4, and then train your model in each one of those separately.
Unfortunately, you generally don't have the time to do that.
And so what ba- what Bootstrapping does,
is you assume basically that your population is your training sample, okay?
So you assume that your population is your training sample.
And so now that you have this S is approximating your P,
then you can draw new samples from
your population by just drawing samples from S instead, okay?
So you have bootstrap samples, is what they're called.
Z sampled from S. And so how that
works is you basically just take your train- your- your training sample, okay?
Say it's of like cardinality N or something.
And you just sample N times from S and this is important,
you do it with replacement.
Because they're pretending that this is the population,
and so doing it with replacement sort of makes that assumption
hold that you're sampling from it as a population.
Okay, so that's bootstrapping.
So you generate all these different bootstrap samples Z on your- from your training set.
And what you can do is you can take your model and
train it on all these separate bootstrap samples,
and then you can sort of look at the variability in
the predictions that your model ends up
making based on these different bootstrap samples.
And that gives you sort of a measure of uncertainty.
I'm not gonna go into too much detail now because that's
not actually what we're gonna use Bootstrapping for.
What we want to use bootstrapping for is we wanna aggregate these two Bootstrap samples.
And so at a very high level,
what that means is we're gonna take a bunch of Bootstrap samples,
train separate models on each and then average their outputs, okay?
So let's make that a little bit more formal.
[NOISE]
So you have bootstrap samples
Z_1 through Z_M say, okay, capital M.
That's just say how many bootstrap samples you're going to take.
Okay, you train [NOISE] a model,
G_M, okay, on Z_M, okay?
Then all you're doing is you're just defining this new sort of meta model.
I'm not putting a subscript on this one to show that it's a meta model, T of M,
which is just the sum of your predictions of your individual models,
divided by the total number of models you have, all right?
And this is just me writing out what I was sort of talking about right up there for bagging.
If you're taking these bootstrap samples and then you're training separate models,
and then you're just aggregating them all together to get this bagging approach.
So if we just do a little bit of analysis from the bias-variance perspective on this,
we can sort of see why this kind of thing might work.
[NOISE]
And so you recall we had this equation up here, right?
The va- variance of the mean is rho sigma squared,
plus 1 minus rho,
over n of sigma squared.
So let me just write that out here.
[NOISE]
And in this case,
our M is actually really uh,
just the number of bootstrap samples.
So we'll just use big M in this case.
And what you're doing is by taking these bootstrap samples,
you're sort of decorrelating the models you're training.
Your bootstrapping [NOISE] is
driving down [NOISE] rho.
Okay. And so by driving this down,
you're sort of making this term get smaller and smaller.
And then your question might be okay,
what about this term here?
And it turns out that basically you can take as many bootstrap samples as you want,
and that will slowly drive down- it increases M and drive down this second term.
And it turns out that one nice thing about bootstrapping,
is that increasing the number of bootstrap models in your training,
doesn't actually cause you to overfit anymore than you were beforehand.
Because all you're doing, is you're driving down this term here.
So more M [NOISE] and it's just less in variance.
[NOISE] All you're doing
is driving down the second term as much
as possible when you're getting more and more bootstrap samples.
So generally, it only improves performance.
And so generally what people will do is they'll sample
more and more models until they see that their error stops going down.
Because that means they basically eliminated this term over here.
So this seems kinda nice, right?
You're decreasing the variance, where is the trade-off coming in? Oh, there is a question there.
[inaudible].
Yeah, there's definitely a bound, right?
Because um, I'm not going to define one formally right now.
Oh, the question is can you define a bound on how much you decrease rho by?
Uh, I'm not- yeah,
so there's definitely a lower bound [NOISE] or, oh yeah,
a lower bound on how far you can decrease rho.
Basically it comes down to your bootstrap samples are still
fairly highly correlated with one another, all right.
Because they're still just drawing it from the same sample set S. Really,
your Z is gonna end up containing about two- each Z is going to contain
about two thirds of S. And so
your Zs are still gonna be fairly highly correlated with each other.
And no, I don't have a formal equation to write down as to
exactly how much that decreases rho by,
or how much that bounds rho by,
you can sort of see intuitively that there is a bound there and that you can't just
magically decrease rho all the way down to 0 and achieve 0 variance.
[NOISE] All right.
So saying that you decrease variance,
that seems very nice.
One issue that comes up with, with uh,
bootstrapping is that in fact you're actually slightly
increasing the bias of your models when you're doing this.
And the reasoning for that
[NOISE] is because of
this sub-sampling that I was talking about here.
Each one of your Zs is now about two-thirds of the
original S. So you're training on less data um,
and so your models are becoming slightly less uh, you know,
complex and so that increases your bias in this case. Yes.
[inaudible]
Yeah, for sure. Um, so the question is,
can you explain the difference between
a random variable and an algorithm in this case, right?
And so you could sorta- at a, at a very high level,
you can think of an algorithm as a classifier- as
a function that's taking in some data and making a prediction.
Right? And if you sort of see those- that whole setup as
sort of like, the probability, the algorithm is
giving some sort of output in the probabilistic perspective,
you can sort of see the algorithm as like a random variable in the case- in this case.
Sort of like, you're basically considering,
sort of the space of possible predictions that
your algorithm can make and that you can sort of see as
a distribution of possible predictions
and that you can approximate that as a random variable.
I mean it is a random variable at some level,
because it's sort of like based on what's training sample you end up with,
your predictions of your output model are gonna change.
And so since you're sampling sort of these random samples from your population set,
you can consider your algorithm as sort of based
on that random sample and therefore a random variable itself.
Okay. So yeah, your bias is slightly increased because [NOISE] of random subsampling,
[NOISE] but generally,
the decrease in variance that you get from doing this,
is much larger than the slight increase in bias you get from,
from doing this randomized subsampling.
So in a lot of cases,
bagging is quite nice.
[NOISE] Okay?
So I've talked a bit about ba- about bagging,
uh, let's talk about decision trees plus bagging now.
Okay. So you recall that decision trees are
high [NOISE] variance,
low bias okay?
And this right here sort of explains why they're a pretty good fit for bagging.
Okay? Because bagging what you're doing,
is you're decreasing the variance of your models for a slight increase in bias.
And since most of your error from
your decision trees is coming from the high variance side of things,
by sort of driving down that variance,
you get a lot more benefit than for a,
a model that would be on the reverse high bias and low variance.
Okay? So, so this makes this like an ideal fit [NOISE] for bagging.
[NOISE]
Okay. [NOISE] So now,
um, this is sort of decision trees plus bagging.
I said that random forests are sort of a version of decision trees plus bagging.
And so what I've described here is actually almost a random forest at this point.
The one key point we're still missing is that random forests
actually introduce even more randomization into each individual decision tree.
And the idea behind that is that- as I had a question from before is this Rho,
you can only drive it down so far through just pure bootstrapping.
But if you can further decorrelate your different random variables,
then you can drive down that variance even further, okay?
Um, and so the idea there is that basically for- at each split for random forests,
at each split, you consider only a fraction
of your total features, right?
So it's sort of like, for that ski example,
maybe like for the first split,
I only let it look at latitude, and then for the second split,
I only let it look at,
uh, the time of the year.
[NOISE] And so this might seem a little bit unintuitive at first,
but you can sort of get the intuition from two ways.
One is that you're decreasing
Rho and then the other one
is you can think that- say you have a classification example,
where you have one very strong predictor that gets you very good performance on its own.
And regardless of what bootstrap sample you select,
your model is probably gonna use that predictor as its first split.
That's gonna cause all your models to be very highly
correlated right at that first split, for example,
and by instead forcing it to,
to sample from different features.
Instead, that's going to increase the,
uh, or decrease the correlation between your models.
And so it's all about decorrelating your models in this case.
[NOISE] Okay.
And that sort of brings to a close a lot of our discussion of bagging.
Are there any questions regarding bagging?
Okay. Now, I've covered bagging.
Let's get a little bit into boosting.
[NOISE] And I'll make this quick.
But basically, whereas bagging we sort of saw in the intuition that
we were decreasing variance,
boosting is sort of actually more of the opposite where
you're decreasing the bias of your models, okay?
So- [NOISE]
and also it- it's basically,
um, more additive in,
um, in how it's doing things.
So versus- [NOISE] you'll recall that for bagging,
you were taking the average of a number of variables.
In boosting, what happens,
you train one model and then you add that prediction into your ensemble.
And then when you turn a new model, you just add that in as a prediction.
And so- and that's a little bit handwavy right now.
So let me actually make that clear through an example.
[NOISE] So say you have a dataset, again, X1,
X2, X2 and you have some data points,
maybe some- that's actually- just call them pluses and minuses.
So you have some more pluses here,
and then maybe a couple of minuses and some pluses here, okay?
And what you- say you're training a size one decision tree.
So decision stumps is what we call them.
And so you only get to ask one question at a time.
And the reason behind this,
just really quickly is that because you're decreasing bias
by restricting your trees to be only depth 1,
you basically are increasing
their amount of bias and decreasing their amount of variance,
which makes them a better fit for boosting kind of methods.
And say that you come up with a,
a decision boundary, okay?
Say this one here, okay?
And what you're gonna do is, on this side you predict positive, right?
And on this side you predict negative.
There's like a reasonable like line that you could draw here,
but it's not perfect, right?
You've made some mistakes.
And in fact, what you can do is you can sort of identify these mistakes.
Now, if we draw this in red, right?
You've got- made these guys as mistakes.
And what boosting does is basically it increases the weights of the mistakes you've made.
And then for the next out- uh,
decision stump that you train,
it's now trained on this modified set.
Which we could, let's just draw it over here.
One. [NOISE] And so now you- these positives,
I'll just draw them much bigger.
You know, you've got big positives here and some small negatives,
and some small positives,
some big negatives here.
And so now your model, to try and get these right,
might pick a decision boundary like this, right?
And this is also basically recursive in that each step, right?
You're gonna be reweighting each of the examples based on how many of
your previous ones have gotten it wrong or right in the past.
[NOISE] And so basically what you're
doing is you can sort of weight each one of these classifiers.
You can determine
[NOISE]
for classifier Gm,
a weight Alpha m,
which is proportional to how many examples you got wrong or right.
So a better classifier, you wanna give it more weight, um, and,
uh, a bad classifier you wanna give it less weight proportional.
And, uh, I think that the exact equation used in AdaBoost, for example,
is just log of 1 minus the error
of your nth model divided with basically log odds, okay?
And then your total classifier is just F of- or let's just call it G of x again.
G of x is just the sum over m of Alpha m,
G of m, right?
And then each G of m is trained
on a weighted- on a reweighted,
actually, reweighted training set.
And so I've glossed over a lot of the details here in interest of time,
but the specifics of an algorithm like this are- will be in the lecture notes.
And this algorithm is actually known as AdaBoost.
[NOISE] And basically through similar techniques,
you can derive algorithms such as XGBoost
or gradient boosting machines that also allow you to
basically reweight the examples you're getting right
or wrong in this sort of dynamic fashion
and slowly adding them in this additive fashion to your composite model.
[NOISE] And that about finishes it for today.
Uh, thanks for coming.
Um, yeah, a great rest of your week.
 Hello everyone. Uh, welcome to CS229.
Um, today we're going to talk about,
um, deep learning and neural networks.
Um, we're going to have two lectures on that,
one today and a little bit more of it on, ah, Monday.
Um, don't hesitate to ask questions during the lecture.
Ah, so stop me if you don't understand something and we'll try
to build intuition around neural networks together.
We will actually start with an algorithm that you guys have seen,
uh, previously called logistic regression.
Everybody remembers logistic regression?
Yes.
Okay. Remember it's a classification algorithm.
Um, we're going to do that.
Explain how logistic regression can be interpreted as
a neural network- specific case of a neural network and then,
we will go to neural networks. Sounds good?
So the quick intro on deep learning.
So deep learning is a- is a set of techniques that is let's say a subset of
machine learning and it's one of the growing techniques that have been
used in the industry specifically for problems in computer vision,
natural language processing and speech recognition.
So you guys have a lot of different tools and,
and uh, plug-ins on your smartphones that uses this type of algorithm.
Ah, the reason it came, uh,
to work very well is primarily the,
the new computational methods.
So one thing we're going to see to- today, um,
is that deep learning is really really computationally expensive and we- people had to
find techniques in order to parallelize the code and
use GPUs specifically in order to graphical processing units,
in order to be able to compute,
uh, the, the, the,
the computations in deep learning.
Ah, the second part is the data available has been growing after,
after the Internet bubble,
the digitalization of the world.
So now people have access to large amounts of data and this type of algorithm has
the specificity of being able to learn a lot when there is a lot of data.
So these models are very flexible and the more you give them data,
the more they will be able to understand the salient feature of the data.
And finally algorithms.
So people have come up with, with new techniques,
uh, in order to use the data,
use the computation power and build models.
So we are going to touch a little bit on all of that,
but let's go with logistic regression first.
Can you guys see in the back?
Yeah? Okay, perfect.
So you remember,
uh, what logistic regression is?
What- we are going to fix a goal for us,
uh, that, uh, is a classification goal.
So let's try to,
to find cats in images.
So find cats in images.
Meaning binary classification.
If there is a cat in the image,
we want to output a number that is close to 1, presence of the cat,
and if there is no cat in the image, we wanna output 0.
Let- let's say for now, ah,
we're constrained to the fact that there is maximum one cat per image, there's no more.
If you are to draw the logistic regression model,
that's what you would do.
You would take a cat.
So this is an image of a cat.
I'm very bad at that.
Um, sorry.
In computer science,
you know that images can be represented as 3D matrices.
So if I tell you that this is a color image of size 64 by 64,
how many numbers do I have to represent those pixels?
[BACKGROUND] Yeah, I heard it,
64 by 64 by 3.
Three for the RGB channel, red, green, blue.
Every pixel in an image can be represented by three numbers.
One representing the red filter,
the green filter, and the, and the blue filter.
So actually this image is of size 64 times 64 times 3.
That makes sense? So the first thing we will do in
order to use logistic regression to find if there is a cat in this image,
we're going to flatten th- this into a vector.
So I'm going to take all the numbers in this matrix and flatten them in a vector.
Just an image to vector operation, nothing more.
And now I can use my logistic regression because I have a vector input.
So I'm going to, to take all of these and push them in an operation that
we call th- the logistic operation which has one part that is wx plus b,
where x is going to be the image.
So wx plus b,
and the second part is going to be the sigmoid.
Everybody is familiar with the sigmoid function?
Function that takes a number between
minus infinity and plus infinity and maps it between 0 and 1.
It is very convenient for classification problems.
And this we are going to call it y hat,
which is sigmoid of what you've seen in class previously,
I think it's Theta transpose x.
But here we will just separate the notation into w and b.
So can someone tell me what's the shape of w?
The matrix W, vector matrix.
Um, what?
Yes, 64 by 64 by 3 as a- yeah.
So you know that this guy here is a vector of 64 by 64 by 3, a column vector.
So the shape of x is going to be 64 by 64 by 3 times 1.
This is the shape and this,
I think it's- that if I don't know,
12,288 and this indeed because we want y-hat to be one-by-one,
this w has to be 1 by 12,288.
That makes sense? So we have a row vector as our parameter.
We're just changing the notations of the logistic regression that you guys have seen.
And so once we have this model,
we need to train it as you know.
And the process of training is that first,
we will initialize our parameters.
These are what we call parameters.
We will use the specific vocabulary of weights and bias.
I believe you guys have heard this vocabulary before, weights and biases.
So we're going to find the right w and the right b in order to be able,
ah, to use this model properly.
Once we initialized them,
what we will do is that we will optimize them,
find the optimal w and b,
and after we found the optimal w and b,
we will use them to predict.
Does this process make sense? This training process?
And I think the important part is to understand what this is.
Find the optimal w and b means defining your loss function which is the objective.
And in machine learning, we often have this, this,
this specific problem where you have a function that you know you want to find,
the network function, but you don't know the values of its parameters.
In order to find them, you're going to use
a proxy that is going to be your loss function.
If you manage to minimize the loss function,
you will find the right parameters.
So you define a loss function,
that is the logistic loss.
Y log of y hat plus 1 minus y log of 1 minus y hat up.
You guys have seen this one.
You remember where it comes from?
Comes from a maximum likelihood estimation,
starting from a probabilistic model.
And so the idea is how can I minimize this function.
Minimize, because I've put the minus sign here.
I want to find w and b that
minimize this function and I'm going to use a gradient descent algorithm.
Which means I'm going to
iteratively compute the derivative of the loss with respect to my parameters.
And at every step, I will update them to make
this loss function go a little down at every iterative step.
So in terms of implementation, this is a for loop.
You will loop over a certain number of iteration and at every point,
you will compute the derivative of the loss with respect to your parameters.
Everybody remembers how to compute this number?
Take the derivative here, you use the fact that the sigmoid function
has a derivative that is sigmoid times 1 minus sigmoid,
and you will compute the results.
We- we're going to do some derivative later today.
But just to set up the problem here.
So, the few things that I wanna- that I wanna touch on here is,
first, how many parameters does this model have? This logistic regression?
If you have to, count them.
So this is the numb- 089 yeah, correct.
So 12,288 weights and 1 bias. That makes sense?
So, actually, it's funny because you can quickly count it by just counting
the number of edges on the- on the- on the drawing plus 1.
Every circle has a bias.
Every edge has a weight because ultimately this
operation you can rewrite it like that, right?
It means every weight has- every weight corresponds to an edge.
So that's another way to count it,
we are going to use it a little further.
So we're starting with not too many parameters actually.
And one thing that we notice is that the number of
parameters of our model depends on the size of the input.
We probably don't want that at some point,
so we are going to change it later.
So two equations that I want you to remember is,
the first one is neuron equals linear plus activation.
So this is the vocabulary we will use in neural networks.
We define a neuron as an operation that has two parts,
one linear part, and one activation
part and it's exactly that. This is actually a neuron.
We have a linear part, wx plus b and
then we take the output of this linear part and we put it in an activation,
that in this case, is the sigmoid function.
It can be other functions, okay?
So this is the first equation, not too hard.
The second equation that I wanna set now is the model
equals architecture plus parameters.
What does that mean?
It means here we're, we're trying to train a logistic regression in order to,
to be able to use it.
We need an architecture which is the following,
a one neuron neural network and the parameters w and b.
So basically, when people say we've shipped a model,
like in the industry, what they're saying is that they found the right parameters,
with the right architecture.
They have two files and these two files are predicting a bunch of things, okay?
One parameter file and one architecture file.
The architecture will be modified a lot today.
We will add neurons all over and the parameters will always be called w and b,
but they will become bigger and bigger.
Because we have more data,
we want to be able to understand it.
You can get that it's going to be hard to understand what a cat is with only that,
that, that many parameters.
We want to have more parameters.
Any questions so far?
So this was just to set up the problem with logistic regression.
Let's try to set a new goal,
after the first goal we have set prior to that.
So the second goal would be, find cat,
a lion, iguana in images.
So a little different than before,
only thing we changed is that we want to now to detect three types of animals.
Either if there's a cat in the image,
I wanna know there is a cat.
If there's an iguana in the image,
I wanna know there is an iguana.
If there is a lion in the image,
I wanna know it as well.
So how would you modify the network
that we previously had in order to take this into account?
Yeah? Yeah, good idea.
So put two more circles,
so neurons, and do the same thing.
So we have our picture here with the cats.
So the cat is going to the right.
64 by 64 by 3 we flatten it, from x1 to xn.
Let's say n represents 64, 64 by 3 and what I will do,
is that I will use three neurons that are all computing the same thing.
They're all connected to all these inputs, okay?
I connect all my inputs x1 to xn to each of these neurons,
and I will use a specific set of notation here. Okay.
Y_2 hat equals a_2_1 sigmoid of
w_ 2_1 plus x plus b_2_1.
And similarly, y_3 hat equals a_3_1,
which is sigmoid of w_ 3_1 x plus b_3_1.
So I'm introducing a few notations here and we'll,
we'll get used to it, don't worry.
So just, just write this down and we're going to go over it.
So [NOISE] the square brackets here represent what we will call later on a layer.
If you look at this network,
it looks like there is one layer here.
There's one layer in which neurons don't communicate with each other.
We could add up to it,
and we will do it later on,
more neurons in other layers.
We will denote with square brackets the index of the layer.
The index, that is, the subscript to this a
is the number identifying the neuron inside the layer.
So here we have one layer.
We have a_1, a_2,
and a_3 with square brackets one to identify the layer. Does that make sense?
And then we have our y-hat that instead of being a single number as it was before,
is now a vector of size three.
So how many parameters does this network have?
[NOISE]
How much?
[BACKGROUND]
Okay. How did you come up with that?
[BACKGROUND].
Okay. Yeah, correct. So we just have three times the,
the thing we had before because we added
two more neurons and they all have their own set of parameters.
Look like this edge is a separate edge as this one.
So we, we have to replicate parameters for each of these.
So w_1_1 would be the equivalent of what we had for the cat,
but we have to add two more,
ah, parameter vectors and biases.
[NOISE] So other question,
when you had to train this logistic regression,
what dataset did you need?
[NOISE]
Can someone try
to describe the dataset. Yeah.
[BACKGROUND]
Yeah, correct. So we need images and labels with it labeled as cat,
1 or no cat, 0.
So it is a binary classification with images and labels.
Now, what do you think should be the dataset to train this network? Yes.
[BACKGROUND]
That's a good idea. So just to repeat.
Uh, a label for an image that has a cat would probably be
a vector with a 1 and two 0s
where the 1 should represent the prese- the presence of a cat.
This one should represent the presence of
a lion and this one should represent the presence of an iguana.
So let's assume I use this scheme to label my dataset.
I train this network using the same techniques here.
Initialize all my weights and biases with a value, a starting value,
optimize a loss function by using gradient descent,
and then use y-hat equals, uh, log to predict.
What do you think this neuron is going to be responsible for?
If you had to describe the responsibility of this neuron.
[BACKGROUND]
Yes. Well, this one.
Lion.
Yeah, lion and this one iguana.
So basically the, the,
the way you- yeah, go for it.
[BACKGROUND]
That's a good question. We're going to talk about that now.
Multiple image contain different animals or not.
So going back on what you said,
because we decided to label our dataset like that,
after training, this neuron is naturally going to be there to detect cats.
If we had changed the labeling scheme and I said
that the second entry would correspond to the cat,
the presence of a cat, then after training,
you will detect that this neuron is responsible for detecting a cat.
So the network is going to evolve depending on the way you label your dataset.
Now, do you think that
this network can still be robust to different animals in the same picture?
So this cat now has,
uh, a friend [NOISE] that is a lion.
Okay, I have no idea how to draw a lion,
but let's say there is a lion here and because there is a lion,
I will add a 1 here.
Do you think this network is robust to this type of labeling?
[BACKGROUND]
It should be. The neurons aren't talking to each other.
That's a good answer actually. Another answer.
[BACKGROUND]
That's a good, uh, intuition because the network,
what it sees is just 1, 1,
0, and an image.
It doesn't see that this one corresponds to- the cat
corresponds to the first one and the second- and the lion corresponds to the second one.
So [NOISE] this is a property of neural networks,
it's the fact that you don't need to tell them everything.
If you have enough data,
they're going to figure it out.
So because you will have also cats with iguanas,
cats alone, lions with iguanas, lions alone,
ultimately, this neuron will understand what it's looking for,
and it will understand that this one corresponds to this lion.
Just needs a lot of data.
So yes, it's going to be robust.
And that's the reason you mentioned.
It's going to be robust to that because the three neurons aren't communicating together.
So we can totally train them independent- independently from each other.
And in fact, the sigmoid here,
doesn't depend on the sigmoid here and doesn't depend on the sigmoid here.
It means we can have one, one,
and one as an output.
[NOISE] Yes, question.
[BACKGROUND]
You could, you could, you could think about it as three logistic regressions.
So we wouldn't call that a neural network yet.
It's not ready yet, but it's
a three neural network or three logistic regression with each other.
[NOISE].
Now, following up on that,
uh, yeah, go for it. A question.
[BACKGROUND]
W and b are related to what?
[BACKGROUND]
Oh, yeah. Yeah. So, so usually you would have Theta transpose x,
which is sum of Theta_i_x_i, correct?
And what I will split it is,
I will spit it in sum of Theta_i_x_i plus Theta_0 times 1.
I'll split it like that.
Theta_0 would correspond to b and these Theta_is would correspond to w_is, make sense?
Okay. One more question and then we move on.
[BACKGROUND]
Good question. That's the next thing we're going to see.
So the question is a follow up on this,
is there cases where we have a constraint where there is only one possible outcome?
It means there is no cat and lion,
there is either a cat or a lion,
there is no iguana and lion,
there's either an iguana or a lion.
Think about health care.
There are many, there are many models that are made to detect,
uh, if a disease, skin disease is present on- based on cell microscopic images.
Usually, there is no overlap between these, it means,
you want to classify a specific disease among a large number of diseases.
So this model would still work but would not be optimal because it's longer to train.
Maybe one disease is super, super
rare and one of the neurons is never going to be trained.
Let's say you're working in a zoo where there is
only one iguana and there are thousands of lions and thousands of cats.
This guy will never train almost,
you know, it would be super hard to train this one.
So you want to start with another model that- where you put the constraint, that's, okay,
there is only one disease that we want to predict and let the model
learn, with all the neurons learn together by creating interaction between them.
Have you guys heard of softmax?
Yes? Somebody, ah, I see that in the back.
[LAUGHTER] Okay. So let's look at softmax a little bit together.
So we set a new goal now,
which is we add
a constraint which is an unique animal on an image.
So at most one animal on an image.
So I'm going to modify the network a little bit.
We're go- we have our cat and there is no lion on the image, we flatten it,
and now I'm going to use the same scheme with the three neurons, a_1, a_2, a_3.
But as an output,
what I am going to use is an exponent, a softmax function.
So let me be more precise, let me,
let me actually introduce another notation to make it easier.
As you know, the neuron is a linear part plus an activation.
So we are going to introduce a notation for the linear part,
I'm going to introduce Z_11 to represent the linear part of the first neuron.
Z_112 to introduce the linear part of the second neuron.
So now our neuron has two parts,
one which computes Z and one which computes a,
equals sigmoid of Z.
Now, I'm going to remove all the activations and make
these Zs and I'm going to use the specific formula.
So this, if you recall,
is exactly the softmax formula.
[NOISE] Yeah.
Okay. So now the network we have,
can you guys see or it's too small? Too small?
Okay, I'm going to just write this formula
bigger and then you can figure out the others, I guess, because,
e of Z_3, 1 divided by sum from
k equals 1 to 3 of e, exponential of ZK_1.
Okay, can you see this one?
So here is for the third one.
If you are doing it for the first one,
you will add- you'll just change this into a 2,
into a 1 and for the second one into a 2.
So why is this formula interesting and why is it not
robust to this labeling scheme anymore?
It's because the sum of the outputs of this network have to sum up to 1. You can try it.
If you sum the three outputs,
you get the same thing in the numerator and on
the denominator and you get 1. That makes sense?
So instead of getting a probabilistic output for each,
each of y, if,
each of y hat 1, y hat 2,
y hat 3, we will get a probability distribution over all the classes.
So that means we cannot get 0.7, 0.6,
0.1, telling us roughly that there is probably a cat and a lion but no iguana.
We have to sum these to 1.
So it means, if there is no cat and no lion,
it means there's very likely an iguana.
The three probabilities are dependent on each
other and for this one we have to label the following way,
1, 1, 0 for a cat, 0, 1,
0 for a lion or 0,
0, 1 for an iguana.
So this is called a softmax multi-class network.
[inaudible].
You assume there is at least one of the three classes,
otherwise you have to add a fourth input that will represent an absence of an animal.
But this way, you assume there is always one of these three animals on every picture.
And how many parameters does the network have?
The same as the second one.
We still have three neurons and although I didn't write it,
this Z_1 is equal to w_11,
x plus b_1, Z_2 same, Z_3 same.
So there's 3n plus 3 parameters.
So one question that we didn't talk about is,
how do we train these parameters?
These, these parameters, the 3n plus 3 parameters, how do we train them?
You think this scheme will work or no?
What's wrong, what's wrong with this scheme?
What's wrong with the loss function specifically?
There's only two outcomes.
So in this loss function,
y is a number between 0 and 1,
y hat same is the probability,
y is either a 0 or 1,
y hat is between 0 and 1,
so it cannot match this labeling.
So we need to modify the loss function.
So let's call it loss three neuron.
What I'm going to do is I'm going to just sum it up for the three neurons.
Does this make sense? So I am just doing three times this loss for each of the neurons.
So we have exactly three times this.
We sum them together.
And if you train this loss function,
you should be able to train the three neurons that you have.
And again, talking about scarcity of one of the classes.
If there is not many iguana,
then the third term of this sum is
not going to help this neuron train towards detecting an iguana.
It's going to push it to detect no iguana.
Any question on the loss function?
Does this one make sense? Yeah?
[inaudible]
Yeah. Usually, that's what will happen is
that the output of this network once it's trained,
is going to be a probability distribution.
You will pick the maximum of those,
and you will set it to 1 and the others to 0 as
your prediction. One more question, yeah.
[inaudible]
If you use the 2-1.
If you use this labeling scheme like 1-1-0 for this network,
what do you think it will happen?
It will probably not work.
And the reason is this sum is equal to 2,
the sum of these entries,
while the sum of these entries is equal to 1.
So you will never be able to match
the output to the input to the label. That makes sense?
So what the network is probably going to do
is it's probably going to send this one to one-half,
this one to one-half, and this one to 0 probably, which is not what you want.
Okay. Let's talk about the loss function for this softmax regression.
[NOISE] Because you know
what's interesting about this loss is if I take this derivative,
derivative of the loss 3N with respect to W2_1.
You think is going to be harder than this derivative,
than this one or no?
It's going to be exactly the same.
Because only one of these three terms depends on W12.
It means the derivative of the two others are 0.
So we are exactly at the same complexity during the derivation.
But this one, do you think if you try to compute,
let's say we define a loss function that corresponds roughly to that.
If you try to compute the derivative of the loss with respect to W2,
it will become much more complex.
Because this number, the output here that is going to impact the loss function directly,
not only depends on the parameters of W2,
it also depends on the parameters of W1 and W3.
And same for this output.
This output also depends on the parameters W2.
Does this makes sense? Because of this denominator.
So the softmax regression needs a different loss function and a different derivative.
So the loss function we'll define is a very common one in
deep learning, it's called the softmax cross entropy.
Cross entropy loss.
I'm not going to- to- into the details of where it comes from but you can
get the intuition of yklog.
So it, it surprisingly looks like the binary croissant,
the binary, uh, the logistic loss function.
The only difference is that we will sum it up on all the- on all the classes.
Now, we will take a derivative of something that looks like that later.
But I'd say if you can try it at home on this one,
uh, it would be a good exercise as well.
So this binary cross entropy loss is very
likely to be used in classification problems that are multi-class.
Okay. So this was the first part on logistic regression types of networks.
And I think we're ready now with the notation that we
introduced to jump on to neural networks.
Any question on this first part before we move on?
So one- one question I would have for you.
Let's say instead of trying to predict if there is a cat or no cat,
we were trying to predict the age of the cat based on the image.
What would you change? This network.
Instead of predicting 1-0,
you wanna predict the age of the cat.
What are the things you would change? Yes.
[inaudible].
Okay. So I repeat.
I, I basically make
several output nodes where each of them corresponds to one age of cats.
So would you use this network or the third one?
Would you use the three neurons neural network or the softmax regression?
Third one.
The third one. Why?
You have a unique age.
You have a unique age,
you cannot have two ages, right.
So we would use a softmax one because we want
the probability distribution along the edge, the ages.
Okay. That makes sense. That's a good approach.
There is also another approach which is using directly a regression to predict an age.
An age can be between zero and plus infi- not plus infinity-
[LAUGHTER].
-zero and a certain number.
[LAUGHTER] And, uh, so let's say you wanna do a regression,
how would you modify your network?
Change the Sigmoid.
The Sigmoid puts the Z between 0 and 1.
We don't want this to happen.
So I'd say we will change the Sigmoid.
Into what function would you change the Sigmoid?
[inaudible]
Yeah. So the second one you said was?
[inaudible]
Oh, to get a Poisson type of distribution.
Okay. So let's, let's go with linear. You mentioned linear.
We could just use a linear function,
right, for the Sigmoid.
But this becomes a linear regression.
The whole network becomes a linear regression.
Another one that is very common in,
in deep learning is called the Rayleigh function.
It's a function that is almost linear,
but for every input that is negative, it's equal to 0.
Because we cannot have negative h,
it makes sense to use this one.
Okay. So this is called rectified linear units, ReLU.
It's a very common one in deep learning.
Now, what else would you change?
We talked about linear regression.
Do you remember the loss function you are using in linear regression? What was it?
[inaudible]
It was probably one of these two;
y hat minus y.
This comparison between the output label and y-hat,
the prediction, or it was the L2 loss;
y-hat minus y in L2 norm.
So that's what we would use. We would modify
our loss function to fit the regression type of problem.
And the reason we would use this loss instead of the one we have
for a regression task is because in optimization,
the shape of this loss is much easier to optimize for
a regression task than it is for a classification task and vice versa.
I'm not going to go into the details of that but that's the intuition.
[NOISE] Okay.
Let's go have fun with neural networks.
[NOISE]
So we, we stick to our first goal.
Given an image, tell us if there is cat or no cat.
This is 1, this is 0.
But now we're going to make a network a little more complex.
We're going to add some parameters.
So I get my picture of the cat.
[NOISE] The cat is moving.
Okay. And what I'm going to do is that I'm going to put more neurons than before.
Maybe something like that.
[NOISE]
So using the same notation you see that
my square bracket-
So using the same notation,
you see that my square bracket here is two
indicating that there is a layer here which is the second layer
[NOISE] while this one is the first layer and this one is the third layer.
Everybody's, er, up to speed with the notations?
Cool. So now notice that when you make a choice of architecture,
you have to be careful of one thing,
is that the output layer has to have the same number of neurons as you
want, the number of classes to be for reclassification and one for a regression.
So, er, how many parameters does this - this network have?
Can someone quickly give me the thought process?
So how much here?
Yeah, like 3n plus 3 let's say.
[inaudible].
Yeah, correct. So here you would have 3n weights plus 3 biases.
Here you would have 2 times 3 weights plus 2 biases because you have
three neurons connected to two neurons and here you will have 2 times 1 plus 1 bias.
Makes sense. So this is the total number of parameters.
So you see that we didn't add too much parameters.
Most of the parameters are still in the input layer.
Um, let's define some vocabulary.
The first word is Layer.
Layer denotes neurons that are not connected to each other.
These two neurons are not connected to each other.
These two neurons are not connected to each other.
We call this cluster of neurons a layer.
And then this has three layers.
So we would use input layer to define the first layer,
output layer to define the third layer because it directly sees
the outputs and we would call the second layer a hidden layer.
And the reason we call it hidden is because
the inputs and the outputs are hidden from this layer.
It means the only thing that this layer sees
as input is what's the previous layer gave it.
So it's an abstraction of the inputs but it's not the inputs.
Does that make sense? And same, it doesn't see the output,
it just gives what it understood to the last neuron
that will compare the output to the ground truth.
So now, why are neural networks interesting?
And why do we call this hidden layer?
Um, it's because if you train this network
on cat classification with a lot of images of cats,
you would notice that the first layers
are going to understand the fundamental concepts of the image,
which is the edges.
This neuron is going to be able to detect this type of edges.
This neuron is probably going to detect some other type of edge.
This neuron, maybe this type of edge.
Then what's gonna happen, is that these neurons are going to communicate
what they found on the image to the next layer's neuron.
And this neuron is going to use the edges that these guys found to figure out that,
oh, there is a - their ears.
While this one is going to figure out, oh,
there is a mouth and so on if you have
several neurons and they're going to communicate what they understood to
the output neuron that is going to construct the face of
the cat based on what it received and be able to tell if there is a cat or not.
So the reason it's called hidden layer is because we - we
don't really know what it's going to figure out but with enough data,
it should understand very complex information about the data.
The deeper you go, the more complex information the neurons are able to understand.
Let me give you another example which is a house prediction example.
House price prediction.
[NOISE]
So let's assume that our inputs are number of bedrooms,
size of the house,
zip code, and wealth of the neighborhood, let's say.
What we will build is a network that has
three neurons in the first layer and one neuron in the output layer.
So what's interesting is that as a human if you were to build, uh,
this network and like hand engineer it,
you would say that, uh,
okay zip codes and wealth or - or sorry.
Let's do that.
Zip code and wealth are able to tell us about the school quality in the neighborhood.
The quality of the school that is next to the house probably.
As a human you would say these are probably good features to predict that.
The zip code is going to tell us if the neighborhood is walkable or not, probably.
The size and the number of bedrooms is going to tell
us what's the size of the family that can fit in this house.
And these three are probably
better information than these in order to finally predict the price.
So that's a way to hand engineer that by hand, as a human in order
to give human knowledge to the network to figure out the price.
In practice what we do here is that we use
a fully-connected layer - fully-connected.
What does that mean? It means that we connect every input of a layer,
every - every input to the first layer,
every output of the first layer to the input of the third layer and so on.
So all the neurons among lay - from one layer to another are connected with each other.
What we're saying is that we will let the network figure these out.
We will let the neurons of the first layer figure out
what's interesting for the second layer to make the price prediction.
So we will not tell these to the network,
instead we will fully connect the network [NOISE] and so on.
Okay. We'll fully connect
the network and let it figure out what are the interesting features.
And oftentimes, the network is going to be able better than
the humans to find these - what are the features that are representative.
Sometimes you may hear neural networks referred as,
uh, black box models.
The reason is we will not understand what this edge will correspond to.
It's - it's hard to figure out that this neuron is
detecting a weighted average of the input features. Does that make sense?
Another word you might hear is end-to-end learning.
The reason we talked about end-to-end learning is because we have an input,
a ground truth, and we don't constrain the network in the middle.
We let it learn whatever it has to learn and we call it
end-to-end learning because we are just training based on the input and the output.
[NOISE]
Let's delve more into the math of this network.
The neural network that we have here which has an input layer,
a hidden layer and an output layer.
Let's try to write down the equations that run
the inputs and pro - propagate it to the output.
We first have Z_1, that is the linear part of the first layer,
that is computed using W_1 times x plus b_1.
Then this Z_1 is given to an activation,
let's say sigmoid, which is sigmoid of Z_1.
Z_2 is then the linear part of the second neuron which is going to
take the output of the previous layer, multiply it by its weights and add the bias.
The second activation is going to take the sigmoid of Z_2.
And finally, we have the third layer which is going to multiply its weights,
with the output of the layer presenting it and add its bias.
And finally, we have the third activation which is simply the sigmoid of the three.
So what is interesting to notice between
these equations and the equations that we wrote here,
is that we put everything in matrices.
So it means this a_3 that I have here, sorry,
this here for three neurons I wrote three equations,
here for three neurons
in the second layer I just wrote a single equation to summarize it.
But the shape of these things are going to be vectors.
So let's go over the shapes,
let's try to define them.
Z_1 is going to be x which is n by 1 times
w which has to be 3 by n because it connects three neurons to the input.
So this z has to be 3 by 1.
It makes sense because we have three neurons.
Now let's go, let's go deeper.
A_1 is just the sigmoid of z_1 so it doesn't change the shape.
It keeps the 3 by 1.
Z_2 we know it,
it has to be 2 by 1 because there are two neurons in
the second layer and it helps us figure out what w_2 would be.
We know a_1 is 3 by 1.
It means that w_2 has to be 2 by 3.
And if you count the edges between the first and the second layer
here you will find six edges, 2 times 3.
A_2, same shape as z_2.
Z_3, 1 by 1,
a_3, 1 by 1,
w_3, it has to be 1 by 2,
because a_2 is 2 by 1 and same for b.
B is going to be the number of neurons so 3 by 1,
2 by 1, and finally 1 by 1.
So I think it's usually very helpful,
even when coding these type of equations,
to know all the shapes that are involved.
Are you guys, like, totally okay with the shapes,
super-easy to figure out? Okay, cool.
So now what is interesting is that we will try to vectorize the code even more.
Does someone remember the difference between stochastic gradient descent
and gradient descent. What's the difference?
[inaudible]
Exactly. Stochastic gradient descent is updates,
the weights and the bias after you see every example.
So the direction of the gradient is quite noisy.
It doesn't represent very well the entire batch,
while gradient descent or batch gradient descent is
updates after you've seen the whole batch of examples.
And the gradient is much more precise.
It points to the direction you want to go to.
So what we're trying to do now is to write down these equations if
instead of giving one single cat image we
had given a bunch of images that either have a cat or not a cat.
So now our input x.
So what happens for
an input batch of m examples?
So now our x is not anymore a single column vector,
it's a matrix with the first image corresponding to x_1,
the second image corresponding to x_2 and so on until the nth image corresponding to x_n.
And I'm introducing a new notation which is the parentheses
superscript corresponding to the ID of the example.
So square brackets for the layer,
round brackets for the idea of the example we are talking about.
So just to give more context on what we're trying to do.
We know that this is a bunch of operations.
We just have a,
a network with inputs, hidden, and output layer.
We could have a network with 1,000 layer.
The more layers we have the more computation and it quickly goes up.
So what we wanna do is to be able to parallelize our code or,
or our computation as much as possible by giving
batches of inputs and parallelizing these equations.
So let's see how these equations are modified when we give it a batch of m inputs.
I will use capital letters to
denote the equivalent of the lowercase letters but for a batch of input.
So Z_1 as an example would be W_1,
let's use the same actually,
W_1 times X plus B_1.
So let's analyze what Z_1 would look like.
Z_1 we know that for every,
for every input example of the batch we will get one Z_1 which should look like this.
Then we have to figure out what have to be the shapes
of this equation in order to end up with this.
We know that Z_1 was 3 by 1.
It mean, it means capital Z_1 has to be 3 by
m because each of these column vectors are 3 by 1 and we have m of them.
Because for each input we forward propagate through the network, we get these equations.
So for the first cat image we get these equations,
for the second cat image we get again equations like that and so on.
So what is the shape of x? We have it above.
We know that it's n by n. What is the shape of w_1?
It didn't change. W_1 doesn't change.
It's not because I will give 1,000 inputs to
my network that the parameters are going to be more.
So the parameter number stays the same even if I give more inputs.
And so this has to be 3 by n in order to match Z_1.
Now the interesting thing is that there is an algebraic problem here.
What is the algebraic problem?
We said that the number of parameters doesn't change.
It means that w has the same shape as it has before, as it had before.
B should have the same shape as it had before, right?
It should be 3 by 1. What's the problem of this equation?
Exactly. We're summing a 3 by m matrix to a 3 by 1 vector.
This is not possible in math. It doesn't work.
It doesn't match. When you do some summations or subtraction,
you need the two terms to be the same shape because you will
do an element-wise addition or an ele- element-wise subtraction.
So what's the trick that is used here?
It's a, it's a technique called broadcasting.
Broadcasting is that- is the fact that we
don't want to change the number of parameters, it should stay the same.
But we still want this operation to be able to be written in parallel version.
So we still want to write this equation because we want to parallelize our code,
but we don't want to add more parameters, it doesn't make sense.
So what we're going to do is that we are going to create a vector b tilde
1 which is going to be b_1 repeated three times.
Sorry, repeated m times.
So we just keep the same number of parameters but just repeat
them in order to be able to write my code in parallel.
This is called broadcasting. And what is convenient is that
for those of you who, uh, the homeworks are in Matlab or Python?
Matlab. Okay. So in Matlab, no Python? [LAUGHTER].
[NOISE]
Thank you. Um, Python. So in Python,
there is a package that is often used to to code these equations. It's numPy.
Some people call it numPy, I'm not sure why.
So numPy, basically numerical Python,
we directly do the broadcasting.
It means if you sum this 3 by m matrix with a 3 by 1 parameter vector,
it's going to automatically reproduce
the parameter vector m times so that the equation works.
It's called broadcasting. Does that make sense?
So because we're using this technique,
we're able to rewrite all these equations with capital letters.
Do you wanna do it together or do you want to do it on your own?
Who wants to do it on their own?
Okay. So let's do it on their own [LAUGHTER] on your own.
So rewrite these with capital letters and figure out the shapes.
I think you can do it at home, wherever,
we're not going to do here, but make sure you understand all the shapes. Yeah.
[inaudible] How how is this
[inaudible]?
So the question is how is this different from principal component analysis?
This is a supervised learning algorithm that
will be used to predict the price of a house.
Principal component analysis doesn't predict anything.
It gets an input matrix X normalizes it, ah,
computes the covariance matrix and then figures out what are
the pri- principal components by doing the the eigenvalue decomposition.
But the outcome of PCA is,
you know that the most important features of
your dataset X are going to be these features.
Here we're not looking at the features.
We're only looking at the output.
That is what is important to us. Yes.
In the first lecture when did you say that the first layers is
the edges in an [inaudible].
So the question is, can you explain why the first layer would see the edges?
Is there an intuition behind it?
It's not always going to see the edges,
but it's oftentimes going to see edges because um,
in order to detect a human face,
let's say, you will train an algorithm to find out whose face it is.
So it has to understand the faces very well.
Um, you need the network to be complex enough
to understand very detailed features of the face.
And usually, this neuron,
what it sees as input are pixels.
So it means every edge here is the multiplication of the weight by a pixel.
So it sees pixels.
It cannot understand the face as a whole because it sees only pixels.
It's very granular information for it.
So it's going to check if pixels nearby have
the same color and understand that there is an edge there, okay?
But it's too complicated to understand the whole face in the first layer.
However, if it understands a little more than a pixel information,
it can give it to the next neuron.
This neuron will receive more than pixel information.
It would receive a little more complex-like edges,
and then it will use this information to build on top of
it and build the features of the face.
So what I'm trying to sum up is that these neurons only see the pixels,
so they're not able to build more than the edges.
That's the minimum thing that they can- the maximum thing they can do.
And it's it's a complex topic,
like interpretation of neural network is
a highly researched topic, it's a big research topic.
So nobody figured out exactly how all the neurons evolve.
Yeah. One more question and then we move on.
Ah, how [inaudible].
So the question is how [OVERLAPPING].
[inaudible].
-how do you decide how many neurons per layer? How many layers?
What's the architecture of your neural network?
There are two things to take into consideration
I would say. First and nobody knows the right answer, so you have to test it.
So you you guys talked about training set,
validation set, and test set.
So what we would do is,
we would try ten different architectures,
train it, train the network on these,
looking at the validation set accuracy of all these,
and decide which one seems to be the best.
That's how we figure out what's the right network size.
On top of that, using experience is often valuable.
So if you give me a problem,
I try always to gauge how complex is the problem.
Like cat classification, do you
think it's easier or harder than day and night classification?
So day and night classification is I give you an image,
I asked you to predict if it was taken during the day or during the night,
and on the other hand you want there's a cat on the image or not.
Which one is easier, which one is harder?
Who thinks cat classification is harder?
Okay. I think people agree.
Cat classification seems harder, why?
Because there are many breeds of cats.
Can look like different things.
There's not many breeds of nights. um, I guess.
[LAUGHTER]
Um, one thing that might be challenging in the day and night classification,
is if you want also to figure it out in house like i- inside,
you know maybe there is a tiny window there and I'm able to tell that is the day
but for a network to understand it you will need a lot more data
than if only you wanted to work outside, different.
So these problems all have their own complexity.
Based on their complexity,
I think the network should be deeper.
The comp- the more complex usually is the problem,
the more data you need in order to figure out the output,
the more deeper should be the network.
That's an intuition, let's say.
Okay. Let's move on guys because I think we have about what, 12 more minutes?
Okay. Let's try to write the loss function
for this problem.
[NOISE].
So now that we have our network,
we have written this propagation equation and I we call it forward propagation,
because it's going forward, it's going from the input to the output.
Later on when we will, we will derive these equations,
we will call them backward propagation,
because we are starting from the loss and going backwards.
So let's let's talk about the optimization problem.
Optimizing w_1, w_2, w_3, b_1, b_2, b_3.
We have a lot of stuff to optimize, right?
We have to find the right values for these and
remember model equals architecture plus parameter.
We have our architecture, if we have our parameters we're done.
So in order to do that, we have to define an objective function.
Sometimes called loss, sometimes called cost function.
So usually we would call it loss if there is only one example in the batch,
and cost, if there is multiple examples in a batch.
So the loss function that,
let- let's define the cost function.
The cost function J depends on y hat n_y.
Okay. So y hat,
y hat is a_3.
Okay. It depends on y hat n_y,
and we will set it to be the sum of the loss functions L_i,
and I will normalize it.
It's not mandatory, but normalize it with 1/n. So what does this mean?
It's that we are going for batch gradient descent.
We wanna compute the loss function for the whole batch, parallelize our code,
and then calculate the cost function that
will be then derived to give us the direction of the gradients.
That is, the average direction of
all the de-de- derivation with respect to the whole input batch.
And L_i will be the loss function corresponding to one parameter.
So what's the error on this specific one input,
sorry not parameter, and it will be the logistic loss.
You've already seen these equations, I believe.
So now, is it more complex to take
a derivative with respect to J like of J with respect to the parameters or of L?
What's the most complex between this one,
let's say we're taking the derivative with respect to w_2, compared to this one?
Which one is the hardest? Who thinks J is the hardest?
Who think it doesn't matter?
Yeah, it doesn't matter because derivation is is a linear operation, right?
So you can just take the derivative inside and you will see that if you know this,
you just have to take the sum over this.
So instead of computing all derivatives on J,
we will com- compute them on L,
but it's totally equivalent.
There's just one more step at the end.
Okay. So now we defined our loss function, super.
We defined our loss function and the next step is optimize.
So we have to compute a lot of derivatives. [NOISE]
And that's called backward propagation.
[NOISE] So the question
is why is it called backward propagation?
It's because what we want to do ultimately is this.
For any l equals 1-3,
we want to do that,
wl equals wl minus Alpha derivative of j with respect to wl,
and bl equals bl minus Alpha derivative of j with respect to bl.
So we want to do that for every parameter in layer 1, 2, and 3.
So it means, we have to compute all these derivatives,
we have to compute derivative of the cost with respect to w1,
w2, w3, b1, b2, b3.
You've done it with logistic regression,
we're going to do it with a neural network,
and you're going to understand why it's called backward propagation.
Which one do you want to start with? Which derivative?
You wanna start with the derivative with respect to w1,
w2, or w3, let's say.
Assuming we'll do the bias later. W what?
W1? You think w1 is a good idea.
I do- don't wanna do w1.
I think we should do w3,
and the reason is because if you look at this loss function,
do you think the relation between w3 and
this loss function is easier to understand or
the relation between w1 and this loss function?
It's the relation between w3 and this loss function.
Because w3 happens much later in the- in the network.
So if you want to understand how much should we move w1 in order to make the loss move?
It's much more complicated than answering the question
how much should w3 move to move the loss.
Because there's much more connections if you wanna compete with w1.
So that's why we call it backward propagation
is because we will start with the top layer,
the one that's the closest to the loss function,
derive the derivative of j with respect to w1.
Once we've computed this derivative which we are going to do next week,
once we computed this number,
we can then tackle this one.
Oh, sorry. Yeah. Thanks. Yeah. Once we computed this number,
we will be able to compute this one very easily. Why very easily?
Because we can use the chain rule of calculus.
So let's see how it works.
What we're- I'm just going to give you, uh,
the one-minute pitch on- on backprop,
but, uh, we'll do it next week together.
So if we had to compute this derivative,
what I will do is that I will separate it into several derivatives that are easier.
I will separate it into the derivative of j with respect to something,
with the something, with respect to w3.
And the question is, what should this something be?
I will look at my equations.
I know that j depends on Y-hat,
and I know that Y-hat depends on z3.
Y-hat is the same thing as a3,
I know it depends on z3.
So why don't- why don't I include z3 in my equation?
I also know that z3 depends on w3,
and the derivative of z3 with respect to w2 is super easy,
it's just a2 transpose.
So I will just make a quick hack and say that
this derivative is the same as taking it with respect to a3,
taking derivative of a3 with respect to z3,
and taking the derivative of z3 with respect to w3.
So you see? Same, same derivative,
calculated in different ways.
And I know this, I know these are pretty easy to compute.
So that's why we call it backpropagation,
it's because I will use the chain rule to compute the derivative of w3,
and then when I want to do it for w2,
I'm going to insert,
I'm going to insert the derivative with z3 times the derivative of
z3 with respect to a2 times the derivative of a2 with respect to z2,
and derivative of z2 with respect to w2.
Does this make sense that this thing here is the same thing as this?
It means, if I wanna compute the derivative of w2,
I don't need to compute this anymore,
I already did this for w3.
I just need to compute those which are easy ones, and so on.
If I wanna compute the derivative of j with respect to w1,
I'm going to- I'm not going to decompose all the thing again,
I'm just going to take the derivative of j with respect to
z2 which is equal to this whole thing.
And then I'm gonna multiply it by the derivative of
z2 with respect to a1 times derivative of
a1 with respect to z1 times the derivative of z1 with respect to w1.
And again, this thing I know it already,
I computed it previously just for this one.
So what's, what's interesting about it is that I'm not gonna redo the work I did,
I'm just gonna store the right values while back-propagating and continue to derivate.
One thing that you need to notice though is that, look,
you need this forward propagation equation in order to
remember what should be the path to take in
your chain rule because you know that this derivative of j with respect to w3,
I cannot use it as it is because w3 is not connected to the previous layer.
If you look at this equation,
a2 doesn't depend on w3,
it depends on z3.
Sorry, like, uh, my bad,
it depends- no, sorry,
what I wanted to say is that z2 is connected to w2,
but a1 is not connected to w2.
So you wanna choose the path that you're going
through in the proper way so that there's no cancellation in these derivatives.
You- you cannot compute derivative of w2 with
respect to- to a1, right?
You cannot compute that, you don't know it.
Okay. So I think we're done for today.
So one thing that I'd like you to do if you have time is
just think about the things that can be tweaked in a neural network.
When you build a neural network, you are not done,
you have to tweak it, you have to tweak
the activations, you have to tweak the loss function.
There's many things you can tweak,
and that's what we're going to see next week. Okay. Thanks.
 Hi everyone. [NOISE] Welcome,
welcome to the second lecture on deep learning for CS229.
So a quick announcement before we start.
There is a Piazza post Number 695 which is the mid-quarter survey for CS229,
so fill it in when you have time.
Okay. So let's get back to deep learning.
So last week together we've seen, uh,
what a neural network is and we started by
defining the logistic regression from a neural network perspective.
We said that logistic regression can be viewed as
a one-neuron neural network where there is
a linear part and an activation part which was sigmoid in that case.
We se- we've seen that sigmoid is a common activation function to be used for
classification tasks because it casts
a number between minus infinity and plus infinity in 0,
into 0, 1 interval which can be interpreted as a probability.
And then we introduced the neural network,
so we started to stack some neurons inside a layer and then stack layers
on top of each other and we said that the
more we stack layers the more parameters we have,
and the more parameters we have, the more our network is
able to copy the complexity of our data because it becomes more flexible.
So, uh, we stopped at a point where we did a forward propagation,
we had an example during training,
we forward propagated through the network, we get the output,
then we compute the cost function which compares this output to the ground truth,
and we were in the process of backpropagating the error to tell
our parameters how they should move in order to detect cats more properly.
Does that make sense for this part?
So today, we're going to continue that.
So we're in the second part, neural networks,
we're going to derive the backpropagation with the chain rule and after that,
ah, we're going to talk about how to improve our neural networks.
Because in practice, it's not because you
designed a neural network that it's going to work,
there's a lot of hacks and tricks that you
need to know in order to make a neural network work.
Okay, let's go.
So first thing that we talked about is
in order to define our optimization problem and find the right parameters,
we need to define a cost function,
and usually we said we would use the letter j to denote the cost function.
So here, when I talk about cost function,
I'm talking about the batch of examples.
It means I'm forward propagating m examples at a time.
You remember why we do that?
What's the reason we use a batch instead of a single example?
Vectorization. We want to use what our
GPU can do and parallelize the computation. So that's what we do.
So we have m examples that go- forward propagate in the network.
And each of them has a loss function associated with them,
the average of the loss functions over the batch give us the cost function.
And we had defined these loss function together.
L of i. Assuming we're still,
and just as a reminder,
we're still in this network where,
where we had a cat, remember?
This one. Remember this guy.
x_1 to x_n.
The cat was flattened into a vector,
RGB matrix into one vector and then there was a neural network with three neurons,
then two neurons, then one neuron.
Remember? Fully-connected here.
Everything. Up, up,
and then we add y hat.
You remember this one? I think that was this one here. Yeah, okay.
So now, we're here, we take m images of cats or non-cats,
forward propagate everything in the network,
compute our loss function for each of them,
average it, and get the cost function.
So our last function was the binary cross-entropy or also called
the loss function- the logistic loss function and it was the following.
y_i log of y hat i plus 1
minus y_i log of 1 minus y hat i.
So let me circle this one,
it's an important one.
And what we said is that this network has many parameters.
And we said, the first layer has w_1,
b_1, the second layer has w_2,
b_2, and the third layer has w_3,
b_3 where the square brackets dis- denotes the layer.
And we have to train all these parameters.
One thing we notice is that because we want to make a good use of the chain rule,
we're going to start by,
by computing the derivative of these guys,
w_3 and b_3 and then come back and do w_2 and b_2 and then back again w_1 and b_1.
In order to use our formulas of
the update of the gradient descent where w would be equal to w minus Alpha
derivative of the cost with respect to w and this for
any layer l between 1 and 3, same for b.
Okay, so let's try to do it.
This is the first number we want to compute.
And remember, the reason we want to compute derivative of
the cost with respect to w_3 is because the relationship
between w_3 and the cost is easier than the relationship between w_1 and the cost
because w_1 had much more connection going through
the network before ending up in the cost computation.
So one thing we should notice
before starting this calculation is that the derivative is linear.
So this, if I take the derivative of j,
I can just take the derivative of l, and it's the same thing,
I just need to add the summation prior to that because derivative is a linear operation.
That makes sense to everyone? So instead of computing this,
I'm going to compute that and then I will add the summation,
it will just make our notation easier.
So I'm taking the derivative of a loss of
one example propagated to the network with respect to w_3.
So let's do the calculation together.
I have a 1, I have
a minus y_i derivative with respect to w_3, of what?
We remember that y hat was equal to sigmoid of w_3 x
plus b or w_3 a_2 plus b because a_2 is the input to the second layer, remember.
So I would write it down here,
sigmoid of w_3 a_2 plus b_3.
Okay?
Yeah.
It's good like that? It's too small?
w_3 a_2 plus b_3.
It's good like that, yeah?
Okay. So we have this term and then we have the second term which is plus
1 minus y_i times derivative of w_3.
Derivative with respect to w_3 of 1.
Oh sorry, I forgot the logarithm here.
Of log of 1 minus sigmoid of
w_3 a_2 plus b_3.
And so just a reminder,
the reason we have this is because we've
written the forward propagation in the previous class.
You guys remember the pro- forward propagation?
We had z_3, which took a_2 as inputs and computed the linear part,
as sigmoid is- is the activation function used in the last neuron over here.
Okay. So let's try to- to compute this derivative.
y_i, so the derivative of log,
[NOISE] log prime equals 1 over log.
Remember this- this- this formula,
so I will just take 1 over,
sorry, 1 over x minus- 1 over x if you put an x here.
So log prime of x.
So I will take one over sigmoid of w_3 a_2 plus b_3.
I know that thing can be written a_3, right?
So I will just write a_3 instead of writing the single a again.
So we have 1 over a_3 times the derivative of a_3 with respect to w_3.
We remember that, I'm going to write it down here.
If we take the derivative of sigmoid of blah, blah, blah.
Let's say, derivative of log of sigmoid over w. What we have is
1 over the sigmoid times the derivative with respect to w_3 of the sigmoid.
Does that makes sense? That's what we're using here.
So the derivative of sigmoid,
sigmoid-prime of x is actually pretty easy to compute.
It's sigmoid of x times 1 minus sigmoid of x.
Okay. So I'm just going to take the derivative.
It's going to give me a- a_3 times 1 minus a_3.
There's still one step because there is a composition of three functions here.
There is a logarithm, there's a sigmoid,
and there is also a linear function,
w_x plus b or w a_2 plus b.
So I also need to take the derivative of the linear part with respect to w_3.
Because I know that sigmoid of w_3, a_2 plus b_3.
If I wanna take the derivative of that with respect to w_3,
I need to go inside and take the derivative of what's inside, okay?
So this will give me the sigmoid or whatever a_3 times 1 minus
a_3 times the derivative with respect to w_3 of the linear part.
[NOISE] Does this make sense?
So I am going to write it here bigger.
Here, I need to take the derivative of the linear part with respect to w_3,
which is equal to a_2 transpose.
So one thing you- you may wanna check,
is when we compute- when I'm trying to compute this derivative.
[NOISE]
I'm trying to compute this derivative.
Why is there a transpose that comes out?
How do you come up with that?
You look at the shape here.
What's the shape of w_3? Someone remembers?
1 by 2.
1 by 2. Yeah, why 1 by 2?
[BACKGROUND]
Yeah, it's connecting two neurons to one neuron.
So it has to be 1 by 2. Usually flip it.
And in order to come back to that,
you can write your forward propagation,
make the shape analysis,
and find out that it's a 1 by 2 matrix.
How about this thing?
What's the shape of that?
[NOISE].
The scalar.
It's a scalar, yeah. So scalar.
So it's 1 by 1. How do you know?
It's because this thing is basically z_3.
It's the linear part of the last neuron and a_3,
we know that it's y-hat.
So it's a scalar between 0 and 1.
So this has to be a scalar as well.
Because taking the sigmoid should not change the shape.
So now, the question is what's the shape of this entire thing?
The shape of this entire thing should be the shape
of w_3 because you're taking the derivative of
a scalar with respect to a higher-dimensional matrix or vector here called a row vector.
Then it means, that the shape of this has to be the same shape of w_3. So 1 by 2.
And you know that when you take this simple derivative in- in real life,
like in- in, uh, with scalars,
not with high-dimensional, you know that this is an easy derivative.
It just should- it should give you a_2, right?
But in higher dimension,
sometimes you have transpose that come up.
And how do you know that the answer is a_2 transpose?
It's because you know that a_2 is a 2 by 1 matrix.
[NOISE] So this is not possible.
It's not possible to get a_2,
because otherwise it wouldn't match the derivative that you are calculating.
So it has to be a_2 transpose.
So either you- you learn the formula by heart or you- you learn how to analyze shapes,
okay? Any questions on that?
Okay. So that's why it's a_2 transpose.
Now, l minus y_i.
So I'm- I'm on this one now.
The second term of the- of the derivative.
And I take the derivative of this.
So I get 1 over 1 minus a_3.
a_3 denotes the sigmoid.
So I'm just copying this back using
the fact that the derivative of the logarithm is 1 over x,
and then I will multiply this by the derivative of 1 minus a_3 with respect to w_3.
I know that there is a minus that needs to come up.
So I will write it down here,
minus 1 and I also have
the derivative of the sigmoid with respect to what's inside the sigmoid.
So a_3 [NOISE] times 1 minus a_3.
And what's the last term?
The last term is simply the one we just talked about.
It's the derivative of what's inside the sigmoid with respect to w_3.
So it's a_2 transpose again.
Okay. So now, I will just simplify.
I know this scalar simplifies with this one.
This one simplifies with that one.
We're going to copy back all the results minus [NOISE] y_i times 1 minus a_3
a_2 transpose plus 1 minus
y_i times the minus- I'm going to put the minus here.
So I'm taking the minus putting it on- on the front times a_3 times a_2 transpose.
And then, quickly looking at that I see that some of the terms will cancel out, right?
Okay. So I have one term here,
y-hat- y_i times minus
a_3 a_2 transpose would cancel out with plus y_i a_3 a_2 transpose.
This makes sense? So like,
the term that we multiply this number,
we cancel out with the term, we multiply this number.
We need to continue.
[NOISE] It gives me y_i times a_2 transpose, this part,
minus a_3 times a_2 transpose.
I, I can factor this because I have the same term a_2 transpose.
And it gives me finally,
y_i minus a_3 times a_2 transpose.
Okay, so it doesn't look that bad actually.
I don't know, when- when we take a derivative of something kin- kinda ugly we- we
expect something ugly to come out but this doesn't seem too bad.
Any questions on that?
I let you write it quickly,
and then we're going to move through to the rest.
So once I get these results,
I can just write down the costs of the derivative with respect to w_3.
I know it's just one minus.
I just need to- to take the summation of this thing.
So y_i minus a_3 times [NOISE] y_2 transpose- a_2 transpose.
And I have a minus sign coming upfront.
So that's my derivative.
[NOISE]
Okay. So we're done with that.
And we can, we can just take this formula,
plugging it back in our gradient descent update rule, and update w_3.
Yeah. Now, the question is,
you can do the same thing as,
as we just did but with b_3.
It's going to be the similar difficulty.
We're going to do it with w_2 now,
and think how does that backpropagate to w_2.
So now it's w_2 star.
We want to compute the derivative of l,
the loss, with respect to w of the second layer.
The question is how I'm gonna get this one without having too much work.
I'm not gonna start over here as we said last time,
I'm going to use the chain rule of calculus.
So I'm going to try to decompose this derivative into several derivatives.
So I know that y hat is the first thing that is connected to the loss function, right.
The output neuron is directly connected to the loss function.
So I'm going to take the derivative of the loss function with respect to
y hat, also called a_3.
Right? This is the easiest one I can calculate.
I also know that a_3,
which is the output activation of the last neuron,
is connected with the linear part of the last neuron, which is z_3.
So I can take the derivative of a_3 with respect to z_3.
Do you remember what this is going to be?
Derivative of a_3 with respect to z_3?
Derivative of Sigmoid.
I know that a_3 equals Sigmoid of z_3.
So this derivative is very simple. It's just that.
It's just a_3 times 1 minus a_3.
All right. So I'm going to continue.
I know that z_3, z_3 is equal to what?
It's equal to w_3, a_2 plus b.
Which path did I need- do I need to take in order to backpropagate?
I don't wanna take the derivative with respect to w_3 because I will only get stuck.
I don't wanna take the derivative with respect to b_3 because I will get stuck.
I will take the derivative with respect to a_2.
Because a_2 will be connected to z_2,
z_2 will be connected to a_1,
and I can backpropagate from this path.
So I'm going to take derivative of z_3 with respect to
a_2 to have my error backpropagate, and so on.
I know that a_2 is equal to Sigmoid of z_2.
So I'm just going to do that.
And I know that this derivative is going to be easy as well.
And finally, I also know that z_2 is connected to w_2.
So I'm going to take derivative of z_2 with respect to w_2.
So just what I want you to get is the thought process of this chain rule.
Why don't we take a derivative with respect to w_3 or b_3?
It's because we will get stuck.
We want the error to back propagate.
And in order for the error to backpropagate,
we have to go through variables that are connected to each other. Does this makes sense?
So now the question is how can we use this?
How can we use the derivative we already have in order to,
to, to, to compute the derivative with respect to w_2?
Can someone tell me how we can use the results from this calculation,
in order not to do it again?
Cache it.
You cache it? Um, so there's another discussion on caching,
which is, which is correct that in order to
get this result very quickly we will use cache.
But, uh, what I want here is to- you to tell me if
these results appear somewhere here. Yeah?
[inaudible] the first three terms.
The first three terms. So this one, this one, and this one?
I'm not sure.
Yeah. Is it the first two terms or the first three terms?
Two.
The first two terms. Yeah. But good intuition.
Yeah. So this result is actually the first two terms here.
We just calculated it.
Okay. What- how do we know that? It's not easy to see.
One thing we know based on what we've written very big on
this board is that the derivative of z_3,
because this is z_3, right?
Derivative of z_3 with respect to w_3 is a_2 transpose.
Right. So I could write here that this thing is derivative
of z_3 with respect to w_3.
Is it correct? So I know
that because I wanted to compute the derivative of the loss to w_3,
I know that I could have written derivative of loss with respect to
w_3 as derivative of loss with respect to z_3,
times derivative of z_3 with respect to w_3.
Correct. And I know that this is a_2 transpose.
So it means that this thing is the derivative of the loss with respect to z_3.
Does that make sense? So I got,
I got my decomposition of the derivative we had.
If we wanted to use the chain rule from here on,
we could have just separated it into two terms, and took the derivative here.
Okay. So I know the result of this thing.
I know that this thing is basically a_3 minus y, times a_2 transpose.
I just flipped it because of the minus sign.
Okay. Is it mine?
[NOISE].
Okay. [NOISE]. Now, tell me what's this term.
What is this term? Let's go there. Yeah.
Sigmoid.
So Sigmoid. I'm just going to write it a_2 times 1 minus a_2.
Does that make sense? Sigmoid times 1 minus Sigmoid.
What is this term?
Uh, oh sorry my bad.
That's not the right one. This one, this one is that.
This one is Sigmoid.
a_2 is Sigmoid of z_2.
So this result comes from this term.
Was- what about this term?
w_3.
Sorry.
w_3.
w_3. Is it w_3 or no? I heard transpose.
How do we know if it's w_3 or w_3 transpose?
So let's look at the shape of this. What's z_3?
One by one.
It's one by one. It's a scalar.
It's the linear part of the last neuron.
What's the shape of that? This is 2, 1.
We have two neurons in the layer.
w_3. We said that it was a 1 by 2 matrix,
so we have to transpose it.
So the result of that is w_3 transpose.
And how about the last term?
Same as here. One layer before.
Yeah, someone said they won't transpose.
Okay. Yeah?
The numbers are [inaudible] that one.
This one?
Yeah.
There is a transpose here.
[inaudible] w_5.
Oh yeah, yeah. You're correct.
You're correct. Thank you.
That's what you mean? Yeah. Yeah. This one was from the z_3, to w_2.
We didn't end up using that because we will get stuck,
so there's no a_2 transpose here.
Thanks. Any other questions or remarks?
So that's cool. Let's, let's, let's write- let's write
down our derivative cleanly on the board.
So we have derivative of our loss function with respect to w_2,
which seems to be equal to a_3 minus y,
from the first term.
The second term seems to be equal to, uh, w_3 transpose.
Then we have a term which is a_2 times 1 minus a_2.
Okay. And finally, finally we have another term that is a_1 transpose.
So are we done or not?
So actually there is that- the thing is there's two ways to compute derivatives.
Either you go very rigorously and do what we did here for w_2,
or you try to do a chain rule analysis,
and you try to fit the terms.
The problem is this result is not completely correct.
There is a shape problem.
It means when we took our derivatives,
we should have flipped some of the terms. We didn't.
There is actually- we,
we won't have time to go into details in
this lecture because we have other things to see, but there is,
uh, a section note I think on the website,
which details the other method which is more rigorous,
which is like that for all the derivatives.
What we are going to see is how you can use chain rule plus
shape analysis to come up with the results very quickly.
Okay. So let's, let's analyze the shape of all that.
We know that the first term is a scalar.
It is a 1 by 1. We know that the second term is the transpose of 1 by 2. So it's 2 by 1.
And we know that this thing here a_2 times 1 minus a_2 is,
uh, 2 by 1.
It's an element-wise product.
And this one is a_1 transpose,
so it's 3 by 1 transpose.
So it is 1 by 3. So there seems to be a problem here.
There is no match between these two operations for example.
Right? So the question is, how- how can,
we how can we put everything together?
If we do it very rigorously,
we know how to put it together.
If you're used to doing the chain rule,
you can quickly sh- quickly do it around.
So after experience, you will be able to,
to fit all these together.
The important thing to know is that here there is an element-wise product, which is here.
So every time you will take the derivative of the Sigmoid
it's going to end up being an element-wise product.
And it's the case whatever the activation that you're using is.
So the right result is this one.
So here I have my element-wise product of a 2 by 1 [NOISE] by a 2 by 1.
So it gives me a 2 by
1 column vector and then I need something that is 1 by 1 and 1 by 3.
How do I know, wha- what I need to have,
I know that the shape of this thing.
W3 needs to be 2 by 3.
It's connecting three neurons to two neurons.
So W2 has to be 2 by 3.
In order to end up with this,
I know that this has to come here A3 minus y and A1 transpose comes at the end.
And here I get my correct answer.
Don't worry if it's the first time th- the chain rule is going quickly, don't worry.
Read the lecture notes with the rigorous parts.
Taking the derivative, it will make more sense.
But I feel it's, uh,
usually in practice, we don't compute these chain rules anymore, uh,
because- because programming frameworks do it for us
but it's important to know at least how the chain rule decomposes,
uh, and also how to make these, compute these derivatives.
If you read research papers specifically.
Any questions on that?
I think I wanna go back to what you mentioned with the cache.
So why is cache very important? That was your question as well?
[BACKGROUND]
Yeah, yeah it has to be.
Right. So it means when you take the derivative of Sigmoid,
you take derivative with respect to
every entry of the matrix which gives you an element-wise product.
Um, going back to the cache.
So one thing is,
it seems that during backpropagation,
there is a lot of terms that appear that were computed during forward propagation.
Right. All these terms; a1 transpose,
a2, a3, all these,
we have it from the forward propagation.
So if we don't cache anything,
we have to recompute them.
It means I'm going backwards but then I feel,
oh, I need a2 actually.
So I have to re- go forward again to get a2.
I go backwards, I need a1.
I need to forward propagate my x again to get a1. I don't wanna do that.
So in order to avoid that,
when I do my forward propagation,
I would keep in memory almost all the values that I'm getting
including the Ws because as you see to compute
the derivative of loss with respect to W2 we need W3,
but also, the activation or linear variables.
So I'm going to save them in my,
in my network during the forward propagation in order to
use it during the backward propagation. So it makes sense.
And again, it's all for computational ef- efficiency.
It has some memory costs.
Okay. So that was backpropagation.
And now I can use my formula of
the costs with respect to the loss function.
And I know that this is going to be my update.
[NOISE] This is going to be used in order to update W2 and I will do the same for W1.
Then you guys can do it at home.
If you wanna meet, wanna make sure you understood,
take the derivative with respect to W1.
Okay. So let's move on to the next part,
[NOISE] which is improving your neural network.
So in practice, when you,
when you do this process of training forward propagation,
backward propagation updates, you don't end up
having a good network mo- most of the time.
In order to get a good network, you need to improve it.
You need to use a bunch of techniques that will make your network work in practice.
The first, the first trick is to use different activation functions.
So together, we've seen one activation function which was Sigmoid.
And we remember the graph of Sigmoid is getting a number
between minus infinity and plus infinity and casting it between 0 and 1.
And we know that the formula is Sigmoid of z equals
1 over 1 plus exponent so minus z.
We also know that the derivative of Sigmoid is Sigmoid of z times 1 minus Sigmoid of z.
Okay. Another very common,
uh, activation function is ReLU.
We talked quickly about it last time.
ReLU of z which is equal to 0 if z is less than 0 and z if z is positive.
So the graph of ReLU looks like something like this.
And finally, another one we were using commonly as well is
tan h. So hyperbolic tangents and
tan h of z exponential z minus
exponential minus z over exponential z plus exponential minus z.
The derivative of tan h is
1 minus tan h squared of z.
And the graph looks kind of like Sigmoid,
but, but it goes between minus 1 and plus 1.
So one question.
Now that I've given you three activation functions,
can you guess why we would use one instead of the other and,
and which one has more benefits?
So when I talk about activation functions,
I talk about the functions that you will put in these neurons after the linear part.
What do you think is the main advantage of Sigmoid? Yeah.
We use it for classification.
Yep. You use it for classification,
between it gives you a probability.
What's the main disadvantage of Sigmoid?
It's easy.
It's easy. That should be an advantage,
should be a benefit. Yeah?
[BACKGROUND]
Correct. If you're at high activation,
if you are at high z's or low z's,
your gradient is very close to 0.
So look here. Based on this graph we know that if z is very big.
If z is very big our gradient is going to be very small,
the slope of this,
of this graph is very, very small. It's almost flat.
Same for z's that are very low in the negative.
Right. What's the problem with having low gradients is when I'm back propagating.
If the z I cached was big,
the gradient is going to be very small and it will be super hard to update
my parameters that are early in the network because the gradient is just going to vanish.
Does that makes sense?
So Sigmoid is one of these activations which,
which works very well in the linear regime,
but has trouble working in saturating
regimes because the network doesn't update the parameters properly.
It goes very, very slowly.
We're going to talk about that a little more.
How about tan h? Very similar, right?
Similar like high z's and low z's lead to saturation of a tan h activation.
ReLU on the other hand doesn't have this problem.
If z is very big in the positives, there is no saturation.
The gradient just passes and the gradient is 1, when we were here.
The slope is equal to 1.
So it's actually just directing the gradient to some entry.
Is not multiplying it by anything when you backpropagate.
So you know this term here,
this term that I have here.
All the a3 minus a3 times 1 minus a3 or 1 minus a2.
If we use ReLU activations,
we would change this with what's-
with- with the derivative of ReLU and the derivative of
ReLU can be written indicator function
of z being positive.
You've seen indicator functions.
So this is equal to 1 if z is positive, 0 otherwise.
Okay. So we will see why we use ReLU mostly. Yeah?
[BACKGROUND]
Yeah. You remember the house prediction example?
In that case, if you want to,
if you want to predict the price of a house based on some features, you would use ReLU.
Because you know that the output should be
a positive number between 0 and plus infinity,
it doesn't make sense to use 1 of tan h or similar. Yep.
[BACKGROUND]
Doesn't really matter. I think if,
if I want my output to be between 0 and 1 I would use Sigmoid,
if I want my output to be between minus 1 and
1 I would use tan h. So you know, there is,
there are some tasks where the output is kind of
a reward or a minus reward that you want to get.
Like in reinforcement learning,
you would use tan h as an output activation which is
because minus 1 looks like a negative reward,
plus 1 looks like a positive reward,
and you want to decide what should be the reward.
Why do we consider these functions?
Good question. Why do we consider these functions?
We can actually consider any functions apart
from the identity function. So let's see why.
Thanks for the transition. [LAUGHTER] Like why do we need activation functions?
So let's assume that we have a network which is the same as before.
So our network is three neurons casting into two neurons casting into one neuron, ah,
and we're trying to use activations are equal to identity functions.
So it means z is given to z.
Let's try to derive the forward propagation, y_hat equals a_3,
equals z_3, equals w_3, a_2 plus b_3.
I know that a_2,
a_2 is equal to z_2 because there is no activation and z_2 is equal to w_2 a_1 plus b_2.
So I can cast here w_2,
w_2 a_1 plus b_2 plus b_3.
I can continue.
I know that a_1 is equal to z_1,
and I know that z_1 is w_1 x plus b,
and b equals w_3 times w_2 times
b_1 plus w_3 times
b_2 plus b_3.
So what's the insight here?
Is that we need activation functions.
The reason is, if you don't choose activation functions,
no matter how deep is your network,
it's going to be equivalent to a linear regression.
So the complexity of the network comes from the activation function.
And the reason we can understand- if we're trying to detect cats,
what we're trying to do is to train a network that
will mimic the formula of detecting cats.
We don't know this formula,
so we want to mimic it using a lot of parameters.
If we just have a linear regression,
we cannot mimic this because we are going to look
at pixel by pixel and assign every weight to a certain pixel.
If I give you an example, it's not going to work anymore. Yeah, yeah.
So I think that's,
that, that goes back to your question as well.
So this is why we need activation functions.
And then the question was, can we use different activation functions and how do we,
how do we put them inside a layer or inside neurons?
Usually, we would use,
there are more activation functions.
I think in CS230 we'll go over a few more but not, not, not today.
These have been designed with experience,
so these are the ones that's,
that, that's work better and lets our networks train.
There are plenty of other activation functions that have been tested.
Usually, you would, you would, uh,
use the same activation functions inside every layer.
So when you, it's,
it's a, it's, it's for, for training.
It doesn't have any special reason I think but when you have a network like that,
you would call this layer a ReLU layer
meaning it's a fully connected layer with ReLU activation.
This one a Sigmoid layer,
it means it's a fully connected layer with the Sigmoid activation.
And the last one is Sigmoid.
I, I think people have been trying a lot of putting,
activat- different activations in different neurons in a layer,
in different layers and the consensus was using one activation in
the layer and also using one of these three activations.
Yeah. So if someone comes up with a better activation that is
obviously helping training our models on different datasets,
people would adopt it but right now these are the ones that work better.
And you know, last time we talked about hyper-parameters a little bit.
These are all hyper-parameters.
So in practice, you're not going to choose these randomly,
you're going to try a bunch of them and choose some
of them that seem to help your model train.
There's a lot of experimental results in deep learning and we don't really
understand fully why certain activations work better than others.
Okay, let's move on.
[NOISE]
Okay, let's go over initialization techniques.
[NOISE]
Uh, actually, let me use this board.
So another trick that you can use
in order to help your network train
are initialization methods and normalization methods.
So, um, earlier we talked about the fact that if z is too big,
or z is too low in the negative numbers,
it will lead to saturation of the network.
So in order to avoid that you can use normalization of the input.
So assume that you have a network where the data is two-dimensional,
x_1, x_2 is our two-dimensional input.
You can assume that x_1, x_2
is distributed like this, let's say.
So this is if I plot x_1 against x_2 for a lot of data,
I will get that type of graph.
Uh, the problem is that if I do my wx plus b,
to compute my z_1,
if xs are very big,
it will lead to very big zs which will lead to saturated activations.
In order to avoid that, one method is to compute the mean of
this data using Mu equals 1
over the size of the batch of data that you have in the training sets.
Sum of xis.
So it's just giving you the mean for x_1,
and the mean for x_2.
You would compute the operation x equals x minus Mu,
and you will get that type of plot.
If you replot the transform data,
let's say x_1 tilde, x_2 tilde.
So here is a little better,
but it's still not good.
In order to solve the problem fully,
we are going to compute Sigma squared,
which is basically the standard deviation squared, so the variance of the data,
and then you will divide by, uh, Sigma squared.
So you would do that and you would make the transformation of
x being equal to x divided by Sigma,
and it will give you a graph that is
centered up here.
So you, you usually prefer to,
to work with a centered data. Yeah?
[inaudible] tilde?
Sorry, oh yeah, yeah,
sorry, sorry, yeah, correct.
So if we subtract the mean of x_1 and x_2,
it will be
[inaudible].
Sorry, it should look like this, but it would be centered.
Okay, and then if you stan- if you standardize it,
it looks like something like that.
So why is it better?
Because if you look at you- your loss function now,
before the loss function would look like something like this.
[NOISE] And after normalizing the inputs,
it may look like something, something like this.
So what's the difference between these two loss functions?
Why is this one easier to train?
It's because if you have the starting point that is here let's say,
their gradient descent algorithm is going to go to
towards approximately the steepest slope.
So we're going to go like there,
and then this one is going to go there,
and then you're going to go there,
and then you're going to go there like that and so on,
until you end up at the right points.
But the steeper slope in this loss contour is always pointing towards the middle.
So if you start somewhere,
it will directly go towards the minimum of your loss function.
So that's why it's helpful usually to normalize.
So this is one method, uh,
and in practice, the way you initialize your weights is very important. Yeah?
[BACKGROUND]
Uh, yes. So.
[BACKGROUND]
Exactly. So here I used a very simple case but you would divide elementwise by,
by the Sigma here, okay?
So like every entry of your matrix you would divide it by the Sigma.
One, one other thing that is important to notice.
This Sigma and Mu are computed over the training set.
You have a training set, you compute the mean of the training, set the standard deviation,
of the training set, and these Sigma and Mu have to be used on the test set as well.
It means now that you want to test your algorithm on the test set,
you should not compute the mean of the test set,
and the standard deviation of the test set and normalize
your test inputs through the network.
Instead, you should use the Mu and the Sigma that were computed on the train set
because your network is used to seeing this type of transformation as an input.
So you want the distribution of the inputs at the first neuron to be always the same,
no matter if it's a train or the test set.
What you do is that [inaudible]
Here? Likely, yeah.
This leads to fewer iterations.
Okay, we have a lot to see so I will,
I will skip a few questions.
So let's, let's delve a little more into vanishing and exploding gradients.
So in order to get an intuition of why we
have these vanishing or exploding gradient problem,
we can consider a network which is very, very
deep and has a two-dimensional input, okay?
And so on. So let's say we have,
let's say we have ten layers in total.
Ten layers plus an output layer.
So assume, assume all the activations are identity functions,
and assume that these biases are equal to 0.
If you compute y hats,
the output of the network with respect to the input.
You know that y hat will be equal to w of layer L,
capital L denotes the last layer,
times a l minus 1 plus bL,
but bL is 0 so we can remove it.
w_l times a_L minus 1.
You know that a_L minus 1 is w_l minus 1
times a_L minus 2 because the activation is an identity function and so on.
You can back propagate, you can go back and you will get that y hat equals
w_L times w_l minus 1 times blah, blah, blah, times w_1 times x.
You get something like that, right?
So now, let's consider two cases.
Let us consider the case where
the w_l matrices are a little bigger than the identity function,
a little larger than the identity function in terms of values.
Let's say w_l, including all these.
So all these matrices which are 2 by 2 matrices,
right, are these ones.
What's the consequence?
The consequences that this whole thing here is going to be equal to 1.5 to the power L,
1.5 to the power L, 0, 0.
It will make y hat explode.
It will make the value of y hat explode,
just because this number is a tiny little bit more than 1.
Same phenomenon, if we had 0.5 instead of 1.5 here, the value,
the multiplicative value of all these matrices will be 0.5 to the power L here,
0.5 to the power L here,
and y hat will always be very close to 0.
So you see, the issue with vanishing exploding gradients is
that all the errors add up like multiply each other.
And if you end up with numbers that are smaller than one,
you will get a totally vanished gradient.
When you go back, if you have
values that are a little bigger than 1 you will get exploding gradients.
So we did it as a forward propagation equation,
we could have done it exactly the same analysis.
We did derivatives, assuming the derivatives
of the weight matrices are a little lower than the identity,
or a little higher than the identity.
So we want to avoid that.
One way that is not perfect to,
to avoid this is to initialize your weights properly,
initialize them into the right range of values.
So you agree that we would prefer the weights to be around 1,
as close as possible to 1.
If they're very close to 1,
we probably can avoid the vanishing and exploding gradient problem.
So let's look at the initialization problem.
The first thing to look at is example of the one neuron.
[NOISE]
If you consider this neuron here,
which has a bunch of inputs and outputs and activation a.
[NOISE] You know that the equation inside the neuron is
a equals whatever function, let's say sigmoid of Z and you know
that z is equal to W_1 X_1 plus W_2 X_2 plus blah,
blah, blah plus W_n X_n.
So it is a dot product between the W's and the X's.
So the interesting thing to notice is that we have n terms here.
So in order for Z to not explode,
we would like all of these terms to be small.
If W's are too big,
then this term will explode with the size of the inputs of the layer.
So instead if we have a large n, it means the input is very large,
what we want is very small W_i's.
So the larger n, the smaller it has to be W_i.
So based on this intuition,
it seems that it would be a good idea to initialize
W_i's with something that is close to 1 over n. We have n terms,
the more terms we have, the more likely Z is going to be big.
But if our initialization says
the more terms you have, the smaller the value of the weights,
we should be able to keep Z in a certain range
that is appropriate to avoid vanishing and exploding gradients.
So this seems to be a possible initialization scheme.
So in practice, I'm going to write a few initialization schemes that we're not gonna prove.
If you're interested in seeing more proofs of that,
you can take CS230,
where we prove this initialization scheme.
May I take down the board?
So there are a few initializations that are commonly used and again, this is,
this is very practical and people have been testing a lot of initializations,
but they ended up using those.
[NOISE] So one is to initialize the weights.
I'm writing the code for those of you who know numPy.
I'm not gonna compile it here.
With whatever shape you are using,
elementwise times the square root
of 1 over n of L minus 1.
So what does that mean? It means that I will look at the number of inputs.
I'm writing an L minus 1 here, n to the L minus 1.
I'm looking at how many inputs are coming to my layer
assuming we're at layer L. How many inputs are coming.
I'm going to initialize the weights of
this layer proportionally to the number of inputs that are coming in.
So the intuition is very similar to what we described there.
So this initialization has been shown to work very well for sigmoid activations.
So if you use sigmoid.
What's interesting is if you use ReLU, it's been,
it's been observed that putting a 2 here
instead of a 1 would make the network train better.
And again, it's very practical.
It's one of the fields that,
that we need more theory on it,
but a lot of observations had been made so far.
Do you guys want to just do that as a project to see
why is this happening? It would be interesting.
Okay. [NOISE] And finally,
there is a more common one that is used which is called the Xavier initialization,
which proposes to update the weights [NOISE] using,
uh, square root of 1 over n_ l minus 1 for tan h. This is another one.
And another one that is I believe called Glorot initialization
recommends to initialize the weights of a layer using the following formula.
So quickly, the, the quick int- intuition behind the last one.
The last one is, is very often used.
The quick intuition is that we're doing
the same thing but also for the backpropagated gradients.
So we're saying the weights are going to multiply the backpropagated gradients.
So we also need to look at,
at how many inputs do we have during the backpropagation.
And L is the number of inputs you have during backpropagation
and L minus 1 is the number of inputs you have during forward propagation.
So taking an average,
a geometric average of those.
[NOISE]
And the reason we have a random function here is because
if you don't initialize your weights randomly,
you will end up with some problem called the symmetry
problem where every neuron is going to learn kind of the same thing.
To avoid that, you will make the neuron starts at different places and let
them evolve independently from each other as much as possible.
So now we have two choices.
Either we go over regularization or optimization.
How much have you talked about regularization so far L1,
L2, early stopping, all that?
Early stopping, everybody remembers what it is?
No? Little bit?
So let's go over optimization, I guess,
and then we will do some regularization depending on the time we have.
[NOISE]
So I believe
so far you've seen
gradient descent and stochastic gradient descent as two possible optimization algorithms.
In practice, there is a trade-off
between these two which is called mini-batch gradient descent.
What is the trade-off?
The trade-off is that batch gradient descent is cool because you can use vectorization,
you can give a batch inputs, forward
propagate it all at once doing vec- using a vectorized code.
Stochastic gradient descent's advantage is that the updates are very quick.
And imagine that you have a dataset with one million images.
One million images in the dataset and you wanna do batch gradient descent.
Do you know how long it's going to take to do one update? Very long.
So we don't want that because maybe we don't need to go
over the full dataset in order to have a good update.
Maybe the updates based on 1,000 examples
might already give us the right direction for the gradient [NOISE] of where to go.
It's not gonna be as good as on
the median example where it's going to be a very good approximation.
So that's why most people would use mini-batch gradient descent,
where you have a trade-off between stochasticity and also vectorization.
So in terms of notation,
[NOISE] I'm going to call X the matrix x_1,
x_2, x_m, and capital Y the same matrix with y_m.
So we have m training examples.
And I'm going to split these into batches.
So I'm going to call the first batch x_1 like
this until x maybe T like that.
And x_1 can contain probably x_1 until x_1,000.
Assuming it's a batch of 1,000 examples.
X_2 then will contain x_1,001 until x_2,000 and so on.
So this is the notation for the batch when I use curly brackets.
Same for Y. [NOISE]
So in terms of algorithm,
how does the Mini-batch gradient descent algorithm work?
We're going to iterate. So for iteration t from 1 to blah, blah, blah,
to how many iteration you wanna do.
We're going to select a batch,
select a batch of x_t- x_t, y_t.
You will forward propagate the batch,
and you will backpropagate the batch.
So by forward propagation, I mean,
you send all the batch to
the network and you compute the loss functions for every example of the batch,
you sum them together and you compute the cost function over the entire batch,
which is the average of the loss functions.
And so assuming- assuming the batch is of size 1,000,
this would be the- the formula to compute the batch over 1,000 examples.
And after the backpropagation, of course,
updates, W_l and D_l for all the l's, for all the layers.
This is the- the equation.
So in terms of graph,
what you're likely to see is that for batch gradient descent,
your cost function j would have looked like that,
if you plot it against the number of iterations.
On the other hand, if you use a Mini-batch gradient descent,
you're most likely to see something like this.
So it is also decreasing as a trend,
but because the gradient is approximated and doesn't necessarily
go straight to the- to the middle of
your loss fun- to the lower point of the loss function,
you will see a kind of graph like that.
The smaller the batch, the more stochasticity.
So the more noise you will have on your cost function graph.
And of course, if you- if we plot
again- if we plot the loss function and this was gradient descent,
so this is the top view of the loss function,
assuming we're in two dimensions.
Your stochastic gradient descent or batch gradient descent would do something like that.
So the difference is- there seem to be less iteration with the red algorithm,
but the iterations are much heavier to compute.
So each of the green iterations are going to be very- very- very quick,
while the red ones are going to be slow to compute. This is a trade off.
Now there is another algorithm that I wanna go over which is called
the momentum- momentum algorithm.
Sometimes called gradient descent plus momentum algorithm.
So what's the intuition behind momentum?
The intuition is, let's look at this loss contour plot.
And I'm doing an extreme case just to illustrate the intuition.
Assume you have the loss that is very extended in one direction.
So this direction is very extended and the other one is smaller.
You're starting at a point like this one.
Your gradient descent algorithm itself is going to follow the falling bar,
it's going to be orthogonal to the current contour,
uh, iso- iso term.
Contour loss is going to go there,
and then there, and then there,
and then there, and so on.
So what you would like is to move it faster
on the horizontal line and slower to the vertical- on the vertical side.
So on this axis you would like to move with smaller updates.
And on this axis,
you wanna move with larger updates, correct?
If this happened, we would probably end up
in the minimum much quicker than we currently are.
So in order to do that, we're going to use a technique called momentum,
which is going to look at the past gradients.
So look at the past updates. Assume we're here.
Assume we are somewhere here.
Gradient descent doesn't look at its past at all.
You just will compute the forward propagation,
compute the backdrop, look at the direction and go to that direction.
What momentum is going to say is look at the past updates that you did
and try to consider these past updates in order to find the right way to go.
So if you look at the past update and you take an average of the past update.
You would take an average of these update going up and the update after it going down.
The average on the vertical side is going to be small,
because one went up, one went down.
But on the horizontal axis,
both went to the same direction.
So the update will not change too much on the vert- on- on this axis.
So you're most likely to do something like that if you use momentum.
Does it make sense the intuition behind it?
So that's the intuition why we want to use momentum.
And for those of you who do physics,
sometimes you can think of momentum as friction.
You know like- like if you- if you launch a rocket and you wanna move it quickly around.
It's not gonna move, because the rocket has a certain weight and has a certain momentum.
You cannot change its direction very, very noisily.
[NOISE]
So let's see
the implementation of- of- of momentum gradient descent.
Oh, and I believe we- we're almost done, right?
Yeah. Okay. [NOISE] So let's look at the- the implementation quickly.
So gradient descent was w equals w minus Alpha,
derivative of the loss with respect to w. What
we are going to do is we're going to use another variable called velocity,
which is going to be the average of the previous velocity and the current weight updates.
So we're going to use that,
and instead of the updates being the derivative directly,
we're going to update the velocity.
So the velocity is going to be a variable that tracks the direction that we should
take regarding the current update and also
the past updates with a factor Beta that is be- going to be the weights.
The interesting point is that in terms of implementation it's one more line of code,
in terms of memory,
it's just one additional variable,
and it actually has a big impact on the optimization.
There are much more optimization algorithms that we're not going to see together today.
In CS230, we teach something called RMSProp and Atom.
That are most likely the- the- the ones that are used the most in deep learning.
Uh, and the reason is, uh,
if you come up with an optimization algorithm,
you still have to prove that it works very well on the wide variety of
application between- before researchers adopt it for their research.
So Atom brings momentum to the deep learning optimization algorithms.
Okay. Thanks guys.
Uh, and that's all for deep learning in CS229 so far.
 Okay. Happy Halloween.
Um, what I want to do today is share with you advice for applying machine learning.
And, and you've heard me allude to this before,
but, um, uh, yeah,
I think over the last several weeks,
you've learned a lot about the mechanics of how to build different learning algorithms.
Everything from linear regression,
logistic regression, SVMs, uh,
uh, Random Forest, is it, uh, uh, neural networks.
And what I want to do today is share with you some
principles for helping you become efficient at how
you apply all of these things to solve
whatever application problem you might want to work on.
Um, and so, uh,
a lot of today's material is actually not that mathematical.
There's also some of the hardest materials as we're in,
in, in, in this class to understand.
Um, it turns out that when you give advice
on how to apply a learning algorithm such as, you know,
"Don't waste lots of time collecting data unless you,
you, you have confidence it's useful to actually spend all that time."
It turns out when I say things like that,
people, you know, easily agree.
They say, "Of course, you shouldn't waste time collecting lots of data
unless you have some confidence it's actually a good use of your time."
That's a very easy thing to agree with.
Um, but the hard thing is when you go
home today and you're actually working on your class project,
right, uh, uh, to apply the principles we talked about today.
When you're actually on the ground talking to a teammate saying,
"All right, do we collect more data for our class project now or not? "
To make the right judgment call for that.
To map the concepts you learned today.
To when you're actually in the hot seat,
you know making a decision to be going,
spending another two days scraping data off the Internet,
or are you gonna tune this algorithm,
tune these parameters to the algorithm and actually make those decisions is actually, um,
uh, it, it, it often takes a lot of,
um, careful thinking to make the mapping.
From the principles we're talking about today,
and then probably all of you go,
"Yep, that makes sense."
But to actually do that when you're in the hot seat making the decisions.
That, that, that's something that, um,
will often take, take some careful thought, I guess.
Um, and I think, uh,
uh, you know, for a long time, um,
the concepts of machine learning have been an art, right, where,
you know, we'll, we'll go to these people that have been doing it for 30 years.
And you say, "Hey, my learning algorithm doesn't work," you know,
uh, uh, uh, what do we do now?
And then they will have some judgment or you go.
And people asked me and for some reason because we've
done it for a long time, we will say, "Oh yeah,
I get more data, or I'll tune that parameter, or try a neural network with big hidden units, and for some reason that'll work.
And what I hope to do today is,
uh, turn that black magic, that, that,
that art into much more refined,
so that you can much more systematically make these decisions yourself, rather than,
uh, talk to someone, um,
who's done this for 30 years, then for,
for some reason is able to give you the good recommendations even if,
you know, that to turn from, um,
more of a black art into more of a systematic engineering discipline.
Um, and, and just, uh, uh, one note.
Uh, some of the work we are gonna do today is not the best approach for,
uh, developing novel machine learning research,
or if you're- if your main goal is to write research papers, uh,
some of what I'll say will apply,
some of what I'll say will not apply,
but I'll come back to that later.
So most of today's focused on how to help you build stuff that works,
right, to build, build applications that work.
Um, so the three key ideas, um,
you see today are first is, uh,
diagnostics for debugging learning algorithms.
Um, one thing you might not know,
or actually if you're working on the class project maybe you know this already is that,
uh, when you implement a learning algorithm for the first time,
it almost never works, right?
At least not the first time.
Uh, uh, uh, and so,
um, what is it- I still remember was- there was a weekend,
um, uh, about a year ago where I implemented Softmax regression on my laptop,
and it worked the first time.
And even to this day, I, I still remember that feeling of surprise,
like, no, there's got to be a bug and I
went in to try to find the bug, and there wasn't a bug.
But, but it's so rare.
[LAUGHTER] So the learning algorithm worked the first time.
I still remember it over a year later.
Uh, and so a lot of the workflow of developing learning algorithms,
it, it actually feels like a debugging workflow, right?
Um, and so I want to help you become systematic at that.
Um, and, uh, uh,
two key ideas here are about error analysis, and integrative analysis.
[NOISE] So how to analyze the errors in your learning algorithm.
And also how to, how to understand what's not working,
what's error analysis, and how to understand what's working,
which is ablative analysis.
And then, and then finding with some philosophies and how to
get started on a machine learning project,
su- such as your class project, okay?
So let's start with, uh,
discussing debugging learning algorithms.
Um, so what happens all the time is you have an idea for a machine learning application.
You implement something, uh,
and then it won't work as well as you hoped.
And the key question is,
what do you do next, right?
When I want to work on machine learning algorithm, that's actually most of my workflow.
We usually have something implemented.
It's just not working that well.
And your ability to decide what to do next has a huge impact on,
on, on your efficiency.
Um, uh, I, I think, uh, when, when,
um, when I was, uh,
when I was an undergrad, uh,
at Carnegie Mellon University,
I had a friend, um, that would, uh,
debug their code by,
um, you know, they write a piece of code.
And then as always, when you write a piece of code,
initially there's always a bunch of syntax errors, right?
And so their debugging strategy was to delete
every single line of code that generated
a syntax error because this is a good way to get rid of errors.
So that wasn't a good strategy.
So in, in, in machine learning as well,
there are good and less good debugging strategies, right?
Um, so let's start with a motivating example.
Uh, let's say we're building an anti-spam classifier.
And, um, let's say you've carefully
chosen a small set of a hundred words to use as features
So instead of using, you know,
10,000 or 50,000 words.
You've chosen a hundred words that you think could be most relevant to, um, anti-spam.
And let's say you start off implementing logistic regularization.
Uh, I think when we talk about this,
this is also, you know,
there's a frequentist and Bayesian school,
but you can think of this as Bayesian which is a progression where, uh,
you have the maximum likelihood term on the left,
and then that second term is the regularization term, right?
Um, so that's, so that's Bayesian logistic regression.
If you're Bayesian or, uh,
which is regression with regularization if you are,
uh, you know, using frequency statistics.
[NOISE] And let say that, um,
logistic regression with regularization or Bayesian logistic regression,
it gets 20% test error which is unacceptably high, right?
Making one in five mistakes on, on your spam filter.
Um, and so what do you do next?
Um, now, for this scenario, I, I wanna,
uh, and, and so, um, for, uh,
when you implement an algorithm like this, uh,
what many teams will do is,
um, try improving the algorithm in different ways.
So what many teams would do is say, "Oh yeah,
I remember, you know, well,
we like big data, more data always helps."
So let's get some more data and hope that solves the problem.
So one or some teams would say,
"Let's get more training examples."
And, and, and it's actually true, you know,
more data pretty much never hurts.
It almost always helps,
but the key question is how much.
Um, or you could try using a smaller set of features.
With a hundred features probably some weren't that relevant.
So let's get rid of some features.
Um, or if you try having a larger set of features,
hundred features is too small, right?
So let's add more features.
Um, uh, or you might want other designs of the features, you know,
instead of, uh, uh, just using features in an e-mail body,
uh, you can use features from the e-mail header.
Uh, the e-mail header has,
um, uh, not just a From To subject,
but also routing information about what's the set of
servers of the Internet that the e-mail took to get to you.
Um, uh, or you could try running gradient descent for more iterations.
That, that, you know, that never hurts, right, for usually.
Uh, uh, from gradient descent, let's switch to Newton's methods.
Uh, or let's try a different value for Lambda.
Um, or, or we say, you know,
forget about Bayesian logistic regression or,
or run regression regularization.
Let's, let's use a totally different algorithm,
like an SVM or neural networks or something, right?
So what happens in a lot of teams is, um,
uh, someone will pick one of these ideas, kind of at random.
Um, it depends on, you know,
what they happen to read the night before, right, about something.
Uh, or, or their experience on the last project.
And sometimes a project,
and sometimes you or the project leader will say,
uh, you know, we'll pick one of these and just say, "Let's try that."
And then spend, spend a few days or few weeks trying that,
and it may or may not be the best thing to do.
So, um, uh, I think that in, in,
in my team's machine learning workflow,
so first, if you actually, you and a few others,
sit down and brainstorm a list of the things you could try, you actually are,
are already ahead of a lot of teams because all teams
will come just by gut feeling, right?
Um, uh, or the most opinionated person will pick one of these things at random and do that,
but you brainstorm a list of things and then,
and then try to evaluate the different options.
You're already ahead of many teams.
Um, oh, sorry, and I think,
uh, uh, yeah and I think, right, you know,
unless you analyze these different options, um, uh,
uh, it's hard to know which of these is actually the best option.
So, um, the most common diagnostic I end up using in developing learning algorithms,
is a, um, bias versus variance diagnostic, right?
And I think I, um,
talked about bias and variance already with a classifier is
highly biased, then it tends to under fit the data.
So high bias is, well, actually.
You guys remember this, right?
If, um, If you have a dataset just like this,
a highly biased classifier may be much too simple,
and high variance classifiers may be much too complex,
and some- something in-between,
you know, with, with trade off bias and variance in an appropriate way, right?
So that's bias and variance.
Um, and so, uh,
it turns out that one of the most common diagnostics I end up using in
pretty much every single machine learning project is a bias versus variance diagnostic.
So understand how much of
your learning algorithm's problem comes from bias and how much of it comes from variance.
Um, and, uh, uh, and,
you know, I- I've had,
I don't know, like former PhD students, right,
that- that learned about bias and variance when they're doing their PhD and
then sometimes even a couple of years after they've graduated from Stanford and worked,
you know, on more practical problems.
They actually tell me that, that,
that their understanding of bias and variances continue to deepen,
right, for, for, for many years.
So this is one of those concepts is, is, um,
if you can systematically apply it, you'll be much more efficient and this is really the,
maybe the single most useful tool I've found,
understanding bias and variance at debugging learning algorithms.
Um, and so what I'm gonna describe,
is a workflow where you would run some diagnostics to figure out what is the problem,
uh, and then try to fix what the problem is.
And so, um, just to summarize this no- this example.
Um, uh, this logistic regression error is unacceptably
high and you want to- and you suspect problems due to high variance or high bias.
And so, um, it turns out that there's a diagnostic that lets
you look at your algorithm's performance and try to figure out if,
um, how much of the problem is variance and how much of the problem is bias.
Oh, and I'm going to say test error,
but if you are developing,
should I really be doing this with a dev set or development set rather than a test set, right?
But so let me, let me explain this, um,
uh, diagnostic in greater detail.
Uh, so it turns out that, um,
if you have a classifier with very high variance,
then the performance on the test set,
or actually would be better,
better practice to use the hold-out cross validation so the, the development set.
You see that the error that you classify has, um, much, uh,
uh, much lower error on the training set than on the development set.
But in contrast, if you have high bias,
then the training error and the test set error and the dev set error will go behind.
So let me sh- let me illustrate this with a picture.
Um, so this is a learning curve and what that means is, um,
on the horizontal axis,
you are going to vary the number of training examples, right?
Uh, and when I talk about bias and variance,
I had a plot where the horizontal axis was the degree of polynomial, right?
You fit a first order, second order,
third order, fourth order polynomial.
In this plot, the horizontal axis is different, it's the number of training examples.
And so it turns out that,
um, whenever you train a learning algorithm, you know,
the more data you have usually,
the better your development set error,
the better your your test set error, right?
This error usually goes down,
when you increase the number of training examples.
The other thing, the other- and,
and let's say that you're hoping to achieve a certain level of desired performance,
you know, for business reasons,
you'd like your spam classifier to achieve
a certain level of design performance and often- sometimes,
desired level of performance is, um,
to do about as well as a human can.
That's a common business objective depending on your application,
uh, but sometimes it can be different, right.
So you have some- your product manager, you know,
tells you that well you,
if you're leading the project,
you think that you need to hit a certain level of
target performance in order for it to be a very useful spam filter.
So the other plot, uh, uh,
to add to this which will help you analyze bias
versus variance is to plot the training error.
Um, now one thing to help you with training error is that it increases, um, uh,
as the training set size increases because,
if you have only one example, right?
Let's say you're building a spam classifier and you have only one training example,
then any algorithm, you know,
can fit one training example perfectly.
And so if your training set size is very small.
The training set error is usually 0, right?
If you have like 5, 10 examples,
you probably can fit all 5 examples perfectly. And it's only if you have a bigger training set
that it becomes harder for the learning algorithm to
fit your training data that well, right?
Or in the the linear regression case,
here you have you have one example,
yeah you can fit a straight line to data,
if you have two examples, you can fit any model,
pretty much to the data,
and have zero training error.
There's only a very, very large training set that a classifier like
logistic regression or linear regression may have
a harder time fitting all of your training examples.
So that's why training error or average training error,
average over your training set, uh,
generally increases, um, as you increase the training set size.
So, um, now there are two characteristics of this plot,
that suggest that, um,
if you plot the learning curves if you see the- this, this pattern,
this suggests that, um,
the algorithm has a large bias problem, right?
And the two properties written at the bottom,
one, the weaker signal,
the one that's harder to rely on,
is that, um, the development set error,
or the test set error is still decreasing,
as you increase the training set size.
So the green curve is still,
you know, still looks like it's going down,
and so this suggests that if you increase
the training set size and extrapolate further to the right,
that the curve would keep on going down.
Um, this turns out to be a weaker signal because sometimes we look at a curve like that,
it's actually quite hard to tell,
you know, to extrapolate to the right.
Uh, uh, if you double the training set size,
how much further would the green curve go down?
It's actually kind of hard to tell.
So I find this a useful signal,
but sometimes it's a bit hard to judge, you know,
exactly where the curve will go if you extrapolate to the right.
Um, the stronger signal is actually the second one,
the fact that there's a huge gap between your training error and your test set error,
or your training or your dev set error would be the better thing to look at.
It's actually a stronger signal that,
um, this particular learning algorithm has,
um, has high variance right,
um, uh, because, uh,
as you increase the training set size,
you find that the gap between, um,
training and test error usually closes, usually reduces.
And so there's still a lot of room, for, um, uh,
making your test set error become closer to your training error.
And so if you see a learning curve like this, this is a strong sign that,
um, you have a variance problem, okay?
Now let's look at what the curve- what the learning curve will look like,
um, if you have a bias problem.
Um, so this is a typical learning curve for high bias which is, uh,
that's your dev set error or your development set cross-validation error, uh, test error,
and you're hoping to hit a level of performance like that,
and your training error looks like that.
And, um, so one sign that you have
a high bias problem is that
this algorithm is not even doing that well on the training set, right?
Even on the training set, you know,
you're not achieving your desired level of performance,
and it's like, look learn, i- i- imagine you know,
you're, you're looking at learning algorithms and say,
it's like this algorithm has seen these examples and
even for examples it's seen, it's not doing as well as you were hoping.
So clearly the algorithm's not fitting the data well enough.
So this is a sign that you have a high bias problem,
not enough features, your learning algorithm is too simple.
And the other signal is that, um,
uh, this is very a small gap between the training and, uh, the test error, right?
And you can imagine when you see a plot like this,
no matter how much more data you get, right,
go ahead and extrapolate to the right,
as far as you want, you know.
No matter how much more data you get,
um, no matter how far you extrapolate to the right of this plot,
the gree- the blue curve, the training error,
is never going to come back down,
to hit the desired level of performance.
Uh, and because the test set error is you
know generally higher than your training set error,
no matter how much more data you have,
no matter how far you extrapolate to the right,
the error is never going to come down to,
to your desired level of performance.
So if you get a, um,
training error and test error curve that looks like this,
you kind of know that, you know,
while getting more training data may help, right?
The green curve could come down, like a little bit.
If you get more training data, uh,
the act of getting more training data by
itself will never get you to where you want to go.
Okay? Um, so let's work through this example.
So for each of the four bullets here, um,
each of the four- first four ideas fixes
either a high variance or a high bias problem, right?
So let's, let's go through them and, and ask, uh,
for the first one,
do you think it, do you think it helps you fix high bias or high variance?
[BACKGROUND]
High variance, right?
Okay. Right. Cool. All right, high variance, right?
A- anyone want to say, say- well, great.
Anyone want to say why? Yeah, okay.
[inaudible]
All right, cool, yes, uh, right. Yeah, right.
I guess if you're fitting a very high order polynomial that wiggles like this,
if you have more data,
it will make it- then you won't have these oscillates,
so crazy even if you have a higher order polynomial.
Right. And, um, if you look at a
high variance curve, um,
this was- wow, there's a lot of latency, you know.
That's all for some reason.
Huh. Right, sSo this is a high variance plot.
Um, and, uh, uh,
and if you have a learning algorithm of high variance, you can,
hopefully, you know, if you extrapolate to the right,
there is some hope that the green curve will keep on coming down.
So, so getting more training data if you have high variance,
which is if you're in this situation,
looks like it could help you- help- it's,
it's worth trying, right?
I can't guarantee it work, but it's worth trying.
[inaudible] when you think about these functions,
like for certain algorithms [inaudible] uniformly distributed.
Oh, I see. Yes. Sorry. That's a good one. So let's see.
Um, the curves will look like this assuming that your training data is IID, right?
Um, the training and dev and test sets are all drawn from the same distribution.
Uh, uh, uh, there is learning theory that suggests that in most cases,
the green curve should decay as 1 over
square roots of m. That's the rate that which it should decay,
uh, until, until it reaches some Bayes error.
That's what the learning theory says.
Does that make sense? Um, and sometime- and,
and learning algorithms errors don't always go to 0, right?
Because sometimes, uh, uh,
there- sometimes, um, the data is just ambiguous.
I don't know, like, uh, I guess, you know,
my PhD students, including Annan,
we do a lot of work in healthcare.
And sometimes when you look at an x-ray,
it's just blurry, and you could try to make a diagnosis, right?
Is there, is there, uh- or I actually,
Annan is working on predicting patient's mortality.
What's the chance that someone dying in the next year or so?
And sometimes you look at a patient's medical record,
and you just can't tell when- what's,
you know, will, will they pass away in the next year or so.
Or you're looking at an x-ray,
you just can't tell is there,
is there a tumor or not?
Because it's just blurry, and so learning algorithm's error don't always decay to zero,
but the theory says that as,
as M increases, it will decay at roughly a rate of 1 over square root of M,
um, toward that baseline error,
which is, which is called Bayes error,
which is the best that you could possibly hope
anything could do given how blurry the images are,
given how noisy the vector is, right?
All right. Um, sorry,
I gave the answer away. [LAUGHTER] Okay.
So uh, try a smaller set of features, uh,
that fixes a high variance problem.
Right? Uh, and one concrete example would be, um,
if you have this dataset and you're fitting a, you know,
10th order polynomial and the curve oscillates all over the place, that's high variance.
You can say, well,
maybe I don't need a 10th order polynomial,
maybe I should use, you know,
only- Wow, I don't know where my- I'm sorry. I don't know what's going on?
[NOISE] Okay. All right.
So maybe you say maybe I don't need my features to be
all of these things, 10th order polynomial,
maybe if this is too high variance,
I'm going to get rid of a lot of features and just use,
you know, a much smaller number of features.
Right? So that fixes,
um, uh, high variance.
Um, and then if you use a larger set of features [NOISE] [inaudible] , right?
Cool. So that's if you're fitting
a straight line to the data and it's not doing that well,
you can go, "Gee, maybe I should add a quadratic term," just add more features, right?
So that fixes variance.
And adding e-mail header features.
[BACKGROUND] Cool.
Yeah. Generally, I would try this if- um,
ah, to try to reduce bias.
And so in the workflow of, um,
how you develop a learning algorithm, ah,
I would recommend that, um, you,
ah- so, so one of the things about,
um, building learning algorithms, is that,
for a new application problem, uh,
it's difficult to know in advance, uh,
if you're gonna run into a high bias or high variance problem, right?
It, it is actually very difficult to know in
advance what's gonna go wrong with your learning algorithm.
And so the advice I tend to give is, uh,
if you're working on a new application,
uh, implement a quick and dirty learning algorithm.
It, it will have like a quick and dirty implementation of something.
So you can run your learning algorithm,
uh, just say- start with logistic regression, right?
Let's start with something simple.
Um, and then run this bias-variance type of analysis, uh,
to see, sort of,
what went wrong and then use that to decide what to do next.
You go to a more complex algorithm,
do you try adding more data?
Um, the, the one exception to this is if you're
working on a domain in which you have a lot of experience, right?
Uh, and, and so for example,
you know, I've done a lot of work on speech recognition.
So because I've done that work,
I kinda have a sense of how much data is needed for the application,
then, then I might just build something more complicated from the get go.
Or, or if you're doing- or if you're working on, say,
face recognition and because you've read a lot of research papers,
you have a sense of how much data is needed.
Then maybe it's worth trying something because you're building on a body of knowledge.
Uh, but, but if you're working on something,
on a brand new application that you and maybe,
you know, no one in the published academic literature has worked on or,
or you don't totally
trust the published results to be representative of your problem,
then I will usually recommend that, um,
you implement a- build a quick and dirty implementation,
look at the bias and variance of the algorithm, uh,
and then use that to better decide what to try next.
Right? Um, so I think,
uh, bias and variance is, uh,
I think, is actua- is really like the single most powerful tool I know,
you know, for analyzing the performance of learning algorithms.
And I do this pretty much in every single machine learning application.
Um, there's one other pattern that I see quite often,
which is, um, uh- which,
which addresses the second set, which is, um,
uh, which is a- which is the optimization algorithm, ah, working.
So, so let me, let me explain this with,
um, a motivating example, right?
So, um, it turns out that when you implement a learning algorithm,
uh, you often have a few guesses for what's wrong.
And if you can systematically test if
that hypothesis is right before you spend a lot of work to try to fix it,
then you could be much more efficient.
So, uh, let's explain that with a concrete example.
So, so you understand those words I just said,
maybe they're a little bit abstract, which is,
um, let's say that, you know,
you tuned your logistic regression algorithm for a while.
And lets say logistic regression gets 2%t error on
spam e-mail and a 2% error on non-spam, right?
And it's okay to have 2% error on spam e-mail, maybe, right?
You know, so you, you have to read a little bit of spam e-mail.
It's like, that's okay.
Uh, but 2% error on non-spam is just not really
acceptable because you're losing 1 in 50 important e-mails.
Um, and let's say that,
uh, you know, your teammate, right,
also try- trains an SVM and they find
in SVM using a linear kernel gets 10% error on spam,
uh, but 0.01% error on non-spam.
All right. And maybe not great,
but for this- for purposes of illustration,
let's say this is acceptable.
Um, but because it turns out logistic regression is more computationally efficient and,
and it may be easier to update, right?
And you get more examples,
run a few more iterations of gradient descent.
Uh, and let's say you want to ship
a logistic regression implementation rather than SVM implementation.
Um, so what do you do next?
It turns out that, um,
one common question you have when training your learning algorithm is,
you often wonder, uh,
is your, um, optimization algorithm converging?
Right? So you know, it's,
it's gradient ascent, is it converging?
And so one thing you might do is, uh,
draw a plot of the training optimization objective,
of J of Theta,
whatever you are maximizing or log likelihood of J of Theta or whatever,
versus the number of iterations.
And, um, often the plot will look like that, right?
And, you know, the curve is,
kind of, going up,
but not that fast.
And if you train it twice as long or even 10 times as long, will that help?
Right? And again, training,
training the algorithm for more iterations,
it, you know, pretty much never hurts.
If, if you regularize the algorithm properly,
training the algorithm longer, you know,
almo- almost always helps, right?
Pretty much never hurts, uh,
but it's the right thing to do to go and burn another 48 hours of,
you know, CPU or GPU cycles to just train this thing longer and hoping it works better.
Right? Maybe. Maybe not.
Um, so is there a,
is there a systematic way to tell- is there a better way, uh,
to tell if you should invest a lot more time,
um, in running the optimization algorithm?
Sometimes it's just hard to tell, right?
So, um, now, the other question that you sometimes wonder- so,
so a lot of- um,
where a lot of this iteration of
debugging learning algorithms is looking at what your learning algorithm is
doing and just asking yourself what are my guesses for what could be wrong.
Uh, and maybe one of your guesses is, well,
maybe optimizing the wrong cost function.
Right? So, so here is what I mean.
Um, what you care about is this, um,
weighted accuracy criteria, uh,
you know, where, uh,
sum over your dev set or test set of, you know,
weights on different examples of whether it gets it right,
uh, where the weights are higher for non-spammed and spam.
Because you really make sure you label non-spam e-mail correctly, right?
So, so maybe that's the weighted accuracy criteria you care about.
Uh, but for logistic regression,
uh, you are maximizing this cost function, right?
Law of likelihood minus this regularization term.
So you're optimizing J of Theta,
when what you actually care about is A of Theta.
So maybe you're optimizing the wrong cost function.
And then one way to change the cost function
would be to fiddle with the parameter Lambda, right?
That's one way to change the definition of J of Theta.
Um, another way to change J of Theta is to
just totally change the cost function you are maximizing,
like change it to the SVM objective, right?
Or, or- and then part of that also means choosing the appropriate value for C. Okay?
And so, um, there is a second diagnostic which, um,
I end up using i- th- th- which is - which I hope you can tell,
is the problem your optimization algorithm?
Uh, in other words is gradient ascent not converging?
Or is the problem that you're just optimizing the wrong function?
Right? And, and we'll see two examples of this thing.
So this is the first example.
Okay? Um, and so here's the diagnostic that can help you figure that out.
So just to summarize this scenario- this, um, this, uh,
example - this running example we're using,
um, the SVM outperforms logistic regression.
If you want to deploy logistic regression.
Uh, let's say that theta SVM for the parameters learned by SVM.
And, and instead of writing the SVM parameters as w and b,
I'm just gonna write the linear SVM.
SVM linear kernel.
You know, using the logistic regression parameterization.
Right? So if you have a linear set of parameters.
Um, and let's say that theta BLR will be the parameters learned by logistic regression.
Right? So I'll, I'll just- yeah,
regularized logistic regression or Bayesian logistic regression.
So you care about weighted accuracy and, uh, uh, um,
uh, and the, the SVM outperforms Bayesian logistic regression.
Okay? So this is one- a one-slide summary of where we are in this example.
So how can you tell if the problem is your optimization algorithm,
uh, meaning that you need to run gradient ascent longer to actually maximize J of Theta.
Um, or this- oh, sorry. And then- right.
And this is the- what BLR tries to maximize.
Right? So, so how do you tell, we have,
we've two possible hypotheses you wanna distinguish between.
One is that, um,
the learning algorithm is not actually finding
the value of Theta that maximizes J of Theta. All right?
For some reason gradient ascent is not converging.
So that would be a problem with the optimization algorithm.
That j of Theta that, that,
that, you know, uh, for,
for the property of the- for
the problem to be with the optimization algorithm it means that,
if only we could have an algorithm that maximizes j of Theta we would do great.
But for some reason gradient ascent isn't doing well.
That's one hypothesis.
The second hypothesis is that J of Theta is just the wrong function to be optimizing.
It is just a bad choice of cost function,
that j of Theta is too different from A of Theta,
that maximizing J of theta doesn't give you,
you know a classifier that does well on A of theta which is what you actually care about.
Okay? Any que- so this is a problem setup.
Is there any, any que- I wanna make sure people understand this.
This is- raise, raise your hand if this makes sense.
Most people? Okay. Cool. Almost everyone, okay.
Good. Any questions about this problem setup?
Why don't you, why don't you [inaudible].
Oh. Uh, thank you. Why not maximize A of Theta directly?
Because A of Theta is non-differentiable.
So we don't actually have,
um, you know there's this indicator function.
So it's- we actually don't- we, uh,
- it turns out maximizing A of Theta explicitly is NP-hard.
Uh, uh, but just- we just don't have great algorithms to try and do, do that.
Okay. So it turns out there's a diagnostic you could
use to distinguish between these of two- these two different problems.
Um, and here's the diagnostic.
Which is, check the cost function that logistic regression is trying to maximize.
So J. And compute that cost function on the parameters found by
the SVM and compute that cost function
on the parameters found by Bayesian logistic regression.
And just see which, which value is higher.
Okay? Um, so there are two cases.
Either, this is greater,
or this is less than or equal to.
Right? They're just two possible cases.
So what I'm gonna do is go over case one and case two
corresponding to this greater than or is less than equal than.
Uh, and let's, let's see what that implies.
So on the next slide, I'm gonna copy over this equation.
Right? That's, that's just a fact that the SVM does
better than Bayesian logistic regression on a problem.
So on the next I'm gonna copy over this first equation.
Um, and then we're gonna consider,
you know, these two cases separately.
So greater than will be case one and less than or equal to will be case two.
Okay? So let me copy over these two equations in the next slide.
Right? So that's the first equation that I just copied over here.
And that's- this is the greater than, this is case one.
Okay? So let's see how to interpret this.
Um, in case one,
J of theta SVM is greater than J of Theta BLR.
Right? Meaning that whatever the SVM was doing, um,
it found a value for Theta which we have written as, Theta SVM.
And theta SVM has a higher value on the cost function J than theta BLR.
But Bayesian logistic regression was trying to maximize J of theta.
Right? I mean Bayesian logistic regression is just using
gradient ascent to try to maximize J of theta.
And so under case one,
this shows that whatever the SVM was doing,
whatever your buddy implementing SVM did.
They managed to find a value for Theta
that actually achieves a higher value of J of Theta,
than your implementation of Bayesian logistic regression.
So this means that Theta BLR fails to maximize the cost function J.
And, uh, and the problem is with the optimization algorithm.
Okay? So this is case one.
Case two, um- again I'm just copying over the first equation.
Right? Because this is just part of our analysis.
This is part of the problem set up.
Uh, then case two is now the second line.
It's now a less than or equal sign.
Okay? So let's see how to interpret this.
Um, so under- if you look at the second equation right?
The less than equal to sign.
It looks like J did a better job than the SVM maximizing J- excuse me.
It looks like Bayesian logistic regression did a better job than the SVM,
um, maximizing J of Theta.
Right? So, you know,
you tell Bayesian logistic regression to maximize J of Theta.
And by golly, it found the- it found the value of Theta.
That's that- it found a value that achieves a higher value of J of Theta than,
than whatever your buddy did using an SVM implementation.
So it actually did a good job
trying to find a value of Theta that drives up J of Theta as much as possible.
But if you look at these two equations in combination what we have is that,
um, the SVM does worse on the cost function J.
But it does better on the thing you actually care about.
A of Theta.
So what these two equations in combination tell you is that having the best value-
the highest value for J of Theta does not
correspond to having the best possible value for A of Theta.
So it tells you that maximizing J of
Theta doesn't mean you're doing a good job on A of Theta.
And therefore, maybe J of Theta is not such a good thing to be maximizing.
Because maximizing it, doesn't actually give you the result you ultimately care about.
So under case two, um,
you can be convinced that j of Theta is
just a- i- i- is not the best function to be maximizing.
Because getting a high value of J of
theta doesn't get you a high value for what you actually care about.
And so the problem is with the objective function of the maximization problem.
And maybe we should just find a different function to maximize.
Okay? So, um,
any questions about this? Right, go ahead.
If you want to change the cost function in case two,
you saw it was the right one. [inaudible]
Yeah. Uh, let me come back to that.
Yeah. It's a g- a complicated answer.
Yeah. All right. Actually, let,
let- let's do this first.
Um, so, uh, all right.
For these four bullets,
does it fix the optimization algorithm or does it fix the optimization objective?
First one. Does it fix
the optimization algorithm or does it fix the optimization objective?
Cool. Second one.
Ah, I don't know what's wrong with this thing.
This is so strange. Okay. All right.
Does it fix the optimization algorithm or fix
the optimization objective? Optimization algorithm, right?
So Newton's method still looks at the same cost function J
of Theta but in some cases it just optimizes it much more efficiently.
Um, this is a funny one.
Usually, you fiddle with lambda, um, to,
uh, uh, trade off bias and variance things.
Right. That, that this is one way to change the optimization objective.
Although uh, uh, uh, usually you change
lambda to just bias and variance rather than this.
Right? Uh, and then trying to use an SVM, right?
Would be one way to totally change the optimization objective.
Okay? So, uh, to,
to address the question just now.
Sometimes we find you have the wrong optimization objective,
is that there, there isn't always an obvious thing to do.
Right? Sometimes you have to,
uh, brainstorm a few ideas.
Is that there, there isn't,
uh, um, always one obvious thing to try.
But at least it tells you that,
that category of things of trying out different optimization objectives is what you want.
Right? Um, all right.
So, um, let's go through a more complex example.
They're, they're, you know,
incorporate some of these- wow, I don't know what's wrong.
I sprayed my laptop.
I wonder if my- this is so strange.
Let me see what I can do. Yeah. All right.
Well. Okay. Let's go for a more complex example, uh, that, that,
that will illustrate some of these concepts, uh, that,
that we've been going through and,
and just let you see another example of these things. Um, uh, oh,
and- and I find that,
um, one- one thing I've learned as a teacher,
you know, one of the ways for you to become good at this, right?
Is to go, you know,
work in a good AI group for five years, right?
Because when you work in a good AI group for some several years,
then you have seen, you know, 10 projects,
and that lets you gain that experience.
But it turns out that it takes,
I don't know, depending on what AI group you work on,
it- it takes- if you work on a different project every year,
then in five years that I guess you work on five projects or something.
I- I actually don't know. Or maybe 10 projects or something.
But, er, one of the reasons that,
um, in, uh, the way I try to explain this,
I'm try to go- give specific scenarios with you so that, um, you know,
my Ph.D students and I, we spent- actually,
we spent like many years working with Stanford Autonomous Helicopter,
but I'm trying to distill the key lessons down for
you so that you don't need to work on a project for, you know,
few years to gain this experience but to give you
some approximation to this knowledge in maybe 20 minutes, right?
The 20 minutes won't give you the depth of
three years of experience but we try to summarize
the key lessons so that we can learn
from experience that others took years to develop.
Um, all right. So, uh,
this helicopter actually sits in my office.
Uh, uh, uh but if you go to my office,
uh, uh, and, you know, grab this helicopter,
uh, uh, and- and- and we ask you to write a piece of code to make this fly by itself,
use the learning algorithm to make this fly by itself.
How do you go about doing so?
So it turns out a good way to, um,
make a helicopter fly by itself is to use,
uh, is to do the following.
Uh, step one is build a,
uh, computer simulator for a helicopter.
So, you know, that's actually a simulator, right?
Like a video game simulator of a helicopter.
Um, the advantage of using, you know,
say a video game simulator of a helicopter,
is you could try a lot of things,
crash a lot in simulation,
you know, which is cheap, whereas crashing a helicopter in real life is- is- is-
is slightly dangerous and- and- and also, uh, more expensive.
Um, uh, but so step one build a simulator of a helicopter.
Step two, uh, choose a cost function.
And for today, I'm just using a relatively simple cost function which is squared error.
So you want the helicopter to fly the position x desired,
and your helicopter is there,
you know, wandered off to some other place x.
So let's use a squared error to penalize it, right?
Um, when we talk about reinforcement learning towards the end of this quarter,
we'll- we'll actually go through the same example again by using, uh,
the reinforcement learning terminology,
understand this slightly- this at a slightly deeper level.
And we'll go over this exact same example,
after you learn about reinforcement learning.
But we'll just go over a slightly simplified- very slightly simplified version today.
Um, and so, uh,
run a reinforcement learning algorithm and what the reinforcement learning algorithm does,
is it tries to minimize that cost function J of Theta.
Um, and so, uh, you know,
and so you learn some set of parameters Theta sub through
RL for controlling the helicopter, right?
And we'll talk about reinforcement learning, you know,
the- the- we'll- you- you'll see all this redone with
proper reinforcement learning notation where J is a reward function,
Theta Rs is the control policy and so on.
But don't worry about that for now.
Um, so let's say you do this,
and the resulting controller, right?
The way you fly the helicopter,
it gets much worse performance than a human pilot, you know,
so the helicopter wobbles all over the place
and doesn't quite stay where you are hoping it will.
So what do you do next, right?
Well, here are some options, uh,
corresponding to the three steps above.
You could work on improving your simulator.
Um, it turns out even today,
you know, we- we- we've had helicopters for what?
I don't know- like, uh, uh, I think, uh,
we started having a lot of commercial helicopters around the 1950s.
You see we have been co- conc- helicopter for many decades now.
But airflow around the helicopter is very complicated.
And even today, there are actually some, uh, uh,
details of how air flows around the helicopter.
The- the aerodynamics textbook, you know,
that- that even, um, AeroAstro people, right?
The experts in AeroAstro cannot fully explain.
So helicopters are incredibly complicated.
And there's almost unlimited headroom,
uh, for building better and more accurate simulations of helicopters.
So maybe you wanna do that or maybe you think that cost function is messed up,
you know, maybe a squared error isn't the best metric, right?
Uh, and- and it turns out, um,
the way helicopter- a helicopter has a tail rotor that blows wind to one side, right?
So I guess, uh,
because the- the- the main rotor spins in one direction,
if it only had a main rotor,
then the body will spin in the opposite direction.
Er, an equal and opposite reaction within torque, right?
So the main rotor spins in one direction.
If it only had a main rotor,
the rotor on top,
and it just spun that, then the body of the helicopter would spin the opposite direction.
So that's why you need a tail rotor to blow air down off to one side,
to not make it, um, uh,
uh, spin in the opposite direction.
Uh, but because of that, it turns out
the helicopter's staying in place, it's actually tilted slightly to a side.
Because a tail rotor blows air in one direction.
So it's pushing you off to one side,
so you have to tilt your helicopter in the opposite direction.
So- so the main rotor blows air to one side,
the tail rotor blows air to the other side.
So you actually stay in place, right?
So a helicopter is actually asymmetric.
Lift in birds is not the same.
So- so- so because of this comp- complication,
maybe squared error isn't the best, um, uh,
uh, error because, you know,
your- your orientation- your optimal orientation is actually not zero, right?
Um, so- so- so maybe you should modify the cost function.
Um, or maybe you wanna modify the, um,
reinforcement learning algorithm because you secretly
suspect that your algorithm is not doing
a great job of minimizing that cost function, right?
That it's not actually finding the value of Theta that absolutely minimizes J of Theta.
So it turns out that, um, uh,
each one of these topics can easily be a PhD thesis, right?
You can definitely work for six years on any one of these topics.
Um, and the problem is, uh, uh,
you know, so I- I actually- I actually know someone that wrote their PhD thesis is on, right?
Uh, improving helicopter simulator, right?
Um, uh, but the problem is maybe a helicopter simulator is good enough.
You can spend six years improving
your helicopter simulator but will that actually get you the result?
And you can write- and you can write a PhD thesis,
and you get a PhD doing that maybe.
But if your goal is not
just to write a PhD thesis, it's actually to make a helicopter fly better.
It's actually not- not totally clear, right?
If- if that's the key thing for you to spend time on.
Um, so what I'd like to do is, uh,
describe to you a set of diagnostics that allows you to use this sort of
logical step-by-step reasoning to debug
which of these three things is what you should actually be spending time on, right?
Um, so is it possible for us to come up with a debugging process to logically reason, uh,
so as to select one of these things to work on and- and have conviction,
and then be relatively confident that this is a useful thing to work on, right?
Um, so here's how we're gonna do it.
Um, so just to summarize a scenario, right?
Um, the controller given by Theta RL performs poorly, right?
So, uh, this is how I would reason through a learning algorithm, right?
So suppose, uh, suppose all of these things were true,
um, suppose that- okay,
corresponding to the three steps in the previous slide,
suppose the helicopter simulator was accurate and suppose,
um, uh, you know, the learning algorithm,
uh, correctly, you know,
minimizes the cost function and suppose J of Theta is a good cost function, right?
If- if all of these things were true,
then the learned parameters should fly well on the actual helicopter, right?
Um, but it doesn't fly well on a helicopter,
so one of these three things is false.
And our job is to figure out,
is- is to identify at least one of these three statements: one,
two or three that's false because that- that- that lets you
sink your teeth into something that to- to- to work on, right?
Um, and I think, uh, uh, um,
to make an analogy to more conventional software debugging,
if a big complicated program,
and for some reason,
your program crashes, you're like the core down to whatever, um,
if you can isolate this big complicated program into one component that crashes,
then you can focus your attention on that component that you
know crashes for some reason and try to find the bug there, right?
And so instead of trying to look over a huge code base,
if you could do binary search or try to isolate
the problem in a smaller part of your code base,
then you can focus your debugging efforts on that part of your code base,
try to figure why it crashes,
and then fix that first.
And after you fix that, it might still crash,
then there may be a second problem to work on but at least you know that, um,
trying to fix the first bug seems like, uh,
seems with a worthwhile thing to do, okay?
So what we're gonna do is, um, uh,
come up with a oh, sorry, that's gradient descent, come up with a set of diagnostics
to isolate the problem to one of these three components, okay?
So the first step is,
uh, let's look at,
um, how well the algorithm flies in simulation, right?
So what I said just now was, uh,
you ran the algorithm and it resulted in
a set of parameters that doesn't do well on your actual helicopter.
So the first thing I will do is just check how well
does this thing even do in simulation, right?
And, uh, uh, there are two possible cases.
Um, if it flies well in simulation but doesn't do well in real life,
then it means something's wrong with the simulator, right?
It- it means it's actually work- working on the simulator because, you know,
if it's already working well in the simulator,
I mean what else could you expect to
learn the reinforcement learning algorithms to do, right?
You know, you told the reinforcement learning algorithm to go and fly
well in the simulator because this is just training simulation.
It's already doing well in the simulator,
so there's not much to improve on there, right?
At least, it's hard to improve on that.
Uh, but- but- but if- if- if you found a learning
algori- if your learning algorithm does well in the simulator but not in real life,
then this means that the simulator,
um, isn't matching real life well.
And so dish- that- that's strong evidence.
That's strong grounds for you to spend some time to improve your simulator. Yeah?
[inaudible].
Oh, yeah. Uh, right.
So to just repeat for
the camera, is it ever the case that it flies bad in the simulator but well in real life?
I wish that happened.
[LAUGHTER] You know, I actually, um, very rarely,
I- I think, uh,
if that happens I will,
I will still work on improving the simulator.
Um, uh, so there,
there is one scenario where that happens,
it turns out that, uh, uh,
when we train this helicopter in the simulator or really, any robot in the simulator,
we often add a lot of noise to he simulator because one lesson we've learned is
that if your simulators is noisy, because simulators are always wrong, right?
Any- any digital simulation is only an approximation in the real world.
So it turns out we have a lot of noise in all of our simulators,
because we think if that the learning algorithm is
robust to all this noise you've thrown at it in simulation.
Then, whatever the noise the real world throws at it,
it has a bigger chance of being robust too, as well.
Um, uh, and so we tend to throw a lot of noise into, into simulators.
And so one case where that does happen is when we find we threw too much noise
added in simulation and tha- that might be a sign we should dial back the noise a bit.
Um, right, cool.
Uh, so, um, yeah, right.
So this first diagnostic tells you should work on improving the simulation.
But just, I think there's a big mismatch between
simulation performance and real world performance.
That's a good sign that,
you know, that you should improve the simulation.
Second, um, this is actually very similar to the diagnostic we use on
the Spam, you know, Bayesian logistic regression and SVM example.
So what we're gonna do is, um,
we're going to measure this equation.
And this is, this again, this is
very similar to our previous equation which is,
take the cost function, similar as the previous example.
Take the cost function J that reinforcement learning is,
uh, totally minimized, right?
That's J and J of theta was a squared error, right?
So take the cost function that, uh, uh,
reinforcement learning was told to minimize and see if
the human achieves better squared error than the reinforcement learning algorithm.
We just see, you know supervise better.
So let's measure the human performance on this squared error cost function um,
and see which one does better.
So there are two cases that equation will be either less than
or it will be greater than or equal to, right, so less, or greater or equal to.
So case one, is um,
say to human is less than
excuse me, J of theta human is less than J of theta RL. That would be this case.
Then, that tells you that the problem
is with the reinforcement learning algorithm, right?
That somehow the human achieves a lower squared error uh, and so, uh,
the learning algorithm is not finding the best possible squared error, that is
some other controller as evidenced by whatever the human is
doing that actually achieves a lower cost function, right?
So in this case,
um, we think the learning algorithm or,
or reinforcement learning algorithm is not doing a good job
minimizing that and we'll work on the reinforcement learning algorithm.
The other case would be if the sign of the inequality is the other way around.
Right? Now in this case,
um, you can infer that the problem is in the cost function.
Because what happens here is,
um, the human is flying better than the reinforcement learning algorithm.
But the human is achieving what looks like a worse cost than
the reinforcement learning algorithm.
So what this tells you is that minimizing J of theta does not correspond to flying well.
Right? Your learning algorithm achieves a better value for J of theta,
you know, J of theta RL is actually smaller than what the human is doing.
So the reinforcement learning algorithm as far as it knows is doing
a great job cause it's finding a value of theta where J of theta is really really small.
But in this last case, um,
you know that finding such a small value of J of theta doesn't correspond to flying
well because a human doesn't achieve such a good value in the cost function but
the helicopter actually just looks better, was flying in a more satisfactory way.
And that tells you that this squared error cost function
is not the right cost function for,
for, for what flying accurately remains, right?
And so um, through this set of diagnostics,
um, uh, you could decide which one of these three things.
Uh, improving the simulator,
improving the RL algorithm,
reinforcement learning algorithm or improving
the cost function is the thing you should work on.
And what happens in- in
this particular project and what often happens in machine learning applications is,
you run this set of diagnostics and this
actually happened when we were working on this helicopter.
We ran this set of diagnostics and then one week we were saying,
"Yep simulator's got a problem, let's work on that."
And then we'd improve the simulator,
improve the simulator and after
a couple of weeks of work we will run these diagnostics and say,
"Oh, looks like the simulator is not good enough."
And maybe there's a problem with the RL algorithm,
then we'll work on that, work on that and improve that.
And after that, after awhile we'll say, "Oh, they'll say that's
also good enough and the problem is in the cost function."
And sometimes the, the location of
the most acute problems shifts right after you've cleared out one set of problems.
It might be the case that now the bottleneck is the simulator, right?
And so, um, I often use this, uh,
workflow to constantly drive prioritization for what to work on next, right?
And, and to answer your question just now about how do you find the new cost function?
It turns out finding a new cost function is actually not that easy.
Uh, so actually one, one of my former PhD students Adam Coates um,
through this type of process realized that
finding a good cost function is actually really difficult.
Uh, because if you want a helicopter to fly and maneuver,
you know, like fly at speed and then make a bank turn, right?
Like how do you mathematically define what is an accurate bank turn?
It's actually really difficult to write down an equation to
specify what is a good way of, I will fly in that and do a turn.
Or is this, how do you specify what is a good turn?
So um, he wound up writing a research paper,
uh, one of the best application paper, it won at ICML.
Uh-uh on, on how to define a good cost function,
it's actually pretty complicated,
but the reason he did it and it was a good use of his time was running
diagnostics like these which gave us
confidence that this was actually a worthwhile problem uh,
and the, that resulted in,
you know making real progress in optimization, right.
Um, any questions about this? All right, cool.
Actually, I think I- all right, anyway,
all right, fun helicopter videos, I always want to show this, but it's fine.
And you guys saw this earlier. All right, so,
um, only one time,
all right, let's go through this.
So, um, uh, in addition to,
um, these specific diagnoses of bias
versus variance and optimization algorithms versus optimization objective.
Um, oh sorry- and when we do RL,
I wanted to just go through that example one more time,
so you see everything you just saw again,
after you learned about reinforcement learning, they tend to squeeze up.
Okay. Now, in addition to these type of diagnostics,
um, uh, how to debug learning algorithms, um,
there's one other set of tools you'll find very useful,
which is, uh, error analysis tools, uh,
which lets you figure out,
which is another way for you to figure out what's working,
what's not working, or really what's not working in the learning algorithm.
[NOISE] So let's let's go through a motivating example.
Um, so let's say you're building a,
um, uh, you know, uh, like a security system,
so when someone walks in front of a door,
you unlock the door knob based on whether or not, you know,
that person is authorized to enter right that, that place.
Um, and so let's say that, uh, uh,
so there are a lot of machine learning applications where
it's not just one learning algorithm, right?
But instead you have a pipeline,
you string together many different steps.
So how do you actually build a face recognition algorithm?
To decide if someone approaching your front door
is authorized to unlock the door, all right.
Well, here's something you could do which is, uh,
you start with a camera image like this, and then,
um, you could do preprocessing to remove the background.
So all that co- co- complicated color in the background, let's get rid of that.
And it turns out that, um,
when you have a camera against a static background, right?
You could actually do this, you know,
with a little bit of noise relatively easily
because if you have a fixed camera that's just like mounted,
you know, on your door frame,
it always sees the same background,
and so you can just look at what pixels have changed and- and just
keep the pixels that have changed compared to- I mean re- because,
you know, this camera always sees that gray background and that, um,
brown bench in the back,
and so you just look at what pixels have changed a lot and,
and this background doesn't really move, right.
So this is- this- this is- this is actually feasible by
just looking at what pixels have changed and
keeping pixels that have changed relative to that.
Um, and so, after getting to the background,
you could run the face detection algorithm, uh,
and then, uh, after detecting the face, it turns out that,
uh, actually, you know,
I've actually worked with a bunch of face detection,
worked with a bunch of face- face recognition systems.
It turns out that, um,
for some of the leading face recognition systems,
so- depends on details, but some of them.
Uh, it turns out that, um,
the appearance of the eyes is a very important cue for recognizing people,
that's why, if you cover your eyes you actually have a much harder time recognizing people,
as eyes are very distinct through people.
Just segment out the eyes,
um, segment out the nose,
and the other thing you- segment out the mouth.
[LAUGHTER] It's Halloween.
[LAUGHTER] All right.
And then- and then feed these features into some other algorithms,
say logistic regression, that then, you know,
finally outputs a label that says,
is this the person, right?
That- that- that, you know- you know,
you're authorized to open the door for.
Um, so it- so in many learning algorithms,
you have a complicated pipeline like this of different components that,
that have to be strung together,
and, uh, you know,
if you read the newspaper articles about-
or if you read research papers in machine learning, often,
uh, uh, the, the research papers will say, oh,
we built a machine translation system,
we've trained a gazillion, you know,
of sentences found on the Internet and that's great and a pure end-to-end system,
so that's like one learning algorithm that sucks in an input,
by sucking an English sentence and spit out the French sentence or something, right?
So that's, that's like one learning algorithm.
It turns out that for a lot of practical applications,
if you don't have a gazillion examples, uh,
you end up designing much more complex machine learning pipelines like this,
where it's not just one monolithic learning algorithm,
but instead there are many different smaller components.
Um, and I think in,
in- uh, uh, I think that, you know,
the, the, the, the, um,
I think that, uh,
having a lot of data's great, all right?
I love having more data,
but big data has also been a little bit over-hyped,
uh, and to model things you could do with small data sets as well.
And in the teams [NOISE] I've worked with,
we find that if, if,
if you have a relatively small dataset,
often you can still get great results.
You know, my teams often get great results at 100 images,
100 training examples or something.
But when you have small data,
it often takes more, uh,
insightful design of machine learning pipelines like this, right?
Um, now, [NOISE] when you have a machine learning pipeline like this, uh,
the things you want to do- what you want to do is,
uh, so, so you build a pipeline like this and it doesn't work, right?
And there's this common workflow.
You build a pipe, you build something,
it doesn't work, so you want to debug it.
So in order to decide which part of the pipeline to work on, um,
it's very useful if you can look at your- the error of your system and try to attribute
the error to the different components so that you
can decide which component to work on next, right?
And, and, there's actually a- I'll tell you a true story, you know,
remember preprocess background removal step, right?
Since you're getting rid of the background,
um, it turns out that, uh,
there are a lot of details of how to do background removal,
uh, for example, um,
the simple way to do it is to look at every pixel and
just see which pixels have changed, uh,
but it turns out that if there's a tree in the background that, you know,
waves a little bit because the wind moves
the tree and blows the leaves and branches around a little bit,
then sometimes the background pixels do change a little bit.
And so they're actually really complicated background removal algorithms,
they try to model basically
the trees and the bushes moving around a little bit in the background,
so you know, that even though the pixels of the tree
moves around is part of the background, you just get rid of it.
So background removal, there's simple versions where you just look
at each pixel and see how much it's changed and there's incredibly complicated versions.
Um, so I actually know someone, uh, that, uh,
uh, was trying to work on a problem like this and
they decided to improve their background removal algorithm.
Uh, and they actually, er,
this real person actually literally wrote a PhD thesis on background removal.
Uh, and so I'm glad he got a PhD,
but it turn- but,
you know, when I look at the problem he was actually trying to solve,
I don't think it actually moved the needle, right?
So- so, um.
Uh, this is one of the nice things about academia,
right, guys, so long as, you know,
you can- you can still publish a paper.
[LAUGHTER]
And- and- and that was technically innovative.
It was actually a very good technical work.
But- but- but- but if- so if your goal is to publish a paper, great, do that, uh,
but then if your goal is to build a better face recognition system,
then I would carefully ask which components should
you actually spend your time to work on, all right?
Um, so here's what you can do with error analysis,
which is, say your overall system has 85% accuracy.
Here's what I would do. I would go in and in your,
uh, dev set, in your development set,
the whole of the cross validation set, right,
uh, go in and for every one of your examples in the dev set,
I would plug into the ground truth for the background.
Meaning that, uh, rather than using a-some, you know,
approximate heuristic algorithm for
roughly cleaning out the background which may or may not work out well,
I would just use Photoshop.
And for every example in the dev set,
I would give it the perfect background removal, right?
So imagine if instead of some noisy algorithm trying to remove the background,
this step of the algorithm was- just had perfect performance, right?
And then you can give it perfect performance on your dev set, on your test set,
just by using Photoshop to just tell it this is a background,
this is a foreground, right?
And let's say that when you plug in this perfect background removal,
the accuracy improves to 85.1%.
And then you can keep on going from left to right in this pi- pipeline which is, um, now,
instead of using some learning algorithm to do face detection,
let's just go in and for the test set, you know,
modify, kind of have the face detection algorithm cheat, right?
Have it just memorize the right location for
the face in the test set and just give it a perfect result in the test set.
So when- when I shaded these things, um,
that means I'm giving it the perfect result, right?
Uh, so let's just go in and on the test set
give it the perfect face detection for every single example,
an- and then look at the final output and see
how that changes the accuracy of the final output, right?
And then, same for these components, um,
eyes segmentation, nose segmentation, mouth segmentation.
Uh, and then- and you do these one at a time.
And then finally for logistic regression,
if you give it the perfect output,
your- your- your- your accuracy should be 100%, right?
Uh, so now, what you can do is look at the sequence of,
um, uh, of steps and see which one gave you the biggest gain.
And it looks like, um,
in this example, it looks like, um,
when you gave it perfect face detection,
the accuracy improved from 85.1 to 91%.
So, you know, roughly a 6% improvement.
And that tells you that,
if only you can improve your face detection algorithm maybe
your overall system could get better by as much as 6%.
So this gives you faith that, you know,
maybe it's worth improving on your face detection component.
And in contrast, this tells you that even if you had perfect background removal,
it's only 0.1% better so maybe don't- don't- don't spend too much time on that.
Um, and it looks like that, uh,
when you gave it perfect eye segmentation,
it went up another 4%.
So maybe that's another good project to prioritize, right?
Um, and if you're in a team,
one common structure would be to do the separate analysis,
and then we have some people work on face detection,
some people work on eyes segmentation.
You could usually do a few things in parallel if you have a large engineering team.
But at least this should give you a sense of
the relative privatization of the different things. Question?
[inaudible]
Yeah, right. So if you just cumulatively,
uh, such as give it perfect eye segmentation,
then add on top of it nose segmentation,
or do you give it perfect eye segmentation and then take that away,
and then give it perfect nose segmentation.
Um, the way I presented it here is done cumulatively.
Uh, um, and- and it turns out that, uh, let's see.
If you give it- once you give it a perfect face,
uh, uh, uh, once you give it, you know,
perfect things in the later stages,
maybe the- the earlier stages doesn't matter that much anymore.
So that's one pattern. It turns out that,
uh, uh, you could do it either way, right?
For the uh, eyes-nose-mouth,
you can do it cumulatively or one at a time and
you'll probably get relatively similar results.
Um, uh, no guarantee,
you might get different results in terms of conclusions.
But, uh, but I think,
to the extent that you are wondering if doing it
cumulatively versus non-cumulatively might give you different results,
I will just do it both ways.
And then- an- and then- and- and I think this, um,
error analysis is not a
hard mathematical rule, if- if that makes sense.
It is not that you do this and then there's a formula that tells you,
okay, work on, uh,
uh, face detection, right?
I think that this should be,
um, married with judgments on,
you know, how- how hard do you think it is to
improve face detection versus eye segmentation, right?
But this at least gives you a sense of- of- of- it gives you a sense of prioritization.
Um, and it's worth doing this in- in- in
multiple ways if- if you think of- if- if- if you're
concerned in the discrepancy in the cumulative and non-cumulative versions, all right?
Um, so when we have a complex machine learning pipeline,
this type of error analysis helps you break down the error,
so attribute the error to different components,
which lets you focus your attention on what to work on.
So if you [inaudible]?
Oh, right. Yeah. If you do face detection
accurately and then your error drops, what does that entail?
Uh, it's not impossible for that to happen,
uh, it would be quite rare.
Uh, I would, uh, uh,
uh- so at a high-level,
what I would do is go in and try to figure out what's going on actually.
I- I wouldn't ignore that.
Uh, uh, so this is something I see.
Sometimes a team gets a- discovers
a weird phenomenon like that and usually ignore it and move on.
I wouldn't do that, I would- it's actually go.
Whenever you find one of these weird things, uh,
I wouldn't gloss over and ignore it,
I would go in and figure what's going on.
Does it make sense? It's- it is like debugging a software [NOISE].
You know, if- if you're- if you're trying to debug a piece of software,
and if- whenever you move your mouse over, you know,
some button, some random pixel color changes,
you go, huh, that's weird.
And then some people just ignore it and say,
"Oh well, the user won't see this."
[LAUGHTER] But I'll say no, let's go figure it out.
[LAUGHTER]
So what you're saying is quite rare but not impossible.
But I would- I would, uh,
I don't have an easy solution for how to figure out what's
going on but I would- I would- wanna figure out what's going on.
Um, all right.
So one last thing before we break.
So error analysis, um,
helps figure out the difference between where you are now,
85% overall system accuracy and 100%, right?
So it tries to explain difference between where you are and,
you know, perfect performance.
There's a different type of analysis called ablative analysis
which figures out the difference between where you
are and something much worse. So- so here's what I mean.
Um, er, so let's say that you built,
um, let's say you built
a good anti-spam classifier by adding
lots of clever features in logistic regression, right?
So a spelling correction because spam is trying
to misspell words to mess up the tokenizer,
uh, uh, to make word look, you know,
spammy words not look like spammy words.
Uh, sender host features.
So, what machine did the e-mail come from?
You know, header features uh,
could have a parser from NLP,
parse a text, uh,
use a JavaScript parser to understand, right?
Or even you can, uh, uh, uh,
fetch the web pages that a- that the e-mail refers to and parse that.
Um, and the question is um,
how much would these- these components really help?
And it turns out, if you're writing a research paper,
you know, sometimes you're writing a research paper and you can say, "Hey.
Look, I built a great spam classifier," and that's okay.
That's, like, a nice result to have.
But if you can explain to your reader,
either in a research paper or in a class project report like a term project,
what ac- what actually made the difference,
that conveys a lot of insights as well.
So, um, so simple logistic regression
without all these clever features got 94% performance,
uh, and with all of your- addition of all these clever features,
you got 99%, uh, uh, accuracy.
So an ablative analysis which we'll do, is, um,
we move the components one at a time to see how it breaks, right?
So just- so just now,
we were adding to the system by making
components perfect with error analysis, this is how it improves.
Here, we're gonna remove things one at a time.
I did not mean to remove that [LAUGHTER].
So let me figure out what's going to pop on. All right.
We move things one at a time to see how it breaks.
So let's see, we remove spelling correction.
And, uh, as the set of features,
the error goes away that.
Then let's remove the sender host features,
we remove email header features and so on until,
uh, when you remove all of these features you end up there.
And again, you could do this cumulatively or
remove one and put it back, remove one and put back.
Uh, uh, you know, or- or you could do it both
ways and see if they give you slightly different insights.
Uh, and so the conclusion from
this particular analysis is that the biggest gap is from the,
uh, text parser features,
because when you remove that the error or the accuracy went down by 4%.
And so, you know,
there is a strong evidence.
If you wanna publish a paper,
you can say like text parser features significantly
improves spam filter accuracy in that level of insight.
An- and then if you're working with spam filter for many years, right, you know,
there- there are- there are really important applications
where sometimes the same team will work on for many years.
So this type of error analysis gives you
intuition about what's important and what's not, uh,
and helps you decide to maybe even double down
on text parser features or maybe if, uh, um, uh,
or maybe if, uh, the sender host features is too computationally expensive to compute,
tells you maybe you can just get rid of that and without too much harm.
And also if you're a publishing paper or sending a report,
this gives much more insight to your report.
Okay? All right.
Um, so that's it for error analysis and ablative analysis.
I hope this was useful for your class projects as well.
I'll take one last question over there.
Uh, how did you chose the order of to remove the features?
Oh. Yeah. Uh, uh, how would you choose the order in which you- no systematic way.
If you didn't have a systematic way you do that, the other way,
the non-cumulative, where you remove one [NOISE] put it back, remove one put it back.
So either way it works. All right, let's break.
Um, uh, and, uh,
problem set two is- is due tonight.
A friendly reminder, and problem set three will be posted, uh,
in the next, like, several tens of minutes.
Okay. Thanks everyone.
 All right. [NOISE] Um, let's get started.
So, um, let's see, logistical reminder,
uh the class midterm, um,
is this Wednesday and it's 48-hour take-home midterm.
Um, and the logistical details you can find,
uh, at this Piazza post, okay?
So the midterm will start Wednesday evening.
You have 48 hours to do it and then submit it online through Gradescope, uh,
and because of the midterm,
there won't be a section,
uh, this Friday, okay?
Oh and the midterm will cover everything up to and including EM,
uh, which we'll spend most of today talking about, okay?
Certainly don't look so stressed. It'll be fun.
[LAUGHTER].
Maybe. All right.
Um, so what I'd like to do today is start our foray into, uh, unsupervised learning.
Uh, so far I've spent a lot of time on
supervised learning algorithms including advice on
how to apply supervised learning algorithms.
These pens are great.
In which you'd have, you know,
positive examples and negative examples and you run
logistic regression or something or SVM or
something to find the line- find the decision boundary between them.
Um, in unsupervised learning,
you're given unlabeled data.
So rather than given data with x and y,
you're given only x.
And so your training set now looks like X1,
X2, up to Xm.
And you're asked to find something interesting about the data.
Uh, so the first unsupervised learning algorithm we'll talk about
is clustering in which given a dataset like this,
hopefully, we can have an algorithm that can figure
out that this dataset has two separate clusters.
Um, and so one of the most common uses of clustering is, uh, market segmentation.
If you have a website,
you know, selling things online,
you have a huge database of many different users and run clustering
to decide what are the different market segments, right?
So there may be, you know,
people of a certain age range, of a certain gender,
people of a different age range,
different level of education,
people that live in the East Coast versus West Coast versus other parts of the country.
But by clustering you can group people into,
uh, different groups, right?
So, um, I want to show you an animation of, um,
really the most commonly used er,
er, clustering algorithm called k-means clustering.
And let me show you an animation of what k-means does and
then we'll write- write out the math an- an- and tell you how you can implement it.
So, um, let's say you're given data like this.
So all these are unlabeled examples.
Uh, so just x plotted here.
And we want an algorithm to try to find maybe the two clusters here.
Uh, the first step of k-means is to pick two points
denoted by the two crop- two crosses called cluster centroids and,
uh, the cluster centroids are your best guess for
where the centers of the two clusters you're trying to find.
And then k-means is an iterative algorithm and repeatedly you do two things.
So first thing is, go through each of your training examples.
Oh I'm sorry. Oh okay. Thank you.
All right. Let me know if that happens again.
Okay. Right. So, uh, you guys saw that, right?
So. Right near two cluster centroids.
So the first thing you do is go through each of your training examples,
the green dots and for each of them you color them either red
or blue depending on which is the closer cluster centroid.
So here we've taken every dot and colored it in red or blue
depending on which side it is- which cluster centroid it's closer to.
And then, uh, the second thing you do, uh, is,
uh, look at all the blue dots and compute the average, right?
Just find the mean of all the blue dots,
um, and move the blue cluster centroid there.
And similarly, look at all the red dots- and look at only the red dots
and find a mean- finding the- oh now what's wrong with this?
Let's say- oh this thing though is very strange.
Right. Apparently, if I keep moving my mouse,
it doesn't do that. All right. Thank you.
Uh, and then find the mean of all the red dots and move your,
uh, red cluster centroid there.
So let me do that, right?
So the cluster centroids move as follows, um,
to the mean of the red and the blue dots and this is
just a standard arithmetic average, right?
Uh, and then you repeat again where you, er,
look at each of the dots and color it either red or
blue depending on which cluster centroid is closer.
So when I recolor every point based on,
you know, what's closer,
so that's the new set of colors, right?
Um, and then the second part of the algorithm was
again look at the blue dots, find the mean,
look at the red dots, find the mean,
and then move the cluster centroids over.
[NOISE] Excuse me, uh,
to that mean, okay?
Um, and so, er,
and it turns out if you keep running the algorithm, nothing changes.
So the algorithm has converged.
So if you look at this picture and you repeatedly color each point
red or blue depending on which cluster centroid is closer, nothing changes.
And you repeatedly look at each of the two clusters of color dots
and compute a mean and move the clu- clu- clusters there, nothing changes.
So this algorithm has converged even if you keep on running these two steps, okay?
So um, let's see.
Let's write down in math what we just did.
[NOISE] All right. So this is,
um, a clustering algorithm and specifically this is a k-means clustering algorithm.
So your dataset now does not come with any labels.
Um, and so in, uh,
k-means, step one is, uh,
initialize the cluster centroids, right?
I'm gonna call them Mu_1 up to Mu_k, uh, randomly, okay?
So this was a step where you plop down the red cross and the blue cross.
Uh, and when they did it on the PowerPoints, you know,
I did it as if we're just choosing these as random vectors.
In practice a good way of the- actually the most common way
to select a random initial cluster centroid isn't quite what I showed,
is to actually pick k examples out of your training set and just
set the cluster centroids to be equal to k randomly chosen the examples, right?
So in a low-dimensional space like a 2D plot,
you know, you can do on the diagram,
it doesn't really matter but when you work with very high dimensional datasets,
the more common way to initialize these is just pick, you know,
k training examples and set the cluster centroids to be
at exactly the location of those examples.
But then low dimensionless spaces it- it- you know,
it doesn't make a big difference.
Um, and then next you repeat until convergence.
Um, step one is-
right? So this is a- well
now I'll just write this down,
okay? Um, so does that make sense?
So the two steps you would alternate between the first one
is set Ci for every value of i.
So for every example,
set Ci equal to, you know,
either 1 or 2 depending on whether, er,
that example Xi is closer to cluster centroid one or cluster centroid two, right?
So ju- just take each point and color either red or blue.
Uh, or and represent that by setting Ci equals 1 or 2,
er, if you have two clusters.
If k is equal to 2, right? Oh yeah.
[inaudible]
Oh. Er, the notes say L1 norm squared?
From this morning?
Uh, what notes were sent out this morning?
[inaudible].
Oh that's red. It shouldn't be L1 norm.
Uh, if it says L1 norm,
that's a mistake. Sorry about that.
Er, but usually- an- and it turns out whether you use
L2 norm and L2 norm squared that gives you
the same answer because the algorithm is the same either way.
But it is usually- do we have a typo on the notes?
[inaudible].
Oh I see. Oh got it. Oh- oh- oh okay.
Let's say in notes we wrote that.
Okay. Cool. But by default,
when we write that norm we actually use- we mean L2 norm.
Yeah, right? But by- by default this is the L2 norm of x if is unspecified.
Er, if it's L1 norm, we usually write this.
So L2 norm is more common and with or without the square it you get the same result.
Okay. Cool. Thank you. All right.
So let's color the dots.
Um, paint each dot either red or blue.
Uh, and then, um, uh,
for this, um, this is,
you know, some key examples and take
all the examples assigned to a certain cluster, right?
Assigned to cluster j and set Mu_j to
be average of all the points assigned to that cluster J. Yeah.
[inaudible]
Oh sure. Er, no that does not work.
Uh, you know, I don't think -I don't know if I have- all right.
Now, that the black markers are working,
um, this is better?
All right let me try to use this?
Is there a part of this that's unclear?
If this part you can't see, I'll write it out more clearly.
Yes. Go ahead. Let's go ahead.
[inaudible].
Oh sure. How I do it, lights in front?
[inaudible].
Got it. Let there be light.
All right. Awesome great.
That was an easy request to satisfy, great, you know, okay.
I guess we'll actually look at it for another minute.
All right. Is that okay? Thank you.
Okay. This wasn't part of it.
Okay. All right. Now, I can move it up.
[NOISE].
All right. Um, so it turns out that,
uh, um, this algorithm can be proven to converge.
Um, the, exactly why it is written out in the lecture notes.
But it turns out, if you write this as
a cost function, right?
Um, so the cost function for a certain set of assignments of, uh,
points of examples to cluster
centroids and for a certain set of positions of the cluster centroids.
So, so c, these are the assignments and these are the centroids, right?
So, so this cost here is sum of your training
set of what's the square distance between each point,
and the cluster centroid it is assigned to, right?
So it turns out, um,
I want to prove this, uh,
little bit more detail in lecture notes but I'm going to prove this.
It turns out that on every iteration,
k-means would drive this cost function down,
um, and so, you know,
beyond a certain point this cost function,
it can't go even,
it can't go, uh, uh, any lower.
Well, this, this can't go below 0, right?
And so this shows that k-means must converge,
or at least this function must converge because it's, uh, a
strictly non-negative function that's going down on every iteration.
So at some point, it has to stop going down,
and then you could declare k-means are converged.
Um, in practice, if you run k-means in a very,
very large data set,
then as you plot the number of iterations, uh,
j may go down,
and you know, and,
and just because of,
a lack of compute or lack of patience,
you might just stop this running after a while.
It is going down too slowly.
So that's sort of k-means in practice where maybe
it hasn't totally converged, we just cut it off and call it good enough.
Um, now, uh, uh,
the most frequently asked question I get for k-means is how do you choose k?
It turns out that, um,
when I use k-means,
I still usually choose k by hand.
And so, and, and this is why.
Which is in unsupervised learning,
um, sometimes it's just ambiguous, right?
How many clusters there are [NOISE].
Right? Um, with this dataset,
some of you will see two clusters,
and some of you will see four clusters,
and it's just inherently ambiguous what is the right number of clusters.
So there are some formulas you can find online,
the criteria like AIC and BIC for automatically choosing the number of clusters.
In practice, I tend not to use them because, uh, um,
I usually look at the downstream application of what you actually want
to use k-means for in order to make a decision on the number of clusters.
So for example, if you're doing a market segmentation,
um, you know, because your marketers want to design different marketing campaigns, right?
For different groups of users,
then your marketers might have the bandwidth to design four separate marketing campaigns,
but not 100 marketing campaigns.
So that would be a good reason to choose four clusters rather than 100 clusters.
So it's often, uh, uh,
if you look at the purpose of what you're doing this for.
Um, I think in the previous exercise,
uh, in the homework, you see a, um,
image compression, uh, exercise where you want to cluster,
uh, colors into smaller number of clusters.
You implement this. This is actually one of the most fun exercises I think.
Um, uh, uh, that, uh,
uh, but so there you'd, you know, be saying, well,
how much do you want to compress the image to decide how many clusters to,
to try to use, okay?
So I usually, um, pick the number of clusters, you know,
either manually or looking at what you want to use k-means cluster for.
Um, when we're trying to cluster news articles, uh,
the Google News example,
I think I showed in the first lecture.
You say, well, how many clusters is going to make sense for,
for, for news articles, okay?
All right. So good. So, uh, yeah?
[inaudible].
Oh sure. Well, k-means get stuck on local minima.
Yes, k-means gets stuck on sort of local minima sometimes.
And so, if you're worried about local minima,
the thing you can do is, uh,
run k-means, say, 10 times, or 100 times,
or 1000 times from different random initializations of the cluster centroids.
And then run it, you know, say 100 times, uh,
and then pick whichever run results in the lowest value for this cost function, okay?
All right. Um, so you'll play with this more in, um,
uh, in the programming exercise.
Now, um, there's a,
there's a problem that seems closely related.
Um, but, but it's actually
quite different ways to write the algorithms which is density estimation.
So, so let me motivate this.
Um, I actually have a- well, right,
sometime back had some friends working on a problem which I'll simplify a little bit,
um, of, uh, uh, you know,
you have aircraft engines coming off the assembly line.
All right. And every time an aircraft engine comes off the assembly line,
you measure some features of these engines.
You measure some features about the vibration,
and you measure some features about the heat that the aircraft engine is producing.
And, um, let's say that you get a dataset,
right, that looks like this, okay?
And, um, the anomaly detection problem
is if you get a new aircraft engine that comes off the assembly line,
and if the vibration feature takes on this value,
and the heat feature takes on this value,
is that aircraft engine an anomalous one,
is it an unusual one, right?
And so the application of this is,
um, that as your aircraft engine comes off the assembly line,
if you see a very unusual signature in terms of
the vibrations and the heat the aircraft engine is generating,
then probably something is wrong with this aircraft engine,
and you have your people, have your,
have your team inspect it further or test it further,
uh, before you ship the airplane,
before you ship the engine to a,
to a airplane maker and then something goes wrong in the air, and there's a,
there's a major accident,
or major disaster, right?
And so anomaly detection, uh, uh,
is most commonly done,
or one of the common ways to, um,
implement anomaly detection is the model p of
x which is given all of these blue examples,
given all of these dots,
can you model what is the density from which x was drawn?
So then if p of x is very small,
then you flag an anomaly, right?
Meaning that, Gee, I think something's funny here, uh,
and maybe someone should inspect this aircraft engine a little bit further.
Um, so anomaly detection is used for,
a task like this,
for an inspection task like this.
Um, it's used for,
um, uh, many years ago,
I was actually working with some telecoms providers, you know, uh, uh,
helping out telecoms company on, um,
anomaly detection to figure out if something's
gone wrong with a cell tower network, right?
So if one day one of the cell towers start throwing off
network patterns that seem very unusual,
then maybe something's wrong with that cell tower,
like something's gone wrong.
We sent out the technicians to fix it.
Uh, it is also used for computer security.
If a computer, say if a computer at Stanford starts sending out very strange,
you know, um, uh, uh, network traffic,
that's very unusual relative to everything it's done before,
relative what this is,
is a very anomalous network traffic,
then maybe IT staff should have a look to
see if that particular computer has been hacked.
So these are some of the applications of anomaly detection.
And the good way to do this is,
given an unlabeled data set, model p of x.
And then if you have very low probability samples,
you flag that as a possible anomaly for further study.
Now, given this dataset,
um, uh, how do you model this?
One interesting thing about this green dot is that
neither the vibration nor the heat signature is actually out of range, right?
You know, like there are a lot of aircraft engines with vibrations in that range.
There are a lot of aircraft engines with heat in that range.
So neither feature by itself is actually that unusual.
It's actually the combination of the two that is unusual.
Um, and so thus,
thus, what I want to do is,
uh, come up with an algorithm to model this.
And in fact, we'll come up with an algorithm that can model, you know,
maybe, maybe your data density looks like this,
maybe more of an L shape like that.
But how do you model p of x with the data coming from an L shape?
Um, and it turns out that there is no textbook distribution, right?
You know, there isn't, you know, if you look at a simple exponential family of model,
the types of distributions,
there is no distribution for modeling very,
very complex distributions like this.
So what we're going to talk about is, um,
the mixture of Gaussians model which we look at data like this,
and say, it looks like this data actually comes from two Gaussian.
There's one Gaussian, maybe there's
one type of aircraft engine that, that, that, you know,
is drawn from a Gaussian like the one below,
and a separate aircraft- type of
aircraft engine that's drawn from a Gaussian like that above.
And this is why there's a lot of probability [NOISE] mass in the L-shaped region,
but very low probability outside that L-shaped region, right?
And, and, and these ellipses I'm drawing are the contours of these two Gaussians, right?
And so, um, what I'd like to do next is,
uh, develop the mixture of Gaussians model, um,
which is useful for anomaly detection,
and, and, uh, uh, and,
and then this will lead us to our second unsupervised programming algorithm, okay?
So, um, in order to make the mixture of Gaussians model a bit easier to develop,
let me just use a one-dimensional example where x is in R, okay?
So, um, let's see.
So let's say that, uh,
we gather a data set that looks like this.
[NOISE]
Right. So it's just one row number.
So it's just on num- number line I plotted a few dots.
Um, so it looks like this data maybe comes from two Gaussians.
Right? It looks like, you know, there's some data from this Gaussian.
And there's some data from that Gaussian on the right.
Um, and is- and if only we knew.
Right? Which example had come from which Gaussian, if,
if we knew that these examples had come from Gaussian 1,
which I want to denote with crosses.
And if only we knew- no, that was here.
What- but actually this is fine. I'll leave that one there.
If only we knew that
these examples had come from Gaussian 2 which I'm going to draw with Os,
then we just fit Gaussian 1 to the crosses,
fit Gaussian 2 to the Os and then we'd be pretty much done.
Right? Um, oh, and, and, and sorry.
And so these are the two Gaussians.
And so the overall density would be something like this.
Right? Tha- that's the probability.
A lot of probability mass on left.
A lot of probability mass on the right, low, less probability mass on the,
uh, in sort of in, in the middle.
Okay? So the overall density I'll just draw again, would be,
low high, low high something like that.
Right? Um, but the reason- and,
and, and if you actually had these labels.
If you knew that these examples came from Gaussian 1,
those examples come from Gaussian 2,
then you can actually use an algorithm very similar to GDA,
Gaussian discriminant analysis to fit this model.
Uh, that the problem with this density estimation problem is,
you just see this data and maybe the data came from two different Gaussians.
But you don't know which example actually came from which Gaussian.
Okay? So the EM algorithm or the expectation-maximization algorithm will allow us to, uh,
fit a model despite not knowing which Gaussian each example that come from.
So let me first write down the,
um, mixture of Gaussians model.
Uh, and then we'll describe the EM algorithm for this.
So let's imagine- let's suppose that as a,
um, so the term we sometimes use is latent,
but latent just means hidden or unobserved.
Um, random variables z.
Right?
And x_i, z_i,
um.
Okay? So- this part here.
So let's imagine that, um,
there's some hidden random variable z and,
and the term latent just means hidden or unobserved.
Right? It means that it exists but you don't get to see the value directly.
So when I say latent,
it just means hidden or unobserved.
So let's imagine that there's a hidden or latent random variable z and,
uh, x_i and z_i have this joint distribution.
And this, this, this is very,
very similar to the model you saw in Gaussian discriminant analysis.
But z_i is multinomial with some set of parameters Phi.
For a mixture of two Gaussians,
this would just be Bernoulli with two values.
But if it were a mixture of k Gaussians then z, you know,
can take on values from 1 through k. [NOISE] Right?
Um, and it was two Gaussians it'll just be Bernoulli.
And then once you know that one example comes from, uh.
Gaussian number j, then x condition that z_i is equal to j.
That is drawn from a Gaussian distribution with some mean and some covariance Sigma.
Okay? So the two unimportant ways.
This is different than GDA.
Um, one, well, I've set z to be 1 of k values instead of one of two values.
And GDA, Gaussian discriminant analysis.
We had z, you know, uh,
why the labels y took on one of two values.
Uh, and then second is,
I have Sigma j instead of Sigma.
So by, by convention when we fit mixture of Gaussians models,
we let each Gaussian have his own covariance matrix Sigma.
But you can actually force it to be the same way you want.
So- but these are the trivial differences.
Uh, the most significant difference is that,
in Gaussian discriminant analysis,
we had labeled examples x_i, y_i.
Where z- y was observed.
Right? And then the main difference between this and Gaussian discriminant analysis is,
now we have replaced that with
this latent or hidden random variable z_i that you do not get to see in the training set.
Okay?
So now, uh,
actually you guys are right.
These pens are terrible.
All right. Oh, that was better.
Cool. All right.
So if we need the z_i's.
Right? Then we can use,
um, maximum likelihood estimation.
All right? So if only we knew the value of the z_i's, which we don't.
But if only we did, then we could use
maximum likelihood estimation or MLE to estimate everything.
You know. So we would write
the log likelihood of the parameters.
Right? Equals sum, um,
log p of x_i,
z_i, you know, given the parameters.
Right? And then you take the derivative,
set the derivatives equal to 0 and then you guys did this in problem set 1.
Right? And, and then you would find that Phi j is equal to
1 over m. Right?
Okay. So if only you knew the values of the z_i's, uh,
then you could use maximum likelihood estimates,
um, will- and, and this is what you get.
And this is pretty much the formulas.
Actually the- the- these two are exactly the formulas,
uh, we had for, uh, Gaussian discriminant analysis.
Except with replace y with z.
Right? And then there's some other formula for Sigma that's written in the lecture notes.
But I won't, but I won't write down here.
Okay? Um, but the reason we can't use this,
use these formulas is we don't actually know what are the values of z.
So what we will do in
the EM algorithm
is two steps.
Um, in the first step,
we will, uh, guess the value of the z's.
And in the second step we will use
these equations using the values of this z's we just guessed.
So let me- so, so sometimes in, um,
the machine learning is something that's called- there's a bootstrap procedure
where you get something that runs an algorithm.
You're using your guesses and then you
update your guesses and then run the algorithm again.
Let me, let me make that concrete by writing this down.
So the EM algorithm has two steps.
The E-step, um,
also called the expectation step is set to w i j.
So w i j, um,
is going to be the probability that z_i is equal to j.
Okay? Um, given all the parameters.
And, and much as we did with, um,
generative learning algorithms, right,
with generative learning algorithms,
we used Bayes' rule to estimate the probability of y given x,
and so to compute this,
you use a similar Bayes' rule type of calculation.
And so this would be [NOISE].
Oops, right, um, where,
for example this term here P of x_i given z_i equals j.
This would be a Gaussian density, right?
This comes from a Gaussian density with mean Mu j and covariance Sigma j, right?
And so this term here would be 1 over 2 Pi,
to the N over 2 Sigma j,
so one-half e to the negative one-half.
All right. And then this term here,
I guess this would be Phi j,
that's just a Bernoulli probability,
remember z is multinomial.
Right, so z is multinomial with parameters Phi.
So I guess the parameters Phi for multinomial distributions
tell you, what's the chance of z being 1, 2, 3, 4,
and so on up to k, and so the chance of z_i being equal to k is
just- chance of z_i being equal to j is just Phi j right?
It's just v to the off one of the parameters in your multinomial probability for,
um, for the odds of z being different values.
okay? And so, um,
and similarly the terms in the denominator.
This term here is from Gaussian and that second term is from the,
um, multinomial probability that you have for z.
And so that's how you plug in all of these numbers and use Bayes rule and use
this equation to compute given- all given the position of all these Gaussians,
what is the chance of w i j taking on a certain value, okay.
And, and so to make this really concrete,
you remember how I guess 1 or 0s, or the other way, um,
If you were to look at these,
uh, if you were to scan from right to left,
remember how, you know,
you get a sigmoid function,
or the sigmoid can be this way or this way or it depends on the sign.
I guess if these are positive samples these are negatives.
You have a sigmoid function like this.
And so w i j is just the height of this Sigma,
it's just a chance of, you know,
each of these examples being,
coming from either the z equals 1 or z equals 0
and then you store all of these numbers in the variables w i j.
Okay. So w i j is just to compute the posterior choice of this,
this example coming from the left Gaussian versus the right Gaussian.
You just saw that in the variable w i j.
So that's the E-step,
um, and you compute the w i j for every single training example i.
Right? I think it's the M-step is, um, yeah.
[BACKGROUND]
Sorry, is this what?
[BACKGROUND]
Oh this one.
Yes.
Sorry, yes. Thank you, there we go, thank you.
Yes, there's a following Gaussian.
Okay. So in the- so the E-step tells us, you know,
we're trying to guess the values of the z's right,
when we figure out what's the probability of z
being 1, 2, 3, 4 up to k was stored here.
And then in the M-step,
what we're going to do is use the formulas we have for maximum likelihood estimation,
and I want you to compare these with the equations I had above, right.
Okay. Well, I hope you see.
So these equations are a lot like the equations above,
except that instead of indicator z_i equals j,
we replaced it with w i j, right?
Which by the way is the expected value of this indicator function.
Right, because the expected value of an indicator function is
just equal to the probability of that thing in the middle being true.
Okay? Um, and then,
and then there's a formula for Sigma j as well that you can get from the lecture notes,
but i won't, I won't write down here.
Okay. So, um, one intuition of,
um, this mixture of Gaussians algorithm is
that it's a little bit like k-means but with soft assignment.
So in k-means, in
the first step we would take each point and just assign it to one of the k,
k cluster centroids, right?
And if it was a little bit closer to
the red cluster centroid than the blue cluster centroid,
we would just assign it to the red cluster centroid.
So even if it was just a little bit closer to one cluster centroid than another,
k means we just make what's called a hard assignment meaning, you know,
whatever cluster centroid it's closest to,
we just assigned it 100 percent of that ce- cluster centroid.
So yeah, EM is,
uh, you can think, uh,
EM implements a softer way of assigning points to
the different cluster centroids because instead of just
picking the one closest Gaussian center and assigning it there,
it uses these probabilities and gives it a waiting,
in terms of how much is assigned to Gaussian 1 versus Gaussian 2.
Um, and the second updates,
you know, the means accordingly, right?
Sum over all the x_i's to the extent they're assigned to
that cluster centroid divided by
the number of examples assigned to that cluster centroid.
Okay? So, so, so that's one intuition be- between EM and k-means,
um, and in a second, uh, uh, but,
but when you run this algorithm,
it turns out that this algorithm will converge with some caveats I'll get to later,
and this will find a pretty decent estimate, um,
of the parameters, you know,
of say fitting a mixture of two Gaussians model.
Okay? So this is, um,
the- a- and so if you are given a dataset of say airplane engines,
you can run this algorithm for the mixture of two Gaussians.
And then when a new airplane engine rolls off the assembly line, um, um, so,
so after you're fitting the k-means algorithm,
you now have a- after fitting the EM algorithm,
you now have a joint density of a P of x comma z.
And so the density for x is just sum of all the values of z of P of x comma z.
And so, and so
a mixture of Gaussians can fit distributions that look like this,
it can fit distributions that look like this, right?
These are, these are both mixtures of two Gaussians.
So this gives you a very rich family of models to fit very complicated distributions.
And now that, um, right,
and you can also fit, I don't know, something like this.
So this is a mixture of two Gaussians,
I guess one thin narrow Gaussian here and one much wider fatter Gaussian.
So a mixture of two Gaussians can actually fit a model of different things, um, uh,
can fit a lot- and a mixture of more than two Gaussians can fit even richer models.
And so by doing this,
you can now model P of x for many complicated densities,
or including this one, right,
this example I had just now.
This will allow you to fit
a probability density function that puts a lot of probability models on,
on a region that looks like this.
And so when you have a new example you can evaluate P of x,
and if P of x is large,
then you can say nope this looks okay and the P of x is less than Epsilon.
You can flag an anomaly and say take a look- take another look at this here.
Okay? So, um, I kind just wrote down this algorithm,
with a little bit of a hand-wavy explanation for how it's derived, right?
So like I said, if only you knew the values of C and just use maximum likelihood estimation,
so let's guess the values of z and plug that
into the formulas of maximum likely estimation.
It turns out that hand-wavy explanation works,
in the particular case of, um,
the EM mixtures of Gaussians but that there is a more formal way of
deriving the EM algorithm that shows that this is a maximum likelihood estimation algorithm,
and that they converge at at least a local optimum.
Um, and in particular,
there- what we'll do is show that if your goal is, um,
uh given a model P of x,
z parameterized by Theta,
if your goal is to maximize P of x,
right? Oh, excuse me.
So this is what maximum likelihood is supposed to do.
That EM is exactly trying to do that, okay.
So, um, I'll go on in a minute to present this more general derivation,
the - the form of general derivation of the EM algorithm tha- that doesn't rely on
this hand-wavy argument of
I guess it's easier use maximum likelihood with the guess values.
So I'll do the the rigorous derivation of EM in a minute.
But before I do that, let me just pause and check if there any questions.
Yeah.
[inaudible].
Um, yeah,
uh, maybe- let's see.
Maybe I'll help to not think of them as weights.
Um, yeah, I think thi- this is actually the weighting you assigned to a certain Gaussian,
so there's one intuition, uh,
hen - hence the weights, but, um,
um, let me think,
what's going to explain this?
So one way to think of this as wij is
how much xi is assigned to,
you know, to- to- to the um, µj Gaussian.
So, um, wij is the strength of how
strongly you want to assign that training example
xi to that cluster or to that- to that particular Gaussian.
Um, and so this is the number of 2,
0, and 1 right?
And, uh, the strength of all the assignments,
and every point is a sign with a total strength equal to 1,
because all these properties must sum up to 1.
And so, when I take this point and assign it, you know,
0.8 to a more close Gaussian and 0.2 to a more distant Gaussian.
And this is our guess for, you know,
well there's an 80% chance that it came with that Gaussian and a 20% chance
it came with the second Gaussian. That makes sense?
[inaudible].
Oh I see. So let's see.
Um, so when you're running the EM algorithm,
you never know what are the true values of z, right?
You're- you're given a data set,
so you're only told the x's,
and as far as we know, uh,
these airplane engines were generated off,
you know, two different Gaussians.
Maybe there are two separate assembly processes.
You know, one from the, uh,
uh, one from plant number one,
one from plant number two,
and maybe they actually operate a little bit differently,
but by the time they merge onto one, um,
uh, you know, by- by the time
the two suppliers of aircraft engines get to you, they've been mixed together,
and so you can't tell anymore which aircraft engine came
from proce- plant one and which pla- aircraft engine came from plant two.
Um, and they you know there are two plants,
where you just see the stream of aircraft engines,
you're hypothesizing that there are two types.
And so in every iteration of EM,
you're taking each, uh,
aircraft engine and guessing, you know, for this one,
I think there's 80% chance that it came from process one,
and a 20% chance it came from process two,
so that's the E-step.
And then in the M-step,
you look at all the engines that you're kind of guessing were generated by process one,
and you update your Gaussian to be a better model for all of the things that
were- that you kind of think were generated by process one.
And if there's something that you're absolutely sure came from process one,
then it has a weight of one close to one in this.
Do you think that was something that, you know,
there's a 10% chance it comes from process one,
then that example is given a lower weight and now you
update the mean for that Gaussian. That make sense?
Cool. All right.
So, [NOISE] 33 minutes.
Yeah.
Okay cool. All right.
Well I still remember when, um,
I was an undergrad doing a summer internship at AT&T Bell Labs.
Um, and then someone a few offices down had
learned about EM for the mixture of Gaussians for the first time,
and he was running it on his computer,
and he's going around to every single office.
Saying, "Oh my God you've got to check this out,
this is unbelievable look at what this algorithm can do for three mixes of Gaussian. "
So tha- that shows you,
those are the type of people I hang out with
[LAUGHTER].
All right. Um, so in order to derive yo - you know,
so -so this is a slightly hand-wavy argument.
As I uh, let's get- let's guess the values of the z's.
Let's just have these weights and plug them into maximum likelihood.
Um, what I would like to do is give
a more rigorous de- derivation for why EM Algorithm is a reasonable algorithm,
and why it's a maximum likelihood estimation algorithm
and why we can expect it to converge.
And it turns out that rather than just proving,
you know, that this is a sound algorithm,
what we'll see on Wednesday is that this view of EM, uh,
allows us to derive EM in a- in
a more correct way for other models as well, the mixtures of Gaussian.
On, on Wednesday, we'll talk about,
uh, uh a model called factor analysis,
it lets you model Gaussians in extremely high dimensional spaces,
where if you have 1,000 dimensional data,
but only 30 examples,
how do you fit a Gaussian into that?
So we'll talk about that on Wednesday.
And it turns out this derivation of EM we're
going to go about- through now is crucial for,
um, applying EM accurately in- in- in problems like that.
Okay, so.
Uh, in order that live up to that derivation,
let me describe, um, Jensen's inequality.
So let f be a, a convex function.
Um, to do EM,
we're actually going to need a concave function,
so it'll be all minus of everything,
but we'll get to that in a second.
But so, a convex function means the second derivative is greater than 0,
or in other words, it looks like that, right?
So that's a convex function.
Uh, let x be a random variable.
Then f of the expected value of x is less than or equal to the expected value of x.
Okay.
Now, [NOISE] um, [NOISE]
maybe, um, here's an example. All right.
So here's the, um, let's see,
there's the function f of x,
and let's say that these are the values 1, 2, 3, 4, 5.
And suppose that X is equal to 1 with
probability one-half and is equal to 5 with probability one-half, right?
Just for the illustration.
Then here is f of 1.
Here is f of 5.
Um, here is f of 3.
And f of 3 is f of the expected value of X,
right, because so the expected value of X.
And sometimes I write this without the square brackets, right.
It's the average of X is equal to 3.
Um, and so the expected value,
excuse me, f of the expected value of X is equal to this value,
whereas the expected value of f of
x is the mean of f of 1 and f of 5.
All right. So the expected value of f of x.
F of x is a 50% chance of being f of 1,
and a 50% chance of being f of 5.
And so the expected value of f of x is equal to this value in the middle.
It's really take these two, right.
Take this value and this value and take their mean.
So is this value up here, and,
and this value is the expected value of f of x.
Okay. And so in this example the expected value
of f of x is greater than f of the expected value of X,
right, as, as predicted by Jensen's inequality.
Um, I'm gonna just draw one illustration that may or may not help,
and some of my friends like it,
I sometimes use it but if it's confusing then don't worry about it.
But it turns out that if you draw a line that connects these two,
then the midpoint of this line, um,
is the height of f of expected value of x,
right, so the height of this.
You know, so, so given these two points,
this point and this point, if you draw this line,
it's called a chord, um,
then the height of this point is the expected value of f of x.
And this point is,
um, f of the expected value of x.
Right. And in any convex function,
you know, really take any convex function.
That's also convex function.
If you draw any chord,
that green point is always higher,
right, than that green point which is why- which is
another way of seeing why Jensen's inequality holds true.
Okay. If this visualization doesn't help don't worry
about it but it's just a- actually what a lot of
my friends do is we keep on forgetting which direction Jensen's inequality goes.
[LAUGHTER] Why are we not using Jensen [LAUGHTER] that's not great.
So a lot of my friends don't remember, we draw this picture and draw that chord,
and we quickly figure out which way the inequality goes.
Um, all right.
So one addendum further.
If f is strictly greater than 0.
And so if this is the case,
we say f is strictly convex.
Then-
Okay.
So, um, let's see, a straight line is also a convex function, right.
So this is a convex function,
this is a convex function and this is a convex function.
It turns out a straight line, that's also a convex function.
But so in this addendum is saying that if f is
a strictly convex function meaning basically it's now a straight line, right.
And a bit modern, it's not a straight line.
But if the curvature if it's always bending up, uh,
then the only way for the left and right-hand sides to be equal is if x is a constant,
meaning it's a random variable that always takes on the same value.
Okay. So Jensen's inequality says that,
you know, um, left-hand side is going to be the same as right hand side.
Sorry, I think I reversed the order of these two for that equation that doesn't have it.
But so Jensen equality says
left-hand side is always less than or equal to the right-hand side,
and the only way it's equal as if X, you know,
is a random variable that always takes on the same value.
Okay, yeah.
What if- What if the value of f of 1 was equal to the value of f of 3. Wouldn't that?
Yeah. So it turns out what if value f of 1 is equal to the value of f of 3.
It turns out, it does.
Vary. So let's see.
So one way that [NOISE] could happen would be if the function were like that.
And then if you take the- draw the chord,
take the mean it's still higher.
[inaudible] The point of f of 1. [inaudible]
Then it's important. If, if you kind of flat that part here,
then the function is not strictly convex.
And so it's still less than equal to but it's not,
but they can't be equal to if x is random.
Okay. So and we'll use this in a little bit.
We'll actually end up using this.
Um, and again for the strict probabilistic, you know,
if those of you that, I don't know, take classes in advanced probability,
the technical way of saying x is a constant is x is equal to EX with probability 1.
You know, I, I think that for all practical human purposes
you do not need to worry about this.
But I think if you [LAUGHTER] take a class in measure theory.
The Professor in measure theory will be happy if you say this and you
say x is a constant but maybe, maybe none of you.
Okay. Just don't worry about it.
Um, oh yes.
Okay. Now, um,
just one more addendum,
um, to this is that
the form of Jensen's inequality we are going to use
is actually a form for a concave function.
So instead of convex,
um, I'm gonna say concave.
And so, you know,
a concave function is just a negative of a convex function, right.
If you take a convex function and take the negative of that, it becomes concave.
And so the whole thing works with the- with everything flipped around the other way.
Okay. And yep, so this is strictly concave.
Okay. So the form of Jensen's inequality we are gonna use is actually the, um,
concave form of Jensen's inequality,
and we're actually going to apply it to the log function.
So the log function, right.
Log x looks like this.
And so that's a concave function.
And so the inequality we'll use would be
in this direction that I have in orange.
All right.
So here's the density estimation problem.
Meaning, density estimation means you want to estimate P of x. All right.
So we have a model
for P of x, z, with parameters theta.
And so, you know, instead of writing out mu,
sigma- mu, sigma, and phi,
like we did for the mixture of Gaussians.
I'm just gonna capture all the parameters you have.
Whatever your parameters are, I'm just gonna capture them in one variable theta.
And you only observe x.
So your training set looks like that.
So the, um, log likelihood of the parameters theta is equal
to some of your training examples log P of x_i, parameterized by theta.
Um, and this in turn is log of sum over z,
P of x_i, z_i parameterized by theta, right.
Because P of x, you know,
is just taking the joint distribution and summing out, marginalizing out z_i.
Okay. [NOISE] And so what we want is maximum likelihood estimation
which is define the value of theta that maximizes this log-likelihood.
And what we would like to do is derive an EM,
derive an algorithm which will turn out to be an EM algorithm as
an iterative algorithm for finding
the maximum likelihood estimates of the parameters theta.
[NOISE]
So, um, let me draw a picture of that,
I'd like you to keep in mind as we go through the math, which is,
you know, the, the horizontal axis
is the space of possible values of the parameters Theta.
And so there's some function O of Theta that you try to maximize.
This right. And so what EM does is, um,
let's say you initialize Theta as some value, you know,
maybe randomly initialize, um,
sim- similar to the k-means cluster centroids.
Where just randomly initialize your mu's with a mixture of Gaussian's.
What the EM algorithm does is in the E-step,
we're going to construct a lower bound shown in green here for the log-likelihood.
And this lower bound,
this green curve has two properties.
One is that it is a lower bound.
So everywhere you look, you know,
over all values of Theta,
the green curve lies below the blue curve.
So this is a lower bound.
And the second property that the green curve has is that it is
equal to the blue curve at the current value of Theta.
Okay. So what the E-step does,
uh, which you'll see later on,
and just keep this picture in mind as we go through the E-step and the M-step is,
um, construct the lower bound that looks like this, right.
Oh, and, and also, uh, to, uh,
to foreshadow probably the derivation.
Right? There- there was an addendum to Jensen's inequality where we said,
well, under these conditions it holds with equality.
Right. E of f of x equals f of e of x.
We said, "Well, the two things are equal with under certain conditions."
Um, we want things to be equal.
We want the green curve to be equal to the blue curve at the old value of Theta.
So we- we'll use that addendum to Jensen's inequality when we drive that.
Um, so this E-step is draw the green curve.
And then what the M-step does is it takes a green curve,
and then it finds the maximum.
Actually, certainly stroke [inaudible] so I'll draw in green.
What the M-step does is it takes the green curve,
and it finds the maximum.
And one step of EM will then move Theta from this green value to this red value.
Okay. So the E-step constructs the green curve,
and the M-step, uh,
finds the maximum of the green curve.
And this is one iteration of EM.
The second iteration of EM,
now that you're at this red thing is will construct a new lower bound,
and then again, you use a different lower bound.
Everywhere the red curve is below the blue curve,
and the values are equal at this new value.
That's the E-step, and then M-step will maximize this red curve,
um, and so on. Now you're here.
Construct another thing, do that.
Right. And you can kinda tell that as you keep running EM,
this is constantly trying to increase L of Theta.
Trying to increase the log-likelihood,
until it converges to a local optimum.
Okay. Um, the EM algorithm does converge only to local optimum.
So if, you know, there was another even bigger thing there that it may never
find its way over to that other- that, uh, better optimum.
But the EM algorithm by repeatedly doing this,
will hopefully converge to a pretty good local optimum.
Okay. All right.
So let's write on how we do that.
Um, let me think.
Actually, let me use the other board.
No, I think this is okay. All right.
So I've already said that our goal is to find the parameters theta that maximize this.
[NOISE] All right.
Uh, and so that equation we said are just now is sum over i log,
sum over zi, p of
xi comma zi given Theta.
Okay. So this is just what we had written down,
I guess, uh, on the left.
What I'm going to do next is,
um, divide by- [NOISE]
multiply and divide by this.
Okay. Um, where Qi of zi is a probability distribution,
i.e., the sum over zi,
Qi of zi equals 1.
Okay. So I'm going to multiply and divide by some probability distribution,
and we'll, we'll decide later
how to come up with this probability distribution Qi, right.
But, you know, I'm allowed to construct
a probability distribution and multiply and divide by the same thing.
Right. Now, if you look at this,
all right, let's put square brackets here.
If this Qi, that is the probability distribution meaning that sum over zi Qi,
zi sums over- sums to 1.
Then this thing inside is, um,
equal to sum over i log of an expected value of zi
drawn from the Qi distribution of [NOISE] right, actually, if I,
let me use colors to make this clearer.
Right. So the way you compute the expected value of z-, you know,
some function of zi is you sum over all the possible values of
zi of the probability of zi times whatever that function is.
So this equation is just the expected value with respect to zi
drawn from that Qi distribution of that thing in the square brackets,
in the purple square brackets.
Now, using the, um,
concave form of Jensen's inequality,
we have that this is greater than
or equal to [NOISE].
So this is a form of Jensen's inequality where,
um, f of E, x is greater than or equal to E of f of x,
where here, um, this is the logarithmic function.
Right. So the log function is a concave function. It looks like that.
And so, um, using the,
I guess here using,
using the form Jensen's inequality with the signs reversed, um.
Right, f of Ex is greater than equals E of fx.
So you get log of expectation is greater than equal to expectation of the log, all right.
And then finally, let me just take this expectation and unpack it one more time.
So this is now sum over i, sum over zi.
[NOISE].
Okay. So I just took this expected value and
turned it back into the sum of the random variable probability, times that thing.
Okay. So, um, if you remember this picture from the middle,
what we wanted to do was to construct a function,
construct this green curve.
There's a lower bound for the blue curve.
And if you view this formula here as a function of Theta right,
so your x, um, x is just your data,
and z is a variable you sum over.
So this whole thing is the function of Theta, right?
Because x's are fixed,
z is just something you f- sum over.
So this whole formula here,
this is a function of the parameters Theta.
And what we've shown is that this thing, you know,
this formula here, this is a lower bound for the log-likelihood,
uh, for- for, for, for this thing.
I guess this is L of Theta.
So- go ahead.
[inaudible].
Oh, how I got to this equation?
Uh, sure. Um, let me think.
So let's see. What's a good way to do this?
Um, uh, yeah.
Let's say that z takes on values from 1 through 5, right.
Let's say z takes on values from 1 through 10.
So you roll a 10 sided dice.
And I want to compute, um, you know,
the expected value of, uh,
some function of, of some function g, g of z.
Right. Then the expected value of g of z is sum of
all the possible values of z of the probability that you get that z,
times g of z.
Right. So that's, that's what's the expected value is of a function of a random variable.
And, and this is- and the expected value of z is sum over z,
P of z times z.
That's the average of random variable.
And so, um, in the notation that we have,
the probability of z taking on different values is denoted by Qi of z,
which is why we wind up with that formula.
Does that makes sense? Does it?
Okay. Is that okay? Does that make sense? Yeah. All right.
If, if one of these steps doesn't make sense, let me know.
Th- other questions?
Okay. All right.
Hope that makes sense. [NOISE]. Um. [NOISE]
Now, one of the things we want when constructing
this green lower bound is we want that green lower
bound to be equal to the blue function at this point, right?
And this is actually how you guarantee that when you optimize the green function.
By improving on the green function,
you're improving on the blue function.
So we want this lower bound to be tight.
Right, the, the two functions be equal, tangent to each other.
So in other words we want this inequality to hold with equality.
So we want, um, yeah,
so we want the left hand side and the right hand side to be equal
for the current value of Theta, right?
[NOISE]
So on a given iteration of EM where
the current parameters are equal to Theta, we want,
we want- I know this was a lot of math but, you know,
we want the left and right hand sides to be equal to each other.
Right. Because that's what it means for,
uh, for the lower bound to be tight,
for the green color to be exactly touching
the blue curve as we construct that lower bound.
And so for this to be true,
we need the random variable inside to be a constant.
So we need P of x_i, z_i,
divided by Qi of z_i to be equal to const- to, to a constant.
Meaning that no matter what value of z_i you plug in,
this should evaluate to the same value.
In other words, the ratio between the numerator and denominator must be the same.
Um, unfortunately so far,
we have not yet specified,
how we choose this distribution for z_i, right.
So, so far the only constraint we have is that
Qi has to be a probability density- has to be a probability distribution over z_i,
but you could choose one of the distributions you want for z_i.
And it turns out that, um, uh,
we can set Qi of z_i to be proportional to p of x_i,
z_i parameterized by Theta.
And this means that for any value of z,
you know, so z_indicates as it could from Gaussian one and Gaussian two.
Right. So this means that the chance of Gaussian one is
proportional to the chance of Gaussian one versus Gaussian two.
Whether z_i takes on one or two is proportional to this.
And I don't want to prove it but one way to ensure this,
and this is proven in the lecture notes.
But it turns out that one way to ensure.
Um, well so the Qis need to sum to 1.
So one way to ensure that this is proportional to
the right-hand side is to just take the right-hand side.
Sorry. Let me move here.
So one- so let's see.
Right. So the Qis have to sum to 1.
And so one way to ensure the proportionality is to just take the right-hand side,
and normalize it to sum to 1.
Um, and after, after a couple of
steps that are in the lecture notes but I don't want to do here,
you can show that this results in sending Qi of z_i to be equal to that,
that posterior probability, okay?
And so, um, sorry I skipped a couple steps here.
You can get from the lecture notes,
but it turns out that if you want this to be
a constant meaning whether you plugged in z_i equals 1 or z_i equals 2 or whatever,
these evaluate to the same constant.
The only way to do that is make sure
the numerator and denominator are proportional to each other.
And because Qi of z_i is a density that must sum to 1.
One way to make sure they're proportional is to just
set this to be with the right-hand side but normalize the sum to 1.
Okay. And we derived this a little bit more carefully in the lecture notes.
So just to summarize,
this gives us the EM algorithm.
Let's take all of this- everything we just did and wrap in the EM algorithm.
In the E-step, we're going to set Qi of
z_i equal to that.
And previously this was the w_i_js.
Right. So instead of- so previously,
we're restoring these probabilities in the variables you call w_i_js.
And then in the M-step,
we're going to take that lower bound that we constructed,
which is this function,
and maximize it with respect to Theta.
Okay. Um, and so remember in the M-step we
constructed this thing on the right-hand side as a lower bound for the log-likelihood.
And so for the fixed value of Q,
you can maximize this with respect to Theta and that updates the Theta,
you know, maximizing the green lower boundary,
that's what the M-step does.
And if you iterate these two steps,
then you find that this should converge to a local optima.
Okay. Oh and just maybe that's the obvious question.
Um, why don't we try to maximize right Theta,
uh, why are we trying to maximize the log-likelihood directly?
It turns out that if you take the mixture of Gaussians model,
try to take derivatives of this and set derivatives equal to 0,
there's no known way to solve for the value of Theta that maximizes the log-likelihood.
But you find that for the mixture of Gaussians model and for
many models including factor analysis that we talked about on Wednesday,
if you actually plug in the Gaussian density- uh,
if you actually plug in that mixture of Gaussians model for P,
um, and take, you know,
take, take derivatives, set derivatives equal to 0 and solve,
you will be able to find an analytic solution to maximize this M step,
and that'll be exactly what we had worked out in the early derivation of the EM algorithm.
Okay. But so this derivation shows that,
uh, the EM algorithm, you know,
is a maximum likelihood estimation algorithm with
optimization solved by constructing lower bounds and optimizing lower bounds, okay?
All right. Um, that's it for today,
and only it's stuff up to here,
right, and so this stuff will be up
to the midterm but we'll talk about factor analysis
a lot on Wednesday, but it will not be on the midterm.
Okay. So let's break for today, and I'll see you guys on Wednesday.
 All right. Hi everyone, welcome back.
Um, so what we'll see today is, um,
additional, uh, elaborations on the EM,
um, on the expectation maximization algorithm.
And so, um, what you see today is,
um, go over, you know,
quick recap of what we talked about EM on Monday,
and then describe how you can monitor if EM is converging.
Um, and, um, uh, on,
on Monday we talked about the mixture of Gaussians model,
and started deriving EM for that.
I want to just take these two equations and map it back to
specifically the E and M steps that you saw for the mixture of Gaussians models, uh,
to see exactly how these map to, um, uh,
you know, updating the weights of the i and so on,
um, how you actually derive the M step.
Um, and then mostly what I want to spend today talking
about is the model called the factor analysis model.
Um, and this model useful for, um, for,
for data, um, that can be very
high-dimensional even when you have very few training examples.
So what I wanna do is talk a bit about properties of Gaussian distributions, and then um,
describe the factor analysis model, uh,
some more about Gaussian distributions and then we'll
derive EM for the factor analysis model.
And, uh, I want to talk about factor analysis for two reasons, because one is,
it's actually a useful algorithm in and of its own right.
And second the derivation for EM for
factor analysis is actually one of the trickier ones, and, uh,
there are key steps in how you actually derive the E and M steps that I think you
learn better or you better- master better by going through the factor analysis example.
Okay. Um, so just to recap,
last Monday or on Monday we had talked about the EM algorithm,
uh, and we wound up figuring out this E-step and this M-step, right?
And remember that if this is the log likelihood that you're trying to maximize,
what the E-step does is it constructs a lower bound uh,
that- this is a function of Theta.
So this thing on the right hand side,
this is a function of the parameters Theta.
And what we proved last time was that, um,
uh, that function is a lower bound of the log likelihood, right?
And depending on what you choose for Q,
you get different lower bound.
So one choice of Q you may get this lower bound,
for a different choice of Q you may get that lower bound.
For a different choice of Q you may get that lower bound,
and what the E-step does is it chooses Q to get the lower bound this tight,
that just touches the log likelihood here at the current value of Theta,
and what the M-step does is it chooses
the parameters Theta that maximizes that lower bound, right?
So that was the EM algorithm that we saw.
Now um, I wanna step through how you would take this, you know,
slightly abstract mathematical definition of EM
and derive a concrete algorithm that you would implement, right?
In, in, in, in, you know, um, in Python.
And so let's, let's just step through this for the mixture of Gaussians model.
Um, so for the mixture of Gaussians model we had a model for P of x i,
z i which is P of x i given z i times p of z i, right?
Um, and our model was that z is multinomial with some set of parameters phi,
and so, you know, the probability of z i to be equal to j is equal to phi j, right?
So phi is just a vector of numbers that sum to 1
specifying what is the chance of z being each of the,
um, k possible discrete values.
And then we have that x i given z i equals j,
that, that is Gaussian with some mean and some covariance, right?
And what we said last time was that, um,
this is a lot like the Gaussian discriminant analysis model,
uh and uh the, the,
the trivial- one trivial difference is this is Sigma j instead of Sigma, right?
GDA, Gaussian discriminant analysis,
had the same Sigma every class but that's not the key difference.
The key difference is that in, um,
this density estimation problem,
z is not observed or z is a latent random variable, right?
Which is why we have all this machinery of, um, of EM.
So now that you have this, um, uh,
model, um, let's see.
So now that you have this model, um,
this is how you would derive the E and the M steps, right?
So the E-step is, you know,
you have Q i of z i, right?
But let, let me just write this as Q i of z i equals j. Thi- this is sort of
the probability of z i equals j. I
know this notation's a little bit strange but under the Q i distribution,
whether you want the chance of z being equal to j, right?
And so, um, in the E-step you would say that the p of z i equals
j given x i parameterized by all of the parameters.
And we actually saw with Bayes' rule, right,
how you would flesh this out, okay?
And what we do in the E-step is,
um, store this number, right?
In, uh, what we wrote as w i j last time, okay?
So you remember, um,
if you have a mixture of two Gaussians, maybe that's the first Gaussian,
that's the second Gaussian,
you have an example x i here so it looks like it's more likely to come from the
first than the second Gaussian and so this would be reflected in w i j.
That, that example is assigned more to the first Gaussian than to the second Gaussian.
So what you implement in code is, you know,
you write code to compute this number and store it in wij.
Um, and then for the M-step,
you will want to maximize over the parameters of the model, right?
Phi, mu, and Sigma,
these are the param- uh pa-parameters of the mixture of
Gaussians of sum over i, sum over z i,
right? Um, and so the way you would actually
derive this is you write this as sum of i. Um,
so z i, you know,
takes on a certain distribution of values.
So z i we tu- turn,
turn z i into j, right?
So z I can be I guess one or two,
if you have a mixture of two Gaussians.
So you sum over all the indices of the different clusters of w i j times log of,
uh, the numerator is,
um, going to be
into the negative one half
times phi j,
um, that's the numerator.
And so, you know,
this term is equal to,
um, this first Gaussian term times that second term right,
because this term is p of xi um, given z i, right?
And the parameters, and this is just p of z i. Does that make sense?
Okay. Um, and then we ta- take this and divide it by w i j, okay?
So I'm, I'm going to step you through the, the,
the steps you would go through if you're deriving EM using that,
you know, E-step and M-step we wrote up above.
But if you're deriving this for the mixture of Gaussians model then these are the,
um, steps of algebra, right?
You would, you would take, okay?
Sorry I'm just realizing that.
So in order to perform this maximization,
what you will do is, um,
you want to maximize this formula, right?
This big double summation with respect to each of the parameters phi, mu, and Sigma.
And so what you would do is, you know,
take this big formula, right?
And take the derivatives with respect to each of the parameters.
So you take the derivative with respect to mu j, dot, dot,
dot there's that big formula on the left,
set it to 0, right?
And take, uh, and then,
and then it turns out if you do this,
um, you will, uh,
derive that mu j should be equal to sum over i to
the i j x i over sum over i to the i j.
And, um, this is what we said was how you update the mean's mu, right?
The w i j's are the strength with which x i.
So w i j is the,
informally this is the strength with
which x i is assigned, right?
To Gaussian j, um,
and more formally this is really p of, um,
z i equals j given x i and the parameters, right?
And so, um, so you end up with this formula.
But the way you compute this formula is by the, the,
the rigorous way to show this is the right formula for updating mu j,
is looking at this objective, taking derivatives,
saying they're zero, zero to maximize, um,
and therefore deriving that equation for mu j, you know, by,
by, by solving for the value of mu j that maximizes this expression, right?
And, uh, similarly, you know,
you take derivatives respect of,
of, of this thing,
with respect to that phi and set it to 0,
take derivatives of this thing, um, right?
And set that to 0 and that's how you would derive
the update equations in the M-step for phi and for Sigma as well.
Okay? Um, so and, and so,
for example, when you do this,
you find that the optimal value for phi is, um.
Let's see. Yeah, yeah.
We had this at the - near the start of Monday's lecture as well, okay?
Um, so this is a process of how you would look at how the
E-steps and M-steps are relative, and apply it to
a specific model such as a mixtures of Gaussians model and that's how you,
you know, solve for the maximization in the M-step, okay?
And so what I'd like to do today is describe the application of EM.
It's a more complex model called the factor of analysis model,
and so it's important that - so I hope you understand the mechanics of how you do this,
because we're going to do this today for a different model, okay?
Any questions about this before I move on?
Okay.
Cool.
Oh, so in order to,
you know, foreshadow a little bit what we'll see when it comes
down to the mixture of Gaussians model, excuse me.
The Factor Analysis model which we talked about,
you know, which is what we'll spend most of the day talking about.
In the factor analysis model,
instead of zi being discrete,
zi will be continuous, right?
And the particular zi will be distributed Gaussian.
So the mixture of Gaussians model we had a joint distribution for x and z,
where x was a discrete random variable.
So in the factor analysis model we'll,
we'll describe a different model.
You know, for p of x and z,
where z is continuous.
And so instead of sum over zi, this would be an integral over zi of dzi, right?
So - so sum becomes an integral.
And - and it turns out that, yeah. Well, right.
Yeah. And - and it turns out that if you go through
the derivation of the EM algorithm that we worked out on Monday,
all of the steps with Jensen's inequality, all of those steps work exactly as before.
Meaning you check every single step for whether
zi was continuous it'll work the same as before
if you have changed the sum to an integral, okay? All right.
So let's see.
So I want to mention one other view of EM
that's equivalent to everything we've seen up until now which is,
let me define j of theta, Q as
this, okay? So is that formula you've seen a few times now.
What we proved on Monday,
was L of theta
is greater than or equal to J of theta, Q right?
And this is true for any theta and any choice of Q, okay?
So using - using Jensen's inequality,
you can show that, you know,
J for any choice of theta and Q is a lower bound for the log likelihood of theta.
So it turns out that an equivalent view of EM as everything we've seen before,
is that at an E-step,
what you're doing is maximize J with respect to Q and in
the M-step maximize J
with respect to theta, right?
So in the E-step you're picking the choice of Q that maximizes this,
and it turns out that the choice of Q we have we'll set J equal to L,
and then in the M-step maximize this with respect to
theta and pushes the value of L even higher.
So this algorithm is sometimes called coordinate ascent.
If you have a function of two variables and you optimize with respect to this,
and also with respect to this,
then go back and forth and optimize with respect to one at a time.
That - that's the procedure that sometimes we call coordinate ascent,
because you're maximizing with respect to one coordinate at a time.
And so EM is a coordinate ascent algorithm relative to this course function J, right?
And - and, you know,
and every iteration J ends up being sent to L which is
why you know that as the algorithm increases J,
you know that the log-likelihood is increasing with every iteration and if you
want to track whether the EM algorithm is converging or how it is converging,
you can plot, you know,
the value of J or the value of L on successive iterations and see if
its validated, scoring
monotonically and then when it plateaus and isn't improving anymore,
then you might have a sense that the algorithm is converging, okay?
All right. Okay. So that's it.
The, basically an algorithm and a mixture of Gaussians.
What I want to do now is - and is going
to talk about the factor analysis algorithm. All right.
So you know, that the factor analysis algorithm will work, actually sorry.
So I want to compare and contrast mixture of
Gaussians with factor analysis we're talking about a little bit,
which is - for the mixture of Gaussians,
let's say n equals 2 and m equals 100, right?
So you have a data set with two features x 1 and x 2.
So n is equal to 2 and maybe you have a data set that looks like this.
You know, there's a mixture of two Gaussians.
We have a pretty good model for this data set, right?
Can fit one Gaussian there,
fit the second Gaussian here.
You kind of capture a distribution like this with a mixture of two Gaussians.
And this is one illustration of when,
when you apply mixture of Gaussians in this picture,
m is much bigger than n, right?
You have a lot more examples than you have dimensions.
Where I would not use mixture of Gaussians
and where you see the minute factor analysis will apply.
Maybe if m is about similar to n,
I don't know, even n is - or even m is much less than n, okay?
And so just for purpose of illustration let say
m equals 30 and n equals 100, right?
So let's say you have 100 dimensional data but only 30 examples.
And so to - to make this more concrete, you know,
many years ago there was
a Stanford PhD student that
was placing temperature sensors all around different Stanford buildings.
And so what you do is model,
you measure the temperature at many different places, right? Around campus.
But if you have 100 sensors,
you know, taking 100 temperature readings around campus.
But only 30 days of data or maybe 30 examples,
then you would have 100 dimensional data because
each example is a vector of 100 temperature readings,
you know, at different points around this building say.
But you may have only 30 examples of - of - if you have say 30 - 30 such vectors.
And so the application that the Stanford PhD student at the time was working on,
was she wanted to model p of x, right?
So this is x as a vector of 100 sensors, 100 temperature readings.
Because if something goes wrong or for
example because a bad case would be if there's a fire in one of the rooms,
then there'll be a very anomalous temperature reading in one place.
And if you can model p of x and if you observe a value of p of x that is very small.
You would say oh it looks like this anomaly there, right?
And we're actually less worried about fires on Stanford.
The use case was actually a - a- was it energy conservation.
If someone unexpectedly leaves the window open in the building you are studying,
you know, and it was hot and was it,
and it's winter and it's warm inside the building and cool air blows in,
and the temperature of one room drops in an anomalous way, you want
to realize if something was going wrong with the windows,
or the - or the temperature in part of the building, okay?
So for an application like that,
you need to model p of x as a joint distribution over,
you know, all of the different senses, right?
If you imagine maybe just in this room,
let say we have 30 sensors in this room,
then the temperatures at
the 30 different points in this room will be highly correlated with each other.
But how do you model this vector of
a 100 - 100 dimensional vector with a relatively small training set?
So it turns out that the problem with applying a Gaussian model.
Well, all right.
So one thing you could do is model this as a single Gaussian.
And say that x is distributed, right?
And if you look in your training set of 30 examples and find
the maximum likelihood estimate parameters, you find that
the maximum likelihood estimate of mu is just the average.
And the maximum likelihood estimate of sigma is this,
but it turns out that if m is less than equal to n,
then sigma, this covariance matrix will be singular.
And singular just means,
uh - non-invertible, okay?
I'll set for another illustration in a second.
But, er, if you look at the formula for
the Gaussian density, right?
So the Gaussian density kind of looks like this, right?
Abstracting away some details.
And when a covariance matrix is singular,
then this term, this determinant term will be 0.
Um, so you end up with one over 0.
Um, and then sigma inverse is also undefined or,
er, blows up to infinity it will depending on how you think about it.
Right but so, you know the inverse of a matrix like,
um, 1, 10, right?
Would be I guess one,
1 over 10, right?
And, er, an example of a non-invertible matrix or singular matrix would be this,
and you can't actually calculate the inverse of that matrix, right?
So it turns out that, um,
if your number of training examples is less than the dimension of the data,
if you use the usual formula to derive the maximum likelihood estimate of Sigma,
you'll end up with a covariance matrix that is singular.
Uh, singular just means non-invertible,
which means our covariance martix looks like this.
And so, you know, the Gaussian density,
if we try to compute p of x you get- you kind of get infinity over 0.
You see, right? Oh, sorry not infinity,
actually 0 over 0.
Sorry, right. It doesn't matter, it's all bad.
Um, and I think- let me just illustrate what this looks like.
Which is, um, let's say m equals 2,
and n equals 2, right?
So you have two-dimensional data x1 and x2,
and, um, uh, so n equals 2,
and the number of training examples is equal to 2.
So it turns out that, um- let's see,
so you see me draw contours of Gaussian densities like this, right?
Like ellipses like that.
It turns out that if you have two examples, a two-dimensional space,
and you compute the most likely- maximum likelihood estimate
of the parameters of the Gaussian to fit to this data,
then it turns out that these contours will look like that, right?
Um, except that, instead of being very thin,
as I'm drawing it, it will be,
it will be infinitely skinny.
So you end up with a Gaussian density where I can't draw lines,
you know, of 0 width on the whiteboard, right?
Um, uh, but it turns out that the contours will be squished infinitely thin.
So you end up with a Gaussian density all- all of whose mass is
on the straight line over there with infinitely thin contours that,
that they're just, you know,
we squish the centers on the,
on the plane that goes on the line,
um, connecting these two points.
And so this is- so first there are,
uh, practical numerical problems, right?
As you end up with 0 over 0 if you try to compute p of x for any example.
And second, um, this is-
this very poorly conditioned Gaussian density puts
all the probability mass on this line segment and so any example, right?
Over there, just a little bit off,
has no probability mass because,
oh, has a probability mass of 0, a probability density of
0 because the Gaussian is squished infinitely thin,
you know, on that, on that line, okay [NOISE].
But, but you can tell, this is just not
a very good just- this is not a very good model, right?
For, for this data.
Um, So what we're gonna do is, ah,
come up with a model that will work even for,
um, for these applications,
even, even for a dataset like this, right?
Um, there's actually a, uh- I think the,
the origins of the factor analysis model, uh,
one of the very early applications was actually a psychological testing.
Um, where, uh, if you have a, you know,
administer a psychology, um, ah,
exam to people to measure different personality attributes, right?
So you might measure- you might have 100 questions or measure 100, uh,
psychological attributes,
um, but have a dataset of 30 persons, right?
And again, you know, doing, doing psych research,
collecting, you know, assembling survey data is hard.
We assume you have a sample of 30 people and each person answers 100 quiz questions.
Um, and so each person is one- gives you one example, right?
X, and the dimension of this is,
um, 100 dimensional, we have only 30 of these.
And so if you want to model p of x,
try to model how correlated are the different psychological attributes of people, right?
Oh, is intelligence correlated with math ability, is that
correlated with language ability, is that correlated with other things,
uh, then how do you build a model for p of x, okay?
All right. So, um,
if the standard Gaussian model doesn't work,
let's look at some alternatives.
Um, one thing you could do is, uh,
constrain
Sigma to be diagonal, right?
So Sigma is a covariance matrix,
is an n by n covariance matrix.
So in this case, it would be a 100 by 100 matrix.
Um, but let's say we constrain it to just
have diagonal entries and 0s on the off diagonals, right?
So these giant 0s, I mean,
the diagonal entries of this square matrix are these values in
all of the entries of the diagonals you set to 0.
So that's one thing you could do.
And this turns out to be,
um- this turns out to correspond to
constraining your Gaussian to have axes align contours.
So this is a Gaussian with 0 off-diagonals.
Um, this would be another one, right?
This would be another one.
So these are examples of Gaussian- of,
of contours of Gaussian densities with,
um, 0 off diagonals.
So the axes here are the X1 and X2, right?
Whereas you cannot model something like
this if your off diagonals are, are 0.
Um, and so you do this,
the maximum likelihood estimate of the parameters Sigma j,
is pretty much what you'd expect actually.
Right. The maximum likelihood estimate of the mean vector mu is the same as before.
And this is maximum likelihood estimate of Sigma j, right?
This kind of knowledge should be no surprise, it's kind of what you'd expect.
Uh, and it turns out that, uh,
right- and, and so the covariance matrix here has n parameters,
instead of n squared or about n squared over two parameters,
the covariance matrix Sigma now just has n parameters,
which is the n diagonal entries.
Now, the problem with this is that,
this modeling assumption assumes that all of your features are uncorrelated, right?
So you know, this just assumes that
any two features they kind of share are, are completely uncorrelated.
And, um, if you have temperature sensors in this room,
it's just not a good assumption to assume
the temperature at all points of this room are completely uncorrelated,
completely independent of each other,
or if you measure, you know,
psychological attributes of people,
it's just not a great assumption to assume that, you know,
the different- different psychological measures you
might have are completely, um, uh, independent.
So while this model would take care of the problem, the,
the technical problem of the covariance matrix
being singular you can fit this model [NOISE],
um you know, on a,
on 100 dimensional dataset with 30 samples.
You can fit this, you won't get this- you could build this model,
you won't run into numerical singular,
um, covariance matrix type problems,
it's just not a very good model where you're just assuming nothing is
correlated to anything else
[NOISE].
Something else that you can do is, um,
uh, make an even stronger assumption.
So this is an even worse model,
but I want to go through it because it will be
a building block for what we'll actually do later,
which is constrain Sigma to be Sigma equals,
um, lowercase Sigma squared times i, right?
And so, um, constrain Sigma to be dia- not only
diagonal but to have the same entry in every single element.
So now you've gone from, um,
I guess n parameters to just one parameter, right?
Uh, and this means that you are constraining
the covariance matrix to- you
are constraining the Gaussian you use to have circular contours.
So this is an example where you can model.
Uh, and this would be another example, right?
And this is- I guess this is another example, okay?
So you can model things like this, where every feature,
not only is every feature uncorrelated but
every feature further has the same variance as every other feature.
Um, and the maximum likelihood is
this, okay?
And again, not, not, not a huge surprise,
just the average over,
uh, the previous values.
So what we'd like to do is,
um, not quite use either of these options, right?
Which assumes- really, the biggest problem is it assumes the features are uncorrelated.
Um, and what I'd like to do is build the model that you can fit even when you
have very high dimensional data and a relatively small number of examples,
um, but that allows you to capture some of the correlations, right?
So if you have 30 temperature sensors in this room,
you know, probably there are some correlations, right?
Probably, this side of the room temperature is gonna be correlated,
and that side of the room temperature is gonna be correlated
and maybe the ambient temperature in this whole building.
The, the temperature of this room really goes up and down as a whole,
but maybe some of the lamps on the side heat up
that side of the room a bit more, so different, the different.
There are correlations but maybe you don't need a full covariance matrix either.
So what [NOISE], what factor analysis will do is, um,
give us a model that you can fit even when you have,
you know, [NOISE] 100 dimensional data and 30 examples.
They capture some of the correlations but that doesn't run into the a,
a- uninvertible, um,
covariance matrixes is that the naive Gaussian model does, okay?
All right. So let me- just check any- let me,
let me describe the model, let me just check,
any questions before I move on? Okay.
[BACKGROUND]
Oh, sure. Yes. Um, yes.
There is one thing you can do.
A common thing to do is apply Wishart prior and what that boils down to is,
um, add a small diagonal value to that- to the maximum likelihood estimate.
Um, it- it kind of, uh,
in a technical sense it takes away the,
uh, non-invertible matrix problem.
Uh, it's actually not the best algorithm for a lot of the types of data.
Um, uh, the- the- the- the Wishart or inverse Wishart prior, yeah.
Others, you know- basically,
take the maximum likelihood for Sigma,
and add, you know,
some constant to the diagonal.
Um, it takes care of the problem in a technical way,
but it is not- it's not the best model for a lot of datasets, I see.
Why do we even think about option two [inaudible]
Oh, yes. Why do you think about option two,
when it's likesemi even worse than option one. Um, yes.
Option two is not a good option,
but I need to use this as a building block for factor analysis.
So you see this is a small component of, uh, of, uh,
see I actually planned these things out,
you know. [LAUGHTER]. Cool, yeah.
And- and- and maybe- actually to- to- to give [inaudible] just- just to mention,
you know, um, just mention some things I see.
Yeah. Actually the- the machine learning work evolves all the time,
which I find fascinating.
But you look at all the big tech companies, um,
a lot of the large tech companies,
they're all like working on exactly the same problems, right?
Every large tech company, you know,
software, AI company, is working on machine translation,
every one of them works on speech recognition,
every one of them works on face recognition,
and I- I- I've been part of these teams myself.
Right? And I think it's great that we have so much progress in machine translation,
because there are so many people,
and so many large companies that work on machine translation.
It's actually really happy to see so much progress in these
problems that every single large tech company,
large software, AI-ish tech company works on.
Um, one of the fascinating things I see is that, um, uh,
because of all this work into large tech companies working on very similar problems,
one of the really overlooked parts of
the machine learning world is small data problems, right?
So there's a lot work in big data if you are Brazilian, English,
and French, and Chinese,
and Spanish sentences is the semi-close models that work.
Um, and I think, uh,
uh, there's actually a lack of attention,
like a disproportionately small amount of attention, on, you know,
small data problems, where instead of,
uh, 100 million images, you maybe have 100 images.
Um, and so, uh,
some of the teams I work with these days,
actually like Landing AI.
Um, I actually spent a lot of my time thinking about small data problems,
because a lot of the practical applications of machine learning,
including a lot of things you see in your class projects,
are actually small data problems.
Right? And I think, um, when- when Annan, uh,
worked with a healthcare system, works at Stanford Hospital,
for some of the problems, you only have 100 examples,
or even 1,000, or even 10,000.
You don't have a million patients with the same medical condition.
And so I think that, um, uh,
a lot of these models- So- and again,
uh, earlier this week,
I was using a slightly modified version of
factor analysis on a manufacturing problem at Landing AI.
Right? And I think a lot of these small data problems are actually
where a lot of the exciting work is to be done in machine learning,
and is somehow- it- it feels like a blind spot of- or if like a- like a- like a gap of,
uh, a lot of the work done in the AI world today. Go ahead.
[inaudible].
Uh, yeah. Why don't we use the same algorithms with this big data?
It turns out that, um,
uh, you know, it turns out- if- if- if you look at the computer vision world, right?
There's a data set that everyone is working on.
Now- now we're past it,
we don't really use it any more,
called ImageNet, which had a million images,
and so there are tons of computer vision architectures that have been heavily
designed for the use case of if you have exactly one million training examples.
Uh, and it turns out that the algorithms that work
best if you have 100 training examples is,
you know, looks like it's different than the best learning algorithm.
I think, um, uh,
uh, and so I think right now,
we actually- I think the machine learning world,
we are not very good at understanding the scaling.
Uh, the best algorithm for one training example,
you know, as far as we are able to invent algorithms as a community,
is different than best algorithm for 1000, best for, for a million,
it's actually different than, um, uh,
uh- actually, and Facebook published a paper recently,
with 3.5 billion images.
The result was cool, it was very large, right?
So I was saying, we don't actually have
a good understanding of how to modify our algorithms,
to have one algorithm work on every single point of this spectrum,
going from one example to,
like, a billion examples.
Uh, and so there's a lot of work optimizing for different points of the spectrum,
uh, and I think there's been, um,
a lot of work optimizing for big data, which is great, you know,
build some of these large systems that handle, like,
whatever, petabytes of data a day, uh, that's great.
But, um, uh, I feel like relative to the number of, um, application opportunities,
there- there's a lot of work on small data well,
that- that I find very exciting,
that- that, uh, and I think of this as an example.
Uh, the reason I was using this,
literally, well, modified version of this,
earlier this week on the manufacturing problem, um,
is because, um, uh,
there isn't that much data in those scenarios, right?
Cool. All right.
That's, um, off-topic.
But let's- let's- let's go and describe- well, hopefully,
maybe so, so this stuff does get used, right?
Uh, so let's- let's talk about the model.
Um, so similar to,
uh, the mixture of Gaussians,
I'm gonna define a model with,
um, P of X,
Z equals P of X,
given Z times P of Z,
uh, and Z is hidden.
Okay? So that's the framework,
same as, um, mixture of Gaussian.
So let me just define the factor analysis model.
So first, um, Z will be drawn- distributed according to the Gaussian density,
where Z is going to be an RD,
where D is less than N. And again,
to think about it, um, maybe you can think of it as,
uh, D equals 3, uh,
uh, N equals 100, M equals 30.
Okay? Um, and- and- but I guess,
ju- just make sure this is a concrete example to think about it.
And what we're going to assume is that X is equal to Mu,
plus, um, Lambda Z.
This is, uh, the capital Greek alphabet Lambda, plus Epsilon,
where Epsilon is just using Gaussian with mean 0, and covariance Psi.
Um, so the parameters of this model are Mu which is N dimensional, um,
Lambda which is N by D,
and Psi which is N by N,
and we're going to assume that Psi is a diagonal.
Okay? Um, and so- let's see.
The second equation, an equivalent way to write that, equivalently,
is that given the value of Z,
the conditional distribution of X, right, X given Z,
this is Gaussian with mean given by Mu plus,
um, Lambda Z, and covariance Psi.
Okay? So once you've given Z- once you sample
Z- so this is P of Z and this is P of- P of Z and this is P of X,
Z- X given Z.
Right? So given Z,
X is computed as Mu plus Lambda Z.
So this is just some constant,
and then you add Gaussian noise to it.
And so this equation, an equivalent way to define this equation,
is to say that the mean of X, uh,
conditioned on Z, is this first term.
Right? Since that's the mean.
And the covariance of X given Z,
is given by this, you know,
additional term Psi, by that noise term that you add to it.
Okay? So let me go through a few examples.
And- and I think the intuition behind this model is, um,
if- if you think that there are
three powerful forces driving temperatures across this room,
maybe one powerful force is just what is the temperature,
you know, here in Palo Alto,
what's the temperature here at Stanford.
And another powerful force is how bright are the lights on the left side of the room,
and how hot does it heat up this side of room,
and another is how hot does is it heat up the right side of the room.
Right? So, you know, let's say there are
three main driving factors affecting the temperature of this room,
then that's when D would be equal to 3.
Then you assume that, you know,
there are three things in the world that drive
the temperature of this room that's three-dimensional,
which is the temperature in Palo Alto,
kind of, around this area, um,
how bright that the light is there,
and how bright that the light is there,
and you try to capture that with three numbers.
Given those three numbers, right?
Given Z, the actual temperature for the 100 sensors we scatter around this room,
will be determined by each sensor, right?
So we plug 30 temperature sensors all over this room.
Each sensor we plant will measure an actual temperature,
that's a linear function of those three powerful forces, um,
and if a sensor is on that side of the room,
it'll be affected more by how bright that the lights are on that side of the room.
Um, uh, if there's a sensor near the door,
it will be more affected by the temperature outside- temperature here in Palo Alto.
Right? But so X will be a linear function,
but this first time I underlined.
Um, but rather than just that term,
there is [inaudible] noise.
Right? So each sensor has its own noise term,
which is governed by this additional noise term Epsilon.
And, um, the assumption that this matrix Psi is diagonal,
it's saying that after you compute the mean,
the noise that you observe at each sensor
is independent of the noise at every other sensor.
Does that make sense? Right? That maybe- maybe the sensor,
you know, up there, right?
Maybe it's just noisy or something, just a gust of wind.
But you assume that the noise of, you observe at different sensors is independent.
The- the additional Epsilon error term has a-
has a diagonal covariance matrix given by Psi.
Okay? So you can- so you can think of that as what,
um, uh, factor analysis is trying to model.
Okay? So let me, um,
just go through a couple of examples of the types of data factor analysis can model.
All right, and again by the constraints of the whiteboard,
I'm going to have to go low-dimensional here, right?
Um, so actually let me- let me go through a couple examples.
So let's say Z is R_1 and X is R_2.
So in this example I guess d is equal to 1,
n is equal to 2 and let's say m is 7, right?
just- just. So what will be a typical example,
generated by- what will be an example of a type of data that this can model?
So this, let me erase this here. All right, so
this would be a typical sample of Z_i right?
which is you know- so this is z is just drawn from a standard Gaussian.
So I guess z is just Gaussian, would mean 0 and unit variance.
So that's the number line and you draw seven points from a Gaussian,
you know, maybe you get a sample like that.
Okay? and now let's say lambda is
2, 1 and let's just say mu is 0, 0, okay?
So now let's compute
lambda x plus mu, right?
so given a typical sample like that um,
if you compute lambda x plus mu,
this will now be the R_2, right?
so here is X_1, here is X_2.
We're gonna take those examples and map them to a line as follows.
Where these examples on R_1.
So- excuse me, lambda z plus mu, okay.
So this is just a real number and so lambda z plus mu is now two-dimensional,
right? Because lambda is a 2 by 1 matrix.
Okay? so you end up with- So this would be
a typical sample- typical random sample of lambda z plus mu and
it's a two-dimensional data-set but all of the examples lie perfectly on a straight line.
Okay? Then finally let's say that psi, the covariance matrix is equal to
this as a diagonal covariance matrix and
so this covariance matrix corresponds to X_2 having a bigger variance than X_1, right?
And so you know this,
this- I guess the density of epsilon has ellipses that look a little bit like this,
it's taller than wide.
The aspect ratio should technically be 1 over root 2 to 1, right?
Because the standard deviations will be root 2, I guess.
And so in the last step of what we are going to do,
x equals lambda z plus mu plus epsilon.
We're going to take each of these points we have and put
a little Gaussian contour. You know there's that shape.
There's this- I'm just drawing one contour of the shape and just put it on top of this,
and if you sample one point from each of these Gaussians,
then maybe you get this example, this example,
this example, this example, okay?
So what I just did was look at each of the Gaussian contours and
sample a point from that Gaussian.
And so the red crosses here are a typical sample drawn from this model.
Okay? and so if you have data that looks like this,
that looks at the red crosses.
The Zs are latent random variables, right?
When you get the dataset you kind of just see Zs.
So what you actually see,
is just you know the red crosses,
that's your training set and if you apply
the factor analysis model with these parameters then you can find EM and so on.
Hopefully you can find parameters that models this dataset pretty well,
but hopefully this gives you sense of the type of dataset this could generate
and so- and so on.
And one way to think of this data is you have
two-dimensional data but most of the data lies on a 1D subspace.
So this is how to think about it,
you have two-dimensional data since n is two.
But most of the data lies on
a roughly one-dimensional subspace meaning it lies roughly on a line,
and then there's a little bit of noise off that line, okay?
All right, let me quickly do one more example
because these are- these are high-dimensional spaces.
I think it's- I think it's useful to build intuition.
All right, so let's go through an example where z is in R_2,
x is in R_3 and let's use m equals 5.
So d equals 2,
n equals 3, okay?
So we have a different set of parameters.
Let's look at the type of data you can generate a factor analysis which is,
here is Z_1 and Z_2.
Z is distributed Gaussian,
standard Gaussian 2D so it would be a circular Gaussian.
So maybe this is what the typical sample, right, looks like.
If you- if you if you sample sort of Z_1 and Z_2 from a standard Gaussian,
right that would be a typical sample in Z_1 and Z_2.
So now- all right, I'm going to do a demo.
Let me take these five examples and just copy them to this piece of paper, okay?
So, all right there, right?
Transferred it from the whiteboard to this piece of paper,
to this brown cardboard.
So now you have Z_1 and Z_2 in a two-dimensional space.
What we're going to do is compute lambda z plus mu,
and this will be 3 by 2,
and this will be 3 by 1.
So what this computation will do as you map from z in two-dimensions to lambda z plus mu,
is you're going to map from two-dimensional data to three-dimensional data.
In other words, you want to take the two-dimensional data lying on the plane in
the whiteboard, and map it, check out
this cool animation into the three-dimensional space
of our classroom
[LAUGHTER].
And then the last step is for each of
these points in this three-dimensional space like X_1 X_2 X_3, right?
We'll have a little Gaussian bump that is axis
aligned because epsilon is the features, the-
the components of epsilon are uncorrelated and
taking each of these five points and add a little bit of fuzziness,
add a little bit of Gaussian noise to it.
And so what you end up with is a set of red crosses and
you end up with a few examples, you know add a little bit of noise,
you end up with- except that they
would have a bit of noise off this plane as well, right?
But so what the factor analysis model can capture is if you have data in 3D, right?
In this 3D space,
but most of the dataset lies on this
maybe roughly two-dimensional pancake but there's a little bit of fuzziness off
the pancake, right, so this would be
an example of the type of data that factor analysis can model.
Okay? All right cool.
Um, and the intuition is really think
of factor analysis can take very high dimensional data,
say, 100 dimensional data and model the data as roughly lying on a three-dimensional,
five dimensional subspace with a little bit of fuzz,
with a little bit of noise off that low dimensional subspace.
Great.
So- [NOISE]
All right. So let's talk about- yeah.
[BACKGROUND]
Oh, right. It does not work as well if the data's not
lying on low dimensional subspace. Um, let's see.
So even in 2D,
if you have, um, this data set, right?
[NOISE] You actually have the freedom to choose Gaussian noises like that,
in which case you can actually model things that are quite far off a subspace.
Uh, but, um, uh,
yeah, I, I, I, you know,
I think when you have a very high dimensional data set,
it's actually very difficult to know what's going on because you
can't visualize these very high dimensional data sets,
uh, and you also don't have enough data to build very sophisticated models.
So, so I feel like yes,
if you have- if the data actually does not roughly lie in a subspace,
then this model, you know,
may not be the best model,
but when you have such high dimensional data in such a small data set, um,
you- is- you can't fit very complex models through it anyway,
so this might be pretty reasonable.
Right. Cool. All right.
So, um- [NOISE] all right.
So it turns out that the derivation of EM for factor analysis is actually,
it's actually one of the trickiest EM derivations,
in terms of how you calculate the e-step,
and how you calculate the m-step.
Um, the whole algorithm is,
you know, describe the- every, every,
every single step, the- step three in great detail in the lecture notes.
But what I want to do is give you the flavor of how to do the derivation,
and to especially draw attention to the trickiest step,
so that if you need to derive an algorithm
like this yourself for maybe a different Gaussian model,
then you know how to do it,
but I won't do every step of the algebra here.
All right? Um, so in order to set ourselves up to derive factor analysis, uh,
EN- ENM for factor analysis,
I wanna describe a few properties of, uh, multivariate Gaussians.
So [NOISE] let's say that X is a vector,
and I'm gonna write this as a partition vector, right?
In which, um, uh,
[NOISE] if there are R components there,
and S components there.
So [NOISE] X_1 is in R_r,
X_2 is in R_S,
and X is in R_ r plus S. Okay?
So if X is Gaussian with mean Mu and covariance Sigma,
then, uh, let- similarly,
let Mu be written as this sort of partition vector.
Right? Just break it up into two sub-vectors,
corresponding to the first R components in the second S components.
And similarly, let the covariance matrix be partitioned into, um,
you know, these four diagonal blocks,
where, I guess, this is R components,
this is S components,
this is R components,
this is S components.
Um, so all this means is,
uh, you take the covariance matrix,
and take the top leftmost R-by-R elements,
and call that Sigma 1, 1.
Right? And, and, uh, and, and,
and then similarly for the other sub-blocks of this, um, covariance matrix.
So in order to derive factor analysis,
one of the things you need to do is compute marginal and,
um, uh, conditional distributions of Gaussians.
So the marginal is,
[NOISE] you know, what is P of X_1.
Right? Um, and so the,
the- if you, you know,
were to derive this, uh,
the way you compute the marginal is to take the joint density [NOISE] of P of X, right?
And you can write this as P of X_1 X_2,
because X can be partitioned into X_1 and X_2,
and integrate out X_2 under P of X_1 X_2, right?
Dx_2, and this will give you P of X_1.
Right? And if you plug in the Gaussian density,
the formula for the Gaussian density,
if you plug in, I guess, you know,
1 over 2 Pi to the N over 2,
is equals to one-half, right?
E to the, you know,
minus one-half, X1 minus Mu 1,
X_2 minus Mu 2,
uh, right?
If you plug this into P of X_1,
X_2, and actually do the integral, um,
then you will find that, um,
the marginal distribution of X_1 [NOISE] is given by;
X_1 is usually a Gaussian,
with mean Mu 1,
and covariance sigma 1, 1.
So it- it's, kind of, not a shocking result,
that the marginal distribution is given just by that and that.
Right? And, and again, the way to show it vigorously is to do this calculation,
[NOISE] but it's actually not shocking,
I guess, that that's what you would get.
Okay? Um, and then the other property you will [NOISE] need to use is a conditional,
which is, um, [NOISE] given the value of X_2,
what is the conditional value of X_1?
Um, and so the way to do that would be,
you know, [NOISE] in theory, you would take P of X_1,
X_2 divide by P of X_2,
right? And then simplify.
And it turns out you can show that, um,
[NOISE] X_1 given X_2, is itself Gaussian,
[NOISE] with some mean and some covariance,
we're just gonna write this Mu of 1 given 2 and Sigma of 1 given 2,
where Mu of 1 given 2 is,
uh- and, and- but this is one of those formulas that I
actually don't- I actually don't manage to remember,
but every time I need it I just look it up.
It's written in the lecture notes as well.
So um, [NOISE] X_2
minus 2 to-
oops.
Okay? [NOISE] So that's how you compute,
um, marginals and conditionals of a Gaussian distribution.
Okay? So [NOISE]
using these properties of,
uh, the multivariate Gaussian density,
let's go through the high-level steps of how you derive the EM algorithm for this.
[NOISE] All right. [NOISE] Um,
step one is, uh,
let's compute- actually, let's, um- excuse me.
[NOISE] Let's derive what is the joint distribution of P of X and Z.
Right? And in particular,
it turns out [NOISE] that if you take Z and X and
stack them up into a vector like so, um,
Z and X viewed as a vector would be Gaussian with mean,
um- [NOISE] with some mean and some covariance,
uh, because X and Z jointly will have a Gaussian density.
And let's try to quickly figure out what are this mean and that covariance matrix.
[NOISE] So that was a definition of these terms.
Um, and so the expected value of Z is equal
to 0 because 0 is- Z is Gaussian with mean 0 and covariance identity,
[NOISE] and the expected value of X is equal to the expected value of Mu plus Lambda Z,
plus epsilon, um, but Z has 0 expected value,
epsilon has 0 expected value,
so that just leaves you with Mu.
And so this mean vector Mu XZ,
is going to equal to 0 Mu.
Right. And so this is D-dimensional,
[NOISE] and this is, uh, N-dimensional.
Okay? Um, and it turns out that,
uh, [NOISE] let's see-
and it turns out that you can [NOISE] similarly compute
the covariance matrix Sigma, right?
Where this is, um,
D dimensions and this is N dimensions.
Um, [NOISE] it turns out that if you take this partition vector,
and compute the covariance matrix,
[NOISE] the four blocks of the covariance matrix can be written as follows. [NOISE] Um-
Okay. And you can,
one at a time, derive what each of these different blocks look like.
Um, and let me just do one of these,
and let me just derive what Sigma 2, 2,
the lower right block is and the rest are
derived similarly and also fleshed out in the lecture notes.
So the way you derive what this block is like is that you say Sigma 2, 2 is x minus Ex,
x minus Ex transpose.
And so if I plug in the definition of x that would be a Lambda z
plus Mu plus Epsilon minus Mu times the same thing.
Right.
Um, so there's x minus Ex.
So there's x minus Ex, okay?
Uh, because the expected value of x is Mu.
So the Mus cancel out.
And then if you do the quadratic expansion,
I guess this becomes expected value of, um, let's see,
Lambda z times
each of these two terms transpose plus- it,
it, it sort of, you know,
a plus b times a plus b, right?
It's a times a times a plus b,
b times a, b plus b.
You get four terms as a result.
And so the first term is Lambda z times Lambda z transpose, which is this,
plus Lambda z Epsilon transpose plus Epsilon,
um, right?
And so, um, this term has 0 expected value because,
uh, Epsilon and, and z,
both have zero expected value uncorrelated.
So this is zero.
This is zero on expectation.
And so you're just left with the expected value of Lambda zz transpose,
Lambda transpose plus the expected value of,
uh, Epsilon Epsilon transpose, right?
Um, and so by the linearity of expectation,
you can take expectation inside a ma- matrix multiplication.
So this Lambda times the expected value of zz transpose times Lambda transpose plus.
And this is just the covariance of Epsilon, right?
Which is- which is Psi.
Um, and then because z is drawn from a standard Gaussian with identity covariance,
that expectation in the middle is just the identity.
So that's Lambda, Lambda transpose plus Psi.
Okay. So that's how you work out what is
this lower right block of this, um, covariance matrix.
I know I did that a little bit quickly,
but every, every step is,
uh, written out, uh,
more slowly in the lecture notes as well.
Okay. And it turns out that if you go through a similar process to figure out,
you know, one at a time using similar process,
one of the other blocks of this covariance matrix,
you find that the other blocks of this covariance matrix are identity,
Lambda, Lambda transpose and the one we just worked out.
Okay. That- that's the one we just worked out.
But so that is the covariance matrix Psi.
[NOISE]
So where we are is that we've figured out that the joint distribution or
the joint density of z x is Gaussian with mean given
by that vector and covariance given
by that matrix, okay?
Um, and so what you could do, uh,
is, um, you write down, right?
P of x_i and try to take the- uh,
so P of x_i will be this Gaussian density.
And what you could do is take derivatives
of the log likelihood with respect to the parameters,
and set the parameters to 0 and solve.
And you find that there is no known closed-form solution.
There is actually no closed-form solution for finding the values of
Lambda and Psi and Mu that maximize this log-likelihood.
So in order to, uh,
fit the parameters of the model,
we're instead going to resort to EM, okay?
And so in the E-step.
Right.
So let's, let's first derive what is the E-step,
which is an E-step, you need to compute this, right?
Now, um, z_i here is a continuous random variable.
When we're fitting a mixture of Gaussian distributions, z_i was discrete,
and so you could have a list of numbers represented by, you know, w_ i_ j,
that just, just at the vector sorting what is
the probability of each of the discrete values of z_i.
But in this case, z_i is a continuous density.
So how do you represent Qi of z_i at a computer?
It turns out that using the formulas we have for the marginal- excuse me,
for the conditional distribution of a Gaussian,
it turns out that if you compute this right hand side,
you find that z_i given x_i,
this is going to be Gaussian with some mean
and some covariance, right?
Where- oh, it's basically those formulas,
Mu of z_i given x_i is
equal to- if you kinda of take that formula and apply it to our thing here,
a zero, uh, plus Lambda transpose.
And, um [NOISE] okay.
So these equations exactly, these two equations,
right, maps to- map to that big Gaussian Density that we have.
Okay. So what you would do in the E-step is,
um, compute this and compute this- compute this vector and compute this matrix,
and store that- store these,
you know, store these as variables,
and your representation of the Q_i is that Q_i is a Gaussian Density,
right, with this mean and this covariance.
So this is what you actually compute to represent Q_i.
Okay.
[NOISE]
All right.
So step two was derive the E-step,
and step three is derive the M-step.
[NOISE] and, um, the derivation of the M-step is,
is quite long and complicated, um,
but I wanna mention
just a key alge- algebraic trick you need to use when deriving the M-step.
Um, so, you know,
we know from the E-step that Q_i of z_i is that Gaussian Density.
Right. So you know, it's 1 over 2 pi to the d over 2,
that thing, and E to the,
right, negative 1.5 dot dot dot.
Right. So tha- that's the formula of a Q_i.
It turns out that,
um, in the M-step,
there will be a few places in the derivation
where you need to compute something like this.
[NOISE] Right.
And one way to approach this would be to plug into the density
for Q_i which is [NOISE] So you'd end up with this.
1 over 2 pi to the d over 2 Sigma, you know, wha- uh,
and so on into the negative 1.5 dot
dot dot times Z_i d_Z_i,
and then try to compute this integral.
Um, it turns out there's a much simpler way to compute this integral.
Anyone know what it is?
All right. Cool. Awesome. Expected value.
So the other way to compute this integral is to notice that this is the expected value of
z_i when z_i is drawn from Q_i, right?
So you know th- the,
the definition of the expected val- value of a random variable is expected value of
z is equal to integral over z probability_z times zdz, right?
That's what the expected value of a random variable is.
And so this integral is
the expected value of z with respect to z drawn from the Q_i distribution.
Um, but we know that Q_i is Gaussian with associated mean and certain variance,
and so the expected value of this- this is just mu of z_i given x_i, right?
It's that thing that you've already computed in the E-step. Makes sense?
And so when students derive the M-step, you know,
for EM implementations of Gaussians,
one of the key things to notice is, uh,
when are you actually taking an expected value with respect to a random variable,
in which case, it's just the value computed already,
and when do you need to plug in this big complicated
integral which can lead to very complicated, very intractable calculations.
Okay. So just when you're- whenever you see this, um, uh,
think about whether you need to be expanding a big complicated integral,
or if it can be interpreted as an expected value.
Okay. Um, and so for the M-step,
it's really, you know, the M-step is [NOISE]
All right. So that's the M-step.
And if you re-write this term as sum over i,
the expected value of z_i, uh,
drawn from Q_i of
this- all right.
It turns out that, um,
if you go ahead and,
uh, plug in the Gaussian density,
here [NOISE] actually on-
one rule of thumb for whether or not you should plug in
a complicated integral or plug in a Gaussian density,
um, this is just a rule of thumb after doing this type of math a long time,
is that see if there's a log in front.
If there's a log in front of a Gaussian density,
basically Gaussian density has an exponentiation, right?
The Gaussian density is 1 over e to the something.
So whenever there's a log in front,
the log exponentiation cancel out,
and this equation simplifies.
So one trick as you're doing
these derivations is just see if there's a log in front of a Gaussian density.
And when there is a plug in,
go ahead and plug in the formula for your Gaussian density,
the log will simplify that,
and what you end up with is the log of
a Gaussian density ends up being a quadratic function,
a quadratic function of the parameters.
And if you take the expected value with respect to a Gaussian density,
respect to quadratic function,
this whole thing ends up being a quadratic function.
Um, and then you can take derivatives of that equation with respect to the parameters.
With respect to mu of that whole thing,
set it to 0,
and then solve and they'll be roughly, um,
level of complexity of,
of maximizing quadratic function.
Okay. Hope that makes sense.
Um, the actual formulas are a little bit complicated.
So I don't- I'll,
I'll leave you to look at the actual formulas in the lecture notes,
but I think the take away is, uh,
don't expand this integral, um,
and when you are deriving this,
plug in the Gaussian densities here because the log will simplify.
Okay. And details of it in the lecture notes.
So let's break for today.
Uh, best of luck with the mid-term and- seriously, I hope you guys do well.
All right. I- I'll see you guys in a, in a few days.
 Hey everyone. Um, let's get started.
So, um, let's see, the plan for the [NOISE] day is,
uh, we'll go over the rest of ICA, independent component analysis.
In particular, talking about CDFs,
cumulative distribution functions [NOISE].
And then, um, actually, uh,
let's do that later.
[NOISE].
All right. So the plan is we'll go over,
uh, the rest of ICA,
independent component analysis, and we'll talk a bit about CDFs,
um, cumulative distribution functions [NOISE],
and then derive the ICA model.
And uh, in the second half of today,
we'll start on the final of the,
um, interesting, four major topics of the class, which is reinforcement learning.
We'll talk about MDPs, or Markov decision processes, okay?
So to recap briefly,
um, we had- you remember the overlapping voices demo.
So we said that in the ICA problem,
independent component analysis problem,
we'll assume we have sources S,
which are RN if you have N speakers.
So for example, if this is speaker one's audio,
then at time T [NOISE], um, S, you know,
superscript parentheses T subscript 1 is the [NOISE]
sound emitted by speaker one at time T.
Sorry, I don't have the- that's interesting.
All right [NOISE]. Just let me go back over a little bit.
Um, and, uh, yeah, we're,
we're using sometimes I to index training examples,
and so the training examples sweep over time, um,
and sometimes usually I use I,
sometimes I use T, I guess in the case where, um,
the, uh, uh, the different examples
come from different points in time in your recording.
And what your microphones record is XI equals A of SI.
So just for now,
let's say you have two speakers and two microphones, in which case,
A will be a 2 by 2 matrix, and in the homework problem,
we have five speakers and five microphones
in which case A will be a 5 by 5 matrix.
We'll talk later, um, about what happens
if the number of speakers and microphones is not the same [NOISE].
And the goal is to find the matrix W, uh,
which should hopefully be A inverse, um,
so that SI is W times X recovered the original sources.
Uh, and we're going to use these W1 up through
WN to represent the rows of this matrix W. Yeah.
[inaudible].
Uh, oh yes you're right.
Thank you. Right. Okay. Thank you.
Okay. [NOISE] So, um,
[NOISE] last time we had [NOISE]. All right.
So remember this is a picture of the Cocktail party problem.
And, uh, last time I showed these pictures about,
you know, why, why, why is ICA even possible, right?
Given two overlapping, um,
voices, how is it even possible to separate them out?
How is there enough information to know,
um, uh, you know, what are the two overlapping voices?
And so one picture [NOISE] we saw was this one,
where if S1 and S2 are uniform between minus 1 and plus 1,
then the distribution of data will look like this [NOISE].
If you pass this data through the mixing matrix A, then your observations,
now the axes have changed to X1 and X2, may look like this,
and your job is to find an unmixing matrix W that
maps this data back to the square, okay?
Now, this example is possible because the examples- because the,
uh, sources S1 and S2,
were distributed uniformly between minus 1 and plus 1.
Um, it turns out human voices, you know,
the recordings per moment in time are not
distributed uniform between minus 1 and plus 1.
And it turns out that, um, uh,
if the data was Gaussian,
then ICA is actually not possible.
Here's what I mean [NOISE].
Let's say that, uh- so,
so the uniform distribution is a highly non-Gaussian [NOISE] distribution, right?
Uniform B minus 1 plus 1, you know,
this is non-Gaussian and that,
that makes ICA possible [NOISE].
Um, what if [NOISE] S1 and S2 came from Gaussian densities, right?
Um, if that were the case,
then this distribution S1 and S2 would be rotationally symmetric.
And so, um, there would be a rotational ambiguity, right?
Any axis could be S1 and S2 [NOISE].
You can't map, you know,
this type of parallelogram back to this square, right?
So, so [NOISE] you can't sort of I think in this parallelogram,
um, you can sort of lead off,
you know, that may be one axis should look like that.
So I'm drawing with a mouse, not doing very well.
Well, second axis should maybe look like that, right?
And by, by inverting that you can get the data back to the square.
But in the case of if the data look like this,
then [NOISE] you actually don't know, um,
because [NOISE] maybe this should be S1 [NOISE] and that should be S2, right?
But so this is rotational ambiguity,
because the Gaussian distribution is, um,
rotationally symmetric, if S1 and S2 are standard Gaussians,
then, then, [NOISE] then this distribution is rotationally symmetric,
and you don't have enough information to recover
the directions that correspond to the original sources, okay?
So it turns out that, um,
there is some ambiguity in the output of ICA.
In particular, last time we talked about,
uh, two sources of ambiguity.
Um, you don't know which is speaker one and which is speaker two, right?
You don't know which one to number speaker one and which one to number speaker two,
and you might take this data and flip it horizontally, uh, reflect this,
you know, on, on,
on the neg- S1 goes to negative S1 [NOISE],
or reflect this, uh, on a vertical axis.
We don't know if it's positive S2 and negative S2.
And in the case of this example,
where S1 is, uh, uniform minus 1 plus 1.
Those are the only sources of ambiguity.
Um, but if the data was Gaussian there would be additional rotational ambiguity
which makes [NOISE] it in part- whi- which actually makes it
impossible to separate out the sources, okay?
So [NOISE] it turns out that, um, all right. cool [NOISE].
So it turns out that the Gaussian density is the only distribution,
um, that is rotationally symmetric.
Uh, if, if, if S1 and S2 are
independent and if the distribution is rotationally symmetric [NOISE],
meaning that the distribution has sort of circular contours,
uh, then it, then it, then it must be a Gaussian density [NOISE].
And so, there is a theorem, uh,
which I'll just state it formally,
that ICA is possible only if your data is non Gaussian, right?
But, but so long as your data is non-Gaussian,
then it is possible to recover the independent sources, okay?
I'm just stating [NOISE] that informally.
Um, so let's [NOISE] let's see [NOISE].
So what I would like to do is, um,
develop [NOISE] the ICA algorithm assuming that the data is non-Gaussian, okay?
Now, um, [NOISE] in order to, uh,
develop the ICA model,
we need to figure out what is the density of S, right?
And I'm going to use P subscript S,
you know, of, uh, the, the, uh,
of the random variable S to represent the,
um, density of S. Um,
an equivalent way to represent the probability of
the density of continuous random variables [NOISE] is via CDF,
which stands for cumulative
uh, distribution functions [NOISE].
And the, uh, the cumulative distribution function of a, uh,
random variable F of S in probability,
is defined as the chance that the random [NOISE] variable is less than that value.
So I guess, um, notations have been inconsistent, sorry,
but this is capital S I'm using to denote the random variable,
and this is some constant.
Right, and, uh, it's that same constant as that lowercase s, okay?
Um, and so for example,
if this is the PDF,
of a random variable S,
maybe of a Gaussian, right?
The CDF is a function that um, [NOISE]
increases from 0 to 1 where,
um, the height of a CDF at a certain point is the probability.
So if you take the curves at the same point, right?
So the height of a CDF at a certain point lowercase s,
is the probability that the random variable
takes [NOISE] on a value equal to this value or lower,
which means that the height of this function is equal to, um, you know,
the probability mass, the area under the curve of your PDF,
um, [NOISE] over to the left of that point, okay?
So that's, uh, I don't know, sometimes this-
some probability and statistics courses teach this concept and some don't I guess,
but there's- so there's a mapping between the PDFs and the CDFs of a function,
of a, of a continuous random variable.
Um, and the relation between the PDF and the CDF is that
the density [NOISE] is equal to the first derivative, right?
Uh, F prime. So if you take the derivative of the CDF,
then you should recover the PDF, [NOISE] okay?
But so I think, um, in order to specify, you know,
some random variable, we could either specify the PDF, right?
The probability density function,
or you can specify the CDF which is just, you know,
let's tell me what's the chance of the random variable taking on
any value less than any particular value S. And by taking the derivative of this,
you can always recover the PDF,
and by integrating this you can always go to the CDF, okay.
And so, um, what we're going to do in, um,
ICA is instead of specifying a PDF for how speakers' voices sound,
we're instead going to specify a CDF, and, uh,
we'll have to choose a CDF that is not the Gaussian density CDF,
because we have to assume that the data is non-Gaussian.
Uh, um, uh, and the CDF, you know,
is a function that always goes from, right, 0 to 1, okay?
So, um, [NOISE].
All right. So we'll specify [NOISE].
So in a little bit, we'll specify some CDF
for the density of the sources of what human voices sound like let's say.
And if you differentiate this, uh,
you will get the PDF of the density of s, right?
Was equal to that. Now, we're um,
going to derive a maximum likelihood estimate mission algorithm in a minute.
But our model is [NOISE] that, X is equal to A_s, um,
which is equal to, I guess w inverse of s,
and s is equal to w_x, right?
So that- that's, that's the model.
And in order to derive a maximum likelihood estimate for the parameters, um,
when you have- [NOISE] so this is going to be the density of x.
Okay? So this is a relationship between, um, ah,
this is the relationship between x and s. X is equal to A_s,
equals W inverse s and s equals W_x, right?
So this is the model. And what I'd like to do is,
let's say you know what's the density of s. Um,
what is the density of x if x is computed as the matrix A times s?
Right? So one step that's tempting to take is to just say,
well, s is equal to W times X.
So the probability of x is just equal to
the probability of s taking on the certain value, right?
So, so I mean this is s, right?
And so the probability of seeing a certain value of x is equal
to the probability of s taking on that corresponding value Because assuming,
W is an invertible matrix, is a bijection.
There's one-to-one mapping between x and s. So to find the probability of X,
just find the probably of s and compute the corresponding probability.
Um, it turns out this is- this is incorrect,
and this works with probability mass functions, for discrete probability distributions,
um, that take on discrete values.
But this is actually incorrect for continuous probability densities.
So let me- let me, um, uh, show an illustration,
and we'll go back to derive what is a correct way of
computing the density of x. Oh, and we'll want,
uh, density of x because um,
when you get the training set,
you only get to observe x, and so for, uh,
finding the maximum likelihood estimate parameters,
you need to know, um,
what's the density of x you can map, you know,
choose the parameters, choose the parameters W that maximizes the likelihood.
Okay? [NOISE] So that's what we want to compute the density of x.
But, um, let's, let's use a simple example.
[NOISE] Let's say the density of s is a indicator,
s is between 0 and 1.
Okay? So this is, um,
s is distributed uniform from [NOISE] 0 to 1.
Um, and let's say [NOISE] x is equal to 2 times s. Okay?
So now notation, A is equal to 2,
[NOISE] W is equal to one-half.
Ah, this is, uh, n equals 1, 1-dimensional example.
So, um, this is the density of s, right?
Uniform distribution from 0 to 1.
And if x is equal to 2 times s,
then this seems like X should be equal- X is
distributed uniformly from 0 to 2, right?
Because if s is uniform from 0 to 1,
you multiply it by 2,
X is distributed uniformly from 0 to 2.
And so the density for X is equal to
this, 1, 2 [NOISE].
Right? And it's now half as tall because, uh,
probability density function z to integrate to 1, right?
So this is a uniform from 0 to 2 probability density function.
And so the correct formula, um, is P of x,
x equals one-half, times
indicator 0 less equal to x less than equal to 2.
Right?
Um, [NOISE].
Okay? And, uh, more generally,
the correct formula for this is actually this times, um,
this is the determinant [NOISE] of the matrix W.
Uh, and in the case of a real number,
the determinant of a row number is just this absolute value which is why,
um, we have the density of x equals one-half.
You know, that's the absolute value of the determinant of W, um,
times, times your- times indicator whether 2 times s is within 0, 0 to 1.
Okay? Um, yeah, right.
So I guess this, uh,
this is indicator 0 less equal to one-half x less or equals to 1,
right, since that's s. Okay?
So this is illustration showing why this is the right way with
the determinant of W multiplied here as the-
as a way to compute the density of X.
Um, and, er, for the- for those of you familiar with, um,
determinants and- oh, and determinants is
a function you can call you know, in NumPy to compute, um, ah,
but also, uh, the intuition of a determinant is it measures how
much it stretches out a, um, local walking,
and so you need to, uh, uh, er,
sort of divide by the determinant of A or multiply by the determinant of W,
um, in order to make sure your distribution still normalizes to 1.
Right? So that's where that comes from.
[NOISE] So, um, we're nearly done.
Just one more decision,
and then we can derive the maximum likelihood estimation,
uh, to derive a maximum likelihood estimate of this, of the parameters.
The last thing we need to do is, um,
[NOISE] choose the density of what your speakers' voices sound like.
[NOISE] And as I said just now, um,
what we're going to do,
is, uh, choose a non-Gaussian distribution.
Right? And so [NOISE] while F of s is equal to the chance of this person's voice.
Right? Random variable s being less than a certain value.
And we need a smooth function that goes between, you know,
0 and 1, um,
where, we need a smooth function that has vaguely that shape.
And so well, what functions we know that are vaguely that shape?
Right? Let's pick the sigmoid function.
Um, and it turns out this will- this will work.
Okay. There are many choices that actually work fine.
Um, it turns out that if you choose a sigmoid function to be the CDF,
then if you look at the PDF this induces,
if you take the derivatives of this.
Right? So take P of x [NOISE] equals the derivative of the CDF.
Um, it turns out that if this is the Gaussian,
then the PDF that this choice induces,
is, uh, something with fatter tails, right?
Um, by which I mean that it goes to 0, you know.
[NOISE] So Gaussian density
goes to 0 very quickly,
right, it's like e to the negative x squared, right?
That's the Gaussian is a square in the exponent of the density.
And it turns out that this particular density, uh,
taken by compute derivative of a sigmoid it goes to 0 more slowly and this captures
human voice and many natural phenomena better than
a Gaussian density because there are a larger number of extreme outliers,
that are more than one or two standard deviations away,
um, but there are actually multiple distributions that work.
You could- if you use a double, double exponential distribution.
So this is an exponential distribution- exponential density.
If you take a symmetric with two sided exponential density for P of s,
it will also work quite well for ICA.
But I think, um, early history of ICA,
you know, researchers, I think it was,
um, might been Terry Sejnowski, uh,
down at the Salk Institute,
just needed a function with these properties.
He picked the sigmoid and plugged it in and it works just fine.
It's been a good enough default that,
um, it's still- it's still widely used, right?
But, but, but, but, but I've used this um,
double-sided exponential or sometimes also called the Laplacian distribution.
This, this works fine as well as a choice of a P of s. Okay?
[NOISE]
So the final step.
Um, the density of s is equal to [NOISE].
Right? The product of the, uh, uh, um, um, let's see.
Ah, it's a product from i equals 1 through
your n sources of the probability of each of the speakers emitting that sound, right?
Ah, because the n speakers
[NOISE] are speaking independently, right? Yeah.
[inaudible].
Say, say that again?
[inaudible] .
Oh, yes. You're right. Sorry about that.
Yes, this should have been [NOISE].
Sorry, yes this should have been a P_s.
Yeah. Thank you. [NOISE] Right,
go from a CDF to PDF by taking derivatives.
All right. Cool. So, um, er,
S is the vector of all, you know,
two speakers' or all five speakers' voices at one moment in time.
So the density of S, right?
S as an RN is the product of the individual speakers' probabilities,
and, um, this is the key assumption of ICA that,
you know, your two speakers or your five speakers are having independent conversations,
and so at every moment in time,
they choose independently of each other what sound to emit.
All right.
Um, and so using the formulas we worked out just now.
The density of x is equal to, um, well, as we did,
the density of, uh, W_x times the determinant of W. [NOISE] Right?
Uh, so- and this is equal to [NOISE]
Okay. Um, and this notation,
uh, W_I transpose x, this is, um [NOISE] right?
Because W_I is the I th row of the matrix W and so,
um, you know, I guess S- S_j is equal to,
um, W_j transpose X, right?
So t- take the corresponding row and multiply it by x to get a corresponding source.
Actually, sorry. I think this right, yeah,
let me use j there to make this clearer.
[NOISE] Okay.
[NOISE] And so, um, this writes out- so this shows
what is the density of x, um, expressed as a function of,
um, P_s, which we've assumed- which effects as a CDF of the Sigmoid as
a, as the derivative of the Sigmoid and as a function of the parameter W. Right?
So this is a model that,
given a setting of the parameter W which is a square matrix,
um, allows us to write down what's the density of X.
[NOISE]
So the final step is,
um, we could use [NOISE] maximum likelihood estimation to estimate the parameters w.
Um, so the log-likelihood of W is equal to sum over
the training examples of log- of, you know, [NOISE]
times by W. Right.
And, um, you can use stochastic gradient ascent.
[NOISE] All right.
Take the derivative of w with respect to the log-likelihood.
Um, and it turns out- this is derived in the lecture notes.
I'll just write it out here. [NOISE]
Times x i. [NOISE]
I hope I got that right. Yeah. Okay. Right. [NOISE] Um, yeah.
And it turns out that, um,
if you use this formula don- don't worry about
the formula for the derivatives, there are full derivations given in the lecture notes.
But it turns out that, um,
if you use the derivative of the log-likelihood with respect to parameter
matrix W and use stochastic gradient ascent to maximize the log likelihood,
uh, run this for a while, then you can get, um,
ICA to find a pretty good matrix W,
um, for unmixing the sources, okay?
So just to recap the whole algorithm, right?
You would have a training set of X_1 [NOISE] up through X_m,
where each of your training examples is the, um,
er, microphone recordings at one moment in time,
[NOISE] and so the time goes from 1 through M.
What you do is initialize the matrix W, say,
randomly and use gradient ascent with
this formula for the derivative in order to maximize the log-likelihood of the data,
and after gradient ascent converges,
you then have a matrix W and you can then recover the sources as S equals W_x.
And then now, we have the sources, you can take, um, say,
S_1_1 through S_1_m and play that through your,
um, your laptop speaker in order to see what source one sounds like.
Right? And so that's how you would take, you know,
overlapping voices and [NOISE] try to unmix them.
Okay. Oh, yeah.
[inaudible]
Oh why is choices A point not a rotation matrix?
Uh, er, boy how to visualize that.
Try plotting it in, um, NumPy, matplotlib I guess.
If you plot the contours of the- so it turns out that if this is S_1 and S_2,
what you do not want is the den- density whose contours look like that.
Um, I haven't done this for a while.
I believe if you take this distribution,
the contours will look like that.
[NOISE] It's been a while since I looked at this,
but I think it'll look like that.
So this is not rotational symmetry.
You're on it. Well, it's Laplace.
Yeah. Okay. Yeah. Oh, yes. Laplace definitely looks like that.
I think Sigmoid looks a bit like that too. Yeah, little like that.
Plot it and see if I'm right, or post on Piazza, if one of you plots it.
So you can see it, I haven't done that for a long time.
Yeah, at the back.
[inaudible]
Oh, um, um, why don't you interact with the derivative of the log? The- th- actually,
yes, the log should be like this, I think. Yes.
[BACKGROUND]
Oh, sorry, uh, g is the sigmoid function.
Yes, so g of z. Yeah, thank you. Right, more questions?
[inaudible]
Sure. What's the, you know,
um, what's the closest non-linear extension of this?
Um, I don't- we don't a have a great answer to that right now frankly, um, uh,
so a bunch of people including,
you know, my former students and me,
have done research to try to extend this to
nonlinear versions and there's some stuff that kind of works,
but I don't think there's like, uh,
tried and true algorithm that I'm ready to say this is a right way to do it.
Um, uh, yeah,
actually maybe I should [NOISE] think I could
say a little bit more about that if you're interested.
Well, yeah, actually, uh, let me- let me try to- [NOISE]
All right. Let's see.
So, so for several-
several years ago and- and still kind of ongoing,
there's been research, um,
some done by my collaborators and me,
some done by others on trying to build nonlinear versions of ICA,
and so some of you might have seen this slightly infamous,
um, Google cat result, right?
Uh, so this one was in the Google Brain project, one of the first projects we did.
This is a few years ago now where, um,
we trained a neural network,
uh, uh, on, um, was it many,
many hours of YouTube videos, uh, and,
and eventually it learnt to
detect cats because apparently there are a lot of cats in YouTube videos.
Um, uh, and so it turns out that the algorithm we used was a,
um, was sparse coding which is actually very closely related to ICA.
Um, and so this rough algorithm was attempting to build a nonlinear version of ICA,
where you train one version one- train- train train one layer of sparse coding let's say,
to extract low level features and then recursively apply this on top,
to learn not just edge detectors,
but object part detectors,
and then eventually, you know,
the somewhat infamous, um,
uh, this somewhat infamous Google cat.
Um, but I think that this is actually still ongoing research.
Um, I think the most interesting research, uh,
some of the most interesting research has been on hierarchical versions of sparse coding,
sparse coding is a different algorithm that turns out to be very closely related to ICA,
and then you can show that they're optimizing very similar things.
So, so I say sparse coding is very similar to ICA,
uh, but they're hierarchical versions of this,
they tried to turn this as a multilayered neural network and it kinda worked,
wherever that shows it can learn interesting features.
But what happened was, uh,
supervised learning then really took off and the whole world shifted a lot of
this attention to supervised learning and
building deeper supervised learning neural networks.
And so, the hierarchical sparse coding running
ICA over and over to learn nonlinear versions.
There- there's very less, uh,
attention from research on the- on that topic than it- than it really deserves.
So may- maybe you or someone in a class could go back and do more research on that.
I, I still think is a promising area. All right.
Um, so let me wrap up with, uh, some ICA examples, um,
so this is actually a former TA from the class, um, Catie Chang.
Um, and so it turns out that, uh,
ICAs are routinely used to clean up EEG data today,
so what's an EEG, right?
Um, place many electrodes on your scalp, uh,
to measure low electrical recordings,
uh, on the surface of your scalp.
So, you know, wha- what does the human brain do, right?
Human brain, your neurons in your brain right now,
uh, fire, generate little pulses of electricity,
and if you put- place electrodes on your scalp,
you can get very weak measurements of the,
um, of the voltage of the electrical activity,
in a, you know, at a certain point in your scalp.
So the analogy to- um, oh, excuse me.
Uh, oh, what's wrong. All right.
So the analogy to the cocktail party problem, the, um,
overlapping speakers' voices is that, you know,
your- your brain [NOISE] does a lot of things at the same time, right?
Your brain helps regulate your heartbeat, um,
part of your brain does that,
another part of your brain, you know,
makes your eyes blink every now and then,
another part of your brain- part of your brain is also
responsible for making sure that you breathe,
another part of your brain is responsible to
thinking about machine learning and stuff like that, right?
[LAUGHTER] So, so your brain actually handles many,
many tasks at the same time.
And as your brain, um,
sorry, not sure what's wrong with this.
Okay. And as your brain, um, uh,
carries out these different tasks in parallel,
uh, different parts of your brain generate different electrical impulses.
So think of there as, um,
imagine that you have a, you know,
cocktail party in your head, right?
So many overlapping voices,
so this is now voices in your head, uh, just going back,
but one- one- one part of your brain is saying,
all right heart, go and beat, heart go and beat,
heart go and beat, and another part of the brain is saying, hey,
breathe in and breathe out, breathe in and breathe out,
another part of the brain is ooh, you know.
What's wrong with this PowerPoint?
[LAUGHTER] That's what my brain is saying, right?
Um, and uh, what each electrode on the surface of your scalp does is it
measures an overlapping combination of all of these voices
because different parts of your brain are sending these electrical impulses,
they add up and so any one point on the surface of your brain,
reflects a sum or a mixture,
re- really a sum of these different voices,
of these different things your brain is doing.
Um, and so, uh, if you- just- just zooming in to the EEG plot, um,
each line is a voltage measured at a single electrode, right?
On say your scalp and, um, these, uh, signals are quite correlated,
you see that when there's a massive voice in your brain shouting,
you know, like, uh, uh, uh,
uh, right, beat your heart or blink your eyes,
that signal can go through all of the different electrodes,
which is why you can see these artifacts reflected in all of
these electrodes, um, uh, sorry.
All right. Turns out a pretty good way to clean up
this data is to take all of these time series
pre- pretty much exactly as we learned about it with
the ICA algorithm [NOISE] and separate it out into the independent components,
and so, um, it turns out in this example,
there are two components corresponding to driving the heartbeat,
um, that's actually the eye blink component,
and so one way to clean up this data- sorry,
I should really wonder what's wrong with this.
All right. Let me try something, [NOISE] um, maybe if I,
[NOISE] uh, oh, that's interesting. All right.
Okay, well, all right.
Um, if you, uh, uh, right,
it says heartbeat, there's eye blink,
and, uh, you don't get, all right.
And, um, if you run the ICA and then remove outs,
I have a person say,
"Oh this heartbeat, this eye blink, can remove,
subtract all those components,
then you can end up with a, um,
much more cleaned up EEG signal,
which you can then use for downstream processing.
So actually we possibly- is, there's been a lot of research on.
You've taken an EEG reading to try to guess at a high-level what you're thinking, right?
It turns out that, uh, uh, if your train a, train a, train a,
you know, supervised learning algorithm, uh,
to try to decide, are you thinking of a noun or a verb, are you thinking of,
uh, something edible, or are you thinking of,
uh, uh, something inedible.
There's been very interesting research, uh,
trying to use an EEG to figure out just at a very coarse level, um,
not- not- not- not quite mindreading every thought you're thinking, but,
but, uh, uh, uh, but, uh,
can we categorize very coarse level thoughts?
Like, are you thinking of a person,
are you thinking of an object?
And you can actually do that to some extent using EEG readings.
But cleaning up the data to get rid of the eye blink, and
the heartbeat artifacts is a very useful, um,
pre-processing step to get cleaner data,
to feed into the learning algorithm,
to try to figure out, try to categorize,
you know, some coarse category of what you're thinking.
Okay. Um, and then more research here,
it turns out that- uh, we're kind of- I,
I mentioned the Google cat thing just now.
It turns out that, um, if you, um, uh, train ICA, uh,
oh, the font is messed up.
Um, if you train ICA on, uh, natural images, um,
ICA will say that the natu- the independent components of natural images are these edges.
Uh, and as in that, you know,
when you see a little image patch in the world,
when you've seen, you know, look, look,
look somewhere in the world, look at just a tiny little piece of the image, right?
Like 10 pixels by 10 pixels.
Um, and if you take that data and model in this ICA,
ICA will say that, uh,
the world is made up of edges or made up of patches like these and that, uh,
the way you end up with images in the world is by each of these patches,
you know, independently saying is there a vertical edge,
is there a horizontal edge, was there,
is there this type of, um,
uh, light on the left, dark on the right?
Is there this type of, uh, lighter on top,
darker on the bottom and so on.
And just by adding all of these voices that you get a typical image fashion of the world.
So they're, they're interesting theories in neuroscience about whether this is how,
you know, the human brain learns to see as well.
So, so very, very same work on, um,
ICA and sparse coding to try to use these mechanisms to explain how, you know,
the human brain tries to explain,
um, uh, tries, tries to learn to perceive images, for example.
Okay? Um, so all right.
So [NOISE] that's it for, um,
uh, the algorithms of ICA,
um, just the final comments.
Um, I think on Monday someone asked,
"Do the number of speakers and number of microphones need to be equal?"
So it turns out that, um, if the number of, uh, um,
microphones is larger than the number of speakers,
that's actually fine, right?
If you- if the number of microphones is larger than the number of speakers,
then if you run ICA or, or a slightly modified version of it,
you'll find that some of the speakers are just silent speakers.
Um, uh, and so, you know,
if you have, uh, 10 microphones and five speakers,
if you run this algorithm on 10 microphones, you can find that, well,
maybe five of the sources are just silent or there are
ways to just not model those five sources as well, right?
If, if you think that, uh, they're just some sources of silence.
So, so, this, so,
so a slightly modified version of this works quite well if,
um, uh, the number of speakers is larger than the number of microphones.
Um, if the- excuse me,
if the number of microphones is larger than the number of speakers,
this, this, this works quite well.
If the number of microphones is smaller than the number of speakers,
then that's still, um, uh,
very much a cutting edge research problem.
Uh, so, so for example, uh,
if you have two speakers and one microphone, um, uh,
it turns out that if you have one male and one female speaker,
so one relatively higher pitch and one much lower pitch,
then you can sometimes have some algorithms
that separate out two voices with one microphone.
Um, but it doesn't work that reliably,
it's a little bit finicky but there have been
research papers published showing that, you know,
you could make a reasonable attempt at separating out, um,
two voices with mi- one microphone if
the pitches are quite different such as this one male one female voice.
Um, uh, uh, but separating out two male voices or two female voices is still very hard,
um, uh, and, and then there's ongoing research in, in those settings.
Right? So that's ICA,
um, and I guess you get to play more of it in your,
um, homework problem as well.
Okay? Any last questions about ICA?
[inaudible]
Oh sorry, say it again?
[inaudible] [NOISE]
Wait, sorry, was because a-
So I'm just wondering why is it that hard [inaudible]
Oh, [NOISE] yeah so, um,
uh, I think- actually if you go through a lot of the math it,
it, it, it just breaks down, I think.
Um, because there- you can have two independent sources
but W is now no longer a square matrix, right?
Of your, what is it?
Um, uh, uh, so- uh, uh, right.
Is that x is equal to AS, right?
And so if, um,
x is a real number and S was two-dimensional,
so I guess this would be, um,
uh, uh, A would be 2 by 1,
S would be- uh, S- uh,
A would be 2 by 1, S would be 2- excuse me,
A would be 1 by 2 and S would be a 2 by 1,
and this is 1 by 1, then,
you know, A inverse kind of doesn't exist, right?
So you need to come up with a way to form the maximum likelihood model.
And, and when you have one microphone,
it's just how do you separate out two overlapping voices,right?
Does that make sense?
So it takes much higher level knowledge,
um, uh, yeah, to separate out two voices.
Does this make sense? Um, so go ahead.
[inaudible]
Oh I see, right, uh, let's see.
So right, so if you don't know how many speakers there are,
you have all these microphones where you have
all- the number of electrodes you have is fixed,
so that's just your data set.
And it turns out that, uh, um,
if you run ICA with a large number of speakers,
you find there are many speakers are silent.
There are also some versions of ICA that you- so if you think that there are,
um, uh, let's see, boy- those transfer some of this.
But it turns out that, um,
if you think that there is a relatively small number of speakers,
then you don't need to explicitly model all the speakers.
Instead, what you would model wou- so again,
um, uh, suppose it's a maximum likelihood estimation problem.
Um, let's say that, uh, x is an R10, right?
So you have 10 recordings.
But you suspect that you only have five speakers.
Then in this case,
I guess the ma- matrix A would be um, what is it?
Uh, was it?
It would be 10 by 5, is it?
Right? To mix the 5 sources into 10 speakers.
And you could, um,
form the maximum likelihood estimation problem assuming the existence of
only five speakers without modeling
a lot of speakers and then finding later that they're all silent.
Does that make sense? So if you form the- so if- if you
parameterize the model like this using A instead of W, um, uh,
then you could form the maximum likelihood estimation problem
where you just assume that there are
five speakers and S is generated by
five speakers mixing through a linear thing plus noise.
But I just think that if you don't know how many speakers you
have or even what you are- what speakers you are working on,
how would you know if you probably had enough microphones?
Oh I see, sure, right.
How do you know if you have- how do you know how many speakers you have?
So I, I think it's one of those things that's a little bit like k-means,
I guess, where you try it and see what works.
And if you find that, uh,
the first few, you know,
speakers will capture most of the variance,
you find that digital speakers are quite silent and they're quite small,
you could just cut it off at that time.
I don't wanna go too much into the different numbers of speakers and,
and, and, uh, microphones, ICA algorithms.
Uh, uh, but let me just take a couple of last questions
and move on. You have a question? Yeah.
Do you ever see a problem with W?
Say it again?
Do you ever see a problem with W?
Oh, do you ever see a problem with W?
Um, I'm sure you can.
It's not usually done in this version of the algorithm,
but I would not be surprised if there are some other versions where you do.
I've, I've not seen that a lot myself actually.
All right, cool.
All right, cool.
Um, let's see.
All right, good, we're far enough along.
Okay, good. Um, so-
[NOISE] Circumstantial- All right.
All right, yeah, let's do these- [NOISE]
All right. Um- [NOISE]
All right. So that wraps up,
um, our chapter on unsupervised learning.
So, um, you learned about I,
guess, k-means clustering, um,
the EM algorithm for mixture of Gaussians, uh,
or really mixture of Gaussians model, um,
factor analysis model, and also PCA.
And then, you know,
today the ICA or independent components analysis algorithm.
And all of these are algorithms that could take as input an unlabeled training set,
just the xi's and no labels.
And we'll find various interesting structures in the data such as
clusters or subspaces or in the case of ICA,
the voices of the independent speakers.
And, and you implement ICA and play with it yourself in the homework problem,
where you get to separate out many five overlapping, um, voices.
The last of the four major topics, I want to cover in this class.
We've talked about supervised learning,
kind of device machine learning, unsupervised learning,
and the fourth and the final major topic we'll cover in this course will be
on reinforcement learning [NOISE].
Okay. So, um, so to motivate reinforcement learning.
Um, let's say you want to have a computer,
uh, learn to fly a helicopter, right?
I think I showed you some of the videos that are in the first lecture,
and so I just skipped that here.
But it turns out that, um,
if you are at every point in time given the position of a helicopter,
called the state of a helicopter,
and you're asked to take an action on how to move the control sticks,
you know, to make the helicopter fly in a certain trajectory.
It turns out that it's very difficult to know what's
the one right answer for how to move the control sticks of a helicopter.
Right. So if you don't have a mapping from X to Y because
you can't quite specify the one true way to fly a helicopter,
um, it's hard to use supervised learning for that, right.
And what reinforcement learning does is, is, is an,
an algorithm that doesn't ask you to tell it the right answer at every step,
it doesn't ask you to tell it exactly what's
the one true way to move the controls of a helicopter at any moment in time.
Instead, your responsibility as a designer
or machine learning engineer or AI engineer is to
specify a reward function that just tells
the helicopter when it's flying well and when it's flying poorly.
So your job as a designer is to write the cost function or
a reward function that gives a helicopter a high reward whenever it's doing well.
Flying accurately, flying the trajectory you want it to,
and it gives the helicopter a larger negative reward,
um, whenever it crashes or does something bad, right?
And I think I, I, I remember, I think, you know, think of it as like training a dog, right?
When do you say good dog, when do you say bad dog?
And the dog figures out when to do more of the good dog things.
And your job is not to tell the dog,
when you can't actually talk to the dog,
and tell it what to do. I guess that doesn't work.
But you can tell it good dog and bad dog,
and hopefully it learns from those positive and negative rewards
how to do more of the good things.
Okay. Um, another example.
Um, let's say you want to write a program to play chess or I guess most, you know,
somewhat famously and, uh, uh,
arguably somewhat slightly overhyped Go, AlphaGo, right.
Um, so it's very difficult to know in
given a certain chess board position or checkers or Go board position,
what is the one true move,
what's the one best move.
So it's very difficult to formulate, um, you know,
playing chess, uh, uh,
as a supervised learning problem.
And instead, um, the mechanisms used to play
chess are much more like reinforcement learning,
where you can, um,
let your program play chess or Go or whatever.
And whenever it wins you go, "Oh good computer."
And when it loses you go, "Oh bad computer."
So that's a reward function.
And the learning algorithm's job is to figure out by
itself how to get more of the positive rewards, right?
And actually common rewards for, uh, learning to play,
uh, chess or checkers or Othello or Go is, uh,
plus a reward of plus 1 for a win,
minus 1 for a lose, and a 0 for a tie, right?
So as you write your chess-playing programs, there has to be a common choice for a reward.
Um, where R is the reward function and S is the state.
Okay. And I will go into the notation, um, in a little bit.
And so as you can imagine, um,
given only this type of information so say a chess-playing program,
it places much more burden on the program to figure out what to do.
Right. In fact, one of the challenges of reinforcement learning is,
uh- so this is called a reward,
and that's called the state.
And the state means, um,
the status of the chessboard.
Where are the P's in the chessboard?
Or the status of the helicopter.
Where exactly is the helicopter?
And you're either right-side up or you're upside down,
and where are you, right?
Um, and it turns out one of the challenges,
one of the things that makes, um,
reinforcement learning hard is,
uh, the credit assignment problem.
And that means that if, uh,
your program is playing a game of chess,
and let's say it loses on move 50.
You know, so it plays a game,
and then on move 50, right, is checkmated and loses to its opponent.
So it gets a reward of negative 1.
But how can the program actually figure out
what it did well and what it did poorly, right?
If you lose a game on move 50,
it might be that the program made a really bad move,
made a blunder at move 20.
And then, you know,
but it just took another 30 moves before its fate was sealed, right.
So in a game of chess, you made a bad mistake early on,
you can still take many, many games- many,
many moves in the game of chess before,
before the final outcome of,
of losing or winning or losing is reached.
Or, um, in a, uh, initiate another- it turns out that, uh,
if you are trying to build a self-driving car,
um, if ever car crashes, right,
chances are the thing the car was doing right before it crashes was brake,
but it's not braking that caused the crash.
It's probably something else that caused it many,
many seconds ago that led to the bad outcome.
So there's a bad outcome.
How does the algorithm know of all the things that it did before,
how does it know what it did well?
What it should do more of and what they should- did poorly,
what it should do less of.
And, and conversely, if there's a good outcome,
you know, like it wins a game of chess.
Well, how do you know what you did well, right?
So that's called the credit assignment problem,
which is when your algorithm gets some reward,
how, how do you actually figure out what you did well and what you did poorly?
So you know what to do you more of and what to do less of, right?
So, um, as we develop reinforcing learning algorithms,
we'll see that the algorithms we use have to at least indirectly,
um, try to solve the credit assignment problem.
Okay. So, um,
reinforcement learning problems like playing chess or flying helicopters or, um, uh,
you know, building these various robots is modeled using the,
um, MDP or the Markov decision process formalism. [NOISE]
Um, and this is a way-
this is a notation and the formalism for modeling how the world works,
and then reinforcement learning algorithms will solve problems using this formalism.
So what's an MDP?
So an MDP is a five tuple.
And let me explain what each of these are.
Um, so S is the set of states.
So for example, uh,
in chess this would be the set of all possible chess positions or in,
uh, flying a helicopter.
This would be the set of all the possible positions,
and orientations, and velocities of your helicopter.
A is the set of actions, um, where, uh,
in the helicopter this would be all the positions you could move
your controls sticks or in chess this would be all the moves you can make, you know, in a,
in a game of chess. [NOISE].
Uh, P subscript sa is a-a state transition probabilities and so, um,
we'll see later these-these state transition probabilities tell you,
if you take a certain, uh, action a and a certain state s,
what is the chance of you ending up at a particular different state s prime?
Great.
Um, gamma is
a discount factor, that's a number between 0 and 1.
Uh, don't worry about this for now,
we'll come back to this in a minute,
and R is that all important reward function.
Okay, so, um,
in order to develop a reinforcement learning algorithm, um,
I'm going to use, as a running example,
a simplified MDP that we can draw on the whiteboard.
Right, so helicopters and chess and go and so on are really complicated MDPs.
So just to illustrate the algorithms,
I want to use a simpler MDP, uh, and this is, um,
an example we've drawn from the textbook Russell and Norvig.
Um, I'm going to use
simplified MDP in which you have a robot navigating this simple maze,
ah, and there's an obstacle.
So this is a grid work, right.
So a robot, you know- well the R2D2 like robot.
Yes, right, um, and it's navigating this very simple maze,
uh, and this is a pillar or this is a wall,
so you can't walk into that wall,
[NOISE] and let me just use,
um, indexing on the states as follows.
Um, so this MDP- let's- let's go through the five top points and talk about what,
uh, the- the- each of the five things are.
So this MDP has 11 states
corresponding to the 11 possible positions that the robot could be in,
right, each of these bank squares.
So there are 11 possible states,
and the actions, um,
are North, South, East and West, right?
You can command your robot to move in any of these directions.
Um, and I don't know if- if you worked with robots before, you know that, um,
when you command a robot, uh, you know,
to head straight, um,
it doesn't always go exactly straight.
Sometimes the wheel slips and veers off at a slight angle,
and so just simplifying the example,
we're going to model it as that, um,
if you command the robot to go North from a certain state,
that there is a 0.8% chance of successfully go the way you told it
to and a 0.1 chance that it will
accidentally veer off to the left or accidentally veer off to the right, okay?
Um, if you're working on real robots,
right, What's a real robot?
Uh, it is actually important to model the noisy dynamics of a robot wheel slipping slightly.
Or the orientation being slightly off.
Now, um, in a real robot,
you have a much bigger state space than the 11 states,
right, so- so this is simplified.
So this is not a realistic model for how
robots actually slip but because of using such a small state space,
I think just for illustration purposes,
we'll- we'll- we'll use this.
Um, and so for example,
the state transition probability would specify these.
You'd say that if you're in the state 3, 1.
So this state 3, 1,
and you command it to go North,
that the chance of getting to the state 3, 2 is,
uh, 0.8, and the chance
of getting to the state 4, 1 is 0.1,
chance again to 2, 1 is 0.1,
um, and the chance of getting to other states is like 3, 3 and other states is equal to 0, okay?
So the state transition probabilities will capture that,
if you're here and decide to go North,
there is a 0.8 chance you are going here,
0.1 chance you are going here,
0.1 chance you are going here, and you know,
you've got 0.0 chance of, right, hopping two steps.
Okay. Um, and- and again just simplifying the MDP example.
We'll just assume that the-the robot, you know,
hits a wall, it just bounces off the wall and stays where it is.
So if you told it to go East,
it slips off and just bounced off the wall and stays exactly where it is.
Now, let's specify the reward function,
uh, we'll come back to discount factor later.
But let's say you want the robot to navigate
to this cell in the upper right-hand corner, um,
and so to incentivize the reward- incentivize the robot to get to this square,
you know, that's the prize or that's the goal anyways,
let's put a plus 1 reward there and, um,
let's say you really don't want the robot to go to this cell,
you could put a negative 1 reward there.
Alright. So, um, the way you specify
the tasks for a robot to do is in designing the reward function.
So in our example,
um, well let me just copy that again, plus 1, minus 1.
Um, we have that the reward at the cell 4, 3 is plus 1,
and the reward at the cell 4, 2 is negative 1.
Um, and then, you know,
if you want the robot to get to the +1 reward cell as quickly as possible,
then, um, again there- there are many ways of designing reward functions.
Well, one common choice would be to,
um, put the negative penalty,
a very small negative penalty,
right, such as a set the reward to negative 0.02 for all other states.
And the effect of a small negative reward like this is to charge it,
right, every- every step it's just loitering around.
So charge it a little bit for using up electricity and wandering around, uh,
because this incentivizes the robot to hurry up and get to the plus 1 reward.
So you give a small penalty, you know,
for- for loitering and wasting electricity.
So this is how an MDP works.
Um, your robot wakes up at some state as 0,
um, at time 0, you know,
as you turn on the robot and the robot says,
"Oh, I'm at that state."
And based on what state it is in, um,
it will get to choose some action,
a0, so decide to only go North,
South, East or West and choose some action.
Based on the action,
the consequence of the choice is it will get to some state S1.
Uh, the state at the next time step,
which is distributed according to
the state transition probabilities governed by
the previous state and the action it chose, right.
So depending on what action it chooses,
there is different chances of moving North, South, East or West.
Now, that there's an S1,
it then has to choose a new action a1,
and as a consequence of the action a1,
it will get to some new state S2,
which is governed by,
um, the state transition probabilities, you know, s1,
a1, and so on, okay?
And- and the robot just keeps on running.
And so the robots will go through a sequence of states S_0,
S_1, S_2 and so on,
depending on the choices it receives,
depending on actions it chooses.
And the total payoff
is written as follows with one more detail,
is that term Gamma.
So think of Gamma as a number like 0.99.
So Gamma is usually chosen to be just slightly less than one,
and what the- so the total payoff
is the sum of rewards or more technically is a sum of discounted rewards,
and what this does is it adds up all the rewards that the robot receives over time,
but the further reward is into the future, um, you know,
the- the- the smaller the Gamma is the power of time that that reward is multiplied by.
Okay. So any reward you get at time 1,
you get all of that.
Every reward you get at time 2 is multiplied by 0.99.
And the reward you get at the next step is multiplied by 0.99 squared, 0.99 cubed and so on.
And so what the, um, discount factor, ah, does,
is it has the effect of giving a smaller weight to rewards in the distant future, um,
and this means that this encourages the robot to also get deposited rewards faster,
um, or postpone the negative rewards, right?
And so in, uh, financial applications, um,
the discount factor has a natural interpretation,
as the time value of money,
because if you have a dollar today,
you know, you're better off having
a dollar today that have being a year- dollar a year from now.
Right? Because when you put the dollar in the bank and earn interests, uh, uh,
for a year on your dollar and so a dollar
today is strictly better than the dollar the future.
Um, and conversely, having to pay $100,
or having to pay one dollar a year from now is also
better than having to pay a dollar today, right?
Because if you could, you know,
save your money and earn interest and then issue
a payment to someone else a year from now rather than now,
then you're actually slightly wealthier, um, and so, uh, uh,
and so Gamma in financial applications has the interpretation as the time value of money,
um, uh, or as the interest rate, I guess.
Um, uh, and but, but, but,
more generally even for non-financial applications, ah, mostly wrote,
most- most the- there are some financial application reinforcement learning programs,
there are lots of non-financial applications as well.
Um, this mechanism of using a discount factor has the effect of
encouraging the system to get to the positive rewards as quickly as possible,
uh, but then also conversely to try to push
the negative rewards as far into the future is possible, right?
And I think, uh, to be pragmatic,
there are two reasons why people use Gamma.
The story I just told, time value of money,
your frontal deposit of rewards, postponed rewards.
That's, uh, that's the story you tend to people- you tend to,
uh, uh, hear people say in terms of why we have a discount factor.
Uh, the other reason we have the discount factor is
actually much more pragmatic one which is that
all the reinforcement learning algorithms you see,
they converge much faster or they weren't
much better if you're willing to have a discount factor.
Whereas it turns out that if Gamma is,
is, equal to 1, if,
if Gamma is not strictly less than 1,
um, uh, it's much harder or,
or there, there are many reinforcement learning algorithms that, uh, may not converge.
It's much harder to prove convergence or they may not converge.
So just as a pragmatic thing.
Um, this makes the job much easier for your algebra.
So I see some of you shaking your heads in dis- in disapproval. [LAUGHTER].
[inaudible].
Yeah.
Yeah.
Yes yeah that's a good point.
Yes. So one of the things if there's no Gamma is that, uh, the rewards,
some of the rewards, you know,
could be- can increase or decrease without bounds.
So by having Gamma it does guarantees that
the total payoff is a finite value or is a bounded value.
So that, that's, that's one of the parts that go into some of
the proofs or some of the reasoning behind
why reinforcement learning algorithms converge.
So, cool, that's good insight.
Um, okay. So the goal of reinforcement learning is to
choose actions over time,
to maximize the expected total payoff.
Okay. And in particular, um,
what most reinforcement learning algorithms will come up with,
is a policy, um, that maps from states to actions.
Right? So the output of most reinforcement learning algorithms will be a policy, um, or,
or controller, in the R world we
tend to use the term policy but policy just means controller,
the maps of states to actions.
So it turns out that, um,
for the MDP that we have, um, right?
It turns out that this is the optimal policy.
So for example, ah,
if you take this example,
this, this cell here,
this cell over here,
this policy is saying pi applied to the state 3, 1,
is equal to West.
Hey, and that- so [NOISE] excuse me.
So it separately worked out,
what is the optimal policy,
and this turns out to be optimal policy in the sense that,
if you, um, we say execute this policy,
so to execute the policy means that whenever you're in the state S,
take the action given by Pi of S,
so that's what it means to execute a certain policy.
And it turns out that, um,
this policy will- I, I,
I worked all separately, right, offline.
Yeah. And, in, um,
uh, on my laptop, uh, uh,
that this is the optimal policy for this MDP,
and it turns out that if you execute this policy,
meaning whenever you're in a certain state, you know,
you'll take the action indicated by the arrow,
that this is the policy that will maximize the expected total payoff.
Okay. Um, and the problem in reinforcement learning is,
given a definition for an MDP,
or given a problem to pose,
the problem as an MDP,
figure out what's the set of states,
what's the set of actions,
um, what are the state transition probabilities,
specified discount factor and specified reward function.
And then to have a reinforcement learning algorithm,
find the policy Pi that maximizes the expected payoff.
And then when you want your robot to act or when you want your chess playing program to act,
um, whenever you're in some state S,
take the action given by Pi of S,
and hopefully this will result in a robot that,
you know, efficiently navigates to the plus 1 state.
Okay? So it turns out that MDPs are quite good at making fine distinctions.
So one example, um,
it's actually not totally obvious whether here
you're better off going North or going West.
Right? And it turns out that there is a trade off.
If you go west here, then, you know,
you're gonna take a longer route to get to the plus one.
So you take longer,
uh, the plus minus is discounted more heavily,
you're taking these penalties along the way, [NOISE] excuse me.
But on the flip side,
if you were to try to go North,
you could try to get there faster.
But on this step,
there's a 0.1% chance that you accidentally slip off to the minus 1 state.
So, so what is the optimal action?
Right? It's actually quite hard to just look at it with your eyes and make a decision.
But it turns out that if you solve for the optimal set of actions in
this MDP in this example is I just take a longer and safer route. Question?
[inaudible]
Does this advice take us in policies?
So, uh, uh, if the optimal set of actions is to cycle around,
then it should find out, uh, uh,
I mean for example, if there are only penalties
everywhere and you just go and run in a circle,
you know, then, then, the- the algorithm will actually chose to do that.
Uh, but in this case,
you want to get to the plus one as quickly as possible.
Right? And so what we'll see, um,
there's one more question. Go ahead.
[inaudible]
So, so, so chess and checkers and go and so on- they're, one,
one complication that is, you take a move.
So actually, all right.
To refine the description to chess.
Um, what happens in playing chess is a state- status,
um, your board, right? So there's your move.
So you see a board that's a state,
and so you make a move and then the opponent makes a move and then that's the new state.
So the state is when you and your opponent
both make- take turns then this comes back to you.
Right? Um, and because you don't know exactly what your opponent will do,
there is a probability distribution over if
I make a move or what's the other person gonna do?
Uh, I guess one last question.
Yeah. Go ahead.
[inaudible] [NOISE]
Yeah. Right. The probabilities are assigned per node,
the 0.8, 0.1, 0.1 where does that come from?
Um, so we'll talk about that later,
uh, in some applications does this learn?
So if you build a robot,
you might not know is it 0.8, 0.1, 0.1 or, you know, 0.7, 0.115, 0.115.
So it's quite common to use data to learn those state transition probabilities as well.
We'll we'll, we'll, see a specific example of that nature.
Okay. So all right.
So where we are just to summarize,
this is how you formulate the problem as an MDP,
um, and then the, the, the,
the job reinforcing learning algorithm is ready to go from the MDP.
to telling you what is a good policy.
Okay. So let's break,
um, and have a good Thanksgiving everyone,
[NOISE] I won't see you for like a week and a half, uh, uh,
enjoy yourselves and we'll, we'll reconvene after Thanksgiving with, uh, with this.
 Welcome back, everyone. I hope you had a good Thanksgiving.
Um, I actually didn't ask, I'm not sure why this chair is here.
All right. Let's get rid of this.
Um, by the way, not sure- um, thanks, Anand.
I'm not sure if you guys are following the news, but in,
in reinforcement learning, we chat a lot about robotics, right?
And one of the, you know, uh,
constant problems a lot of people use reinforcement learning to solve is robotics and,
um, I think, ah, uh, back in May,
um, the InSight Mars lander had launched from, um,
here in California and it's about to make an attempt at landing
on the planet Mars in the next 2.5 hours or so,
so excited about that, uh,
I think that is actually one of the grandest,
um, applications of robotics because, you know,
with a- with 20 minute light-speed from Earth to Mars,
you know, once it starts its landing,
there is nothing anyone on Earth can do and so I think that's
one of the most exciting applications of autonomous robotics.
When you launch this thing, it's now about 20,
20 light minutes away from planet Earth,
so you actually can't control it in real time,
uh, and you just have to hope like crazy that your software
works well enough for it to land on this planet, you know.
Uh, and then so we, we will find out a little bit
afternoon if the landing happened successfully or not.
I, I think, um, so I,
I just get excited about stuff like this,
I, I hope you guys do too.
And for those of you that are from California, I mean,
take some pride that it launched from the home state of California and,
and is now nearing its,
er, landing on Mars.
Okay, um, all right.
So, um, what I wanna do today is,
uh, continue our discussion on reinforcement learning.
Do a quick recap of the MDP or the Markov decision process framework.
Um, and then we'll start to talk about algorithms for solving MDPs.
In particular, we need to define, uh,
something called the value function which tells you how good it is to be
in different states of the MDP and then, um,
we'll define the value function and then talk about an algorithm
called value iteration for computing the value function
and this will help us figure out how to actually
find a good controller or find a good policy for an MDP,
and then we'll wrap up with our learning state transition probabilities
and how to put all these
together into an actual reinforcement learning algorithm that you can implement.
Um, to recap, um,
our motivating example- running example from the last time,
from before Thanksgiving was,
uh, this 11-state MDP.
And we said that an MDP comprises a five tuple,
a lists of five things with, er, states.
So that example had 11 states.
Um, actions, and in this example the actions were the compass directions;
North, South, East, and West,
I can try to go in each of the four compass directions.
The state transition probabilities and in the example,
if the robot attempts to go North,
it has an 80% chance of heading North and a 0.1%
chance of veering off to the left and a 0.1 chance of veering off to the right.
Um, Gamma is a number slightly less than 1,
um, usually slightly less than 1,
there is a discount factor, think of this as 0.99,
um and R is the reward function that helps us specify where we want the robot to end up.
Um, and so what we said last time was that, um,
the way an MDP works is you start off in some state S_0,
um, this one's much better, you choose an action, uh,
a_0, and as a result of that,
it transitions to a new state, S_1,
which is drawn according to P_s_0 a_0.
Um, and then you choose a new action a_1 and as a result
the MDP transitions to some new state P_s_1 a_1,
um, and the total payoff is the sum of rewards, right?
Um, and the goal is to come up with a way, um,
and formally the goal is to come up with a policy, Pi,
which is a mapping from the states to the actions, uh,
that will tell you how to choose actions from whatever stage you are in so that
the policy maximizes the expected value of the total payoff, okay?
Um, and so I think last time I,
I kinda claimed that this is the optimal policy for this MDP, right?
Um, and what this means for example is,
if you look at this state, um,
this policy is telling you that Pi of 3, 1 equals,
uh, West, I guess,
or you can write West or left, well,
what do you call that left arrow, right,
where from this state, um,
from the state 3,1, you know,
the best action to take is to go left, it's to go West.
And so if you're executing this policy what that means is that, um,
on every step the action you choose would be, you know,
Pi, right, of the,
the state that you're in, okay?
So, um, what I'd like to do is now,
uh, to find the value function.
So, how, how, how, how did I come up with this, right?
Well, what I'd like to do is, have you,
um, learn given an MDP,
given this five tuple,
how do you compute the optimal policy?
And one of the challenges with, um,
finding the optimal policy is that, you know,
there's a- there's an exponentially large number of possible policies, right?
If you have 11 states and four actions per state,
the number of possible policies is, er,
4 to the power of 11 which is not that big because 11 is a small MDP, right?
Because the number of, of policies- possible policies for,
for an MDP is combinatorially large, is,
uh, number of actions, the power of the number of states.
So how do you find the best policy?
Okay. So what you learn today is,
um, how to compute the optimal policy.
Now, in order to develop an algorithm for computing an optimal policy,
um, we'll need to define three things.
So just as a roadmap.
Um, what I'm about to do is define V_Pi,
V_star, and Pi_star, okay?
Um, and based on these definitions we'll see that- we'll,
we'll come to the, uh, definition.
We will- uh, derive that Pi_star is the optimal policy, okay?
But so let's, let's go through these few definitions.
Um, first V_Pi.
So for a policy Pi,
V_Pi is a function mapping from states to the rules, uh,
[NOISE] is such that V_Pi of S is the expected total payoff,
um, for starting in state S and executing Pi.
And so sometimes we write this as V_Pi of S is
the expected total payoff
given that you execute the policy Pi and the initial state,
S_0 is equal to S, okay?
So the definition of V_Pi,
this is called the, um,
value function for a policy.
Well, this is called the value function.
[NOISE]
For the policy Pi, okay?
Um, and so what the value function for a policy Pi denoted v_Pi is?
Is it tells you for any state you might start in,
there's a function mapping of states to rewards, right?
For any state you might start in what's
your expected total payoff if you start off your robot in that state,
and if you execute the policy Pi?
And execute the policy Pi means take actions according to the policy Pi.
Right? So here's a, here's a specific example.
Um, this policy.
So let's consider the follo- following policy Pi, right.
Um,
[NOISE]
so this is not a great policy.
You know, from some of these states,
it looks like it's heading for the minus 1 reward or sorry.
So if one of the reward was plus 1 that we get here.
And secondly, this is called an absorbing state.
Meaning that if you ever get to the plus 1 and minus 1,
then the world ends and then there are no more rewards or penalties after that.
Right? So but so this is actually not a very good policy,
so the policy is any function mapping from the states to the actions.
So this is one policy that says, uh,
in this state, you know,
this policy tells you in this state for one go north,
which is actually a pretty bad thing to do, right, is take you to the minus 1 reward.
So this is not a great policy,
um, but, but this is just a policy.
And v_Pi for this policy,
um, looks like this.
Okay. Um, don't worry too much about the specific numbers.
But you've- if you look at this policy,
you see that from this set of states it's
pretty efficient at getting you to the really bad reward,
and from this set of states it's pretty efficient at getting you to the good reward right,
with some mixing because of the noise in the robot veering off to the side.
And so, you know,
these numbers are all negative.
And those numbers are at least somewhat positive.
Right. So but so v_Pi is just,
um, if you start from say this state,
from the state 1, 1 on expectation,
you're expecting some these counts of rewards will be negative 0.88.
Okay? Um, so that's what v_Pi is.
Right. Now, um,
the following equation.
Let me think, uh,
governs, um, the value function.
It's called, it's called Bellman's equation.
Um, and this says
that your expected payoff
at a given state is the reward that you receive plus the discount factor,
times the future reward.
So let me, let me actually explain,
um, the intuition behind this, right?
Which is that, um,
let's say you start off at some state s_0, right?
So and again, let's,
let's say s is equal to s_0.
So v_Pi of s is equal to,
well, just for your robot waking up in that- I'm going to add to that in a second, okay?
But just for the sake,
just for this- for the fact that your robot woke up, um,
in this state s,
you get the immediate- you get a reward R of s_0 right away.
This is something that's called- this is also called the immediate reward.
[NOISE] Right.
Uh, because, you know,
just for the, for the, uh,
good fortune or bad fortune of starting off in this state,
the robot gets a reward right away.
This is called the immediate reward.
And then it will take some action and get to some new state s_1.
Where it will receive, you know,
Gamma times the reward of s_1.
And then [NOISE]. Right.
And then it will get some future reward at the next step and so on.
Um, and just to flesh out the definition,
the value function v_Pi is really this.
Given that you execute the policy Pi and
our s_0 equals s, right, and you start off in this state as 0.
Now, what I'm going to do is rewrite this part of the equation little bit.
I'm going to factor out.
I'm just going to take the rest of this and factor out one factor of Gamma.
So let me put parentheses around this,
right, and just take out Gamma there.
Okay. So I'm just, you know, taking this previously this was Gamma squared, right?
But adding the parenthesis here,
I'm just taking out one factor of Gamma,
uh, that multiplies in the rest of that equation,
okay? Does that make sense?
No. So as Gamma R of s_1 plus gamma squared R of s_2,
plus dot, dot, dot equals Gamma times R of s_1 plus.
Okay. So that's, that's what I did down there, right,
just factor out one, one factor of Gamma.
And so, um, this is the,
the value of state s is the immediate reward,
plus Gamma times the expected future rewards.
Right? So this, the expected value of this
is really v_Pi of s_1.
Right. So this- and,
and so the second term here, this,
this is the expected future rewards, right?
So Bellman's equation says that,
um, the value of a state,
the value- the expected total payoff you get if
your robot wakes up in a state s is the immediate reward plus Gamma,
times the expected future rewards.
Okay. Right. And, and this thing under,
you know, above the curly braces is really, um,
uh, asking if your robot wakes up at the state s_1,
and executes Pi, what is the expected total payoff, right?
And this when your robot wakes up in state s_1 then it'll take an action, gets s_2,
take an action, get s_3,
and this somewhat discounts the rewards for a bit, starts off with the state s_1.
Okay. Makes sense?
So, um, uh,
this- based on this,
you can write out what- justify Bellman's equation,
which is, um, and, excuse me.
And the mapping from this equation to this equation.
[NOISE].
All right. The mapping from the equation on top to the equation at the bottom is that,
S maps to S_0 and S prime maps to S_1, right?
Um, and, what was I going to say, um, and so if we have that V_Pi of S equals,
um, makes sense? [BACKGROUND]. So the value of,
um, state S is, uh,
R of S plus V_Pi of S prime,
where this is really S_0 and this is S_1.
Uh, and and in, in the notation of MDP,
if you want to write a long sequence of states,
we tend to use S_0,
S_1, S_2, S_3, and S_4,
and so on, but if you have, want to look at
just the current state and the state you'd get to after one time step,
we tend to use S and S prime for that.
So that's why there's this mapping between these two pieces of notation.
Uh, so S prime let's say you get to after one step,
well, let's see, what is S prime drawn from, right?
This so- the, the,
the state S prime or S_1 is the state you get to after one time step.
So what is, what is the distribution the S prime is drawn from?
S prime is drawn from P of what?
S.
Okay, P of S, and then?
Pi of S.
Pi of S, pretty cool. Does that make sense?
Because, um, in state S,
you will take action a equals Pi of s, right.
So we're executing the policy Pi.
So that means that when you're in a state S,
you're gonna take the action a given by Pi of S,
because Pi of S tells you,
please take this action a when you're in sate S. And so, um,
S prime is drawn from P of Sa,
where a is equal to Pi of S, right?
Because they- because that's the action you took,
which is why S prime,
the state you get to after one time step,
is drawn from a distribution S Pi of S, okay?
Wow, that pen really left a mark.
So putting all that together, that's why- well,
I just write out again, where Bellman's equation which is, um,
V_Pi of S equals R of S plus
the discount factor times the expected value of V_Pi of S prime.
And so this term here is just sum of
S prime V S Pi of S, V_Pi of S prime.
So that underlying term I guess is this just underline term here, okay?
Um, now, notice that this gives you
a linear system of equations for actually solving for the value function.
Um, so let's say I give you a policy, right?
It could be a good policy, could be a bad policy,
and you want to solve for V_Pi of S. What this, um, does is,
if you think of V_Pi of S as the unknown you're trying to solve for, um, given Pi,
right, these equations [NOISE] ,
um, these
equa- the Bellman's equations
defines a linear system of equations,
uh, in terms of V_Pi of S as the ve- values to be solved for.
So make sure- here's a, here's a specific example.
Um, let's take the state V1,
right, so this is the state V1, okay.
What this- what Bellman's equation this tells us is,
V_Pi of the state 3, 1 is
equal to the immediate reward you get at the state 3,1,
plus the discount factor times,
well, sum of S prime PS Pi of S V_Pi of S prime, right?
So, um, when- let's see- le,
le- let's say that Pi of 3,1 is north, right?
So let's say you try to go north.
If you try to go north from this state,
then you have a 0.8 chance of getting to 3, 2,
plus a 0.1 chance of, uh, veering, uh,
left, plus a 0.1 chance of veering right.
Um, let me just close out that parenthesis, okay.
So that's what Bellman's equation says about these values.
All right, and if your goal is to solve for the value function,
then these things I'm just circling in purple are the unknown variables [NOISE] okay?
And, um, if you have 11 states,
uh, like in our MDP,
then this gives you a system of 11 linear equations with 11 unknowns.
Um, uh, and so using sort of a linear algebra solver,
you could solve explicitly for the value of these 11 unknowns. Does that make sense?
Okay. So the way you would- so let's say I give you a policy Pi,
you know, any policy Pi.
Um, the way you can solve for the value function is,
create an, an 11 dimensional vector, um,
with V_Pi of, you know, 1, 1,
V_Pi of 1, 2 and so on,
down to the V_Pi of whether is the last thing.
You have 11 states, so V_Pi of 3, 3 or whatever, of 4, 3, right?
So if you want to,
er, solve for those, um,
11 numbers I wrote up just, uh,
in terms of defining V_Pi, what you can do is,
I'll give you a policy Pi,
you can then construct an 11 dimensional vector,
you know, 11 dimensional vector of unknown values that you want to solve for.
And Bellman's equations for each of the 11 states,
um, for each of the 11 states you could plug in on the left-hand side.
This gives you one equation for how one of the values is
determined as a linear function of a few other of the values in this vector, okay?
And so, um, what this does is it sets up
a linear system of equations with 11 variables and 11 unknowns, right?
And using a linear algebra solver, you,
you will be able to solve this linear system of equations. Does that make sense?
Okay. Um, all right.
And so this works so long as you have a discrete-
If you have 11 states, you know,
it takes like a, it,
it takes almost a- takes almost no time,
right, in a computer to solve a linear system of 11 equations.
So that's how you would actually get those values,
if you're ever called on to solve for V_Pi, okay?
[NOISE] Actually, the, the- did what I just say make sense?
Raise your hand if what I just explained made sense.
Okay, good, awesome, great.
All right, good.
So moving on our roadmap,
um, we've defined V_Pi,
let's now define V_star.
Um, so [NOISE].
So V star is the optimal value function.
And we'll define it as V star of S
equals max over all policies Pi of V Pi of S. Okay.
Um, one of the I don't know, slightly confusing things about
reinforcement learning terminology is that there are two types of value function.
There's value function for a given policy
Pi and there is the optimal value function V star.
So both of these are called value functions,
but one is a value function for a specific policy,
could be a great policy, could be a terrible policy, can be the optimal policy.
The other is V star which is the optimal- optimal value function.
So V star is defined as,
um, look at the value for, you know,
any- lo- lo- look across all of the possible policies you could have all, um, 4-11.
Over all the combinatorially large number of possible policies for this MDP.
And V star of this is,
well let's just take the max,
where was of all the possible- of all the policies
you know anyone could implement of all the possible policies,
let's take the value of the best possible policy for that state, so that's V star.
Okay. And that's the optimal- optimal,
um, optimal value function.
And it turns out that, um,
there is a different version of Bellman's equations for this.
And again, there's a Bellman's equation for V_Pi,
for value of a policy.
And then there's a different version of
Bellman's equations for the optimal value function, right?
So just as the two versions of value functions,
there are two versions of Bellman's equations.
But let me just write this out and hopefully this will make sense.
Um, actually let's think this through.
So let's say you start off your robot in a state S,
what is the best possible expected sum of discounted rewards?
What's the best possible payoff you could get, right?
Well, ah, just for the privilege of waking up in state S,
the robot will receive an immediate reward R of S, all right?
And then it has to take some action and after taking some action,
it will get to some other state S prime.
Um, you know, and after some other state S prime
it will receive, right, future expected rewards V star of S prime,
and we have to discount that by Gamma, right?
So, sorry. So well,
the state S prime was arrived at but [NOISE] you're taking
some action a from the initial state.
Um, and so whatever the action is you know,
for- if, if you take action a, right?
Okay, um, so if you take an action a in the state S,
then your total payoff will be- expected total payoff will be the immediate reward
plus Gamma times the expected value of the future payoff.
But what is the action a that we should plug it in here?
Right. Well, the optimal action to take in the MDP is
whatever action maximizes your expected total payoff,
maximizes the expected sum of rewards which is why
the action you want to plug in is just whatever action a maximizes that.
Okay. So this is Bellman's equations for the optimal value function,
which says that, ah,
the best possible expected total payoff you could receive
starting from state S is the immediate reward R of S,
plus max over all possible actions of whatever action allows you to maximize,
you know, your expected total payoff- expected future payoff, okay?
So this is the expected future payoff,
or expected future reward, okay.
Um, now based on the argument we just went through,
um, this allows us to figure out how to
compute Pi star of S as well, right?
Which is, um, let's say-
let's say we have a way of computing V star of S, but we don't yet.
But let's say I tell you what is the V star over S,
and then I ask you, you know,
what is the action you should take in a given state?
So remember, Pi, Pi star,
oh Pi star is going to be optimal policy, right?
And so, um, what should Pi star of S be, right?
Which is le- let's say- let's say we're computing V star.
Um, and now I'll see you,
"Hey, my robot's in state S,
what is the best action I should take from the state S, right?
Then how do I- how do I decide what actions to take in the state S? What, what optimal?
What do you think is the best action to take from the state?
And the answer is almost given in the equation above, yeah.
[inaudible].
Yeah, cool. Awesome, right.
So the best action to take in state S,
and best means of maximizing respect to total payoff.
But the action that maximizes your expected total payoff is, you know,
what- whatever action we were choosing a up here.
And so it's just argmax over a of that.
And because Gamma is just a constant that,
that doesn't affect the argmax,
usually we just eliminate that since it's just a positive number, right?
So this gives us the strategy we will use for finding, um,
the optimal policy for an MDP, which is, um,
we're going to find a way to compute V star of S,
which we don't have a way of doing yet, right?
V star was defined as a max over a combinatorially or exponentially large number policy.
So we don't have a way of computing V star yet.
But if we can find the way to compute V star,
then you know, using this equation,
sorry, let me just scratch this out.
Using this equation gives you a way for every state of every state S,
to pretty efficiently compute this argmax, um,
and therefore figure out what is the optimal action for every state, okay?
[NOISE].
All right, um.
So all right. So just to practice with confusing notation.
All right, let's see if you understand this equation.
I'm, I'm just claiming this. I'm not proving this.
But for every state as V star of S equals V of Pi star of S,
is greater than V Pi of S, all right?
For every policy Pi in every state S, okay?
So ho- hope this equation makes sense.
Ah, this is what I'm claiming. I didn't prove this.
What I'm claiming is that, um,
the optimal value for state S is- this is the optimal value function on the left.
This is the value function for Pi star.
So this is- this is the optimal value function.
This is the value function for a specific policy Pi,
where the policy Pi happens to be Pi star.
And so what I'm claiming here is that- wh- what I'm writing here is that, um,
the optimal value for state S is equal to
the value function 4 Pi star applied to the state S,
and just as greater than equal to V Pi of S for any other policy Pi, okay?
Right. All right. So, um,
the strategy you can use for finding for optimal policy is: one, ah, find V star.
Two, you know, use
the argmax equation to
find Pi star, okay?
And so what we're going to do is- well, step two, right?
We, we know how to do from the argmax equation.
So what we're gonna do is talk about an algorithm for actually
computing V star because if you can compute V star,
then this equation helps- allows you to pretty quickly find the optimal, um,
action for every state [NOISE]. So, um.
So value iteration is,
ah, is an algorithm you can use to,
um, to find V star.
So let me just write out the algorithm, um.
So this is um-
Okay? So in the value iteration algorithm,
you initialize the estimated value of every state to 0,
and then you update these estimated values using Bellman's equation.
And this is the, uh, optimal value function,
the V star version of Bellman's equations, right?
And, um,
[NOISE] so to be concrete about how you implement this,
you know, if you're implementing this, right?
If you are implementing this in Python, um,
what you would do is create
a 11 dimensional vector to store all the values of V of S. So you create a,
you know, 11 dimensional vector, right?
That, that represent V of 1, 1,
V of 1, 2, you know,
down to V of 4, 3, right?
So this is, um, 11 dimensional vector corresponding to the 11 states.
Um, [NOISE] oh, I'm sorry I shou - wait did I say 11?
We got 10 states in the MDP, don't we? Wait.
Yes, we have 10 states. We've been saying 11 all long?
Sorry. Okay, 10.
Um, uh, yeah, uh, wait.
[inaudible].
11?
[inaudible].
Oh, Yes. You're right. Sorry. Yes, 11.
Okay. Sorry. Yes, 11 states. Okay, It's all right.
Right. So 11 states MDP so you create an initial, ah,
create an 11 dimensional vector um,
and initialize all of these values to 0.
And then you will repeatedly update, um,
the estimated value of every state according to Bellman's equations, right?
Um, and so uh, there, there,
there are actually two ways to interpret this um,
and sim- similar to,
er, similar to gradient descent, right?
We've written out, you know,
a gradient descent rule for updating the Theta,
uh, the, the, vector parameters Theta.
And what you do is, you know, then you have,
um- and what you do is you update all of the components of Theta simultaneously, right?
And so that's called a synchronous update, er, in gradient descent.
So one way to- so the way you would, um, er,
update this equation in what's called a synchronous update,
would be if you compute the right hand side for
all 11 states and then you simultaneously overwrite all 11 values at the same time.
And then you compute all 11 values for
the right-hand side and then you simultaneously update all 11 values, okay?
Um, the alternative would be an asynchronous update.
And an asynchronous update,
what you do is you compute v of 1, 1, right?
And the value of v of 1, 1 depends on some of the,
the other values on the right hand side, right?
But the asynchronous update,
you compute v of 1, 1 and then you overwrite this value first.
And then you use that equation to compute v of 1, 2.
And then you update this and then you observe update these one at a time.
And the difference between synchronous and asynchronous is um, you know,
if you're using asynchronous update by the time you're using V
of 4, 3 which depends on some of the earlier values,
you'd be using a new and refreshed value of some of the earlier values on your list, okay?
Um, it turns out that
value iteration works fine with either synchronous update or asynchronous updates.
But, um, for the,
er, er, but, um,
er, because it vectorizes better,
because you can use more efficient matrix operations.
Most people use asynchronous update but it turns out that the algorithm will
work whether using a synchronous or an asynchronous update.
So I, I, I, I guess unless,
unless otherwise uh, uh,
you know, stated you should usually assume that.
Whe- when I talk about, uh, value iteration,
I'm referring to asynchronous update where you compute all the values,
all 11 values using the- a- an- and then update all 11 values at the same time, okay?
Was there a question just now, someone had, yeah.
[inaudible]
Yeah,
yes.
So I think there,
there, uh, uh, yes.
So how do you represent the absorbing state?
The sync state? We get to plus 1 minus 1 then the world ends.
Um, in this framework one way to code that up would be to say that um,
the state transition parameters from that to any other state is 0.
That is one way to, to, to- that, that will work.
Uh, another way would be, um,
less- done less often
maybe mathematically a bit cleaner but not how people tend to do this,
would be to take your, um,
11 state MDP and then create a 12 state,
and a 12 state always goes back to itself with no further rewards.
So both, both of these will give you the same result.
Mathematically, it's pretty more convenient to just set, you know,
P of Sa S prime equals 0 for all other states.
It's not [inaudible] probably but that,
that will give you the right answer as well.
Yeah. All right.
Cool. Um, so just as a point of notation,
if you're using synchronous updates,
you can think of this as, um,
taking the old value function,
er, O estimate, right?
And using it to compute the new estimate, right?
So this, this, you know,
assuming the synchronous update,
you have some, uh,
previous 11 dimensional vector with
your estimates of the value from the previous iteration.
And after doing one iteration of this,
you have a new set of estimates.
So one step of this algorithm is sometimes called the Bellman backup operator.
And so where you update V equals B of V, right?
Where, uh, where now V is,
a 11 dimensional vector.
So you have an order 11 dimensional vector,
compute the Bellman backup operator with
just that equation there and update V according to V of P. Um,
and so one thing that you see in the,
um, problem set, uh,
is prove- is, er, er, showing that,
um, this will make a V of S converge to V star, okay?
So it turns out that, um,
okay, so it turns out that, um,
er, you can prove and you'll see more details of this in the problem set,
that by repeatedly and forcing Bellman's,
er, equations, that this equa- this,
this algorithm will cause your vector of 11 values or cause
V to converge to your optimal value function of V star, okay?
Um, and more details. You- you'll see
in the homework and a little bit in the lecture notes.
And it turns out this algorithm actually converges quite quickly, right?
Um, to, to, to give you a flavor,
I think that, uh, with the discount factor,
the discount factor is 0.99,
it turns out that you can show that the error, er,
reduces, you know, by a factor of 0.99 on every iteration, um,
and so V actually converges quite,
quickly geometrically quickly or exponentially quickly,
um, to the optimal value function, V star.
And so if it's, you know, if the discount factor is 0.99, then we've like a few,
we've 100 iterations or a few hundred iterations,
V would be very close to V star, okay?
And, and the discount factor is 0.9, then we've just,
you know, 10 or a few dozens of iterations that'll be very close to V star.
So these algorithm actually converges quite quickly to V star, okay?
Um, so let's see.
[NOISE].
All right. So just to put everything together,
um, if you- if
you run value iteration on that MDP,
you end up with this. Um, er,
so this is V star, okay?
So it's a list of 11 numbers telling you what is the optimal, um,
expected pay off for starting off in each of the 11 possible states.
And so, um, I had previously said,
I think I said last week,
uh, o- of the week before Thanksgiving,
that this is the optimal policy, right?
So, you know, let's just use as a case study how
you compute the optimal action for that state,
um, given this V star, all right?
Well, what you do is you,
you actually just use this equation.
And so, um, if you were to go west,
then if you were to compute,
I guess this term, um,
sum of S prime west or left I guess, right?
P of S A, S prime V star of S prime is equal to,
um, if you were to go west, you have a, um-
Right.
Um, right.
So if you're in this state,
and if you attempt to go left,
then there's a 0.8 chance you end up there with,
ah, ah, V star of 0.75.
There's a 0.1 chance.
You know if you try to go left,
there's 0.1 chance you veer off to the north and have a 0.069.
And then there's 0.1 chance that you actually go
south and bounce off the wall and end up with a 0.71.
And so the expected future reward,
the expected future payoff given this equation is that if you tend to go west,
you end up with a 0.740 as expected future rewards.
Whereas if you were to go north,
and we do a similar computation.
[NOISE] You know, so 0.8 times 0.69,
plus 0.1 times 0.75,
plus 0.1 times 0.49,
is the appropriate weighted average.
You find that this is equal to 0.676.
Um, which is why the expected future rewards for if you go west, if you go no- ah,
left is 0.740 which is quite a bit higher than if you go north,
which is why we can conclude based on this little calculation,
um, that the optimal policy is to go left by that state, okay?
And- and really, and technically you check north,
south, east, and west and make sure that going west gives a high reward.
And that's how you can conclude that going west is actually the better action,
at this state, okay?
So that's the value iteration.
And based on this, if you,
um, ah, are given an MDP you can implement this,
ah, south of V star and, ah,
ah, be able to, ah,
compute Pi star, okay?
All right. Few more things to go over.
But before I move on, ah,
let me check if there any questions, yeah.
[inaudible]
Oh, sure yep. Is the number of states always finite?
So in what we're discussing so far, yes.
But what we'll see on Wednesday is how to generalize this framework.
I'll, I'll do this a little bit later but it
turns out if you have a continuous state MDP, ah,
one of the things that's often
done I guess is to discretize into finite number of states.
Ah, but then there are also some other versions of, um,
ah, you know, value iteration that applies directly to continuous states as well.
Okay, cool. All right.
So [NOISE].
Um, what I describe is an algorithm called value iteration.
The other, um, I know, common, ah,
sort of textbook algorithm for solving for MDP is,
is called policy iteration.
And let me just- I'll just write out what the algorithm is.
So here's the algorithm which is, um,
you know initialize Pi randomly, right?
[NOISE].
Okay, so let's see what this algorithm does.
So we'll talk of pros and cons of valuation versus policy iteration in a little bit.
Um, in policy iteration, ah,
instead of solving for the optimal policy V star,
so in- in value iteration our focus of attention was V star, right?
Where, um, you know,
you do a lot of work to try to find the value function.
And then once you solve for V star,
you then figure out the best policy.
In policy iteration, the focus of attention is
on the policy Pi rather than the value function.
And so initialize Pi randomly.
So that means for- for each of the 11 states pick a random action, right?
So a random initial Pi.
And then we're going to repeatedly carry out these two steps.
Um, the first step is, um,
solve for the value function for the policy Pi, right?
And remember, um, for V Pi,
this was a linear system of equations, right?
With 11 variables, with 11 unknowns
in a linear- there is a linear system of 11 equations with 11 unknowns.
And so using a sort of linear algebra solver or linear equation solver,
given a fixed policy Pi,
you could just, you know,
at the cost of inverting a matrix roughly, right?
You can solve for- you can solve for all of these 11 values.
And so in policy iteration,
um, you would, you know,
use a linear solver to solve for
the optimal value function for this policy Pi that we just randomly initialized.
And then set V to be the value function for that policy.
Okay, um, and so this is done quite efficiently with the linear solver.
And then the second step of policy iteration
is pretend that V is the optimal value function,
and update Pi of S,
you know, using the Bellman's equations for the optimal value function,
right, or updated, um,
as you saw right how you update Pi of S. And then you iterate,
and then give it a new policy,
you then solve that linear system equations for your new policy Pi.
So you get a new V_Pi and you keep on iterating these two steps,
um, until convergence, okay? Yeah.
[inaudible]
Yeah, yep. Yes, that's right.
So in, in, in value,
ah, yeah, yeah, yeah, yeah.
So in, in value iteration, um, ah,
actu- in value iteration think about
value iterations as waiting to the end to compute Pi of S, right?
Solve for v star first, and then compute Pi of S. Whereas in policy iteration,
we're coming up with a new policy on every single iteration, right?
Okay? So, um, pros and cons of poly- and,
and it turns out that this algorithm will also converge to the optimal policy.
Um, pros and cons of policy iteration versus value iteration.
Policy iteration requires solving this linear system of
equations in order to, um, get V_Pi.
And so it turns out that if you have a relatively small state space,
um, like if you have 11 states,
it's really easy to solve a linear system of equations,
ah, you know, of 11 equations in order to get V_Pi.
And so in a relatively small set of states like 11 states or really anything, you know,
like a few hundred states, um,
policy iteration would work quite quickly.
Ah, but if you have a [NOISE] relatively large set of states,
you know, like 10,000 states or,
or, or a million states.
Um, then this step would be much slower.
At least if you do it right by solving linear system of equations and then
I would favor a value iteration over policy iterations.
So for larger problems,
usually value iteration will, um, ah, ah,
usually I would use value iteration because solving this linear system of equations,
you know, is, is pretty expensive if it's- it's like a million Pi.
Is a million equations and a million unknowns, that's quite expensive.
But even 11 states 11 unknowns is a very small system of equations.
Um, and then one,
one other pros and cons,
one of the, ah,
ah, differences that- that's maybe,
maybe more academic and practical.
But it turns out that if you use value iteration, um,
V will converge towards V star,
but it won't ever get to exactly V star, right?
So just as, if you apply gradient descent for linear regression,
gradient descent gets closer and closer and closer to the global optimum,
but it never, you know,
gets exactly the global optimum.
It just gets really, really close, really, really fast.
Actually gradient descent, actually turns out asymptotically
converges geometrically quickly or exponentially quickly, right?
But they've been never quite gets, you know,
definitively to the optimal,
to the one optimal value.
Whereas, you, you saw using normal equations it just
jumped straight to the optimal value and there's no,
you know, converging slowly.
And so value iteration converges to a V star,
but it doesn't ever end up at exactly the value of V star.
Ah, this difference may be a bit academic because in practice it,
it doesn't have, ah, right?
Ah, ah, but in policy iteration, um,
if you iterate this algorithm then after a finite number of iterations, ah,
this algorithm will stop changing meaning that after a
certain number of iterations Pi of S will just not change anymore, right?
So you find Pi of S update the value function,
and then after another integration.
When you take these argmax's,
you end up with exactly the same policy.
And so, ah, just- just to solve for
the optimal value and the optimal policy, and then just,
you know, ah, ah,
it doesn't converge- it doesn't just converge to what the optimal value.
It just gets the optimal value when it- when it converges, okay?
Um, so I think in practice I actually see value iteration used much more,
ah, ah, ah, because, um,
solving these linear system equations gets expensive, you know,
if you have a larger state space but, um,
value iteration, excuse me,
val- I see value iteration used much more.
But if you have a small problem, you know,
I think you could also use policy iteration which may converge a little bit faster.
If, if you have a small problem, okay?
[NOISE] All right, good.
So the last thing is,
um, kinda putting it together, right?
And what if you don't know
[NOISE].
So it turns out that when you apply this to a practical problem,
you know, in- in- in robotics right.
Um, one common scenario you run into is if you do not know what is P of S, A.
If you don't know the state transition priorities right.
So when we built the MDP we said, well,
let's say the robot if you're going off you know,
has a 0.8 chance of going off and a 0.1 chance of veering off to the left or right.
If you actually- again it's a very simplified robot.
But, if you build a actual robot or build a helicopter or whatever,
play- play- play chess against an opponent.
Uh, the state transition probabilities are often not known in advance.
And so in many MDP implementations you need to estimate this from data.
And so the workflow of many reinforcement learning projects will be that,
um, you will have some policy and have the robot run around,
you know, just have a robot run around a maze and count
up of all the times you had to take the action north,
how often did it actually go north and how
often do they veer off to the left or right, right?
And so you use those statistics to estimate the state transition probabilities.
So let me just write this out.
So you estimate.
So after you're taking maybe a random policy it takes some policy,
executes some policy in the MDP for a while.
And then you would estimate this from data.
And so, the obvious formula would be,
estimate P of Sa S prime to be number of times took action a,
in the state S and got to S prime
and divide that by the number of times you took
action a in state S, right.
So P of Sa S prime estimates- does actually a maximum likelihood estimate.
When you look at the number of times,
you took action a in state S,
and of that was a fraction of times you got to the state S prime right.
Or one over S and the above is 0, 0 right.
[NOISE] And a common heuristic is,
if you've never taken this action in this state before,
if the number of times you try action A in state S is 0.
So you've never tried this action in this state.
So you have no idea what it's going to do.
They just assume that the state transition probability is 1 over 11, right?
That it randomly takes you to another state.
So this would be common heuristics that people
use when implementing reinforcement learning algorithms, okay?
And it turns out that you can use Laplace smoothing for this if you wish,
but you don't have to.
Because, so you're in Laplace smoothing right.
So it would be, you know, adds 1 to the numerator and add 11 to the denominator would be,
if you were to use Laplace smoothing,
which avoids the problems of 0 over 0s as well.
But it turns out that unlike the Naive Bayes algorithm,
these solvers of MDPs are not that sensitive to 0 values.
So if- if one of your estimates were probably a 0, you know,
unlike Naive Bayes' where having a 0 probability was very
problematic for the classifications made by Naive Bayes,
it turns out that MDP solvers,
including evaluation of policy iteration,
they do not give sort of
nonsensical/horrible results just because of a few probabilities that are exactly 0.
And so in practice,
you can use Laplace smoothing if you wish.
But because the reinforcement learning algorithms don't- don't perform
that badly if these estimates often will be a zero
in practice, Laplace moving is not commonly unison.
What I just wrote is- is more common.
Okay.
So to put it together.
All right, if I give you
a robot and asked you to implement a MDP Solver to find the good policy for this robot,
what you will do is the following.
Take actions with respect to some policy pi.
To get the experience in the MDP.
Right. So go ahead and let your robot lose and have it execute some policy for awhile.
And then update estimates of P of Sa.
Based on the observations of whether robot goes and takes different states,
update- update the estimates of P of Sa.
Solve, um, Bellman's equation using value iteration
to get V and then update.
So this is the value iteration we are putting together.
If you want to plug in policy innovation instead in this step that's also okay.
But so if you actually get the robot, um,
you know, yeah right- right.
If you actually get a robot, uh,
where you do not know in advance the state transition probabilities,
then this is what you would do in order to,
um, iterate a few times I guess.
Repeatedly find a- find a-
find a policy given your current estimate of the state transition probabilities.
Get some experience, update your estimates,
find a new policy and kind of repeat this process
until hopefully it converges to a good policy.
Okay.
Now just to
add more color and more richness to this,
we usually think of-
we usually think of the reward function as being given,
right, as part of the problem specification.
But sometimes you see that the reward function may be unknown.
And so for example,
if you're building a stock trading application
and the reward is the returns on a certain day,
it may not be a function of the state and it may be a little bit random.
Um, or if your robot is running around but depending on where it goes,
it may hit different bumps in the road and you
want to give it a penalty every time it hits the bump.
We're going to build a self-driving car right,
every time it hits a bump, hits a pothole,
you give it a negative reward,
then sometimes the rewards are a random function of the environments.
And so sometimes you can also estimate the expected value of a reward.
But- but in- in some applications,
if the reward is a random function of the state,
then this process allows you to also estimate the expected value of the reward from
every state and then running this will help you to converge. Okay yeah.
[inaudible]
Yeah, cool. [NOISE].
[inaudible]
Yeah, cool. Great question. So let me,
let me talk about exploration, right.
So it turns out that, um,
this one [NOISE] so it turns out
this algorithm will work okay for some problems but the- the- there's one other,
ah, again to add richness to this,
there's one other, um,
issue that this is not solving which is the exploration problem.
And [NOISE] in, in reinforcement learning sometimes you hear
the term exploration versus exploitation, [NOISE] right?
Which is, um, let me use a different MDP example, right.
Which is, um, if your robot, you know,
starts off here and if there is a, um,
plus 1 reward here,
right and maybe a plus 10 reward here.
If just by chance during the first time you run the robot it
happens to find its way to the plus 1 then if you run this algorithm,
it may figure out that going to the plus 1 is a good way, right?
We were giving it a discount factor and there is
a fuel surcharge of minus 0.02 on every step.
So if just by chance your robot happens to find its way to
the plus 1 the first few times you run this algorithm then this algorithm is,
um, is uh, locally greedy, right.
Ah, it may figure out that this is a great way to get to plus
1 reward and then the world ends, it stops giving these minus 0.02 surcharges for fuel.
And so this particular algorithm may converge to a bad,
you know, kind of local optima where it's always heading to the plus 1.
And as it hits the plus 1,
it sometimes will veer off randomly right and get a little bit more experience
in the right half of the state space and end up with pretty good estimates of,
ah, what happens in the right half of this state space.
And, um, and it may never find this hard-to-define
plus 10 pot of gold over on the lower left, okay?
So this problem is sometimes called actually, well,
it is called the exploration versus exploitation problem which is, um,
when you're acting in an MDP, you know,
how aggressively or how greedy should you be
at just taking actions to maximize your rewards?
And so the algorithm we describe is relatively greedy, right?
Meaning that, um, is taking your best estimate of the state transition probabilities
and rewards and is just taking whatever actions and this is really saying, you know,
pick the policy that maximizes
your current estimate of the expected rewards and it's just acting greedily,
meaning on every step it's just executing the policy that
it thinks allows it to maximize the expected payoff, right?
And what this algorithm does not do at all is explore which is
the process of taking actions that may appear less optimal at the outset,
um, such as if the robot hasn't seen this plus 10 reward, it doesn't know how to get there,
maybe it should, you know,
just try going left a couple of times just for the heck of it,
right, to see what happens.
Because even if it seems less,
even if going left from
the perspective of the current state of the knowledge of the robot,
um, maybe if it tries some new things it's never
tried before maybe it will find a new pot of gold, okay.
So this is called the exploration versus exploitation trade-off,
um, and this is actually not just an academic problem.
It turns out that some of the large online web advertising platforms,
ah, have the same problem as well.
And again, I, I, I, I have,
have mixed feelings about the advertising business.
It's very lucrative but it causes other problems, um, as well but,
but it turns out that for some of the large online ad platforms,
um, ah, you know, when a,
when an advertiser, um,
starts selling a new ad or your posts and
you add on one of the large online ad platforms,
the ad platform does not know who is most likely to click on this ad, right?
And so pure explo- pure exploitation,
boy exploitation has such horrible connotations
especially [LAUGHTER] for online ad platforms.
Ah, it's the technical term, not a,
not a social term when used in this context.
But the pure, you know,
reinforcement learning sends exploitation policy not,
not the other even more horrible sense of exploitation.
Um, would be to always just show you,
show, show users the ads that, you know,
they are most likely to click on to drive short-term revenues
because we want to just show people the ad they're most likely to click on to drive short-term revenue.
Whereas an exploration policy for large,
you know, some of these large online ad platforms,
is to show people some ads that may not be
what we think you are most likely to click on in this moment
in time but by showing you that ad or by showing
the pool of users an ad that you might be less likely to click on,
maybe we'll learn more about your interests.
And that, um, increases the effectiveness of these large or
these ad platforms at finding more relevant ads, right?
And for example, I don't know,
um, probably not- I, I, I guess,
ah there are probably no advertisements for ah,
Mars landers as I know.
But if the large online ad platforms
don't know that I'm actually pretty interested in Mars landers
if it shows me an ad for a Mars lander which I don't think such a thing exists, right?
If I did I click on it and they may learn that
showing me ads for Mars landers is a great thing,
right, ah, or, or some other thing that you may not know you're interested in.
So this is actually a real problem.
There are, um, some of the large online ad platforms, ah, um,
actually do explicitly consider exploration versus exploitation and make
sure that sometimes it shows ads
that may not be the most likely you'll click on but, you know,
allows us to gather information to then be better
situated to figure out where the future rewards to be better positioned to,
ah, learn how to match ads not just to you but to other users like you, right?
Um, sorry.
Okay but so in order to make sure their reinforcement learning algorithm,
um, ah, explores as was exploits a, um, ah,
a common a, a modification to
this would be tak- instead of taking actions with respect to Pi,
you may have a, um, a 0.9 chance.
[NOISE] Respect to Pi and 0.1 chance,
[NOISE] take an action randomly, okay.
And so, um, this particular,
[NOISE] exploration policy is called
Epsilon-greedy where on every time step and on every time step you toss a biased coin.
But on every time step,
let's say 90% of the chance you execute whatever you think is
the current best policy and with 10% chance you just take a random action.
And this type of exploration policy, um,
increases the odds that you know,
every now and then maybe just by chance, right,
it'll find it's way to the plus 10 pot of
gold, and learn state transition probabilities and,
and, and then eventually, um,
end up exploring the state-space more thoroughly, okay.
Um, this is called Epsilon-greedy exploration and,
um, it's a little bit of a misnomer I think.
So in, in, in the way we think of Epsilon-greedy Epsilon is, um,
say 0.1 is the chance of taking a random action instead of the greedy action.
Um, this algorithm is,
has always been a little bit strangely named because, ah,
if 0, 0.1 is actually the chance of you acting randomly, right.
So Epsilon greedy sounds like you're being greedy 0.1 of the time but,
but you're actually taking actions randomly 0.1 at a time
so Epsilon-greedy is actually maybe 1 minus Epsilon-greedy.
So th- these name has always been a little bit,
um, off but that's what,
that's, that's how people use this term.
Epsilon-greedy exploration means Epsilon of the time which is the hyperparameter,
which is the parameter of the algorithm you act randomly into- instead
of going to what you think is the best policy, okay.
And it turns out that, um,
if you implement this algorithm with, um,
Epsilon-greedy exploration then this,
ah, ah, this algorithm,
ah, will converge to the optimal policy for any discrete state MDP, right.
Ah, sometimes they take a long time because, you know, if there's a,
if it takes a long time to randomly find plus 10, it, it,
it could take a long time before it randomly stumbles upon the plus 10 pot of gold.
But, um, this algorithm with an,
with an exploration policy will converge to the optimal,
um, will, will converge to the optimal policy for any MDP. What is your question?
[inaudible]
Yeah, yeah, so, right, should you always keep epsilon constant or should you use a dynamic epsilon.
So yes, ah, there are, there, there are.
There are many heuristics for how to explore, ah.
One reasonable thing to do would be we start with
a large value of epsilon and we slowly shrink it.
Um, another common heuristic would be,
um, there is a different,
ah, type of exploration called Boltzmann exploration,
which you can look up if you want which is, ah,
if you think that the value of going north is,
um, you know, 10 and the value of going south is 1,
then there is such a huge difference that you
should bias your action to upgrading to the bigger result,
the, the bigger reward and,
ah, you could have the probability be f E to the value basically time,
ah, divide, times of a times the scaling factor, right?
So that's called Boltzmann exploration where instead
of having a 10% chance of taking an action completely at random,
ah, you could just, you know,
have a very strong bias to,
heading toward the higher values but also have some probability to go into
lower values but where
the exact probability depends on the difference in ideal values is.
So another probably the, I think Epsilon-greedy,
I feel like I see this used the most often
for these types of MDPs and then Boltzmann exploration
which is why I just drive this also. Two more questions before we wrap up, go ahead.
[inaudible]
Yes, can you get a reward for reaching states you've never seen before?
Yes, there is a fascinating line of research called intrinsic reinforcement learning.
Ah, and it really started by search indexing.
If you Google for intrinsic,
intrinsic motivation, you find some research papers on.
Um, and then there was some recent followup work I think by
DeepMind or some other groups but intrinsic motivation
is the term to Google where you reward
a reinforcement learning algorithm for finding new things about the world.
Just one last question.
How many actions you should take with respect to Pi?
Sorry, say that again?
How many actions you should take with respect to Pi before updating the Pi?
I see, right. How often,
how many actions you should you take before updating Pi?
Um, there's no harm to do it as frequently as possible.
Ah, in the, if you're doing this with a real robot what,
you know, I've seen is, um,
this is sometimes going to physical robot and so, you know, I don't know,
when we're flying helicopters you go out to the field for the day,
collect a lot of data, and they go back to
the lab in the evening and rerun the algorithms.
Ah, but if there's no barrier to running this all the time,
then it doesn't hurt the performance,
it's just running as frequently as it can.
All right, that's it for basis of MDP.
Um, on Wednesday, we'll continue with generalizing all these to continuous state MDPs.
Okay, let's break, I'll see you on Wednesday.
 All right. Hey, everyone. Welcome back.
Um, so let's continue our discussion today of reinforcement learning and MDPs.
And specifically, what I hope you learned from
today is how to apply reinforcement learning,
um, even to continuous state or infinite state MDPs.
Um, so we'll talk about discretization, model-based RL,
we'll talk about models/simulation and fitted value iteration is the main algorithm,
um, I want to lead up to for today.
Um, just to recap.
Because we're gonna build on what we had learned, uh,
in the last two lectures,
I wanna make sure that you have the notation fresh in your mind.
Um, MDP was states,
actions, transition probabilities, discount factor reward. That was an example.
Um, V Pi was the value function for a policy Pi which is the expected payoff.
If you execute that policy starting from
a state S and V star was the optimal value function.
And last time, we figured out that if you know what is V star,
then uh, Pi star,
the optimal policy or the optimal action for a given state,
can be computed as the argmax of that, right?
Um, and, uh, one, one,
one thing though we'll come back to later is,
uh, an equivalent way of writing that formula,
is that this is the expectation with respect to S prime drawn from
P_sa of V star of S prime, right?
So when we go to, uh, er, er,
we've been, we have been working with discrete state MDPs with an 11 state MDP.
So this is the sum over all the states S prime.
But when we have- when we go to continuous state MDPs,
the generalization of this or what this becomes-
this is the expected value with respect to
S prime drawn from the state transition probabilities here with- of,
uh, index Pi_sa, covers state, covers that action of the value that you attain in the future.
So V star of S prime.
Okay? Um, and then we saw the value iteration algorithm.
We're also- so we talked about valuation policy iteration.
But today, uh, we're built on value iteration,
but the value iteration algorithm uses Bellman's equations, uh,
which says, take the left-hand side,
set it to the right-hand side, right?
And, uh, for, for V star,
if V was equal to V star of the left-hand side is equal to the right-hand side,
that was, um, oh, I'm sorry.
It's missing a max there.
Right? Um, for- if V was equal to V star,
then the left-hand side and the right-hand side will be equal to each other.
But, uh, what value iteration does is an algorithm that initializes V of S as
0 and repeatedly carries its update until V converges to V star.
And after that, you can then, um,
compute Pi star or find for every state find,
the optimal action A.
Okay? So um, because we're gonna build on this notation and this set of ideas today,
I just want to make sure all this makes sense, right?
Any questions about that before we move on?
No? Okay. Cool. All right.
So, um, no?
No. Okay. So everything we've done so far was built
on the MDP having a finite set of states.
Right? So with the 11 state MDP,
S was a discrete set of states.
Um, last time on Monday,
I think somebody asked, "Well,
how do you deal with continuous states?"
So we'll, we'll, we'll work on that today.
But, uh, let's say you want to build a, um,
[NOISE] uh, let me draw a car.
Right? Let's say you want to build a ar,
you know, maybe a self-driving car.
Right? Um, ah, the state space of a car is, um, let's see.
I'm gonna- well, instead of taking the- my artistic side view of the car,
if you take a top-down view of a car.
Right? So this is from the satellite imagery, you know,
top-down view of a car where we have two views of the car heading this way.
Um, how do you model the state of a car, right?
Well, a common way to model the state of a car that's driving around on planet Earth,
is that you need to know the position.
Right? Um, and so that can be represented as x,
y, uh, two numbers represent,
you know, longitude or latitude or or something.
Right? Um, you probably want to know the, uh,
orientation of a car by maybe measured relative to North,
you know, what's the orientation of [NOISE] the car?
Um, and then it turns out if you're driving at very low speeds, this is fine.
But if you're driving at anything other than very low speeds, then, um,
[NOISE] we'll often include in the state-space,
also the velocities and angular velocity.
So x dot is the velocity in the x direction.
So x dot is dx, dt.
Right? Or it's the velocity and acceleration, y dot.
So velocity in y direction and Theta dot is the angular velocity,
the rate at which your car is turning.
Okay? And it's sort of, um,
up to you, how you want to model the car,
is it important to model the current angle of the steering wheel,
is it important to model how worn down is your front
left tire as opposed to how worn down is your, your right tire.
So depending on the application you are building,
is up to you to decide what is the,
um, state-based- state-space you want to use to model this car,
um, and I guess- and, and if you are building a car to,
to, to race in a racetrack,
maybe it is important to model what is the temperature of the [NOISE] engine and how,
you know, worn down is each of your four tires separately.
But for a lot of normal driving, uh, this would be,
uh, uh, you know, sufficient level of detail to model the state-space.
Okay? Um, but so this is a, uh, six-dimensional,
uh, um, so this is a,
uh, six-dimensional state-space representation.
Oh, and for those that work in robotics, uh,
that would be called the kinematic model of the car,
and that would be the dynamics model of the car.
Right? If, if you want to model their velocities as well.
Um, or let's see actually, all right.
How about a helicopter?
Right? I don't know.
I hope this is a helicopter. All right.
Ah, the states- how, how,
how do you model a state-space of a helicopter?
Helicopter flies around in 3D rather than drives around in 2D.
And so common way to model the state-space helicopter would be to
model it as-
[NOISE]
having a position x, y, z.
Um, and then also, a 3D orientation of a helicopter is usually modeled with,
uh, three numbers which we sometimes call the roll, pitch, and yaw.
Right? So you're- if, if,
if you're in an airplane roll is that you are rolling to left or right,
pitch is are you pitching up and down,
and yaw is, you know, are you facing North,
South, East or West, right?
So there's one way to turn the three dimensional orientation
of an object like an airplane or a helicopter into three numbers.
So, so, uh, er,
the, the details aren't important.
And if you actually work on a helicopter, you would figure this out.
But for today's purposes it's just- right, uh,
I- I guess the [NOISE] roll, pitch, yaw.
Right? But that, uh, um, to represent the, uh,
orientation of a three-dimensional object flying around,
this is conventionally represented with three numbers,
uh, such as roll, pitch and yaw.
Um, and then [NOISE] x dot, y dot,
z dot, Phi dot,
Theta dot, Psi dot.
They're linear velocity and the, um, angular velocity.
Okay? Um, [NOISE] maybe just one last example.
So it turns out in, in,
in reinforcement learning, uh,
maybe early, early history of reinforcement learning,
one of the problems that a lot of people just happened to work on, um,
uh, and, and, and therefore,
you'd see in a lot reinforcement learning textbooks,
there's something called the inverted pendulum problem.
But what that is, is a little toy, um,
which is a little cart,
that's on wheels, that's on a track, um,
and you have a little pole that is attached to this cart and there's a
free [NOISE] swivel there, right?
Uh, and so this pole just flops over or
this poll just swings freely and there's no motor,
uh, there's no motor at this- at this little hinge there.
[NOISE] And so the inverted pendulum problem is- see that I've prepared this.
Right? Is- [LAUGHTER] no.
I've always prepared this.. If, if, if you have, uh,
if you have a free pole and if this is your cart moving left and right,
the inverted pendulum problem is, you know,
can you, if you see it swivel,
can you kind of balance that, right?
[LAUGHTER] Um, and so,
uh, one of the- so common textbook examples of, uh,
um, reinforcement learning is, uh,
can you choose actions over time to move
this left and right so as to keep the pole oriented upward?
Right? And so for a problem like this, um,
if you have a linear rail just a one-dimensional, you know,
like a railway track that this cart is on,
the state-space would be x which is the,
uh, position of the cart.
Um, [NOISE] Theta which is the, ah,
orientation of the pole as was x dot and Theta dot.
Right? So this would be a
four-dimensional state-space for the- for the inverted pendulum if,
if it's like running left and right on
a railway track- on a one-dimensional railway track, right?
Um, all right.
Cool. So, uh,
for all of these problems,
if you want to build, you know,
a self-driving car and have it do something or, um,
build an autonomous helicopter [NOISE] and have it either hover stably or
fly a trajectory or keep the pole upright in inverted pendulum.
These are examples of robotics problems where you would
model the state space as a continuous state-space.
So what I wanna do today is focus on problems where the state-space [NOISE] is, um, R_n.
So n-dimensional set of row numbers.
And in these examples,
I guess n would be 4 or 6 or 12.
Right? Oh, and, uh, again,
for the- for the mathematicians in this class, technically,
uh, angles are not real numbers because they wrap around,
and they only go to 360, and then they wrap around to 0.
But I think for the purposes of today,
uh, that's not important.
So we just treat this as R_n.
Oh, yeah. Okay. [NOISE].
[NOISE] So [NOISE] all right.
Um, so the most straight- straightforward way-
[NOISE] the most straightforward way
to work with a continuous state space is discretization where,
um, you know, you might have in this example a two-dimensional state-space,
maybe, ah, x and Theta for the inverted pendulum.
And then you just lay down the cell or grid values, right?
And discretize it back to a- a discrete state problem.
Ah, and so, you know,
so you can give the states a set of names, one, two, three, four,
whatever and anywhere within that little square you just
pretend that your MDP in the robot is in state number 1.
So this takes a continuous state problem and turns it back to a discrete state problem.
Um, this is such a simple straightforward way to do it.
Ah, this is actually reasonable to do for small problems.
Um, and if you have
a relatively small low dimensional states MDP like an inverted pendulum problem,
you know, four-dimensional, it's actually perfectly
fine to discretize the state 3 and solve it this way.
Ah, let me describe some disadvantages of discretization first.
And then- and then we chat a little bit about when you should just
use discretization because even though it's not the best algorithm,
it works fine for smaller problems.
But for bigger problems,
we'll have to go to more sophisticated algorithms like fitted value iteration, okay?
But, um, so what are the problems with discretization, right?
Well first, actually this marker is-
[NOISE] this is a very-
you know there's kind of a naive representation,
ah, for, ah, V_star,
ah, and Pi_star, right?
Which is- you know,
remember the very first problem we talked about,
ah, predicting housing crisis?
Um, imagine if x was the size of a house,
and the vertical axis was the price of a house, ah,
and you have a dataset that looked like this, [NOISE] all right?
Discretization is the- the- the discretization equivalent of trying
to fit a function as data would be to look for the input feature and,
um, you know let's discretize it into Phi values.
And for each of these little buckets in- in each of these five intervals,
let's fit a constant function,
right, or something like that, right?
So this staircase would be how,
you know, discretization will represent the price of a house as a function of the size.
Um, and the analogy is that what
we're doing in reinforcement learning is we want to approximate the value function.
And if you were to discretize it then,
um, on the x-axis is maybe the state.
And now, I'm down to one-dimensional state, right?
Cause that's where I can plot.
And you are saying that, well,
let's approximate the value function, you know,
as a- as a staircase function,
as a function of the set of states, right?
And you know- and this is not terrible.
If you have a lot of data and very few input features,
you can get away with this. This will work, okay?
But, it- it- it- it doesn't,
it doesn't seem to s- allow you to fit a smoother function, right?
Um, so that's one downside.
It's just not a very good representation.
Um, and the second downside is the, ah, dimensionality.
All right. Some- somewhat fancifully named cursive dimensionality,
which is, ah, and Richard Bellman had given this name, and this is a cool sounding name.
But, what it means is that if, um,
the state space is in R_n, um, and discretize,
you know, each dimension into k values,
then you get k_n discrete states, right?
So if we discretize, ah,
position and orientation into 10 values which is quite small,
then you end up with you know 10-n states which
grows exponentially and in the dimensional state space n. So, um,
discretization works fine if you have relatively low dimensional problems,
like two-dimensions, no problem,
four dimensions maybe it's okay.
But they were very high-dimensional state spaces.
Ah, this is- this is not a good- this is not a good representation, right?
And, um, it turns out the cursive dimensionality- to take a slight aside
from continuous state spaces because dimensionality also
applies for very large discrete state MDPs.
So for example, one of the places people have
applied reinforcement learning is in factory optimization, right?
So we have a factory with 100 machines in a factory
and if every machine in the factory is doing something slightly different, um,
then if you have 100 machines in a giant factory, ah,
each- and each machine can be in k different states,
then the total number of states of your factory is,
um, k to the power of 100, right?
And so even if- so- so cursive dimensionality also
applies to very large discrete state spaces such as if you have a factory,
with 100 machines, and then your total state space becomes k to the 100.
Um, and it turns out that for this to have a discrete state space,
ah, fitted value iteration can be a much better algorithm as well.
We'll get to fitted evaluation in a little bit, okay?
So, um, let's see.
So some practical- so, ah,
now despite all this criticism of digitalization
if you have a small state space there's a simple method,
ah, to try to apply, you know.
And- and if- if you have a very small state space,
go ahead and discretize it if you want
quick things to try and just get something working.
Ah, so let me share with you some maybe guidelines.
Ah, this is- this is how I do it I guess, right?
If you have a, you know,
two-dimensional state space or
three dimensional state space, it's no problem, just discretize.
Of usually for a lot of problems,
uh, it's just fine.
Um, if you have maybe a 4-6 dimensional state space,
um, you know, I would think about it,
ah, and it will still often work.
So for the inverted pendulum which is four-dimensional state space,
it works just fine.
Um, I've had some friends work on, ah,
trying to, ah, drive a old bicycle, right?
Which you can model as a six-dimensional state space, ah,
and you know discretization it- it kind of
works as it- it- it works if you put some work into it.
Ah, one of the tricks you want to use as you approach
the 4-6 dimensional state space range is,
ah, choose your discretization more carefully.
So for example, if the state S2 is really important.
So if you think the- the actions you need to take or the value of
the performance is really sensitive to the state S2 and less in the state S1,
then, um, in this range people end up designing, um,
unequal discretizations where you might discretize S2 much more finely than S1, right?
And- and the reason you do that is, ah,
the number of states, the number of discrete states is now blowing up exponentially.
Something to the power of 4, something to the power of 6.
And these tricks allow you to just reduce
a little bit the number of discrete states you end up having to model.
Um, I think, you know,
if you have a 7-8 dimensional problem,
ah, I- that- that's pushing it.
That's when I would kind of be nervous, and- and,
you know, be increasingly inclined to not use discretization.
I personally rarely used discretization for problems that are eight-dimensional.
Ah, and then when your problems that are even higher-dimensional than this.
You know like 9, 10,
and higher than I would very seriously consider,
um, ah, an algorithm that does not discretize.
Very rare, um, to use discretization for- for problems this high.
Even seven to eight is quite rare.
I've seen it done in rare occasions
but- but- and- and - and these things get worse exponentially, right?
With the number of dimensions.
So maybe there's a set of guidelines for when to use
discretization and when to seriously consider doing something else.
All right. So, um,
in the alternative approach that you see today, ah,
what you will be able to do is to approximate V star
directly without resorting to discretization, okay?
And, um, uh, there'll be an analogy that will make later,
uh, just, you know alluding to this plot again.
Right, so this analogy between linear regression where you're trying to approximate
y as a function of X and value iteration,
where you're trying to learn or approximate V as a function of s. Right, that's v star.
Which is that in linear regression, um,
you say let's approximate X as a linear function of y, right, um,
or if you don't want to use the roll features y, ah,
what you can do is,
um, use, you know,
theta transpose, theta transpose phi, oh,
I'm sorry, got that totally mixed up.
Right, where phi of X is the features of x, ah, so if, um, ah, right?
So this is what linear regression does where if X is
your housing price then maybe phi of X is equal to,
you know, X_1, X_2,
X_1 squared, X_1, X_2 and so on, right?
So that's how, that's how you can use
linear regression to approximate the price of a house,
either as a function of the raw features or as a function of some,
you know, slightly more sophisticated, slightly more complex set of features of the house.
And what, we will- what,
what you see in, um,
fitted value iteration is a model where we will approximate v star of s as,
um, a linear function of features of the state.
Okay? So that's the algorithm we'll build up to.
And, uh, um, uh,
yeah we're going to try to use linear regression with a lot
of modifications to approximate the value function.
Okay? And, and, and again in reinforcement learning in value iteration, um, the,
the- your goal is to find a good approximation to
the value function because once you have that you can then use,
you know, the equation we had earlier to
compute the optimal action for every state, right?
So, so we just focused on computing the value function.
Now in order to derive the fitted value iteration algorithm, um,
it turns out that, uh, um,
fits it value iteration, um,
works best with a model with simulator of the MDP.
So let me describe what that means and
how you get the model and then we'll talk about how you can
actually you implement the fitted value iteration algorithm
and have it work on these types of problems.
Okay?
All right.
So, um, what a model of a or
a simulator of your robot is- is just a function that takes as input
a state, takes as inputs an action and it outputs
the next state S prime drawn from the state transition probabilities.
Okay? Um, and the way that a model is built,
um, is that, um, uh,
the states and the actions,
uh, above, uh, uh, and,
and let's see, and the way the model is built is the state is just a row value vector.
Okay? Oh, and, um,
I think for simplicity, uh,
for now let's assume that the action space is discrete.
Um, it turns out that for a lot of MDPs,
the state space can be very high dimensional,
and the action space is much lower-dimensional than the state space.
Uh, so for example for a car, you know,
S is, uh, uh, six-dimensional.
But the space of actions is just two dimensional, right?
The steering and braking.
Uh, It turns out for a helicopter you know the state space is 12-dimensional.
And I guess you probably mostly, I wouldn't expect
you to know how a helicopter flies but it turns out there you have, uh,
four-dimensional actions in a helicopter.
The way you fly a helicopter is you have two control sticks,
so your left hand and your right hand you can move,
uh, uh, has two-dimensions of control.
And for the inverted pendulum, I guess,
the state space is 4D and the action spaces is just 1D, right?
You move left or right.
So you actually see in a lot, um,
reinforcing learning problems that it's quite common
for the state-space to be much higher dimensional than the action space.
And so, um, let's say for now
that we do not want to discretize the state space because it's too high dimensional.
But just for the sake of simplicity let's say
we discretize the action space for now, right?
Which is, which is usually much easier to do.
But I think as we develop fitted value integration as well, uh,
we'll- we'll you might- you'll get hints of
when maybe you don't need to discretize your action space either,
but let's just say we have a discrete,
discrete action space for now.
Okay?
So, all right
so how do you get a model, right?
Um, one way to
build a model is to use a physics simulator.
So, um, you know in the case of an inverted pendulum, right?
It turns out that, uh, uh,
well the action is what's the acceleration you apply to
either positive or negative or to the, to accelerate to the left or the right.
Then it turns out that,
um, uh, let's see,
so the state space is four-dimensional, right, and it turns out that, um,
if you sort of flip open a- a physics textbook Newtonian mechanics, uh,
if you know the weight of the car,
the weight of the pole,
um, uh, uh, yeah I think that's it actually.
If you know the mass of the car and the mass of the pole,
uh, and the length of the pole,
it turns out you can derive equations about what is the velocity, right?
So it turns out S dot is equal,
you know, don't- don't worry about this.
Think of the math as decoration rather than something you need to learn where,
you know, L is the length of the pole,
M is the mass of one of these things actually don't worry about it.
M is the pole mass, uh,
A is the force extended and so on.
Um, uh and, and a conventional physics textbook will,
will, kind of let you derive these equations, uh, or,
or rather than trying to derive this yourself using, uh, uh, you know,
either yourself using Newtonian mechanics or finding the help of a physicist friend, uh,
there are also a lot of, um, uh,
open source, uh, physics simulators and software packages.
Where you can download an open source simulator plug in the dimensions
and mass and so on of your system, and then they'll spit out of the simulator like this.
It tells you how the state evolves from one time step to another time step.
All right, and so- but so in this example the simulator would say that,
um, S prime is equal to S plus,
you know, Delta t times I guess,
uh, times S dot,
where Delta t could be lets say 0.1 seconds, right?
So you want to simulate this at 10 hertz, uh,
so that 10, 10 updates per second so that
the time difference between the current state and
the next state is one-tenth of a second.
Then you write a simulator like this.
Okay? Um, and, but- and, and really,
the most common way to do this is not to actually derive the, um, uh,
physics update equations and the most common way to do this is to just
download one or the open source physics engines, right?
So, um, this will work okay for,
uh, problems like the inverted pendulum.
Um, I once used a sort of physics engines to build
a simulator for a four-legged robot and manage to used
reinforcement learning to get a four-legged robot to walk around, right?
So it, it, it works.
Although um, um,
the second way to get a model is to learn it from data.
All right, and I, I personally end up using this much more often.
So, um, here's what I mean.
There actually- let's say you want to build a,
uh, controller for an autonomous helicopter, right?
So, so really, this is a case study.
And what I'm describing is real,
like this will actually work, right?
Uh, let's say you wanna build, uh, uh,
let's say you haven't- let's say you have
a helicopter and you want to build an autonomous controller for it.
What you can do is, um,
start your helicopter off in some state S0, right?
So with, uh, GPS accelerometers, magnetic compass,
you can just measure the position and orientation of
the helicopter and then have a human pilot,
fly the helicopter around.
So the human pilot, [NOISE] you know,
using control sticks, will move the helicopter.
They'll, they'll, they'll command the helicopter with some action A0,
and then a 10th of a second later,
[NOISE] the helicopter will get to
some slightly different position and orientation as one.
And then the human pilot, you know,
will just keep on moving the control sticks, uh,
and rec- so you record down what actions they are taken, A1.
And based on that, the helicopter [NOISE] will get to some new state S2,
and then they will [NOISE] take some action A2,
[NOISE] that get to some state S3,
[NOISE] and so on.
And, and [NOISE] let me just write this as S_T, right?
So in other words, what you do is, uh,
take the helicopter out to the field and hire a human pilot to fly this thing
for a while and record the position of the helicopter 10 times a second,
and also record all the actions that human pilot was taking on the control stick.
Okay. Um, and then do this not just one time,
but do this m times.
So let me use, uh, superscript 1.
[NOISE] Or you get the idea.
All that, great. Uh, to denote the first, uh, trajectory.
So you do this a second time, [NOISE] right?
And so on and, and, uh,
maybe do this m times, right?
So ba- thi- this is just a lot of math that's saying fly the helicopter around,
you know, m times, right?
And then record everything that happened.
And now, um, your goal is to apply, [NOISE] uh,
supervised learning, [NOISE] right?
To estimate S_t plus
1 as a function of S_t [NOISE] and A_t, right?
So the job of the model- the job of the simulator is to
take as input the current state and the current action,
[NOISE] and tell you where the helicopter is gonna go,
you know, the- like 0.1 seconds later.
And so, um, given all this data,
what you can do is apply a supervised learning algorithm to predict
wha- what is the next state S prime as a function of the current state and action, right?
And, and the other notation as [NOISE] in,
in when I drew that box for the simulator above,
I was using S prime to denote S_t plus 1 and,
uh, S and a, right?
So that's the mapping between the notations.
Um, and so [NOISE] if you use the linear regression version
[NOISE] of this idea,
um, you will say,
[NOISE] let's approximate S_t plus 1 as a linear function of the previous state,
plus another linear function of the previous state.
Um, and it turns out this actually works okay for helicopters flying at slow speeds.
This is actually not a terrible model, if, uh,
if your helicopter is moving slowly,
uh, and, and, and not flying upside down.
If, if your helicopter is flying in a relatively level way at kind of a slow speed,
this model is not too bad.
[NOISE] Um, if you're flying a helicopter in a highly dynamic situations, flying very fast,
making a very fast aggressive turn,
then this is not a great model but this is actually okay for slow speeds, right?
Um, and so I
guess A here will be,
uh, n by n matrix because,
uh, the state space is n-dimensional, you know, uh, uh,
so A is a square matrix and B,
um, will usually be a tall skinny matrix I guess,
whereas the dimension of B is
the dimension of the state space by the dimension of the action space.
Okay? [NOISE] And so,
um, in order to fit the parameters a and b,
[NOISE] you would minimize with respect to the parameters A and
B of this, [NOISE] uh,
okay. [NOISE] So you
wanna approximate S_t plus 1 as a function of that,
and so, you know,
pretty natural to fit the parameters of this linear model in a way
that minimizes the squared difference
between the left-hand side the right-hand side. Wait, did I screw up?
Yes.
Go ahead.
[inaudible].
Uh, say that again.
[inaudible].
Oh, sure. Uh, what's the difference between flying a helicopter m times versus flying a helicopter,
once, very, very long.
Uh, uh in this example,
it, it makes no difference.
Yeah. This, this is fine either way.
Uh, uh, u- u- unless, um, uh- yeah for practical purposes, it doesn't matter.
Uh, sorry. Uh, for,
for, um, for the purposes of this class, it doesn't matter.
For practical purposes, if you fly the helicopter m times,
it turns out the fuel burns down slowly.
And so the way the helicopter changes slowly and you wanna
average over how much fuel do you have or wind conditions,
this is what actually is done.
But for the purposes of understanding this algorithm,
flying a single time for a long time,
you know, works just fine as well.
Okay? Um, so this is the linear regression version of this, and, uh, uh,
and we, we actually talk about, uh, uh,
some other models later,
uh, called LQR and LQG.
Uh, you, you see this linear regression version of a model as well.
Just read, just a linear mo- model, the dynamics, right?
Uh, um, uh, we- we'll,
we'll come back to linear models dynamics later, uh, next week.
But it turns out that, um,
if you want to use a nonlinear model,
uh, you know, plugging in non-linear.
If, if you, you can also plug in,
right, Phi of S, you know,
and maybe phi prime of a as well,
if you want to have a lan- non-linear model.
Um, and, and this will work even better depending on your choice of features.
Okay? Now, um, [NOISE] finally,
having run this little linear regression thing,
where you- and it- it's not quite linear regression because A and B are matrices,
but, uh, but you can minimize this objective.
And it turns out to- this turns out to be equivalent
to running linear regression n times.
Um, so S has 12 dimensions.
This turns out to be equivalent to running linear regression
n times to predict the first state,
second state, third state variable, and so on, right?
That- that's- this is what- what this is equivalent to.
But having done this,
you now have a choice of two possible models.
One model would be to just [NOISE] say my model will set S_t plus 1
as A_St [NOISE] plus B_At,
uh, another version. [NOISE]
Would be to set St plus 1 equals A_st plus B_at plus Epsilon t,
where Epsilon t is distributed.
[NOISE] Uh, maybe from, uh, from a
Gaussian- from a Gaussian density.
Okay? Um, and so this first model would be a deterministic model,
and this model would be a stochastic model, right?
And, um, if you use a stochastic model,
then that's saying
that- [NOISE] right.
When you're running your simulator,
when you're running the model, um,
every time you generate St plus one,
you would be sampling this Epsilon from a Gaussian vector,
and adding it to the prediction of your linear model, and,
and if you use a stochastic model,
what that means is that, you know,
if you simulate your helicopter flying around,
your simulator will generate random noise that adds and subtracts
a little bit to the state space of
helicopter as if there were little wind gusts blowing it,
blowing the helicopter around, okay?
Um, and this is, uh,
uh, uh, yeah, right.
So, um, right.
So it turn- and,
uh, in, um, in most cases,
when you're building reinforcement learning models- oh,
and so the, the approach we're taking here,
this is called model-based reinforcement
learning where you're going to build a model of your robot,
and then let's train the reinforcement learning algorithm in the simulator,
and then take the policy you learn,
take the policy of how you learned in simulation and
apply it back on your real robot, right?
So this, this, this approach we're taking is called model-based RL.
[NOISE] Um, there is an alternative called model-free RL,
which is you just run your reinforcement learning algorithm on the robot directly,
and let the robot bash the robot around and so on, and let it learn.
I think that in terms of robotics applications, uh, um,
I think model-based RL has been taking off faster.
A lot of the most promising approaches are
model-based RL because if you have a physical robot, you know,
you just can't afford to have
a reinforcement learning algorithm bash your robot around for too long,
or how many helicopters do you want to crash before your learning algorithm figures it out?
Um, model-free RL works fine if you
want to play video games because if you're trying to get a computer or,
or, or, or play chess, or Othello or Go, right?
Because, um, you have a perfect simulator
for the video game which is a video game itself,
and so your, your,
your RL algorithm can, I don't know,
blow up hundreds of millions of times in a video game,
and, and that's fine, uh, for so- for,
for playing video games or for playing,
um, like, uh, you know, traditional games,
model-free approaches can work fine,
but I- most of the, um,
a lot of the, uh, uh, uh,
success applications of reinforcement learning robots have been model-based.
Although again, the field is evolving quickly so there's
this very interesting work at the intersection of model-based and model-free,
that that, that, gets more complicated, right?
But I- I- I want to say, if you want to use something tried and true, you know,
for robotics problems seriously because they're using model-based RL,
because you can then fly a helicopter in simulation,
let it crash a million times, right?
And no one's hurt, there's no physical damage anywhere in the world.
It was just, uh, uh, Okay.
And, uh, um, and- oh,
and just one last tip.
One thing we learned,
um, uh, building these,
uh, reinforcement learning algorithms for a lot of robots is that,
um, you know, having run this model,
you might ask, well,
how do I choose the distribution for this noise, right?
Uh, there- how, how,
how do you model the distribution for the noise?
Um, one thing you could do is estimate it from data.
But as a practical matter,
what happens is so long as you remember to inject- so let's see.
It turns out if you use a deterministic simulator, uh,
a lot of reinforcement learning algorithms will learn a very brittle model, uh,
that works in your simulator but doesn't actually
work when you put it into your real robot, right?
And so if you- if you actually look on YouTube or Twitter, um,
in the last year or two,
there have been a lot of cool-looking videos.
There are people using reinforcement learning to
control various weirdly-configured robots, like a
snake robot or some five-legged thing or some- whatever.
it's just a cool random,
I- I- this is- I- I- I'm not good at drawing this but, you know,
if you build a five-legged robot,
I don't even know what has five legs, right?
How do you control that?
It turns out that if you have a deterministic simulator,
um, using these methods,
it's not that hard to generate
a cool-looking video of your reinforcement learning algorithm,
supposedly controlling a five-legged thing or some crazy,
you know, a worm with, uh,
two legs or something, these crazy robots that you can build in a simulator.
But it turns out that, um,
uh, even those easy,
it's, uh, well, not easy.
Even though you can generate those types of videos in the deterministic simulator,
um, if you use a deterministic model of a robot, uh,
and you ever actually tried to build a physical robot,
and you take that policy from your physics simulator to the real robot, uh, the,
the odds of it working on the real robot are quite low,
if you use a deterministic simulator, right?
Because the problem with simulators is
that your simulator is never 100% accurate, right?
You know, it's always just a little bit off.
And one of the lessons we learned,
uh, that we've- I hope you learned, uh, [NOISE] uh,
applying RL to a lot of robots is that if you want
your model-based RL work to work not just in simulation and generate a cool video,
but you want it to actually work on a physical robot,
like a physical helicopter that you own,
that is really important to add some noise to your simulator.
Because if the policy you learn is,
um, robust to a slightly stochastic simulator,
then the odds of it generalizing,
um, uh, you know, to the, to the real world,
to the physical real world is much higher
than if you had a completely deterministic simulator.
So I think whenever I'm building a robot, right?
I- I- I pretty much- actually,
you know, I don't think I- oh,
with one exception- okay,
I [inaudible] will talk about that next week,
but with one, with one very narrow exception,
I pretty much never use deterministic simulators, uh, when,
when working on robotic control problems, unless- uh, uh,
assuming, assuming I want it to work in the real world as well, right?
Um, and, uh, and again,
you know, tips and tricks.
Uh, so, uh, the most important thing is to add some noise,
and then, uh, sometimes the exact distribution of noise.
Yeah, go ahead and try to pick something realistic,
but the exact distribution of noise actually matters less,
I want to say than just the fact of remembering to add some noise.
Okay.
[NOISE]
By the way,
I- you guys really don't know this,
but my PhD thesis, uh,
was, um, using reinforcement learning to fly helicopters.
So, so I'm trying to,
I don't know, so,
so you're telling me someone has crashed a bunch of
helicopters [LAUGHTER] model helicopters,
and has lived through the pain and the joys of seeing this stuff work or not work.
[LAUGHTER]
[NOISE]
All right. So now that you have built a model,
built a simulator, uh, for your helicopter,
for your four-legged robot or for your car, um,
how do you, um,
how do you approximate the value function, right?
So, um, in order to apply, um,
fitted value iteration, the first step is to choose features
of the state s. Right.
And then, um, we approximate v of s. You know,
we approximate v-star using a function v of s,
which is going to be Theta transpose Phi of s. Um,
and so, I don't know.
And so, uh, you know,
in the case of, uh,
uh, in, in, the case of the,
um, uh, inverted pendulum, right?
Then Phi of s,
maybe you have x, x-dot,
maybe you've x squared or x times x-dot or x,
uh, times the polar orientation, and so on.
So take, take your state to s,
and think up some nonlinear features that,
that you think might be useful for representing the value.
Um, and remember that what the value is,
the value of a state is your expected payoff from that state,
expected sum of discounted rewards.
So the value function captures,
if your robot starts off in that state,
you know, how well is it gonna do if it starts here?
So when you're designing features pick a bunch of features that you think hope convey,
um, how well is your robot doing. That makes sense?
And so, uh, maybe for the inverted pendulum, for example,
if the pole is way over to the right,
then maybe the pole will fall over given a reward of minus 1 when the pole falls over.
Right? Uh, but so, sorry.
I'm overloading the notation a bit.
Theta is both the angle of the pole as well as the parameters.
But, but, but if the pole is falling way over that looks extreme pretty badly,
unless, um, x-dot is very large and positive, right?
And so maybe there's interaction between Phi and x-dot.
So you might say, "Well, let me have a new feature,
which is the angle of the pole multiplied by the velocity."
Right? Because then-, uh,
because it seems like these two variables kind of depend on each other.
Um, so, so, so just as when you are trying to predict the price of a house,
you would say, "Well, what are the most useful features predicting the price of a house?"
Uh, um, you would do something similar,
um, for fitted evaluation.
And one nice thing about-, um, uh,
one nice thing about model-based RL is that once- model-based reinforcement learning,
is that once you have built a model,
you see a little bit that you can collect
an essentially infinite amount of data from your model.
Right? And so with a lot of data,
you can usually afford to choose a larger number of features,
because you can generate a ton of data with which to fit this linear function.
And so, you know, you- you're,
you're usually not super constrained in terms of, uh,
needing to be really careful not to choose
too many features because of fear of overfitting.
You could get so much data from your simulator that, you know,
you can usually make up quite a lot of features,
uh, and then some of the features end up not being useful, it's okay.
Because you can get enough data for running
your simulator for the algorithm to still fit a pretty good set of parameters Theta,
even if you have a lot of features.
Because you can have a lot- you can generate a lot of data to fit this function.
Okay. So, um, let's talk through the fitted value iteration algorithm.
Let's see. All right. You know what?
This is a long algorithm.
Let me just use a fresh board for this.
[NOISE].
All right. So, uh,
let me just write down the original value iteration algorithm for these v states.
Uh, so what we had previously was we would update V of s according to R of s,
plus Gamma, max over a, right?
So this is what we had, um, last Monday.
And, uh, I said at the start of today's lecture that you can also write this as this.
[NOISE].
Okay. So let's take that and generalize it to a fitted value iteration.
[NOISE].
All right. Um, so first,
let's choose a set of states
randomly, and let's initialize the parameters to equal 0, okay?
Um, and what we're going to do is where-,uh,
so, so let's see.
In linear regression, you learn a mapping from x-y,
and you have a discrete set of examples for x,
and you fit a function mapping from x and y.
So and what we're going to do here,
we're going to learn the mapping from s to v of
s. And we are going to take a discrete set of examples for s,
and try to figure out what is v of s for them,
and then for the straight line, you know,
to try to model this relationship, right.
So, so just as you have a finite set of examples,
a finite set of houses that you see a certain set of
values of x in your training set for predicting housing prices.
We're gonna see, you know,
a certain set of states,
and then use that finite set of examples to use linear regression to fit v of s. Right?
So that's what this initial sample is meant to do.
And so, um, this is the outermost loop of value iteration- of fitted value iteration.
And then for i equals 1 [NOISE] through m.
[NOISE]
Let's see, [NOISE] uh.
All right. So, um,
what we're going to do is, um,
go over each of these m states, uh,
go over each of these m states, right,
and for each one of them, um,
we're going to- and for each one of those states of each one of those actions,
we're going to take a sample of k things in order to estimate that expected value.
Right. And so this expectation
is over S prime drawn from this state transition distribution.
They say, you know, from this state,
if you take this action where you get to the next.
And so, uh, these two loops this for i
equals 1 through m. And for each action a
this is just looping over every state and every action,
and taking k samples.
Sampling k samples of where you get to if you take an action a in a certain status.
Right. And so [NOISE] um, uh,
and by taking that k examples and computing this average q a,
right, is your estimate of that expectation.
Okay. So, so all we've done so far is,
uh, take k samples, you know,
from this distribution of with S prime is drawn and average V of s. Oh,
actually, uh, oh, I'm sorry.
And, uh, if I move R of s inside,
sorry, then that's q of a. Yeah.
Okay, that makes sense?
[NOISE] Sorry.
Let me just rewrite this to move R of s inside [NOISE].
Fix this up a little bit. So this is written as Gamma.
If you write this as max over a,
of R of s plus Gamma, uh, [NOISE]
Yeah. Okay. Yes, sorry.
So we move the max and expectation out,
then this is, this is q of a.
Okay?
Um, next,
let's set y i equals max over
a of q of a [NOISE].
And so by taking the max over a of q of a,
um, that's what y i is.
Is your estimate at the right-hand side of value iteration.
Okay. [NOISE] And so
y i is your estimate for,
um, for this quantity,
for the right hand side of value iteration.
Now, in the original value iteration algorithm,
um, I'm, I'm just using VI to approximate that to abbreviate value iteration.
In the original algorithm,
what we did was we set V of S i to be equal to y i, right?
In the original value iteration algorithm,
we would compute the right hand side, this purple thing,
and then set V of s equals to that, right,
just set right-hand side equal to- I
set the left hand side equal to the right-hand side.
But in, um, fitted value iteration, you know,
V of s is now approximated by a linear function.
So you can't just go into a linear function,
and set the value of the points individually.
So what we're going to do instead is in fitted Vi,
we're going to use linear regression to make V of Si as close as possible to yi.
But V of Si is now represented as a linear function of the state.
So a linear function of the features of state.
So V of Si is Theta transpose Phi of Si,
and you want that to be close to yi.
And so the final step is run
linear regression to choose
the parameters Theta that minimizes the squared error, okay? [NOISE]
Does that make sense?
Okay, um, oh, yes. Let me just make my curly braces match.
Yeah. Okay, okay.
So that's fitted. Uh, go ahead, question?
[inaudible].
Oh, this one? Oh, this one?
Oh, no, the, the m is used differently.
Uh, so when we were learning a model m was
just how many times you fly the helicopter in order to build a model.
And the number of times you fly the helicopter in order to build a physics model,
to build a model, the helicopter dynamics has,
has nothing to do with this m,
which is the number of states you use in order to,
sort of, anchor, or in order to, uh, uh,
so I think I'm actually- so the,
the, the way to think about this is,
is you want to learn a mapping from states to B of S.
And so, uh, this sample,
this m states is- we're gonna choose m states on the x axis, right?
So, uh, and that m is the number of points you choose on the x axis.
And then in each, uh, iteration,
the value iteration we're gonna go through this procedure.
So you have sub S1 up to Sm.
Right. And then for each of these,
you're going to compute some value yi using this procedure.
And then you fit a straight line to the sample of yi's.
[inaudible].
Uh, think of this- think of the way you build a model and the way you
apply fitted value evaluation as two completely separate operations.
So, um, you can have one team of ten engineers flying a helicopter
around 1,000 times, build a model,
run the linear regression and then they have a model and
then they could publish the model on the Internet and
a totally different team could download their model and
do this and the second team does not need to talk to the first team at all,
other than downloading the model off the Internet.
There is a question.
[inaudible]
Oh, yes. Good question.
You mean they're sampling, they're sampling k times, right?
Yeah. That's a great question, yes.
That was a- yes.
That was one my next points which is the reason you sample from this distribution
is because you're using- so you should do
this if you are using a stochastic simulator, right?
And then actually it does.
Actually, I just wanted to ask you guys what should you do?
How can you simplify this algorithm if you use
a deterministic simulator instead of a stochastic simulator?
Oh, well, let's see. So if you use a determinic- deterministic simulator then, you know,
given a certain state at
a certain action it will always map to the exact same S-prime right?
So how can you simplify the algorithm?
[inaudible] action instead of drawing k times,
you only need to draw once.
Yeah, yeah, cool. Great. Yes. So if you're a deterministic simulator,
you can set k equals 1 and set the sample only once because this distribution,
it always returns the same value.
So all of these k samples would be exactly
the same so you might as well just do this once rather than K times.
Make sense? Okay cool. Yeah.
[inaudible]
Oh, this one?
[inaudible]
Oh, no. This is, um,
this is actually a square bracket.
Um, the thing is, um,
we're trying to approximate this expectation and the way you
approximate the mean is you'd sample k times if you take the average, right?
Right. So- so what we've done here is in order to approximate this expectation,
we're gonna draw k samples and then sum over
them and divide by k. So you average over the k samples.
All right, cool. Got some more question?
What's the little [inaudible] how many states
you'll get from K sample and [inaudible]
Let's see. So how do you choose M and how do you test for overfitting and so,
you know, one- once you have a model,
one of the nice things about model-based RL is let's say that Phi of S,
right, let's say that Phi of S has 50 features.
So let's say you chose 50 features to approximate
the value function of your inverted pendulum system.
Then we know that- you know that you're going to be fitting linear regression,
right, to this 50-dimensional state-space.
I mean this step here,
this is really linear regression, right?
And so you can ask,
if you want to run linear regression with 50 parameters,
how many examples do you need to fit linear regression?
And I will say you know if M was maybe 500,
right, maybe you'd be okay.
You have 500 examples to 50-50 parameters.
But if for computational reasons,
if- if it doesn't run too slowly ,
to even set M equals 1,000 or even 5,000,
then there's no harm to letting M be bigger.
So usually M, you might as well set to be as big as you feel
like, subject to the program not taking too long to run because it- it,
you know if- if you're,
um, if you're fitting- unlike supervised learning,
if you're fitting data to housing prices,
um, you need to go out and, you know,
collect data right off Craigslist or- or what's
on Zillow or Trulia or Redfin or whatever about prices of houses.
And so data is expensive to collect in the real world.
But once you have a model,
you can set M equals 5,000 or 10,000 or
100,000 and just- and then your algorithm will run more slowly.
But as long as your algorithm doesn't run too slowly,
there is no harm to setting M to be bigger. Makes sense?
Um, all right cool.
So, um, so I know that there's a lot
going on to this algorithm but this is fitted value iteration.
And if you do this, uh, this,
you can get reasonable behavior on a lot of
robots by choosing a set of features and learning
the value function to approximate the value of the- really
approximate the expected payoff of a robot starting off in different states.
Okay. Um, now just a few details
to wrap up, again, some practical aspects of how you do this.
After you've learned all these parameters,
this- you've now learned- go ahead, yeah.
[inaudible]
Oh, I see. Yes, thank you.
Um, yes.
So in this, um,
expression, where do you get V of S prime j from?
Yes. So you would get this from Theta transpose Phi of S prime j,
using the parameters of Theta from the last iteration of fitted value iteration.
Ju- just as in value iteration,
this is the values from the last iteration that you use to update a new iteration.
So then you use the last value of Theta to update the new one. Yeah, thank you.
Cool. Oh, and, um,
one- one other thing you could do which is, um,
I talked about the linear regression version of this algorithm which is, you know,
this whole- this whole exercise is about generating a sample of S
and Y so you can apply linear regression to
predict the value of Y from the values of S, right?
But there's nothing in this algorithm that says you have to use linear regression.
In order to- now that you've generated this dataset,
that's this box that I have here,
this- this is linear regression,
right, but you don't have to use linear regression.
In modern yo- deep reinforcement learning, um,
one of the ways- well one of the ways to go from reinforcement learning
to deep reinforcement learning is to just use a neural network with this step instead.
Then you can- then- then you call that deep reinforcement learning where- no.
But, hey, it's legit, you know.
[LAUGHTER]
Um, uh, but, but,
you can also use locally weighted linear regression
or whatever regression algorithm you want in order
to estimate y as a function of the state s. Yeah,
and actually if you use a neural network,
it relieves the need to choose features Phi as well,
you can feed in the raw features.
You know, your angle, your orientation and,
and using neural networks,
to learn that mapping in a supervised learning way.
Okay, um, all right.
So one last, ah, ah, important,
ah, I guess practical implementational detail, which is, um,
fitted VI right, uh, gives, uh,
approximation to V star.
And this, um, implicitly defines Pi star.
Right, because the definition for Pi star is that, um,
right. So, um, when you're running a robot,
you know, you need to execute the policy Pi right,
given the state you're gonna pick an action,
given the state you're gonna pick an action.
And, and having computed V star,
it only implicitly defines the optimal policy Pi star.
All right, um, and
so ah if you're running a robo- if you're running a robot in real time,
then you know actually if you fly a helicopter,
you might have to choose control actions at 10 hertz
meaning 10 times a second you're given the state, you have to choose an action.
Uh, uh, if you're building a self-driving car,
again a 10 hertz controller wo- would be pretty reasonable.
I guess choose a new action and maybe 10 times a second would be pretty reasonable.
Um, but how do you compute this expectation and this maximization 10 times per second?
So, um, in what we use for fitted value iteration,
we used right, a sample,
uh, of- we use k examples to approximate the expectation.
Right, but if you're running this, um,
in real time on a helicopter, you know,
probably you don't want to, uh, uh, uh,
at least I know for my robotics implementations I have been reluctant to use
a random number generator right in the inner loop of how we control a helicopter.
Right it, it, it might work but I,
but I think, you know,
it's approximately- if you want to compute
this arg max, you need to approximate this expectation and
do you really want to be running a random number generator on a helicopter?
And if you're really unlucky the random number gen-
generator generates an unlucky value,
will your helicopter do something you know, bad and crash?
Oh, I, I, I would,
again just emotionally I don't feel very good if,
uh, your self-driving car has a random number generator and,
and a loop of how it's choosing to drive.
Right, um, so just as a practical matter, ah,
ah, ah, there are a couple of tricks that people often use.
Which is, um, the simulator
is often of this form, right.
Okay, so most simulators have this form,
next state is equal to some function of the pre- uh,
previous state and action plus some noise.
And so one thing that is often done is, um,
for your deployment or for the,
you know for the, for, for,
for the actual policy you implement on the robot.
Um, set epsilon t equals 0 and set k equals 1.
Right and so, um,
so, so, so this,
this is a reasonable way to make this policy run on a helicopter,
which is during training you do want to add noise to
the simulator because it causes a policy you learn to be much more robust.
So little errors in the simulator,
your simulator is always a little bit off.
You know maybe it didn't quite simulate wind gusts
or when you turn the helicopter does it bank exactly the right amount.
Simulator is always, you know,
it's in practice is always a little bit off.
Um, so it's important to have noise in the simulator in model based RL.
But when you're deploying this in a physical simulator, um,
one thing you could do that'll be very reasonable is just get rid of
the noise and set k equals 1 and so what you would do is,
um, uh, let's see.
Um, whenever you're in the state s,
pick the action a according to
arg max over a of v of s, a.
Right so, uh, this f is this f from here.
So this is the simulator
with the noise removed.
Okay and so what you would do is actually,
and, and, you know, computers are now
fast enough you can- you could do this 10 times a second.
Right if you want to control a helicopter or a self car at 10 hertz,
you could actually easily do this, you know, at,
at, at 10 times a second,
which is your car or your helicopter is in some physical state in the world.
So you know what is S and so you can
quickly for every possible action a that you could take,
use a simulator to simulate where your helicopter will go,
um, if you were to take that action.
So go ahead and run your simulator,
you know, once for each possible action you could take.
Right, computers are actually fast enough to do this in real time.
Um, and then for each of the possible next actions you could get to,
compute v apply to that.
Uh, so, so this is really right S prime, um, uh,
drawn from P_sa, uh,
but with a deterministic simulator, right.
Right, so every 10th of a second you could
in your simulator try out every single possible action,
use your simulator to figure out where you would go under each and every single possible action,
and apply your value function to see of all of
these possible actions, which one gets my helicopter,
you know, in the next one-tenth of a second to the state that
looks best according to the value functions you've learned from fitted value iteration.
Okay, um, and it turns out if you do this then you can,
this is how you actually implement something that runs in real time.
And, oh, and I just mentioned, you know, the, the,
the idea of a training with stochastic simulator and just setting the noise to zero,
it's one of those things that's not very vigorously justified but in practice this,
this works well. Yes, question?
[inaudible].
Oh yes. Ah, so, so, um,
for the purposes of this, you can assume you have a discretized action space,
ah, and it turns out that for a self-driving car it's
actually okay to discretize the action space.
Uh, for a helicopter,
we tend not to discretize the action space but, um,
it turns out if f is a continuous function,
then you can use other methods as well.
Right this is about optimizing over the, I,
I didn't mean to talk about this and sorry this is getting a little bit deeper.
But, uh, even if a was a continuous thing, uh,
you can actually use real time optimization algorithms, uh,
to very quickly try to optimize
this function even as the function that it contains actually.
Uh, there's a literature on something called model predictive control which,
which can actually, you can actually do these optimizations in
real time and use to fly a helicopter. Just one last question.
So what's your different action you'll transition from the next stage?
So how do you know when you're still looking?
Do you make an observation or do you use your [inaudible]?
Wait oh, uh, uh say, what's the question again?
So once you are, like when you have had the helicopter,
once you pick an action you'll transition to the next stage so do you make
an observation to set up where you are or do you use the [inaudible].
Oh, I use an observation, yeah, yes, yes.
So you take an action and then
your helicopter will do something, there will be some wind,
your model may be off and so you would then a 10th of a second later,
take another you know GPS reading, accelerometer reading,
magnetic compass reading and use
the helicopter sensors that tell you where you actually are.
Now, cool. Okay, cool.
All right, I hope, uh, yeah,
hope- hopefully this was helpful.
I feel like, you know, the- I think it's fascinating that the
excitement about self-driving cars and flying helicopters and all that,
it gives well-balanced equations like these.
I, I think that's kinda cool.
[LAUGHTER] Okay, that's great.
Thanks, I'll see you guys next week.
 Okay, hey everyone. So welcome to the final week of the class.
Uh, what I wanna do today,
is share with you a few generalizations of,
um, reinforcement learning and of MDPs.
So you've learned about the basic MDP formulas of state action,
state transition probability, discount factor and rewards.
Um, the first thing you see today is two, you know,
slight generalizations of this framework to
state-action rewards and to the finite horizon MDPs.
They're making it a little bit easier for you to model certain types of problems,
certain types of robots,
so certain types of factory automation problems will be easier to
model with these two small generalizations.
So we'll talk about those first, and then second,
we'll talk about linear dynamical systems.
Last Wednesday, you saw a fitted value iteration which was a way
to solve for an MDP even when the state-space may be infinite,
even when the state space is a set of
real numbers known as RN so it's an infinite list of states.
So a continuous set states,
we use fitted value iteration in which we had to use a function approximator,
right, like linear regression,
to try to approximate the value function.
There's one very important special case of an MDP where even
if the state space is infinite or continuous real numbers,
uh, there's one important special case where you can still
compute the value function exactly without needing to use,
you know, like a linear function approximator or to use something like
linear regression in the inner loop of fitted value iteration.
Um, and so you also see that today,
and when you can take a robot or
some factory automation task or whatever problem and model it in this framework,
it turns out to be incredibly efficient because you can fit a continuous- fit
a value function as a function of the states without needing to approximate anything,
just compute the exact value function,
uh, even though the state space is continuous.
So, um, this is a framework that doesn't apply to all problems,
but when it does apply,
it's incredibly convenient and incredibly efficient.
So you see that in the second half of today.
Um, uh, yes.
Uh, uh, one, one tactical uh,
two, two tactical things, um, let's see,
from the questions that we're getting from students,
some students are asking us, uh,
how is grading in CS229?
Whatever I did well and this,
you know, didn't do so well in that.
Um, for people taking the class,
pass-fail, a C minus or better is a passing grade.
This is quite- I think this is standard at Stanford.
Uh, and, um, I think CS229 has historically been one of the heavy workload classes.
We know that people taking CS229- yeah, I see a few heads nodding.
[LAUGHTER].
I said, sorry, people, uh, uh,
taking CS229 end up, you know,
putting a lot of work in this class more,
maybe frankly more than average for even Stanford classes.
And so we've usually been quite nice.
With respect to, to grading partly, and we acknowledge that.
So I think, uh, uh, yeah I just thought that as well so don't,
don't, don't, don't sweat too much.
Do work hard for the final project,
but just don't, don't sweat too much.
Um, uh and, uh,
on Wednesday after class,
I had a funny question.
After I talked about the fitted value iteration question,
someone came up to me and said, "Hey Andrew,
um, you know, this algorithm you,
you just taught us, does it actually work?
Like it- does it actually work on autonomous helicopter?"
And the answer is yes.
Uh, the algorithms I'm teaching,
you know, if you, uh, uh,
the fitted evaluation as you learned last week,
it will work on a finite autonomous helicopter at low speeds.
So if you fly very high speeds, very dynamic maneuvers,
crazy bang, flipping upside down you,
you need a bit more than that,
but for flying a helicopter at low speeds the,
the exact algorithm that you learned, uh,
last Wednesday as well as any of the algorithms you'll learn today including, uh, LQR.
You know, if you actually ever need to fly an autonomous helicopter for real,
these algorithms will actually work.
These simply will work quite well for flying a helicopter at low speeds,
maybe not at very,
very high speeds and crazy dynamic maneuvers.
But at those low speeds these algorithms,
pretty much as I'm presenting them,
will work. So, um, okay.
So the first generalization to the MDP framework that I want to describe is,
um, state-action rewards. Um, and so, um,
so far we've had the rewards be a function
mapping from the states to the set of real numbers,
and with state action rewards- um,
this is a slight modification to the MDP formalism.
Where now, the reward function R is
a function mapping from states and actions to the rewards.
Um, and so, you know,
in an MDP you start from the state S0,
you take an action a0,
then based on that, you get S1, take an action a1,
take a state S2, uh,
get to a state S2, take an action a2 and so on.
And with the state-action rewards,
the total payoff is written like this.
Um, and this is, uh, this, this,
this allows you to model that different actions may have different costs.
Uh, for example, in the little robot wandering around the maze example,
um, maybe it's more costly for the robot to move than to stay still.
And so, uh, if you have an action for the robot to stay
still the reward can be, you know, 0,
for staying still and a slight negative reward for moving because you're burning,
uh, because, because you're using electricity.
Um, right, uh, and so in that case,
uh, Bellman's equation becomes this, V star equals,
right. Um, where now,
you still break down the value of a state as the sum of the immediate reward plus the,
you know, expected future rewards.
But now, the immediate reward you get depends on
the action that you take in the current state, right?
So this is a- and so this is Bellman's equations.
And if, uh, and notice that previously,
um, we had the max kind of over here,
but now you need to choose the action,
a, that maximizes your immediate reward plus your discounted future reward,
which is why the max kind of moved right.
If you look at the equation,
uh, if you look at this equation,
I guess the max had to move outside because now the immediate reward you get,
uh, depends on the action you choose at this step in time as well.
So these models that different actions,
um, may have different costs. Yeah.
[inaudible]
Uh, yes. Uh, yes.
Yes, this max applies to the entire expression, right, yeah.
[inaudible]
Uh, let's see. So in this formulation,
reward is determined based on the state and action,
yes that is correct.
So, um, in this formulation,
the reward depends on the, uh,
current state and the current action but on the next state you get to.
Right, um, oh, and by the way, there, there are multiple variations of formulations of MDPs,
but this is, um,
one convenient one. I guess.
The model that different costs and I think the- and,
and actually- and you find in a helicopter a common, um,
formulation of this would be to say that yanking aggressive on the control stick, uh,
should be assigned a higher costs because yanking
the control stick aggressively causes your helicopter to jerk around more,
and so maybe you want to penalize that by setting
reward function that penalizes very aggressive maneuvers.
The, the- this gives you the, uh, as a,
as a problem designer, um,
uh, sort of more flexibility.
Um, and then, uh,
uh, and then finally- so let me just write this on top.
In this formulation, um,
the optimal action- so,
uh, right, so in order to compute the value function,
you can still use value iteration, right, which is still,
you know V of S gets updated as basically the right-hand side from Bellman's equations.
So, um, value iteration works just fine for the state-action reward formulation as well.
And, uh, if you apply value iteration until V converges to V star,
then the optimal action is,
um, is, is just the argmax of that thing, right?
So, so, pi star is just the,
uh, argmax of this thing.
Where now, when you're given state,
you wanna choose the action that maximizes
your immediate reward plus your expected future rewards.
Okay. Yeah, so I think just maybe another example, um,
if you want to use an MDP to, um, um,
plan a shortest route for robot to,
say drive from here in Stanford,
to drive up to San Francisco, right?
Then, if it cost different amounts to drive
on different road segments because of traffic or because of the, uh,
speed limit on different road, then this allows you to say that, well driving
this distance on this road costs this much in terms of fuel consumption or in terms of,
uh, time and so on, right?
So the state action rewards. Or, or in factory maintenance,
uh, if you send in a team to maintain
a machine that has a certain cost versus if you do nothing that has a different cost.
But then the machine breaks down that has yet another cost depending on your actions.
Okay, so that's the first generalization.
Um, the second generalization is the finite horizon MDP.
Um, and in a finite horizon MDP, um,
we're going to replace the discount factor, Gamma,
with a horizon time,
T. Uh, and- and we'll- we'll just forget about the discount vector.
And in a finite horizon, um, MDP,
the MDP will run for,
um, a fi- a finite number of T steps.
You start with state S_0,
take an action a_0, get to S_1, take action a_1,
get to state S_T take an action a_T,
at time step T and then the world ends,
and then we're done, right?
So the payoff is
this finite sum and-
and there's just a full stop at the end of that.
Um, you can also apply discounting but usually when you have a finite horizon MDP,
maybe there's no need to apply discounting,
and so, um, this model is a problem where there are,
you know, T time steps and then the world ends after that, right?
Wo- world end sounds a bit dire.
But, uh, um, yeah,
if you're flying an airplane or you're flying a helicopter,
and you know you only have fuel,
you know, for 30 minutes, right?
Uh, er, or an RC helicopter,
let say you have 20, 30 minutes of fuel,
then you know that you're going to run this thing
for 30 minutes and then you're done and so
the goal is to accumulate as many rewards as possible up until you,
you know, run out of fuel and then you have to land, right?
So that'll be an example of a finite horizon MDP.
Now, um, and- and- and the goal is to maximize this payoff,
um, or the expected payoff over these T time steps, okay?
Now, one interesting, uh,
property of a finite horizon- of, of,
of a finite horizon MDP is that the action you take,
um, may depend on what time it is on the clock, right?
So there's a clock marching from,
you know, timestep 0 to timestep T whereupon,
right, the world ends whe- whereupon that's all the rewards the MDP is trying to collect.
And one interesting effect of this is that,
um, this pen isn't that great, is that,
um, the optimal action may depend on what,
uh, what the time is on the clock.
So, uh, let's say your robot is running around this maze and there's
a small plus 1 reward here and a much larger plus 10 reward there,
and, um, let's say your robot is here, right,
then the optimal action for whether you go left or go
right will depend on how much time you have left on the clock.
If you have only, you know,
two or three times steps left on the clock,
it's better to just rush and get the plus 1.
But we still have, you know,
10, 20 ticks left on the clock,
then you should just go and get the plus 10 reward, right?
And so in this example,
Pi star of S, um,
it's not well defined because well,
the- the optimal action to take when your robot is here in this stage,
should you go left or should you go right?
Um, it actually depends on what time it is on the clock,
and so Pi star in this example, um,
should be written as the Pi star subscript T of S,
uh, because the optimal action,
um, depends on what time T it is.
The technical term for this is that this is a non-stationary- non-stationary policy.
Um, a non-stationary means,
uh, it depends on the time,
a- as it changes over time, right.
Whereas in contrast, up until now, we've been seeing,
you know, Pi star of S is the optimal policy before we- before this formula, right,
we just said Pi star of S,
and that was a stationary policy and stationary means, uh,
there's no change over time, okay?
So one- one- one thing that, um,
I didn't quite prove but that was implicit was
that the optimal action you take in the original formulation,
uh, is the same action,
right, no matter what time it is in the MDP.
So in the original formulation that you saw last week,
the optimal policy was stationary,
meaning that the optimal policy is the same policy,
you know, no matter what time it is,
it doesn't change over time.
Whereas in the final horizon MDP setting, um,
the optimal policy, you know,
the optimal action changes over time and so this is a non-stationary policy.
So stationary versus non-stationary just means,
does it change over time or does it not change over time?
Okay? So, um, right.
If you're using a non-stationary policy anyway, uh,
you can also build an MDP with
non-stationary transition probabilities or non-stationary rewards- non-stationary.
Um, actually so maybe here's an example.
Um, let's say you're driving from campus from
Palo Alto to San Francisco and we know that rush,
hour, is that- what like 5:00 PM or 6:00 PM or something, right?
And- and- and maybe- maybe the weather forecasts
even says it's going to rain at 6:00 PM or something, right?
But so you know that the dynamics of how you
drive your car from here to San Francisco will change over time,
uh, as in the time it takes, you know,
to drive on a certain segment of the road,
is a function of time and if you want to build an MDP to solve for,
um, the best way to drive from here to San Francisco say,
then the state transitions, um,
so S_t plus 1 is drawn from state transition probabilities indexed by the state at
time T and the action at time T. And
if these state transition probabilities change over time,
then, um, if you index it by the time t,
this would be an example of a non-stationary,
um, of a non-stationary state transition probabilities, okay?
Um, al- al- alternatively,
if you want non-stationary rewards,
then you can have R_t T of S_a, uh,
is the reward you get for taking a certain action, um,
uh, you know, o- o- for being at a certain state at a certain time, okay?
Um, so all of these are different variations of- of- of MDPs, um,
and so maybe just a few examples of when you want a, ah,
finite horizon MDP or use,
um, non-stationary, uh, state transitions.
Uh, so let's see.
Um, if you're flying an airplane, right?
For- for- for some airplanes, uh,
something like for commercial- for very large commercial airplanes,
uh, sometimes over a third of the weight of the airplane comes from the fuel, right?
So actually, if you take on a large commercial airplane, you know, when you take off, uh,
from, uh, SFO and you fly
to- I don't know- I don't know where you guys fly to, I don't know.
Fly to- fly to London or something.
Right, direct flight from here to London.
Uh, by the time the plane lands,
you- you get a much lighter airplane than when you took off, um,
because, uh, maybe sometimes- maybe like a third of the weight disappears,
you know, because of burning fuel.
And so the, the dynamics, the, um,
how the airplane feels between takeoff and landing is
actually different because the weight is dramatically different,
and so, um, uh,
this would be one example of
where the state transition probability changes in a pretty predictable way, right?
Um, or- uh, right.
I already mentioned, um,
uh, weather forecasts, right.
Where, uh, weather forecasts or traffic forecasts if you're driving here or, uh, yeah,
drivi- yeah, if you're driving over different types of terrain over time,
then you know that it's gonna rain tomorrow.
Uh, we are gonna know it's gonna rain tonight and the ground will turn muddy,
you know, then all- all the traffic will turn bad.
Um, uh, and then- and then, I don't know, industrial automation.
Um, some of my friends
work on industrial automation and I think that maybe one example, um,
if you run a factory 24 hours a day,
then the cost of labor, you know,
getting people to come into the factory to do some work at noon is actually easier,
right, and less costly than getting someone to show up
in the factory to do some work at 3:00 AM, right?
And so depending on, um, uh,
really labor availability over time,
the cost of taking different actions, uh,
and the cost of, um,
and the likelihood of transitioning into
different state transition probabilities can vary over the 24-hour clock as well, right?
And so these are other examples of when, um, uh,
uh, you can have a non-stationary policy and non-stationary state transitions, okay?
Now, um, let's talk about how you would actually solve for a finite horizon MDP,
and I think for the sake of simplicity, uh,
for the most part, I'm going to not bother with non-stationary transitions and rewards.
Fo- for the most part, I'll focus on- for the most part,
I just going to forget about, you know,
the fact that this could be varying.
Um, I- I- I'll mention it briefly but I- I wanna focus on the finite horizon aspect.
So-
so let me define,
um, the optimal value function.
Um, so this is the optimal value function for
time t for starting at state S. So this is the,
um, ah, expected total payoff.
Starting in state S at time t,
and if you execute,
you know, the best possible policy, okay?
So now the, um,
optimal value function depends on what time it is,
uh, because, if, if you look at, I don't know,
that example of the plus 1 reward on the left and the plus 10 reward on the right,
depending on how much time you have left on the clock,
the amount of rewards you can accumulate can be quite different, right?
If you have more time, you have more than, you know,
you can- more time to get to the plus 10 reward,
in the, the plus 1 and plus 10 reward that I drew- uh,
example that I drew just now.
And so, um, in this example,
value iteration, um, becomes the following.
It actually becomes a dynamic programming algorithm,
uh, which you'll see in a second, okay?
Which is that- let's see, um,
[NOISE] all right, I'm gonna need three lines, let me do this here.
[NOISE]
All right, which is that V star of
t of S is equal to max over A,
R of S, A plus-
okay? Um, and, uh,
actually, this is a question for you.
So there's, there's one missing thing here, right?
So we're saying that the optimal value,
you can get when you start off in state as at time t is the max over all actions of
the immediate reward plus sum of
s prime state transition reward S prime times V star of S prime,
and then what should go in that box.
T plus 1. Okay. Cool, awesome, great.
Right?
And then pi star of S is just,
you know, argmax of a, right?
Of the- of the same thing,
of this whole expression up on top.
Um, and so this formula defines Vt as a function of Vt plus 1.
So this is like, um [NOISE] this is like the iterative step, right?
Given V10, you can compute V9,
given V9, you can compute V8,
given V8, you can compute V7.
Um, and so to start this off,
there's just one last thing we need to define,
which is V capital T at the finite step,
uh, at the final step when the clock is about to run out.
Um, all you get to do is choose the action a,
that maximizes the immediate reward, and then,
and then, and then there's no sum after that, right?
So, um, if you start off at state S at the final time step t,
then you get to take an action and you get an immediate reward,
and then there is no next state because the world just ends right after
that step which is why the optimal value at
time t is just max over a of
the immediate reward because what happens after that doesn't matter, okay?
So this is a dynamic programming algorithm in which this,
um, uh, uh, algorithm- this step on top defines,
you know, allows you to compute V star of t,
[NOISE] and then the inductive step or the n plus 1 step, I guess, is,
uh, if you then having computed V star of t for every state S, right?
So, you know, you compute this for every state S. Having done this,
you can then compute V star T minus 1 using this,
um, inductive step, then V star t minus 2 and so on down to V star of 0.
So you compute this at every state,
and then based on this,
you can compute- oh sorry,
2 pi star of t right?
Compute the optimal policy,
the non-stationary policy for every state as
a function of both the state and the time, okay?
Um, all right, cool.
And, and I think, uh,
again, I don't want to dwell on this,
but if you want to work with
non-stationary- state transition probabilities or non-stationary rewards,
then this algorithm hardly changes in that you can just add you know, if,
if your rewards and state transceiver is indexed by time as well,
then this is just a very small modification to this algorithm.
And it turns out that once you're using a finite horizon
MDP making the rewards and state transition rewards as non-stationary is,
is just a small tweak, right?
So you could yeah, yeah.
[inaudible] Uh, can you say that again.
In which form will it disappear? The attributes [inaudible].
This one? Oh a non-stationary.
So in the end you get a policy pi star subscript T of S.
[inaudible].
I'm sorry. This one?
This one. Oh, I see, sure yes.
Pi star, this is a non-stationary policy.
Yes so that's why I like yeah, yeah.
Sorry, yeah, yeah. So this- the optimal policy will be a non-stationary policy.
Yes. uh, I, I, think, uh, yes, I think,
uh, I was using pi star to, to not,
not to denote that it has to be
a fixed function type, but yes, [inaudible] . Thank you.
Yeah. Right. If you
take big T to infinity can it just become the usual value iteration?
So the- let me think.
So there are two things with that.
So the two frameworks are closely related right,
you can kind of see relationship between the valuation.
One problem with taking this framework to big T to infinity
is that the values become unbounded, right?
Yeah and that's actually one of the reasons why we
use a discount factor when you have an infinite horizon MDP,
when the MDP just goes on forever.
One of the things that discount factor does is it makes sure
that the value function doesn't grow without bound, right?
And in fact, you know, if,
if the rewards are bounded by- right,
by some R max then when you use discounting then V,
you know, is bounded by I guess R max over 1 minus Gamma, right?
By the sum of a geometric sequence.
And so but, but in a finite h orizon repeat because you only add up t rewards it,
it can't get bigger than T times R max. Yeah.
[inaudible].
Let me think. So I think that, boy.
So I think, you know, what you find is that- let's see.
Um, actually let me just draw a 1D grid just to make life simpler, right?
So let's say there's a plus 1 reward there and a plus 10 reward there.
If you look at the optimal value function,
um, depending on what time it is.
If you have two times- and let's say that the dynamics are deterministic, right?
Uh, so there's no noise then if you have two times steps left,
then I guess V star would be,
you know, 10, 10, 10, 1, 1, 1, 0, 0, 0, right?
And so, uh, depending on where you are,
I guess if you're, uh, uh, uh yeah.
Actually, in fact I guess if you're here and there's nothing you can do right,
you can't get either reward in time.
Uh, but depending on whether you're here or here or here
the optimal action will change when we compute with this pi star. This makes sense?
Yeah, that's fine.
Okay, well yeah. Maybe I do, do encourage you there.
If this- if you actually build
a little grid simulator and use these equations to compute pi star and V star,
you will see that the optimal policy when you have lots of time will be this.
Wherever you are go for the 10 rewards,
but when the clock runs down then the optimal policy
will end up being a mixer, go left and go right.
All right, cool. Hope that was okay.
Yeah [NOISE].
All right. So, um-
[NOISE]
So the last thing I want to share with you today is,
uh, Linear Quadratic Regulation.
And as I was saying at the start, um,
LQR applies only in a relatively small set of problems.
But whenever it applies,
this is a great algorithm,
and I just, you know, use it whenever, right,
it seems reasonable to apply because it's, uh, uh,
is very efficient, and sometimes gives very good control policies.
And, um, let's see.
And so LQR, um,
applies in the following setting.
So let's see.
In order to specify an MDP,
we need to specify the states actions,
the state transition release.
I'm going to use a finite horizon formulation so capital T and rewards.
This, this also works with
the discounted MDP formalism but this would be a little bit easier,
a little bit more convenient to develop with the finite horizon setting,
so let me just use that today.
And LQR applies under a specific set of circumstances,
which is that, this set of states is an RN.
Set of actions is in RD and so to specify the state transition probabilities,
we need to tell you what's the distribution of the next state given the previous state.
So to specify the state transition probabilities,
I'm gonna say that the way S t plus 1 evolves is going to be as a linear function.
Some matrix A times S t plus some matrix B times A t plus some noise and
sorry there's a little bit of notation overloading again, sorry
about that, A is both the set of actions as well as this matrix A,
right so there's two separate things but same symbol.
I think, I think that the field of- a lot
the ideas from LQR came from traditional controls.
It's from, uh, what- from,
um, I guess from EE and Mechanical Engineering.
A lot of ideas from reinforcement learning came from computer science.
So these two literatures kind of evolved,
and then when the literatures merge,
you end up with clashing notations.
So CS people use A to denote the set of actions and the,
the set of mechanical engineering and EE people use A to denote
this matrix and when we merge
these two literatures the notation ends up being overloaded, right?
Okay. Oh and then, uh,
it turns out one thing we'll see later is that
this noise term it will- we'll see later is actually not super important.
But for now, let's just assume that the noise term is distributed Gaussian
with some mean 0 and some covariance sigma subscript w, okay?
But we'll see later that the noise will,
will be less important than you think.
Right. And so this matrix A is going to be R n by n. And this matrix B is going to be R n
by d where n and d are respectively
the dimensions of the state space and the dimension of the action space.
So for driving a car, for example,
we saw last time that maybe the state space is six dimensional.
So if you're driving a car where the state-space is XY theta x dot y dot theta
dot and the action space is steering controls so maybe A is two-dimensional right,
acceleration and steering, right.
Okay. So let's see.
So to specify an MDP we need to specify this five tuple, right?
So we specify three of the elements.
The fourth one, t is just some number,
right, so that's easy.
And then the final assumption we need to apply LQR,
is that the reward function has the following form.
That the reward is negative of
s transpose U s plus a transpose V a,
where U is n by n,
V is d by
d and U V are a positive semi-definite.
Okay? So these are matrices being bigger than zero, so positive semi-definite.
Okay. So the fact that U and V are positive semi-definite
that implies that S T U s is greater than or equal to 0 and excuse me,
S transpose U s sorry,
a transpose V a is also greater than or equal to 0.
Okay? So here's one example.
If you want to fly an autonomous helicopter and if you want,
you know, the state, the state vector to be close to 0.
So the state vector captures position,
orientation, velocity, angular velocity.
If you want a helicopter to just hover in place,
then maybe you want the state to be regulated or to, to,
to be controlled near some zero position and so if
you choose U equals the identity matrix,
and V also equal to the identity matrix,
this, this would be different dimensions, right?
This would be an n by n identity matrix, this would be a d by d ide- identity matrix.
Then R of s a ends up equal to negative norm of s squared plus norm of a squared.
Okay. And so this allows you to,
this allows you to specify the reward function that penalizes, you know,
we have a quadratic cost function,
the state deviating from 0 or, if you want,
the actions deviating from 0,
thus penalizing very large jerky motions on the control sticks or we set V equal to 0,
then this second term goes away.
Okay? So these are some of the cost functions you can
specify in terms of a quadratic cost function.
Okay. Now again, you know,
just so that you can see the generalization, um,
if you want non-stationary dynamics,
this model is quite simple to change where you can
say the matrices A and B depend on the time t. You can also say these,
you know, the matrices U and V depend on the time t. So if you have
non-stationary state transition probabilities or
non-stationary cost function that's how you would modify this.
But I won't, I won't use this generalization for today, okay?
Now, so the two key assumptions
of the LQR framework are that first,
the state transition dynamics,
the way your states change,
is as a linear function of the previous state and action plus some noise,
and second, that the reward function is a,
you know, quadratic cost function, right?
So these are the two key assumptions.
And so first, you know, where,
where do you get the matrices A and B.
One thing that we talked about on Wednesday already was-
so again this will actually work if you are trying to
apply LQR to fly an autonomous helicopter.
This would work for helicopter flying at low speeds.
Which is if you fly the helicopter around, [NOISE] you know,
start with some state S_0, take an action A_0, um,
get to state S_1,
do this until you get to S t, right?
And then this was the first trial,
and then you do this m times.
So we talked about this on Wednesday.
So fly the helicopter through m trajectory so t time steps each and then we know that
we want, S t plus 1 is approximately A S t plus B A t and so you can minimize,
okay. So we want
the left and the right hand side to be close to each other.
So you can, you know,
minimize the squared difference between the left hand side and the right hand side in a,
in a procedure a lot like linear regression in order to fit matrices A and B.
So if you actually fly a helicopter
around and collect this type of data and fit this model to it,
this will work, you know,
this is actually a good pretty reasonable model
for the dynamics of a helicopter at low speeds.
Okay? So this is one way to do it.
Um, so let's see.
Method 1 is to learn it, right?
A second method is to linearize a non-linear model.
[NOISE] So um,
let me just describe the ideas at a,
at a high level,
um, which is let's say that- and,
and I think for this it might be,
um, useful to think of the inverted pendulum, right?
So that was a, you know, so imagine you have a,
a, a inverted pendulum.
That was that, right? You have a pole and you're trying
to- you have a long vertical pole and you're trying to keep the pole balanced.
Um, so for an inverted pendulum like this,
if you download an open source physics simulator or if you have a friend, you know, from,
from the wi- with a physics degree to help
you derive the Newtonian mechanics equations for this.
Um, ah, [NOISE] let's see.
I, I actually tried to work through the,
the physics equations in the inverted pendulum one.
These are pretty complicated.
But I don't know [LAUGHTER].
Um, but you might have
a [NOISE] function that tells
you that if the state is a certain position orientation with the pole velocity,
angular velocity and you as-,
um, ah, what is it?
Um, apply a certain acceleration,
the actions accelerate left or accelerate right,
then, you know, one-tenth of a second later,
the state will get to this, right?
So, so, your physics friend can help you derive this equation.
Um, and, an- and then maybe plus noise, right?
Le- let me just ignore the noise for now.
Um, and so what you have is a function [NOISE],
right? Then maps from the state,
um, x x. Theta Theta.
That's a position of the cart and the angle of the pole and
the velocities and angular velocities that maps from the current state at time t,
oh, excuse me, comma [NOISE] at, right?
Maps from the, I guess,
current state vector to the next state vector,
um, as a function of the current state and current action.
Okay? So, um, here's what linearization means and,
I'm going to use a 1D example.
So because I can only draw on a flat board, right?
I can't, you know, because, because of the two-dimensional nature of the whiteboard,
um, I'm just going to use a- let's,
let's suppose that you have St plus 1 equals f of
St. And let me just forget- let me just ignore the action for now.
So I have one input and one output so I can draw this more easily on the whiteboard.
Um, so we have some function like this.
So the x-axis is St,
and y-axis is St plus 1,
and this is the function f, right?
We'll plug in back the, um, action later.
What the linearization process does is, um,
you pick a point and I'm going to call this point St over bar,
and we're going to, you know,
take the derivative of f and fill a straight line.
So we're not drawing a straight line very well.
Let's take the tangent straight line at this point St-bar,
and uh, what, ah,
and, and we're going to use this straight line.
Let's draw line green.
And we're going to use a green straight line to approximate the function f. Okay.
And so if you look at the equation for the green straight line, um,
the green straight line is a function mapping from St to St plus 1.
And S-bar is the point around which you're linearizing the function.
So S-bar, um, is a constant.
And this function is actually defined by St plus 1, um,
is approximately [NOISE] the derivative of the function at S-bar times
St minus S-bar plus f of S-bar t. Okay.
Um, and so, ah, so S-bar t is a constant, right?
And this equation expresses St plus 1 as a linear function of St.
So think of S-bar t as a fixed number, right?
It doesn't vary. So given some fixed S-bar, um,
this equation here- this is actually the equation of the green straight line,
which is it says, you know, if,
if you use a green straight line to approximate the function f,
just tells you what is St plus 1 as a function of St, and this is a, you know,
linear and affine relationship between St plus 1 and St, okay?
Um, so that's how you would linearize a function.
Um, and, and in the more general case
where, um, [NOISE].
And in a more general case where, um,
St plus 1 is actually a function of, you know,
putting this back in where both St and at, um,
the formula becomes, um, let me see.
Um, well, I'll write out the formula in a second.
Ah, but in this example,
S-bar t is usually chosen to be a typical value
[NOISE] for S, right? And so in particular,
if you expect your helicopter to be doing a pretty good job hovering near the state 0,
then, uh, it'll be pretty reasonable to choose S-bar t to be the vector of all 0s .
Because if you look at how good is the green line as an approximation of the blue line,
right, in a small region like this, you know,
the green line is actually pretty close to the blue line.
And so if you choose S-bar to
be the place where you expect your helicopter to spend most of its time,
then the green line is not too bad in approximation to the true function to the physics.
Oh, excuse me, or if you expect for the inverted pendulum,
if you expect that your inverted pendulum will spend most of
its time with the pole upright and the velocity not too large,
then you choose S-bar to be maybe the 0 vector.
Um, and so long as your pendulum-
your inverted pendulum is spending most of its time kind of,
you know, close to the 0 state,
then the green line is not to get an approximation for the blue line, right?
So this is an approximation,
but you try to choose,
um ah, be- because- I mean in,
in this little region it's actually not that bad that an approximation,
ah, it's only when you go really far away, right,
there there's a huge gap between the linear approximation,
um, and the true function f, okay?
Um, all right.
And so, um,
in the more general case where f is a function of both the state and the action,
then what you have to do is, ah,
the input now becomes St,
At because f maps from St,
At to St plus 1.
And then instead of choosing S-bar t,
you're choosing S-bar t,
a-bar t, which is a typical state and action,
ah, around which you linearize the function.
Or let me just write down the formula for that.
[NOISE]
Um, in which you would say,
if you linearize f around a point given by S-bar t,
a-bar t kind of the typical values,
then the formula you have is St plus 1 is given by f of S-bar t,
a-bar t plus the gradient with respect to S, [NOISE]
transpose S_t minus S_t bar.
[NOISE]
Okay. So this is the generalization of the 1D function we measured just now,
or we wrote down just now,
which says that, you know,
the next state is approximately this point around you, which you linearize,
plus the gradient with respect to S times how much the state differs from
the linearization point plus the gradient respect to
the actions times how much the actions vary from a-bar, okay?
And this kind of generalizes
that equation you wrote. [NOISE].
So, um, so this equation expresses St plus 1 as a,
ah, linear function or technically an affine function of
the previous state and the previous action, right?
With some matrices in between.
And from this, you know,
after some algebraic munging,
you can re-express this as St plus 1 equals ASt plus Bat.
Um, and, and just- there- there's just one other little detail which is,
um, you might need to redefine St to add an intercept term.
Right. And because this is, is a affine function
with an intercept term rather than the linear function.
But so from this formula,
you know, with a little bit of algebraic munging,
you should be able to figure out whether the matrix is a and b, ah, ah,
but you might need to add an intercept term to the S,
but this is just an affine function to kind of rewrite
in terms of matrices a and b, okay?
Um, all right.
So right,
I hope that makes sense, right?
That this thing, this linearization thing
expresses St plus 1 as a linear function of St and at, right?
This is just a linear- is just- the wa- way St plus 1 varies,
you know, is just some matrix times St,
some matrix times at, um, and that's why with some munging,
you can get into this formula for some matrix a and b, okay?
Um, but because there are some constants floating around as well, like this,
you might need an extra intercept term to multiply to a to give you that extra constant.
[NOISE]
That's where we are. Um, we now have that for
these MDPs either by learning a linear model with the matrices A and B,
um, or by taking a nonlinear model and linearizing it.
Like you just saw, you can model- hopefully model an MDP as a,
um, [NOISE] linear dynamical system, meaning this, you know,
S_T plus 1 is this linear function or the previous state and action,
as well as hopefully with a quadratic reward function or the- really,
the- er, right, in the form that we saw just now.
Um, so let me just summarize the problem we want to solve.
a_ST, oops sorry, sorry.
S_t plus 1 equals A S_T plus B_at plus w_t,
so this is a noise term, um,
and then R of S, a
equals negative S transpose U_S plus a transpose V_a.
All right. And this is a finite horizon MDP.
And so the total payoff is R of S_0,
a_0 plus dot dot dot plus R S_T.
Okay. [NOISE] So let's
figure out a dynamic programming algorithm for this.
[NOISE] The remarkable problem,
the- the remarkable property of LQR, um,
and what makes this so useful is that if you are
willing to model your MDP using those sets of equations,
then the value function is a quadratic function, right?
Um, and so let me show you what I mean.
And so if your- if your model,
if your MDP can be modeled as this type of linear dynamical system,
with a quadratic cost function, uh,
then it turns out that V star is
a quadratic function and so you can compute V star exactly, right?
Um, so let me show you what I mean.
We're going to develop
a dynamic programming algorithm to compute the optimal value function V star.
Similar to, uh, what we did a bit earlier
today with the finite horizon MDP with a finite set of states,
let's start with the final time step and we will work backwards.
So, um, V star t of S_T is equal
to max over a_T of R of S_T , a_T.
Um, this is max over
a_T over negative, right?
Um, but this is always greater than or equal to 0 because V is positive semi-definite.
And so the optimal action is actually to just choose the action 0, um,
and so the max over this is equal to the negative S_T transpose U S_T because,
because V is a positive semi-definite matrix.
This thing is always greater than 0.
And then- and so this tells us also that Pi star of the final action is the argmax.
So the optimal action is to choose, you know,
the vector of 0 actions at the last time step, okay?
So this is the base case for the dynamic programming step of,
um, value iteration where, uh,
the optimal value at the last time step
is just choose the action that maximizes the immediate reward,
uh, which means maximize this, right?
And this is maximized by choosing the action 0 at the last time step, okay?
Now, these blue pens keep, let's see if this is any better, ooh, okay.
Now, the key step to the dynamic programming implementation is the following,
which is suppose that V star t
plus 1 S_t plus 1 is equal to a quadratic function.
Right.
Um, okay.
So in the- uh-huh.
[inaudible].
Yes. It's true that this term is also greater than 0 without the minus sign.
Without the minus sign, that term is positive and so,
but you only get to maximize with respect to 80 right?
So, so the best you could do for this term is set it to 0.
Thank you. All right, cool, tank you. All right.
Now, for the inductive case, um,
we want to go from V_t plus 1- V_star t plus 1 to
computing V star t, right? And the key observation that makes LQR work is,
um, let's suppose V star t plus 1,
the optimal value function at the next time step,
let's suppose is a quadratic function.
So in particular, let's suppose V star t plus 1 is this,
you know, quadratic function, uh,
parameterized by some matrix capital Phi t plus 1 which is an n
by n matrix and some constant offset Psi which is a real number.
Um, what we will be able to show is that if you do one step of dynamic programming, uh,
if this is true for V star plus 1 that V_t after one step as you go from V star plus
1 to V_t that the optimal value function
V_t is also going to be a quadratic function with a very similar form,
right, with I guess t plus 1 replaced by t, right?
Um, and so in the dynamic programming step, um,
we are going to update V_t S_t
equals max of A_t R of S_t, a_T plus.
And then, you know,
I- I think you remember, right, previously,
um, I'm going to write this in green,
previously we had sum of S prime or actually St plus
1 I guess to be S_t a_t S_t plus 1,
V star t plus 1 St plus 1.
So that's what we had previously where we
had a discrete state space and we were summing over it.
But now that we have a continuous state space,
this formula becomes expected value with respect to S_t plus 1 drawn from
the state transition probabilities [NOISE] ,
uh, V star t plus 1 S_t plus 1 [NOISE].
Uh, yeah. Okay.
[NOISE] So the optimal value when the clock is at time t is choose the
action a that maximizes immediate reward plus the expected value of, you know,
your future rewards when the clock has now ticked from time t to time t plus 1,
you're going to state S_t plus 1 at time t plus 1, right?
So, um, let's see.
So, ah, this is a pretty beefy piece of algebra to do.
Um, I think I feel like showing this full result is, I don't know,
is like at the level of complexity of a, you know,
typical CS 229 homework problem which is quite hard [LAUGHTER].
But let me just show the outline of how you do this derivation and why,
you know, why this inductive step works.
Well, but I think you- but,
but if you want you could work through the algebra details yourself at home.
Um, which is that- let me do this on the next board.
So V star_t of S_t is equal to max over a_t
of the immediate reward, right?
So that's the immediate reward.
And then plus the expected value with respect to S_t plus 1,
is drawn from a Gaussian with
mean AS_t plus Ba_t and covariance Sigma w. Ah,
so remember S_t plus 1 is equal to AS_t plus Ba_t plus W_t,
where W_t is Gaussian with mean 0 and covariance Sigma w. Right?
So ah, if you choose an action a_t,
then this is the distribution of the next state at time t plus 1.
Um, and then expected value of
[NOISE]
this quadratic term.
Um, because this quadratic term here,
kind of the inductive case was what we showed was
V star for the-
for the next time step, right?
So it turns out that, um, let's see.
So this is a quadratic function,
and this expectation is the expected value of
a quadratic function with respect to s drawn from a Gaussian, right?
With a certain mean and certain variance.
So it turns out that, um,
the expected value of this thing, right?
Well, this whole thing that I just circled.
This thing simplifies into, er,
a big quadratic function
[NOISE]
of the action a_t, right?
Um, and then, ah,
and so in order to, you know,
derive the argmax or to derive V star of S,
you would derive this big quadratic function.
Um, take derivatives with respect to a_t,
ah, set to 0, right?
And solve for a_t.
Okay? And if you go through all that algebra,
then you actually- then you end up with the formula for a_t as follows.
Um,
okay? And um,
I'm gonna use, I'm gonna do- I'm gonna take that big matrix and denote that L_t.
Okay? Um, and so this shows also that pi star
at time t of S_t is equal to L_t times S_t.
Okay? So, um,
[NOISE] one to- to take away from this is that,
under the assumptions we have, right?
Linear dynamical system with quadratic cost function.
Ah, the optimal action is
a linear function
of the state S_t.
Right? And, ah, this is not a claim that is made through functional approximation.
Ah, what I'm- I'm not saying that you could fit
a straight line t optimal action and if you fit a straight line,
that you get this linear function.
Right? That's not what we're saying.
We're saying that, um, of all the functions,
anyone could possibly come up within the world,
linear or non-linear, the best function,
the best action is linear.
So there is no approximation here.
Right? So it's just that, you know,
it's just a fact that if you have linear dynamical system,
the best possible action at any state is going to be a linear function um,
ah, of of that state.
Right? So there's no there's- we haven't approximated anything.
Right?
Um,
[NOISE]
let me see.
Yeah, all right. Let me,
let me, let me write this here.
Um, and then the other step is that ah,
if you take the optimal action and plug it into the definition of V star,
then by simplifying which again is quite a lot of algebra, but without the simplifying,
you end up with this equation.
Um, where again I'll- I'll just write out the formula as is, you know. [NOISE]
Okay.
Okay.
All right. [BACKGROUND] Um,
so to summarize the whole algorithm,
right, let's, let's put everything together.
And, and so- sorry.
And so what these two equations do is they allow you to go from V star T
plus 1 which is defined in terms of Phi T plus 1 and Psi T plus 1.
And it allows you to recursively go back to
figure out what is V star T using these two equations.
Right. So Phi T depends on Phi T plus 1,
Psi T depends on Phi T plus 1 and Psi T plus 1.
Uh, and this Sigma w,
this is the covariance of w_t.
Right. This, this Sigma subscript w. This is not a summation over w,
this is a Sigma matrix subscripted by w. That was a covariance matrix for
the noise terms you are adding on every step in our linear dynamical system.
Okay. And, and this are trace operators, some of the diagonals.
Okay? So just to summarize.
[NOISE] Um, here's the algorithm.
You initialize Phi T to be
equal to negative u and Psi T equals 0.
Um, and so, you know,
that's just taking this equation and mapping it there.
Right? So the final time step, ah,
that those two, oh, sorry,
it should be capital T. Right.
So that, um, those two equations for Phi and Psi,
it defines V star of capital T. And then you would,
um, you know, recursively calculate, um,
Phi T and Psi T using Phi T plus 1 and Psi T plus 1.
So you go from,
you know, for T equals T minus 1,
T minus 2 and so on and go back when count down from,
right, T minus 1 to T minus 2 and so on down to 0.
Um, calculate L_t as above.
Right. and L_t was a formula I guess we had over there, um,
saying how the optimal action is
a function of the current state depending on A, and B, and Phi.
Ah, and then finally,
Pi star of S_t equals L_t of S_t.
Okay? Um, and this algorithm,
the remarkable thing, what one really cool thing about
LQR is that there is no approximation anywhere.
Right? You, you might need to, um,
make some approximation steps in order to approximate
a helicopter as a linear dynamical system by, you know,
fitting matrices A and B to data or by taking a nonlinear thing and linearizing it,
and you might need to just restrict- constrict,
you know, restrict your choice of possible reward functions.
Reward function is quadratic.
But once you've made those assumptions,
none of this is approximate,
everything is exact. Right. Question?
[inaudible]
Yes, that's right. Yep, yeah.
So the approximation step needed are, ah, ah,
getting your MDP into the form of a linear dynamical system with quadratic reward.
So that is approximate.
But once you specify the MTP like that,
all of these calculations were exact, right?
So, so we're not approximating the value function or quadratic function,
is that the value function is a quadratic function and you're computing it exactly.
And the optimal policy is a linear function and you
just computing, computing that exactly.
Okay. Um, I want to mention- before we wrap up,
I want to mention one,
one unusual fun fact about LQR and this is very specific to LQR.
Uh, and, and, and it's convenient,
uh, but, but, er,
let me say what the fact is and just be careful that this doesn't give you
the wrong intuition because it doesn't apply to anything other than LQR,
which is that if you look at where, um,
so first, if you look at the formula for L, ah, let me see.
Move this around. [NOISE] All right.
If you look at the formula for L_t,
you need to compute, I mean the,
you know, the goal of doing all this work is to find the optimal policy.
Right? So you want to find L_t so that you can compute the optimal policy.
You notice that L_t, um,
just depends on Phi but not Psi.
Right? Um, so, you know,
and, and maybe it's gonna make sense.
You're going to- when you take an action,
you get to some new state and
your future payoffs is a quadratic function plus a constant.
It doesn't matter what that constant is.
Right? And so in order to compute the optimal action, in order to compute L_t,
you need to, you need to know Phi or actually Phi T
plus 1 but you don't need to know what is Psi T plus 1.
Right. Now, if you
look at the way we do the dynamic programming,
the backwards recursion, um,
what if you implement a piece of code that doesn't involve it to compute Psi, right?
So these are the two equations you use,
update Phi and Psi.
But whether, you know,
let's say you delete this line of code.
Just don't bother to compute it and just don't
bother to compute that and don't bother to compute that.
Right? So you notice that Phi depends on Phi T plus 1,
but it doesn't depend on Psi.
Uh, and so you can implement the whole thing and compute the optimal policy and
compute the optimal actions without ever computing Psi.
Right. Now the funny thing about this is that the only place that Sigma w
appears is that it
affects only Psi T. Right?
So, you know, if,
if we do what I've just cross out in orange and just don't bother to compute
Psi T. Then the whole algorithm doesn't even use Sigma w. Right.
So one very interesting property of the LQR, um, ah,
of this formalism is that the optimal policy does not depend on Sigma w. Right.
Um, and I think,
ah, maybe this is a, ah,
so V star depends on Sigma w,
because if the noise is very large,
if there's a huge gust of wind blowing a helicopter all over the place,
then the value would be worse.
But Pi star and L_t, uh,
do not depend on the Sigma w. Okay.
Um, so this is a property that is very specific to LQR,
don't, don't, don't overgeneralize it to other reinforcement learning algorithms.
But this, um, I think the intuition to, ah,
um, take from this is first,
if you are actually applying this system,
you know, don't bother to, don't,
don't- I say don't,
don't try to hard to estimate Sigma w, because you,
you don't actually need to use it, uh,
which is why when we're fitting a linear model,
I didn't talk too much about how you actually estimate Sigma w. Because in LQR system,
it literally doesn't matter in
a mathematical sense in terms of what does the optimal policy you compute.
And the second, the maybe slightly more useful intuition to take away from this,
is that, ah, for a lot of MDPs,
if you're building a robot,
you know, ah, um, remember to add some noise to your system
but the exact noise you add doesn't matter as much as one might think.
So what I've seen in,
in working on a lot of robots,
a lot of MDPs is, you know,
do add some noise to the system and make sure your learning algorithm is robust to noise.
And the form of the noise you add, it does matter.
I don't say it doesn't matter at all.
I mean, in, in LQR, it doesn't matter at all.
For other MDPs, it does matter.
But I think the fact that you've remembered to add some noise is often
in practice more important than the exact details of,
you know, is the noise 10% higher or is the noise 10% lower.
If, if the noise is 100% higher or lower,
that will often make a big difference, but, ah,
but, but when I'm, you know,
training a model of our helicopter or something,
the noise is something that, you know,
I pay a little bit of attention to but I pay much more attention to
making sure that the matrices A and B are accurate than,
and then, you know, a little bit sloppiness in the
act of using your noise model is something that an MDP can probably survive,
that your policy can survive.
Okay. Let's take one last question. Yes.
[inaudible].
Oh V? Uh, ah, oh I see.
Sorry, yes. Let me see my notes.
Oh V. That was,
ah, this is a V. Yes, thanks, yeah.
Okay, cool. Thanks everyone.
Let's break and I'll see you for the final lecture on [NOISE] Wednesday.
Thanks everyone.
 All right. Hey, everyone,
actually started a little bit late.
So welcome to the, uh,
final lecture of, uh,
CS229 of this quarter or I guess,
uh, to the home viewers,
welcome to the season finale.
So what I'd like to do today is, um,
wrap up our discussion on reinforcement learning and then,
um, and then we'll conclude the class.
Um, so I think you know,
over the last, uh,
few lectures you saw a lot of,
uh, uh, we- we saw a lot of NAV.
So maybe as a brief interlude here are some videos.
Um, so self-autonomous helicopter,
um, you know, this is a project that, uh, I know Pieter Abbeel,
Adam Coates, uh, some- some former students here,
now some of the machine learning greats worked on when they were,
um, PhD students here.
Uh, and- and- and I think, uh,
using algorithms similar to the ones you learned in this class,
how do you make a helicopter fly?
So just for fun, this is a video shot on top of
one of the Stanford, uh, soccer fields.
I was actually the camera man that day [LAUGHTER] ,
um, and zooming out the camera.
See the trees touching the sky.
[BACKGROUND].
Say, uh, um,
it- it- it turns out- that's a small radio-controlled helicopter.
It turns out that, uh,
when you're very far away you can't tell if this is
a small radio-controlled helicopter or if there's
like a helicopter with people sitting in it [LAUGHTER].
So, um, uh, there was- actually there's,
uh, you know, foot is on, uh,
a kind of a soccer field, the big,
uh, grass field off San Hill Road and turns out across San Hill Road,
um, one of the high rises there was a- there
was an elderly lady that lives in one of those apartments.
And when she saw that, she would call 9-1-1 and say,
"Hey, there's a helicopter about to crash."
[LAUGHTER] And then the- the firemen would come out, so,
[LAUGHTER] I had to tell them that and I- I think they were partly relieved,
partly disappointed that there was no one for us for- for them to save.
And, uh, um, and so- and- and I think, uh, let's see.
Uh, uh, one of the things I promised to do, um,
in the debugging learning algorithms lecture was just go over the um,
reinforcement learning example again.
So let me just do that now but, uh,
with notation that I think you now understand compared to- oh, yes.
Why is the helicopter flying upside down?
Oh, uh, it was an aerobatic stunt.
Uh, yeah, I- I don't think there's
any good reason for flying a helicopter upside down [LAUGHTER] ,
uh, other than that you can.
Uh, there- there a lot of videos of self-autonomous helicopters flying
all sorts of stunts, go to heli.stanford.edu,
heli.stanford.edu and the Stanford autonomous helicopter did- did-
did a lot more than flying upside down.
Uh, it could, I mean,
make some maneuvers that looked aerodynamically
impossible such as the helicopter that looks like it's tumbling,
just spinning randomly but staying same place in the air, right?
Um, it's called a chaos maneuver and if you look you go, wow.
This helicopter was turning upside down,
spinning around the air in every single direction but it was
just staying right there in the air not crashing,
and so there are maneuvers like that- that, um,
the very best human pilots in the world can fly with helicopters and I think,
uh, this was just, uh, um, uh,
a demonstration I guess, uh,
and I think a lot of this work wound up influencing some of
the later work on the quadcopter drones in a few research labs and.
Yeah, I think, uh,
it was a difficult control problem and it was, uh,
it was one of those things you do when you're,
you- you when you're a university and you want to solve the hardest problems around.
But I wanted to step through a few of the debugging process
that we went through as we were building a helicopter like this.
So, uh, when you're trying to get the helicopter to fly upside down,
fly stunts, you don't want to crash too often.
So step one is build a model or build a simulator of a helicopter, right?
Much- much as you saw, um,
when we start to talk about fitted value iteration and then,
um, choose a reward function, uh,
like that, and it turns out that
specific reward function for staying in place is not that high,
you know, like the quadratic function like that works okay.
But if you want the helicopter to fly aggressive maneuvers it's actually quite
tricky to specify what is a good turn for a helicopter, right?
Um, and then what you do is you run reinforcement learning algorithm, um,
to try to maximize say
the finite horizon MDP formulation and maximize sum of rewards over T timesteps,
so you get a policy Pi.
And then whenever you do this,
the first time you do this,
you find that the resulting controller does much worse than the human pilot,
and the question is what do you do next, right?
This is- by the way- this is almost- I think this is
almost exactly the slide I showed you last time except I cleaned up the slide
using reinforcement learning notation rather than
the slightly simplified notation you saw
before [NOISE] you learned about reinforcement learning.
And so the question is, um,
and- and again if you're working on
the reinforcement learning problem yourself, you know, uh,
there's a good chance you have to answer this question yourself for
whatever robot or other reinforcement learning or
factory automation or stock trading system or whatever it is,
um, you are trying to get to work in reinforcement learning.
But do you want to improve the model sim- model
or do you want to modify the reward function or do you want to,
uh, modify the reinforcement learning algorithm. All right.
And modifying the reinforcement learning algorithm includes things like,
uh, playing with the discretization that you're using.
Um, if you're taking a continuous state MDP and discretizing it to solve over
finite state MDP formulation or modifying the reinforcement learning algorithm
includes also maybe choosing new features to use in fitted value iteration, right?
There are a lot of things you could try.
Or maybe instead of using a linear function approximator,
instead of fitting a linear function for fitted value iteration.
Maybe you want to use a bigger,
you know, deep neural network, right?
Um, but so which of these steps is the most useful thing to do?
So this is the analysis of those three things, uh, you know,
if, I'll give you a second to read this, right?
But if these three statements are true,
then the learn controller should have flown well on the helicopter.
Right? Um, and so
those three sentences correspond to the three things in yellow that you could work on,
um, there's a problem that,
you know, um, statement 1 is false,
that the simulator isn't good enough,
there's a problem that statement 2 is false.
That, um, ah, oh,
sorry I think actually two or three are reversed.
But, uh, the three statements corresponds to the three things in yellow.
I think the two and three are in, uh,
are in, uh, opposite order, right?
Ah, as the RL algorithm maximizing some rewards is a reward function,
actually the right thing to maximize.
And so here are the diagnostics you could use,
um, to see if this helicopter simulator is accurate,
uh, well, first check if,
um, the policy flies well in simulation.
If your policy flies well in simulation but not in real life,
then this shows that the problem is with
your simulator and you should try to learn a better model for your helicopter, right?
And, and if you're using a linear model this with the matrices a and b, um,
if, you know, st plus 1 equals ast plus bat,
if you're [inaudible] try,
try getting more data or maybe try a non-linear model,
but if you find that the problem's not your simulator,
if you find that, uh,
your policy is flying poorly in simulation and flying poorly in real life,
right, then this is the diagnostic I would use.
Um, so I shall show these two lines.
So let human be the human control policy,
so hire a human pilot, right? Which, which we did.
We're fortunate to have one of the best- one, one of,
um, America's top, you know,
aerobatic helicopter pilots working with us, and he,
using his control sticks and radio control,
can make a helicopter fly upside-down,
tumble, do flips, loops, rolls.
So we had a very good human pilot, um,
help us, uh, fly the helicopter manually.
So what you can do is, um,
test whether or not the,
uh- so this, this thing here, right?
That's just a pay off of the,
um, learn policy as measured on your reward function.
So check if, um,
the learn policy achieves a better or worse pay off than a human pilot can, right?
And so that means, you know,
go ahead and let the learn policy fly the helicopter and we get
the human to fly the helicopter and compute the sum of rewards
on the sequence of states that these two systems take the helicopter through and
just see whether the human or the learn policy achieves a higher payoff,
achieves a higher sum of rewards.
And if, um, the payoff achieved
by the learning algorithm is less than the payoff achieved by the human,
then this shows that, um,
the learn policy's not actually maximizing the sum of rewards, right?
Because whether the human is doing, you know,
he or she is doing a better job,
maximizes the sum of rewards then the learn policy.
So this means that you should, you know,
consider working on the reinforcement learning algorithm to try to
make it do a better job maximizing the sum of rewards, right?
Um, and then on the flip side,
this inequality goes the other way, right?
Uh, so if pa- if, if the payoff or
the RL algorithm is greater than the payoff of the human,
then what that means is that, you know,
RL algorithm is actually doing a better job,
maximizing the sum of rewards,
but it's still flying worse.
So what this tells you is that,
doing a really good job maximizing the
sum of rewards does not correspond to how you actually want
the helicopter to fly and so that means that maybe you should work on,
um, improving the reward function,
that the reward function is not capturing what's actually most
important to fly a helicopter well and then,
then you modify the reward function, right?
So in a typical workflow,
uh, hoping to describe to you what,
what it feels like to work on a machine learning project like this,
and this was a big multi-year machine learning project,
but when you're working on a big complicated machine learning project like this,
um, the bottleneck moves around meaning that you build a helicopter,
you get a human pilot to fly it,
you know, gets in the work,
they run these diagnostics and maybe the first time you do this you'll find, wow,
the simulation's really inaccurate,
then you are going to work on improving the simulator for a couple months.
And then, you know, and every now and then you come back and rerun
this diagnostic and maybe for the first two months of the project,
you keep on saying, "Yup,
simulator is not good enough, simulator
is not good enough, simulator is not good enough."
After working on the simulator for a couple months you,
you may find that, um,
item 1 is no longer the problem,
you might then find that,
um, item 3 is the problem,
the simulator's now good enough,
but when you run this diagnostic,
two months into the project, you might say,
"Wow, looks like your RL algorithm, uh,
is maximizing the reward function but this is not good flying."
So now I think the biggest problem for the project or the biggest bottleneck
for the project is that the ref- the reward function is not good enough,
and then you might spend, you know,
another one or two,
or three, or, or,
or sometimes longer months working to try to improve the reward function,
then you might do that for a while,
and then when the reward function is good enough then that exposes
the next problem in your system which might be that the RL algorithm isn't good enough.
And so the problem you should be working on
actually moves around and it's different in different phases of the project.
And, um, when you're working on this it feels like every time you
solve the current problem that exposes the next most important problem to work
on and then you work on that and you solve that then this helps you identify and
expose the next most important problem to work on
and you kind of keep doing that or you keep iterating,
and keep solving problems until hopefully,
you get a helicopter that does what you want it to, make sense?
Okay. Um, but I think [NOISE] teams that have the discipline to, um,
prioritize according to diagnostics like this,
uh, tend to be much more efficient,
the teams that kind of go by gut feeling in terms of selecting,
you know, what to, what to spend the time on.
All right, um, any,
any questions about this?
[inaudible].
Oh, sorry, say that again.
[inaudible] the simulator's
accurate [inaudible].
Yeah, uh, I, I kind of wanna say yes,
um, let me think.
Yeah, I would usually check step 1 first and then
if I think simulator is okay then look at steps 2 and 3.
Um, maybe one, one other thing, uh, er, about,
when you work on these projects there is some judgment involved so I
think I'm presenting these things as though- as a rigid mathematical formula,
that's cut and dry, this formula says,
now work on step 1, then this one says,
now work on step 3.
Um, there is, there is, um,
more judgment involved because when you run these diagnostics you might say, well,
it looks like the simulators not that good but it's kinda good,
it's little bit ambiguous, and oh it looks like,
you know, uh, and so that's what it often feels like.
And so a team would get together,
look at the evidence from all three steps and then say, you know, "Well,
maybe the simulator is not that good but it's maybe good
enough and but both the reinforcement- the,
the reward function is really bad, let's focus on that."
So there is some,
um- so rather than a hard and fast rule there,
there is some judgment needed to,
to make these decisions,
uh, but having a,
um- so when leading machine learning teams often my teams will, you know,
run these diagnostics, get together and look at
the evidence and then discuss and debate what's the best way to move forward,
but I think the process in making sure that discussion and
the debate is much better than the alternative,
which is, you know,
someone just picks something kind of at random and,
and the team does that, right?
Yeah, okay. Cool. Um-
All right, cool.
So, um, just, uh,
yeah maybe you, while I have the laptop up, you know,
a little bit for fun but a little bit because I'm,
uh, to illustrate fitted value iteration.
Um, let me just show another,
um, reinforcement learning video.
Um, oh, by the way,
one of the- I- I think if I look at the future of AI,
the future of machine learning, you know,
there's a lot of hype about reinforcement learning for game playing which is fine.
You know, we all like- we all love, uh,
computers playing computer games,
like that's a great thing I think or something, er.
But- but I think that some of the most exciting applications of
reinforcement learning coming down the pipe I think will be robotics.
So I think over the next few years,
even though there are
only a few success stories of reinforcement learning applied to robotics.
There are more and more right now.
One of the trends I see, you know, when you look at, uh,
the academic publications and some of the things making
their way into industrial environments is I think in the next several years,
just based on the stuff I see,
my friends in many different companies,
in many different institutes working on,
I think there will be a rise of, uh,
reinforcement learning algorithms applied to robotics.
I think this would be one important area to- to- to watch out for. All right.
Uh, but, uh, uh,
so, you know, uh, uh,
this is another Stanford video,
this is again just using reinforcement learning to get a robot dog,
um, to climb over obstacles like these.
Uh, my friends that were less generous, um [NOISE] ,
uh, uh, did not want to think of this as a robot dog.
Uh, they thought it was more like a robot cockroach, uh, [LAUGHTER].
But I think cockroaches don't have four legs, right,
cockroaches have six legs [LAUGHTER].
Um, yeah but so,
uh, how do you program a robot dog like this,
right, to, uh, climb over terrain?
So one of the key components, this is work by,
um, Zico Kolter, uh,
now a Carnegie Mellon professor, uh,
another one of the machine learning greats, uh, is,
ah, ah, a key part of this was,
ah, value function of approximation, uh,
where it- dog starts on the left and it goes get to the right then, uh,
the approximate value function kind of,
um, ah, I- I'm- I'm sort of finding a little bit, right?
But- but the approximate value function tells it,
uh, given the 3D shape of the terrain, uh,
the middle plot is a height map where
the different shades tell you how- how- how tall is the terrain,
uh, but given the shape of the terrain, the dog, uh,
learns a value function that tells it what is the cost of putting
his feet on different locations to the terrain and it learns among other things,
you know, not to put his feet at the edge of a cliff because then it's
likely to slip off the edge of a cliff and fall over, right?
And so, um, but- but hopefully this gives a visualization of whether,
uh, learn value function for a very complicated function they'll say.
And- and the state is very high-dimensional,
this is all kind of projected onto a 2D space so you can visualize it.
But- but this is what, uh,
a simplified value function looks like for a robot like this.
Okay. All right.
So with that,
um, let me return to the white board [BACKGROUND] um.
So, um, there's just one class of algorithms I want to describe to you
today which are called policy search algorithms.
And uh, sometimes, uh, policy searches are also called,
uh, direct policy search.
And, um, to explain what this means,
so far our approach to reinforcement learning has
been to first learn or approximate the value function,
you know, approximate V star and then use that
to learn or at least hopefully approximate Pi star, right?
So we have- you saw value iteration,
top, we had policy iteration.
But philosophy to reinforcement learning was
to estimate the value function and then use that,
you know, that equation with the arg max to figure out what is Pi star.
So this is an indirect way of getting a policy
because we- we first try to figure out what's the value function.
In direct policy search, um,
we try to find a good policy directly,
hence the term direct policy search because you
don't- you go straight for trying to find a good policy
without the intermediate step of finding an approximation to the value function.
So, um, let's see.
I'm going to use, uh,
as the motivating example the inverted pendulum.
Right. So that is that thing with a free hinge here,
and let's say your actions are to accelerate left or to accelerate right, right?
And then you can have- and you can have states to accelerate strong,
accelerate less strong, accelerate right.
You got more than two actions but let's just say you've
inverted pendulum with, um, two actions.
So, um, if you
want to- I- I'll- I'll talk about pros and cons of direct policy search later.
But if you want to apply polic- direct policy search,
you want to apply policy search,
the first step is to,
um, come up with the class of policies you'll entertain or
come up with the set of functions you use to approximate the policy.
So, um, again to make an analogy,
when, uh, you saw logistic regression for the first time, you know,
we kind of said that we would approximate y as the hypothesis,
um, right, whose form was governed by this sigmoid function.
And you remember in week 2 when,
uh, I first described logistic regression,
I kind of pulled this out of a hat,
right, and said, "Oh yeah, trust me,
let's use the logistic function," and- and then later,
we saw this was a special case of the generalized linear model.
Um, but, you know,
we just had to write down some form for how we will predict y as a function of x.
So in direct policy search,
we will have to come up with a form for Pi, right?
So we have to just come up with a function for algorithms in h. Um,
in direct policy search,
we'll have to come up with a way for how we approximate the policy Pi.
Right? And so, you know,
one thing we have to do is say, well,
maybe the action were approximate with some policy Pi, um,
maybe parameterized by Theta and is now a function of the state,
and maybe it'll be 1 over 1 plus e to the negative Theta transpose,
you know, to state vector.
Right? Where the same vector maybe something like,
um, x, x dot, uh,
and- and the angle- and the angle dot right
if- if this angle is Phi and maybe add an intercept there.
Okay. And- and I- I switch this from Theta to Phi to avoid,
uh, conflict in the notation.
Okay. Um, this isn't really the formative policy we'll write.
So let me- let me make one more definition and then I'll, um,
show you a form of a specific form of policy you can use,
but it's actually not quite this.
We'll- we'll need to tweak this a little bit.
So, uh, the direct policy search algorithm we'll use,
will use a stochastic policy.
So this is a new definition.
Um, so stochastic policy is a function.
Right.
Um, so we're going to use,
um, for the direct policy search algorithm that you see today,
we are going to use stochastic policies meaning that,
um, on every time step, uh,
the policy will tell you what's the chance you want to
accelerate left versus what's the chance you want to accelerate right,
and then you use a random gen- number generator to select either left or
right to accelerate on the inverted pendulum depending on the policies- no,
depending on the probabilities output by this policy.
Okay. Um, and so here's one example.
Um, let's see which is
you can have [BACKGROUND].
So, you know, continuing with the inverted pendulum,
here's one policy that, um,
[BACKGROUND] might be reasonable,
uh, where you say that, um, let's see.
Right. So, you know,
in a state s, the chance that you take
the accelerate right action is given by this sigmoid function.
And the chance that in a state s,
you take the accelerate left action is given by that.
Okay. Um, and here's one example for why this might be a reasonable policy.
So let's say the state vector s is 1, x,
x dot phi, phi dot, um,
where, you know, this angle of the inverted pendulum,
um, is the angle phi.
And let's say for the sake of argument that
we set the parameter of this policy phi to be,
um, 0, 0, 0, 1, 0.
So in this case,
this is saying that, um,
let's see, so theta transpose s is just equal to phi, right?
And so in this case, uh, right, because,
you know, theta transpose s is just 1 times phi,
everything else gets multiplied by 0.
And so in this case is saying that the chance you accelerate to
the right is equal to 1 over 1 plus e to the negative,
how far is the pole tilted over to the right.
Um, and so this policy gives you
the effect that the further the pole is tilted to the right,
the more aggressively you want to accelerate to the right, okay?
So this is a very simple policy,
it's not a great policy,
but it's not a totally unreasonable policy, which is well,
look at how far the pole is tilted to the left or the right, apply sigmoid function,
and then accelerate to the left or right, you know,
depending on how far it's tilted to the right.
Um, now, uh, and,
and, and because this is the, right,
so this is really the chance of taking the accelerate right action as a function of the,
um, pole angle Pi, right?
Now, this is not the best policy because it ignores all the features other than phi.
Um, but if you were to set theta equals,
you know, 0, negative 0.5,
0, 1, 0, then this policy,
um, the negative 0.5 now multiplies into the x position.
Right. Uh, now this new policy if you have this value of theta,
it takes into account how far is your cart is already to the right,
um, where I guess this is the x distance, right?
And the further your cart is already, I guess if,
if your cart is on a set of rails,
right, is on a set of railway track.
And you don't want to fall off the rail- and you want to keep the cart kind of centered,
you don't want it to fall off the end of your table.
But this now says the further this is to the right already well,
the less likely you should be to accelerate to the right.
Okay? And so maybe this is
a slightly better policy than with this set of parameters.
And more generally, what you would like is to
come up with five numbers that tells you how to trade off,
how much you should accelerate to the right based on the position, velocity, angle,
and angular velocity, um,
of the current state of the cart- of the,
of the inverted pendulum.
And what a direct policy search algorithm will do is, um,
help you come up with a set of numbers that results
in hopefully a reasonable policy for controlling the inverted pendulum.
Hope- and in a policy that hopefully result in a appropriate set of
probabilities that cause it to
accelerate to the right whenever it's good to do so and accelerate to the left,
you know, more often when it's good to do so.
Okay. So, um, all right.
So our goal is to find the parame- find parameters
theta so that when
we execute pi of s,
a, um, we maximize, well,
max over theta the expected value of R of s_0 is 0 plus dot,
dot, dot, plus, okay?
Um, and so the reward function could be negative
1 whenever the inverted pendulum falls over,
uh, and 9 whenever it stays up that of, of, of, whatever,
or something that measures how well your inverted pendulum is doing.
But the goal of a direct policy search algorithm is to
choose a set of parameters theta so that we execute the policy,
you maximize your expected payoff.
And I'm gonna use the finite horizon setting,
um, for the algorithm that we'll talk about today.
Okay? Uh, and then one,
one other difference between policy search compared to, um,
estimating the value function is that in direct policy search here s_0 is,
um, a fixed initial state, okay?
Um, it turns out that when we were estimating the value function v-star,
um, you found the best possible policy for starting from any state.
Right. And there's kind of no matter what state you start from is
simultaneously the best possible policy for all states.
In direct policy search,
we assume that either there's a fixed start state- fixed
initial state s0 or there's a fixed distribution over initial state.
So I'm gonna try to maximize the expected reward with respect to your initial state or
respect to your initial probability distribution over what is the initial state.
Okay. So that's, that's one other, um, difference.
So, um, let me think how I'm going to do this. All right.
So let's write this out.
The goal is to maximize overall theta,
the expected value of R of s_0,
a_0, plus R of s_1,
a_1, plus dot, dot,
dot up to R of sc, aT
um, you know, given pi theta.
And, um, in order to simplify the math we'll write on this board today,
um, I'm just going to set T equals 1 to simplify the math,
uh, in order to not carry such a long summation.
But it turns out that, um,
uh, so I'm just gonna do like a 2 times set MDP, uh,
just to simplify the derivation,
but everything works, you know,
just with a longer sum if you,
uh, have a more general version of T. Okay.
Um, and so this term here,
the expectation is equal to sum over all possible state action sequences, right?
And again, this will go up to sT and aT.
But as we said T equals 1 of,
um, what's the chance your MDP starts out in some state s_0?
So this is your initial state distribution times the chance that in
that state you take the first action a_0- oh, actually sorry.
Let me just- let me write this out.
Right. So the chance of your MDP going through this state action sequence, right,
times, times that, right.
So that's what it means to sort of compute the expected value of, uh, the payoff.
Um, and so instead of writing out this sum,
I'm just gonna call this the payoff, right?
And so this is equal to sum of s_0,
a_0, s_1, s_1, a_1 of the chance your MDP starts in state 0,
times the chance that in state 0,
you end up choosing the action a_0 times, um, uh,
the chance governed by
the state transition probabilities that you end up in state 1, uh, state s_1,
times the chance at state s_1 you end up choosing, let's see,
s_1 and then times the payoff, okay.
And so what we're going to really do is, um,
derive a gradient ascent algorithm- actually a stochastic gradient ascent algorithm as
a function of theta to maximize this thing- to maximize the expected value of this thing.
And that- and this is a, um,
this is how we'll do direct policy search.
Okay. So let me just write out the algorithm,
and then we'll go through why, um,
the algorithm that I write down is maximizing this expected payoff.
[NOISE].
So this algorithm is called the, um, reinforce algorithm.
Ah, the objective of the reinforce algorithm, um, uh,
had a few other bells and whistles, but,
but I'm gonna to explain the core of the idea.
But the reinforcing- the reinforce algorithm, um,
does the following which is you're going to run your MDP, right?
And just you know run it for a trajectory of T timesteps.
So, um, again, you know,
I'm just gonna, [NOISE] well.
Right. And and actually you would, uh, right.
Technically, you would, um,
run it for T timesteps but, you know,
let, let's just say for now,
we'll - we'll do only the thing in blue.
We run it for one timestep,
because we set capital T equal to 1.
Um, and then you would compute the payoff, right,
equals R of s0 + R of s1 and again,
in the more general case, you know,
plus dot dot dot plus R of st right?
[NOISE] And then you perform the following update which
is Theta gets updated as Theta plus the learning rate alpha, times.
Right? Um, and then times the payoff.
Right? And again, I'm just setting capital T equals 1.
If capital T was bigger,
you would just sum this all the way up to time T. Okay?
So that's the algorithm.
Um, that's on every iteration through the reinforce algorithm,
through the reinforce algorithm,
you will take your robot,
take your inverted pendulum, um,
run it through T timesteps,
uh, executing your current policy,
so choose actions randomly according to
the current stochastic policy using current values of the parameters Theta,
compute the total sum of rewards you receive, that's called a payoff
and then update Theta using this funny formula.
Right? Now, on every iteration of this algorithm,
um, you're going to update Theta.
And it turns out that reinforce is a stochastic gradient ascent algorithm.
Um, and you remember when we talked about,
uh, linear regression, right?
You saw me draw pictures like this.
It is a global minimum.
Then uh, gradient descents with just,
you know, take a straight path to the minimum,
but stochastic gradient descent would take
a more random path right towards the minimum and it kind of
oscillates around then, maybe it doesn't
quite converge unless you slowly decrease the learning rate alpha.
So this is what we have for stochastic gradient descent,
um, for linear regression.
What we'll see in a minute,
is that reinforce is a
stochastic gradient ascent algorithm meaning that each of these updates is random,
because it depends on what was
this state action sequence that you just saw and what was the payoff that you just saw.
But what will this show is that on expectation, the average update.
You know, this- this update to Theta.
This thing you are adding to theta,
that on average let's see,
that- that on average this update here is exactly in the direction of the, um, gradient.
So that on average,
um, you know, because, uh, every-every loop,
every time through this loop you're making
a random update to Theta and it's random and
noisy because it depends on this random state sequence.
Right? That and this state sequence is random because of
the state transition probabilities and also
because of the fact that you're choosing actions randomly.
But on- but the expected value of this update, uh,
you'll see in a little bit it turns out to be exactly the direction of the gradient.
Um, which is why this, uh,
reinforce algorithm is a gradient ascent algorithm.
Okay? So let's, uh,
let's show that now.
Okay.
So [NOISE] all right.
So what we want to do is maximize the expected payoff
which is a formula we derive up there and so,
um, we're going to,
want to take derivatives with respect to Theta of the expected pay-off.
Right? Of, uh, I'm just gonna copy
that formula up there
[NOISE].
Okay? So there's a chance
of that, you're going through that state-action sequence times the pay off.
And so we want to take derivatives of this and, you know,
so we can like go uphill using gradient ascent.
Um, so we're going to do this in, uh, four steps.
Um, now, first, um,
let me remind you when you take the derivative of three,
of- of a product of three things.
Right? So let's say that you have, uh, three functions,
f of Theta times g of Theta times h of Theta.
So by the product rule of,
um, you know, derivatives product rule from calculus,
the derivative of the product of three things is obtained by,
um, you know, taking the derivatives of each of them one at a time.
Right? So this is f prime times g times h plus,
um, g prime
here plus h prime.
Okay? So the product rule from calculus is
that if you want to take derivatives of a product of three things,
then you kind of take the derivatives one at a time and you end up with three sums.
Right? And so we're going to apply the product rule to this where, um,
we have- here we have two different terms that depend on, um,
Theta, and so when we take the derivative of this thing with respect to theta,
we're gonna have two terms.
Uh, that correspond to taking derivative of
this ones and taking the derivative of that ones.
Right? And so, um,
this derivative is equal to,
so the first term is the sum over all the state action sequences,
um, P of s0,
um, and then let's see.
So now we have pi of Theta, excuse me.
The derivative with respect to pi Theta,
s0, a0.
Right? And then plus, um,
[NOISE].
Right? And then times the payoff.
Right? So the whole thing here is then multiplied by the payoff.
Okay? So we just apply the product rule for calculus where,
uh, for the first term in the sum,
we kinda took the derivative of this first thing and then for
the second term of the sum we took the derivative of this second thing.
Okay? And now, um,
I'm gonna make one more algebraic trick which is,
I'm going to multiply and divide by that same term,
and then multiply and divide by the same thing here.
Right? So lots of multiply,
multiply and divide by the same thing.
Right? And then finally,
um, if you factor out.
So now, the final step is, um,
I'm- I'm gonna factor out these terms I'm underlining.
Ah, right?
Because this terms I underlined,
this is just you know,
the probability of the whole state sequence.
Right? And again, for the orange thing,
this this orange thing.
Right? These two orange things multiplied
together is equal to that orange thing on that box as well.
And so the final step is to factor out the orange box which is just P of s0,
a0, s1, a1, right?
So that's the thing I boxed-up in orange times,
then those two terms
involving the derivatives
[NOISE].
Times the payoff [NOISE].
Okay? And I think, ah, right,
where- because I guess this term goes there,
[NOISE] and this term goes there, okay?
And so this is just equal to,
um, well- and if you look at the reinforce algorithm,
right, that we wrote down,
ah, this is just equal to sum over, you know,
all the state action sequences times the
probability of the gradient update,
right.
[NOISE] Because, ah, I guess I'm running out of colors.
But, you know, this is a gradient update and that's just like equal to this thing, okay?
So what this shows is that, um,
even though on each iteration the direction of the gradient updates is random, um,
the, ah, the expected value of how you
update the parameters is exactly equal to the derivative of your objective,
of your expected total payoff, right.
So we started saying that this formula is your expected total payoff,
um, so let's figure out what's the derivative of your expected total payoff,
and we found that the expected- the- the derivative,
your expected total payoff,
the derivative of the thing you want to maximize is equal
to the expected value of your gradient update.
And so this proves that, um,
on average, you know,
if you have a very small learning rate,
you end up averaging over many steps, right?
But on average, the updates that reinforce is taking on
every iteration is exactly in the direction of the derivative of the,
um, expected total payoff that you're trying to maximize, okay makes sense?
Yes, any questions about this? Yeah.
[inaudible].
Oh, it is independent of the choice of its function.
Um, this is true for any form of a stochastic policy,
ah, where the definition is that, you know,
Pi Theta of s0, [NOISE] ah,
a0 has to be the chance of taking that action in that state,
but this could be any function you want.
Ah, it could be a softmax,
it could be a logistic function of many,
many different complicated features,
it could be- or it has to be a continuous de- or it has to be a differential function.
And actually one of the reasons we shifted to stochastic policies was because,
um, previously just have two actions,
is either left or right, right?
And so you can't define a derivative over a discontinuous function like either left or
right but now we have a probability that
shifts slowly between what's the probability to go left, versus
go right and by making this a continuous function of Theta,
you can then take derivatives and plot gradient ascent,
but it does need to be a logistic function.
Yeah, go ahead.
Ah, [inaudible]?
Sure. So, um, ah,
another way to train a, um,
helicopter controller is you use supervised learning,
where you have a human expert train,
um, you know, so you can also actually have a human pilot demonstrate in this state,
take this action, right,
and then you use supervised learning to just learn
directly a mapping from a state to the action.
Um, I think this, I don't know,
this might be okay for low speed helicopter flight,
I don't think it works super well, ah,
I bet you could do this and not crash a helicopter, but, ah, um, ah,
ah, but to get the best results,
I wouldn't use this approach, um, yeah.
It turns out for some of the maneuvers it'll actually
fly better than human pilots as well,
um, yeah, no.
Cool. All right.
Um, and so, um,
for other types of policies,
um, let's see, right.
[NOISE]
So, ah, direct policy search also works,
um, if you have continuous value actions and you don't want to discretize the actions.
So here's a simple example.
Let's say a is a real number, ah,
such as the magnitude of the force you apply to accelerating left or right. All right.
So run discretizing, you invert your pendulum,
you wanna output a continuous number of how hard you swerve to left or right.
Um, or for a self-driving car maybe Theta is
the steering angle which is a real value number.
So simple policy would be a equals,
you know, Theta transpose S, um,
and then plus [NOISE] Gaussian noise.
And if just for the purpose of training,
you're willing to pretend that your policy is to
apply the action Theta transpose S and add a little bit of Gaussian noise to it,
then, um, the whole framework for
reinforce but this type of gradient descent also, ah, will,
will also work, great, um,
and then I guess if you're actually implementing this,
you can probably turn off the Gaussian noise variability,
there, there are little tricks like that as well.
Um, so let's see.
Some pros and cons of, um, so,
whe - whe- when should you use direct policy search and when should you
use value iteration or a value function based type of approach?
Um, so it, ah,
turns out there's one setting, ah,
actually there are two settings where direct policy search works much better.
One is if you have a, um,
POMDP, ah, PO in this case stands for partially observable.
[NOISE] And that's if for example, um, you know,
for the inverted pendulum,
um, does a polar angle Phi, you have,
you have a car and this is your position x. Um,
and what this is saying that the state space is, ah,
x, x dot Phi, Phi dot.
All right? [NOISE] But let's say that, um,
you have sensors on this inverted pendulum that allow you to
measure only the position and only the angle of the inverted pendulum.
[NOISE] Uh, so you might have an angle sensor, you know,
down here and you may have a position sensor for your inverted pendulum,
but maybe you don't know the velocity or you don't know the angular velocity, right.
So this is an example of a partially observable Markov decision process because,
ah, and what this means is that on every step,
you do not get to see the host state because you,
you don't have enough sensors to tell you
exactly what is the state of the entire system, okay?
So in a partially observable MDP,
um, at each step,
you get a partial and potentially
noisy measurement of the state,
right, and then have to take actions, or,
have to choose an action a.
[NOISE]
Using these partial and potentially noisy elements, right?
Which is, uh, maybe you only observe the position and the angle,
but your senses aren't even totally accurate.
So you get a slightly noisy,
you know, estimate of the position.
You get a slightly noisy estimate of the angle but you just have to choose
an action based on your noisy estimates of just two of the four state variables, right?
Um, it turns out
that there's been a lot of
academic literature trying to generalize value function base approaches,
the POMDPs, ah, and they're very complicated algorithms in
the literature on trying to apply value function based approaches of POMDPs.
But those algorithms despite their very high level of complexity,
you know, are not- are not widely in production, right?
Um, but if you use the direct policy search algorithm,
then there's actually very little problem.
Oh, let me just write this out.
So let's say the observation is on every timestep you
observe y equals x Phi plus noise, right?
So you just don't know whether it's a state.
And in a POMDP you cannot approximate a value function.
Or even if you knew what was V star, right?
You can't compute Pi-star because,
uh, and maybe you know what is Pi star best.
This can compute V star and Pi star.
But if you don't know what the state is,
you can't apply Pi star to the state because- so- so how do you choose an action.
Um, if you're using direct policy search,
then here's one thing you could do.
Which is you can say that, uh,
Pi of, um, given an observation,
the chance of going to the right given your current observation is equal
to 1 over 1 plus e to negative Theta transpose y,
where I guess y can be 1,
right, x plus noise, Phi plus noise.
But, sorry that's x plus noise,
Phi plus noise, right?
And so you could run reinforce using just the observations you have to, um,
try to- stochastically try to randomly choose an action,
and nothing in the framework we talked about prevents this algorithm from working.
And so direct policy search just works very
naturally even if you have only partial observations of the state.
Um, and more generally instead of plugging the direct observations this can
be any set of features, right?
I'll just make a side comment for those who don't know
what common filters are. Don't worry if you don't.
But one common- one common way of, uh, uh,
using direct policy search would be to use some estimates such as common filter,
or probabilistic graphical model or something to use your historical estimates.
Look, don't, don't just look at your one, uh,
set of measurements now but look at all the historical meas- measurements.
And then there are algorithms such as something we call
the common filter that lets you estimate whatever is the current state,
the full state vector.
You can plug that full state vector estimate into
the features you use to choose- to choose an action.
That's a common design paradigm.
If you don't know what the common filter is, don't worry about it.
Ah, we take- take one of Stephen Boyd's classes or something, I don't know. Yeah, right.
But, but that's one common paradigm where you can use your partial observations as
for the full state and plug that as a feature into the policy search, okay?
So that's one setting where direct policy search works.
Um, just, just applies in a way that
value function approximation is very difficult to even get to apply.
Um, now one last thing is,
uh, one last consideration
so should you apply search policy search algorithm
or a value function approximation algorithm?
Oh, it turns out, um,
the reinforce algorithm is,
is actually very inefficient.
Ah, as in, ah, you end up, you know, whe- when,
when you look at research papers on the reinforce algorithm,
it's not unusual for people that run the reinforce algorithm for like a million iterations,
or 10 million iterations.
So you just have to train. It turns out that gradient estimates
for the reinforce algorithm even though the expected value is right,
it's actually very noisy.
And so if you train the reinforce algorithm,
you end up just running it for a very,
very, very long time, right?
It does work but it is a pretty inefficient algorithm.
So that's one disadvantage of the reinforce algorithm is that
the gradient estimates on expectation are exactly what you want it to be,
but there's a lot of variance in the gradient.
So you have to run it for a long time for a very small learning range.
Um, but one other reason to use, um,
direct policy search is,
is kind of ask yourself,
do you think Pi star is simpler?
Or is V star simpler, right?
And so, um, here's what I mean,
there are, ah, ah, ah,
there- in, in robotics,
there's sometimes what we call low-level control tasks.
And, uh, one way to think of low-level control task is flying a helicopter.
Hovering a helicopter is example of a low-level control task.
And one way to inform of when you think of low-level control task is kind of a really skilled human,
um, you know, holding a joystick.
Control this thing, making seat of the pants decisions, right?
So those are kind of almost instinctual,
in a tiny fraction of a second,
almost by feel you could control the thing.
Those, those are- tend to be low-level control tasks.
Those are seat of the pants, holding a joystick,
a skilled person could balance the inverted pendulum or,
you know, steer a helicopter.
Those are low-level control tasks.
In contrast, um, playing chess is not a low-level control task.
You know, because for the most part,
a very good chess player is not really a seat of the pants, you know,
take that- make a decision in like- in,
in 0.1 seconds, right.
You kind of have to think multiple steps ahead.
Um, and in low-level control tasks,
there's usually some control policy that is quite simple.
A very fun- simple function mapping some state actions, that's pretty good.
And so that allows you to specify a relatively simple class of functions of
Pi star and direct policy search would be relatively promising for tasks like those.
Whereas in contrast, if you want to play chess or play Go,
or do these things where we have multiple steps of reasoning,
um, I think that,
if you're driving a car on a straight road,
that's a low-level control task.
Where you just look at the road and you just, you know,
you know turn the steering a little bit to stay on the road.
So that's a low-level control task.
But if you are planning how to, um, you know,
overtake this car and avoid that other car,
or there's a pedestrian and a bicycle is along the way,
then that's less of a low level control task.
Um, and that requires more multi-step reasoning, right?
I guess depends on how aggressive of a driver you are, right?
Driving on the highway, you know,
may require more or less multistep reasoning.
Where you want to, ah,
overtake this car before the truck comes in this lane.
So that- that type of thing is,
um, more multi-step reasoning.
Um, and approaches like that tend to be
difficult for a very simple like a linear function to be a good policy.
And for those things in playing chess, playing Go,
playing checkers, um, a value function approximation approach may be more promising.
Okay, um, cool.
So any, um,
questions about the- oh, and so, um,
okay for, for, ah, autonomous helicopter flight, ah,
actually, my first attempt for flying
helicopters were actually a direct policy search because flying helicopters,
are actually a seat of the pants thing.
Ah, but then when you try to fly more complex maneuvers,
then you end up using something maybe closer to
value function approximation methods if you want to fly a very complicated maneuver, right?
Um, oh, so the video you saw just now,
of the helicopter flying upside down,
the algorithm implemented on, you know,
for that particular video that was a direct policy search algorithm, right?
Not, not exactly this one, a little bit different.
But that was a direct policy search algorithm.
But if you want the helicopter to fly a very complicated maneuver,
then you need something maybe closer to a value function approximator.
And so the- and there is exciting research on how to blend
direct policy search approaches together with value function approximation approaches.
So actually AlphaGo.
Ah, ah, ah, ah, one of the reasons AlphaGo works was,
um, sorry, ah, you know Go playing program, right, by DeepMind,
ah, was, was a blend of ideas from both of these types of
literature which enabled it to scale to a much bigger system to play Go and,
you know clearly at a very, very impressive level.
All right. Any questions about this, anyone?
[NOISE]
All right. Um, so just final application examples, um, you know,
reinforcement learning today, um,
is, uh, making strong- let's see.
So there's a lot of work on reinforcement learning for game playing,
Checkers, Chess, um, uh, Go.
That is exciting, um,
reinforcement learning today is used in, uh,
is used in a growing number of robotics applications,
um, I think for controlling a lot of robots.
Um, there is a, uh- if you've go to the robotics conferences,
if you look at some of the projects being done by some of
the very large companies that make very large machines, right.
Uh, I have many friends in multiple, you know,
large companies making large machines that
are increasingly using reinforcement learning to control them.
Um, there is fascinating work, uh,
using reinforcement learning for optimizing,
um, entire factory deployments.
Um, there is, uh,
academic research work, uh,
still in research for a class, I know,
actually may- maybe Science to be deployed on
using reinforcement learning to build chatbots.
Um, uh, uh, and actually, on, on,
on using reinforcement learning to, uh,
build a, uh, AI-based guidance counselor,
for example, right, where,
uh, the actions you take up,
of what you say to students, and then,
and then the reward is, you know,
do you manage to help a student navigate their coursework and navigate their career.
Uh, there is, uh, uh,
and that's also starting to be applied to healthcare,
where- one of the keys of reinforcement learning is,
this is a sequential decision making process, right?
Where, do you have to take a sequence of decisions that may affect your reward over time?
And I think um,
uh, and, uh, in,
in healthcare, there is work on medical planning,
where, um, the goal is not to, you know,
send you to get, uh,
a blood test and then we're done, right?
In, in, in complicated, um,
medical procedures, we might essentially get a blood test,
then based on the outcome of the blood test,
we might send you to get a biopsy or not,
or we might ask you to take a drug and then come back in two weeks.
But this is a very complicated sequential decision-making process
for a treatment of complicated healthcare conditions,
and so there's fascinating work on trying to apply
reinforcement learning to this set of multi-step reasoning,
where it's not about, well,
we'll send you for a treatment and then you'll never see again for the rest your life.
It's about here's the first thing you do then come back,
let's see what state you get to after taking this blood test,
or let's see what- state you get to after trying a drug,
and then coming back in a week to see what has happened to symptoms.
But I think that, um,
these are all sectors where reinforcement learning,
uh, is making inroads, um,
or, or even actually, stock trading.
Okay, maybe not the most inspiring one,
but one of my friends, um,
on the East Coast was, uh, uh, in,
in- was, uh- and just actually,
if, if you or your parents,
uh, invest in mutual funds,
this may be being used to, um,
buy and sell shares with them today,
depending on what bank they are investing.
I know what bank is doing this, but I won't say it out loud.
Uh, but, um, uh, uh, but,
uh, if you want to buy or sell, you know, say,
a million shares of stock,
a, a very large volume of stock,
you may not want to do it in
a very public way because that will affect the price of the shares, right?
So everyone knows that a very large investor
is about to buy a million shares or buy 10 million shares or whatever,
that will, um, cause the price to increase, uh,
and this, this is,
this disadvantages the person wanting to buy shares.
But so that's been very interesting work on using reinforcement learning to, um,
decide how to sequence out your, your buy,
how to buy the stock in small lots,
and this trading market is called dark pools.
You could Google if you're curious.
Actually, don't bother, uh, uh, uh, to,
to try to, um,
buy a very large lot of shares.
Also, sell a very large lot of shares without affecting
the market price too much because the way you affect
the market price always breaks against,
you know, is always against you,
it's always bad, right.
Um, so there's work laid down as well.
So anyway, I think, um, uh,
many applications- I personally think that one of
the most exciting areas for reinforcement learning will be robotics,
but, uh, we'll, we'll see what,
what happens over the next few years, right?
Okay. All right.
So let's see,
well, just five more minutes.
Um, and, and just to wrap up, I think,
you know, um, uh,
we've gone through quite a lot of stuff.
I guess, uh, for supervised learning to, uh, learning theory,
and advice for applying learning algorithms to unsupervised learning,
although- was it, K-means, PCA, uh,
EM mixture of Gaussians, uh,
factor analysis, and PrinCo analysis to most recently,
reinforcement learning with the value function approaches,
fitted value iteration, policy search.
So, um, feels like we did, feels like,
feels like- I- feels like you've seen a lot of learning algorithms.
Um, go ahead.
[inaudible]
How is reinforcement learning compared to adversarial learning?
I think of those as pretty distinguished literatures.
Uh, uh, yeah, yeah,
so I think, uh, and again, actually, I, I,
I know a lot of
non-publicly known facts about the machine learning world, but, uh,
one of the things that I actually happen to know is that, er,
uh, some of the ideas in adversarial learning, uh, uh, you know,
so can you tweak a picture by,
you know, very little bit,
by tweaking a bunch of pixel values that are not visible to human eye,
that fools the learning algorithm into thinking that this picture is actually a cat,
when it's clearly not a cat or whatever.
So I actually know that there are attackers out in the world
today using techniques like that to attack,
you know, websites, to try to fool, um, uh, ah,
you know- some of the websites I'm pretty sure you guys use,
and fool their anti-spam,
anti-fraud, anti-undermining democracy types of algorithms into,
um, [LAUGHTER] into making poor decisions.
Uh, so, so it's an exciting to time to do machine learning, right?
[LAUGHTER] That, that, we get to fight battles like these [LAUGHTER]. Yeah, I'm sorry.
Um, uh, okay, um,
and, and I think, you know, I think with,
with- really, I think that with
the things that you guys have learned in machine learning,
I think all of you, um, uh,
are now very knowledgeable, right?
I think all of you are experts in all the ideas of core machine learning,
and I hope that, um- I,
I think you- when you look around the world,
there are so many worthwhile projects you
could do with machine learning and the number of
you that know these techniques is so small that I hope that,
um, you take these skills.
Um, and some of you will go, you know,
build businesses and make a lot of money, that's great.
Some of you will take these ideas and, uh,
help drive basic research at Stanford or at other institutions.
I think that's fantastic.
But I think whatever you are doing,
the number of worthwhile projects on the planet is so large and
the number of you that actually know how to use
these techniques is so small that I hope that,
um, you take these skills you're learning from
this course and go and do something meaningful,
go and do something that helps other people.
Um, I think we are seeing in the Silicon Valley that there a lot of ways, you know,
to build very valuable businesses, uh,
and some of you do that and that's great,
but I hope that you do it in a way that helps other people.
Um, uh, I think, er,
over the past few years we've seen,
uh, um- I think that,
er, in Silicon Valley,
maybe 10 years ago,
the contract we had with society was that people would
trust us with their data and then we'll use their data to help them.
But I think in the past year,
that contract feels like it has been broken and
the world's faith in Silicon Valley has been shaken up,
but I think that places even more pressure on all of us, on all of you,
to make sure that, um,
the work you go out in the world to do is
work that actually is respectful of individuals,
respectful of individual's privacy,
is transparent and open, and ultimately is,
uh, helping drive forward,
um, uh, humanity, or helping people,
or helping drive forward basic research,
or building products that actually help people,
rather than, um, exploit their foibles for profit but to their own harm.
So I hope that all of you will take your superpowers that you now have,
and, um, go out to,
to, to do meaningful work.
Um, and let's see,
um, and I think, uh- oh, and,
and lastly, just, just on a personal note,
I want, to, you know, thank all of you.
On behalf of the TAs and the whole teaching team and myself,
I want to thank all of you for your hard work.
Uh, sometimes, they go over the homework problems.
They look at probably some of these problems and go, "Wow,
there she got that problem, I thought that was really hard," or the project milestones go,
"Hey, that's really cool, look forward to seeing
your final project results at the final poster session."
So, um, I know that all of you have worked really hard.
Uh, uh, and if you didn't,
don't tell me, but I think almost all of you have [LAUGHTER] but,
but, but, I will make sure you know there's
a- I think it wasn't that long ago that I was a student,
you know, working late at night on homework problems and,
and I know that many of you have been doing that, uh, for the homework,
for studying for the midterm,
um, for working on your final term project.
So, um I want to make sure, um,
you know I'm very grateful for the hard work you put into this course,
and I hope that, um- I hope that, uh, your,
your hard earned skills will also reward you very well in the future,
and also help you do work that,
that you find as meaningful,
so thank you very much [APPLAUSE].